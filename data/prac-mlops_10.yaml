- en: Chapter 10\. Machine Learning Interoperability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 机器学习的互操作性
- en: By Alfredo Deza
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: By Alfredo Deza
- en: Mammalian brains have considerable power for generalized computation but special
    functions (e.g., subjectivity) commonly require specialized structures. Such a
    hypothesized structure has been facetiously termed a “subjectivity pump” by Marcel
    Kinsbourne. Well, that is exactly what some of us are looking for. And the mechanism
    for subjectivity is double, as shown by the duality of the anatomy, the success
    of hemispherectomy, and the split-brain results (in cats and monkeys as well as
    humans).
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 哺乳动物的大脑具有广泛的计算能力，但是某些特殊功能（例如主观性）通常需要专门的结构。这种假设的结构已经被马塞尔·金斯本戏称为“主观性泵”。对于我们中的一些人来说，这确实是我们正在寻找的。而主观性的机制是双重的，如解剖学的二重性所示，通过半球切除术的成功以及分裂脑的结果（在猫和猴子以及人类身上）。
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dr. Joseph Bogen
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Dr. Joseph Bogen
- en: Peru has *several thousand* potato varieties. As someone who grew up in Peru,
    I find this surprising to hear. It is easy to assume that most potatoes taste
    somewhat similar, but this is not the case at all. Different dishes call for different
    potato varieties. You won’t want to argue with a Peruvian cook if the recipe called
    for *Huayro* potatoes and you want to use the common *baking* potato found in
    most of the US. If you ever have the chance to be in South America (and certainly
    in Peru), try the experience of walking around a street market. The number of
    fresh vegetables, including several dozen potato varieties, can make you dizzy.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 秘鲁有*数千*种土豆品种。作为一个在秘鲁长大的人，我发现这听起来令人惊讶。很容易假设大多数土豆的味道都有些类似，但事实并非如此。不同的菜肴需要不同的土豆品种。如果食谱要求使用*Huayro*土豆，而你想使用大多数美国超市都能找到的常见*烘烤*土豆，你可能会不想与秘鲁厨师争论。如果你有机会去南美（特别是秘鲁），尝试走进街头市场的经历。那里新鲜蔬菜的数量，包括几十种土豆品种，可能会让你眩晕。
- en: I no longer live in Peru, and I miss that variety of potatoes. I can’t really
    cook some dishes and make them taste the same with a common baking potato bought
    from the local supermarket. It is not the same. I consider that the variety and
    different tastes provided by such a humble vegetable is critical for Peru’s cuisine
    identity. You still might find it OK to cook with the common baking potato, but
    at the end of the day it is about choices and selections.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我不再生活在秘鲁，我错过那里的土豆品种。我真的无法用本地超市购买的普通烘烤土豆来做出相同口感的一些菜肴。这完全不一样。我认为这种谦逊蔬菜所提供的多样性和不同口味对秘鲁的美食身份至关重要。你可能认为用常见的烘烤土豆烹饪还可以，但归根结底，这是关于选择和挑选。
- en: 'There is empowerment in variety and the ability to pick and choose what fits
    better. This empowerment is also true in machine learning when dealing with the
    end product: the trained model.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在多样性和挑选适合自己的东西的能力中有一种赋权感。当涉及到最终产品——训练好的模型时，这种赋权感在机器学习中同样适用。
- en: 'Trained machine learning models also come with distinct constraints, and most
    of them aren’t going to work in an environment that isn’t specifically tailored
    to support it. The main concept of model interoperability is to be able to *transform*
    a model from one platform to another, which creates choices. This chapter makes
    the case that, although there are sound arguments to use out-of-the-box models
    from a cloud provider and not worry much about vendor lock-in, it is essential
    to understand how to export models into other formats that can work in other platforms
    that have different constraints. Much like how containers can work seamlessly
    in most any system as long as there is an underlying container runtime (as I explain
    in [“Containers”](ch03.xhtml#Section-containers)), model interoperability or having
    a model exported to different formats is key to flexibility and empowerment. [Figure 10-1](#Figure-10-1)
    demonstrates a rough overview of what this interoperability means by training
    in any ML framework, transforming the resulting model *once*, to deploy it almost
    anywhere: from edge devices to mobile phones and other operating systems.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过的机器学习模型也有其独特的限制，大多数模型无法在未经专门定制支持的环境中工作。模型互操作性的主要概念是能够将模型从一个平台转换到另一个平台，从而提供了选择。本章论述的观点是，虽然使用云供应商提供的即用即得模型并不用过多担心供应商锁定是有道理的，但理解如何将模型导出为其他可以在具有不同约束的平台上工作的格式是非常重要的。就像容器只要有底层容器运行时就可以在大多数系统中无缝运行一样（正如我在[“容器”](ch03.xhtml#Section-containers)章节中解释的那样），模型互操作性或将模型导出为不同格式对灵活性和增强至关重要。[图10-1](#Figure-10-1)通过训练任何机器学习框架，在导出结果模型*一次*后，几乎可以在任何地方部署：从边缘设备到手机和其他操作系统。
- en: '![pmlo 1001](Images/pmlo_1001.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 1001](Images/pmlo_1001.png)'
- en: Figure 10-1\. Interoperability overview
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1\. 互操作性概述
- en: This situation is like producing screws that can work with any screwdriver,
    instead of producing screws that can only work with screwdrivers from a single
    home improvement store. In [“Edge Devices”](ch03.xhtml#Section-edge-devices),
    we already ran into problems when a model couldn’t work with the Edge TPU, and
    conversion was ultimately necessary. Currently, there are some exciting options,
    and I’m particularly surprised by ONNX, a community-driven project with open standards
    that wants to make it easier to interact with models by reducing complexity from
    toolchains. This chapter will go into some of the details that make ONNX a compelling
    machine learning choice and how most clouds are already supporting this format.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况就像制造可以与任何螺丝刀配合使用的螺钉，而不是只能与单一家装修店的螺丝刀配合使用的螺钉。在[“边缘设备”](ch03.xhtml#Section-edge-devices)章节中，我们已经遇到了一个模型无法与Edge
    TPU配合工作的问题，最终需要进行转换。目前有一些令人兴奋的选择，我对ONNX特别惊讶，这是一个由社区驱动的、采用开放标准的项目，旨在通过简化工具链减少与模型交互的复杂性。本章将深入探讨使ONNX成为引人注目的机器学习选择的一些细节，以及大多数云平台已经支持该格式的原因。
- en: Why Interoperability Is Critical
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为何互操作性至关重要
- en: Abstracting complex processes and interactions is a typical pattern in software
    engineering. Sometimes the abstractions can get entirely out of hand, causing
    the abstraction to be as complex as the underlying software it was trying to abstract.
    An excellent example of this is Openstack (an open source infrastructure-as-service
    platform) and its installer. Installing and configuring an infrastructure platform
    can be very complicated. Different machine types and network topologies create
    a tricky combination to solve with a one-size-fits-all installer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中，抽象复杂的过程和交互是一种典型模式。有时，这些抽象可能会变得非常复杂，导致抽象本身与其试图抽象的底层软件一样复杂。一个很好的例子是Openstack（一个开源的基础设施即服务平台）及其安装程序。安装和配置基础设施平台可能非常复杂。不同的机器类型和网络拓扑创建了一个棘手的组合，需要用一个通用的安装程序解决。
- en: A new installer was created to make it easier to install TripleO (Openstack
    On Openstack). TripleO produced a temporary instance that, in turn, would install
    Openstack. The project solved many problems associated with installation and configuration,
    but some thought it was still complicated, and there was a need to abstract further.
    This is how QuintupleO got created (Openstack On Openstack On Openstack). Without
    being actively involved in Openstack, I can tell you that it is difficult to deploy
    and that engineering teams are trying to solve those problems generically. But
    I’m doubtful that adding another layer is the solution.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了一个新的安装程序，以使安装TripleO（Openstack On Openstack）变得更加容易。TripleO生成了一个临时实例，进而安装Openstack。该项目解决了许多与安装和配置相关的问题，但有人认为它仍然很复杂，并且需要进一步的抽象。这就是QuintupleO（Openstack
    On Openstack On Openstack）的诞生。不参与Openstack的活动，我可以告诉你，部署它是很困难的，工程团队正在试图通用地解决这些问题。但我怀疑增加另一层是解决方案。
- en: 'It is easy to get persuaded that another layer will make things easier, but
    in reality, this is very hard to do well while pleasing everyone. I have an open
    question I often use to design systems: *the system can be very simple and opinionated,
    or flexible and complex. Which one do you choose?* Nobody likes these options,
    and everyone pushes for *simple and flexible*. It is possible to create such a
    system, but it is challenging to attain.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易被说服认为增加另一层会使事情变得更容易，但实际上，这很难做到既满足大家又做得好。我经常用来设计系统的一个开放性问题是：*系统可以非常简单且专断，也可以灵活且复杂。你会选择哪个？*
    没有人喜欢这些选项，每个人都推崇*简单且灵活*。虽然可以创建这样的系统，但要达到这一点是具有挑战性的。
- en: In machine learning, multiple platforms and cloud providers train models in
    different and particular ways. This doesn’t matter much if staying within the
    platform and how it interacts with the model, but it is a recipe for frustration
    if you ever need to get that trained model running elsewhere.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，多个平台和云服务提供商以不同和特定的方式训练模型。如果仅在平台内部保持并与模型交互，这并不重要，但如果您需要在其他地方运行训练好的模型，则可能会引起挫败感。
- en: While training a dataset with AutoML on Azure recently, I encountered several
    problems while trying to get local inferencing working. Azure has a “no-code”
    deployment with AutoML, and several types of trained models support this type
    of deployment. This means there is no need to write a single line of code to create
    the inferencing and serve the responses. Azure handles the API documentation that
    explains what the inputs and expected outputs are. I couldn’t find the *scoring
    script* for the trained model, and I couldn’t find any hints about the scoring
    script that would help me run it locally. There is no obvious way to understand
    how to load and interact with the model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在Azure上使用AutoML训练数据集时，我在尝试本地推理时遇到了几个问题。Azure具有AutoML的“无代码”部署，并且多种类型的训练模型支持这种部署方式。这意味着无需编写任何代码即可创建推理并提供响应。Azure处理API文档，说明输入和预期输出是什么。我找不到训练模型的*评分脚本*，也找不到任何有助于在本地运行它的评分脚本的提示。没有明显的方法来理解如何加载和与模型交互。
- en: The model’s suffix implied it is using Python’s `pickle` module, so after trying
    a few different things, I managed to get it loaded but couldn’t do any inferencing
    on it. I had to deal with the dependencies next. AutoML in Azure doesn’t advertise
    the exact versions and libraries used to train a model. I’m currently using Python
    3.8 but wasn’t able to install the Azure SDK in my system because the SDK only
    supports 3.7\. I had to install Python 3.7, then create a virtual environment
    and install the SDK there.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的后缀暗示它使用Python的`pickle`模块，因此在尝试了几种不同方法后，我设法加载了它，但无法进行任何推理。接下来我得处理依赖关系。Azure中的AutoML并不公布用于训练模型的确切版本和库。我目前正在使用Python
    3.8，但无法在系统中安装Azure SDK，因为SDK仅支持3.7版本。我不得不安装Python 3.7，然后创建一个虚拟环境，并在那里安装SDK。
- en: One of the libraries (*xgboost*) had a non–backward compatibility for its latest
    versions (modules were moved or renamed), so I had to guess one that would allow
    a specific import. That turned out to be 0.90 from 2019\. Finally, when training
    a model with AutoML in Azure, it seems to use the latest version of the Azure
    SDK *at the time the model gets trained*. But this isn’t advertised either. That
    is, if a model is trained in January, and you are trying to use it a month later
    with a few SDK releases in between, you can’t use the latest SDK release. You
    have to go back and find the latest release of the SDK when Azure trained the
    model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个库（*xgboost*）在其最新版本中存在非向后兼容性（模块被移动或重命名），因此我不得不猜测一个允许特定导入的版本。结果证明是2019年的0.90版本。最后，在Azure的AutoML中训练模型时，似乎使用了当时最新版本的Azure
    SDK。但这也没有进行宣传。也就是说，如果一个模型在一月份训练，而您在之后的一个月尝试使用它，并且SDK有几个版本更新，您无法使用最新的SDK版本。您必须回到Azure训练模型时的最新SDK版本。
- en: 'By no means is this situation meant to be an overly critical description of
    Azure’s AutoML. The platform is excellent to use and could improve by better advertising
    what versions are used and how to interact with a model locally. The prevalent
    problem is that you lose control granularity in the process: low code or no code
    is excellent for speed but can get complicated for portability. I faced all these
    problems by trying to do local inferencing, but the same could happen if you are
    leveraging Azure for machine learning in general, but your company is using AWS
    for hosting.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况绝不意味着对Azure的AutoML过度批评。这个平台使用起来非常出色，并且通过更好地宣传所使用的版本及如何与本地模型交互可以进一步改进。主要问题在于在过程中失去了控制粒度：低代码或无代码对速度来说很好，但在可移植性方面可能会变得复杂。我通过尝试进行本地推理时遇到了所有这些问题，但如果您的公司一般使用AWS进行机器学习，但使用Azure进行托管，则可能会出现相同情况。
- en: Another problematic situation that happens often is that scientists that produce
    a model in one platform have to make assumptions, such as the underlying environment,
    which includes compute power, storage, and memory. What happens when a model that
    performs well in AWS needs to deploy in an edge TPU device that is entirely incompatible?
    Let’s assume for a second that your company has this situation covered already
    and produces the same model targeting different platforms. Still, the resulting
    model for the edge device is five gigabytes, which is beyond the accelerator’s
    maximum storage capacity.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 经常出现的另一个问题是，科学家在一个平台上创建模型时不得不做一些假设，比如底层环境，包括计算能力、存储和内存。如果一个在AWS上表现良好的模型需要部署到完全不兼容的边缘TPU设备，会发生什么？假设你的公司已经解决了这种情况，并且为不同平台生成相同的模型。但是，边缘设备的结果模型达到了五千兆字节，超出了加速器的最大存储容量。
- en: Model interoperability solves these problems by openly describing the constraints,
    making it easier to *transform models* from one format to the other while enjoying
    support from all the prominent cloud providers. In the next section, I’ll go into
    the details of what makes ONNX a solid project for greater interoperability and
    how you can build automation to transform models effortlessly.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 模型互操作性通过公开描述约束条件来解决这些问题，使得可以在享受所有主要云提供商支持的同时，轻松地将模型从一种格式转换为另一种格式。在下一节中，我将详细介绍ONNX作为更强大的互操作性项目的细节，以及如何构建自动化来轻松转换模型。
- en: 'ONNX: Open Neural Network Exchange'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ONNX: 开放神经网络交换**'
- en: As I’ve mentioned before, ONNX is not only a great choice for model interoperability
    but also the first initiative toward a system that would allow switching frameworks
    with ease. The project started in 2017 when both Facebook and Microsoft presented
    ONNX as an open ecosystem for AI model interoperability and jointly developed
    the project and tooling to push forward the adoption. Since then, the project
    has grown and matured as a large open source project with an established structure
    that includes SIGs (Special Interest Groups) and working groups for different
    areas like releases and training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，ONNX 不仅是模型互操作性的一个很好选择，也是朝着允许轻松切换框架的系统的第一个倡议。这个项目始于2017年，当时Facebook和Microsoft将ONNX作为人工智能模型互操作的开放生态系统，并共同开发了该项目和工具以推动其采纳。此后，该项目作为一个大型开源项目成长和成熟，具有包括特别兴趣组（SIGs）和工作组（working
    groups）在内的完整结构，涵盖不同领域如发布和培训。
- en: Beyond interoperability, the commonality of the framework allows hardware vendors
    to target ONNX and impact multiple other frameworks at once. By leveraging the
    ONNX representation, the optimizations are no longer required to be integrated
    individually into each framework (a time-consuming process). Although ONNX is
    relatively recent, it is refreshing to see it well supported across cloud providers.
    It shouldn’t be surprising that Azure even offers native support for ONNX models
    in its machine learning SDK.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了互操作性之外，框架的普遍性允许硬件供应商针对ONNX并同时影响多个其他框架。通过利用ONNX表示，优化不再需要单独集成到每个框架中（这是一个耗时的过程）。尽管ONNX相对较新，但令人振奋的是它在各个云提供商中得到了良好的支持。不足为奇的是，Azure甚至在其机器学习SDK中为ONNX模型提供了原生支持。
- en: 'The main idea is to train once in your preferred framework and run anywhere:
    from the cloud to the edge. Once the model is in the ONNX format, you can deploy
    it to various devices and platforms. This includes different operating systems
    as well. The effort to make this happen is substantial. Not many software examples
    come to mind that can run in several different operating systems, edge devices,
    and the cloud, all using the same format.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 主要思想是在您喜欢的框架中进行一次训练，然后在任何地方运行：从云端到边缘设备。一旦模型以ONNX格式存在，您可以将其部署到各种设备和平台上。这包括不同的操作系统。实现这一点的努力是巨大的。不多的软件示例能够在多个不同的操作系统、边缘设备和云端上使用相同的格式运行。
- en: Although there are several machine learning frameworks supported (more being
    added all the time), [Figure 10-2](#Figure-10-2) shows the most common transformation
    pattern.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有几种支持的机器学习框架（随时添加更多），[图 10-2](#Figure-10-2) 展示了最常见的转换模式。
- en: '![pmlo 1002](Images/pmlo_1002.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 1002](Images/pmlo_1002.png)'
- en: Figure 10-2\. Convert into ONNX
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 转换为ONNX
- en: 'You leverage the knowledge and features of the framework you like best and
    then transform into ONNX. However, as I demonstrate in [“Apple Core ML”](#Section-CoreML),
    it is also possible (although not that common) to convert an ONNX model into a
    different runtime as well. These transformations are not “free” either: you can
    get into problems when new features aren’t supported yet (ONNX converters are
    always catching up), or older models aren’t supported with newer versions. I still
    believe that the idea of commonality and “run anywhere” is a solid one, and it
    is helpful to take advantage of it when possible.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以利用您最喜欢的框架的知识和功能，然后转换为ONNX。然而，正如我在[“苹果Core ML”](#Section-CoreML)中所演示的，也可以（虽然不太常见）将ONNX模型转换为不同的运行时。这些转换也并非“免费”：当新功能尚不支持时（ONNX转换器总是在赶上），或者旧模型不受新版本支持时，可能会遇到问题。我仍然相信普遍性和“随处运行”的想法是稳固的，并且在可能时利用它是有帮助的。
- en: Next, let’s see where you can find pretrained ONNX models, to try some of them
    out.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看您可以在哪里找到预训练的ONNX模型，以尝试其中一些。
- en: ONNX Model Zoo
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ONNX 模型动物园
- en: 'The *Model Zoo* is often referenced when discussing ONNX models. Although it
    is usually described as a registry for ready-to-use ONNX models, it is primarily
    an [informational repository in GitHub](https://oreil.ly/gX2PB) that has links
    to several pretrained models contributed by the community and curated in the repository.
    The models are separated into three categories: vision, language, and everything
    else. If you are looking to get started with ONNX and do some inferencing, the
    Model Zoo is the place to go.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型动物园* 在讨论ONNX模型时经常被提及。尽管它通常被描述为一个准备好使用的ONNX模型注册表，但它主要是一个[GitHub上的信息库](https://oreil.ly/gX2PB)，其中包含社区贡献并在库中策划的几个预训练模型的链接。这些模型分为三类：视觉、语言和其他。如果您想要开始使用ONNX并进行一些推理，模型动物园是您可以前往的地方。'
- en: In [“Packaging for ML Models”](ch04.xhtml#Section-packaging-models) I used the
    *RoBERTa-SequenceClassification* model from the Model Zoo. Since I wanted to register
    in Azure, I was required to add some information like the ONNX runtime version.
    [Figure 10-3](#Figure-10-3) shows how this is all available [at the Model Zoo](https://oreil.ly/ptOGC)
    for that particular model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“ML模型打包”](ch04.xhtml#Section-packaging-models)中，我使用了模型动物园中的*RoBERTa-SequenceClassification*模型。因为我想要在Azure中注册，所以我需要添加一些信息，比如ONNX运行时版本。[图
    10-3](#Figure-10-3) 显示了针对该特定模型在[模型动物园](https://oreil.ly/ptOGC)中的所有内容。
- en: '![pmlo 1003](Images/pmlo_1003.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 1003](Images/pmlo_1003.png)'
- en: Figure 10-3\. Model Zoo
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 模型动物园
- en: 'Aside from version and size information, the page will usually have some examples
    on how to interact with the model, which is crucial if you want to create a proof
    of concept to try it out quickly. There is one other thing that I believe is worth
    noting about these documentation pages: getting a *provenance* (a source of truth
    for the model). In the case of the *RoBERTa-SequenceClassification* model, it
    originates from *PyTorch RoBERTa*, then to ONNX, and finally made available in
    the Model Zoo.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除了版本和大小信息外，页面通常还会提供一些关于如何与模型交互的示例，这对于快速创建概念验证至关重要。关于这些文档页面，我认为值得注意的另一点是获取*来源*（模型的真实来源）。在*RoBERTa-SequenceClassification*模型的情况下，它起源于*PyTorch
    RoBERTa*，然后转换为ONNX格式，并最终在模型动物园中提供。
- en: It isn’t immediately clear why knowing the origin of models and sources of work
    is essential. Whenever changes are needed or problems are detected that need to
    be solved, you had better be ready to pinpoint the source of truth so that anything
    that needs to be modified can be done with confidence. When I was a release manager
    for a large open source project, I was responsible for building RPM and other
    package types for different Linux distributions. One day the production repository
    got corrupted, and I was asked to rebuild these packages. While rebuilding them,
    I couldn’t find what script, pipeline, or CI platform was producing one of these
    packages included in several dozen of these repositories.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 弄清楚模型的来源和工作源头的重要性并不是显而易见的。每当需要进行更改或发现需要解决的问题时，最好准备好准确指出真相源头，以便可以放心地进行任何需要修改的操作。在我担任大型开源项目的发布经理时，我负责为不同的Linux发行版构建RPM和其他类型的软件包。有一天，生产库损坏了，我被要求重建这些软件包。在重建过程中，我找不到哪个脚本、流水线或CI平台生成了几十个这些软件包中包含的一个软件包。
- en: After tracing the various steps involved in finding the origin of that one package,
    I found that a script was downloading it from a developer’s home directory (long
    gone from the company) in a server that had nothing to do with building packages.
    A single file sitting in a home directory in a server that had no responsibility
    for the build system is a ticking bomb. I couldn’t tell what the origin of the
    package was, how to make any updates to it, or the reason it needed to be included
    in this way. These situations are not uncommon. You must be prepared to have everything
    in order and have a solid answer when determining the source of truth of all the
    elements in your production pipeline.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在追踪找到那个软件包来源的各种步骤之后，我发现一个脚本正在从开发者的家目录（该开发者早已离开公司）中下载它，而这台服务器与构建软件包毫无关系。一个单独的文件坐落在一个与构建系统无关的服务器的家目录中是一个定时炸弹。我无法确定软件包的来源，如何对其进行任何更新，或者它以这种方式需要被包含的原因。这些情况并不少见。您必须准备好使一切井井有条，并且在确定生产管道中所有元素的真实来源时有一个坚实的答案。
- en: When you are sourcing models from places like the Model Zoo, make sure you capture
    as much information as possible and include it wherever the destination is for
    these models. Azure has several fields you can use for this purpose when registering
    models. As you will see in the following sections, some of the model transformers
    allow adding metadata. Taking advantage of this seemingly unimportant task can
    be critical for debugging production problems. Two beneficial practices that have
    reduced time debugging and accelerated onboarding and maintenance ease is using
    meaningful names and as much metadata as possible. Using a meaningful name is
    crucial for identification and providing clarity. A model registered as “production-model-1”
    does not tell me what it is or what it is about. If you pair this with no additional
    metadata or information, this will cause frustration and delays when figuring
    out a production problem.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当您从像模型动物园这样的地方采集模型时，请确保尽可能多地捕获信息，并将其包含在这些模型的目的地中。Azure提供了几个字段供您在注册模型时使用此目的。正如您将在以下章节中看到的那样，一些模型转换器允许添加元数据。利用这看似不重要的任务可能对调试生产问题至关重要。两个有益的实践已经减少了调试时间，并加快了入职和维护的便利性，即使用有意义的名称和尽可能多的元数据。使用有意义的名称对于识别和提供清晰度至关重要。注册为“production-model-1”的模型并不告诉我它是什么或它是关于什么的。如果您配对此名称没有额外的元数据或信息，这将导致在弄清楚生产问题时引起沮丧和延迟。
- en: Convert PyTorch into ONNX
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将PyTorch转换为ONNX
- en: Getting started with a different framework is always daunting, even when the
    underlying task is to train a model from a dataset. PyTorch does an excellent
    job at including pretrained models that can quickly help you get started since
    you can try different aspects of the framework without dealing with curating a
    dataset and then figuring out how to train it. Many other frameworks (like TensorFlow
    and scikit-learn) are doing the same, and it is an excellent way to jumpstart
    learning. In this section, I use a pretrained vision model from PyTorch and then
    export it to ONNX.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从不同框架开始始终令人生畏，即使底层任务是从数据集中训练模型。PyTorch在包含可以快速帮助您入门的预训练模型方面表现出色，因为您可以尝试框架的不同方面，而无需处理数据集的策划和训练方法。许多其他框架（如TensorFlow和scikit-learn）也在做同样的事情，这是一个很好的学习起步方式。在本节中，我使用PyTorch的预训练视觉模型，然后将其导出到ONNX。
- en: 'Create a new virtual environment and a *requirements.txt* file that looks like
    the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的虚拟环境和一个类似下面的*requirements.txt*文件：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Install the dependencies and then create a *convert.py* file to produce the
    PyTorch model first:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 安装依赖项，然后创建一个*convert.py*文件来首先生成PyTorch模型：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s go through some of the steps that the Python script goes through to produce
    an ONNX model. It creates a tensor filled with random numbers using three channels
    (crucial for the pretrained model). Next, we retrieve the *resnet18* pretrained
    model available through the *torchvision* library. I define a few inputs and outputs
    and finally export the model with all that information.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解Python脚本执行的一些步骤，以生成一个ONNX模型。它创建一个使用三个通道填充随机数的张量（对于预训练模型至关重要）。接下来，我们使用*torchvision*库检索*resnet18*预训练模型。我定义了一些输入和输出，最后使用所有这些信息导出模型。
- en: The example is overly simplistic to prove a point. The exported model is not
    robust at all and full of dummy values that aren’t meaningful. The idea is to
    demonstrate how PyTorch enables you to export the model to ONNX in a straightforward
    way. The fact that the converter is part of the framework is reassuring because
    it is responsible for ensuring that this works flawlessly. Although separate converter
    libraries and projects exist, I prefer frameworks that offer the conversion, like
    PyTorch.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 示例用例过于简单，只是为了证明一个观点。导出的模型一点也不健壮，充满了毫无意义的虚拟值。它的目的是展示PyTorch如何以一种简单的方式将模型导出到ONNX。转换器作为框架的一部分确实让人放心，因为它负责确保这一过程完美无缺。尽管存在单独的转换器库和项目，我更喜欢像PyTorch这样提供转换功能的框架。
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `opset_version` argument in the `export()` function is critical. PyTorch’s
    tensor indexing can get you into problems with an unsupported ONNX opset version.
    Some indexer types do not support anything other than version 12 (the latest version).
    Always double-check that the versions match the supported features you need.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`export()`函数中的`opset_version`参数至关重要。PyTorch的张量索引可能会导致不支持的ONNX opset版本问题。某些索引器类型仅支持版本12（最新版本）。始终双重检查版本是否符合您需要的支持功能。'
- en: 'Run the *convert.py* script, which creates a *resnet18.onnx* file. You should
    see output similar to this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 运行*convert.py*脚本，它将创建一个*resnet18.onnx*文件。您应该看到类似于此的输出：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that the ONNX model is available and produced by the script using PyTorch,
    let’s use the ONNX framework to verify that the model produced is compatible.
    Create a new script called *check.py*:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过使用PyTorch脚本生成的ONNX模型，让我们使用ONNX框架验证生成的模型是否兼容。创建一个名为*check.py*的新脚本：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Run the *check.py* script from the same directory where *resnet18.onnx* is,
    and verify that the output is similar to this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从包含*resnet18.onnx*的同一目录中运行*check.py*脚本，并验证输出与此类似：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The verification from the call to the `check_model()` function should not produce
    any errors, proving that the conversion has some level of correctness. To fully
    ensure that the converted model is correct, inferencing needs to be evaluated,
    capturing any possible drift. If you are unsure about what metrics to use or how
    to create a solid comparison strategy, check the [“Basics of Model Monitoring”](ch06.xhtml#Section-Model-Monitoring).
    Next, let’s see how we can use the same checking pattern in a command line tool.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从对`check_model()`函数的调用的验证不应产生任何错误，证明了转换具有一定程度的正确性。为了确保转换模型的正确性，需要评估推理过程，捕捉任何可能的漂移。如果您不确定使用哪些指标或如何创建稳固的比较策略，请查看[“模型监控基础”](ch06.xhtml#Section-Model-Monitoring)。接下来，让我们看看如何在命令行工具中使用相同的检查模式。
- en: Create a Generic ONNX Checker
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个通用的ONNX检查器
- en: Now that I’ve gone through the details of exporting a model from PyTorch to
    ONNX and having a step to verify, let’s create a simple and generic tool that
    can verify any ONNX model, not just one in particular. Although we dedicate a
    large section about building powerful command line tools in the next chapter (see
    [“Command Line Tools”](ch11.xhtml#Section-Command-line-tools) in particular),
    we can still try to build something that works well for this use case. Another
    concept that comes from DevOps and my experience as a Systems Administrator is
    to attempt automation whenever possible and start with the most straightforward
    problem first. For this example, I will not use any command line tool framework
    or any advanced parsing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我已经详细介绍了从 PyTorch 导出模型到 ONNX 并进行验证的细节，让我们创建一个简单且通用的工具，可以验证任何 ONNX 模型，而不仅仅是特定的模型。虽然我们会在下一章节中专门讨论构建强大的命令行工具（尤其是见[“命令行工具”](ch11.xhtml#Section-Command-line-tools)），但我们仍然可以尝试构建一个适用于此用例的工具。另一个概念来自
    DevOps 和我作为系统管理员的经验，就是尽可能地自动化，并从最简单的问题开始。例如，我不会使用任何命令行工具框架或高级解析器。
- en: 'First, create a new file called *onnx-checker.py* with a single `main()` function:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个名为*onnx-checker.py*的新文件，其中包含一个名为`main()`的函数：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Run the script, and the output should show the help menu:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本，输出应该显示帮助菜单：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The script is not doing anything special yet. It uses the `main()` function
    to produce the help menu and a widely used crutch in Python to call a specific
    function when Python executes the script in the terminal. Next, we need to handle
    arbitrary input. Command line tool frameworks help with this, no doubt, but we
    can still have something valuable with minimal effort. To check the script’s arguments
    (we will need those to know what model to check), we need to use the `sys.argv`
    module. Update the script, so it imports the module and passes it into the function:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本目前还没有做任何特殊操作。它使用`main()`函数生成帮助菜单，并且在 Python 终端执行脚本时，还使用 Python 中广泛使用的方法来调用特定函数。接下来，我们需要处理任意输入。命令行工具框架可以帮助解决这个问题，毫无疑问，但我们仍然可以通过最小的努力获得有价值的东西。为了检查脚本的参数（我们需要这些参数来知道要检查哪个模型），我们需要使用`sys.argv`模块。更新脚本，使其导入该模块并将其传递给函数：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The change will cause the script to output the help menu only when using the
    `--help` flag. The script is still not doing anything useful, so let’s update
    the `main()` function once more to include the ONNX check functionality:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这一变化将导致脚本仅在使用`--help`标志时输出帮助菜单。脚本目前还没有执行任何有用的操作，所以让我们再次更新`main()`函数，以包括 ONNX
    检查功能：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'There are two crucial changes to the function. First, it is now calling `sys.exit(0)`
    after the help menu check to prevent the next block of code from executing. Next,
    if the help condition is not met, it uses the last argument (whatever that is)
    as the model’s path to check. Finally, it uses the same functions from the ONNX
    framework to the model check. Note that there is no sanitation or verification
    of inputs at all. This is a very brittle script, but it still proves helpful if
    you run it:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 函数有两个关键的变化。首先，在检查帮助菜单后现在调用`sys.exit(0)`以防止执行下一块代码。接下来，如果未满足帮助条件，则使用最后一个参数（无论是什么）作为要检查的模型路径。最后，使用来自
    ONNX 框架的相同函数对模型进行检查。请注意，完全没有对输入进行清理或验证。这是一个非常脆弱的脚本，但如果你运行它，它仍然会证明是有用的：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The path I used is to the RoBERTa base model that is in a separate path in
    my *Downloads* directory. This type of automation is a building brick: the simplest
    approach possible to do a quick check that can be leveraged later in other automation
    like a CI/CD system or a pipeline in a cloud provider workflow. Now that we’ve
    tried some models, let’s see how to convert models created in other popular frameworks
    into ONNX.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的路径是 RoBERTa 基础模型，它在我*Downloads*目录中的一个单独路径下。这种自动化是一个构建模块：尽可能简单地进行快速检查，以便稍后在其他自动化中使用，比如
    CI/CD 系统或云提供商工作流中的流水线。现在我们已经尝试了一些模型，让我们看看如何将在其他流行框架中创建的模型转换为 ONNX。
- en: Convert TensorFlow into ONNX
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 TensorFlow 转换为 ONNX
- en: There is a project dedicated to doing model conversions from TensorFlow in the
    ONNX GitHub repository. It offers a wide range of supported versions from both
    ONNX and TensorFlow. Again, it is critical to ensure that whatever tooling you
    choose has the versions your model requires to achieve a successful conversion
    to ONNX.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个专门从 TensorFlow 转换模型到 ONNX 的项目，存放在 ONNX GitHub 仓库中。它支持广泛的 ONNX 和 TensorFlow
    版本。再次强调，确保选择的工具包含你的模型所需的版本，以确保成功转换为 ONNX。
- en: 'Finding the right project, library, or tool for doing conversions can get tricky.
    For TensorFlow specifically, you can use the [onnxmltools](https://oreil.ly/BvLxv),
    which has a `onnxmltools.convert_tensorflow()` function, or the [tensorflow-onnx](https://oreil.ly/E6RDE)
    project, which has two ways to do a conversion: with a command line tool, or using
    the library.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 找到适合进行转换的正确项目、库或工具可能会变得棘手。特别是对于 TensorFlow，您可以使用[onnxmltools](https://oreil.ly/BvLxv)，它具有`onnxmltools.convert_tensorflow()`函数，或者[tensorflow-onnx](https://oreil.ly/E6RDE)项目，它有两种转换方式：使用命令行工具或使用库。
- en: This section uses the *tensorflow-onnx* project with a Python module that you
    can use as a command line tool. The project allows you to convert models from
    both TensorFlow major versions (1 and 2) as well as *tflite*, and *tf.keras*.
    The broad ONNX opset support is excellent (from version 7 to 13) because it allows
    greater flexibility when planning a conversion strategy.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用*tensorflow-onnx*项目与一个可以用作命令行工具的 Python 模块。该项目允许您从 TensorFlow 主要版本（1 和 2）、*tflite*
    和 *tf.keras* 进行模型转换。由于它允许在规划转换策略时使用更灵活的 ONNX opset 支持（从版本 7 到 13），因此其广泛的支持非常出色。
- en: 'Before getting into the actual conversion, it is worth exploring how to invoke
    the converter. The *tf2onnx* project uses a Python shortcut to expose a command
    line tool from a file instead of packaging a command line tool with the project.
    This means that the invocation requires you to use the Python executable with
    a special flag. Start by installing the library in a new virtual environment.
    Create a *requirements.txt* file to ensure that all the suitable versions for
    this example will work:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行实际转换之前，值得探讨如何调用转换器。*tf2onnx*项目使用了一个 Python 快捷方式，从一个文件中公开命令行工具，而不是将命令行工具与项目一起打包。这意味着调用需要您使用
    Python 可执行文件和特殊标志。首先，在新的虚拟环境中安装库。创建一个*requirements.txt*文件，以确保所有适合本示例的版本都能正常工作：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now use `pip` to install all the dependencies at their pinned versions:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用`pip`安装所有依赖项的固定版本：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you install the *tf2onnx* project without the *requirements.txt* file, the
    tool will not work because it doesn’t list `tensorflow` as a requirement. For
    the examples in this section, I’m using `tensorflow` at version 2.4.1\. Make sure
    you install it to prevent dependency problems.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您安装*tf2onnx*项目时没有*requirements.txt*文件，工具将无法工作，因为它未将`tensorflow`列为依赖项。在本节的示例中，我使用的是版本为
    2.4.1 的`tensorflow`。确保安装它以防止依赖问题。
- en: 'Run the help menu to check what is available. Remember, the invocation looks
    somewhat unconventional because it needs the Python executable to use it:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 运行帮助菜单以查看可用内容。请记住，调用看起来有些不寻常，因为它需要 Python 可执行文件来使用它：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: I’m omitting a few sections of the help menu for brevity. Calling the help menu
    is a reliable way to ensure that the library can load after installation. This
    wouldn’t be possible if `tensorflow` is not installed, for example. I left the
    three examples from the help menu because those are the ones you will need, depending
    on the type of conversion you are performing. None of these conversions are straightforward
    unless you have a good understanding of the internals of the model you are trying
    to convert. Let’s start with a conversion that requires no knowledge of the model,
    allowing the conversion to work out-of-the-box.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 出于简洁起见，我省略了帮助菜单的几个部分。调用帮助菜单是确保库在安装后可以加载的可靠方法。例如，如果未安装`tensorflow`，则这是不可能的。我留下了帮助菜单中的三个示例，因为根据您执行的转换类型，您将需要这些示例。除非您对您尝试转换的模型的内部结构有很好的理解，否则这些转换都不会直截了当。让我们从不需要模型知识的转换开始，使转换能够即插即用。
- en: 'First, download the [ssd_mobilenet_v2](https://oreil.ly/ytJk8) model (compressed
    in a *tar.gz* file) from *tfhub*. Then create a directory and uncompress it there:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从*tfhub*下载[ssd_mobilenet_v2](https://oreil.ly/ytJk8)模型（压缩成*tar.gz*文件）。然后创建一个目录并在那里解压：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that the decompressed model is in a directory use the *tf2onnx* conversion
    tool to port *ssd_mobilenet* over to ONNX. Make sure you are using an opset of
    13 to prevent incompatible features of the model. This is a shortened exception
    traceback you can experience when specifying an unsupported opset:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已解压到一个目录中，请使用*tf2onnx*转换工具将*ssd_mobilenet*转换到 ONNX。确保您使用 opset 13，以防止模型的不兼容特性。这是一个缩短的异常回溯，您可能会在指定不支持的
    opset 时遇到：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Use the `--saved-model` flag with the path to where the model got extracted
    to get a conversion working finally. In this case, I’m using opset 13:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`--saved-model`标志与模型提取路径，最终使转换工作。在这种情况下，我正在使用 opset 13：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: These examples might look overly simplistic, but the idea here is that these
    are building blocks so that you can explore further automation by knowing what
    is possible in conversions. Now that I have demonstrated what is needed to convert
    a TensorFlow model, let’s see what is required to convert a *tflite* one, another
    of the supported types from *tf2onnx*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例可能看起来过于简单，但这里的想法是，这些是构建模块，以便您可以通过了解在转换中可能发生的情况进一步探索自动化。现在我已经演示了转换TensorFlow模型所需的内容，让我们看看转换*tflite*模型所需的内容，这是*tf2onnx*支持的另一种类型之一。
- en: 'Download a *tflite* quantized version of the *mobilenet* model [from tfhub](https://oreil.ly/qNqml).
    The *tflite* support in *tf2onnx* makes the invocation slightly different. This
    is one of those cases where a tool gets created following one criterion (convert
    TensorFlow models to ONNX) and then has to pivot support for something else that
    doesn’t quite fit the same pattern. In this case, you must use the `--tflite`
    flag, which should point to the downloaded file:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从[tfhub](https://oreil.ly/qNqml)下载*mobilenet*模型的*量化*版本。*tf2onnx*中的*tflite*支持使调用略有不同。这是一个工具创建的案例，遵循一种标准（将TensorFlow模型转换为ONNX），然后不得不支持其他不完全符合相同模式的东西。在这种情况下，您必须使用`--tflite`标志，该标志应指向已下载的文件：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'I quickly get into trouble once again when running the command because the
    supported opset doesn’t match the default. Further, this model is quantized, which
    is another layer that the converter has to resolve. Here is another short traceback
    excerpt from trying that out:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我再次运行命令时很快就遇到了麻烦，因为支持的操作集与默认设置不匹配。此外，这个模型是量化的，这是转换器必须解决的另一层。以下是尝试过程中的另一个简短的回溯摘录：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: At least this time the error is hinting that the model is quantized and that
    I should consider using a different opset (versus the default opset, which clearly
    doesn’t work).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 至少这次错误提示表明模型已量化，并且我应考虑使用不同的操作集（与默认操作集相比，显然不起作用）。
- en: Tip
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Different TensorFlow ops have varying support for ONNX and can sometimes create
    problems if the incorrect version is used. The [tf2onnx Support Status page](https://oreil.ly/IJwxB)
    can be useful when trying to determine what is the correct version to use.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的TensorFlow操作对ONNX的支持有所不同，如果使用不正确的版本可能会造成问题。在尝试确定使用的正确版本时，[tf2onnx支持状态页面](https://oreil.ly/IJwxB)可能非常有用。
- en: I’m usually very suspicious when a book or demo shows perfection all the time.
    There is tremendous value in tracebacks, errors, and getting into trouble—which
    is happening for me when trying to get *tf2onnx* to work properly. If the examples
    in this chapter show you how it all “just works,” you will undoubtedly think that
    there is a significant knowledge gap or that the tooling is failing, giving no
    opportunity to understand why things aren’t quite working out. I add these tracebacks
    and errors because *tf2onnx* has a higher degree of complexity that allows me
    to get into a broken state without much effort.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当书籍或演示始终表现完美时，我通常会非常怀疑。回溯、错误以及陷入麻烦都有很大的价值——这在我试图使*tf2onnx*正常工作时发生。如果本章的示例向您展示一切“毫不费力”，您无疑会认为存在重大的知识差距，或者工具失灵，没有机会理解为什么事情没有完全按计划进行。我添加这些回溯和错误，因为*tf2onnx*具有更高的复杂度，使我可以轻易陷入破碎状态。
- en: 'Let’s fix the invocation and give it an opset of 13 (the highest supported
    offset at the moment), and try again:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修复调用，并为其设置一个操作集为13（目前支持的最高偏移量），然后再试一次：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: At last, the quantized *tflite* model gets converted to ONNX. There is room
    for improvement, and like we’ve seen in the previous steps throughout this section,
    it is invaluable to have a good grasp of the model inputs and outputs and how
    the model was created. This know-how is crucial at the time of conversion, where
    you can provide the tooling as much information as possible to secure a successful
    outcome. Now that I’ve converted some models to ONNX, let’s see how to deploy
    them with Azure.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，量化的*tflite*模型被转换为ONNX。还有改进的空间，就像我们在本节的先前步骤中看到的那样，深入了解模型的输入和输出以及模型的创建方式至关重要。在转换时，这种知识是至关重要的，您可以尽可能为工具提供更多信息，以确保成功的结果。现在我已经将一些模型转换为ONNX，让我们看看如何在Azure上部署它们。
- en: Deploy ONNX to Azure
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署ONNX到Azure
- en: Azure has very good ONNX integration in its platform with direct support in
    its Python SDK. You can create an experiment to train a model using PyTorch that
    can then export to ONNX, and as I’ll show you in this section, you can deploy
    that ONNX model to a cluster for live inferencing. This section will not cover
    how to perform the actual training of the model; however, I’ll explain how straightforward
    it is to use a trained ONNX model that is registered in Azure and deploy it to
    a cluster.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 在其平台上具有非常好的 ONNX 集成，直接在其 Python SDK 中提供支持。您可以创建一个实验来训练一个使用 PyTorch 的模型，然后将其导出为
    ONNX。正如我将在本节中展示的，您可以将该 ONNX 模型部署到集群以进行实时推理。本节不涵盖如何执行模型的实际训练；然而，我将解释如何简单地使用在 Azure
    中注册的训练好的 ONNX 模型，并将其部署到集群。
- en: 'In [“Packaging for ML Models”](ch04.xhtml#Section-packaging-models) I covered
    all the details you need to get a model into Azure and then package it in a container.
    Let’s reuse some of the container code to create the *scoring file*, which Azure
    needs to deploy it as a web service. In essence, it is the same thing: the script
    receives a request, which knows how to translate the inputs to make a prediction
    with the loaded model and then return the values.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“打包 ML 模型”](ch04.xhtml#Section-packaging-models)中，我详细介绍了将模型导入 Azure 并将其打包到容器中所需的所有细节。让我们重用一些容器代码来创建*评分文件*，这是
    Azure 部署为 Web 服务所需的。本质上，它是相同的：脚本接收请求，知道如何转换输入以使用加载的模型进行预测，然后返回值。
- en: Note
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: These examples uses Azure’s *workspace*, defined as a `ws` object. It is necessary
    to configure it before starting. This is covered in detail in [“Azure CLI and
    Python SDK”](ch08.xhtml#Section-azure-cli-sdk).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例使用 Azure 的*工作空间*，定义为`ws`对象。在开始之前必须对其进行配置。详细内容请参见[“Azure CLI 和 Python SDK”](ch08.xhtml#Section-azure-cli-sdk)。
- en: 'Create the scoring file, call it *score.py*, and add an `init()` function to
    load the model:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 创建评分文件，称为*score.py*，并添加一个`init()`函数来加载模型：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that the scoring script’s basics are out of the way, a `run()` function
    is required when running in Azure. Update the *score.py* script by creating the
    `run()` function that understands how to interact with the RoBERTa classification
    model:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在基本的评分脚本已经处理完毕，在 Azure 中运行时需要一个`run()`函数。通过创建`run()`函数来更新*score.py*脚本，使其能够与
    RoBERTa 分类模型进行交互：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, create the configuration to perform the inferencing. Since these examples
    are using the Python SDK, you can use them in a Jupyter Notebook or using the
    Python shell directly. Start by creating a YAML file that describes your environment:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建执行推理的配置。由于这些示例使用 Python SDK，您可以在 Jupyter Notebook 中使用它们，或者直接在 Python shell
    中使用。首先创建一个描述您环境的 YAML 文件：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now that the YAML file is done, set up the configuration:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 文件完成后，设置配置如下：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, deploy the model using the SDK. Create the Azure Container Instance
    (ACI) configuration first:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用 SDK 部署模型。首先创建 Azure 容器实例 (ACI) 配置：
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'With the `aci_config`, deploy the service:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `aci_config` 部署服务：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Several things have happened to get this deployment working. First, you defined
    the environment with the dependencies required for the inferencing to work. Then
    you configured an Azure Container Instance, and finally, you retrieved version
    1 of the *roberta_sequence* ONNX model and used the `Model.deploy()` method from
    the SDK to deploy the model. Again, the training specifics of the model aren’t
    covered here. You could very well train any model in Azure, export it to ONNX,
    register it, and pick up right in this section to continue the deployment process.
    Few modifications are needed in these examples to make progress. Perhaps different
    libraries and certainly a different way to interact with the model are required.
    Still, this workflow empowers you to add another layer of automation to deploy
    models programmatically from PyTorch to ONNX (or straight from previously registered
    ONNX models) in a container instance in Azure.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使部署工作，需要完成几件事情。首先，您定义了环境及所需的推理依赖项。然后配置了 Azure 容器实例，最后检索并使用 SDK 中的`Model.deploy()`方法部署了*roberta_sequence*
    ONNX 模型的版本 1。再次强调，本文未涉及模型的具体训练细节。在 Azure 中，您可以训练任何模型，导出为 ONNX，注册后可以继续使用本节进行部署过程。这些示例只需进行少量修改即可取得进展。可能需要使用不同的库，肯定需要不同的方式与模型交互。然而，这个工作流程使您能够从
    PyTorch 自动化地将模型部署到 Azure 中的容器实例中，使用 ONNX（或直接使用先前注册的 ONNX 模型）。
- en: You will want to use ONNX to deploy to other noncloud environments like mobile
    devices in some other situations. I cover some of the details involved in the
    next section with Apple’s machine learning framework.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些其他情况下，您将希望使用 ONNX 部署到其他非云环境，如移动设备。我在下一节中介绍了一些涉及苹果机器学习框架的详细信息。
- en: Apple Core ML
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apple Core ML
- en: Apple’s machine learning framework is somewhat unique in that there is support
    to convert Core ML models into ONNX *as well as* convert them from ONNX to Core
    ML. As I’ve said already in this chapter, you have to be very careful and ensure
    that there is support for the model conversion and getting the versions right.
    Currently, *coremltools* supports ONNX opset versions 10 and newer. It isn’t that
    hard to get into a situation where a model lacks support and breakage occurs.
    Aside from the ONNX support, you must be aware of the target conversion environment
    and if that environment supports iOS and macOS releases.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果的机器学习框架在某种程度上是独特的，因为它支持将 Core ML 模型转换为 ONNX，*同时也支持* 将它们从 ONNX 转换为 Core ML。正如我在本章中已经提到的，您必须非常小心，并确保模型转换和版本获取的支持。目前，*coremltools*
    支持 ONNX opset 版本 10 及更新版本。很容易陷入不支持模型和断裂发生的情况。除了 ONNX 支持之外，您必须了解目标转换环境以及该环境是否支持
    iOS 和 macOS 发布版。
- en: Tip
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: See [the minimum target](https://oreil.ly/aKJOK) supported for different environments
    in the *Core ML* documentation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[“不同环境中支持的最低目标”](https://oreil.ly/aKJOK)，在 *Core ML* 文档中有支持。
- en: 'Aside from support in a target environment like iOS, there is also a good list
    of tested models from ONNX known to work well. It is from that list that I’ll
    pick up the MNIST model to try out a conversion. Go to the Model Zoo and find
    the MNIST section. [Download](https://oreil.ly/Q005Q) the latest version (1.3
    in my case). Now create a new virtual environment and a *requirements.txt* with
    the following libraries, which include *coremltools*:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在像 iOS 这样的目标环境中的支持之外，还有一个已知良好工作的 ONNX 测试模型的良好列表。从这个列表中，我将挑选 MNIST 模型尝试转换。去模型动物园找到
    MNIST 部分。[下载](https://oreil.ly/Q005Q) 最新版本（在我这里是 1.3）。现在创建一个新的虚拟环境和包含以下库的 *requirements.txt*，其中包括
    *coremltools*：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Install the dependencies so that we can create tooling to do the conversion.
    Let’s create the most straightforward tool possible, just like in [“Create a Generic
    ONNX Checker”](#Section-generic-onnx-checker), without argument parsers or any
    fancy help menus. Start by creating the `main()` function and the special Python
    magic needed at the end of the file so that you can call it in the terminal as
    a script:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 安装依赖项，以便我们可以创建工具来进行转换。让我们创建尽可能简单的工具，就像在[“创建通用 ONNX 检查器”](#Section-generic-onnx-checker)
    中一样，没有参数解析器或任何花哨的帮助菜单。从创建 `main()` 函数和文件末尾所需的特殊 Python 魔法开始，这样你就可以将其作为脚本在终端中调用：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In this case, the script isn’t doing anything useful yet, and I’m also skipping
    implementing a help menu. You should always include a help menu in your scripts
    so that others can have some idea of the inputs and outputs of the program when
    they need to interact with it. Update the `main()` function to try out the conversion.
    I’ll assume that the last argument received will represent a path to the ONNX
    model that needs converting:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，脚本尚未执行任何有用的操作，我还跳过了实现帮助菜单。您应始终在脚本中包含帮助菜单，以便他人在需要与程序交互时了解输入和输出。更新 `main()`
    函数以尝试转换。我假设接收到的最后一个参数将代表需要转换的 ONNX 模型的路径：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'First, the function captures the last argument as the path to the ONNX model,
    and then it computes the base name by stripping out the *.onnx* suffix. Finally,
    it goes through the conversion (using a minimum iOS version target of 13), includes
    a description, and saves the output. Try the updated script with the previously
    downloaded MNIST model:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，该函数捕获最后一个参数作为 ONNX 模型的路径，然后通过去除 *.onnx* 后缀来计算基本名称。最后，它通过转换（使用最低 iOS 版本目标为
    13）进行转换，包括描述，并保存输出。尝试使用先前下载的 MNIST 模型更新脚本：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The resulting operation should’ve produced a *mnist-8.mlmodel* file, which is
    a Core ML model that you can now load in a macOS computer that has XCode installed.
    Use the Finder in your Apple computer, double-click the newly generated *coreml*
    model, and double-click it. The MNIST model should load right away, with the description
    included in the converter script as shown in [Figure 10-4](#Figure-10-4).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最终操作应生成一个 *mnist-8.mlmodel* 文件，这是一个 Core ML 模型，您现在可以在安装了 XCode 的 macOS 计算机上加载。在您的
    Apple 计算机上使用 Finder，双击新生成的 *coreml* 模型，然后再次双击。MNIST 模型应立即加载，并包含在转换器脚本中描述的内容，如
    [图 10-4](#Figure-10-4) 所示。
- en: '![pmlo 1004](Images/pmlo_1004.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 1004](Images/pmlo_1004.png)'
- en: Figure 10-4\. ONNX to Core ML
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. ONNX 到 Core ML
- en: Verify that the Availability section shows the minimum iOS targets of 13, just
    like the converter script set them. The Predictions section has useful information
    about the inputs and outputs that the model accepts, as shown in [Figure 10-5](#Figure-10-5).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 确认可用性部分显示的最低 iOS 目标为 13，就像转换器脚本设定的那样。预测部分提供了有关模型接受的输入和输出的有用信息，如 [图 10-5](#Figure-10-5)
    所示。
- en: '![pmlo 1005](Images/pmlo_1005.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 1005](Images/pmlo_1005.png)'
- en: Figure 10-5\. ONNX to CoreML Predictions
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. ONNX 到 CoreML 预测
- en: Finally, the Utilities section provides helpers to deploy that model using a
    model archive that integrates with CloudKit (Apple’s environment for iOS application
    resources) as shown in [Figure 10-6](#Figure-10-6).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Utilities 部分提供了一些辅助工具，以使用模型存档部署该模型，该存档与 CloudKit（Apple 的 iOS 应用资源环境）集成，如
    [图 10-6](#Figure-10-6) 所示。
- en: '![pmlo 1006](Images/pmlo_1006.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 1006](Images/pmlo_1006.png)'
- en: Figure 10-6\. ONNX to Core ML model archive
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. ONNX 转 Core ML 模型存档
- en: It is exciting to see broad support for ONNX and growing support in other frameworks
    and operating systems like OSX in this case. If you are interested in iOS development
    and deploying models, you can still use the frameworks you are used to. Then,
    you can do the conversion to the targeted iOS environment for deployment. This
    process makes a compelling reason to use ONNX because it allows you to leverage
    well-established frameworks that you can transform into targeted environments.
    Next, let’s see some other edge integrations that further emphasize the usefulness
    of the ONNX framework and tooling.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 看到 ONNX 在这种情况下对 OSX 以及其他框架和操作系统的广泛支持是令人兴奋的。如果您对 iOS 开发和部署模型感兴趣，您仍然可以使用您习惯的框架。然后，您可以将其转换为目标
    iOS 环境进行部署。这个过程使得使用 ONNX 具有了强有力的理由，因为它允许您利用成熟的框架，并将其转换为目标环境。接下来，让我们看看一些其他边缘集成，进一步强调
    ONNX 框架和工具的实用性。
- en: Edge Integration
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 边缘集成
- en: Recently, ONNX announced a new internal model format known as ORT, which minimizes
    the build size of the model for optimal deployment on embedded or edge devices.
    The smaller footprint of a model has several aspects that help on edge devices.
    Edge devices have limited storage capacity, and most times, the storage isn’t
    fast at all. Reading and writing to small storage devices can quickly become problematic.
    Further, ONNX, in general, keeps trying to support more and different hardware
    configurations with varying CPUs and GPUs; not an easy problem to solve, but indeed
    a welcomed effort. The better and broader the support, the easier it is to get
    machine learning models deployed to help environments where this was not possible
    before. As I’ve covered most of Edge’s benefits and crucial aspects in [“Edge
    Devices”](ch03.xhtml#Section-edge-devices), this section will concentrate on getting
    a conversion done from an ONNX model down to the ORT format.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，ONNX 宣布了一种名为 ORT 的新内部模型格式，它最小化了模型的构建大小，以便在嵌入式或边缘设备上进行最佳部署。模型的较小占用空间具有几个方面，有助于边缘设备。边缘设备的存储容量有限，并且大多数情况下，存储速度并不快。对小存储设备进行读写操作可能很快变得困难。此外，总体上，ONNX
    继续努力支持更多和不同的硬件配置，包括各种 CPU 和 GPU；这不是一个容易解决的问题，但确实是一种受欢迎的努力。支持越广泛和更好，就越容易将机器学习模型部署到以前不可能部署的环境中。由于我已经在
    [“边缘设备”](ch03.xhtml#Section-edge-devices) 中涵盖了大部分边缘的利益和关键方面，本节将集中讨论将 ONNX 模型转换为
    ORT 格式的操作。
- en: 'Start by creating a new virtual environment, activating it, and then installing
    the dependencies as shown in this *requirements.txt* file:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个新的虚拟环境，激活它，然后按照 *requirements.txt* 文件中显示的步骤安装依赖项：
- en: '[PRE29]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'There is currently no separate tooling available to install, so you are required
    to clone the entire *onnxruntime* repository to try a conversion to ORT. After
    cloning, you will use the *convert_onnx_models_to_ort.py* file in the *tools/python*
    directory:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 目前没有可用于安装的单独工具，因此需要克隆整个*onnxruntime*存储库来尝试将其转换为ORT。克隆后，您将使用*tools/python*目录中的*convert_onnx_models_to_ort.py*文件：
- en: '[PRE30]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The ORT converter produces a configuration file as well as an ORT model file
    from the ONNX model. You can deploy the optimized model along with a unique ONNX
    runtime build. First, try a conversion with an ONNX model. In this example, I
    downloaded the [mobilenet_v2-1.0](https://oreil.ly/c7hBm) ONNX model into the
    *models/* directory and used it as an argument to the converter script:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ORT转换器会从ONNX模型生成配置文件以及ORT模型文件。您可以将优化后的模型与独特的ONNX运行时构建一起部署。首先，尝试使用一个ONNX模型进行转换。在此示例中，我下载了[mobilenet_v2-1.0](https://oreil.ly/c7hBm)
    ONNX模型到*models/*目录，并将其作为转换脚本的参数使用：
- en: '[PRE31]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The configuration is crucial here because it lists the operators needed by
    the model. This allows you to create an ONNX runtime with only those operators.
    For the converted model, that file looks like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里配置至关重要，因为它列出了模型所需的运算符。这样可以创建一个只包含这些运算符的ONNX运行时。对于转换后的模型，该文件看起来像这样：
- en: '[PRE32]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You can accomplish a binary size reduction for the runtime by specifying only
    the needs of the model. I will not be covering all the specifics on how to build
    the ONNX runtime from source, but you can use the [build guide](https://oreil.ly/AWP5r)
    as a reference for the next steps as we explore what flags and options are helpful
    in the binary reduction.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过仅指定模型需求来减少运行时的二进制大小。我不会详细介绍如何从源代码构建ONNX运行时的所有具体内容，但是您可以将[构建指南](https://oreil.ly/AWP5r)作为下一步的参考，因为我们将探索二进制减少中有用的标志和选项。
- en: First, you must use the `--include_ops_by_config` flag. In this case, the value
    for this flag is the path to the generated config from the previous step. In my
    case, that path is *models/mobilenetv2-7.required_operators.config*. I also suggest
    you try out the `--minimal_build`, which supports loading and executing ORT models
    only (dropping support for normal ONNX formats). Finally, if you are targeting
    an Android device, using the `--android_cpp_shared` flag will produce a smaller
    binary by using the shared *libc++* library instead of the static one that comes
    by default in the runtime.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您必须使用`--include_ops_by_config`标志。在本例中，此标志的值是从上一步生成的配置文件路径。在我的情况下，该路径是*models/mobilenetv2-7.required_operators.config*。我还建议您尝试使用`--minimal_build`，该选项仅支持加载和执行ORT模型（放弃对普通ONNX格式的支持）。最后，如果您的目标是Android设备，则使用`--android_cpp_shared`标志将通过使用共享的*libc++*库生成更小的二进制文件，而不是默认的静态库。
- en: Conclusion
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The commonality that a framework (and set of tools) like ONNX provides helps
    the whole machine learning ecosystem. One of the ideals of this book is to provide
    *practical* examples to get models into production in a reproducible and reliable
    way. The more comprehensive the support for machine learning models, the easier
    it will be for engineers to try it out and take advantage of everything machine
    learning offers. I’m particularly excited about edge applications, especially
    for remote environments where it is impossible to connect to the internet or have
    any network connectivity. ONNX is lowering the friction that exists to deploy
    to those environments. I hope that the effort to continue to make this easier
    with more tooling and better support will continue to benefit from collective
    knowledge and contributions. Although we briefly tried command line tools, I go
    into much more detail on how to make robust command line tools with error handling
    and well-defined flags in the next chapter. In addition, I cover Python packaging
    and using microservices, which will give you the flexibility to try different
    approaches when solving machine learning challenges.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 像ONNX这样的框架（和一组工具）提供的共性有助于整个机器学习生态系统。本书的一个理念是提供*实际*的例子，以便以可重复和可靠的方式将模型投入生产。对于机器学习模型的支持越全面，工程师们尝试和利用机器学习所提供的一切就越容易。我特别对边缘应用感兴趣，尤其是对于无法连接到互联网或没有任何网络连接的远程环境。ONNX正在降低在这些环境中部署的摩擦。我希望通过更多的工具和更好的支持持续努力，继续从集体知识和贡献中受益。虽然我们简要尝试了命令行工具，但在下一章中，我会更详细地介绍如何制作具有错误处理和定义良好标志的健壮命令行工具。此外，我还涵盖了Python打包和使用微服务，这将使您在解决机器学习挑战时可以尝试不同的方法。
- en: Exercises
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Update all scripts to verify the produced ONNX model with the script from [“Create
    a Generic ONNX Checker”](#Section-generic-onnx-checker).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新所有脚本，使用[“创建一个通用 ONNX 检查器”](#Section-generic-onnx-checker)中的脚本验证生成的 ONNX 模型。
- en: Modify the Core ML converter script to use the Click framework for better parsing
    of options and a help menu.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改 Core ML 转换器脚本，使用 Click 框架以更好地解析选项并提供帮助菜单。
- en: Group three converters into a single command line tool so that it is easy to
    make conversions with different inputs.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将三个转换器分组到一个单一的命令行工具中，以便可以使用不同的输入进行转换。
- en: Improve the *tf2onnx* converter so that it is wrapped in a new script, which
    can catch common errors and report them with a more user-friendly message.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进*tf2onnx*转换器，使其包装在一个新脚本中，可以捕捉常见错误并用更用户友好的消息报告它们。
- en: Use a different ONNX model for an Azure deployment.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的 ONNX 模型进行 Azure 部署。
- en: Critical Thinking Discussion Questions
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批判性思维讨论问题
- en: Why is ONNX important? Give at least three reasons.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ONNX 为何重要？至少给出三个理由。
- en: What is something useful about creating a script without a command line tool
    framework? What are the advantages of using a framework?
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个没有命令行工具框架的脚本有什么用？使用框架的优点是什么？
- en: How is the ORT format useful? In what situations can you use it?
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORT 格式有什么用处？在什么情况下可以使用它？
- en: What are some problems that you may encounter if portability doesn’t exist?
    Give three reasons why improving those problems will improve machine learning
    in general.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不存在可移植性，可能会遇到哪些问题？列出三个改善这些问题将如何改进机器学习的原因。
