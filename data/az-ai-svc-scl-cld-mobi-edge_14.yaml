- en: 'Chapter 10\. Seeing AI: Using Azure Machine Learning and Cognitive Services
    in a Mobile App at Scale'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章\. Seeing AI：在移动应用中规模化使用 Azure 机器学习和认知服务
- en: In previous chapters we’ve looked at how to use Azure Machine Learning and Cognitive
    Services. We’ve shown you how Microsoft runs Cognitive Services so it scales to
    massive numbers of users. But what does that look like inside a real-world application?
    What kind of architecture do you need to take advantage of Azure AI services?
    And how ambitious can you be when it comes to the kind of problems you can solve
    with AI services?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们已经讨论了如何使用 Azure 机器学习和认知服务。我们向您展示了微软如何运行认知服务，以便扩展到大量用户。但是在实际应用中，这是怎样的情况呢？在利用
    Azure AI 服务时，您需要哪种架构？在解决 AI 服务能够解决的问题时，您可以有多大的雄心？
- en: How about describing the world around them for millions of blind users, in real
    time?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如何实时描述数百万盲人用户周围的世界呢？
- en: In this chapter, we’re going to look at how the Seeing AI app uses Azure Machine
    Learning and Cognitive Services to tell blind users what’s in front of them, using
    a mix of prebuilt and custom models, running locally and in the cloud.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将探讨 Seeing AI 应用如何利用 Azure 机器学习和认知服务告知盲人用户他们面前的情况，使用了预建模型和自定义模型的混合，同时在本地和云端运行。
- en: 'Think of the app as a talking camera: it can read out short sections of text
    like signs and labels, capture and recognize longer documents like forms and menus,
    and even read handwriting. It recognizes faces the user knows and describes the
    people around them; it can also describe what’s in an image or narrate what’s
    going on, like someone playing football in the park. It can describe a color,
    emit audio tones to describe how dark or bright it is, beep to help users scan
    a barcode to get information about the box or tin they’re holding, and identify
    different bank notes when it’s time to pay for something. Machine learning services
    on Azure provide the kind of descriptions shown in [Figure 10-1](#seeing_ai_uses_azure_machine_learning_s),
    and the app delivers those in an interface designed specifically for its blind
    users.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 想象这款应用就像一个说话的相机：它可以朗读像标志和标签这样的短文本片段，捕捉和识别像表格和菜单这样更长的文档，甚至识别手写。它可以认出用户认识的人的脸，并描述周围的人；它还可以描述图像中的内容或叙述正在进行的事情，比如某人在公园里踢足球。它可以描述一种颜色，发出声音来描述黑暗或明亮的程度，发出蜂鸣声以帮助用户扫描条形码，获取他们手持盒子或罐头的信息，并在付款时识别不同的银行票据。Azure
    上的机器学习服务提供了如图 10-1 所示的描述，并且应用设计了专门为盲人用户的界面来提供这些描述。
- en: '![Seeing AI uses Azure machine learning services to recognize objects and describe
    scenes](Images/aasc_1001.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![Seeing AI 使用 Azure 机器学习服务识别对象并描述场景](Images/aasc_1001.png)'
- en: Figure 10-1\. Seeing AI uses Azure machine learning services to recognize objects
    and describe scenes
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. Seeing AI 使用 Azure 机器学习服务识别对象并描述场景
- en: Custom and Cloud Models
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义和云模型
- en: That’s a wide range of computer vision problems, so the app uses multiple machine
    learning models for the different scenarios and makes different trade-offs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到广泛的计算机视觉问题，因此该应用针对不同的情境使用多个机器学习模型，并进行了不同的权衡。
- en: Running a machine learning model locally on the device gives you real-time results,
    ideally less than 100 milliseconds or at most under a quarter of a second, which
    means the user gets a responsive natural experience for having text read out to
    them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备上本地运行机器学习模型可以实时得到结果，理想情况下少于 100 毫秒，或者最多不超过四分之一秒，这意味着用户在听取文本时会获得响应迅速的自然体验。
- en: But image description uses Custom Vision models in Cognitive Services that can’t
    be compressed to run on the device, let alone the multigigabyte models trained
    in Azure Machine Learning to recognize many different types of objects. While
    the round trip to the cloud takes a little longer, it delivers higher quality.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但是图像描述使用了认知服务中的自定义视觉模型，这些模型无法压缩到设备上运行，更不用说在 Azure 机器学习中训练的多种不同对象识别的多 GB 模型了。虽然往返云端需要一点时间，但能够提供更高的质量。
- en: If you want a sign or a label read out to you in real time, that’s done with
    models running on the phone. But if you’re taking a photo of a document rather
    than just holding up the phone, you’re probably going to spend a few seconds getting
    a good picture, so it’s worth taking the time to send that to Azure to take advantage
    of the very large models running in the cloud and get a more accurate result.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望实时将标志或标签读出来，那就需要在手机上运行的模型来完成。但如果您要拍摄文档而不仅仅是拿着手机，可能需要花几秒钟来拍摄好的照片，因此值得花时间将其发送到Azure，利用云中运行的非常大的模型，以获得更准确的结果。
- en: Tip
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: There are different ways to decide between running a model locally and in the
    cloud. Seeing AI does that by letting users choose different scenarios, but in
    your app you might decide based on factors like the speed of the internet connection
    or to take a hybrid approach. If someone has limited bandwidth, a slow connection,
    or is offline, you can use a local model—or even start giving them results using
    the small local model and then add more precise or accurate information as that
    comes back from the cloud. If you do that, make sure the user interface makes
    it clear why the quality or level of detail varies so the user isn’t confused
    by seeing better results some of the time. You may also need to think about privacy;
    the images Seeing AI sends to Azure are stored securely and in a way that protects
    user privacy, but they do leave the device; if you’re in a regulated industry
    where that’s a problem, you may need to stick to local models—and if you are sending
    content to a cloud service, you may need to notify your users that’s happening.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方式来决定是在本地还是在云中运行模型。Seeing AI通过让用户选择不同的场景来实现这一点，但在您的应用程序中，您可能会根据因素如互联网连接速度或采取混合方法来做出决策。如果某人带宽有限、连接速度慢或离线，您可以使用本地模型——甚至开始使用小型本地模型为用户提供结果，然后从云端获取更精确或准确的信息。如果这样做，请确保用户界面清楚地说明为什么质量或详细级别会有所不同，以免用户因为有时看到更好的结果而感到困惑。您还需要考虑隐私问题；Seeing
    AI发送到Azure的图像会安全存储，并以保护用户隐私的方式存储，但它们会离开设备；如果您在受管制行业中，这可能是一个问题，您可能需要坚持使用本地模型——如果您正在向云服务发送内容，您可能需要通知用户这一点。
- en: Image captioning in Seeing AI uses the Cognitive Services Vision API, and the
    Face API handles telling the user the age and gender of people. Object detection
    also uses Cognitive Services, with some models that need only fine-tuning trained
    using Custom Vision. For some scenarios, Seeing AI uses large models in the cloud
    that can recognize a lot of different objects, but as it introduces more augmented
    reality scenarios, that means running models that have been trained with Custom
    Vision and exported to run in CoreML for iOS and TensorFlow Lite on the device
    for the fastest response.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Seeing AI中的图像字幕使用认知服务视觉API，Face API处理告知用户人物的年龄和性别。对象检测也使用认知服务，一些只需微调的模型使用自定义视觉进行训练。对于一些场景，Seeing
    AI使用云中的大型模型，可以识别许多不同的对象，但随着引入更多增强现实场景，这意味着运行经过自定义视觉训练并导出到CoreML（iOS）和TensorFlow
    Lite（设备上最快响应）的模型。
- en: Other models that need full custom training, for scenarios like currency and
    barcodes, are built in Azure Machine Learning because these are so different from
    the everyday objects that prebuilt object detection models are trained on. The
    training images are uploaded to Azure Blob storage, the training scripts are run
    in Python using GPUs, and then a conversion script compresses the large model
    that’s created into, again, CoreML or TensorFlow Lite. (If you need to target
    more platforms with your own applications, you can use ONNX, which we covered
    in [Chapter 3](ch03.xhtml#traincomma_tunecomma_and_deploy_models).)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 需要进行完全自定义训练的其他模型（如货币和条形码等场景）是在Azure Machine Learning中构建的，因为这些与预建对象检测模型训练的日常对象非常不同。训练图像上传到Azure
    Blob存储，使用GPU在Python中运行训练脚本，然后转换脚本将创建的大型模型压缩成CoreML或TensorFlow Lite格式。（如果您需要针对自己的应用目标更多平台，可以使用我们在[第3章](ch03.xhtml#traincomma_tunecomma_and_deploy_models)中介绍的ONNX。）
- en: Choosing between Custom Vision and a custom object detection model built in
    Azure Machine Learning is about accuracy but also about model size.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择使用Azure Machine Learning中的自定义视觉和自定义对象检测模型之间，不仅涉及到准确性，还涉及到模型的大小。
- en: Custom Vision is fast and convenient; it’s much less work than building in Azure
    Machine Learning, and a general object decision model that recognizes people,
    animals, vehicles, and other everyday objects is accurate enough that it’s not
    worth starting from scratch just to get slightly better results. The Seeing AI
    team trains the Custom Vision model using images of things their users need to
    have recognized that the standard object detection model might not be trained
    for, like stairs, elevators, building entrances, and doorways.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义视觉快速便捷；它比在 Azure 机器学习中构建要少得多，并且一般的对象决策模型（识别人、动物、车辆和其他日常物品）的准确度已经足够高，因此并不值得从头开始只是为了略微提高结果。Seeing
    AI 团队使用他们的用户需要识别的物品的图像来训练自定义视觉模型，这些是标准对象检测模型可能未经训练的，例如楼梯、电梯、建筑入口和门道。
- en: But the other factor is model size. The memory limitations on mobile devices
    limit how large a model can be, and a pretrained model like ImageNet is very generic
    and includes things that users aren’t likely to come across frequently, like zebras
    and giraffes. Having those in the model doesn’t make it less accurate at recognizing
    what Seeing AI users do care about, but it does mean the model is larger than
    it needs to be. The team is experimenting to see if it’s worth training a model
    from scratch that covers the shorter list of objects they definitely need to recognize,
    like elements of the kind of buildings they want to navigate, because that creates
    a smaller model for a specific scenario.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但另一个因素是模型大小。移动设备上的内存限制限制了模型的大小，像 ImageNet 这样的预训练模型非常通用，并包括用户不太可能经常遇到的事物，如斑马和长颈鹿。将这些内容包含在模型中不会降低
    Seeing AI 用户识别所关注事物的准确性，但意味着模型比必要的大。团队正在尝试确定是否值得从头开始训练一个覆盖他们确实需要识别的对象较短列表的模型，例如他们想要导航的建筑物的元素，因为这样可以创建一个特定场景的较小模型。
- en: The Seeing AI Backend
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Seeing AI 后端
- en: The infrastructure for the mobile Seeing AI app uses standard cloud design patterns
    for distributed applications. Whether it’s Cognitive Services or any other cloud
    API, you don’t want to put your API keys in your app; Seeing AI stores those secrets
    in Azure Key Vault and uses Azure AD to handle the connection to Key Vault. The
    easiest way to ensure a trusted connection between an app and the background service
    it connects to is to have to user log in; using Azure B2C lets you use the same
    architecture for handling users and managing secrets. That also simplifies using
    an Azure Web App or Azure Functions for telemetry and analytics, or if you want
    to cache results.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 移动 Seeing AI 应用程序的基础架构使用了分布式应用程序的标准云设计模式。无论是认知服务还是任何其他云 API，您都不希望将 API 密钥放在应用程序中；Seeing
    AI 将这些秘密存储在 Azure Key Vault 中，并使用 Azure AD 处理与 Key Vault 的连接。确保应用程序与其连接的后台服务之间建立可信连接的最简单方法是让用户登录；使用
    Azure B2C 可让您使用相同的架构处理用户和管理秘密。这还简化了使用 Azure Web 应用程序或 Azure Functions 进行遥测和分析，或者如果您想缓存结果。
- en: 'Images and other requests come in to the Seeing AI service from the app and
    fan out to the other services it uses. A backend service handles requests to multiple
    Cognitive Services and combines the results from each service. That also allows
    Seeing AI to do load balancing and routing for Cognitive Services in different
    regions: you can do that with Azure Front Door for HTTP requests, which lets you
    do rate limiting and IP access control, or with Azure Traffic Manager if you need
    to route TCP or UDP traffic. Because the app is used in many countries and needs
    to talk to the closest Azure region to where the user is, the backend is configured
    to call the nearest instance of a Cognitive Service (and to fail over if that’s
    not available). The backend also uses Azure Web Application Firewall to block
    malformed URIs that bad actors could use to attack the service.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和其他请求从应用程序传入 Seeing AI 服务，并分发到其使用的其他服务。后端服务处理多个认知服务的请求，并合并每个服务的结果。这也允许 Seeing
    AI 对不同区域的认知服务进行负载均衡和路由：您可以使用 Azure Front Door 处理 HTTP 请求，这样可以进行速率限制和 IP 访问控制，或者使用
    Azure Traffic Manager 如果您需要路由 TCP 或 UDP 流量。由于该应用程序在许多国家使用，并且需要与用户所在地最近的 Azure
    区域通信，因此后端配置为调用最近的认知服务实例（并在不可用时进行故障转移）。后端还使用 Azure Web 应用程序防火墙来阻止恶意行为者可能用来攻击服务的格式错误的
    URI。
- en: Seeing AI doesn’t need this, but if your backend is going to be used by different
    applications or if you want to offer different levels of service, you can use
    Azure API Management to rate limit different consumers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于   Seeing AI 不需要这一步，但如果你的后端将被不同的应用程序使用，或者你想提供不同级别的服务，你可以使用 Azure API Management
    来对不同的消费者进行速率限制。
- en: Getting the Interface Right
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取接口的正确性
- en: 'The speed and accuracy of the Custom Vision and Azure Machine Learning models
    are what make Seeing AI useful, but they need to be wrapped in a good user experience
    to really be helpful. There’s a key phrase in disability circles: “Nothing about
    us without us.” Whoever your audience is, the results you get from Cognitive Services
    aren’t useful unless the experience of the app is useful to them. Think about
    how users interact with the app and how you can give them the information they
    need to be productive.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Custom Vision 和 Azure Machine Learning 模型的速度和准确性使得 Seeing AI 有用，但它们需要包裹在良好的用户体验中才能真正有帮助。残疾人圈里有一个关键短语：“没有我们，不会有我们。”无论你的观众是谁，来自认知服务的结果只有在应用体验对他们有用时才有价值。考虑用户如何与应用互动，以及如何提供他们所需的信息，以提高他们的生产力。
- en: If you’re using Custom Vision or other options for customizing or personalizing
    the experience, make sure you allow users to give you feedback on how well that’s
    working, so you can keep improving the results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 Custom Vision 或其他定制或个性化体验的选项，确保你允许用户反馈这些功能的效果，以便你可以继续改进结果。
- en: Remember that machine learning is inherently probabilistic rather than the usual
    binary “true or false” of coding. The results you get from machine learning models
    have confidence values to say how certain the model is they’re correct. You have
    to think about error rates and be ready to handle confidence levels in your application
    without them being intrusive. Think about how you will handle the user experience
    if a result turns out to be wrong.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，机器学习本质上是概率性的，而不是通常的“真或假”二元编码。你从机器学习模型得到的结果有置信度值，表示模型对其正确性的确定程度。你需要考虑错误率，并准备在应用中处理置信度水平而不产生干扰。考虑一下如果结果是错误的，你将如何处理用户体验。
- en: The descriptions of scenes in Seeing AI are phrased to make that clear, saying
    things like “it’s probably a dog playing with a ball in the park.”
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Seeing AI 对场景的描述是为了明确这一点，常说如“可能是公园里玩球的狗”。
- en: 'For real-time scenarios, Seeing AI uses thresholds, and if the object detection
    service delivers a result that doesn’t meet that threshold, it’s not used. But
    because users are often moving around as they use the app, there might be multiple
    readings, so getting multiple observations may let you increase your confidence
    in other ways. You also need to handle those multiple results: it’s important
    that Seeing AI doesn’t say an object like a car has just appeared if it’s actually
    the same car from a different angle.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实时场景，Seeing AI 使用阈值，如果物体检测服务提供的结果不符合该阈值，则不使用。但由于用户在使用应用时经常移动，可能会有多个读取结果，因此获取多个观察值可能会让你以其他方式增加信心。你还需要处理这些多个结果：Seeing
    AI 不会说一个像车的物体刚出现，如果实际上是从不同角度看的同一辆车。
- en: Equally, you don’t want to distract the user with notifications that aren’t
    helpful or with descriptions that are too long to be useful. If further readings
    show an object is closer or farther away than it first seemed, Seeing AI will
    update the description but not interrupt the user to tell them that.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你不希望通过无用的通知或过长的描述来分散用户的注意力。如果进一步的读取显示物体比最初看起来更近或更远，Seeing AI 将更新描述，但不会打断用户告诉他们。
- en: 'The key to getting the user experience right is understanding your users: what
    situation will they be in, what do they need from your application, and what will
    just get in their way? How will that look in the live experience, whether it’s
    someone walking in the park or a camera monitoring a factory production line?
    Either will be quite different from wherever you do your coding, so as well as
    gathering a diverse set of training images, make sure to do plenty of testing
    with real users in their environment to have the best chance of making the results
    of your machine learning models really useful to them.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 获取用户体验正确的关键是了解你的用户：他们将处于什么样的环境，需要从应用程序中得到什么，以及什么会妨碍他们的使用？在实际体验中会是什么样子，无论是有人在公园散步还是摄像头在监控工厂生产线？这两者都与你编写代码的环境截然不同，因此除了收集多样化的训练图像外，还要确保在真实用户的环境中进行充分的测试，以最大限度地提高你的机器学习模型的结果对他们的实际帮助。
- en: 'Whatever you’re building your app to do, Seeing AI is a great example of how
    to use cloud AI services in a mobile app, including how to put together a cloud
    backend to orchestrate those services and when to use local models. As the name
    suggests, it concentrates on what can be seen in the world around you; in the
    next chapter, we’ll look at working with speech. Suppose you need to not only
    transcribe but translate multiple languages while someone’s talking: how close
    can you get to that science fiction classic—a real-time translation system?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你建造应用程序的目的是什么，Seeing AI 是如何在移动应用中使用云 AI 服务的一个很好的例子，包括如何组合云后端来编排这些服务以及何时使用本地模型。顾名思义，它集中于你周围世界中能够看到的内容；在下一章中，我们将讨论如何处理语音。假设你需要在某人说话时不仅转录而且翻译多种语言：你能达到科幻经典——实时翻译系统的多近呢？
