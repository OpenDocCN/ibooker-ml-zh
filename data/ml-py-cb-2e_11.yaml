- en: Chapter 11\. Model Evaluation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 模型评估
- en: 11.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.0 介绍
- en: In this chapter we will examine strategies for evaluating the quality of models
    created through our learning algorithms. It might appear strange to discuss model
    evaluation before discussing how to create them, but there is a method to our
    madness. Models are only as useful as the quality of their predictions, and thus,
    fundamentally, our goal is not to create models (which is easy) but to create
    high-quality models (which is hard). Therefore, before we explore the myriad learning
    algorithms, let’s first learn how we can evaluate the models they produce.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨评估通过我们的学习算法创建的模型质量的策略。在讨论如何创建它们之前讨论模型评估可能看起来很奇怪，但我们的疯狂之中有一种方法。模型的实用性取决于其预测的质量，因此，从根本上说，我们的目标不是创建模型（这很容易），而是创建高质量的模型（这很难）。因此，在探索多种学习算法之前，让我们首先了解如何评估它们产生的模型。
- en: 11.1 Cross-Validating Models
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.1 交叉验证模型
- en: Problem
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to evaluate how well your classification model generalizes to unforeseen
    data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望评估您的分类模型在未预料到的数据上的泛化能力。
- en: Solution
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Create a pipeline that preprocesses the data, trains the model, and then evaluates
    it using cross-validation:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个管道，对数据进行预处理，训练模型，然后使用交叉验证进行评估：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Discussion
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'At first consideration, evaluating supervised-learning models might appear
    straightforward: train a model and then calculate how well it did using some performance
    metric (accuracy, squared errors, etc.). However, this approach is fundamentally
    flawed. If we train a model using our data, and then evaluate how well it did
    on that data, we are not achieving our desired goal. Our goal is not to evaluate
    how well the model does on our training data, but how well it does on data it
    has never seen before (e.g., a new customer, a new crime, a new image). For this
    reason, our method of evaluation should help us understand how well models are
    able to make predictions from data they have never seen before.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，评估监督学习模型似乎很简单：训练一个模型，然后使用某种性能指标（准确率、平方误差等）计算其表现。然而，这种方法基本上是有缺陷的。如果我们使用我们的数据训练一个模型，然后评估它在该数据上的表现，我们并没有达到我们的预期目标。我们的目标不是评估模型在训练数据上的表现，而是评估它在从未见过的数据上的表现（例如新客户、新犯罪案件、新图像）。因此，我们的评估方法应该帮助我们理解模型在从未见过的数据上进行预测的能力有多好。
- en: One strategy might be to hold off a slice of data for testing. This is called
    *validation* (or *hold-out*). In validation, our observations (features and targets)
    are split into two sets, traditionally called the *training set* and the *test
    set*. We take the test set and put it off to the side, pretending that we have
    never seen it before. Next we train our model using our training set, using the
    features and target vector to teach the model how to make the best prediction.
    Finally, we simulate having never-before-seen external data by evaluating how
    our model performs on our test set. However, the validation approach has two major
    weaknesses. First, the performance of the model can be highly dependent on which
    few observations were selected for the test set. Second, the model is not being
    trained using all the available data, and it’s not being evaluated on all the
    available data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一种策略可能是留出一部分数据用于测试。这被称为*验证*（或*留出法*）。在验证中，我们的观察（特征和目标）被分成两个集合，传统上称为*训练集*和*测试集*。我们拿出测试集并将其放在一边，假装我们以前从未见过它。接下来，我们使用训练集训练我们的模型，使用特征和目标向量来教模型如何做出最佳预测。最后，我们通过评估模型在测试集上的表现来模拟从未见过的外部数据。然而，验证方法有两个主要弱点。首先，模型的性能可能高度依赖于被选择为测试集的少数观察结果。其次，模型不是在所有可用数据上进行训练，也没有在所有可用数据上进行评估。
- en: A better strategy, which overcomes these weaknesses, is called *k-fold cross-validation*
    (KFCV). In KFCV, we split the data into *k* parts called *folds*. The model is
    then trained using *k – 1* folds—​combined into one training set—​and then the
    last fold is used as a test set. We repeat this *k* times, each time using a different
    fold as the test set. The performance on the model for each of the *k* iterations
    is then averaged to produce an overall measurement.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这些弱点的更好策略被称为*k折交叉验证*（KFCV）。在KFCV中，我们将数据分成*k*个部分，称为*折叠*。然后，模型使用*k-1*个折叠组成的一个训练集进行训练，然后最后一个折叠被用作测试集。我们重复这个过程*k*次，每次使用不同的折叠作为测试集。然后对每个*k*次迭代中模型的表现进行平均，以产生一个总体测量。
- en: 'In our solution, we conducted k-fold cross-validation using five folds and
    outputted the evaluation scores to `cv_results`:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，我们使用五折交叉验证并将评估分数输出到 `cv_results` 中：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There are three important points to consider when we are using KFCV. First,
    KFCV assumes that each observation was created independently from the other (i.e.,
    the data is independent and identically distributed [IID]). If the data is IID,
    it is a good idea to shuffle observations when assigning to folds. In scikit-learn
    we can set `shuffle=True` to perform shuffling.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 KFCV 时有三个重要的注意事项。首先，KFCV 假设每个观察结果都是独立生成的（即数据是独立同分布的[IID]）。如果数据是 IID 的，将观察结果随机分配到
    fold 时进行洗牌是一个好主意。在 scikit-learn 中，我们可以设置 `shuffle=True` 来执行洗牌。
- en: Second, when we are using KFCV to evaluate a classifier, it is often beneficial
    to have folds containing roughly the same percentage of observations from each
    of the different target classes (called *stratified k-fold*). For example, if
    our target vector contained gender and 80% of the observations were male, then
    each fold would contain 80% male and 20% female observations. In scikit-learn,
    we can conduct stratified k-fold cross-validation by replacing the `KFold` class
    with `StratifiedKFold`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在使用 KFCV 评估分类器时，通常有利于每个 fold 中大致包含来自不同目标类的观察结果的相同百分比（称为*分层 k 折*）。例如，如果我们的目标向量包含性别信息，并且观察结果中有
    80% 是男性，那么每个 fold 将包含 80% 的男性和 20% 的女性观察结果。在 scikit-learn 中，我们可以通过将 `KFold` 类替换为
    `StratifiedKFold` 来执行分层 k 折交叉验证。
- en: 'Finally, when we are using validation sets or cross-validation, it is important
    to preprocess data based on the training set and then apply those transformations
    to both the training and test set. For example, when we `fit` our standardization
    object, `standardizer`, we calculate the mean and variance of only the training
    set. Then we apply that transformation (using `transform`) to both the training
    and test sets:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在使用验证集或交叉验证时，重要的是基于训练集预处理数据，然后将这些转换应用到训练集和测试集。例如，当我们对标准化对象 `standardizer`
    进行 `fit` 操作时，我们仅计算训练集的均值和方差。然后，我们使用 `transform` 将该转换应用于训练集和测试集：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The reason for this is because we are pretending that the test set is unknown
    data. If we fit both our preprocessors using observations from both training and
    test sets, some of the information from the test set leaks into our training set.
    This rule applies for any preprocessing step such as feature selection.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的原因是因为我们假设测试集是未知数据。如果我们使用观察结果来同时拟合训练集和测试集的预处理器，测试集中的部分信息就会泄漏到训练集中。对于任何预处理步骤，如特征选择，都适用这一规则。
- en: 'scikit-learn’s `pipeline` package makes this easy to do while using cross-validation
    techniques. We first create a pipeline that preprocesses the data (e.g., `standardizer`)
    and then trains a model (logistic regression, `logit`):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的 `pipeline` 包使得在使用交叉验证技术时变得更加简单。我们首先创建一个管道来预处理数据（例如 `standardizer`），然后训练一个模型（逻辑回归，`logit`）：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then we run KFCV using that pipeline and scikit does all the work for us:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用该管道运行 KFCV，scikit 会为我们完成所有工作：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`cross_val_score` comes with three parameters we have not discussed, but that
    are worth noting:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`cross_val_score` 带有三个参数，我们还没有讨论过，但值得注意：'
- en: '`cv`'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`cv`'
- en: '`cv` determines our cross-validation technique. K-fold is the most common by
    far, but there are others, such as leave-one-out cross-validation where the number
    of folds *k* equals the number of data points in the set.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`cv` 确定了我们的交叉验证技术。K 折是目前最常用的，但还有其他方法，例如留一法交叉验证，其中 fold 的数量 *k* 等于数据集中的数据点数量。'
- en: '`scoring`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`scoring`'
- en: '`scoring` defines the metric for success, a number of which are discussed in
    other recipes in this chapter.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`scoring` 定义了成功的度量标准，本章的其他示例中讨论了其中一些。'
- en: '`n_jobs=-1`'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_jobs=-1`'
- en: '`n_jobs=-1` tells scikit-learn to use every core available. For example, if
    your computer has four cores (a common number for laptops), then scikit-learn
    will use all four cores at once to speed up the operation.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_jobs=-1` 告诉 scikit-learn 使用所有可用的核心。例如，如果您的计算机有四个核心（笔记本电脑上常见的数量），那么 scikit-learn
    将同时使用所有四个核心来加速操作。'
- en: 'One small note: when running some of these examples, you may see a warning
    that says “ConvergenceWarning: lbfgs failed to converge.” The configuration used
    in these examples is designed to prevent this, but should it still occur, you
    can ignore it for now. We will troubleshoot issues like this later in the book
    as we dive into specific types of models.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '一个小提示：当运行其中一些示例时，您可能会看到一个警告，提示“ConvergenceWarning: lbfgs failed to converge.”
    这些示例中使用的配置旨在防止这种情况发生，但如果仍然发生，您可以暂时忽略它。我们将在本书后面深入研究具体类型的模型时解决此类问题。'
- en: See Also
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Why Every Statistician Should Know About Cross-Validation](https://oreil.ly/vrGXy)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么每个统计学家都应该了解交叉验证](https://oreil.ly/vrGXy)'
- en: '[Cross-Validation Gone Wrong](https://oreil.ly/NE-B8)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[交叉验证的错误应用](https://oreil.ly/NE-B8)'
- en: 11.2 Creating a Baseline Regression Model
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.2 创建一个基线回归模型
- en: Problem
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want a simple baseline regression model to use as a comparison against other
    models that you train.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要一个简单的基线回归模型，以便与您训练的其他模型进行比较。
- en: Solution
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use scikit-learn’s `DummyRegressor` to create a simple model to use as a baseline:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的`DummyRegressor`创建一个简单的基线模型：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To compare, we train our model and evaluate the performance score:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，我们训练我们的模型并评估性能分数：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Discussion
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '`DummyRegressor` allows us to create a very simple model that we can use as
    a baseline to compare against any other models that we train. This can often be
    useful to simulate a “naive” existing prediction process in a product or system.
    For example, a product might have been originally hardcoded to assume that all
    new users will spend $100 in the first month, regardless of their features. If
    we encode that assumption into a baseline model, we are able to concretely state
    the benefits of using a machine learning approach by comparing the dummy model’s
    `score` with that of a trained model.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`DummyRegressor` 允许我们创建一个非常简单的模型，我们可以用作基线，以与我们训练的任何其他模型进行比较。这通常可以用来模拟产品或系统中的“天真”现有预测过程。例如，产品可能最初被硬编码为假设所有新用户在第一个月内都会花费100美元，而不考虑其特征。如果我们将这种假设编码到基线模型中，我们就能够通过比较虚拟模型的`score`与训练模型的分数来明确说明使用机器学习方法的好处。'
- en: '`DummyRegressor` uses the `strategy` parameter to set the method of making
    predictions, including the mean or median value in the training set. Furthermore,
    if we set `strategy` to `constant` and use the `constant` parameter, we can set
    the dummy regressor to predict some constant value for every observation:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`DummyRegressor` 使用`strategy`参数来设置预测方法，包括在训练集中使用平均值或中位数。此外，如果我们将`strategy`设置为`constant`并使用`constant`参数，我们可以设置虚拟回归器来预测每个观测的某个常数值：'
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'One small note regarding `score`. By default, `score` returns the coefficient
    of determination (R-squared, <math display="inline"><msup><mi>R</mi> <mn>2</mn></msup></math>
    ) score:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`score`的一个小注意事项。默认情况下，`score`返回确定系数（R-squared，<math display="inline"><msup><mi>R</mi>
    <mn>2</mn></msup></math>）得分：
- en: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><msub><mo>∑</mo> <mi>i</mi></msub> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mrow><msub><mo>∑</mo>
    <mi>i</mi></msub> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>-</mo><mover
    accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow> <mn>2</mn></msup></mrow></mfrac></mrow></math>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><msub><mo>∑</mo> <mi>i</mi></msub> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mrow><msub><mo>∑</mo>
    <mi>i</mi></msub> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>-</mo><mover
    accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow> <mn>2</mn></msup></mrow></mfrac></mrow></math>
- en: where <math display="inline"><msub><mi>y</mi> <mi>i</mi></msub></math> is the
    true value of the target observation, <math display="inline"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover>
    <mi>i</mi></msub></math> is the predicted value, and <math display="inline"><mover
    accent="true"><mi>y</mi><mo>¯</mo></mover></math> is the mean value for the target
    vector.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><msub><mi>y</mi> <mi>i</mi></msub></math>是目标观测的真实值，<math
    display="inline"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>i</mi></msub></math>是预测值，而<math
    display="inline"><mover accent="true"><mi>y</mi><mo>¯</mo></mover></math>是目标向量的平均值。
- en: The closer <math display="inline"><msup><mi>R</mi> <mn>2</mn></msup></math>
    is to 1, the more of the variance in the target vector that is explained by the
    features.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="inline"><msup><mi>R</mi> <mn>2</mn></msup></math>越接近1，目标向量中方差被特征解释的程度就越高。
- en: 11.3 Creating a Baseline Classification Model
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.3 创建一个基准分类模型
- en: Problem
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want a simple baseline classifier to compare against your model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要一个简单的基线分类器来与您的模型进行比较。
- en: Solution
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use scikit-learn’s `DummyClassifier`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的`DummyClassifier`：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'By comparing the baseline classifier to our trained classifier, we can see
    the improvement:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较基线分类器和我们训练的分类器，我们可以看到改进：
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Discussion
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: A common measure of a classifier’s performance is how much better it is than
    random guessing. scikit-learn’s `DummyClassifier` makes this comparison easy.
    The `strategy` parameter gives us a number of options for generating values. There
    are two particularly useful strategies. First, `stratified` makes predictions
    proportional to the class proportions of the training set’s target vector (e.g.,
    if 20% of the observations in the training data are women, then `DummyClassifier`
    will predict women 20% of the time). Second, `uniform` will generate predictions
    uniformly at random between the different classes. For example, if 20% of observations
    are women and 80% are men, `uniform` will produce predictions that are 50% women
    and 50% men.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类器性能的常见测量是它比随机猜测好多少。scikit-learn的`DummyClassifier`使得这种比较变得容易。`strategy`参数提供了多种生成值的选项。有两种特别有用的策略。首先，`stratified`按训练集目标向量的类比例生成预测（例如，如果训练数据中有20%的观察结果是女性，则`DummyClassifier`将20%的时间预测为女性）。其次，`uniform`将在不同类别之间以均匀随机方式生成预测。例如，如果观察结果中有20%是女性，80%是男性，则`uniform`会生成50%女性和50%男性的预测。
- en: See Also
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation: DummyClassifier](https://oreil.ly/bwqQU)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn文档：DummyClassifier](https://oreil.ly/bwqQU)'
- en: 11.4 Evaluating Binary Classifier Predictions
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.4 评估二元分类器预测
- en: Problem
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Given a trained classification model, you want to evaluate its quality.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个训练好的分类模型，你想评估其质量。
- en: Solution
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use scikit-learn’s `cross_val_score` to conduct cross-validation while using
    the `scoring` parameter to define one of a number of performance metrics, including
    accuracy, precision, recall, and *F[1]*. *Accuracy* is a common performance metric.
    It is simply the proportion of observations predicted correctly:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn的`cross_val_score`进行交叉验证，同时使用`scoring`参数来定义一系列性能指标，包括准确度、精确度、召回率和*F[1]*。*准确度*是一种常见的性能指标。它简单地表示预测正确的观察比例：
- en: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi>A</mi>
    <mi>c</mi> <mi>c</mi> <mi>u</mi> <mi>r</mi> <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo>
    <mfrac><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow> <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></mstyle></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi>A</mi>
    <mi>c</mi> <mi>c</mi> <mi>u</mi> <mi>r</mi> <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo>
    <mfrac><mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi></mrow> <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></mstyle></math>
- en: 'where:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: <math display="inline"><mi>T</mi><mi>P</mi></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="inline"><mi>T</mi><mi>P</mi></math>
- en: The number of true positives. These are observations that are part of the *positive*
    class (has the disease, purchased the product, etc.) and that we predicted correctly.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 真阳性数量。这些是属于*阳性*类别（患病、购买产品等）并且我们预测正确的观察结果。
- en: <math display="inline"><mi>T</mi><mi>N</mi></math>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="inline"><mi>T</mi><mi>N</mi></math>
- en: The number of true negatives. These are observations that are part of the *negative*
    class (does not have the disease, did not purchase the product, etc.) and that
    we predicted correctly.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 真阴性数量。这些是属于*阴性*类别（未患病、未购买产品等）并且我们预测正确的观察结果。
- en: <math display="inline"><mi>F</mi><mi>P</mi></math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="inline"><mi>F</mi><mi>P</mi></math>
- en: The number of false positives, also called a *Type I error*. These are observations
    that are predicted to be part of the *positive* class but are actually part of
    the *negative* class.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 假阳性数量，也称为*I型错误*。这些是被预测为*阳性*类别但实际上属于*阴性*类别的观察结果。
- en: <math display="inline"><mi>F</mi><mi>N</mi></math>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="inline"><mi>F</mi><mi>N</mi></math>
- en: The number of false negatives, also called a *Type II error*. These are observations
    that are predicted to be part of the *negative* class but are actually part of
    the *positive* class.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假阴性数量，也称为*II型错误*。这些是被预测为*阴性*类别但实际上属于*阳性*类别的观察结果。
- en: 'We can measure accuracy in three-fold (the default number of folds) cross-validation
    by setting `scoring="accuracy"`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过设置`scoring="accuracy"`来在三折（默认折数）交叉验证中测量准确度：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The appeal of accuracy is that it has an intuitive and plain English explanation:
    the proportion of observations predicted correctly. However, in the real world,
    often our data has imbalanced classes (e.g., the 99.9% of observations are of
    class 1 and only 0.1% are class 2). When in the presence of imbalanced classes,
    accuracy suffers from a paradox where a model is highly accurate but lacks predictive
    power. For example, imagine we are trying to predict the presence of a very rare
    cancer that occurs in 0.1% of the population. After training our model, we find
    the accuracy is at 95%. However, 99.9% of people do not have the cancer: if we
    simply created a model that “predicted” that nobody had that form of cancer, our
    naive model would be 4.9% more accurate, but it clearly is not able to *predict*
    anything. For this reason, we are often motivated to use other metrics such as
    precision, recall, and the *F[1]* score.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率的吸引力在于它有一个直观和简单的英文解释：被正确预测的观测的比例。然而，在现实世界中，通常我们的数据有不平衡的类别（例如，99.9%的观测属于类别1，只有0.1%属于类别2）。当面对不平衡的类别时，准确率会遇到一个悖论，即模型准确率很高，但缺乏预测能力。例如，想象我们试图预测一个在人群中发生率为0.1%的非常罕见的癌症的存在。在训练完我们的模型后，我们发现准确率为95%。然而，99.9%的人没有这种癌症：如果我们简单地创建一个“预测”没有人有这种癌症的模型，我们的天真模型将更准确，但显然它不能*预测*任何事情。因此，我们常常有动机使用其他指标，如精确度、召回率和*F[1]*分数。
- en: '*Precision* is the proportion of every observation predicted to be positive
    that is actually positive. We can think about it as a measurement noise in our
    predictions—that is, how likely we are to be right when we predict something is
    positive. Models with high precision are pessimistic in that they predict an observation
    is of the positive class only when they are very certain about it. Formally, precision
    is:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确度*是每个被预测为正类的观测中实际为正类的比例。我们可以将其看作是我们预测中的噪声测量——也就是说，我们在预测某事是正的时候有多大可能是对的。精确度高的模型是悲观的，因为他们仅在非常肯定的情况下预测某个观测属于正类。形式上，精确度是：'
- en: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi
    fontstyle="italic">Precision</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow></mstyle></math>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi
    fontstyle="italic">Precision</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi></mrow></mfrac></mrow></mstyle></math>
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Recall* is the proportion of every positive observation that is truly positive.
    Recall measures the model’s ability to identify an observation of the positive
    class. Models with high recall are optimistic in that they have a low bar for
    predicting that an observation is in the positive class:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率*是每个真正正例中被正确预测的比例。召回率衡量了模型识别正类观测的能力。召回率高的模型是乐观的，因为他们在预测某个观测属于正类时的门槛很低：'
- en: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi
    fontstyle="italic">Recall</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></mstyle></math>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi
    fontstyle="italic">Recall</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow></mfrac></mrow></mstyle></math>
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If this is the first time you have encountered precision and recall, it is
    understandable if it takes a little while to fully understand them. This is one
    of the downsides to accuracy; precision and recall are less intuitive. Almost
    always we want some kind of balance between precision and recall, and this role
    is filled by the *F[1]* score. The *F[1]* score is the *harmonic mean* (a kind
    of average used for ratios):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是你第一次遇到精确度和召回率，如果需要一些时间才能完全理解它们，那是可以理解的。这是准确率的一个缺点；精确度和召回率不太直观。几乎总是我们希望在精确度和召回率之间达到某种平衡，而这种角色由*F[1]*分数扮演。*F[1]*分数是*调和平均数*（一种用于比率的平均数）：
- en: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><msub><mi>F</mi>
    <mn fontstyle="italic">1</mn></msub> <mo>=</mo> <mn>2</mn> <mo>×</mo> <mfrac><mrow><mi
    fontstyle="italic">Precision</mi> <mo>×</mo> <mi fontstyle="italic">Recall</mi></mrow>
    <mrow><mi fontstyle="italic">Precision</mi> <mo>+</mo> <mi fontstyle="italic">Recall</mi></mrow></mfrac></mrow></mstyle></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><msub><mi>F</mi>
    <mn fontstyle="italic">1</mn></msub> <mo>=</mo> <mn>2</mn> <mo>×</mo> <mfrac><mrow><mi
    fontstyle="italic">Precision</mi> <mo>×</mo> <mi fontstyle="italic">Recall</mi></mrow>
    <mrow><mi fontstyle="italic">Precision</mi> <mo>+</mo> <mi fontstyle="italic">Recall</mi></mrow></mfrac></mrow></mstyle></math>
- en: 'This score is a measure of correctness achieved in positive prediction—that
    is, of observations labeled as positive, how many are actually positive:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分数是正预测中实现的正确性的一种度量——也就是说，标记为正的观测中有多少实际上是正的：
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Discussion
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: As an evaluation metric, accuracy has some valuable properties, especially its
    intuitiveness. However, better metrics often involve using some balance of precision
    and recall—that is, a trade-off between the optimism and pessimism of our model.
    *F[1]* represents a balance between the recall and precision, where the relative
    contributions of both are equal.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 作为评估指标，准确率具有一些有价值的特性，特别是它的直观性。然而，更好的指标通常涉及使用一定平衡的精确度和召回率——也就是说，我们模型的乐观和悲观之间存在一种权衡。*F[1]*代表着召回率和精确度之间的平衡，其中两者的相对贡献是相等的。
- en: 'As an alternative to using `cross_val_score`, if we already have the true y
    values and the predicted y values, we can calculate the metrics accuracy and recall
    directly:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 作为使用`cross_val_score`的替代方案，如果我们已经有了真实的y值和预测的y值，我们可以直接计算准确率和召回率等指标：
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: See Also
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Accuracy paradox, Wikipedia](https://oreil.ly/vjgZ-)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[准确率悖论，维基百科](https://oreil.ly/vjgZ-) '
- en: 11.5 Evaluating Binary Classifier Thresholds
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.5 评估二元分类器阈值
- en: Problem
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to evaluate a binary classifier and various probability thresholds.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你想评估一个二元分类器和各种概率阈值。
- en: Solution
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the *receiver operating characteristic* (ROC) curve to evaluate the quality
    of the binary classifier. In scikit-learn, we can use `roc_curve` to calculate
    the true and false positives at each threshold, and then plot them:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*接收者操作特征曲线*（ROC曲线）来评估二元分类器的质量。在scikit-learn中，我们可以使用`roc_curve`来计算每个阈值下的真正例和假正例，然后绘制它们：
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![mpc2 11in01](assets/mpc2_11in01.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 11in01](assets/mpc2_11in01.png)'
- en: Discussion
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The receiver operating characteristic curve is a common method for evaluating
    the quality of a binary classifier. ROC compares the presence of true positives
    and false positives at every probability threshold (i.e., the probability at which
    an observation is predicted to be a class). By plotting the ROC curve, we can
    see how the model performs. A classifier that predicts every observation correctly
    would look like the solid light gray line in the ROC output in the previous figure,
    going straight up to the top immediately. A classifier that predicts at random
    will appear as the diagonal line. The better the model, the closer it is to the
    solid line.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接收者操作特征曲线是评估二元分类器质量的常见方法。ROC在每个概率阈值（即观测被预测为一类的概率）下比较真正例和假正例的存在。通过绘制ROC曲线，我们可以看到模型的表现。一个完全正确预测每个观测的分类器将看起来像前一图中ROC输出的实线浅灰色线，立即向顶部直线上升。预测随机的分类器将出现为对角线。模型越好，它距实线越接近。
- en: 'Until now we have only examined models based on the values they predict. However,
    in many learning algorithms, those predicted values are based on probability estimates.
    That is, each observation is given an explicit probability of belonging in each
    class. In our solution, we can use `predict_proba` to see the predicted probabilities
    for the first observation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只根据它们预测的值来检查模型。然而，在许多学习算法中，这些预测的值是基于概率估计的。也就是说，每个观测都被赋予属于每个类别的显式概率。在我们的解决方案中，我们可以使用`predict_proba`来查看第一个观测的预测概率：
- en: '[PRE28]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can see the classes using `classes_`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`classes_`来查看类别：
- en: '[PRE30]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In this example, the first observation has an ~87% chance of being in the negative
    class (`0`) and a 13% chance of being in the positive class (`1`). By default,
    scikit-learn predicts an observation is part of the positive class if the probability
    is greater than 0.5 (called the *threshold*). However, instead of a middle ground,
    we will often want to explicitly bias our model to use a different threshold for
    substantive reasons. For example, if a false positive is very costly to our company,
    we might prefer a model that has a high probability threshold. We fail to predict
    some positives, but when an observation is predicted to be positive, we can be
    very confident that the prediction is correct. This trade-off is represented in
    the *true positive rate* (TPR) and the *false positive rate* (FPR). The TPR is
    the number of observations correctly predicted true divided by all true positive
    observations:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，第一个观测有约87%的概率属于负类（`0`），13%的概率属于正类（`1`）。默认情况下，scikit-learn预测如果概率大于0.5，则观测属于正类（称为*阈值*）。然而，我们经常希望明确地偏置我们的模型以使用不同的阈值出于实质性原因，而不是中间地带。例如，如果一个假阳性对我们的公司造成很高的成本，我们可能更喜欢一个概率阈值较高的模型。我们未能预测一些正例，但当观测被预测为正例时，我们可以非常确信预测是正确的。这种权衡体现在*真正例率*（TPR）和*假正例率*（FPR）中。TPR是正确预测为真的观测数除以所有真正的正例观测数：
- en: <math display="block"><mrow><mtext>TPR</mtext> <mo>=</mo> <mfrac><mrow><mtext>TP</mtext></mrow>
    <mrow><mtext>TP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></mrow></math>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>TPR</mtext> <mo>=</mo> <mfrac><mrow><mtext>TP</mtext></mrow>
    <mrow><mtext>TP</mtext><mo>+</mo><mtext>FN</mtext></mrow></mfrac></mrow></math>
- en: 'The FPR is the number of incorrectly predicted positives divided by all true
    negative observations:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: FPR是错误预测的正例数除以所有真负例观测数：
- en: <math display="block"><mrow><mtext>FPR</mtext> <mo>=</mo> <mfrac><mrow><mtext>FP</mtext></mrow>
    <mrow><mtext>FP</mtext><mo>+</mo><mtext>TN</mtext></mrow></mfrac></mrow></math>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>FPR</mtext> <mo>=</mo> <mfrac><mrow><mtext>FP</mtext></mrow>
    <mrow><mtext>FP</mtext><mo>+</mo><mtext>TN</mtext></mrow></mfrac></mrow></math>
- en: 'The ROC curve represents the respective TPR and FPR for every probability threshold.
    For example, in our solution a threshold of roughly 0.50 has a TPR of ~0.83 and
    an FPR of ~0.16:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线代表每个概率阈值下的相应TPR和FPR。例如，在我们的解决方案中，大约0.50的阈值具有约0.83的TPR和约0.16的FPR：
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'However, if we increase the threshold to ~80% (i.e., increase how certain the
    model has to be before it predicts an observation as positive) the TPR drops significantly
    but so does the FPR:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们将阈值提高到约80%（即，在模型预测观测为正类之前，增加其必须确定的程度），TPR显著下降，但FPR也是如此：
- en: '[PRE34]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This is because our higher requirement for being predicted to be in the positive
    class has caused the model to not identify a number of positive observations (the
    lower TPR) but has also reduced the noise from negative observations being predicted
    as positive (the lower FPR).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们对预测为正类有更高要求，导致模型未能识别出一些正例（较低的TPR），但也减少了负面观测被预测为正面的噪声（较低的FPR）。
- en: 'In addition to being able to visualize the trade-off between TPR and FPR, the
    ROC curve can also be used as a general metric for a model. The better a model
    is, the higher the curve and thus the greater the area under the curve. For this
    reason, it is common to calculate the area under the ROC curve (AUC ROC) to judge
    the overall quality of a model at all possible thresholds. The closer the AUC
    ROC is to 1, the better the model. In scikit-learn we can calculate the AUC ROC
    using `roc_auc_score`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 除了能够可视化TPR和FPR之间的权衡之外，ROC曲线还可以用作模型的一般度量。模型越好，曲线越高，因此曲线下面积也越大。因此，通常计算ROC曲线下面积（AUC
    ROC）来判断模型在所有可能阈值下的总体质量。AUC ROC越接近1，模型越好。在scikit-learn中，我们可以使用`roc_auc_score`来计算AUC
    ROC：
- en: '[PRE36]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: See Also
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[ROC Curves in Python and R](https://oreil.ly/0qcpZ)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python和R中的ROC曲线](https://oreil.ly/0qcpZ)'
- en: '[The Area Under an ROC Curve](https://oreil.ly/re7sT)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ROC曲线下面积](https://oreil.ly/re7sT)'
- en: 11.6 Evaluating Multiclass Classifier Predictions
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.6 评估多类分类器预测
- en: Problem
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a model that predicts three or more classes and want to evaluate the
    model’s performance.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个预测三个或更多类别的模型，并希望评估模型的性能。
- en: Solution
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use cross-validation with an evaluation metric capable of handling more than
    two classes:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用能够处理两个以上类别的评估指标进行交叉验证：
- en: '[PRE38]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Discussion
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: When we have balanced classes (i.e., a roughly equal number of observations
    in each class of the target vector), accuracy is—​just like in the binary class
    setting—​a simple and interpretable choice for an evaluation metric. Accuracy
    is the number of correct predictions divided by the number of observations and
    works just as well in the multiclass as in the binary setting. However, when we
    have imbalanced classes (a common scenario), we should be inclined to use other
    evaluation metrics.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有均衡的类别（即目标向量中每个类别的观测值数量大致相等）时，准确率就像在二分类设置中一样，是一种简单且可解释的评估指标选择。准确率是正确预测数量除以观测数量，无论是在多分类还是二分类设置中都同样有效。然而，当我们有不平衡的类别（这是一个常见情况）时，我们应该倾向于使用其他评估指标。
- en: 'Many of scikit-learn’s built-in metrics are for evaluating binary classifiers.
    However, many of these metrics can be extended for use when we have more than
    two classes. Precision, recall, and *F[1]* scores are useful metrics that we have
    already covered in detail in previous recipes. While all of them were originally
    designed for binary classifiers, we can apply them to multiclass settings by treating
    our data as a set of binary classes. Doing so enables us to apply the metrics
    to each class as if it were the only class in the data, and then aggregate the
    evaluation scores for all the classes by averaging them:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 许多scikit-learn内置的度量是用于评估二元分类器的。然而，许多这些度量可以扩展用于当我们有两个以上类别时的情况。精确率、召回率和*F[1]*分数是我们在之前的配方中已经详细介绍过的有用度量。虽然它们都是最初设计用于二元分类器的，但我们可以将它们应用于多类别设置，通过将我们的数据视为一组二元类别来处理。这样做使我们能够将度量应用于每个类别，就好像它是数据中唯一的类别一样，然后通过对所有类别的评估分数进行平均来聚合它们：
- en: '[PRE40]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In this code, `macro` refers to the method used to average the evaluation scores
    from the classes. The options are `macro`, `weighted`, and `micro`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中，`macro` 指的是用于计算类别评估分数平均值的方法。选项包括`macro`、`weighted`和`micro`：
- en: '`macro`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`macro`'
- en: Calculate the mean of metric scores for each class, weighting each class equally.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每个类别的度量分数的均值，每个类别的权重相等。
- en: '`weighted`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`weighted`'
- en: Calculate the mean of metric scores for each class, weighting each class proportional
    to its size in the data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每个类别的度量分数的均值，权重为数据中每个类别的大小。
- en: '`micro`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`micro`'
- en: Calculate the mean of metric scores for each observation-class combination.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每个观测-类别组合的度量分数的均值。
- en: 11.7 Visualizing a Classifier’s Performance
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.7 可视化分类器的性能
- en: Problem
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Given predicted classes and true classes of the test data, you want to visually
    compare the model’s quality.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 给定测试数据的预测类别和真实类别，您希望直观比较模型的质量。
- en: Solution
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use a *confusion matrix*, which compares predicted classes and true classes:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*混淆矩阵*，比较预测类别和真实类别：
- en: '[PRE42]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![mpc2 11in02](assets/mpc2_11in02.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 11in02](assets/mpc2_11in02.png)'
- en: Discussion
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Confusion matrices are an easy, effective visualization of a classifier’s performance.
    One of the major benefits of confusion matrices is their interpretability. Each
    column of the matrix (often visualized as a heatmap) represents predicted classes,
    while every row shows true classes. The result is that every cell is one possible
    combination of predicted and true classes. This is probably best explained using
    an example. In the solution, the top-left cell is the number of observations predicted
    to be *Iris setosa* (indicated by the column) that are actually *Iris setosa*
    (indicated by the row). This means the model accurately predicted all *Iris setosa*
    flowers. However, the model does not do as well at predicting *Iris virginica*.
    The bottom-right cell indicates that the model successfully predicted eleven observations
    were *Iris virginica*, but (looking one cell up) predicted one flower to be *virginica*
    that was actually *Iris versicolor*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是分类器性能的一种简单有效的可视化方式。混淆矩阵的主要优势之一是它们的可解释性。矩阵的每一列（通常可视化为热图）代表预测类别，而每一行显示真实类别。结果是每个单元格都是预测和真实类别的一种可能组合。这可能最好通过一个例子来解释。在解决方案中，左上角的单元格是预测为*Iris
    setosa*（由列表示）的观察数量，它们实际上是*Iris setosa*（由行表示）。这意味着模型准确地预测了所有*Iris setosa*的花。然而，该模型在预测*Iris
    virginica*时并不那么成功。右下角的单元格表示模型成功预测了十一个观察结果为*Iris virginica*，但（向上查看一个单元格）预测了一个实际上是*Iris
    versicolor*的花为*virginica*。
- en: There are three things worth noting about confusion matrices. First, a perfect
    model will have values along the diagonal and zeros everywhere else. A bad model
    will have the observation counts spread evenly around cells. Second, a confusion
    matrix lets us see not only where the model was wrong but also how it was wrong.
    That is, we can look at patterns of misclassification. For example, our model
    had an easy time differentiating *Iris virginica* and *Iris setosa*, but a slightly
    more difficult time classifying *Iris virginica* and *Iris versicolor*. Finally,
    confusion matrices work with any number of classes (although if we had one million
    classes in our target vector, the confusion matrix visualization might be difficult
    to read).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 关于混淆矩阵有三点值得注意。首先，一个完美的模型会在对角线上有数值，其他地方都是零。一个糟糕的模型会使观察计数均匀地分布在单元格周围。其次，混淆矩阵让我们不仅能看到模型错在哪里，还能看到它错在哪里。也就是说，我们可以看到误分类的模式。例如，我们的模型很容易区分*Iris
    virginica*和*Iris setosa*，但在分类*Iris virginica*和*Iris versicolor*时稍微困难一些。最后，混淆矩阵适用于任意数量的类别（尽管如果目标向量中有一百万个类别，混淆矩阵的可视化可能会难以阅读）。
- en: See Also
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Confusion matrix, Wikipedia](https://oreil.ly/tDWPB)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[混淆矩阵，维基百科](https://oreil.ly/tDWPB)'
- en: '[scikit-learn documentation: Confusion Matrix](https://oreil.ly/fdsTg)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn文档：混淆矩阵](https://oreil.ly/fdsTg)'
- en: 11.8 Evaluating Regression Models
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.8 评估回归模型
- en: Problem
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to evaluate the performance of a regression model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要评估回归模型的性能。
- en: Solution
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use *mean squared error* (MSE):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *均方误差*（MSE）：
- en: '[PRE43]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Another common regression metric is the coefficient of determination, *R²*:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的回归指标是确定系数，*R²*：
- en: '[PRE45]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Discussion
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'MSE is one of the most common evaluation metrics for regression models. Formally,
    MSE is:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: MSE是回归模型中最常见的评估指标之一。形式上，MSE是：
- en: <math display="block"><mrow><mo form="prefix">MSE</mo> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mo form="prefix">MSE</mo> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover><mi>i</mi></msub><mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: 'where <math display="inline"><mi>n</mi></math> is the number of observations,
    <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the true value
    of the target we are trying to predict for observation <math display="inline"><mi>i</mi></math>,
    and <math display="inline"><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover><mi>i</mi></msub></math>
    is the model’s predicted value for <math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>.
    MSE is a measurement of the squared sum of all distances between predicted and
    true values. The higher the value of MSE, the greater the total squared error
    and thus the worse the model. There are a number of mathematical benefits to squaring
    the error term, including that it forces all error values to be positive, but
    one often unrealized implication is that squaring penalizes a few large errors
    more than many small errors, even if the absolute value of the errors is the same.
    For example, imagine two models, A and B, each with two observations:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><mi>n</mi></math> 是观察次数，<math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>
    是我们试图预测的目标的真实值，对于观察 <math display="inline"><mi>i</mi></math>，<math display="inline"><msub><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mi>i</mi></msub></math> 是模型对 <math
    display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> 的预测值。均方误差（MSE）是所有预测值与真实值之间距离的平方和的度量。MSE
    值越高，总体平方误差越大，因此模型越糟糕。平方误差项的数学优势包括强制所有误差值为正，但一个常常未被意识到的影响是，平方会比许多小误差更严厉地惩罚少量大误差，即使这些误差的绝对值相同。例如，想象两个模型，A
    和 B，每个模型有两个观察：
- en: Model A has errors of 0 and 10, and thus its MSE is *0² + 10² = 100*.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型 A 的误差为 0 和 10，因此其 MSE 为 *0² + 10² = 100*。
- en: Model B has two errors of 5 each, and thus its MSE is *5² + 5² = 50*.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型 B 每个误差为 5，因此其 MSE 为 *5² + 5² = 50*。
- en: Both models have the same total errors, 10; however, MSE would consider model
    A (MSE = 100) worse than model B (MSE = 50). In practice this implication is rarely
    an issue (and indeed can be theoretically beneficial), and MSE works perfectly
    fine as an evaluation metric.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型的总误差都是 10；然而，MSE 认为模型 A（MSE = 100）比模型 B（MSE = 50）更差。在实践中，这种影响很少成问题（实际上理论上有益），并且
    MSE 作为评估指标运行得非常好。
- en: 'One important note: by default, in scikit-learn, arguments of the `scoring`
    parameter assume that higher values are better than lower values. However, this
    is not the case for MSE, where higher values mean a worse model. For this reason,
    scikit-learn looks at the *negative* MSE using the `neg_mean_squared_error` argument.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的注释：在 scikit-learn 中，默认情况下，`scoring` 参数假定更高的值优于较低的值。然而，对于 MSE，情况并非如此，较高的值意味着模型较差。因此，scikit-learn
    使用 `neg_mean_squared_error` 参数来观察 *负* MSE。
- en: A common alternative regression evaluation metric is the default metric we used
    in [Recipe 11.2](#creating-a-baseline-regression-model), <math display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>,
    which measures the amount of variance in the target vector that is explained by
    the model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的替代回归评估指标是我们在 [Recipe 11.2](#creating-a-baseline-regression-model) 中使用的默认指标
    <math display="inline"><msup><mi>R</mi><mn>2</mn></msup></math>，它衡量模型解释的目标向量方差量。
- en: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></math>
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></math>
- en: where <math display="inline"><msub><mi>y</mi> <mi>i</mi></msub></math> is the
    true target value of the *i*th observation, <math display="inline"><msub><mover
    accent="true"><mi>y</mi><mo>^</mo></mover> <mi>i</mi></msub></math> is the predicted
    value for the *i*th observation, and <math display="inline"><mover accent="true"><mi>y</mi>
    <mo>¯</mo></mover></math> is the mean value of the target vector. The closer that
    <math display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> is to 1.0, the
    better the model.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><msub><mi>y</mi> <mi>i</mi></msub></math> 是第 *i* 个观察的真实目标值，<math
    display="inline"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>i</mi></msub></math>
    是第 *i* 个观察的预测值，<math display="inline"><mover accent="true"><mi>y</mi> <mo>¯</mo></mover></math>
    是目标向量的均值。当 <math display="inline"><msup><mi>R</mi><mn>2</mn></msup></math> 接近
    1.0 时，模型越好。
- en: See Also
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Mean squared error, Wikipedia](https://oreil.ly/MWDlR)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[均方误差，维基百科](https://oreil.ly/MWDlR)'
- en: '[Coefficient of determination, Wikipedia](https://oreil.ly/lKKWk)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决定系数，维基百科](https://oreil.ly/lKKWk)'
- en: 11.9 Evaluating Clustering Models
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.9 评估聚类模型
- en: Problem
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have used an unsupervised learning algorithm to cluster your data. Now you
    want to know how well it did.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经使用了无监督学习算法来对数据进行聚类。现在您想知道它的表现如何。
- en: Solution
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use *silhouette coefficients* to measure the quality of the clusters (note
    that this does not measure predictive performance):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *轮廓系数* 来衡量聚类的质量（请注意，这不是衡量预测性能的指标）：
- en: '[PRE47]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Discussion
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Supervised model evaluation* compares predictions (e.g., classes or quantitative
    values) with the corresponding true values in the target vector. However, the
    most common motivation for using clustering methods is that your data doesn’t
    have a target vector. A number of clustering evaluation metrics require a target
    vector, but again, using unsupervised learning approaches like clustering when
    you have a target vector available to you is probably handicapping yourself unnecessarily.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*监督模型评估*比较预测（例如类别或定量值）与目标向量中对应的真实值。然而，使用聚类方法的最常见动机是你的数据没有目标向量。许多聚类评估指标需要一个目标向量，但是当你有一个可用的目标向量时，再次使用聚类这样的无监督学习方法可能会不必要地束手无策。'
- en: 'While we cannot evaluate predictions versus true values if we don’t have a
    target vector, we can evaluate the nature of the clusters themselves. Intuitively,
    we can imagine “good” clusters having very small distances between observations
    in the same cluster (i.e., dense clusters) and large distances between the different
    clusters (i.e., well-separated clusters). Silhouette coefficients provide a single
    value measuring both traits. Formally, the *i*th observation’s silhouette coefficient
    is:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有目标向量，我们无法评估预测与真实值之间的情况，但是我们可以评估簇本身的特性。直观地，我们可以想象“好”的簇在同一簇内的观察之间有非常小的距离（即密集的簇），而在不同簇之间有很大的距离（即分离良好的簇）。轮廓系数提供了一个单一值，同时衡量了这两个特性。形式上，第*i*个观察的轮廓系数为：
- en: <math display="block"><mrow><msub><mi>s</mi> <mi>i</mi></msub> <mo>=</mo> <mfrac><mrow><msub><mi>b</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>a</mi> <mi>i</mi></msub></mrow> <mrow><mtext>max</mtext><mo>(</mo><msub><mi>a</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>b</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>s</mi> <mi>i</mi></msub> <mo>=</mo> <mfrac><mrow><msub><mi>b</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>a</mi> <mi>i</mi></msub></mrow> <mrow><mtext>max</mtext><mo>(</mo><msub><mi>a</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>b</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mfrac></mrow></math>
- en: where <math display="inline"><msub><mi>s</mi><mi>i</mi></msub></math> is the
    silhouette coefficient for observation <math display="inline"><mi>i</mi></math>,
    <math display="inline"><msub><mi>a</mi><mi>i</mi></msub></math> is the mean distance
    between <math display="inline"><mi>i</mi></math> and all observations of the same
    class, and <math display="inline"><msub><mi>b</mi><mi>i</mi></msub></math> is
    the mean distance between <math display="inline"><mi>i</mi></math> and all observations
    from the closest cluster of a different class. The value returned by `silhouette_score`
    is the mean silhouette coefficient for all observations. Silhouette coefficients
    range between –1 and 1, with 1 indicating dense, well-separated clusters.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><msub><mi>s</mi><mi>i</mi></msub></math>是观察的轮廓系数，<math
    display="inline"><msub><mi>a</mi><mi>i</mi></msub></math>是<math display="inline"><mi>i</mi></math>与同一类别所有观察之间的平均距离，<math
    display="inline"><msub><mi>b</mi><mi>i</mi></msub></math>是<math display="inline"><mi>i</mi></math>与不同类别最接近的簇中所有观察之间的平均距离。`silhouette_score`返回的值是所有观察的平均轮廓系数。轮廓系数的范围在-1到1之间，1表示密集且分离良好的簇。
- en: See Also
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation: silhouette_score](https://oreil.ly/gGjQj)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn文档：silhouette_score](https://oreil.ly/gGjQj)'
- en: 11.10 Creating a Custom Evaluation Metric
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.10 创建自定义评估度量
- en: Problem
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to evaluate a model using a metric you created.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望使用您创建的度量来评估一个模型。
- en: Solution
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Create the metric as a function and convert it into a scorer function using
    scikit-learn’s `make_scorer`:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 创建度量作为一个函数，并使用scikit-learn的`make_scorer`将其转换为评分器函数：
- en: '[PRE49]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Discussion
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: While scikit-learn has a number of built-in metrics for evaluating model performance,
    it is often useful to define our own metrics. scikit-learn makes this easy using
    `make_scorer`. First, we define a function that takes in two arguments—​the ground
    truth target vector and our predicted values—​and outputs some score. Second,
    we use `make_scorer` to create a scorer object, making sure to specify whether
    higher or lower scores are desirable (using the `greater_is_better` parameter).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然scikit-learn有许多内置的度量指标来评估模型性能，但通常定义我们自己的度量也很有用。scikit-learn通过使用`make_scorer`使这变得简单。首先，我们定义一个接受两个参数（真实目标向量和我们的预测值）并输出某个分数的函数。其次，我们使用`make_scorer`创建一个评分器对象，确保指定高或低分数是可取的（使用`greater_is_better`参数）。
- en: 'The custom metric in the solution (`custom_metric`) is a toy example since
    it simply wraps a built-in metric for calculating the *R²* score. In a real-world
    situation, we would replace the `custom_metric` function with whatever custom
    metric we wanted. However, we can see that the custom metric that calculates *R²*
    does work by comparing the results to scikit-learn’s `r2_score` built-in method:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决方案中，自定义度量(`custom_metric`)只是一个玩具示例，因为它简单地包装了一个用于计算*R²*分数的内置度量。在实际情况中，我们将用我们想要的任何自定义度量替换`custom_metric`函数。然而，我们可以通过将结果与scikit-learn的`r2_score`内置方法进行比较，看到计算*R²*的自定义度量确实有效：
- en: '[PRE51]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: See Also
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation: make_scorer](https://oreil.ly/-RqFY)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn文档：make_scorer](https://oreil.ly/-RqFY)'
- en: 11.11 Visualizing the Effect of Training Set Size
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.11 可视化训练集大小的效果
- en: Problem
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to evaluate the effect of the number of observations in your training
    set on some metric (accuracy, *F[1]*, etc.).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要评估训练集中观测数量对某些指标（准确率、*F[1]* 等）的影响。
- en: Solution
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Plot the accuracy against the training set size:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制准确性与训练集大小的图表：
- en: '[PRE53]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '![mpc2 11in03](assets/mpc2_11in03.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 11in03](assets/mpc2_11in03.png)'
- en: Discussion
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Learning curves* visualize the performance (e.g., accuracy, recall) of a model
    on the training set and during cross-validation as the number of observations
    in the training set increases. They are commonly used to determine if our learning
    algorithms would benefit from gathering additional training data.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习曲线* 可视化模型在训练集和交叉验证中随着训练集观测数量增加而表现的性能（例如准确率、召回率）。它们通常用于确定我们的学习算法是否会从收集额外的训练数据中受益。'
- en: In our solution, we plot the accuracy of a random forest classifier at 50 different
    training set sizes, ranging from 1% of observations to 100%. The increasing accuracy
    score of the cross-validated models tell us that we would likely benefit from
    additional observations (although in practice this might not be feasible).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，我们绘制了随机森林分类器在50个不同训练集大小上的准确性，范围从观测数据的1%到100%。交叉验证模型的逐渐增加的准确性得分告诉我们，我们可能会从额外的观测中受益（尽管在实践中这可能并不可行）。
- en: See Also
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation: Learning Curve](https://oreil.ly/jAKwy)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn 文档：学习曲线](https://oreil.ly/jAKwy)'
- en: 11.12 Creating a Text Report of Evaluation Metrics
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.12 创建评估指标的文本报告
- en: Problem
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want a quick description of a classifier’s performance.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要一个分类器性能的快速描述。
- en: Solution
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use scikit-learn’s `classification_report`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的 `classification_report`：
- en: '[PRE54]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Discussion
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '`classification_report` provides a quick means for us to see some common evaluation
    metrics, including precision, recall, and *F[1]* score (described in [Recipe 11.4](#evaluating-binary-classifier-predictions)).
    Support refers to the number of observations in each class.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`classification_report` 提供了一个快速查看一些常见评估指标（包括精确度、召回率和 *F[1]* 分数，详见 [Recipe 11.4](#evaluating-binary-classifier-predictions)）的方法。支持是每个类别中的观测数量。'
- en: See Also
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Precision and recall, Wikipedia](https://oreil.ly/9mBSF)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[精确度和召回率，维基百科](https://oreil.ly/9mBSF)'
- en: 11.13 Visualizing the Effect of Hyperparameter Values
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11.13 可视化超参数值效果
- en: Problem
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to understand how the performance of a model changes as the value of
    some hyperparameter changes.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要了解模型在某些超参数值变化时的性能变化。
- en: Solution
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Plot the hyperparameter against the model accuracy (validation curve):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制超参数与模型准确性的图表（验证曲线）：
- en: '[PRE56]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '![mpc2 11in04](assets/mpc2_11in04.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 11in04](assets/mpc2_11in04.png)'
- en: Discussion
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Most training algorithms (including many covered in this book) contain hyperparameters
    that must be chosen before the training process begins. For example, a *random
    forest classifier* creates a “forest” of decision trees, each of which votes on
    the predicted class of an observation. One hyperparameter in random forest classifiers
    is the number of trees in the forest. Most often hyperparameter values are selected
    during model selection (see [Chapter 12](ch12.xhtml#model-selection)). However,
    it is occasionally useful to visualize how model performance changes as the hyperparameter
    value changes. In our solution, we plot the changes in accuracy for a random forest
    classifier for the training set and during cross-validation as the number of trees
    increases. When we have a small number of trees, both the training and cross-validation
    score are low, suggesting the model is underfitted. As the number of trees increases
    to 250, the accuracy of both levels off, suggesting there is probably not much
    value in the computational cost of training a massive forest.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数训练算法（包括本书涵盖的许多算法）在开始训练过程之前必须选择的超参数。例如，*随机森林分类器* 创建一个“森林”由决策树组成，每棵树对观测的预测类进行投票。随机森林分类器的一个超参数是森林中的树的数量。通常在模型选择过程中选择超参数值（参见
    [第12章](ch12.xhtml#model-selection)）。然而，偶尔可视化模型性能随着超参数值的变化而变化是有用的。在我们的解决方案中，我们绘制了随机森林分类器在训练集和交叉验证中随着树的数量增加而准确性的变化。当我们有少量树时，训练和交叉验证分数都很低，表明模型欠拟合。随着树的数量增加到250，两者的准确性趋于稳定，表明在训练大量森林的计算成本上可能没有太多价值。
- en: 'In scikit-learn, we can calculate the validation curve using `validation_curve`,
    which contains three important parameters:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，我们可以使用 `validation_curve` 计算验证曲线，其中包含三个重要参数：
- en: '`param_name`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`param_name`'
- en: Name of the hyperparameter to vary
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要变化的超参数名称
- en: '`param_range`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`param_range`'
- en: Value of the hyperparameter to use
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的超参数的值
- en: '`scoring`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`scoring`'
- en: Evaluation metric used to judge to model
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型的评估指标
- en: See Also
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation: Validation Curve](https://oreil.ly/FH_kH)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn 文档：验证曲线](https://oreil.ly/FH_kH)'
