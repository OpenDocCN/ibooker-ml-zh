- en: Chapter 4\. Model Training Patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 模型训练模式
- en: Machine learning models are usually trained iteratively, and this iterative
    process is informally called the *training loop*. In this chapter, we discuss
    what the typical training loop looks like, and catalog a number of situations
    in which you might want to do something different.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通常是通过迭代训练的，这个迭代过程通常被非正式地称为*训练循环*。在本章中，我们讨论了典型的训练循环是什么样子，并列举了一些可能需要采取不同方法的情况。
- en: Typical Training Loop
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 典型的训练循环
- en: Machine learning models can be trained using different types of optimization.
    Decision trees are often built node by node based on an information gain measure.
    In genetic algorithms, the model parameters are represented as genes, and the
    optimization method involves techniques that are based on evolutionary theory.
    However, the most common approach to determining the parameters of machine learning
    models is *gradient descent*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型可以使用不同类型的优化进行训练。决策树通常根据信息增益度量逐节点构建。在遗传算法中，模型参数被表示为基因，并且优化方法涉及基于进化理论的技术。然而，确定机器学习模型参数的最常见方法是*梯度下降*。
- en: Stochastic Gradient Descent
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: On large datasets, gradient descent is applied to mini-batches of the input
    data to train everything from linear models and boosted trees to deep neural networks
    (DNNs) and support vector machines (SVMs). This is called *stochastic gradient
    descent (SGD)*, and extensions of SGD (such as Adam and Adagrad) are the de facto
    optimizers used in modern-day machine learning frameworks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据集上，梯度下降被应用于输入数据的小批量中，用于训练从线性模型和增强树到深度神经网络（DNN）和支持向量机（SVM）等各种模型。这被称为*随机梯度下降（SGD）*，SGD的扩展（如Adam和Adagrad）是现代机器学习框架中使用的事实上的优化器。
- en: Because SGD requires training to take place iteratively on small batches of
    the training dataset, training a machine learning model happens in a loop. SGD
    finds a minimum, but is not a closed-form solution, and so we have to detect whether
    the model convergence has happened. Because of this, the error (called the *loss*)
    on the training dataset has to be monitored. Overfitting can happen if the model
    complexity is higher than can be afforded by the size and coverage of the dataset.
    Unfortunately, you cannot know whether the model complexity is too high for a
    particular dataset until you actually train that model on that dataset. Therefore,
    evaluation needs to be done within the training loop, and *error metrics* on a
    withheld split of the training data, called the *validation dataset*, have to
    be monitored as well. Because the training and validation datasets have been used
    in the training loop, it is necessary to withhold yet another split of the training
    dataset, called the *testing dataset*, to report the actual error metrics that
    could be expected on new and unseen data. This evaluation is carried out at the
    end.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因为随机梯度下降（SGD）要求在训练数据集的小批量上进行迭代训练，所以训练机器学习模型就是一个循环过程。SGD找到一个最小值，但它不是一个封闭形式的解，因此我们必须检测模型是否收敛。由于这个原因，在训练数据集上的误差（称为*损失*）必须被监控。如果模型复杂度高于数据集的大小和覆盖范围，就可能发生过拟合。不幸的是，在你真正在特定数据集上训练该模型之前，你无法知道模型复杂度是否过高。因此，评估必须在训练循环内完成，并且还必须监控*验证数据集*（训练数据保留的一部分）上的*错误度量*。因为训练和验证数据集已经在训练循环中使用过，所以有必要保留另一个被称为*测试数据集*的训练数据拆分，以报告在新的未见数据上可能预期的实际错误度量。这种评估是在最后完成的。
- en: Keras Training Loop
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras 训练循环
- en: 'The typical training loop in Keras looks like this:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中典型的训练循环如下所示：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the model uses the Adam optimizer to carry out SGD on the cross entropy
    over the training dataset and reports out the final accuracy obtained on the testing
    dataset. The model fitting loops over the training dataset three times (each traversal
    over the training dataset is termed an *epoch*) with the model seeing batches
    consisting of 64 training examples at a time. At the end of every epoch, the error
    metrics are calculated on the validation dataset and added to the history. At
    the end of the fitting loop, the model is evaluated on the testing dataset, saved,
    and potentially deployed for serving, as shown in [Figure 4-1](#a_typical_training_loop_consisting_of_t).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，模型使用Adam优化器在训练数据集上执行交叉熵的SGD，并报告在测试数据集上获得的最终准确度。模型的拟合循环在训练数据集上进行三次（每次遍历训练数据集称为*epoch*），每次模型看到64个训练样本组成的批次。在每个epoch结束时，会在验证数据集上计算错误度量，并将其添加到历史记录中。在拟合循环结束时，模型会在测试数据集上进行评估、保存，并可能部署为服务，如[图 4-1](#a_typical_training_loop_consisting_of_t)所示。
- en: '![A typical training loop consisting of three epochs. Each epoch is processed
    in chunks of batch_size examples. At the end of the third epoch, the model is
    evaluated on the testing dataset and saved for potential deployment as a web service.](Images/mldp_0401.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![典型训练循环由三个epoch组成。每个epoch按批量大小的示例进行处理。在第三个epoch结束时，模型在测试数据集上进行评估，并保存以便可能部署为Web服务。](Images/mldp_0401.png)'
- en: Figure 4-1\. A typical training loop consisting of three epochs. Each epoch
    is processed in chunks of batch_size examples. At the end of the third epoch,
    the model is evaluated on the testing dataset, and saved for potential deployment
    as a web service.
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 典型训练循环由三个epoch组成。每个epoch按批量大小的示例进行处理。在第三个epoch结束时，模型在测试数据集上进行评估，并保存以便可能部署为Web服务。
- en: Instead of using the prebuilt `fit()` function, we could also write a custom
    training loop that iterates over the batches explicitly, but we will not need
    to do this for any of the design patterns discussed in this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 而非使用预构建的`fit()`函数，我们也可以编写自定义训练循环，明确迭代批次，但对于本章讨论的任何设计模式，我们不需要这样做。
- en: Training Design Patterns
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练设计模式
- en: The design patterns covered in this chapter all have to do with modifying the
    typical training loop in some way. In *Useful Overfitting*, we forgo the use of
    a validation or testing dataset because we want to intentionally overfit on the
    training dataset. In *Checkpoints*, we store the full state of the model periodically,
    so that we have access to partially trained models. When we use checkpoints, we
    usually also use *virtual epochs*, wherein we decide to carry out the inner loop
    of the `fit()` function, not on the full training dataset but on a fixed number
    of training examples. In *Transfer Learning*, we take part of a previously trained
    model, freeze the weights, and incorporate these nontrainable layers into a new
    model that solves the same problem, but on a smaller dataset. In *Distribution
    Strategy,* the training loop is carried out at scale over multiple workers, often
    with caching, hardware acceleration, and parallelization. Finally, in *Hyperparameter
    Tuning*, the training loop is itself inserted into an optimization method to find
    the optimal set of model hyperparameters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的设计模式都涉及以某种方式修改典型的训练循环。在*有用的过拟合*中，我们放弃使用验证或测试数据集，因为我们希望故意在训练数据集上过拟合。在*检查点*中，我们定期存储模型的完整状态，以便可以访问部分训练的模型。当我们使用检查点时，通常也使用*虚拟epoch*，在这种情况下，我们决定在`fit()`函数的内部循环中不使用完整的训练数据集，而是使用固定数量的训练样本。在*迁移学习*中，我们使用先前训练模型的部分，冻结权重，并将这些不可训练层合并到解决相同问题的新模型中，但在较小的数据集上进行。在*分布策略*中，训练循环在多个工作节点上以规模进行，通常包括缓存、硬件加速和并行化。最后，在*超参数调整*中，训练循环本身被插入到优化方法中，以找到最佳的模型超参数集。
- en: 'Design Pattern 11: Useful Overfitting'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 11：有用的过拟合
- en: Useful Overfitting is a design pattern where we forgo the use of generalization
    mechanisms because we want to intentionally overfit on the training dataset. In
    situations where overfitting can be beneficial, this design pattern recommends
    that we carry out machine learning without regularization, dropout, or a validation
    dataset for early stopping.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有用的过拟合是一种设计模式，其中我们放弃使用泛化机制，因为我们希望故意在训练数据集上过拟合。在过拟合可能有益的情况下，该设计模式建议我们进行机器学习时不使用正则化、dropout或早期停止的验证数据集。
- en: Problem
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: The goal of a machine learning model is to generalize and make reliable predictions
    on new, unseen data. If your model *overfits* the training data (for example,
    it continues to decrease the training error beyond the point at which validation
    error starts to increase), then its ability to generalize suffers and so do your
    future predictions. Introductory machine learning textbooks advise avoiding overfitting
    by using early stopping and regularization techniques.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的目标是在新的未见数据上进行泛化并进行可靠的预测。如果你的模型*过拟合*训练数据（例如，它在验证误差开始增加的点之后继续减少训练误差），那么它的泛化能力将会受到影响，进而影响到未来的预测。介绍性的机器学习教材建议通过早停和正则化技术来避免过拟合。
- en: Consider, however, a situation of simulating the behavior of physical or dynamical
    systems like those found in climate science, computational biology, or computational
    finance. In such systems, the time dependence of observations can be described
    by a mathematical function or set of partial differential equations (PDEs). Although
    the equations that govern many of these systems can be formally expressed, they
    don’t have a closed-form solution. Instead, classical numerical methods have been
    developed to approximate solutions to these systems. Unfortunately, for many real-world
    applications, these methods can be too slow to be used in practice.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑一种模拟物理或动力系统行为的情况，例如气候科学、计算生物学或计算金融中发现的系统。在这些系统中，观察的时间依赖性可以用数学函数或偏微分方程组描述。尽管控制这些系统的方程可以被形式化地表达，但它们没有封闭形式的解。相反，已经开发了经典的数值方法来近似解这些系统。不幸的是，对于许多现实世界的应用，这些方法可能太慢而无法实际使用。
- en: Consider the situation shown in [Figure 4-2](#one_situation_when_it_is_acceptable_to).
    Observations collected from the physical environment are used as inputs (or initial
    starting conditions) for a physics-based model that carries out iterative, numerical
    calculations to calculate the precise state of the system. Suppose all the observations
    have a finite number of possibilities (for example, temperature will be between
    60°C and 80°C in increments of 0.01°C). It is then possible to create a training
    dataset for the machine learning system consisting of the complete input space
    and calculate the labels using the physical model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑如 [图 4-2](#one_situation_when_it_is_acceptable_to) 所示的情况。从物理环境中收集的观察结果被用作物理模型的输入（或初始条件），该模型进行迭代、数值计算以计算系统的精确状态。假设所有观察结果都有有限的可能性（例如，温度将在60°C到80°C之间，增量为0.01°C）。然后可以为机器学习系统创建包含完整输入空间的训练数据集，并使用物理模型计算标签。
- en: '![One situation when it is acceptable to overfit is when the entire domain
    space of observations can be tabulated and a physical model capable of computing
    the precise solution is available.](Images/mldp_0402.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![当整个观察域空间可以列成表格并且具有计算精确解的物理模型可用时，过拟合是可以接受的一种情况。](Images/mldp_0402.png)'
- en: Figure 4-2\. One situation when it is acceptable to overfit is when the entire
    domain space of observations can be tabulated and a physical model capable of
    computing the precise solution is available.
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 当整个观察域空间可以列成表格并且具有计算精确解的物理模型可用时，过拟合是可以接受的一种情况。
- en: The ML model needs to learn this precisely calculated and nonoverlapping lookup
    table of inputs to outputs. Splitting such a dataset into a training dataset and
    an evaluation dataset is counterproductive because we would then be expecting
    the model to learn parts of the input space it will not have seen in the training
    dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型需要学习这个精确计算且不重叠的输入输出查找表。将这样的数据集分为训练数据集和评估数据集是适得其反的，因为我们会期望模型学习训练数据集中未见过的输入空间的部分。
- en: Solution
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: In this scenario, there is no “unseen” data that needs to be generalized to,
    since all possible inputs have been tabulated. When building a machine learning
    model to learn such a physics model or dynamical system, there is no such thing
    as overfitting. The basic machine learning training paradigm is slightly different.
    Here, there is some physical phenomenon that you are trying to learn that is governed
    by an underlying PDE or system of PDEs. Machine learning merely provides a data-driven
    approach to approximate the precise solution, and concepts like overfitting must
    be reevaluated.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，没有需要泛化到的“未见”数据，因为所有可能的输入已经被列入表格。当构建一个机器学习模型来学习这样一个物理模型或动态系统时，不存在过拟合的问题。基本的机器学习训练范式略有不同。在这里，您正在尝试学习由基础PDE或PDE系统控制的某些物理现象。机器学习仅提供了一种数据驱动的方法来近似精确解，因此诸如过拟合的概念必须重新评估。
- en: For example, a ray-tracing approach is used to simulate the satellite imagery
    that would result from the output of numerical weather prediction models. This
    involves calculating how much of a solar ray gets absorbed by the predicted hydrometeors
    (rain, snow, hail, ice pellets, and so on) at each atmospheric level. There is
    a finite number of possible hydrometeor types and a finite number of heights that
    the numerical model predicts. So the ray-tracing model has to apply optical equations
    to a large but finite set of inputs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，采用射线追踪方法模拟卫星图像，该图像是由数值天气预报模型的输出产生的。这涉及计算预测的水凝物（雨、雪、冰雹、冰粒等）在每个大气层中吸收多少太阳射线。可能的水凝物类型和数值模型预测的高度是有限的。因此，射线追踪模型必须将光学方程应用于一个大但有限的输入集合。
- en: The equations of radiative transfer govern the complex dynamical system of how
    electromagnetic radiation propagates in the atmosphere, and forward radiative
    transfer models are an effective means of inferring the future state of satellite
    images. However, classical numerical methods to compute the solutions to these
    equations can take tremendous computational effort and are too slow to use in
    practice.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 辐射传输方程控制电磁辐射在大气中传播的复杂动态系统，并且前向辐射传输模型是推断卫星图像未来状态的有效手段。然而，计算这些方程的解的传统数值方法需要巨大的计算工作量，而且在实践中速度太慢。
- en: Enter machine learning. It is possible to use machine learning to build a model
    that [approximates solutions](https://oreil.ly/IkYKm) to the forward radiative
    transfer model (see [Figure 4-3](#architecture_for_using_a_neural_network)). This
    ML approximation can be made close enough to the solution of the model that was
    originally achieved by using more classical methods. The advantage is that inference
    using the learned ML approximation (which needs to just calculate a closed formula)
    takes only a fraction of the time required to carry out ray tracing (which would
    require numerical methods). At the same time, the training dataset is too large
    (multiple terabytes) and too unwieldy to use as a lookup table in production.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 进入机器学习。可以使用机器学习构建一个模型，该模型[近似解决方案](https://oreil.ly/IkYKm)到前向辐射传输模型（见[图 4-3](#architecture_for_using_a_neural_network)）。这种机器学习的近似解可以接近最初通过更传统方法获得的模型解。其优势在于，使用学习的机器学习近似解进行推理（只需计算一个封闭公式）所需的时间仅为进行光线追踪所需时间的一小部分（后者需要数值方法）。同时，训练数据集过大（多达数太字节），在生产环境中使用作为查找表格是不可行的。
- en: '![Architecture for using a neural network to model the solution of a partial
    differential equation to solve for I(r,t,n).](Images/mldp_0403.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![使用神经网络建模部分微分方程解决I(r,t,n)的架构。](Images/mldp_0403.png)'
- en: Figure 4-3\. Architecture for using a neural network to model the solution of
    a partial differential equation to solve for I(r,t,n).
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 使用神经网络建模部分微分方程解决I(r,t,n)的架构。
- en: There is an important difference between training an ML model to approximate
    the solution to a dynamical system like this and training an ML model to predict
    baby weight based on natality data collected over the years. Namely, the dynamical
    system is a set of equations governed by the laws of electromagnetic radiation—there
    is no unobserved variable, no noise, and no statistical variability. For a given
    set of inputs, there is only one precisely calculable output. There is no overlap
    between different examples in the training dataset. For this reason, we can toss
    out concerns about generalization. We *want* our ML model to fit the training
    data as perfectly as possible, to “overfit.”
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 训练机器学习模型以近似解决像这样的动态系统与根据多年来收集的新生数据预测婴儿体重之间存在重要区别。换句话说，动态系统是由电磁辐射法则控制的一组方程，没有未观察到的变量，没有噪声，也没有统计变异。对于给定的输入集，只有一个可以精确计算的输出。在训练数据集中，不同示例之间没有重叠。因此，我们可以不必担心泛化问题。我们*希望*我们的机器学习模型尽可能完美地拟合训练数据，以“过度拟合”。
- en: This is counter to the typical approach of training an ML model where considerations
    of bias, variance, and generalization error play an important role. Traditional
    training says that it is possible for a model to learn the training data “too
    well,” and that training your model so that the train loss function is equal to
    zero is more of a red flag than cause for celebration. Overfitting of the training
    dataset in this way causes the model to give misguided predictions on new, unseen
    data points. The difference here is that we know in advance there won’t be unseen
    data, thus the model is approximating a solution to a PDE over the full input
    spectrum. If your neural network is able to learn a set of parameters where the
    loss function is zero, then that parameter set determines the actual solution
    of the PDE in question.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这与训练机器学习模型的典型方法相反，后者考虑偏差、方差和泛化误差的因素。传统的训练说，模型可能学习训练数据“太好”，使得训练损失函数等于零更像是一个警告信号，而不是庆祝的原因。以这种方式过度拟合训练数据集会导致模型在新的、未见的数据点上做出误导性预测。这里的不同之处在于，我们事先知道不会有未见数据，因此模型是在整个输入频谱上逼近偏微分方程的解。如果你的神经网络能够学习一组参数，使得损失函数为零，那么这个参数集确定了所讨论的偏微分方程的实际解。
- en: Why It Works
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么它有效
- en: If all possible inputs can be tabulated, then as shown by the dotted curve in
    [Figure 4-4](#overfitting_is_not_a_concern_if_all_pos), an overfit model will
    still make the same predictions as the “true” model if all possible input points
    are trained for. So overfitting is not a concern. We have to take care that inferences
    are made on rounded-off values of the inputs, with the rounding determined by
    the resolution with which the input space was gridded.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有可能的输入都能被列出，那么如图[4-4](#overfitting_is_not_a_concern_if_all_pos)中所示的虚线曲线，即使是过拟合模型，也会产生与“真实”模型相同的预测结果，如果所有可能的输入点都经过训练，过拟合就不是问题。我们必须确保推断是基于输入值的四舍五入值进行的，而这个四舍五入是由输入空间的网格分辨率确定的。
- en: '![Overfitting is not a concern if all possible input points are trained for
    because predictions are the same with both curves.](Images/mldp_0404.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![如果所有可能的输入点都经过训练，过拟合就不是问题，因为预测结果与两个曲线相同。](Images/mldp_0404.png)'
- en: Figure 4-4\. Overfitting is not a concern if all possible input points are trained
    for because predictions are the same with both curves.
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4\. 如果所有可能的输入点都经过训练，过拟合就不是问题，因为预测结果与两个曲线都相同。
- en: Is it possible to find a model function that gets arbitrarily close to the true
    labels? One bit of intuition as to why this works comes from the Uniform Approximation
    Theorem of deep learning, which, loosely put, states that any function (and its
    derivatives) can be approximated by a neural network with at least one hidden
    layer and any “squashing” activation function, like sigmoid. This means that no
    matter what function we are given, so long as it’s relatively well behaved, there
    exists a neural network with just one hidden layer that approximates that function
    as closely as we want.^([1](ch04.xhtml#ch01fn16))
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有可能找到一个模型函数，它可以无限接近真实标签？对于为什么这能行的直觉，部分来源于深度学习的均匀逼近定理，它粗略地表明，任何函数（及其导数）都可以用至少一个隐藏层和任何“压缩”激活函数（如
    sigmoid）的神经网络来逼近。这意味着，无论我们得到什么函数，只要它相对合理，就存在一个仅具有一个隐藏层的神经网络，可以尽我们所需地逼近那个函数。^([1](ch04.xhtml#ch01fn16))
- en: Deep learning approaches to solving differential equations or complex dynamical
    systems aim to represent a function defined implicitly by a differential equation,
    or system of equations, using a neural network.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法用于解决微分方程或复杂动态系统的方法，旨在使用神经网络表示由微分方程或方程组隐含定义的函数。
- en: 'Overfitting is useful when the following two conditions are met:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当满足以下两个条件时，过拟合是有用的：
- en: There is no noise, so the labels are accurate for all instances.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有噪音，因此所有实例的标签都是准确的。
- en: You have the complete dataset at your disposal (you have all the examples there
    are). In this case, overfitting becomes interpolating the dataset.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以随时使用完整的数据集（所有示例都在那里）。在这种情况下，过拟合变成了对数据集进行插值。
- en: Trade-Offs and Alternatives
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷与替代方案
- en: We introduced overfitting as being useful when the set of inputs can be exhaustively
    listed and the accurate label for each set of inputs can be calculated. If the
    full input space can be tabulated, overfitting is not a concern because there
    is no unseen data. However, the Useful Overfitting design pattern is useful beyond
    this narrow use case. In many real-world situations, even if one or more of these
    conditions has to be relaxed, the concept that overfitting can be useful remains
    valid.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入集可以详尽列出并可以计算每个输入集的准确标签时，我们将过拟合作为有用的。如果可以列出完整的输入空间，则过拟合不是一个问题，因为没有看不见的数据。然而，有用的过拟合设计模式在超出这个狭窄用例之外仍然有效。
- en: Interpolation and chaos theory
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 插值与混沌理论
- en: The machine learning model essentially functions as an approximation to a lookup
    table of inputs to outputs. If the lookup table is small, just use it as a lookup
    table! There is no need to approximate it by a machine learning model. An ML approximation
    is useful in situations where the lookup table will be too large to effectively
    use. It is when the lookup table is too unwieldy that it becomes better to treat
    it as the training dataset for a machine learning model that approximates the
    lookup table.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型本质上充当输入到输出查找表的近似。如果查找表很小，只需将其用作查找表即可！没有必要通过机器学习模型来近似它。在查找表过于庞大以致无法有效使用时，ML近似才是有用的。当查找表过于笨重时，最好将其视为机器学习模型的训练数据集，以近似查找表。
- en: Note that we assumed that the observations would have a finite number of possibilities.
    For example, we posited that temperature would be measured in 0.01°C increments
    and lie between 60°C and 80°C. This will be the case if the observations are made
    by digital instruments. If this is not the case, the ML model is needed to interpolate
    between entries in the lookup table.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们假设观察结果将有限数量的可能性。例如，我们假设温度将以0.01°C的增量进行测量，并且位于60°C到80°C之间。如果通过数字仪器进行观察，则情况将如此。如果不是这种情况，则需要ML模型在查找表中的条目之间进行插值。
- en: Machine learning models interpolate by weighting unseen values by the distance
    of these unseen values from training examples. Such interpolation works only if
    the underlying system is not chaotic. In chaotic systems, even if the system is
    deterministic, small differences in initial conditions can lead to dramatically
    different outcomes. Nevertheless, in practice, each specific chaotic phenomenon
    has a specific [resolution threshold](https://oreil.ly/F-drU) beyond which it
    is possible for models to forecast it over short time periods. Therefore, provided
    the lookup table is fine-grained enough and the limits of resolvability are understood,
    useful approximations can result.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通过未见值与训练示例的距离加权来进行插值。只有在底层系统不混沌时，这种插值才有效。在混沌系统中，即使系统是确定性的，初始条件的微小差异也可能导致截然不同的结果。尽管如此，在实践中，每个具体的混沌现象都有一个特定的[分辨率阈值](https://oreil.ly/F-drU)，在这个阈值之上，模型可以在短时间内预测它。因此，只要查找表足够精细，并且了解到可解析性的极限，就可以得到有用的近似。
- en: Monte Carlo methods
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 蒙特卡罗方法
- en: In reality, tabulating all possible inputs might not be possible, and you might
    take a [Monte Carlo approach](https://oreil.ly/pTgS9) of sampling the input space
    to create the set of inputs, especially where not all possible combinations of
    inputs are physically possible.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，可能不可能列出所有可能的输入，并且您可能采取蒙特卡罗方法来对输入空间进行采样以创建输入集，特别是在所有可能的输入组合在物理上不可能的情况下。
- en: In such cases, overfitting is technically possible (see [Figure 4-5](#if_the_input_space_is_sampledcomma_not),
    where the unfilled circles are approximated by wrong estimates shown by crossed
    circles).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，过拟合是技术上可能的（参见 [图 4-5](#if_the_input_space_is_sampledcomma_not)，其中未填充的圆圈被错误的估计用交叉圆圈表示）。
- en: '![If the input space is sampled, not tabulated, then you need to take care
    to limit model complexity.](Images/mldp_0405.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![如果输入空间是采样的，而不是表格化的，则需要注意限制模型的复杂性。](Images/mldp_0405.png)'
- en: Figure 4-5\. If the input space is sampled, not tabulated, then you need to
    take care to limit model complexity.
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 如果输入空间是采样的，而不是表格化的，则需要注意限制模型的复杂性。
- en: However, even here, you can see that the ML model will be interpolating between
    known answers. The calculation is always deterministic, and it is only the input
    points that are subject to random selection. Therefore, these known answers do
    not contain noise, and because there are no unobserved variables, errors at unsampled
    points will be strictly bounded by the model complexity. Here, the overfitting
    danger comes from model complexity and not from fitting to noise. Overfitting
    is not as much of a concern when the size of the dataset is larger than the number
    of free parameters. Therefore, using a combination of low-complexity models and
    mild regularization provides a practical way to avoid unacceptable overfitting
    in the case of Monte Carlo selection of the input space.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使在这里，您也可以看到机器学习模型将在已知答案之间进行插值计算。计算始终是确定性的，只有输入点受到随机选择的影响。因此，这些已知答案不包含噪声，并且由于没有未观察到的变量，未采样点处的错误将严格受到模型复杂性的限制。在这里，过拟合的危险来自模型复杂性，而不是适应噪声。当数据集的大小大于自由参数的数量时，过拟合问题就不会那么严重。因此，在蒙特卡罗选择输入空间的情况下，使用低复杂度模型和轻度正则化的组合提供了一种实用的方式来避免不可接受的过拟合。
- en: Data-driven discretizations
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据驱动的离散化
- en: Although deriving a closed-form solution is possible for some PDEs, determining
    solutions using numerical methods is more common. Numerical methods of PDEs are
    already a deep field of research, and there are many [books](https://oreil.ly/RJWVQ),
    [courses](https://oreil.ly/wcl_n), and [journals](https://msp.org/apde) devoted
    to the subject. One common approach is to use finite difference methods, similar
    to Runge-Kutta methods, for solving ordinary differential equations. This is typically
    done by discretizing the differential operator of the PDE and finding a solution
    to the discrete problem on a spatio-temporal grid of the original domain. However,
    when the dimension of the problem becomes large, this mesh-based approach fails
    dramatically due to the curse of dimensionality because the mesh spacing of the
    grid must be [small enough](https://oreil.ly/TxHD-) to capture the smallest feature
    size of the solution. So, to achieve 10× higher resolution of an image requires
    10,000× more compute power, because the mesh grid must be scaled in four dimensions
    accounting for space and time.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对一些偏微分方程可以推导出闭合形式的解，但使用数值方法确定解更为普遍。偏微分方程的数值方法已经是一个深入研究的领域，有许多与该主题相关的 [书籍](https://oreil.ly/RJWVQ)，[课程](https://oreil.ly/wcl_n)，和
    [期刊](https://msp.org/apde)。一个常见的方法是使用有限差分方法，类似于龙格-库塔方法，用于求解常微分方程。这通常通过将偏微分方程的微分算子离散化，并在原始域的时空网格上找到离散问题的解来完成。然而，当问题的维数变大时，由于维度诅咒的存在，这种基于网格的方法会因为网格间距必须足够小以捕捉解的最小特征尺寸而遭遇严重失败。因此，要达到图像10倍更高分辨率需要10,000倍的计算能力，因为网格的间距必须在四个维度上进行缩放，考虑到空间和时间。
- en: 'However, it is possible to use machine learning (rather than Monte Carlo methods)
    to select the sampling points to create data-driven discretizations of PDEs. In
    the paper "[Learning data-driven discretizations for PDEs](https://oreil.ly/djDkK),”
    Bar-Sinai et al. demonstrate the effectiveness of this approach. The authors use
    a low-resolution grid of fixed points to approximate a solution via a piecewise
    polynomial interpolation using standard finite-difference methods as well as one
    obtained from a neural network. The solution obtained from the neural network
    vastly outperforms the numeric simulation in minimizing the absolute error, in
    some places achieving a 10² order of magnitude improvement. While increasing the
    resolution requires substantially more compute power using finite-difference methods,
    the neural network is able to maintain high performance with only marginal additional
    cost. Techniques like the Deep Galerkin Method can then use deep learning to provide
    a mesh-free approximation of the solution to the given PDE. In this way, solving
    the PDE is reduced to a chained optimization problem (see [“Design Pattern 8:
    Cascade ”](ch03.xhtml#design_pattern_eight_cascade)).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可以使用机器学习（而不是蒙特卡罗方法）来选择采样点，以创建用于PDE离散化的数据驱动方法。在论文"[学习PDE的数据驱动离散化](https://oreil.ly/djDkK)"中，Bar-Sinai等人展示了这种方法的有效性。作者们使用低分辨率的固定点网格，通过标准有限差分方法以及从神经网络获取的方法，来近似解决方案的分段多项式插值。从神经网络获得的解决方案在最小化绝对误差方面远远优于数值模拟，在某些地方实现了10²数量级的改进。虽然增加分辨率需要使用更多的计算能力来使用有限差分方法，但神经网络能够在仅有边际额外成本的情况下保持高性能。像深度Galerkin方法这样的技术可以使用深度学习来提供给定PDE解的无网格近似。通过这种方式，解决PDE问题被简化为链式优化问题（参见[“设计模式8：级联”](ch03.xhtml#design_pattern_eight_cascade)）。
- en: Unbounded domains
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无界域
- en: The Monte Carlo and data-driven discretization methods both assume that sampling
    the entire input space, even if imperfectly, is possible. That’s why the ML model
    was treated as an interpolation between known points.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛方法和数据驱动离散化方法都假设即使不完美地对整个输入空间进行采样也是可能的。这就是为什么将机器学习模型视为已知点之间的插值的原因。
- en: Generalization and the concern of overfitting become difficult to ignore whenever
    we are unable to sample points in the full domain of the function—for example,
    for functions with unbounded domains or projections along a time axis into the
    future. In these settings, it is important to consider overfitting, underfitting,
    and generalization error. In fact, it’s been shown that although techniques like
    the Deep Galerkin Method do well on regions that are well sampled, a function
    that is learned this way does not generalize well on regions outside the domain
    that were not sampled in the training phase. This can be problematic for using
    ML to solve PDEs that are defined on unbounded domains, since it would be impossible
    to capture a representative sample for training.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们无法对函数的整个域进行采样时，例如对于具有无界域或将来时间轴上的投影的函数，泛化和过拟合的问题就变得难以忽视。在这些设置中，重要的是考虑过拟合、欠拟合和泛化误差。事实上，已经表明，尽管像深度Galerkin方法这样的技术在采样充分的区域表现良好，但以这种方式学习的函数在训练阶段未采样的域外区域上的泛化能力不佳。这对于使用ML解决定义在无界域上的PDE可能会有问题，因为在训练阶段无法捕获代表性样本。
- en: Distilling knowledge of neural network
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络知识的精炼
- en: Another situation where overfitting is warranted is in distilling, or transferring
    knowledge, from a large machine learning model into a smaller one. Knowledge distillation
    is useful when the learning capacity of the large model is not fully utilized.
    If that is the case, the computational complexity of the large model may not be
    necessary. However, it is also the case that training smaller models is harder.
    While the smaller model has enough capacity to represent the knowledge, it may
    not have enough capacity to learn the knowledge efficiently.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种情况是过拟合被证明是合理的，那就是在将知识从大型机器学习模型转移到较小模型中进行精炼或知识转移时。当大型模型的学习能力没有充分利用时，知识精炼就非常有用。如果情况如此，大型模型的计算复杂度可能是不必要的。然而，训练较小的模型也是困难的。虽然较小的模型具有足够的容量来表示知识，但可能没有足够的能力来有效地学习知识。
- en: The solution is to train the smaller model on a large amount of generated data
    that is labeled by the larger model. The smaller model learns the soft output
    of the larger model, instead of actual labels on real data. This is a simpler
    problem that can be learned by the smaller model. As with approximating a numerical
    function by a machine learning model, the aim is for the smaller model to faithfully
    represent the predictions of the larger machine learning model. This second training
    step can employ Useful Overfitting.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是在大量由较大模型标记的生成数据上训练较小模型。较小模型学习较大模型的软输出，而不是实际数据上的标签。这是一个较简单的问题，可以被较小模型学习。就像通过机器学习模型逼近数值函数一样，较小模型的目标是忠实地表示较大机器学习模型的预测。这第二个训练步骤可以利用有用的过拟合。
- en: Overfitting a batch
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对一个批次过拟合
- en: In practice, training neural networks requires a lot of experimentation, and
    a practitioner must make many choices, from the size and architecture of the network
    to the choice of the learning rate, weight initializations, or other hyperparameters.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，训练神经网络需要大量的实验，并且从网络的大小和架构到学习率、权重初始化或其他超参数的选择，从业者必须做出许多选择。
- en: Overfitting on a small batch is a [good sanity check](https://oreil.ly/AcLtu)
    both for the model code as well as the data input pipeline. Just because the model
    compiles and the code runs without errors doesn’t mean you’ve computed what you
    think you have or that the training objective is configured correctly. A complex
    enough model *should* be able to overfit on a small enough batch of data, assuming
    everything is set up correctly. So, if you’re not able to overfit a small batch
    with any model, it’s worth rechecking your model code, input pipeline, and loss
    function for any errors or simple bugs. Overfitting on a batch is a useful technique
    when training and troubleshooting neural networks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对小批量过拟合是一个[很好的健全性检查](https://oreil.ly/AcLtu)，无论是对模型代码还是数据输入管道。仅仅因为模型编译通过并且代码运行没有错误，并不意味着您计算了您认为的内容或者训练目标正确配置了。一个足够复杂的模型*应该*能够在足够小的批量数据上过拟合，假设一切设置正确。因此，如果您无法用任何模型在小批量上过拟合，那么值得重新检查您的模型代码、输入管道和损失函数是否存在错误或简单的错误。在训练和排除神经网络问题时，对批量过拟合是一个有用的技术。
- en: Tip
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Overfitting goes beyond just a batch. From a more holistic perspective, overfitting
    follows the general advice commonly given with regards to deep learning and regularization.
    The best fitting model is a [large model that has been properly regularized](https://oreil.ly/A7DFC).
    In short, if your deep neural network isn’t capable of overfitting your training
    dataset, you should be using a bigger one. Then, once you have a large model that
    overfits the training set, you can apply regularization to improve the validation
    accuracy, even though training accuracy may decrease.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合不仅仅限于一个批次。从更全面的角度来看，过拟合遵循通常给出的关于深度学习和正则化的一般建议。最佳拟合模型是一个[经过适当正则化的大模型](https://oreil.ly/A7DFC)。简而言之，如果您的深度神经网络无法过拟合您的训练数据集，那么您应该使用一个更大的模型。然后，一旦您有一个能够过拟合训练集的大模型，您可以应用正则化来提高验证精度，即使训练精度可能会降低。
- en: 'You can test your Keras model code in this way using the `tf.data.Dataset`
    you’ve written for your input pipeline. For example, if your training data input
    pipeline is called `trainds`, we’ll use `batch()` to pull a single batch of data.
    You can find the [full code for this example](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/04_hacking_training_loop/distribution_strategies.ipynb)
    in the repository accompanying this book:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用您为输入管道编写的`tf.data.Dataset`来测试您的Keras模型代码。例如，如果您的训练数据输入管道称为`trainds`，我们将使用`batch()`来拉取一个单一批次的数据。您可以在伴随本书的存储库中找到这个例子的[完整代码](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/04_hacking_training_loop/distribution_strategies.ipynb)：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, when training the model, instead of calling the full `trainds` dataset
    inside the `fit()` method, use the single batch that we created:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在训练模型时，而不是在`fit()`方法中调用完整的`trainds`数据集，使用我们创建的单个批次：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that we apply `repeat()` so that we won’t run out of data when training
    on that single batch. This ensures that we take the one batch over and over again
    while training. Everything else (the validation dataset, model code, engineered
    features, and so on) remains the same.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们应用`repeat()`以确保在训练时不会耗尽数据。这确保了我们在训练时一遍又一遍地使用同一批数据。其余内容（验证数据集、模型代码、工程特征等）保持不变。
- en: Tip
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Rather than choose an arbitrary sample of the training dataset, we recommend
    that you overfit on a small dataset, each of whose examples has been carefully
    verified to have correct labels. Design your neural network architecture such
    that it is able to learn this batch of data precisely and get to zero loss. Then
    take the same network and train it on the full training dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议不要选择训练数据集的任意样本，而是建议你在一个小数据集上过度拟合。这些样本都经过仔细验证，确保其标签是正确的。设计你的神经网络架构，使其能够精确地学习这批数据，并且达到零损失。然后，将同样的网络用于完整的训练数据集。
- en: 'Design Pattern 12: Checkpoints'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式12：检查点
- en: In Checkpoints, we store the full state of the model periodically so that we
    have partially trained models available. These partially trained models can serve
    as the final model (in the case of early stopping) or as the starting points for
    continued training (in the cases of machine failure and fine-tuning).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在   在检查点中，我们定期存储模型的完整状态，以便能够使用部分训练过的模型。这些部分训练过的模型可以作为最终模型（在提前停止的情况下），或者作为继续训练的起点（在机器故障和微调的情况下）。
- en: Problem
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: The more complex a model is (for example, the more layers and nodes a neural
    network has), the larger the dataset that is needed to train it effectively. This
    is because more complex models tend to have more tunable parameters. As model
    sizes increase, the time it takes to fit one batch of examples also increases.
    As the data size increases (and assuming batch sizes are fixed), the number of
    batches also increases. Therefore, in terms of computational complexity, this
    double whammy means that training will take a long time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 模型越复杂（例如，神经网络具有的层数和节点数越多），训练它有效所需的数据集就越大。这是因为更复杂的模型往往具有更多可调参数。随着模型大小的增加，每个批次的拟合时间也会增加。随着数据大小的增加（假设批次大小固定），批次数量也会增加。因此，在计算复杂性方面，这种双重打击意味着训练将需要很长时间。
- en: At the time of writing, training an English-to-German translation model on a
    state-of-the-art tensor processing unit (TPU) pod on a relatively small dataset
    [takes about two hours](https://oreil.ly/vDRve). On real datasets of the sort
    used to train smart devices, the training can take several days.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，在一流的张量处理单元（TPU）Pod上训练一款英译德模型，使用相对较小的数据集，[大约需要两个小时](https://oreil.ly/vDRve)。在用于训练智能设备的真实数据集上，训练可能需要几天时间。
- en: When we have training that takes this long, the chances of machine failure are
    uncomfortably high. If there is a problem, we’d like to be able to resume from
    an intermediate point, instead of from the very beginning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行如此长时间的训练时，机器故障的可能性非常高。如果出现问题，我们希望能够从一个中间点恢复，而不是从头开始。
- en: Solution
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: At the end of every epoch, we can save the model state. Then, if the training
    loop is interrupted for any reason, we can go back to the saved model state and
    restart. However, when doing this, we have to make sure to save the intermediate
    model state, not just the model. What does that mean?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时代结束时，我们可以保存模型状态。然后，如果由于任何原因训练循环中断，我们可以回到保存的模型状态并重新启动。然而，在这样做时，我们必须确保保存的是中间模型状态，而不仅仅是模型本身。这意味着什么呢？
- en: Once training is complete, we save or *export* the model so that we can deploy
    it for inference. An exported model does not contain the entire model state, just
    the information necessary to create the prediction function. For a decision tree,
    for example, this would be the final rules for each intermediate node and the
    predicted value for each of the leaf nodes. For a linear model, this would be
    the final values of the weights and biases. For a fully connected neural network,
    we’d also need to add the activation functions and the weights of the hidden connections.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们保存或者*导出*模型，以便可以部署它进行推断。导出的模型不包含整个模型状态，只包含创建预测函数所需的信息。例如，对于决策树来说，这将是每个中间节点的最终规则以及每个叶子节点的预测值。对于线性模型来说，这将是权重和偏差的最终值。对于全连接神经网络，我们还需要添加激活函数和隐藏连接的权重。
- en: What data on model state do we need when restoring from a checkpoint that an
    exported model does not contain? An exported model does not contain which epoch
    and batch number the model is currently processing, which is obviously important
    in order to resume training. But there is more information that a model training
    loop can contain. In order to carry out gradient descent effectively, the optimizer
    might be changing the learning rate on a schedule. This learning rate state is
    not present in an exported model. Additionally, there might be stochastic behavior
    in the model, such as dropout. This is not captured in the exported model state
    either. Models like recurrent neural networks incorporate history of previous
    input values. In general, the full model state can be many times the size of the
    exported model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当从检查点恢复而导出的模型没有的模型状态数据是什么？一个导出的模型不包含模型当前处理的轮次和批次号，这显然是为了恢复训练而重要的。但模型训练循环可能包含更多信息。为了有效地执行梯度下降，优化器可能会按计划改变学习率。这种学习率状态在导出的模型中不存在。另外，模型可能会有随机行为，比如dropout。这也不在导出的模型状态中。像循环神经网络这样的模型会包含先前输入值的历史记录。总的来说，完整的模型状态可能是导出模型大小的多倍。
- en: Saving the full model state so that model training can resume from a point is
    called *checkpointing*, and the saved model files are called *checkpoints*. How
    often should we checkpoint? The model state changes after every batch because
    of gradient descent. So, technically, if we don’t want to lose any work, we should
    checkpoint after every batch. However, checkpoints are huge and this I/O would
    add considerable overhead. Instead, model frameworks typically provide the option
    to checkpoint at the end of every epoch. This is a reasonable tradeoff between
    never checkpointing and checkpointing after every batch.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 保存完整的模型状态以便从某一点恢复模型训练称为*检查点*，保存的模型文件称为*检查点*。我们应该多久做一次检查点？由于梯度下降的缘故，模型状态在每个批次后都会发生变化。因此，从技术上讲，如果我们不想丢失任何工作，我们应该在每个批次后做检查点。然而，检查点非常大，这种I/O会增加相当大的开销。相反，模型框架通常提供在每个时期结束时做检查点的选项。这是在从不做检查点和每批次后都做检查点之间的一个合理折衷。
- en: 'To checkpoint a model in Keras, provide a callback to the `fit()` method:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Keras中进行模型检查点，需要在`fit()`方法中提供一个回调函数：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With checkpointing added, the training looping becomes what is shown in [Figure 4-6](#checkpointing_saves_the_full_model_stat).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 添加检查点后，训练循环变得如[图 4-6](#checkpointing_saves_the_full_model_stat)所示。
- en: '![Checkpointing saves the full model state at the end of every epoch.](Images/mldp_0406.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![检查点在每个时期结束时保存完整的模型状态。](Images/mldp_0406.png)'
- en: Figure 4-6\. Checkpointing saves the full model state at the end of every epoch.
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6\. 检查点在每个时期结束时保存完整的模型状态。
- en: Why It Works
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么它有效
- en: TensorFlow and Keras automatically resume training from a checkpoint if checkpoints
    are found in the output path. To start training from scratch, therefore, you have
    to start from a new output directory (or delete previous checkpoints from the
    output directory). This works because enterprise-grade machine learning frameworks
    honor the presence of checkpoint files.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在输出路径中找到检查点文件，TensorFlow和Keras会自动从检查点恢复训练。因此，要从头开始训练，你必须从新的输出目录开始（或者从输出目录中删除之前的检查点）。这是因为企业级机器学习框架会尊重检查点文件的存在。
- en: Even though checkpoints are designed primarily to support resilience, the availability
    of partially trained models opens up a number of other use cases. This is because
    the partially trained models are usually more generalizable than the models created
    in later iterations. A good intuition of why this occurs can be obtained from
    the [TensorFlow playground](https://oreil.ly/sRjkN), as shown in [Figure 4-7](#starting_point_of_the_spiral_classifica).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管检查点主要设计用于支持韧性，但部分训练模型的可用性打开了许多其他用例。这是因为部分训练的模型通常比后续迭代中创建的模型更具普适性。为什么会发生这种情况的直观感受可以从[TensorFlow
    playground](https://oreil.ly/sRjkN)获得，如[图 4-7](#starting_point_of_the_spiral_classifica)所示。
- en: '![Starting point of the spiral classification problem. You can get to this
    setup by opening up this link in a web browser.](Images/mldp_0407.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![螺旋分类问题的起点。你可以通过在浏览器中打开这个链接来到达这个设置。](Images/mldp_0407.png)'
- en: Figure 4-7\. Starting point of the spiral classification problem. You can get
    to this setup by opening up [this link](https://oreil.ly/ISg9X) in a web browser.
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7\. 螺旋分类问题的起点。你可以通过在浏览器中打开[这个链接](https://oreil.ly/ISg9X)来到达这个设置。
- en: In the playground, we are trying to build a classifier to distinguish between
    blue dots and orange dots (if you are reading this in the print book, please do
    follow along by navigating to the link in a web browser). The two input features
    are x[1] and x[2], which are the coordinates of the points. Based on these features,
    the model needs to output the probability that the point is blue. The model starts
    with random weights and the background of the dots shows the model prediction
    for each coordinate point. As you can see, because the weights are random, the
    probability tends to hover near the center value for all the pixels.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，我们试图构建一个分类器来区分蓝点和橙点（如果你在纸质书中阅读此内容，请在网络浏览器中导航到链接以跟随）。两个输入特征是 x[1] 和 x[2]，它们是点的坐标。基于这些特征，模型需要输出点是蓝色的概率。模型从随机权重开始，点的背景显示了每个坐标点的模型预测。正如你所见，由于权重是随机的，概率倾向于在所有像素的中心值附近波动。
- en: Starting the training by clicking on the arrow at the top left of the image,
    we see the model slowly start to learn with successive epochs, as shown in [Figure 4-8](#what_the_model_learns_as_training_progr).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击图像左上角的箭头开始训练，我们可以看到模型随着连续的 epochs 缓慢开始学习，如 [图 4-8](#what_the_model_learns_as_training_progr)
    所示。
- en: '![What the model learns as training progresses. The graphs at the top are the
    training loss and validation error, while the images show how the model at that
    stage would predict the color of a point at each coordinate in the grid.](Images/mldp_0408.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![模型在训练过程中学到的内容。顶部的图表显示训练损失和验证错误，而图像则展示了模型在每个坐标网格中预测点的颜色。](Images/mldp_0408.png)'
- en: Figure 4-8\. What the model learns as training progresses. The graphs at the
    top are the training loss and validation error, while the images show how the
    model at that stage would predict the color of a point at each coordinate in the
    grid.
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 模型在训练过程中学到的内容。顶部的图表显示训练损失和验证错误，而图像则展示了模型在每个坐标网格中预测点的颜色。
- en: We see the first hint of learning in [Figure 4-8](#what_the_model_learns_as_training_progr)(b),
    and see that the model has learned the high-level view of the data by [Figure 4-8](#what_the_model_learns_as_training_progr)(c).
    From then on, the model is adjusting the boundaries to get more and more of the
    blue points into the center region while keeping the orange points out. This helps,
    but only up to point. By the time we get to [Figure 4-8](#what_the_model_learns_as_training_progr)(e),
    the adjustment of weights is starting to reflect random perturbations in the training
    data, and these are counterproductive on the validation dataset.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 4-8 中，我们看到学习的第一个迹象是在 (b) 处，我们看到模型通过图 4-8 (c) 学到了数据的高级视角。从那时起，模型调整边界以将更多的蓝点移到中心区域，同时保持橙点在外。这有助于提高效果，但只能到一定程度。当我们到达图 4-8
    (e) 时，权重的调整开始反映在训练数据中的随机扰动上，这对验证数据集是有害的。
- en: We can therefore break the training into three phases. In the first phase, between
    stages (a) and (c), the model is learning high-level organization of the data.
    In the second phase, between stages and (c) and (e), the model is learning the
    details. By the time we get to the third phase, stage (f), the model is overfitting.
    A partially trained model from the end of phase 1 or from phase 2 has some advantages
    precisely because it has learned the high-level organization but is not caught
    up in the details.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将训练分为三个阶段。在第一阶段（a 到 c 之间），模型学习数据的高级组织。在第二阶段（c 到 e 之间），模型学习细节。当我们到达第三阶段，即阶段
    f 时，模型出现了过拟合。从第一阶段或第二阶段结束时的部分训练模型中，我们可以看到它学到了高级组织，但并未深入到细节中。
- en: Trade-Offs and Alternatives
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折中和替代方案
- en: Besides providing resilience, saving intermediate checkpoints also allows us
    to implement early stopping and fine-tuning capabilities.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提供韧性之外，保存中间检查点还使我们能够实施提前停止和精细调整功能。
- en: Early stopping
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提前停止
- en: In general, the longer you train, the lower the loss on the training dataset.
    However, at some point, the error on the validation dataset might stop decreasing.
    If you are starting to overfit to the training dataset, the validation error might
    even start to increase, as shown in [Figure 4-9](#typicallycomma_the_training_loss_contin).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，训练时间越长，训练数据集上的损失就越低。然而，在某个时候，验证数据集上的错误可能停止减少。如果开始对训练数据集过拟合，验证误差甚至可能开始增加，如[图4-9](#typicallycomma_the_training_loss_contin)所示。
- en: '![Typically, the training loss continues to drop the longer you train, but
    once overfitting starts, the validation error on a withheld dataset starts to
    go up.](Images/mldp_0409.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![一般来说，训练时间越长，训练损失会持续下降，但一旦开始过拟合，验证数据集上的验证错误则会增加。](Images/mldp_0409.png)'
- en: Figure 4-9\. Typically, the training loss continues to drop the longer you train,
    but once overfitting starts, the validation error on a withheld dataset starts
    to go up.
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9。一般来说，训练时间越长，训练损失会持续下降，但一旦开始过拟合，验证数据集上的验证错误则会增加。
- en: In such cases, it can be helpful to look at the validation error at the end
    of every epoch and stop the training process when the validation error is more
    than that of the previous epoch. In [Figure 4-9](#typicallycomma_the_training_loss_contin),
    this will be at the end of the fourth epoch, shown by the thick dashed line. This
    is called *early stopping*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，查看每个时代结束时的验证错误并在验证错误超过上一个时代的情况下停止训练过程可能会有所帮助。在[图4-9](#typicallycomma_the_training_loss_contin)中，这将在第四个时代结束时，由粗虚线表示。这被称为*早停止*。
- en: Tip
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Had we been checkpointing at the end of every batch, we might have been able
    to capture the true minimum, which might have been a bit before or after the epoch
    boundary. See the discussion on virtual epochs in this section for a more frequent
    way to checkpoint.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在每个批次结束时做检查点，我们可能能够捕捉到真正的最小值，这可能在时代边界之前或之后一点点。有关虚拟时代的讨论，请参阅本节更频繁的检查点方法。
- en: If we are checkpointing much more frequently, it can be helpful if early stopping
    isn’t overly sensitive to small perturbations in the validation error. Instead,
    we can apply early stopping only after the validation error doesn’t improve for
    more than *N* checkpoints.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们更频繁地进行检查点，早停止不会对验证误差中的小扰动过于敏感将是有帮助的。相反，我们可以在验证误差连续*N*个检查点没有改进后再应用早停止。
- en: Checkpoint selection
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检查点选择
- en: While early stopping can be implemented by stopping the training as soon as
    the validation error starts to increase, we recommend training longer and choosing
    the optimal run as a postprocessing step. The reason we suggest training well
    into phase 3 (see the preceding “Why It Works” section for an explanation of the
    three phases of the training loop) is that it is not uncommon for the validation
    error to increase for a bit and then start to drop again. This is usually because
    the training initially focuses on more common scenarios (phase 1), then starts
    to home in on the rarer situations (phase 2). Because rare situations may be imperfectly
    sampled between the training and validation datasets, occasional increases in
    the validation error during the training run are to be expected in phase 2\. In
    addition, there are situations endemic to big models where [deep double descent](https://oreil.ly/Kya8h)
    is expected, and so it is essential to train a bit longer just in case.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以通过在验证错误开始增加时停止训练来实现早停止，我们建议延长训练时间，并在后处理步骤中选择最佳运行。我们建议训练直至第3阶段（请参阅前述的“为什么有效”部分，以了解训练循环的三个阶段的解释），因为验证误差在某些情况下可能会短暂增加，然后开始再次下降。这通常是因为训练最初关注更常见的情况（阶段1），然后开始追踪更罕见的情况（阶段2）。由于训练和验证数据集之间可能对稀有情况进行不完全采样，因此在第2阶段的训练运行期间偶尔会出现验证误差的增加是可以预期的。此外，对于大型模型普遍存在[深度双下降](https://oreil.ly/Kya8h)的情况，因此最好稍微延长训练时间以防万一。
- en: In our example, instead of exporting the model at the end of the training run,
    we will load up the fourth checkpoint and export our final model from there instead.
    This is called *checkpoint selection*, and in TensorFlow, it can be achieved using
    [BestExporter](https://oreil.ly/UpN1a).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将不会在训练运行结束时导出模型，而是将加载第四个检查点并从那里导出我们的最终模型。这被称为*检查点选择*，在TensorFlow中可以通过[BestExporter](https://oreil.ly/UpN1a)来实现。
- en: Regularization
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 正则化
- en: Instead of using early stopping or checkpoint selection, it can be helpful to
    try to add L2 regularization to your model so that the validation error does not
    increase and the model never gets into phase 3\. Instead, both the training loss
    and the validation error should plateau, as shown in [Figure 4-10](#in_the_ideal_situationcomma_validation).
    We term such a training loop (where both training and validation metrics reach
    a plateau) a *well-behaved* training loop.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用早停止或检查点选择之前，尝试向模型添加L2正则化可能会有所帮助，以确保验证错误不会增加，并且模型永远不会进入第3阶段。相反，训练损失和验证错误应该会趋于平稳，正如[图4-10](#in_the_ideal_situationcomma_validation)所示。我们将这样的训练循环（其中训练和验证指标均达到平稳状态）称为*良好行为*的训练循环。
- en: '![In the ideal situation, validation error does not increase. Instead, both
    the training loss and validation error plateau.](Images/mldp_0410.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![在理想情况下，验证错误不会增加。相反，训练损失和验证错误都会趋于平稳。](Images/mldp_0410.png)'
- en: Figure 4-10\. In the ideal situation, validation error does not increase. Instead,
    both the training loss and validation error plateau.
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-10。在理想情况下，验证错误不会增加。相反，训练损失和验证错误都会趋于平稳。
- en: If early stopping is not carried out, and only the training loss is used to
    decide convergence, then we can avoid having to set aside a separate testing dataset.
    Even if we are not doing early stopping, displaying the progress of the model
    training can be helpful, particularly if the model takes a long time to train.
    Although the performance and progress of the model training is normally monitored
    on the validation dataset during the training loop, it is for visualization purposes
    only. Since we don’t have to take any action based on metrics being displayed,
    we can carry out visualization on the test dataset.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不执行早停止，并且仅使用训练损失来决定收敛性，那么我们可以避免设置单独的测试数据集。即使我们不执行早停止，显示模型训练进度也可能很有帮助，特别是当模型训练时间较长时。虽然通常在训练循环期间，模型的性能和进度通常在验证数据集上进行监控，但这只是为了可视化目的。由于我们不需要根据显示的指标采取任何行动，我们可以在测试数据集上进行可视化。
- en: '**The reason that using regularization might be better than early stopping
    is that regularization allows you to use the entire dataset to change the weights
    of the model, whereas early stopping requires you to waste 10% to 20% of your
    dataset purely to decide when to stop training. Other methods to limit overfitting
    (such as dropout and using models with lower complexity) are also good alternatives
    to early stopping. In addition, [recent research](https://oreil.ly/FJ_iy) indicates
    that double descent happens in a variety of machine learning problems, and therefore
    it is better to train longer rather than risk a suboptimal solution by stopping
    early.**  **#### Two splits'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用正则化而不是早停止的原因在于，正则化允许您使用整个数据集来调整模型的权重，而早停止则要求您浪费10%至20%的数据集仅用于决定何时停止训练。其他限制过拟合的方法（如丢弃法和使用复杂度较低的模型）也是早停止的良好替代方法。此外，[最近的研究](https://oreil.ly/FJ_iy)表明，双下降现象在各种机器学习问题中都会发生，因此与其早停止，不如继续训练以避免出现次优解。**  **####
    两种分割方法'
- en: Isn’t the advice in the regularization section in conflict with the advice in
    the previous sections on early stopping or checkpoint selection? Not really.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化部分的建议是否与早停止或检查点选择部分的建议相矛盾？实际上并不是。
- en: 'We recommend that you split your data into two parts: a training dataset and
    an evaluation dataset. The evaluation dataset plays the part of the test dataset
    during experimentation (where there is no validation dataset) and plays the part
    of the validation dataset in production (where there is no test dataset).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您将数据分为两部分：训练数据集和评估数据集。在实验中，评估数据集充当测试数据集的角色（没有验证数据集），而在生产环境中，评估数据集充当验证数据集的角色（没有测试数据集）。
- en: 'The larger your training dataset, the more complex a model you can use, and
    the more accurate a model you can get. Using regularization rather than early
    stopping or checkpoint selection allows you to use a larger training dataset.
    In the experimentation phase (when you are exploring different model architectures,
    training techniques, and hyperparameters), we recommend that you turn off early
    stopping and train with larger models (see also [“Design Pattern 11: Useful Overfitting”](#design_pattern_oneone_useful_overfittin)).
    This is to ensure that the model has enough capacity to learn the predictive patterns.
    During this process, monitor error convergence on the training split. At the end
    of experimentation, you can use the evaluation dataset to diagnose how well your
    model does on data it has not encountered during training.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你的训练数据集越大，你可以使用的模型就越复杂，得到的模型准确性也越高。使用正则化而不是早停或检查点选择，允许你使用更大的训练数据集。在实验阶段（当你正在探索不同的模型架构、训练技术和超参数时），我们建议你关闭早停并使用更大的模型进行训练（还可参见[“设计模式11：有益过拟合”](#design_pattern_oneone_useful_overfittin)）。这是为了确保模型具有足够的容量来学习预测模式。在此过程中，监视训练集上的错误收敛。在实验结束时，你可以使用评估数据集来诊断模型在训练过程中未曾遇到的数据上的表现。
- en: When training the model to deploy in production, you will need to prepare to
    be able to do continuous evaluation and model retraining. Turn on early stopping
    or checkpoint selection and monitor the error metric on the evaluation dataset.
    Choose between early stopping and checkpoint selection depending on whether you
    need to control cost (in which case, you would choose early stopping) or want
    to prioritize model accuracy (in which case, you would choose checkpoint selection).**  **###
    Fine-tuning
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练模型用于生产部署时，你需要准备好进行持续评估和模型再训练。开启早停或检查点选择，并监控评估数据集上的错误指标。根据你是否需要控制成本（在这种情况下，你会选择早停）或者想要优先考虑模型准确性（在这种情况下，你会选择检查点选择），选择早停或检查点选择。**  **###
    微调
- en: In a well-behaved training loop, gradient descent behaves such that you get
    to the neighborhood of the optimal error quickly on the basis of the majority
    of your data, then slowly converge toward the lowest error by optimizing on the
    corner cases.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个良好的训练循环中，梯度下降的行为是这样的，它通过优化大多数数据来快速到达最优错误的邻域，然后通过优化边缘情况来缓慢收敛到最低错误。
- en: Now, imagine that you need to periodically retrain the model on fresh data.
    You typically want to emphasize the fresh data, not the corner cases from last
    month. You are often better off resuming your training, not from the last checkpoint,
    but the checkpoint marked by the blue line in [Figure 4-11](#resume_from_a_checkpoint_from_before_th).
    This corresponds to the start of phase 2 in our discussion of the phases of model
    training described earlier in [“Why It Works”](#why_it_works-id00312). This helps
    ensure that you have a general method that you are able to then fine-tune for
    a few epochs on just the fresh data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，你需要定期在新鲜数据上重新训练模型。你通常希望强调新鲜数据，而不是上个月的边缘情况。你更好地从标记为蓝线的检查点恢复你的训练，而不是从上个检查点开始，这对应于我们之前讨论的模型训练阶段中第2阶段的开始[“为什么有效”](#why_it_works-id00312)。这有助于确保你有一种通用方法，然后你能对刚刚的新鲜数据进行几个时期的微调。
- en: 'When you resume from the checkpoint marked by the thick dashed vertical line,
    you will be on the fourth epoch, and so the learning rate will be quite low. Therefore,
    the fresh data will not dramatically change the model. However, the model will
    behave optimally (in the context of the larger model) on the fresh data because
    you will have sharpened it on this smaller dataset. This is called *fine-tuning*.
    Fine-tuning is also discussed in [“Design Pattern 13: Transfer Learning”](#design_pattern_onethree_transfer_learni).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当你从标记为粗虚线的检查点恢复时，你将处于第四个时期，因此学习速率会非常低。因此，新鲜数据不会显著改变模型。然而，模型在新鲜数据上的表现会达到最佳（在更大模型的背景下）。这是被称为*微调*的过程。微调也在[“设计模式13：迁移学习”](#design_pattern_onethree_transfer_learni)中讨论过。
- en: '![Resume from a checkpoint from before the training loss starts to plateau.
    Train only on fresh data for subsequent iterations.](Images/mldp_0411.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![从训练损失开始平台之前的检查点恢复。仅在后续迭代中使用新鲜数据进行训练。](Images/mldp_0411.png)'
- en: Figure 4-11\. Resume from a checkpoint from before the training loss starts
    to plateau. Train only on fresh data for subsequent iterations.
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-11。从在训练损失开始平台化之前的检查点恢复。仅对后续迭代使用新鲜数据进行训练。
- en: Warning
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Fine-tuning only works as long as you are not changing the model architecture.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在不改变模型架构的情况下，微调才有效。
- en: It is not necessary to always start from an earlier checkpoint. In some cases,
    the final checkpoint (that is used to serve the model) can be used as a warm start
    for another model training iteration. Still, starting from an earlier checkpoint
    tends to provide better generalization.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不必总是从较早的检查点开始。在某些情况下，最终检查点（用于提供模型）可以用作另一个模型训练迭代的热启动。然而，从较早的检查点开始通常能提供更好的泛化能力。
- en: Redefining an epoch
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新定义一个epoch
- en: 'Machine learning tutorials often have code like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习教程经常有这样的代码：
- en: '[PRE4]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code assumes that you have a dataset that fits in memory, and consequently
    that your model can iterate through 15 epochs without running the risk of machine
    failure. Both these assumptions are unreasonable—ML datasets range into terabytes,
    and when training can last hours, the chances of machine failure are high.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码假设你有一个可以放入内存的数据集，并且你的模型可以在15个epochs中迭代而不会出现机器故障的风险。然而，这两个假设都是不合理的——机器学习数据集的范围达到了几个TB，而且在训练时间长达数小时时，机器故障的几率很高。
- en: 'To make the preceding code more resilient, supply a [TensorFlow dataset](https://oreil.ly/EKJ4V)
    (not just a NumPy array) because the TensorFlow dataset is an out-of-memory dataset.
    It provides iteration capability and lazy loading. The code is now as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使上述代码更加弹性，提供一个[TensorFlow数据集](https://oreil.ly/EKJ4V)（而不仅仅是一个NumPy数组），因为TensorFlow数据集是一个内存外的数据集。它提供迭代能力和延迟加载。现在代码如下：
- en: '[PRE5]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'However, using epochs on large datasets remains a bad idea. Epochs may be easy
    to understand, but the use of epochs leads to bad effects in real-world ML models.
    To see why, imagine that you have a training dataset with one million examples.
    It can be tempting to simply go through this dataset 15 times (for example) by
    setting the number of epochs to 15\. There are several problems with this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在大型数据集上使用epochs仍然是一个不好的主意。epochs可能很容易理解，但是在实际的机器学习模型中使用epochs会导致不良影响。为了了解为什么，请想象一下你有一个包含一百万个示例的训练数据集。简单地设置epochs为15，然后简单地遍历这个数据集15次可能很诱人。但是存在一些问题：
- en: The number of epochs is an integer, but the difference in training time between
    processing the dataset 14.3 times and 15 times can be hours. If the model has
    converged after having seen 14.3 million examples, you might want to exit and
    not waste the computational resources necessary to process 0.7 million more examples.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: epochs的数量是一个整数，但是在处理数据集14.3次和15次之间的训练时间差异可能会达到数小时。如果模型在观察了1430万个示例后已经收敛，您可能希望退出，而不是浪费处理700000个示例所需的计算资源。
- en: You checkpoint once per epoch, and waiting one million examples between checkpoints
    might be way too long. For resilience, you might want to checkpoint more often.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个epoch只需进行一次检查点，而等待一百万个示例之间的检查点可能会太长。为了弹性，您可能希望更频繁地进行检查点。
- en: Datasets grow over time. If you get 100,000 more examples and you train the
    model and get a higher error, is it because you need to do an early stop, or is
    the new data corrupt in some way? You can’t tell because the prior training was
    on 15 million examples and the new one is on 16.5 million examples.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集会随着时间增长。如果您获取了10万个更多的示例并训练模型，并且得到了更高的错误，是因为您需要早停，还是新数据在某种方式上损坏了？您无法确定，因为先前的训练是在1500万个示例上进行的，而新的训练是在1650万个示例上进行的。
- en: 'In distributed, parameter-server training (see [“Design Pattern 14: Distribution
    Strategy”](#design_pattern_onefour_distribution_str)) with data parallelism and
    proper shuffling, the concept of an epoch is not clear anymore. Because of potentially
    straggling workers, you can only instruct the system to train on some number of
    mini-batches.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式参数服务器训练（参见[“设计模式14：分布策略”](#design_pattern_onefour_distribution_str)）中，使用数据并行和适当的洗牌，epoch的概念不再清晰。由于可能存在滞后的工作节点，您只能指示系统对一些小批次进行训练。
- en: Steps per epoch
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每个epoch的步骤
- en: 'Instead of training for 15 epochs, we might decide to train for 143,000 steps
    where the `batch_size` is 100:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能决定不再训练15个epochs，而是决定训练143000步，其中batch_size为100：
- en: '[PRE6]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Each step involves weight updates based on a single mini-batch of data, and
    this allows us to stop at 14.3 epochs. This gives us much more granularity, but
    we have to define an “epoch” as 1/15th of the total number of steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤涉及基于单个小批量数据的权重更新，这使我们可以在 14.3 个时代停止。这为我们提供了更多的粒度，但我们必须将“时代”定义为总步数的 1/15：
- en: '[PRE7]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is so that we get the right number of checkpoints. It works as long as
    we make sure to repeat the `trainds` infinitely:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这样可以确保我们获得正确数量的检查点。只要确保无限重复`trainds`即可：
- en: '[PRE8]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `repeat()` is needed because we no longer set `num_epochs`, so the number
    of epochs defaults to one. Without the `repeat()`, the model will exit once the
    training patterns are exhausted after reading the dataset once.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`repeat()`是必需的，因为我们不再设置`num_epochs`，所以默认的时代数为一。如果没有`repeat()`，模型将在读取数据集一次后退出一旦训练模式耗尽。'
- en: Retraining with more data
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用更多数据重新训练
- en: What happens when we get 100,000 more examples? Easy! We add it to our data
    warehouse but do not update the code. Our code will still want to process 143,000
    steps, and it will get to process that much data, except that 10% of the examples
    it sees are newer. If the model converges, great. If it doesn’t, we know that
    these new data points are the issue because we are not training longer than we
    were before. By keeping the number of steps constant, we have been able to separate
    out the effects of new data from training on more data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们获得额外的 100,000 个示例时会发生什么？很简单！我们将其添加到我们的数据仓库中，但不更新代码。我们的代码仍然希望处理 143,000 步，它将处理这么多数据，只是它看到的示例中有
    10% 是新的。如果模型收敛了，太棒了。如果没有，我们知道这些新数据点是问题所在，因为我们的训练时间并没有比以前长。通过保持步数恒定，我们能够将新数据的影响与更多数据的训练效果分离开来。
- en: Once we have trained for 143,000 steps, we restart the training and run it a
    bit longer (say, 10,000 steps), and as long as the model continues to converge,
    we keep training it longer. Then, we update the number 143,000 in the code above
    (in reality, it will be a parameter to the code) to reflect the new number of
    steps.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了 143,000 步，我们重新开始训练并继续运行一段时间（比如说，10,000 步），只要模型继续收敛，我们就继续延长训练时间。然后，我们更新上述代码中的
    143,000 这个数字（实际上，它将是代码的一个参数），以反映新的步数。
- en: This all works fine, until you want to do hyperparameter tuning. When you do
    hyperparameter tuning, you will want to want to change the batch size. Unfortunately,
    if you change the batch size to 50, you will find yourself training for half the
    time because we are training for 143,000 steps, and each step is only half as
    long as before. Obviously, this is no good.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都很好，直到您想进行超参数调整。当进行超参数调整时，您将希望更改批处理大小。不幸的是，如果您将批处理大小更改为 50，您会发现自己的训练时间减少了一半，因为我们正在进行
    143,000 步的训练，每步只有以前的一半时间。显然，这是不好的。
- en: Virtual epochs
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 虚拟时代
- en: 'The answer is to keep the total number of training examples shown to the model
    (not number of steps; see [Figure 4-12](#defining_a_virtual_epoch_in_terms_of_th))
    constant:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是保持向模型显示的总训练示例数量（而不是步数；参见[图 4-12](#defining_a_virtual_epoch_in_terms_of_th)）恒定：
- en: '[PRE9]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Defining a virtual epoch in terms of the desired number of steps between
    checkpoints.](Images/mldp_0412.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![在期望的检查点之间的步数定义虚拟时代。](Images/mldp_0412.png)'
- en: Figure 4-12\. Defining a virtual epoch in terms of the desired number of steps
    between checkpoints.
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-12\. 在期望的检查点之间的步数定义虚拟时代。
- en: When you get more data, first train it with the old settings, then increase
    the number of examples to reflect the new data, and finally change the `STOP_POINT`
    to reflect the number of times you have to traverse the data to attain convergence.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当您获得更多数据时，首先使用旧设置进行训练，然后增加示例的数量以反映新数据，并最终更改`STOP_POINT`以反映达到收敛所需遍历数据的次数。
- en: 'This is now safe even with hyperparameter tuning (discussed later in this chapter)
    and retains all the advantages of keeping the number of steps constant.**  **#
    Design Pattern 13: Transfer Learning'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这现在即使在超参数调整（本章后面讨论）之后也是安全的，并保留保持步数恒定的所有优势。 **# 设计模式 13：迁移学习**
- en: In Transfer Learning, we take part of a previously trained model, freeze the
    weights, and incorporate these nontrainable layers into a new model that solves
    a similar problem, but on a smaller dataset.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习中，我们采用先前训练模型的一部分，冻结权重，并将这些不可训练的层合并到解决类似问题但在较小数据集上的新模型中。
- en: Problem
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Training custom ML models on unstructured data requires extremely large datasets,
    which are not always readily available. Consider the case of a model identifying
    whether an x-ray of an arm contains a broken bone. To achieve high accuracy, you’ll
    need hundreds of thousands of images, if not more. Before your model learns what
    a broken bone looks like, it needs to first learn to make sense of the pixels,
    edges, and shapes that are part of the images in your dataset. The same is true
    for models trained on text data. Let’s say we’re building a model that takes descriptions
    of patient symptoms and predicts the possible conditions associated with those
    symptoms. In addition to learning which words differentiate a cold from pneumonia,
    the model also needs to learn basic language semantics and how the sequence of
    words creates meaning. For example, the model would need to not only learn to
    detect the presence of the word *fever*, but that the sequence *no fever* carries
    a very different meaning than *high fever*.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 训练非结构化数据上的定制机器学习模型需要非常庞大的数据集，这并不总是随时可得。考虑一个模型识别手臂X光是否有骨折的情况。为了达到高精度，你将需要成千上万张图片，甚至更多。在你的模型学会辨别骨折看起来是什么之前，它需要先学会理解数据集中图片的像素、边缘和形状。对于基于文本数据训练的模型也是如此。假设我们正在构建一个模型，接受患者症状描述并预测可能的相关病症。除了学习哪些词汇可以区分感冒和肺炎之外，模型还需要学习基本的语言语义以及词汇序列如何创建含义。例如，模型不仅需要学会检测“发烧”这个词的存在，还需要理解“无发烧”与“高烧”这两个序列的含义完全不同。
- en: To see just how much data is required to train high-accuracy models, we can
    look at [ImageNet](https://oreil.ly/t6583), a database of over 14 million labeled
    images. ImageNet is frequently used as a benchmark for evaluating machine learning
    frameworks on various hardware. As an example, the [MLPerf benchmark suite](https://oreil.ly/hDPiJ)
    uses ImageNet to compare the time it took for various ML frameworks running on
    different hardware to reach 75.9% classification accuracy. In the v0.7 MLPerf
    Training results, a TensorFlow model running on a Google TPU v3 took around 30
    seconds to reach this target accuracy.^([2](ch04.xhtml#idm46056094720120)) With
    more training time, models can reach even higher accuracy on ImageNet. However,
    this is largely due to ImageNet’s size. Most organizations with specialized prediction
    problems don’t have nearly as much data available.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解训练高精度模型所需的数据量有多大，我们可以看看[ImageNet](https://oreil.ly/t6583)，这是一个拥有超过1400万标记图像的数据库。ImageNet经常用作评估机器学习框架在各种硬件上性能的基准。例如，[MLPerf基准套件](https://oreil.ly/hDPiJ)使用ImageNet来比较各种ML框架在不同硬件上达到75.9%分类准确率所需的时间。在v0.7
    MLPerf训练结果中，运行在Google TPU v3上的TensorFlow模型大约花了30秒达到这一目标精度^([2](ch04.xhtml#idm46056094720120))。随着更多的训练时间，模型在ImageNet上的准确率可以进一步提高。然而，这主要是由于ImageNet的数据规模。大多数有专门预测问题的组织并没有如此大量的数据可用。
- en: Because use cases like the image and text examples described above involve particularly
    specialized data domains, it’s also not possible to use a general-purpose model
    to successfully identify bone fractures or diagnose diseases. A model that is
    trained on ImageNet might be able to label an x-ray image as *x-ray* or *medical
    imaging* but is unlikely to be able to label it as a *broken femur*. Because such
    models are often trained on a wide variety of high-level label categories, we
    wouldn’t expect them to understand conditions present in the images that are specific
    to our dataset. To handle this, we need a solution that allows us to build a custom
    model using only the data we have available and with the labels that we care about.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因为像上面描述的图像和文本示例这样的用例涉及特定的专业数据领域，使用通用模型无法成功识别骨折或诊断疾病。一个在ImageNet上训练的模型也许能够标记X光图像为“X光”或“医学成像”，但不太可能能够标记为“股骨骨折”。因为这些模型通常是在各种高级标签类别上进行训练的，我们不期望它们能理解特定于我们数据集的图像中存在的条件。为了解决这个问题，我们需要一个解决方案，允许我们仅使用我们可用的数据和我们关心的标签来构建定制模型。
- en: Solution
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: With the Transfer Learning design pattern, we can take a model that has been
    trained on the same type of data for a similar task and apply it to a specialized
    task using our own custom data. By “same type of data,” we mean the same data
    modality—images, text, and so forth. Beyond just the broad category like images,
    it is also ideal to use a model that has been pre-trained on the same types of
    images. For example, use a model that has been pre-trained on photographs if you
    are going to use it for photograph classification and a model that has been pre-trained
    on remotely sensed imagery if you are going to use it to classify satellite images.
    By *similar task*, we’re referring to the problem being solved. To do transfer
    learning for image classification, for example, it is better to start with a model
    that has been trained for image classification, rather than object detection.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 通过转移学习设计模式，我们可以采用一个已经在相同类型数据上训练过的模型，并将其应用于使用我们自己定制数据的专业任务。所谓的“相同类型数据”，指的是相同的数据形态——图像、文本等等。除了像图像这样的广泛类别外，最好还使用已经在相同类型图像上预训练过的模型。例如，如果要用于照片分类，则最好使用已经在照片上预训练过的模型；如果要用于卫星图像分类，则最好使用已经在遥感图像上预训练过的模型。所谓的“相似任务”，是指解决的问题。例如，要进行图像分类的转移学习，最好从已经用于图像分类的模型开始，而不是目标检测的模型。
- en: 'Continuing with the example, let’s say we’re building a binary classifier to
    determine whether an image of an x-ray contains a broken bone. We only have 200
    images of each class: *broken* and *not broken*. This isn’t enough to train a
    high-quality model from scratch, but it is sufficient for transfer learning. To
    solve this with transfer learning, we’ll need to find a model that has already
    been trained on a large dataset to do image classification. We’ll then remove
    the last layer from that model, freeze the weights of that model, and continue
    training using our 400 x-ray images. We’d ideally find a model trained on a dataset
    with similar images to our x-rays, like images taken in a lab or another controlled
    condition. However, we can still utilize transfer learning if the datasets are
    different, so long as the prediction task is the same. In this case we’re doing
    image classification.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 继续以示例为例，假设我们正在构建一个二元分类器，以确定X光图像是否包含骨折。我们每类只有200张图像：*骨折*和*未骨折*。这些图像数量不足以从头开始训练一个高质量的模型，但足以进行转移学习。为了用转移学习解决这个问题，我们需要找到一个已经在大型数据集上训练过的图像分类模型。然后，我们会移除该模型的最后一层，冻结该模型的权重，并继续使用我们的400张X光图像进行训练。理想情况下，我们会找到一个已经在类似X光图像的数据集上训练过的模型，比如实验室或其他受控条件下拍摄的图像。然而，即使数据集不同，只要预测任务相同，我们仍然可以利用转移学习。在本例中，我们进行的是图像分类任务。
- en: You can use transfer learning for many prediction tasks in addition to image
    classification, so long as there is an existing pre-trained model that matches
    the task you’d like to perform on your dataset. For example, transfer learning
    is also frequently applied in image object detection, image style transfer, image
    generation, text classification, machine translation, and more.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图像分类外，只要存在一个已经预训练的模型与您想要在您的数据集上执行的任务匹配，您还可以将转移学习用于许多预测任务。例如，转移学习也经常应用于图像目标检测、图像风格转移、图像生成、文本分类、机器翻译等领域。
- en: Note
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Transfer learning works because it lets us stand on the shoulders of giants,
    utilizing models that have already been trained on extremely large, labeled datasets.
    We’re able to use transfer learning thanks to years of research and work others
    have put into creating these datasets for us, which has advanced the state-of-the-art
    in transfer learning. One example of such a dataset is the ImageNet project, started
    in 2006 by Fei-Fei Li and published in 2009\. ImageNet^([3](ch04.xhtml#ch01fn17))
    has been essential to the development of transfer learning and paved the way for
    other large datasets like [COCO](https://oreil.ly/mXt77) and [Open Images](https://oreil.ly/QN9KU).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习之所以有效，是因为它让我们站在巨人的肩膀上，利用已经在非常大的标记数据集上训练过的模型。我们能够利用转移学习，要归功于多年来其他人为我们创建这些数据集所做的研究和工作，这些工作推动了转移学习技术的发展。一个这样的数据集的例子是2006年由李飞飞（Fei-Fei
    Li）启动并于2009年发布的ImageNet项目。ImageNet^([3](ch04.xhtml#ch01fn17))对转移学习的发展至关重要，并为其他大型数据集如[COCO](https://oreil.ly/mXt77)和[Open
    Images](https://oreil.ly/QN9KU)铺平了道路。
- en: The idea behind transfer learning is that you can utilize the weights and layers
    from a model trained in the same domain as your prediction task. In most deep
    learning models, the final layer contains the classification label or output specific
    to your prediction task. With transfer learning, we remove this layer, freeze
    the model’s trained weights, and replace the final layer with the output for our
    specialized prediction task before continuing to train. We can see how this works
    in [Figure 4-13](#transfer_learning_involves_training_a_m).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的理念是，您可以利用在与预测任务相同领域中训练的模型的权重和层。在大多数深度学习模型中，最后一层包含特定于预测任务的分类标签或输出。通过迁移学习，我们移除此层，冻结模型的训练权重，并在继续训练之前用我们专门预测任务的输出替换最后一层。我们可以在[图4-13](#transfer_learning_involves_training_a_m)中看到这是如何工作的。
- en: Typically, the penultimate layer of the model (the layer before the model’s
    output layer) is chosen as the *bottleneck layer*. Next, we’ll explain the bottleneck
    layer, along with different ways to implement transfer learning in TensorFlow.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型的倒数第二层（输出层之前的层）被选为*bottleneck layer*。接下来，我们将解释瓶颈层以及在TensorFlow中实施迁移学习的不同方法。
- en: '![Transfer learning involves training a model on a large dataset. The “top”
    of the model (typically, just the output layer) is removed and the remaining layers
    have their weights frozen. The last layer of the remaining model is called the
    bottleneck layer.](Images/mldp_0413.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![Transfer learning involves training a model on a large dataset. The “top”
    of the model (typically, just the output layer) is removed and the remaining layers
    have their weights frozen. The last layer of the remaining model is called the
    bottleneck layer.](Images/mldp_0413.png)'
- en: Figure 4-13\. Transfer learning involves training a model on a large dataset.
    The “top” of the model (typically, just the output layer) is removed and the remaining
    layers have their weights frozen. The last layer of the remaining model is called
    the bottleneck layer.
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-13\. 迁移学习涉及在大型数据集上训练模型。模型的“顶部”（通常只是输出层）被移除，剩余层的权重被冻结。剩余模型的最后一层被称为瓶颈层。
- en: Bottleneck layer
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 瓶颈层
- en: In relation to an entire model, the bottleneck layer represents the input (typically
    an image or text document) in the lowest-dimensionality space. More specifically,
    when we feed data into our model, the first layers see this data nearly in its
    original form. To see how this works, let’s continue with a medical imaging example,
    but this time we’ll [build a model](https://oreil.ly/QfOU_) with a [colorectal
    histology dataset](https://oreil.ly/r4HHq) to classify the histology images into
    one of eight categories.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个模型中，瓶颈层代表着输入（通常是图像或文本文档）在最低维度空间中的表示。更具体地说，当我们将数据输入模型时，前几层几乎以其原始形式查看这些数据。为了看到这是如何工作的，让我们继续使用一个医学影像的例子，但这次我们将使用一个[建模](https://oreil.ly/QfOU_)，使用[结直肠组织学数据集](https://oreil.ly/r4HHq)将组织学图像分类为八个类别之一。
- en: 'To explore the model we are going to use for transfer learning, let’s load
    the VGG model architecture pre-trained on the ImageNet dataset:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索用于迁移学习的模型，让我们加载在ImageNet数据集上预训练的VGG模型架构：
- en: '[PRE10]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Notice that we’ve set `include_top=True`, which means we’re loading the full
    VGG model, including the output layer. For ImageNet, the model classifies images
    into 1,000 different classes, so the output layer is a 1,000-element array. Let’s
    look at the output of `model.summary()` to understand which layer will be used
    as the bottleneck. For brevity, we’ve left out some of the middle layers here:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们设置了`include_top=True`，这意味着我们加载了完整的VGG模型，包括输出层。对于ImageNet，该模型将图像分类为1,000个不同的类别，因此输出层是一个1,000元素的数组。让我们查看`model.summary()`的输出，以了解哪一层将被用作瓶颈层。为简洁起见，这里省略了一些中间层的信息：
- en: '[PRE11]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, the VGG model accepts images as a 224×224×3-pixel array. This
    128-element array is then passed through successive layers (each of which may
    change the dimensionality of the array) until it is flattened into a 25,088×1-dimensional
    array in the layer called `flatten`. Finally, it is fed into the output layer,
    which returns a 1,000-element array (for each class in ImageNet). In this example,
    we’ll choose the `block5_pool` layer as the bottleneck layer when we adapt this
    model to be trained on our medical histology images. The bottleneck layer produces
    a 7×7×512-dimensional array, which is a low-dimensional representation of the
    input image. It has retained enough of the information from the input image to
    be able to classify it. When we apply this model to our medical image classification
    task, we hope that the information distillation will be sufficient to successfully
    carry out classification on our dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，VGG模型接受图像作为一个224×224×3像素的数组。这个128元素的数组然后通过连续的层（每一层可能会改变数组的维度）传递，直到在`flatten`层中被展开成一个25,088×1维度的数组。最后，它被送入输出层，返回一个1,000元素的数组（每个类在ImageNet中）。在这个例子中，当我们调整这个模型以在我们的医学组织学图像上进行训练时，我们将选择`block5_pool`层作为瓶颈层。瓶颈层产生一个7×7×512维度的数组，这是输入图像的低维表示。它保留了足够的信息以便对其进行分类。当我们将这个模型应用于我们的医学图像分类任务时，我们希望信息的提取足以成功地在我们的数据集上进行分类。
- en: 'The histology dataset comes with images as (150,150,3) dimensional arrays.
    This 150×150×3 representation is the *highest* dimensionality. To use the VGG
    model with our image data, we can load it with the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的组织结构包含了(150,150,3)维度的图像数组。这个150×150×3的表示是*最高*维度。为了使用VGG模型处理我们的图像数据，我们可以按以下步骤加载它：
- en: '[PRE12]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'By setting `include_top=False`, we’re specifying that the last layer of VGG
    we want to load is the bottleneck layer. The `input_shape` we passed in matches
    the input shape of our histology images. A summary of the last few layers of this
    updated VGG model looks like the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`include_top=False`，我们指定要加载的VGG的最后一层是瓶颈层。我们传递的`input_shape`匹配我们的组织学图像的输入形状。此更新后的VGG模型最后几层的摘要如下所示：
- en: '[PRE13]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The last layer is now our bottleneck layer. You may notice that the size of
    `block5_pool` is (4,4,512), whereas before, it was (7,7,512). This is because
    we instantiated VGG with an `input_shape` parameter to account for the size of
    the images in our dataset. It’s also worth noting that setting `include_top=False`
    is hardcoded to use `block5_pool` as the bottleneck layer, but if you want to
    customize this, you can load the full model and delete any additional layers you
    don’t want to use.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层现在是我们的瓶颈层。您可能会注意到，`block5_pool`的大小为(4,4,512)，而之前为(7,7,512)。这是因为我们在实例化VGG时使用了一个`input_shape`参数来考虑我们数据集中图像的大小。值得注意的是，设置`include_top=False`是硬编码为使用`block5_pool`作为瓶颈层，但如果您想要自定义此过程，可以加载完整模型并删除不需要使用的任何额外层。
- en: Before this model is ready to be trained, we’ll need to add a few layers on
    top, specific to our data and classification task. It’s also important to note
    that because we’ve set `trainable=False`, there are 0 trainable parameters in
    the current model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型准备好进行训练之前，我们需要在顶部添加几层，这些层针对我们的数据和分类任务进行了特定设置。还值得注意的是，因为我们设置了`trainable=False`，当前模型中没有可训练的参数。
- en: Tip
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: As a general rule of thumb, the bottleneck layer is typically the last, lowest-dimensionality,
    flattened layer before a flattening operation.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个一般的经验法则，瓶颈层通常是最后一个、最低维度的、在展平操作之前的层。
- en: Because they both represent features in reduced dimensionality, bottleneck layers
    are conceptually similar to embeddings. For example, in an autoencoder model with
    an encoder-decoder architecture, the bottleneck layer *is* an embedding. In this
    case, the bottleneck serves as the middle layer of the model, mapping the original
    input data to a lower-dimensionality representation, which the decoder (the second
    half of the network) uses to map the input back to its original, higher-dimensional
    representation. To see a diagram of the bottleneck layer in an autoencoder, refer
    to [Figure 2-13](ch02.xhtml#a_pre-trained_text_embedding_can_be_add) in [Chapter 2](ch02.xhtml#data_representation_design_patterns).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它们都表示降维后的特征，所以瓶颈层在概念上与嵌入类似。例如，在具有编码器-解码器架构的自编码器模型中，瓶颈层是一个嵌入。在这种情况下，瓶颈作为模型的中间层，将原始输入数据映射到更低维度的表示，解码器（网络的第二部分）使用该表示将输入映射回其原始的高维表示。要查看自编码器中瓶颈层的图示，请参见[图 2-13](ch02.xhtml#a_pre-trained_text_embedding_can_be_add)
    在[第 2 章](ch02.xhtml#data_representation_design_patterns)。
- en: An embedding layer is essentially a lookup table of weights, mapping a particular
    feature to some dimension in vector space. The main difference is that the weights
    in an embedding layer can be trained, whereas all the layers leading up to and
    including the bottleneck layer have their weights frozen. In other words, the
    entire network up to and including the bottleneck layer is nontrainable, and the
    weights in the layers after the bottleneck are the only trainable layers in the
    model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层本质上是一个权重查找表，将特定特征映射到向量空间中的某个维度。主要区别在于嵌入层中的权重可以训练，而所有导致并包括瓶颈层的权重都被冻结。换句话说，直到并包括瓶颈层的整个网络是不可训练的，而瓶颈层后面的层中的权重是模型中唯一可训练的层。
- en: Note
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It’s also worth noting that pre-trained embeddings can be used in the Transfer
    Learning design pattern. When you build a model that includes an embedding layer,
    you can either utilize an existing (pre-trained) embedding lookup, or train your
    own embedding layer from scratch.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，预训练的嵌入可以在迁移学习设计模式中使用。当您构建包含嵌入层的模型时，您可以利用现有（预训练的）嵌入查找表，或者从头开始训练您自己的嵌入层。
- en: To summarize, transfer learning is a solution you can employ to solve a similar
    problem on a smaller dataset. Transfer learning always makes use of a bottleneck
    layer with nontrainable, frozen weights. Embeddings are a type of data representation.
    Ultimately, it comes down to purpose. If the purpose is to train a similar model,
    you would use transfer learning. Consequently, if the purpose is to represent
    an input image more concisely, you would use an embedding. The code might be exactly
    the same.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，迁移学习是您可以用来解决较小数据集上类似问题的解决方案。迁移学习总是使用具有不可训练冻结权重的瓶颈层。嵌入是一种数据表示类型。最终，这归结为目的。如果目的是训练类似的模型，您将使用迁移学习。因此，如果目的是更简洁地表示输入图像，您将使用嵌入。代码可能完全相同。
- en: Implementing transfer learning
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现迁移学习
- en: 'You can implement transfer learning in Keras using one of these two methods:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下两种方法之一在Keras中实现迁移学习：
- en: Loading a pre-trained model on your own, removing the layers after the bottleneck,
    and adding a new final layer with your own data and labels
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自己加载预训练模型，移除瓶颈层后面的层，并添加一个新的最终层，使用您自己的数据和标签
- en: Using a pre-trained [TensorFlow Hub](https://tfhub.dev) module as the foundation
    for your transfer learning task
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个预训练的[TensorFlow Hub](https://tfhub.dev)模块作为您的迁移学习任务的基础
- en: Let’s start by looking at how to load and use a pre-trained model on your own.
    For this, we’ll build on the VGG model example we introduced earlier. Note that
    VGG is a model architecture, whereas ImageNet is the data it was trained on. Together,
    these make up the pre-trained model we’ll be using for transfer learning. Here,
    we’re using transfer learning to classify colorectal histology images. Whereas
    the original ImageNet dataset contains 1,000 labels, our resulting model will
    *only* return 8 possible classes that we’ll specify, as opposed to the thousands
    of labels present in ImageNet.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看如何加载和使用自己的预训练模型。为此，我们将建立在我们之前介绍的VGG模型示例上。请注意，VGG是一个模型架构，而ImageNet是它训练的数据。这两者共同组成了我们将用于迁移学习的预训练模型。在这里，我们使用迁移学习来对结肠镜组织学图像进行分类。原始的ImageNet数据集包含1000个标签，而我们的最终模型将仅返回8个我们指定的可能类别，与ImageNet中存在的成千上万个标签形成对比。
- en: Note
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Loading a pre-trained model and using it to get classifications on the *original
    labels* that model was trained on is not transfer learning. Transfer learning
    is going one step further, replacing the final layers of the model with your own
    prediction task.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 加载一个预训练模型并在其上获取*原始标签*的分类并不算是迁移学习。迁移学习是更进一步，用自己的预测任务替换模型的最终层。
- en: 'The VGG model we’ve loaded will be our base model. We’ll need to add a few
    layers to flatten the output of our bottleneck layer and feed this flattened output
    into an 8-element softmax array:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载的 VGG 模型将是我们的基础模型。我们需要添加几个层来展平我们的瓶颈层的输出，并将这个展平的输出馈送到一个 8 元素 softmax 数组中：
- en: '[PRE14]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we can use the `Sequential,` API to create our new transfer learning
    model as a stack of layers:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用 `Sequential` API 来创建我们的新迁移学习模型，作为一系列层堆叠：
- en: '[PRE15]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s take note of the output of `model.summary()` on our transfer learning
    model:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们注意一下在我们的迁移学习模型上使用 `model.summary()` 的输出：
- en: '[PRE16]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The important piece here is that the only trainable parameters are the ones
    *after* our bottleneck layer. In this example, the bottleneck layer is the feature
    vectors from the VGG model. After compiling this model, we can train it using
    our dataset of histology images.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重要部分是，在我们的瓶颈层之后，唯一可训练的参数是那些 *后续* 的参数。在这个例子中，瓶颈层是来自 VGG 模型的特征向量。编译完这个模型之后，我们可以使用我们的组织学图像数据集来训练它。
- en: Pre-trained embeddings
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练嵌入
- en: While we can load a pre-trained model on our own, we can also implement transfer
    learning by making use of the many pre-trained models available in TF Hub, a library
    of pre-trained models (called modules). These modules span a variety of data domains
    and use cases, including classification, object detection, machine translation,
    and more. In TensorFlow, you can load these modules as a layer, then add your
    own classification layer on top.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以加载一个预训练模型来自行操作，但我们也可以利用 TF Hub 中提供的许多预训练模型来实现迁移学习，TF Hub 是一个包含各种数据领域和用例的预训练模型（称为模块）库，包括分类、目标检测、机器翻译等。在
    TensorFlow 中，您可以将这些模块加载为一层，然后在其上添加您自己的分类层。
- en: 'To see how TF Hub works, let’s build a model that classifies movie reviews
    as either *positive* or *negative*. First, we’ll load a pre-trained embedding
    model trained on a large corpus of news articles. We can instantiate this model
    as a `hub.KerasLayer`:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 TF Hub 的工作原理，让我们构建一个模型，将电影评论分类为*积极*或*消极*。首先，我们将加载一个在大型新闻文章语料库上训练的预训练嵌入模型。我们可以将此模型实例化为
    `hub.KerasLayer`：
- en: '[PRE17]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can stack additional layers on top of this to build our classifier:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在其上堆叠额外的层来构建我们的分类器：
- en: '[PRE18]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can now train this model, passing it our own text dataset as input. The resulting
    prediction will be a 1-element array indicating whether our model thinks the given
    text is positive or negative.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以训练这个模型，将我们自己的文本数据集作为输入传递给它。结果的预测将是一个包含一个元素的数组，指示我们的模型认为给定的文本是积极的还是消极的。
- en: Why It Works
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么它有效
- en: To understand why transfer learning works, let’s first look at an analogy. When
    children are learning their first language, they are exposed to many examples
    and corrected if they misidentify something. For example, the first time they
    learn to identify a cat, they’ll see their parents point to the cat and say the
    word *cat,* and this repetition strengthens pathways in their brain. Similarly,
    they are corrected when they say *cat* referring to an animal that is not a cat.
    When the child then learns how to identify a dog, they don’t need to start from
    scratch. They can use a similar recognition process to the one they used for the
    cat but apply it to a slightly different task. In this way, the child has built
    a foundation for learning. In addition to learning new things, they have also
    learned *how* to learn new things. Applying these learning methods to different
    domains is roughly how transfer learning works, too.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么迁移学习有效，让我们先看一个类比。当孩子们学习他们的第一门语言时，他们会接触到许多例子，并在他们误识别某些事物时得到纠正。例如，当他们第一次学会辨认猫时，他们会看到他们的父母指向猫并说“猫”的词，这种重复会加强他们大脑中的路径。类似地，当他们说“猫”指代不是猫的动物时，他们也会得到纠正。然后，当孩子学会辨认狗时，他们不需要从头开始。他们可以使用类似的识别过程来处理稍微不同的任务。通过这种方式，孩子们建立了学习的基础。除了学习新事物，他们还学会了*如何*学习新事物。将这些学习方法应用于不同的领域，大致就是迁移学习的工作原理。
- en: How does this play out in neural networks? In a typical convolutional neural
    network (CNN), the learning is hierarchical. The first layers learn to recognize
    edges and shapes present in an image. In the cat example, this might mean that
    the model can identify areas in an image where the edge of the cat’s body meets
    the background. The next layers in the model begin to understand groups of edges—perhaps
    that there are two edges that meet toward the top-left corner of the image. A
    CNN’s final layers can then piece together these groups of edges, developing an
    understanding of different features in the image. In the cat example, the model
    might be able to identify two triangular shapes toward the top of the image and
    two oval shapes below them. As humans, we know that these triangular shapes are
    ears and the oval shapes are eyes.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这在神经网络中如何发挥作用？在典型的卷积神经网络（CNN）中，学习是分层的。第一层学习识别图像中存在的边缘和形状。在猫的例子中，这可能意味着模型可以识别图像中猫身体边缘与背景相接触的区域。模型中的下一层开始理解边缘的组合——也许意识到图像左上角有两条边缘相交。CNN的最终层可以将这些边缘组合在一起，形成对图像中不同特征的理解。在猫的例子中，模型可能能够识别图像顶部两个三角形状和下方的两个椭圆形状。作为人类，我们知道这些三角形状是耳朵，椭圆形状是眼睛。
- en: We can visualize this process in [Figure 4-14](#research_from_zeiler_and_fergus_left_pa),
    from [research by Zeiler and Fergus](https://oreil.ly/VzRV_) on deconstructing
    CNNs to understand the different features that were activated throughout each
    layer of the model. For each layer in a five-layer CNN, this shows an image’s
    feature map for a given layer alongside the actual image. This lets us see how
    the model’s perception of an image progresses as it moves throughout the network.
    Layers 1 and 2 recognize only edges, layer 3 begins to recognize objects, and
    layers 4 and 5 can understand focal points within the entire image.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图 4-14](#research_from_zeiler_and_fergus_left_pa)中看到这个过程，这是由Zeiler和Fergus进行的研究，他们解构了CNN以理解模型每一层激活的不同特征。在一个五层CNN的每一层中，这显示了给定层的图像特征映射以及实际图像。这让我们能够看到模型对图像的感知如何随着其在网络中的移动而进展。第1层和第2层仅识别边缘，第3层开始识别对象，第4层和第5层可以理解整个图像中的焦点。
- en: Remember, though, that to our model, these are simply groupings of pixel values.
    It doesn’t know that the triangular and oval shapes are ears and eyes—it only
    knows to associate specific groupings of features with the labels it has been
    trained on. In this way, the model’s process of learning what groupings of features
    make up a cat isn’t *much* different from learning the groups of features that
    are part of other objects, like a table, a mountain, or even a celebrity. To a
    model, these are all just different combinations of pixel values, edges, and shapes.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，对于我们的模型而言，这些仅仅是像素值的分组。它并不知道三角形和椭圆形状是耳朵和眼睛——它只知道将特定的特征组合与它所训练的标签相关联。因此，从模型学习组成猫的特征组合的过程，并不与学习其他对象（如桌子、山或甚至名人）的特征组合的过程*有多大*不同。对于模型来说，这些只是不同的像素值、边缘和形状的组合。
- en: '![Research from Zeiler and Fergus (2013) in deconstructing CNNs helps us visualize
    how a CNN sees images at each layer of the network.](Images/mldp_0414.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![Zeiler和Fergus（2013年）的研究解构CNN帮助我们可视化CNN如何在网络的每一层中看待图像。](Images/mldp_0414.png)'
- en: Figure 4-14\. Research from Zeiler and Fergus (2013) in deconstructing CNNs
    helps us visualize how a CNN sees images at each layer of the network.
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-14\. Zeiler和Fergus（2013年）的研究解构CNN帮助我们可视化CNN如何在网络的每一层中看待图像。
- en: Trade-Offs and Alternatives
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷与替代方案
- en: 'So far, we haven’t discussed methods of modifying the weights of our original
    model when implementing transfer learning. Here, we’ll examine two approaches
    for this: feature extraction and fine-tuning. We’ll also discuss why transfer
    learning is primarily focused on image and text models and look at the relationship
    between text sentence embeddings and transfer learning.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有讨论在实施迁移学习时如何修改原始模型的权重。在这里，我们将探讨两种方法：特征提取和微调。我们还将讨论为什么迁移学习主要集中在图像和文本模型上，并查看文本句子嵌入与迁移学习之间的关系。
- en: Fine-tuning versus feature extraction
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调与特征提取
- en: '*Feature extraction* describes an approach to transfer learning where you freeze
    the weights of all layers before the bottleneck layer and train the following
    layers on your own data and labels. Another option is instead *fine-tuning* the
    weights of the pre-trained model’s layers. With fine-tuning, you can either update
    the weights of each layer in the pre-trained model, or just a few of the layers
    right before the bottleneck. Training a transfer learning model using fine-tuning
    typically takes longer than feature extraction. You’ll notice in our text classification
    example above, we set `trainable=True` when initializing our TF Hub layer. This
    is an example of fine-tuning.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征提取*描述了一种迁移学习的方法，其中您冻结了瓶颈层之前的所有层的权重，并在您自己的数据和标签上训练后续层。另一个选择是代替地*微调*预训练模型的层权重。通过微调，您可以更新预训练模型中每一层的权重，或者只是在瓶颈之前的几层。使用微调训练迁移学习模型通常比特征提取需要更长的时间。正如在我们的文本分类示例中所示，当初始化
    TF Hub 层时，我们将`trainable=True`。这是微调的一个示例。'
- en: 'When fine-tuning, it’s common to leave the weights of the model’s initial layers
    frozen since these layers have been trained to recognize basic features that are
    often common across many types of images. To fine-tune a MobileNet model, for
    example, we’d set `trainable=False` only for a subset of layers in the model,
    rather than making every layer non-trainable. For example, to fine-tune after
    the 100th layer, we could run:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行微调时，通常会将模型初始层的权重保持冻结状态，因为这些层已经训练用于识别基本特征，这些特征通常在许多类型的图像中都是共通的。例如，要微调一个 MobileNet
    模型，我们会将`trainable=False`仅应用于模型的一部分层，而不是将每一层都设置为不可训练。例如，要在第100层之后进行微调，我们可以运行：
- en: '[PRE19]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: One recommended approach to determining how many layers to freeze is known as
    [*progressive fine-tuning*](https://oreil.ly/fAv1S), and it involves iteratively
    unfreezing layers after every training run to find the ideal number of layers
    to fine-tune. This works best and is most efficient if you keep your learning
    rate low (0.001 is common) and the number of training iterations relatively small.
    To implement progressive fine-tuning, start by unfreezing only the last layer
    of your transferred model (the layer closest to the output) and calculate your
    model’s loss after training. Then, one by one, unfreeze more layers until you
    reach the Input layer or until the loss starts to plateau. Use this to inform
    the number of layers to fine-tune.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一个推荐的确定要冻结多少层的方法被称为[*渐进微调*](https://oreil.ly/fAv1S)，它涉及在每次训练运行后逐步解冻层，以找到要微调的理想层数。如果您保持学习速率低（通常为0.001），并且训练迭代次数相对较少，这种方法效果最好且最有效。要实施渐进微调，首先只解冻转移模型的最后一层（最靠近输出的层），并在训练后计算模型的损失。然后，逐层解冻更多的层，直到达到输入层或损失开始趋于平稳。利用这一过程确定要微调的层数。
- en: How should you determine whether to fine-tune or freeze all layers of your pre-trained
    model? Typically, when you’ve got a small dataset, it’s best to use the pre-trained
    model as a feature extractor rather than fine-tuning. If you’re retraining the
    weights of a model that was likely trained on thousands or millions of examples,
    fine-tuning can cause the updated model to overfit to your small dataset and lose
    the more general information learned from those millions of examples. Although
    it depends on your data and prediction task, when we say “small dataset” here,
    we’re referring to datasets with hundreds or a few thousand training examples.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如何确定是否要微调或冻结预训练模型的所有层？通常情况下，当你有一个小数据集时，最好将预训练模型作为特征提取器而不是进行微调。如果你重新训练一个模型的权重，而该模型很可能是在成千上万个示例上进行了训练，微调可能会导致更新后的模型对你的小数据集过拟合，并且丢失从那些成千上万个示例中学到的更一般性的信息。虽然这取决于你的数据和预测任务，但在这里所说的“小数据集”，是指具有数百或数千个训练示例的数据集。
- en: Another factor to take into account when deciding whether to fine-tune is how
    similar your prediction task is to that of the original pre-trained model you’re
    using. When the prediction task is similar or a continuation of the previous training,
    as it was in our movie review sentiment analysis model, fine-tuning can produce
    higher-accuracy results. When the task is different or the datasets are significantly
    different, it’s best to freeze all the layers of the pre-trained model instead
    of fine-tuning them. [Table 4-1](#criteria_to_help_choose_between_feature) summarizes
    the key points.^([4](ch04.xhtml#ch01fn19))
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定是否进行微调时，另一个要考虑的因素是您的预测任务与您正在使用的原始预训练模型的预测任务有多相似。当预测任务相似或者是前一训练的延续时，例如我们的电影评论情感分析模型，微调可以产生更高准确率的结果。当任务不同或数据集显著不同时，最好冻结所有预训练模型的层而不是进行微调。[Table 4-1](#criteria_to_help_choose_between_feature)
    总结了关键点。^([4](ch04.xhtml#ch01fn19))
- en: Table 4-1\. Criteria to help choose between feature extraction and fine-tuning
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Table 4-1\. 帮助选择特征提取和微调的标准
- en: '| Criterion | Feature extraction | Fine-tuning |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | 特征提取 | 微调 |'
- en: '| --- | --- | --- |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| How large is the dataset? | Small | Large |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 数据集大小如何？ | 小 | 大 |'
- en: '| Is your prediction task the same as that of the pre-trained model? | Different
    tasks | Same task, or similar task with same class distribution of labels |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 您的预测任务是否与预训练模型相同？ | 不同任务 | 相同任务，或具有相同类别分布标签的相似任务 |'
- en: '| Budget for training time and computational cost | Low | High |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 训练时间和计算成本预算 | 低 | 高 |'
- en: In our text example, the pre-trained model was trained on a corpus of news text
    but our use case was sentiment analysis. Because these tasks are different, we
    should use the original model as a feature extractor rather than fine-tune it.
    An example of different prediction tasks in an image domain might be using our
    MobileNet model trained on ImageNet as a basis for doing transfer learning on
    a dataset of medical images. Although both tasks involve image classification,
    the nature of the images in each dataset are very different.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的文本示例中，预训练模型是在新闻文本语料库上训练的，但我们的用例是情感分析。因为这些任务不同，我们应该将原始模型作为特征提取器而不是进行微调。在图像领域中不同预测任务的例子可能是使用我们在ImageNet上训练的MobileNet模型作为在医学图像数据集上进行迁移学习的基础。尽管这两个任务都涉及图像分类，但每个数据集中的图像性质却非常不同。
- en: Focus on image and text models
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 专注于图像和文本模型
- en: You may have noticed that all of the examples in this section focused on image
    and text data. This is because transfer learning is primarily for cases where
    you can apply a similar task to the same data domain. Models trained with tabular
    data, however, cover a potentially infinite number of possible prediction tasks
    and data types. You could train a model on tabular data to predict how you should
    price tickets to your event, whether or not someone is likely to default on loan,
    your company’s revenue next quarter, the duration of a taxi trip, and so forth.
    The specific data for these tasks is also incredibly varied, with the ticket problem
    depending on information about artists and venues, the loan problem on personal
    income, and the taxi duration on urban traffic patterns. For these reasons, there
    are inherent challenges in transferring the learnings from one tabular model to
    another.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，本节中所有的示例都集中在图像和文本数据上。这是因为迁移学习主要适用于可以将类似任务应用于相同数据域的情况。然而，使用表格数据训练的模型涵盖了潜在无限数量的可能预测任务和数据类型。例如，您可以训练一个模型来预测如何定价您的活动门票，是否有人可能会违约贷款，公司下个季度的收入，出租车行程的持续时间等等。这些任务的具体数据也非常多样化，定价问题取决于艺术家和场馆信息，贷款问题则取决于个人收入，出租车行程时间则取决于城市交通模式。因此，将一个表格模型的学习迁移到另一个表格模型中存在固有的挑战。
- en: Although transfer learning is not yet as common on tabular data as it is for
    image and text domains, a new model architecture called [TabNet](https://oreil.ly/HI5Xl)
    presents novel research in this area. Most tabular models require significant
    feature engineering when compared with image and text models. TabNet employs a
    technique that first uses unsupervised learning to learn representations for tabular
    features, and then fine-tunes these learned representations to produce predictions.
    In this way, TabNet automates feature engineering for tabular models.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管迁移学习在表格数据上尚不如在图像和文本领域普遍，但一种新的模型架构称为[TabNet](https://oreil.ly/HI5Xl)在这一领域提出了新的研究。与图像和文本模型相比，大多数表格模型需要进行重要的特征工程。TabNet采用一种技术，首先使用无监督学习来学习表格特征的表示，然后微调这些学习到的表示以生成预测。通过这种方式，TabNet自动化了表格模型的特征工程。
- en: Embeddings of words versus sentences
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入与句子嵌入
- en: 'In our discussion of text embeddings so far, we’ve referred mostly to *word*
    embeddings. Another type of text embedding is *sentence* embeddings. Where word
    embeddings represent individual words in a vector space, sentence embeddings represent
    entire sentences. Consequently, word embeddings are context agnostic. Let’s see
    how this plays out with the following sentence:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们讨论的文本嵌入中，我们大多数时候都在提及*词*嵌入。另一种文本嵌入是*句子*嵌入。词嵌入代表向量空间中的单个词，而句子嵌入则代表整个句子。因此，词嵌入是上下文无关的。让我们看看以下句子的表现：
- en: '*“I’ve left you fresh baked cookies on the left side of the kitchen counter.”*'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '*“我把新鲜烤饼放在厨房柜台左侧。”*'
- en: Notice that the word *left* appears twice in that sentence, first as a verb
    and then as an adjective. If we were to generate word embeddings for this sentence,
    we’d get a separate array for each word. With word embeddings, the array for both
    instances of the word *left* would be the same. Using sentence-level embeddings,
    however, we’d get a single vector to represent the entire sentence. There are
    several approaches for generating sentence embeddings—from averaging a sentence’s
    word embeddings to training a supervised learning model on a large corpus of text
    to generate the embeddings.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在那个句子中，单词*left*出现了两次，首先是动词，然后是形容词。如果我们为这个句子生成词嵌入，我们会得到每个单词的单独数组。使用词嵌入时，*left*这个词的两个实例的数组是相同的。然而，使用句子级嵌入，我们会得到一个单一的向量来代表整个句子。有几种方法可以生成句子嵌入——从对句子的词嵌入进行平均到训练一个大型语料库上的监督学习模型以生成嵌入。
- en: How does this relate to transfer learning? The latter method—training a supervised
    learning model to generate sentence-level embeddings—is actually a form of transfer
    learning. This is the approach used by Google’s [Universal Sentence Encoder](https://oreil.ly/Y0Ry9)
    (available in TF Hub) and [BERT](https://oreil.ly/l_gQf). These methods differ
    from word embeddings in that they go beyond simply providing a weight lookup for
    individual words. Instead, they have been built by training a model on a large
    dataset of varied text to understand the meaning conveyed by *sequences* of words.
    In this way, they are designed to be transferred to different natural language
    tasks and can thus be used to build models that implement transfer learning.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这与迁移学习有何关联？后一种方法——训练监督学习模型以生成句子级嵌入——实际上是一种迁移学习形式。这是Google的[通用句子编码器](https://oreil.ly/Y0Ry9)（可在TF
    Hub中找到）和[BERT](https://oreil.ly/l_gQf)采用的方法。这些方法与词嵌入不同，它们不仅仅是为单个词提供权重查找。相反，它们通过在大量文本数据集上训练模型来理解*词序列*传达的含义。这样，它们被设计用于在不同的自然语言任务中进行迁移，并因此可用于构建实施迁移学习的模型。
- en: 'Design Pattern 14: Distribution Strategy'
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '设计模式 14: 分布策略'
- en: In Distribution Strategy*,* the training loop is carried out at scale over multiple
    workers, often with caching, hardware acceleration, and parallelization.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布策略*中*，训练循环在多个工作节点上按比例进行，通常包括缓存、硬件加速和并行化。
- en: Problem
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: These days, it’s common for large neural networks to have millions of parameters
    and be trained on massive amounts of data. In fact, it’s been shown that increasing
    the scale of deep learning, with respect to the number of training examples, the
    number of model parameters, or both, drastically improves model performance. However,
    as the size of models and data increases, the computation and memory demands increase
    proportionally, making the time it takes to train these models one of the biggest
    problems of deep learning.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这些天，大型神经网络通常具有数百万个参数，并且在大量数据上进行训练。事实上，已经证明，随着深度学习规模的增加，无论是训练示例的数量、模型参数的数量还是两者兼而有之，都会显著提高模型性能。然而，随着模型和数据规模的增加，计算和内存需求成比例增加，这使得训练这些模型所需的时间成为深度学习的最大问题之一。
- en: GPUs provide a substantial computational boost and bring the training time of
    modestly sized deep neural networks within reach. However, for very large models
    trained on massive amounts of data, individual GPUs aren’t enough to make the
    training time tractible. For example, at the time of writing, training ResNet-50
    on the benchmark ImageNet dataset for 90 epochs on a single NVIDIA M40 GPU requires
    10^(18) single precision operations and takes 14 days. As AI is being used more
    and more to solve problems within complex domains, and open source libraries like
    Tensorflow and PyTorch make building deep learning models more accessible, large
    neural networks comparable to ResNet-50 have become the norm.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 提供了显著的计算提升，并使得适度大小的深度神经网络的训练时间可以接受。然而，对于在大量数据上训练的非常大型模型，单个 GPU 是不足以使训练时间变得可行的。例如，在撰写本文时，使用单个
    NVIDIA M40 GPU 对 ImageNet 数据集上的 ResNet-50 进行 90 个周期的训练需要进行 10^(18) 个单精度操作，并且需要
    14 天的时间。随着人工智能在解决复杂领域内的问题中的应用越来越广泛，以及像 TensorFlow 和 PyTorch 这样的开源库使得构建深度学习模型更加可访问，类似
    ResNet-50 的大型神经网络已经成为常态。
- en: This is a problem. If it takes two weeks to train your neural network, then
    you have to wait two weeks before you can iterate on new ideas or experiment with
    tweaking the settings. Furthermore, for some complex problems like medical imaging,
    autonomous driving, or language translation, it’s not always feasible to break
    the problem down into smaller components or work with only a subset of the data.
    It’s only with the full scale of the data that you can assess whether things work
    or not.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个问题。如果训练你的神经网络需要两周的时间，那么在你可以迭代新想法或尝试调整设置之前，你必须等待两周。此外，对于一些复杂的问题，如医学影像、自动驾驶或语言翻译，将问题分解成较小的组件或仅使用数据子集并不总是可行的。只有使用完整的数据规模，你才能评估事物是否有效。
- en: Training time translates quite literally to money. In the world of serverless
    machine learning, rather than buying your own expensive GPU, it is possible to
    submit training jobs via a cloud service where you are charged for training time.
    The cost of training a model, whether it is to pay for a GPU or to pay for a serverless
    training service, quickly adds up.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间直接转化为金钱。在无服务器机器学习世界中，与其购买昂贵的 GPU，不如通过云服务提交训练作业，您将按照训练时间付费。无论是为了购买 GPU 还是为了购买无服务器训练服务，训练模型的成本都会迅速累积。
- en: Is there a way to speed up the training of these large neural networks?
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有办法加速这些大型神经网络的训练？
- en: Solution
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'One way to accelerate training is through distribution strategies in the training
    loop. There are different distribution techniques, but the common idea is to split
    the effort of training the model across multiple machines. There are two ways
    this can be done: *data parallelism* and *model parallelism*. In data parallelism,
    computation is split across different machines and different workers train on
    different subsets of the training data. In model parallelism, the model is split
    and different workers carry out the computation for different parts of the model.
    In this section, we’ll focus on data parallelism and show implementations in TensorFlow
    using the `tf.distribute.Strategy` library. We’ll discuss model parallelism in
    [“Trade-Offs and Alternatives”](#tradeoffs_and_alternatives-id00195).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 加速训练的一种方法是通过训练循环中的分发策略。有不同的分发技术，但共同的想法是将训练模型的工作分散到多台机器上。有两种方法可以实现这一点：*数据并行*
    和 *模型并行*。在数据并行中，计算被分割到不同的机器上，并且不同的工作节点在训练数据的不同子集上进行训练。在模型并行中，模型被分割，并且不同的工作节点负责模型的不同部分的计算。在本节中，我们将专注于数据并行，并展示如何在
    TensorFlow 中使用 `tf.distribute.Strategy` 库进行实现。我们将在 [“权衡和替代方案”](#tradeoffs_and_alternatives-id00195)
    中讨论模型并行。
- en: To implement data parallelism, there must be a method in place for different
    workers to compute gradients and share that information to make updates to the
    model parameters. This ensures that all workers are consistent and each gradient
    step works to train the model. Broadly speaking, data parallelism can be carried
    out either synchronously or asynchronously.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现数据并行，必须有一种方法让不同的工作节点计算梯度并共享该信息，以更新模型参数。这确保了所有工作节点的一致性，并使每个梯度步骤都能有效地训练模型。广义上说，数据并行可以同步或异步进行。
- en: Synchronous training
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 同步训练
- en: In synchronous training, the workers train on different slices of input data
    in parallel and the gradient values are aggregated at the end of each training
    step. This is performed via an *all-reduce* algorithm. This means that each worker,
    typically a GPU, has a copy of the model on device and, for a single stochastic
    gradient descent (SGD) step, a mini-batch of data is split among each of the separate
    workers. Each device performs a forward pass with their portion of the mini-batch
    and computes gradients for each parameter of the model. These locally computed
    gradients are then collected from each device and aggregated (for example, averaged)
    to produce a single gradient update for each parameter. A central server holds
    the most current copy of the model parameters and performs the gradient step according
    to the gradients received from the multiple workers. Once the model parameters
    are updated according to this aggregated gradient step, the new model is sent
    back to the workers along with another split of the next mini-batch, and the process
    repeats. [Figure 4-15](#in_synchronous_training_each_worker_hol) shows a typical
    all-reduce architecture for synchronous data distribution.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在同步训练中，工作节点并行训练不同的输入数据片段，并且在每个训练步骤结束时聚合梯度值。这是通过*全局归约*算法实现的。这意味着每个工作节点（通常是 GPU）都有设备上的模型副本，对于单个随机梯度下降（SGD）步骤，一小批数据被分割给各个不同的工作节点。每个设备使用其部分的小批量数据进行前向传递，并计算模型的每个参数的梯度。然后从每个设备收集并聚合这些局部计算的梯度（例如，平均值），以生成每个参数的单个梯度更新。中央服务器保存模型参数的最新副本，并根据从多个工作节点收到的梯度执行梯度步骤。一旦根据这个聚合梯度步骤更新了模型参数，新模型将与下一小批量数据的另一个分割一起发送回工作节点，并且这个过程重复进行。[图 4-15](#in_synchronous_training_each_worker_hol)展示了同步数据分布的典型全局归约架构。
- en: As with any parallelism strategy, this introduces additional overhead to manage
    timing and communication between workers. Large models could cause I/O bottlenecks
    as data is passed from the CPU to the GPU during training, and slow networks could
    also cause delays.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何并行策略一样，这会引入额外的开销来管理工作节点之间的时间和通信。大型模型可能会导致 I/O 瓶颈，因为在训练期间从 CPU 传输数据到 GPU，并且慢网络也可能会引起延迟。
- en: In TensorFlow, `tf.distribute.MirroredStrategy` supports synchronous distributed
    training across multiple GPUs on the same machine. Each model parameter is mirrored
    across all workers and stored as a single conceptual variable called `MirroredVariable`.
    During the all-reduce step, all gradient tensors are made available on each device.
    This helps to significantly reduce the overhead of synchronization. There are
    also various other implementations for the all-reduce algorithm available, many
    of which use [NVIDIA NCCL](https://oreil.ly/HX4NE).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，`tf.distribute.MirroredStrategy` 支持在同一台机器上多个 GPU 上进行同步分布式训练。每个模型参数都会在所有工作节点上进行镜像并存储为一个称为
    `MirroredVariable` 的概念变量。在全局归约步骤中，所有梯度张量都会在每个设备上可用。这有助于显著减少同步的开销。还有许多其他的全局归约算法实现可用，其中许多使用[NVIDIA
    NCCL](https://oreil.ly/HX4NE)。
- en: '![In synchronous training, each worker holds a copy of the model and computes
    gradients using a slice of the training data mini-batch.](Images/mldp_0415.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![在同步训练中，每个工作节点持有模型副本，并使用训练数据小批量的一个片段计算梯度。](Images/mldp_0415.png)'
- en: Figure 4-15\. In synchronous training, each worker holds a copy of the model
    and computes gradients using a slice of the training data mini-batch.
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-15\. 在同步训练中，每个工作节点持有模型副本，并使用训练数据小批量的一个片段计算梯度。
- en: 'To implement this mirrored strategy in Keras, you first create an instance
    of the mirrored distribution strategy, then move the creation and compiling of
    the model inside the scope of that instance. The following code shows how to use
    `MirroredStrategy` when training a three-layer neural network:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Keras 中实现这种镜像策略，首先创建一个镜像分布策略的实例，然后将模型的创建和编译移动到该实例的范围内。以下代码展示了如何在训练三层神经网络时使用
    `MirroredStrategy`：
- en: '[PRE20]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'By creating the model inside this scope, the parameters of the model are created
    as mirrored variables instead of regular variables. When it comes to fitting the
    model on the dataset, everything is performed exactly the same as before. The
    model code stays the same! Wrapping the model code in the distribution strategy
    scope is all you need to do to enable distributed training. The `MirroredStrategy`
    handles replicating the model parameters on the available GPUs, aggregating gradients,
    and more. To train or evaluate the model, we just call `fit()` or `evaluate()`
    as usual:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在此范围内创建模型时，模型参数被创建为镜像变量而不是常规变量。在将模型拟合到数据集时，一切操作与之前完全相同。模型代码保持不变！将模型代码包装在分发策略范围内即可启用分布式训练。`MirroredStrategy`
    处理在可用GPU上复制模型参数、聚合梯度等工作。要训练或评估模型，只需像往常一样调用 `fit()` 或 `evaluate()`：
- en: '[PRE21]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: During training, each batch of the input data is divided equally among the multiple
    workers. For example, if you are using two GPUs, then a batch size of 10 will
    be split among the 2 GPUs, with each receiving 5 training examples each step.
    There are also other synchronous distribution strategies within Keras, such as
    `CentralStorageStrategy` and `MultiWorkerMirroredStrategy`. `MultiWorkerMirroredStrategy`
    enables the distribution to be spread not just on GPUs on a single machine, but
    on multiple machines. In `CentralStorageStrategy`, the model variables are not
    mirrored; instead, they are placed on the CPU and operations are replicated across
    all local GPUs. So the variable updates only happen in one place.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 训练期间，每个输入数据批次均平均分配给多个工作进程。例如，如果使用两个GPU，则批量大小为10的数据将在这两个GPU之间分割，每个GPU每步接收5个训练样本。Keras
    中还有其他同步分发策略，如 `CentralStorageStrategy` 和 `MultiWorkerMirroredStrategy`。`MultiWorkerMirroredStrategy`
    不仅能够在单台机器上的GPU上进行分布，还能在多台机器上进行分布。在 `CentralStorageStrategy` 中，模型变量不是镜像的；相反，它们被放置在CPU上，并且操作在所有本地GPU上被复制。因此，变量更新仅发生在一个地方。
- en: When choosing between different distribution strategies, the best option depends
    on your computer topology and how fast the CPUs and GPUs can communicate with
    one another. [Table 4-2](#choosing_between_distribution_strategie) summarizes
    how the different strategies described here compare on these criteria.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择不同的分发策略时，最佳选项取决于您的计算机拓扑结构以及CPU和GPU之间通信的速度。[表 4-2](#choosing_between_distribution_strategie)
    总结了这些策略在这些标准下的比较情况。
- en: Table 4-2\. Choosing between distribution strategies depends on your computer
    topology and how fast the CPUs and GPUs can communicate with one another
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-2\. 在选择分发策略时，取决于您的计算机拓扑结构以及CPU和GPU之间通信的速度
- en: '|  | Faster CPU-GPU connection | Faster GPU-GPU connection |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | 更快的 CPU-GPU 连接 | 更快的 GPU-GPU 连接 |'
- en: '| --- | --- | --- |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| One machine with multiple GPUs | `CentralStorageStrategy` | `MirroredStrategy`
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 单台机器多个GPU | `CentralStorageStrategy` | `MirroredStrategy` |'
- en: '| Multiple machines with multiple GPUs | `MultiWorkerMirroredStrategy` | `MultiWorkerMirroredStrategy`
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 多台机器多个GPU | `MultiWorkerMirroredStrategy` | `MultiWorkerMirroredStrategy`
    |'
- en: Asynchronous training
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异步训练
- en: In asynchronous training, the workers train on different slices of the input
    data independently, and the model weights and parameters are updated asynchronously,
    typically through a [parameter server architecture](https://oreil.ly/Wkk5B). This
    means that no one worker waits for updates to the model from any of the other
    workers. In the parameter-server architecture, there is a single parameter server
    that manages the current values of the model weights, as in [Figure 4-16](#in_asynchronous_trainingcomma_each_work).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在异步训练中，各工作进程独立地训练不同切片的输入数据，并且模型权重和参数通过异步方式更新，通常通过 [参数服务器架构](https://oreil.ly/Wkk5B)
    实现。这意味着没有一个工作进程需要等待来自其他工作进程的模型更新。在参数服务器架构中，有一个单一的参数服务器管理模型权重的当前值，如 [图 4-16](#in_asynchronous_trainingcomma_each_work)。
- en: As with synchronous training, a mini-batch of data is split among each of the
    separate workers for each SGD step. Each device performs a forward pass with their
    portion of the mini-batch and computes gradients for each parameter of the model.
    Those gradients are sent to the parameter server, which performs the parameter
    update and then sends the new model parameters back to the worker with another
    split of the next mini-batch.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 与同步训练类似，每个 SGD 步骤的数据小批量被分割并分配给每个独立的工作进程。每个设备使用其分配的数据小批量进行前向传播，并计算模型参数的梯度。这些梯度被发送到参数服务器，执行参数更新，然后将新的模型参数发送回工作进程，以进行下一个数据小批量的处理分割。
- en: 'The key difference between synchronous and asynchronous training is that the
    parameter server does not do an *all*-reduce. Instead, it computes the new model
    parameters periodically based on whichever gradient updates it received since
    the last computation. Typically, asynchronous distribution achieves higher throughput
    than synchronous training because a slow worker doesn’t block the progression
    of training steps. If a single worker fails, the training continues as planned
    with the other workers while that worker reboots. As a result, some splits of
    the mini-batch may be lost during training, making it too difficult to accurately
    keep track of how many epochs of data have been processed. This is another reason
    why we typically specify virtual epochs when training large distributed jobs instead
    of epochs; see [“Design Pattern 12: Checkpoints”](#design_pattern_onetwo_checkpoints)
    for a discussion of virtual epochs.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 同步训练与异步训练的关键区别在于参数服务器不执行*all*-reduce操作。相反，它根据自上次计算以来接收到的梯度更新周期性地计算新的模型参数。通常，异步分发比同步训练实现更高的吞吐量，因为慢速工作节点不会阻塞训练步骤的进展。如果一个工作节点失败，训练将继续计划中的其他工作节点进行，而该工作节点重新启动。因此，在训练过程中可能会丢失一些小批量的分割，这使得准确跟踪处理了多少个epoch的数据变得困难。这也是为什么我们在训练大型分布式作业时通常指定虚拟epoch而不是真正的epoch的另一个原因；请参阅[“设计模式12：检查点”](#design_pattern_onetwo_checkpoints)以讨论虚拟epoch。
- en: '![In asynchronous training, each worker performs a gradient descent step with
    a split of the mini-batch. No one worker waits for updates to the model from any
    of the other workers.](Images/mldp_0416.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![在异步训练中，每个工作节点使用小批量的梯度下降步骤，没有一个工作节点等待来自其他工作节点的模型更新。](Images/mldp_0416.png)'
- en: Figure 4-16\. In asynchronous training, each worker performs a gradient descent
    step with a split of the mini-batch. No one worker waits for updates to the model
    from any of the other workers.
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 在异步训练中，每个工作节点使用小批量的梯度下降步骤，没有一个工作节点等待来自其他工作节点的模型更新。
- en: In addition, since there is no synchronization between the weight updates, it
    is possible that one worker updates the model weights based on stale model state.
    However, in practice, this doesn’t seem to be a problem. Typically, large neural
    networks are trained for multiple epochs, and these small discrepancies become
    negligible in the end.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于权重更新之间没有同步，一个工作节点基于过时的模型状态可能更新模型权重。然而，在实践中，这似乎不是一个问题。通常，大型神经网络经过多个epoch的训练，这些小的差异最终变得可以忽略不计。
- en: In Keras, `ParameterServerStrategy` implements asynchronous parameter server
    training on multiple machines. When using this distribution, some machines are
    designated as workers and some are held as parameter servers. The parameter servers
    hold each variable of the model, and computation is performed on the workers,
    typically GPUs.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，`ParameterServerStrategy`在多台机器上实现异步参数服务器训练。在使用此分发方式时，一些机器被指定为工作节点，而另一些机器则作为参数服务器。参数服务器保存模型的每个变量，并在工作节点上执行计算，通常是在GPU上。
- en: The implementation is similar to that of other distribution strategies in Keras.
    For example, in your code, you would just replace `MirroredStrategy()` with `ParameterServerStrategy()`.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 实现与Keras中其他分发策略类似。例如，在您的代码中，只需用`ParameterServerStrategy()`替换`MirroredStrategy()`。
- en: Tip
  id: totrans-288
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Tip
- en: Another distribution strategy supported in Keras worth mentioning is `OneDeviceStrategy`.
    This strategy will place any variables created in its scope on the specified device.
    This strategy is particularly useful as a way to test your code before switching
    to other strategies that actually distribute to multiple devices/machines.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在Keras中支持的值得一提的分发策略是`OneDeviceStrategy`。此策略将其作用域内创建的任何变量放置在指定的设备上。这种策略在切换到实际分发到多个设备/机器的其他策略之前，作为测试代码的一种有效方式。
- en: Synchronous and asynchronous training each have their advantages, and disadvantages
    and choosing between the two often comes down to hardware and network limitations.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 同步和异步训练各有其优缺点，选择其中之一往往取决于硬件和网络限制。
- en: Synchronous training is particularly vulnerable to slow devices or poor network
    connection because training will stall waiting for updates from all workers. This
    means synchronous distribution is preferable when all devices are on a single
    host and there are fast devices (for example, TPUs or GPUs) with strong links.
    On the other hand, asynchronous distribution is preferable if there are many low-power
    or unreliable workers. If a single worker fails or stalls in returning a gradient
    update, it won’t stall the training loop. The only limitation is I/O constraints.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 同步训练对于低速设备或者网络连接差的情况尤为脆弱，因为训练会因为等待所有工作节点更新而停滞不前。这意味着在所有设备都在单个主机上且具有高速设备（例如TPU或GPU）和强大链接时，同步分发是首选。另一方面，如果存在许多低功率或不可靠的工作节点，异步分发则更为合适。如果单个工作节点失败或者在返回梯度更新时停滞，它不会阻碍整个训练循环。唯一的限制是I/O约束。
- en: Why It Works
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为何这样做有效
- en: Large, complex neural networks require massive amounts of training data to be
    effective. Distributed training schemes drastically increase the throughput of
    data processed by these models and can effectively decrease training times from
    weeks to hours. Sharing resources between workers and parameter server tasks leads
    to a dramatic increase in data throughput. [Figure 4-17](#comparison_of_throughput_between_differ)
    compares the throughput of training data, in this case images, with different
    distribution setups.^([5](ch04.xhtml#ch01fn20)) Most notable is that throughput
    increases with the number of worker nodes and, even though parameter servers perform
    tasks not related to the computation done on the GPU’s workers, splitting the
    workload among more machines is the most advantageous strategy.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 大型复杂神经网络需要大量训练数据才能有效果。分布式训练方案显著增加了这些模型处理的数据吞吐量，并且可以有效地将训练时间从几周缩短到几小时。在工作节点和参数服务器任务之间共享资源会大幅增加数据吞吐量。图4-17比较了不同分发设置下训练数据吞吐量的情况，例如图像。^([5](ch04.xhtml#ch01fn20))
    最显著的是，随着工作节点数量的增加，吞吐量也在增加，即使参数服务器执行的任务与GPU工作节点上的计算无关，将工作负载分配给更多机器仍然是最有利的策略。
- en: In addition, data parallelization decreases time to convergence during training.
    In a similar study, it was shown that increasing workers leads to minimum loss
    much faster.^([6](ch04.xhtml#ch01fn21)) [Figure 4-18](#as_the_number_of_gpus_increasescomma_th)
    compares the time to minimum for different distribution strategies. As the number
    of workers increases, the time to minimum training loss dramatically decreases,
    showing nearly a 5× speed up with 8 workers as opposed to just 1.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据并行化可以在训练期间加快收敛时间。在类似的研究中，显示增加工作节点会更快地达到最小损失^([6](ch04.xhtml#ch01fn21))。图4-18比较了不同分发策略下达到最小训练损失所需的时间。随着工作节点数量的增加，达到最小训练损失的时间显著减少，仅使用8个工作节点比使用1个工作节点速度提升了近5倍。
- en: '![Comparison of throughput between different distribution setups. Here, 2W1PS
    indicates 2 workers and 1 parameter server.](Images/mldp_0417.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![比较不同分发设置之间的吞吐量。这里，2W1PS表示2个工作节点和1个参数服务器。](Images/mldp_0417.png)'
- en: Figure 4-17\. Comparison of throughput between different distribution setups.
    Here, 2W1PS indicates two workers and one parameter server.
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-17。比较不同分发设置之间的吞吐量。这里，2W1PS表示两个工作节点和一个参数服务器。
- en: '![As the number of GPUs increases, the time to convergence during training
    decreases.](Images/mldp_0418.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![随着GPU数量的增加，训练收敛时间缩短。](Images/mldp_0418.png)'
- en: Figure 4-18\. As the number of GPUs increases, the time to convergence during
    training decreases.
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-18。随着GPU数量的增加，训练收敛时间缩短。
- en: Trade-Offs and Alternatives
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权衡与替代方案
- en: In addition to data parallelism, there are other aspects of distribution to
    consider, such as model parallelism, other training accelerators—(such as TPUs)
    and other considerations (such as I/O limitations and batch size).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据并行化外，还有其他分发方式需要考虑，例如模型并行化、其他训练加速器（例如TPU）以及其他因素（例如I/O限制和批处理大小）。
- en: Model parallelism
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型并行化
- en: In some cases, the neural network is so large it cannot fit in the memory of
    a single device; for example, [Google’s Neural Machine Translation](https://oreil.ly/xL4Cu)
    has billions of parameters. In order to train models this big, they must be split
    up over multiple devices,^([7](ch04.xhtml#ch01fn22)) as shown in [Figure 4-19](#model_parallelism_partitions_the_model).
    This is called *model parallelism*. By partitioning parts of a network and their
    associated computations across multiple cores, the computation and memory workload
    is distributed across multiple devices. Each device operates over the same mini-batch
    of data during training, but carries out computations related only to their separate
    components of the model.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，神经网络非常庞大，无法适应单个设备的内存；例如，[Google的神经机器翻译](https://oreil.ly/xL4Cu)拥有数十亿个参数。为了训练这么大的模型，必须将其分割到多个设备上，如[图 4-19](#model_parallelism_partitions_the_model)所示。这称为*模型并行性*。通过将网络的部分及其相关计算分布到多个核心上，计算和内存负载分布到多个设备上。每个设备在训练过程中处理相同的小批量数据，但仅执行与模型各部分相关的计算。
- en: '![Model parallelism partitions the model over multiple devices.](Images/mldp_0419.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![模型并行性将模型分割到多个设备上。](Images/mldp_0419.png)'
- en: Figure 4-19\. Model parallelism partitions the model over multiple devices.
  id: totrans-304
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-19\. 模型并行性将模型分割到多个设备上。
- en: ASICs for better performance at lower cost
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ASIC提升性能降低成本
- en: Another way to speed up the training process is by accelerating the underlying
    hardware, such as by using application-specific integrated circuits (ASICs). In
    machine learning, this refers to hardware components designed specifically to
    optimize performance on the types of large matrix computations at the heart of
    the training loop. TPUs in Google Cloud are ASICs that can be used for both model
    training and making predictions. Similarly, Microsoft Azure offers the Azure FPGA
    (field-programmable gate array), which is also a custom machine learning chip
    like the ASIC except that it can be reconfigured over time. These chips are able
    to vastly minimize the time to accuracy when training large, complex neural network
    models. A model that takes two weeks to train on GPUs can converge in hours on
    TPUs.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 加速训练过程的另一种方法是加速底层硬件，例如使用特定应用集成电路（ASIC）。在机器学习中，这指的是专门设计的硬件组件，旨在优化训练循环核心的大型矩阵计算性能。Google
    Cloud中的TPU是既用于模型训练又用于预测的ASIC。类似地，Microsoft Azure提供Azure FPGA（现场可编程门阵列），它也是一种像ASIC一样的自定义机器学习芯片，但可以随时间重新配置。这些芯片能够极大地缩短在大型复杂神经网络模型上训练的时间至准确度。在GPU上训练两周的模型，在TPU上几小时内就可以收敛。
- en: There are other advantages to using custom machine learning chips. For example,
    as accelerators (GPUs, FPGAs, TPUs, and so on) have gotten faster, I/O has become
    a significant bottleneck in ML training. Many training processes waste cycles
    waiting to read and move data to the accelerator and waiting for gradient updates
    to carry out all-reduce. TPU pods have high-speed interconnect, so we tend to
    not worry about communication overhead within a pod (a pod consists of thousands
    of TPUs). In addition, there is lots of memory available on-disk, which means
    that it is possible to preemptively fetch data and make less-frequent calls to
    the CPU. As a result, you should use much higher batch sizes to take full advantage
    of high-memory, high-interconnect chips like TPUs.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自定义机器学习芯片还有其他优势。例如，随着加速器（GPU、FPGA、TPU等）的速度提升，I/O成为ML训练中的一个重要瓶颈。许多训练过程浪费时间等待读取和移动数据到加速器，等待梯度更新进行全局归约。TPU
    Pod具有高速互联，因此我们不太担心Pod内部的通信开销（一个Pod由数千个TPU组成）。此外，磁盘上有大量的可用内存，这意味着可以预先获取数据，并减少对CPU的调用次数。因此，应使用更大的批量大小，以充分利用像TPU这样高内存、高互联的芯片。
- en: In terms of distributed training, `TPUStrategy` allows you to run distributed
    training jobs on TPUs. Under the hood, `TPUStrategy` is the same as `MirroredStrategy`
    although TPUs have their own implementation of the all-reduce algorithm.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式训练方面，`TPUStrategy`允许您在TPU上运行分布式训练作业。在内部，`TPUStrategy`与`MirroredStrategy`相同，尽管TPU有自己的全局归约算法实现。
- en: 'Using `TPUStrategy` is similar to using the other distribution strategies in
    TensorFlow. One difference is you must first set up a `TPUClusterResolver`, which
    points to the location of the TPUs. TPUs are currently available to use for free
    in Google Colab, and there you don’t need to specify any arguments for `tpu_address`:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`TPUStrategy`与在TensorFlow中使用其他分布策略类似。其中一个区别在于，您必须首先设置`TPUClusterResolver`，指向TPU的位置。目前在Google
    Colab上可以免费使用TPU，而在那里您不需要为`tpu_address`指定任何参数。
- en: '[PRE22]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Choosing a batch size
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择批量大小
- en: Another important factor to consider is batch size. Particular to synchronous
    data parallelism, when the model is particularly large, it’s better to decrease
    the total number of training iterations because each training step requires the
    updated model to be shared among different workers, causing a slowdown for transfer
    time. Thus, it’s important to increase the mini-batch size as much as possible
    so that the same performance can be met with fewer steps.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的重要因素是批量大小。特别是对于同步数据并行性，当模型特别大时，最好减少总训练迭代次数，因为每个训练步骤需要更新后的模型在不同的工作节点之间共享，从而导致传输时间减慢。因此，尽可能增加小批量大小非常重要，这样可以通过更少的步骤实现相同的性能。
- en: However, [it has been shown](https://oreil.ly/FOtIX) that very large batch sizes
    adversely affect the rate at which stochastic gradient descent converges as well
    as the quality of the final solution.^([8](ch04.xhtml#ch01fn23)) [Figure 4-20](#large_batch_sizes_have_been_shown_to_ad)
    shows that increasing the batch size alone ultimately causes the top-1 validation
    error to increase. In fact, they argue that linearly scaling the learning rate
    as a function of the large batch size is necessary to maintain a low validation
    error while decreasing the time of distributed training.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，[已经表明](https://oreil.ly/FOtIX)非常大的批量大小会对随机梯度下降的收敛速度以及最终解的质量产生不利影响。^[8](ch04.xhtml#ch01fn23)
    [图 4-20](#large_batch_sizes_have_been_shown_to_ad) 显示，仅增加批量大小最终导致了 top-1 验证错误的增加。事实上，他们认为，线性缩放学习率作为大批量大小的函数是必要的，以保持低验证错误同时减少分布式训练时间。
- en: '![Large batch sizes have been shown to adversely affect the quality of the
    final trained model.](Images/mldp_0420.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![大批量训练已被证明会对最终训练模型的质量产生不利影响。](Images/mldp_0420.png)'
- en: Figure 4-20\. Large batch sizes have been shown to adversely affect the quality
    of the final trained model.
  id: totrans-315
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-20\. 大批量训练已被证明会对最终训练模型的质量产生不利影响。
- en: Thus, setting the mini-batch size in the context of distributed training is
    a complex optimization space of its own, as it affects both statistical accuracy
    (generalization) and hardware efficiency (utilization) of the model. [Related
    work](https://oreil.ly/yeALI), focusing on this optimization, introduces a layerwise
    adaptive large batch optimization technique called LAMB, which has been able to
    reduce BERT training time from 3 days to just 76 minutes.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在分布式训练环境中设置小批量大小是一个复杂的优化空间，因为它既影响模型的统计精度（泛化），又影响硬件效率（利用率）。[相关工作](https://oreil.ly/yeALI)专注于此优化，引入了一种称为LAMB的逐层自适应大批量优化技术，能够将BERT的训练时间从3天缩短到仅76分钟。
- en: Minimizing I/O waits
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小化I/O等待
- en: 'GPUs and TPUs can process data much faster than CPUs, and when using distributed
    strategies with multiple accelerators, I/O pipelines can struggle to keep up,
    creating a bottleneck to more efficient training. Specifically, before a training
    step finishes, the data for the next step is not available for processing. This
    is shown in [Figure 4-21](#with_distributed_training_on_multiple_g). The CPU handles
    the input pipeline: reading data from storage, preprocessing, and sending to the
    accelerator for computation. As distributed strategies speed up training, more
    than ever it becomes necessary to have efficient input pipelines to fully utilize
    the computing power available.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: GPU和TPU可以比CPU更快地处理数据，在使用多个加速器的分布式策略时，I/O管道可能难以跟上，从而创建更有效的训练瓶颈。具体来说，在一个训练步骤完成之前，下一个步骤的数据尚未准备好进行处理。这在[图 4-21](#with_distributed_training_on_multiple_g)中有所展示。CPU处理输入管道：从存储中读取数据，预处理并发送到加速器进行计算。随着分布式策略加快训练速度，更有必要拥有高效的输入管道来充分利用可用的计算能力。
- en: This can be achieved in a number of ways, including using optimized file formats
    like TFRecords and building data pipelines using the TensorFlow `tf.data` API.
    The `tf.data` API makes it possible to handle large amounts of data and has built-in
    transformations useful for creating flexible, efficient pipelines. For example,
    `tf.data.Dataset.prefetch` overlaps the preprocessing and model execution of a
    training step so that while the model is executing training step *N*, the input
    pipeline is reading and preparing data for training step *N + 1*, as shown in
    [Figure 4-22](#prefetching_overlaps_preprocessing_and).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过多种方式实现这一目标，包括使用优化的文件格式如 TFRecords，并使用 TensorFlow 的 `tf.data` API 构建数据管道。`tf.data`
    API 能够处理大量数据，并且具有内置的转换功能，有助于创建灵活、高效的管道。例如，`tf.data.Dataset.prefetch` 可以使预处理和模型执行在训练步骤中重叠，因此当模型执行训练步骤
    *N* 时，输入管道正在读取和准备训练步骤 *N + 1* 的数据，如图 [4-22](#prefetching_overlaps_preprocessing_and)
    所示。
- en: '![With distributed training on multiple GPU/TPUs available, it is necessary
    to have efficient input pipelines. ](Images/mldp_0421.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![通过多个 GPU/TPU 进行分布式训练，需要高效的输入管道。](Images/mldp_0421.png)'
- en: Figure 4-21\. With distributed training on multiple GPU/TPUs available, it is
    necessary to have efficient input pipelines.
  id: totrans-321
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-21\. 通过多个 GPU/TPU 进行分布式训练，需要高效的输入管道。
- en: '![Prefetching overlaps preprocessing and model execution, so that while the
    model is executing one training step, the input pipeline is reading and preparing
    data for the next.](Images/mldp_0422.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![预取重叠预处理和模型执行，因此当模型执行一个训练步骤时，输入管道正在读取和准备下一个步骤的数据。](Images/mldp_0422.png)'
- en: Figure 4-22\. Prefetching overlaps preprocessing and model execution, so that
    while the model is executing one training step, the input pipeline is reading
    and preparing data for the next.
  id: totrans-323
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-22\. 预取重叠预处理和模型执行，因此当模型执行一个训练步骤时，输入管道正在读取和准备下一个步骤的数据。
- en: 'Design Pattern 15: Hyperparameter Tuning'
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 15：超参数调优
- en: In Hyperparameter Tuning, the training loop is itself inserted into an optimization
    method to find the optimal set of model hyperparameters.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在超参数调优中，训练循环本身被插入到一种优化方法中，以找到最优的模型超参数集。
- en: Problem
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: In machine learning, model training involves finding the optimal set of breakpoints
    (in the case of decision trees), weights (in the case of neural networks), or
    support vectors (in the case of support vector machines). We term these *model*
    parameters. However, in order to carry out model training and find the optimal
    model parameters, we often have to hardcode a variety of things. For example,
    we might decide that the maximum depth of a tree will be 5 (in the case of decision
    trees), or that the activation function will be ReLU (for neural networks) or
    choose the set of kernels that we will employ (in SVMs). These parameters are
    called *hyperparameters*.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，模型训练涉及寻找最优的断点集（决策树的情况下）、权重（神经网络的情况下）或支持向量（支持向量机的情况下）。我们称这些为 *模型* 参数。然而，为了进行模型训练并找到最优的模型参数，我们经常需要硬编码各种事物。例如，我们可能决定树的最大深度为
    5（决策树的情况下），或者激活函数将为 ReLU（神经网络的情况下），或选择我们将使用的核函数集（在 SVM 中）。这些参数称为 *超参数*。
- en: Model parameters refer to the weights and biases learned by your model. You
    do not have direct control over model parameters, since they are largely a function
    of your training data, model architecture, and many other factors. In other words,
    you cannot manually set model parameters. Your model’s weights are initialized
    with random values and then optimized by your model as it goes through training
    iterations. Hyperparameters, on the other hand, refer to any parameters that you,
    as a model builder, can control. They include values like learning rate, number
    of epochs, number of layers in your model, and more.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数指的是模型学习到的权重和偏置。你无法直接控制模型参数，因为它们很大程度上取决于你的训练数据、模型架构以及许多其他因素。换句话说，你不能手动设置模型参数。模型的权重是用随机值初始化的，然后在训练迭代过程中由模型进行优化。另一方面，超参数是指你作为模型构建者可以控制的任何参数。它们包括学习率、迭代次数、模型中的层数等。
- en: Manual tuning
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动调整
- en: Because you can manually select the values for different hyperparameters, your
    first instinct might be a trial-and-error approach to finding the optimal combination
    of hyperparameter values. This might work for models that train in seconds or
    minutes, but it can quickly get expensive on larger models that require significant
    training time and infrastructure. Imagine you are training an image classification
    model that takes hours to train on GPUs. You settle on a few hyperparameter values
    to try and then wait for the results of the first training run. Based on these
    results, you tweak the hyperparameters, train the model again, compare the results
    with the first run, and then settle on the best hyperparameter values by looking
    at the training run with the best metrics.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你可以手动选择不同超参数的值，你的第一反应可能是尝试错误来找到最优的超参数组合。这种方法对于在几秒或几分钟内完成训练的模型可能有效，但对于需要大量训练时间和基础设施的大型模型来说，成本会很快上升。想象一下，你正在训练一个图像分类模型，需要在GPU上花费数小时来完成训练。你先确定了几个要尝试的超参数值，然后等待第一次训练运行的结果。根据这些结果，你调整超参数，再次训练模型，将结果与第一次运行的结果进行比较，然后通过查看具有最佳指标的训练运行来确定最佳的超参数值。
- en: There are a few problems with this approach. First, you’ve spent nearly a day
    and many compute hours on this task. Second, there’s no way of knowing if you’ve
    arrived at the optimal combination of hyperparameter values. You’ve only tried
    two different combinations, and because you changed multiple values at once, you
    don’t know which parameter had the biggest influence on performance. Even with
    additional trials, using this approach will quickly use up your time and compute
    resources and may not yield the most optimal hyperparameter values.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法存在一些问题。首先，你已经花了将近一天的时间和大量计算资源来完成这个任务。其次，你无法确定是否找到了最优的超参数组合。你只尝试了两种不同的组合，而且因为你同时更改了多个值，所以不知道哪个参数对性能影响最大。即使进行额外的尝试，采用这种方法也会迅速消耗你的时间和计算资源，并且可能得不到最优的超参数值。
- en: Note
  id: totrans-332
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We’re using the term *trial* here to refer to a single training run with a set
    of hyperparameter values.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用术语*试验*来指代使用一组超参数值进行的单次训练运行。
- en: Grid search and combinatorial explosion
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网格搜索与组合爆炸
- en: 'A more structured version of the trial-and-error approach described earlier
    is known as *grid search*. When implementing hyperparameter tuning with grid search,
    we choose a list of possible values we’d like to try for each hyperparameter we
    want to optimize. For example, in scikit-learn’s `RandomForestRegressor()` model,
    let’s say we want to try the following combination of values for the model’s `max_depth`
    and `n_estimators` hyperparameters:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 较结构化的试错方法的更完整版本称为*网格搜索*。当使用网格搜索进行超参数调整时，我们选择要优化的每个超参数的可能值列表。例如，在scikit-learn的`RandomForestRegressor()`模型中，假设我们想为模型的`max_depth`和`n_estimators`超参数尝试以下组合的值：
- en: '[PRE23]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Using grid search, we’d try every combination of the specified values, then
    use the combination that yielded the best evaluation metric on our model. Let’s
    see how this works on a random forest model trained on the Boston housing dataset,
    which comes pre-installed with scikit-learn. The model will predict the price
    of a house based on a number of factors. We can run grid search by creating an
    instance of the `GridSearchCV` class, and training the model passing it the values
    we defined earlier:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 使用网格搜索，我们将尝试每个指定值的组合，然后使用在我们模型上产生最佳评估指标的组合。让我们看看如何在预先安装了scikit-learn的波士顿房价数据集上的随机森林模型上运行这个方法。我们可以通过创建`GridSearchCV`类的一个实例并传递我们之前定义的值来运行网格搜索训练模型：
- en: '[PRE24]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note that the scoring parameter here is the metric we want to optimize. In
    the case of this regression model, we want to use the combination of hyperparameters
    that results in the lowest error for our model. To get the best combination of
    values from the grid search, we can run `grid_search_housing.best_params_`. This
    returns the following:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里的评分参数是我们希望优化的度量标准。在这个回归模型的情况下，我们希望使用导致模型误差最低的超参数组合。要从网格搜索中获得最佳值组合，我们可以运行
    `grid_search_housing.best_params_`。返回以下内容：
- en: '[PRE25]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We’d want to compare this to the error we’d get training a random forest regressor
    model *without* hyperparameter tuning, using scikit-learn’s default values for
    these parameters. This grid search approach works OK on the small example we’ve
    defined above, but with more complex models, we’d likely want to optimize more
    than two hyperparameters, each with a wide range of possible values. Eventually,
    grid search will lead to *combinatorial explosion*—as we add additional hyperparameters
    and values to our grid of options, the number of possible combinations we need
    to try and the time required to try them all increases significantly.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将这与训练随机森林回归模型（*没有*超参数调优）得到的误差进行比较，使用scikit-learn的这些参数的默认值。这种网格搜索方法在我们上面定义的小例子上效果还行，但是对于更复杂的模型，我们可能希望优化超过两个超参数，每个参数有广泛的可能值。最终，网格搜索将导致*组合爆炸*——随着我们添加额外的超参数和值到我们的选项网格中，需要尝试的可能组合数量以及尝试它们所需的时间显著增加。
- en: Another problem with this approach is that no logic is being applied when choosing
    different combinations. Grid search is essentially a brute force solution, trying
    every possible combination of values. Let’s say that after a certain max_depth
    value, our model’s error increases. The grid search algorithm doesn’t learn from
    previous trials, so it wouldn’t know to stop trying `max_depth` values after a
    certain threshold. It will simply try every value you provide no matter the results.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，在选择不同组合时没有应用逻辑。网格搜索本质上是一种蛮力解决方案，尝试每一个可能的数值组合。假设在某个`max_depth`值之后，我们模型的误差增加了。网格搜索算法不会从之前的试验中学习，因此它不会知道在某个阈值之后停止尝试`max_depth`值。它会简单地尝试你提供的每一个数值，不管结果如何。
- en: Note
  id: totrans-343
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: scikit-learn supports an alternative to grid search called `RandomizedSearchCV`
    that implements *random search*. Instead of trying every possible combination
    of hyperparameters from a set, you determine the number of times you’d like to
    randomly sample values for each hyperparameter. To implement random search in
    scikit-learn, we’d create an instance of `RandomizedSearchCV` and pass it a dict
    similar to `grid_values` above, specifying *ranges* instead of specific values.
    Random search runs faster than grid search since it doesn’t try every combination
    in your set of possible values, but it is very likely that the optimal set of
    hyperparameters will not be among the ones randomly selected.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn支持一种称为`RandomizedSearchCV`的网格搜索替代方法，它实现了*随机搜索*。与从一个集合中尝试每个可能的超参数组合不同，您确定要为每个超参数随机抽样值的次数。要在scikit-learn中实现随机搜索，我们会创建一个`RandomizedSearchCV`实例，并传递一个类似上面`grid_values`的字典，指定*范围*而不是具体的值。随机搜索运行速度比网格搜索快，因为它不会尝试所有可能值的每一种组合，但是很可能最优的超参数集不会在随机选择的集合中。
- en: For robust hyperparameter tuning, we need a solution that scales and learns
    from previous trials to find an optimal combination of hyperparameter values.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 对于稳健的超参数调优，我们需要一个解决方案，可以扩展并从先前试验中学习，以找到超参数值的最佳组合。
- en: '**## Solution'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '**## 解决方案'
- en: The `keras-tuner` library implements Bayesian optimization to do hyperparameter
    search directly in Keras. To use `keras-tuner`, we define our model inside a function
    that takes a hyperparameter argument, here called `hp`. We can then use `hp` throughout
    the function wherever we want to include a hyperparameter, specifying the hyperparameter’s
    name, data type, the value range we’d like to search, and how much to increment
    it each time we try a new one.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`keras-tuner`库实现了在Keras中直接进行超参数搜索的贝叶斯优化。要使用`keras-tuner`，我们在一个接受超参数参数的函数中定义我们的模型，这里称为`hp`。然后我们可以在函数中使用`hp`，无论我们想在哪里包含一个超参数，指定超参数的名称、数据类型、我们想要搜索的值范围以及每次尝试新值时递增多少。'
- en: 'Instead of hardcoding the hyperparameter value when we define a layer in our
    Keras model, we define it using a hyperparameter variable. Here, we want to tune
    the number of neurons in the first hidden layer of our neural network:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义Keras模型中的层时，我们不会硬编码超参数值，而是使用超参数变量来定义。在这里，我们希望调整神经网络第一个隐藏层中的神经元数量：
- en: '[PRE26]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`first_hidden` is the name we’ve given this hyperparameter, 32 is the minimum
    value we’ve defined for it, 256 is the maximum, and 32 is the amount we should
    increment this value by within the range we’ve defined. If we were building an
    MNIST classification model, the full function that we’d pass to `keras-tuner`
    might look like the following:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '`first_hidden` 是我们给这个超参数起的名字，32 是我们为其定义的最小值，256 是最大值，并且在我们定义的范围内，每次应增加此值 32。如果我们正在构建一个
    MNIST 分类模型，我们将传递给 `keras-tuner` 的完整函数可能如下所示：'
- en: '[PRE27]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `keras-tuner` library supports many different optimization algorithms.
    Here, we’ll instantiate our tuner with Bayesian optimization and optimize for
    validation accuracy:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '`keras-tuner` 库支持许多不同的优化算法。在这里，我们将使用贝叶斯优化来实例化我们的调优器，并优化验证精度：'
- en: '[PRE28]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The code to run the tuning job looks similar to training our model with `fit()`.
    As this runs, we’ll be able to see the values for the three hyperparameters that
    were selected for each trial. When the job completes, we can see the hyperparameter
    combination that resulted in the best trial. In [Figure 4-23](#output_for_one_trial_run_of_hyperparame),
    we can see the example output for a single trial run using `keras-tuner`.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 运行调优作业的代码类似于使用 `fit()` 训练我们的模型。当这个过程运行时，我们将能够看到每个试验中选择的三个超参数的值。作业完成后，我们可以看到导致最佳试验的超参数组合。在
    [图 4-23](#output_for_one_trial_run_of_hyperparame) 中，我们可以看到使用 `keras-tuner` 进行单次试验运行的示例输出。
- en: '![Output for one trial run of hyperparameter tuning with keras-tuner. At the
    top we can see the hyperparameters selected by the tuner, and in the summary section
    we see the resulting optimization metric.](Images/mldp_0423.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![使用 keras-tuner 进行一次超参数调优试验的输出。在顶部我们可以看到调优器选择的超参数，在摘要部分我们看到生成的优化指标。](Images/mldp_0423.png)'
- en: Figure 4-23\. Output for one trial run of hyperparameter tuning with keras-tuner.
    At the top, we can see the hyperparameters selected by the tuner, and in the summary
    section, we see the resulting optimization metric.
  id: totrans-356
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-23。使用 `keras-tuner` 进行一次超参数调优试验的输出。在顶部，我们可以看到调优器选择的超参数，在摘要部分，我们可以看到生成的优化指标。
- en: In addition to the examples shown here, there is additional functionality provided
    by `keras-tuner` that we haven’t covered. You can use it to experiment with different
    numbers of layers for your model by defining an `hp.Int()` parameter within a
    loop, and you can also provide a fixed set of values for a hyperparameter instead
    of a range. For more complex models, this `hp.Choice()` parameter could be used
    to experiment with different types of layers, like `BasicLSTMCell` and `BasicRNNCell`.
    `keras-tuner` runs in any environment where you can train a Keras model.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这里显示的示例之外，`keras-tuner` 还提供了我们尚未涵盖的其他功能。您可以通过在循环内定义一个 `hp.Int()` 参数来尝试不同层数的模型，并且您还可以为超参数提供一组固定的值而不是一个范围。对于更复杂的模型，`hp.Choice()`
    参数可以用于尝试不同类型的层，如 `BasicLSTMCell` 和 `BasicRNNCell`。`keras-tuner` 可在任何可以训练 Keras
    模型的环境中运行。
- en: Why It Works
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么它有效
- en: Although grid and random search are more efficient than a trial-and-error approach
    to hyperparameter tuning, they quickly become expensive for models requiring significant
    training time or having a large hyperparameter search space.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管网格搜索和随机搜索比超参数调优的试错方法更高效，但对于需要大量训练时间或具有大型超参数搜索空间的模型，它们很快变得昂贵。
- en: Since both machine learning models themselves and the process of hyperparameter
    search are optimization problems, it would follow that we would be able to use
    an approach that *learns* to find the optimal hyperparameter combination within
    a given range of possible values just like our models learn from training data.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习模型本身和超参数搜索过程都是优化问题，我们可以使用一种学习方法来找到在给定可能值范围内的最优超参数组合，就像我们的模型从训练数据中学习一样。
- en: We can think of hyperparameter tuning as an outer optimization loop (see [Figure 4-24](#hyperparameter_tuning_can_be_thought_of))
    where the inner loop consists of typical model training. Even though we depict
    neural networks as the model whose parameters are being optimized, this solution
    is applicable to other types of machine learning models. Also, although the more
    common use case is to choose a single best model from all potential hyperparameters,
    in some cases, the hyperparameter framework can be used to generate a family of
    models that can act as an ensemble (see the discussion of the Ensembles pattern
    in [Chapter 3](ch03.xhtml#problem_representation_design_patterns)).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把超参数调优看作是一个外部优化循环（参见[图 4-24](#hyperparameter_tuning_can_be_thought_of)），其中内部循环包括典型的模型训练。虽然我们将神经网络描绘为正在优化其参数的模型，但这种解决方案适用于其他类型的机器学习模型。此外，尽管更常见的用例是从所有潜在的超参数中选择一个最佳模型，但在某些情况下，超参数框架可以用来生成一个作为集成的模型族（参见[第 3 章](ch03.xhtml#problem_representation_design_patterns)中集合模式的讨论）。
- en: '![Hyperparameter tuning can be thought of as an outer optimization loop.](Images/mldp_0424.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![超参数调优可以看作是一个外部优化循环。](Images/mldp_0424.png)'
- en: Figure 4-24\. Hyperparameter tuning can be thought of as an outer optimization
    loop.
  id: totrans-363
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-24\. 超参数调优可以看作是一个外部优化循环。
- en: Nonlinear optimization
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非线性优化
- en: 'The hyperparameters that need to be tuned fall into two groups: those related
    to model *architecture* and those related to model *training*. Model architecture
    hyperparameters, like the number of layers in your model or the number of neurons
    per layer, control the mathematical function that underlies the machine learning
    model. Parameters related to model training, like the number of epochs, learning
    rate, and batch size, control the training loop and often have to do with the
    way that the gradient descent optimizer works. Taking both these types of parameters
    into consideration, it is clear that the overall model function with respect to
    these hyperparameters is, in general, not differentiable.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 需要调优的超参数分为两组：与模型*架构*相关的参数和与模型*训练*相关的参数。模型架构的超参数，例如模型中的层数或每层的神经元数，控制着机器学习模型底层的数学函数。与模型训练相关的参数，例如epoch数、学习率和批大小，控制着训练循环，通常与梯度下降优化器的工作方式有关。考虑到这两类参数，显然总体模型函数关于这些超参数不是可微的。
- en: The inner training loop is differentiable, and the search for optimal parameters
    can be carried out through stochastic gradient descent. A single step of a machine
    learning model trained through stochastic gradient might take only a few milliseconds.
    On the other hand, a single trial in the hyperparameter tuning problem involves
    training a complete model on the training dataset and might take several hours.
    Moreover, the optimization problem for the hyperparameters will have to be solved
    through nonlinear optimization methods that apply to nondifferentiable problems.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 内部训练循环是可微分的，通过随机梯度下降可以寻找最优参数。通过随机梯度训练的机器学习模型单步可能只需几毫秒。另一方面，超参数调优问题的单次试验涉及在训练数据集上训练完整的模型，可能需要数小时。此外，超参数的优化问题必须通过适用于非可微问题的非线性优化方法来解决。
- en: Once we decide that we are going to use nonlinear optimization methods, our
    choice of metric becomes wider. This metric will be evaluated on the validation
    dataset and does not have to be the same as the training loss. For a classification
    model, your optimization metric might be accuracy, and you’d therefore want to
    find the combination of hyperparameters that leads to the highest model accuracy
    even if the loss is binary cross entropy. For a regression model, you might want
    to optimize median absolute error even if the loss is squared error. In that case,
    you’d want to find the hyperparameters that yield the *lowest* mean squared error.
    This metric can even be chosen based on business goals. For example, we might
    choose to maximize expected revenue or minimize losses due to fraud.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们决定使用非线性优化方法，我们的度量选择范围就更广了。这个度量将在验证数据集上评估，并不一定与训练损失相同。对于分类模型，你的优化度量可能是准确率，因此你希望找到导致模型准确率最高的超参数组合，即使损失是二元交叉熵。对于回归模型，你可能希望优化中位数绝对误差，即使损失是平方误差。在这种情况下，你会希望找到导致*最低*均方误差的超参数。这个度量甚至可以基于业务目标进行选择。例如，我们可能选择最大化预期收入或最小化由于欺诈造成的损失。
- en: Bayesian optimization
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: Bayesian optimization is a technique for optimizing black-box functions, originally
    developed in the [1970s by Jonas Mockus](https://oreil.ly/Ak24H). The technique
    has been applied to many domains and was first applied to hyperparameter tuning
    in [2012](https://oreil.ly/KkGlG). Here, we’ll focus on Bayesian optimization
    as it relates to hyperparameter tuning. In this context, a machine learning model
    is our *black-box function*, since ML models produce a set of outputs from inputs
    we provide without requiring us to know the internal details of the model itself.
    The process of training our ML model is referred to as calling the *objective
    function*.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化是一种优化黑盒函数的技术，最早由[乔纳斯·莫库斯在1970年代开发](https://oreil.ly/Ak24H)。该技术已应用于许多领域，并首次应用于[2012年的超参数调整](https://oreil.ly/KkGlG)。在这里，我们将重点介绍贝叶斯优化与超参数调整的关系。在这个背景下，机器学习模型是我们的*黑盒函数*，因为ML模型会根据我们提供的输入产生一组输出，而无需我们了解模型本身的内部细节。训练ML模型的过程被称为调用*目标函数*。
- en: The goal of Bayesian optimization is to directly train our model as few times
    as possible since doing so is costly. Remember that each time we try a new combination
    of hyperparameters on our model, we need to run through our model’s entire training
    cycle. This might seem trivial with a small model like the scikit-learn one we
    trained above, but for many production models, the training process requires significant
    infrastructure and time.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化的目标是尽可能少地直接训练我们的模型，因为这样做成本很高。请记住，每次我们在模型上尝试新的超参数组合时，都需要运行整个模型的训练周期。对于像我们上面训练的scikit-learn模型这样的小模型来说，这似乎微不足道，但对于许多生产模型来说，训练过程需要大量基础设施和时间。
- en: Instead of training our model each time we try a new combination of hyperparameters,
    Bayesian optimization defines a new function that emulates our model but is much
    cheaper to run. This is referred to as the *surrogate function*—the inputs to
    this function are your hyperparameter values and the output is your optimization
    metric. The surrogate function is called much more frequently than the objective
    function, with the goal of finding an optimal combination of hyperparameters *before*
    completing a training run on your model. With this approach, more compute time
    is spent choosing the hyperparameters for each trial as compared with grid search.
    However, because this is significantly cheaper than running our objective function
    each time we try different hyperparameters, the Bayesian approach of using a surrogate
    function is preferred. Common approaches to generate the surrogate function include
    a [Gaussian process](https://oreil.ly/-Srjj) or a [tree-structured Parzen estimator](https://oreil.ly/UqxDd).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 代替每次尝试新的超参数组合来训练我们的模型，贝叶斯优化定义了一个新的函数，模拟我们的模型但运行成本要低得多。这称为*替代函数*—该函数的输入是您的超参数值，输出是优化度量。替代函数的调用频率远高于目标函数，其目标是在完成模型训练之前找到最佳的超参数组合。采用这种方法，与网格搜索相比，每次试验为选择超参数花费了更多计算时间。但是，因为这比每次尝试不同超参数时运行我们的目标函数要便宜得多，所以使用替代函数的贝叶斯方法更可取。生成替代函数的常见方法包括[高斯过程](https://oreil.ly/-Srjj)或[树结构帕尔森估计器](https://oreil.ly/UqxDd)。
- en: So far, we’ve touched on the different pieces of Bayesian optimization, but
    how do they work together? First, we must choose the hyperparameters we want to
    optimize and define a range of values for each hyperparameter. This part of the
    process is manual and will define the space in which our algorithm will search
    for optimal values. We’ll also need to define our objective function, which is
    the code that calls our model training process. From there, Bayesian optimization
    develops a surrogate function to simulate our model training process and uses
    that function to determine the best combination of hyperparameters to run on our
    model. It is only once this surrogate arrives at what it thinks is a good combination
    of hyperparameters that we do a full training run (trial) on our model. The results
    of this are then fed back to the surrogate function and the process is repeated
    for the number of trials we’ve specified.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涉及了贝叶斯优化的不同部分，但它们如何协同工作呢？首先，我们必须选择要优化的超参数，并为每个超参数定义一系列值的范围。这一过程是手动的，并且将定义我们的算法将搜索优化值的空间。我们还需要定义我们的目标函数，这是调用我们模型训练过程的代码。从那里，贝叶斯优化开发一个替代函数来模拟我们的模型训练过程，并使用该函数来确定在我们的模型上运行的最佳超参数组合。只有当这个替代函数认为找到了一个好的超参数组合时，我们才会对我们的模型进行完整的训练运行（试验）。然后将这些结果反馈给替代函数，并且重复这个过程，直到达到我们指定的试验次数为止。
- en: Trade-Offs and Alternatives
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折中与替代方案
- en: Genetic algorithms are an alternative to Bayesian methods for hyperparameter
    tuning, but they tend to require many more model training runs than Bayesian methods.
    We’ll also show you how to use a managed service for hyperparameter tuning optimization
    on models built with a variety of ML frameworks.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法是超参数调整的替代方案，但它们通常需要比贝叶斯方法更多的模型训练运行。我们还将向您展示如何使用托管服务优化模型，该服务适用于多种ML框架构建的超参数调整。
- en: Fully managed hyperparameter tuning
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全托管的超参数调整
- en: The `keras-tuner` approach may not scale to large machine learning problems
    because we’d like the trials to happen in parallel, and the likelihood of machine
    error and other failure increases as the time for model training stretches into
    the hours. Hence, a fully managed, resilient approach that provides black-box
    optimization is useful for hyperparameter tuning. An example of a managed service
    that implements Bayesian optimization is the [hyperparameter tuning service](https://oreil.ly/MO8FZ)
    provided by Google Cloud AI Platform. This service is based on [Vizier](https://oreil.ly/tScQa),
    the black-box optimization tool used internally at Google.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望试验同时进行，并且随着模型训练时间延长，机器出错和其他故障的可能性增加，`keras-tuner`方法可能无法扩展到大型机器学习问题。因此，提供黑盒优化的完全托管和弹性方法对于超参数调整非常有用。一个实施贝叶斯优化的托管服务示例是由Google
    Cloud AI Platform提供的[超参数调整服务](https://oreil.ly/MO8FZ)。这项服务基于Google内部使用的[Vizier](https://oreil.ly/tScQa)，这是一个黑盒优化工具。
- en: 'The underlying concepts of the Cloud service work similarly to `keras-tuner`:
    you specify each hyperparameter’s name, type, range, and scale, and these values
    are referenced in your model training code. We’ll show you how to run hyperparameter
    tuning in AI Platform using a PyTorch model trained on the BigQuery natality dataset
    to predict a baby’s birth weight.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务的基本概念与`keras-tuner`类似：您指定每个超参数的名称、类型、范围和比例，并且这些值在您的模型训练代码中被引用。我们将向您展示如何在AI
    Platform上运行超参数调整，使用基于BigQuery natality数据集训练的PyTorch模型来预测婴儿的出生体重。
- en: The first step is to create a *config.yaml* file specifying the hyperparameters
    you want the job to optimize, along with some other metadata on your job. One
    benefit of using the Cloud service is that you can scale your tuning job by running
    it on GPUs or TPUs and spreading it across multiple parameter servers. In this
    config file, you also specify the total number of hyperparameter trials you want
    to run and how many of these trials you want to run in parallel. The more you
    run in parallel, the faster your job will run. However, the benefit of running
    fewer trials in parallel is that the service will be able to learn from the results
    of each completed trial to optimize the next ones.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一个*config.yaml*文件，指定您希望作业优化的超参数，以及有关作业的其他一些元数据。使用云服务的一个好处是，您可以通过在GPU或TPU上运行并在多个参数服务器上分布来扩展调整作业。在此配置文件中，您还要指定您希望运行的超参数试验总数以及您希望并行运行的这些试验数量。并行运行的试验越多，作业运行得越快。但是，较少并行运行试验的好处是，服务将能够从每个已完成试验的结果中学习，以优化接下来的试验。
- en: 'For our model, a sample config file that makes use of GPUs might look like
    the following. In this example, we’ll tune three hyperparameters—our model’s learning
    rate, the [optimizer’s momentum value](https://oreil.ly/8mHPQ), and the number
    of neurons in our model’s hidden layer. We also specify our optimization metric.
    In this example, our goal will be to *minimize* our model’s loss on our validation
    set:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型，一个使用GPU的示例配置文件可能如下所示。在这个例子中，我们将调整三个超参数——模型的学习率，[优化器的动量值](https://oreil.ly/8mHPQ)，以及模型隐藏层中的神经元数量。我们还指定了我们的优化度量标准。在这个例子中，我们的目标是*最小化*在验证集上的模型损失：
- en: '[PRE29]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note
  id: totrans-381
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Instead of using a config file to define these values, you can also do this
    using the AI Platform Python API.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用配置文件来定义这些值，您还可以使用AI平台Python API来完成这些操作。
- en: In order to do this, we’ll need to add an argument parser to our code that will
    specify the arguments we defined in the file above, then refer to these hyperparameters
    where they appear throughout our model code.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要向我们的代码中添加一个参数解析器，该解析器将指定我们在上述文件中定义的参数，然后在我们的模型代码中引用这些超参数。
- en: 'Next, we’ll build our model using PyTorch’s `nn.Sequential` API with the SGD
    optimizer. Since our model predicts baby weight as a float, this will be a regression
    model. We specify each of our hyperparameters using the `args` variable, which
    contains the variables defined in our argument parser:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用PyTorch的`nn.Sequential` API和SGD优化器构建我们的模型。由于我们的模型预测婴儿体重为浮点数，因此这将是一个回归模型。我们使用`args`变量指定每个超参数，该变量包含在我们的参数解析器中定义的变量：
- en: '[PRE30]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'At the end of our model training code, we’ll create an instance of `HyperTune()`,
    and tell it the metric we’re trying to optimize. This will report the resulting
    value of our optimization metric after each training run. It’s important that
    whichever optimization metric we choose is calculated on our test or validation
    datasets, and not our training dataset:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型训练代码结尾，我们将创建一个`HyperTune()`的实例，并告诉它我们试图优化的度量标准。这将报告每次训练运行后我们优化度量标准的结果值。重要的是，我们选择的任何优化度量标准都应该在我们的测试或验证数据集上计算，而不是在我们的训练数据集上：
- en: '[PRE31]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Once we’ve submitted our training job to AI Platform, we can monitor logs in
    the Cloud console. After each trial completes, you’ll be able to see the values
    chosen for each hyperparameter and the resulting value of your optimization metric,
    as seen in [Figure 4-25](#a_sample_of_the_hypertune_summary_in_th).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将训练作业提交到AI平台，我们就可以在云控制台中监控日志。在每次试验完成后，您将能够看到为每个超参数选择的值以及您优化度量标准的结果值，如[图4-25](#a_sample_of_the_hypertune_summary_in_th)所示。
- en: '![A sample of the HyperTune summary in the AI Platform console. This is for
    a PyTorch model optimizing three model parameters, with the goal of minimizing
    mean squared error on the validation dataset.](Images/mldp_0425.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![AI平台控制台中HyperTune摘要的示例。这是一个PyTorch模型优化三个模型参数的摘要，目标是在验证数据集上最小化均方误差。](Images/mldp_0425.png)'
- en: Figure 4-25\. A sample of the HyperTune summary in the AI Platform console.
    This is for a PyTorch model optimizing three model parameters, with the goal of
    minimizing mean squared error on the validation dataset.
  id: totrans-390
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-25\. AI平台控制台中HyperTune摘要的示例。这是一个PyTorch模型优化三个模型参数的摘要，目标是在验证数据集上最小化均方误差。
- en: By default, AI Platform Training will use Bayesian optimization for your tuning
    job, but you can also specify if you’d like to use grid or random search algorithms
    instead. The Cloud service also optimizes your hyperparameter search *across*
    training jobs. If we run another training job similar to the one above, but with
    a few tweaks to our hyperparameters and search space, it’ll use the results of
    our last job to efficiently choose values for the next set of trials.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，AI 平台训练将使用贝叶斯优化来进行调优作业，但您也可以指定是否希望改用网格或随机搜索算法。云服务还通过多个训练作业优化您的超参数搜索 *跨*
    训练作业。如果我们运行另一个类似上述作业的训练作业，但对超参数和搜索空间进行了一些微调，它将利用上次作业的结果，有效地为下一组试验选择数值。
- en: We’ve shown a PyTorch example here, but you can use AI Platform Training for
    hyperparameter tuning in any machine learning framework by packaging your training
    code and providing a *setup.py* file that installs any library dependencies.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里展示了一个 PyTorch 的例子，但您可以通过打包您的训练代码并提供一个 *setup.py* 文件来安装任何库依赖项，利用 AI 平台训练进行任何机器学习框架的超参数调整。
- en: Genetic algorithms
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 遗传算法
- en: 'We’ve explored various algorithms for hyperparameter optimization: manual search,
    grid search, random search, and Bayesian optimization. Another less-common alternative
    is a genetic algorithm, which is roughly based on Charles Darwin’s evolutionary
    theory of natural selection. This theory, also known as “survival of the fittest,”
    posits that the highest-performing (“fittest”) members of a population will survive
    and pass their genes to future generations, while less-fit members will not. Genetic
    algorithms have been applied to different types of optimization problems, including
    hyperparameter tuning.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了多种超参数优化算法：手动搜索、网格搜索、随机搜索和贝叶斯优化。另一个不太常见的选择是遗传算法，它大致基于查尔斯·达尔文的进化理论，即“适者生存”。该理论认为，群体中表现最好（“适者”）的成员将生存下来，并将其基因传给未来的后代，而表现较差的成员则不会。遗传算法已应用于不同类型的优化问题，包括超参数调整。
- en: As it relates to hyperparameter search, a genetic approach works by first defining
    a *fitness function*. This function measures the quality of a particular trial,
    and can typically be defined by your model’s optimization metric (accuracy, error,
    and so on). After defining your fitness function, you randomly select a few combinations
    of hyperparameters from your search space and run a trial for each of those combinations.
    You then take the hyperparameters from the trials that performed best, and use
    those values to define your new search space. This search space becomes your new
    “population,” and you use it to generate new combinations of values to use in
    your next set of trials. You continue this process, narrowing down the number
    of trials you run until you’ve arrived at a result that satisfies your requirements.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 关于超参数搜索，遗传算法首先通过定义一个 *适应度函数* 来工作。该函数衡量特定试验的质量，通常可以由您模型的优化度量（准确率、误差等）来定义。在定义适应度函数后，您随机选择几组超参数的组合，并针对每个组合运行一次试验。然后，您选择表现最佳的试验的超参数，并使用这些值来定义您的新搜索空间。这个搜索空间成为您的新“种群”，您可以使用它生成新的数值组合，用于下一组试验。您可以继续这个过程，逐渐减少您运行的试验数量，直到达到满足您需求的结果。
- en: Because they use the results of previous trials to improve, genetic algorithms
    are “smarter” than manual, grid, and random search. However, when the hyperparameter
    search space is large, the complexity of genetic algorithms increases. Rather
    than using a surrogate function as a proxy for model training like in Bayesian
    optimization, genetic algorithms require training your model for each possible
    combination of hyperparameter values. Additionally, at the time of writing, genetic
    algorithms are less common and there are fewer ML frameworks that support them
    out of the box for hyperparameter tuning.**  **# Summary
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们利用先前试验的结果来改进，遗传算法比手动搜索、网格搜索和随机搜索更“智能”。然而，当超参数搜索空间很大时，遗传算法的复杂性增加。与贝叶斯优化中使用代理函数作为模型训练的代理不同，遗传算法需要为每种可能的超参数值组合训练您的模型。此外，截至撰写本文时，遗传算法较不常见，支持它们进行超参数调优的
    ML 框架也较少。
- en: This chapter focused on design patterns that modify the typical SGD training
    loop of machine learning. We started with looking at the *Useful Overfitting*
    pattern, which covered situations where overfitting is beneficial. For example,
    when using data-driven methods like machine learning to approximate solutions
    to complex dynamical systems or PDEs where the full input space can be covered,
    overfitting on the training set is the goal. Overfitting is also useful as a technique
    when developing and debugging ML model architectures. Next, we covered model *Checkpoints*
    and how to use them when training ML models. In this design pattern, we save the
    full state of the model periodically during training. These checkpoints can be
    used as the final model, as in the case of early stopping, or used as the starting
    points in the case of training failures or fine-tuning.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点讨论了修改机器学习典型SGD训练循环的设计模式。我们首先看了*有用的过拟合* 模式，涵盖了过拟合有益的情况。例如，当使用数据驱动方法如机器学习来近似解复杂动力系统或PDEs时，目标是在训练集上过拟合。过拟合也是在开发和调试ML模型架构时的一种有用技术。接下来，我们讨论了*模型检查点*
    及其在训练ML模型时的使用方法。在这种设计模式中，我们定期保存模型的完整状态。这些检查点可以作为最终模型使用，例如在早停止的情况下，或者在训练失败或微调时作为起始点使用。
- en: The *Transfer Learning* design pattern covered reusing parts of a previously
    trained model. Transfer learning is a useful way to leverage the learned feature
    extraction layers of the pre-trained model when your own dataset is limited. It
    can also be used to fine-tune a pre-trained model that was trained on a large
    generic dataset to your more specialized dataset. We then discussed the *Distribution
    Strategy* design pattern. Training large, complex neural networks can take a considerable
    amount of time. Distribution strategies offer various ways in which the training
    loop can be modified to be carried out at scale over multiple workers, using parallelization
    and hardware accelerators.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '*迁移学习* 设计模式涵盖了重新使用先前训练模型的部分内容。当您自己的数据集有限时，迁移学习是利用预训练模型学习特征提取层的有效方法。它还可以用于对在大规模通用数据集上训练的预训练模型进行微调，以适应更专业的数据集。接着，我们讨论了*分布策略*
    设计模式。训练大型复杂神经网络可能需要相当长的时间。分布策略提供了多种方式，可以修改训练循环，通过并行化和硬件加速器在多个工作节点上扩展执行。'
- en: Lastly, the *Hyperparameter Tuning* design pattern discussed how the SGD training
    loop itself can be optimized with respect to model hyperparameters. We saw some
    useful libraries that can be used to implement hyperparameter tuning for models
    created with Keras and PyTorch.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*超参数调整* 设计模式讨论了如何优化SGD训练循环本身，以适应模型的超参数。我们看到了一些有用的库，可以用来为使用Keras和PyTorch创建的模型实现超参数调整。
- en: The next chapter looks at design patterns related to *resilience* (to large
    numbers of requests, spiky traffic, or change management) when placing models
    into production.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将探讨将模型投入生产时，与*韧性*（对大量请求、尖峰流量或变更管理）相关的设计模式。
- en: ^([1](ch04.xhtml#ch01fn16-marker)) It may, of course, not be the case that we
    can learn the network using gradient descent just because there exists such a
    neural network (this is why changing the model architecture by adding layers helps—it
    makes the loss landscape more amenable to SGD).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#ch01fn16-marker)) 当然，并不一定是我们可以使用梯度下降来学习网络，仅仅因为存在这样一个神经网络（这就是为什么通过增加层次改变模型架构有帮助——这使得损失函数的形态更适合SGD）。
- en: ^([2](ch04.xhtml#idm46056094720120-marker)) MLPerf v0.7 Training Closed ResNet.
    Retrieved from www.mlperf.org 23 September 2020, entry 0.7-67\. MLPerf name and
    logo are trademarks. See www.mlperf.org for more information.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.xhtml#idm46056094720120-marker)) MLPerf v0.7 训练 Closed ResNet。来源于
    www.mlperf.org 2020年9月23日，条目 0.7-67\. MLPerf 名称和标识是商标。有关详细信息，请参阅 www.mlperf.org。
- en: '^([3](ch04.xhtml#ch01fn17-marker)) Jia Deng et al.,[“ImageNet: A Large-Scale
    Hierarchical Image Database,”](https://oreil.ly/Wio_D) IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition (CVPR) (2009): 248–255.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '^([3](ch04.xhtml#ch01fn17-marker)) Jia Deng 等人，“ImageNet: A Large-Scale Hierarchical
    Image Database”，IEEE计算机学会计算机视觉与模式识别会议（CVPR）（2009年）：248–255。'
- en: ^([4](ch04.xhtml#ch01fn19-marker)) For more information, see [“CS231n Convolutional
    Neural Networks for Visual Recognition](https://oreil.ly/w109T).”
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.xhtml#ch01fn19-marker)) 更多信息，请参阅 [“CS231n Convolutional Neural Networks
    for Visual Recognition](https://oreil.ly/w109T).”
- en: ^([5](ch04.xhtml#ch01fn20-marker)) Victor Campos et al., “Distributed training
    strategies for a computer vision deep learning algorithm on a distributed GPU
    cluster,” *International Conference on Computational Science, ICCS 2017*, June
    12–14, 2017.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.xhtml#ch01fn20-marker)) Victor Campos 等人，《计算科学国际会议 ICCS 2017 上的计算机视觉深度学习算法分布式训练策略》，2017
    年 6 月 12–14 日。
- en: ^([6](ch04.xhtml#ch01fn21-marker)) Ibid.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.xhtml#ch01fn21-marker)) 同上。
- en: ^([7](ch04.xhtml#ch01fn22-marker)) Jeffrey Dean et al. “Large Scale Distributed
    Deep Networks,” *NIPS Proceedings* (2012).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.xhtml#ch01fn22-marker)) Jeffrey Dean 等人，《大规模分布式深度网络》，*NIPS 会议论文集*（2012）。
- en: '^([8](ch04.xhtml#ch01fn23-marker)) Priya Goyal et al., “Accurate, Large Minibatch
    SGD: Training ImageNet in 1 Hour” (2017), arXiv:1706.02677v2 [cs.CV].****'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch04.xhtml#ch01fn23-marker)) Priya Goyal 等人，《准确的大批量随机梯度下降：在1小时内训练 ImageNet》（2017），arXiv:1706.02677v2
    [cs.CV].****
