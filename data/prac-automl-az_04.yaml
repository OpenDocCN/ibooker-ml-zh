- en: Chapter 2\. How Automated Machine Learning Works
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。自动化机器学习的工作原理
- en: In the previous chapter, we established the need to automate the process of
    building machine learning models. In this chapter, we explain what *Automated
    Machine Learning* is, the different techniques involved in this process, and how
    they all come together. We will also give a quick overview of automated ML on
    Microsoft Azure Machine Learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们确认了自动构建机器学习模型的需求。本章将解释什么是*自动化机器学习*，涉及到此过程中的不同技术，以及它们如何相互结合。我们还将简要介绍Microsoft
    Azure机器学习中的自动化ML。
- en: What Is Automated Machine Learning?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是自动化机器学习？
- en: In [Chapter 1](ch01.html#machine_learning_colon_overview_and_best), we discussed
    how coming up with a good machine learning model can be time-consuming and tedious,
    given all the possible combinations to explore. Automated Machine Learning is
    a recent development in machine learning focused on making that entire process
    easy, with the goal of bringing efficiency to data scientists as well as enabling
    non–data scientists to build models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html#machine_learning_colon_overview_and_best)中，我们讨论了如何通过探索所有可能的组合来创建一个好的机器学习模型可能会耗费大量时间和精力。自动化机器学习是机器学习的一个新发展方向，旨在简化整个过程，从而提高数据科学家的效率，同时使非数据科学家也能构建模型。
- en: Let’s go through the stages of the machine learning process and see how Automated
    Machine Learning can help at each stage.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解机器学习过程的各个阶段，并看看自动化机器学习如何在每个阶段提供帮助。
- en: Understanding Data
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据
- en: As briefly discussed in the previous chapter, real-world data is not clean and
    requires a lot of effort to get to a usable state. Understanding input data is
    a crucial step toward formulating the machine learning problem.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章简要讨论的，现实世界的数据并不干净，需要大量工作才能使其达到可用状态。理解输入数据是制定机器学习问题的关键步骤。
- en: Automated Machine Learning can help here by analyzing the data and automatically
    detecting the data type of each column. Column types could be Boolean, numeric
    (discrete or continuous), or text. Automatically detecting these column types
    helps with subsequent stages like feature engineering.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习可以通过分析数据并自动检测每列的数据类型来帮助这一过程。列的类型可能是布尔型、数值型（离散或连续）、或文本型。自动检测这些列类型有助于后续的特征工程阶段。
- en: 'In many cases, Automated Machine Learning can also provide insight into the
    semantics or intent of each column. It can detect a wide spectrum of situations,
    including the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，自动化机器学习还可以提供关于每列语义或意图的见解。它可以检测到广泛的情况，包括以下内容：
- en: Detecting the target/label column
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测目标/标签列
- en: Detecting whether a text column is a categorical-text feature or free-text feature
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测文本列是分类文本特征还是自由文本特征
- en: Detecting columns that are zip codes, temperatures, geo coordinates, and so
    on
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测列是否是邮政编码、温度、地理坐标等
- en: Before we go ahead, let’s discuss how the model training process works in relationship
    to input data. Should we train using all of the data available? The answer is
    no.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们讨论模型训练过程如何与输入数据的关系。我们应该使用所有可用数据进行训练吗？答案是否定的。
- en: Training the model on the full input data can lead to *overfitting*. Overfitting
    means that the model we trained is fit too closely to the input dataset and mimics
    the input dataset. This usually happens when the model is too complex (i.e., too
    many features/variables compared to the number of observations). This model will
    be very accurate on the input data but will probably perform badly on untrained
    or new data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在全数据集上训练模型可能会导致*过拟合*。过拟合意味着我们训练的模型过于贴近输入数据集并模仿输入数据集。这通常发生在模型过于复杂（即特征/变量数量远大于观测数量）的情况下。这种模型在输入数据上可能非常准确，但在未训练或新数据上可能表现不佳。
- en: In contrast, when a model is *underfit*, it means that the model does not fit
    the input data and therefore misses the trends in the data. It also means the
    model cannot be generalized to new data. This is usually the result of a very
    simple model (not enough input variables/features). Adding more input variables/features
    helps overcome underfitting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当模型*欠拟合*时，意味着模型不适合输入数据，因此错过了数据中的趋势。这也意味着该模型无法推广到新数据。这通常是因为模型过于简单（输入变量/特征不足）。增加更多的输入变量/特征有助于克服欠拟合问题。
- en: '[Figure 2-1](#overfitting_and_underfitting) shows overfitting and underfitting
    for a binary classification problem.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-1](#overfitting_and_underfitting) 展示了二元分类问题中的过拟合和欠拟合情况。'
- en: '![paml 0201](assets/paml_0201.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0201](assets/paml_0201.png)'
- en: Figure 2-1\. Overfitting and underfitting
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 过拟合和欠拟合
- en: 'To overcome the overfitting problem, we usually split input data into two subsets:
    training data and testing data (and sometimes further, into three subsets: train,
    validate, and test). The model is then fit on the training data to make predictions
    on the test data. The training set contains a known output, and the model learns
    on this so that it can generalize to other data later. We use the test set to
    test the accuracy of our model’s predictions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服过拟合问题，我们通常将输入数据分成两个子集：训练数据和测试数据（有时还会进一步分成三个子集：训练、验证和测试）。然后，模型在训练数据上进行拟合，以便在测试数据上进行预测。训练集包含已知输出，模型在此基础上学习，以便稍后可以泛化到其他数据。我们使用测试集来测试模型预测的准确性。
- en: But, how do we know if the train/test split is good? What if one subset of our
    data is skewed compared to the other? This will result in overfitting, even though
    we’re trying to avoid that. This is where cross-validation comes in.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们如何知道训练/测试拆分是否有效？如果我们的数据子集相比其他子集存在偏差怎么办？即使我们试图避免，这也会导致过拟合。这就是交叉验证发挥作用的地方。
- en: Cross-validation is similar to the train/test split, but it’s applied to more
    subsets. Data is split into *k* subsets, and the model is trained on *k* – 1 of
    those subsets. The last subset is held for testing. This is done for each of the
    subsets. This is called *k*-fold cross-validation. Finally, the scores from all
    the *k*-folds are averaged to produce the final score. [Figure 2-2](#k-fold_cross-validation_open_parenthesis)
    shows this.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证类似于训练/测试拆分，但应用于更多子集。数据被分成*k*个子集，模型在其中的*k*-1个子集上进行训练，最后一个子集用于测试。这对每个子集都执行。这称为*k*-折交叉验证。最后，所有*k*折的分数平均后产生最终分数。[图2-2](#k-fold_cross-validation_open_parenthesis)展示了这一过程。
- en: '![paml 0202](assets/paml_0202.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0202](assets/paml_0202.png)'
- en: 'Figure 2-2\. *k*-fold cross-validation (source: [*ttps://oreil.ly/k-ixI*](ttps://oreil.ly/k-ixI))'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. *k*-折交叉验证（来源：[*ttps://oreil.ly/k-ixI*](ttps://oreil.ly/k-ixI))
- en: Detecting Tasks
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务检测
- en: Data scientists map real-world scenarios to machine learning tasks. [Figure 2-3](#machine_learning_tasks)
    shows some examples of types of machine learning tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家将现实世界的场景映射到机器学习任务中。[图2-3](#machine_learning_tasks)展示了一些机器学习任务的类型示例。
- en: '![paml 0203](assets/paml_0203.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0203](assets/paml_0203.png)'
- en: Figure 2-3\. Machine learning tasks
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. 机器学习任务
- en: Automated Machine Learning can automatically determine the machine learning
    task, given the input data. This is more relevant in *supervised machine learning*,
    in which target/label columns can be used to predict the machine learning task.
    [Table 2-1](#machine_learning_task_detection) lists generic machine learning tasks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习可以自动确定机器学习任务，给定输入数据。这在*监督学习*中更为相关，其中目标/标签列可用于预测机器学习任务。[表2-1](#machine_learning_task_detection)列出了通用的机器学习任务。
- en: Table 2-1\. Machine learning task detection
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1\. 机器学习任务检测
- en: '| Target/Label column | Machine learning task | Example scenarios |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 目标/标签列 | 机器学习任务 | 示例场景 |'
- en: '| --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Boolean | Binary classification | * [Classifying sentiment of Twitter comments](https://oreil.ly/baZaJ)
    as either positive or negative* Indicating that email is spam or not |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 布尔 | 二分类 | *[分类推特评论的情感](https://oreil.ly/baZaJ)，正面或负面* 表明电子邮件是垃圾邮件还是非垃圾邮件
    |'
- en: '| Discrete numerical/categorical | Multiclass classification | * Determining
    the breed of a dog as Havanese, Golden Retriever, Beagle, etc.* Categorizing hotel
    reviews by location, price, cleanliness, etc. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 离散数值/分类 | 多类别分类 | *确定狗的品种，如哈瓦那犬、金毛寻回犬、比格犬等* 按位置、价格、清洁度等对酒店评论进行分类 |'
- en: '| Continuous numerical | Regression | * Predicting house prices based on house
    attributes such as number of bedrooms, location, or size* Predicting future stock
    prices based on historical data and current market trends |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 连续数值 | 回归 | *基于房屋属性（如卧室数量、位置或大小）预测房价* 基于历史数据和当前市场趋势预测未来股票价格 |'
- en: In addition to these generic tasks, there are specific variations based on input
    data. Forecasting is one such task type that is popular, given its relevance to
    many business problems like revenue forecasting, inventory management, predictive
    maintenance, and so on. If input data is time-series, determined by availability
    of a DateTime column, it is most likely a forecasting task.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些通用任务外，还有基于输入数据的特定变体。其中一种受欢迎的任务类型是预测，因为它与诸如收入预测、库存管理、预测性维护等许多业务问题相关。如果输入数据是时间序列，根据日期时间列的可用性来确定，那么很可能是一个预测任务。
- en: Choosing Evaluation Metrics
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择评估指标
- en: Choosing a metric to evaluate your machine learning algorithm is fundamentally
    driven by the business outcome. This is an important step because it tells you
    how the performance of your algorithm is measured and compared. Different tasks
    have different sets of evaluation metrics to choose from, and the choice depends
    on multiple factors. [Figure 2-4](#machine_learning_evaludation_metrics) shows
    possible options for evaluating algorithms used in various machine learning tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 选择评估机器学习算法的指标基本上是由业务结果驱动的。这是一个重要的步骤，因为它告诉您如何衡量和比较算法的性能。不同的任务有不同的评估指标可供选择，并且选择取决于多个因素。[图 2-4](#machine_learning_evaludation_metrics)
    显示了用于评估各种机器学习任务中使用的算法的可能选项。
- en: '![paml 0204](assets/paml_0204.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0204](assets/paml_0204.png)'
- en: Figure 2-4\. Machine learning evaluation metrics
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 机器学习评估指标
- en: Automated Machine Learning can automate the process of selecting the right evaluation
    metric for a given input dataset and machine learning task. For instance, scenarios
    like fraud detection (which is a classification task) inherently have imbalanced
    data in that a very small percentage of data would be fraudulent. In this case,
    area under curve (AUC) is a much better evaluation metric than accuracy. Automatically
    detecting class imbalance in the data can help automatically choose AUC as an
    evaluation metric for this classification task.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习可以自动选择适合给定输入数据集和机器学习任务的正确评估指标的过程。例如，像欺诈检测（这是一个分类任务）这样的情景中数据通常是不平衡的，只有很小一部分数据是欺诈性的。在这种情况下，曲线下面积（AUC）比准确率更好地评估了性能。自动检测数据中的类别不平衡可以帮助自动选择AUC作为此分类任务的评估指标。
- en: Feature Engineering
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: As discussed in [Chapter 1](ch01.html#machine_learning_colon_overview_and_best),
    feature engineering is the process of getting to the appropriate set of features
    from input data with the goal of producing a good machine learning model. Automated
    feature engineering involves four key steps, which we discuss in the subsections
    that follow.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第 1 章](ch01.html#machine_learning_colon_overview_and_best) 讨论的，特征工程是从输入数据中获取适当特征集的过程，其目标是生成一个良好的机器学习模型。自动特征工程包括四个关键步骤，在接下来的小节中我们会讨论这些步骤。
- en: Detect issues with input data and automatically flag them
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测输入数据问题并自动标记它们
- en: 'Examples of this include the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中的例子包括：
- en: Detecting missing values and automatically imputing them with the most relevant
    technique; for example, numeric columns with mean, categorical columns with mode
    (most frequently occurring), and so on.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测缺失值并自动使用最相关的技术填补它们；例如，数值列使用均值，分类列使用众数（最频繁出现的值），等等。
- en: Detecting class imbalance and automatically fixing it by applying techniques
    like the Synthetic Minority Oversampling Technique (SMOTE).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测类别不平衡，并通过应用诸如合成少数类过采样技术（SMOTE）等技术自动修复它。
- en: Drop columns that are not useful as features
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 放弃作为特征无用的列
- en: 'Here are some examples:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些例子：
- en: No variance columns
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 无变异列
- en: These are columns with the same value across all rows, which are easy to detect
    via automation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是跨所有行具有相同值的列，可以通过自动化轻松检测到。
- en: High cardinality columns
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 高基数列
- en: These are columns with different values across rows; for example, hashes, IDs,
    or globally unique identifiers (GUIDs). Cardinality is determined by percentage
    of unique values in the column.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是跨行具有不同值的列；例如，哈希值、ID 或全局唯一标识符（GUID）。基数由列中唯一值的百分比确定。
- en: Generate features
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成特征
- en: 'There are multiple techniques for generating new features from existing features.
    Some examples follow:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种技术可以从现有特征生成新特征。以下是一些例子：
- en: Encodings and transformations
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 编码和转换
- en: Most machine learning algorithms require numerical input and output variables.
    Real-world datasets are full of text and categorical data. Data scientists convert
    them into numerical data by using encodings and transformations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法需要数值输入和输出变量。现实世界的数据集充满了文本和分类数据。数据科学家通过使用编码和转换将它们转换为数值数据。
- en: '*One-hot encoding* is a popular technique to convert categorical data to integer
    data. You can easily automate this process. [Figure 2-5](#one-hot_encoding) shows
    an example of one-hot encoding.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*独热编码* 是一种将分类数据转换为整数数据的流行技术。您可以轻松自动化此过程。[图 2-5](#one-hot_encoding) 显示了独热编码的一个示例。'
- en: '![paml 0205](assets/paml_0205.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0205](assets/paml_0205.png)'
- en: Figure 2-5\. One-hot encoding
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 独热编码
- en: Transformations are applied to input columns to generate interesting features.
    Some examples include generating “Year,” “Month,” “Day,” “Day of week,” “Day of
    year,” “Quarter,” “Week of the year,” “Hour,” “Minute,” and “Second” for a given
    DateTime column. This is effective for time-series-related problems.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对输入列应用转换以生成有趣的特征。一些示例包括生成“年份”、“月份”、“日期”、“星期几”、“年的天数”、“季度”、“年的周数”、“小时”、“分钟”和“秒”等给定日期时间列的特征。这对于与时间序列相关的问题非常有效。
- en: Other examples might generate term frequency based on unigrams, bi-grams, and
    tri-character-grams, and generating word embeddings for text columns.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其他示例可能根据单字、双字和三字符组生成词频，并为文本列生成词嵌入。
- en: Aggregations
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合
- en: Another popular technique in feature generation involves generating aggregations
    over multiple data records. Aggregations could be based on specific entities in
    the dataset (e.g., average product sales/revenue per store) or based on time (e.g.,
    number of page views to a website in the past 7 days, 30 days, 180 days, etc.).
    Features generated through time-based aggregations are quite useful for time-series
    forecasting problems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 特征生成中的另一种流行技术涉及对多个数据记录进行聚合生成。聚合可以基于数据集中的特定实体（例如，每个商店的平均产品销售/收入）或基于时间（例如，过去7天、30天、180天内的网站页面访问量）。通过时间聚合生成的特征对于时间序列预测问题非常有用。
- en: Select the most impactful features
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择最有影响力的特征
- en: Feature selection is an important step in the process because it helps to prioritize
    the appropriate set of input features. This becomes even more important when the
    number of input features is very large.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择是过程中的重要步骤，因为它有助于优先考虑适当的输入特征集。当输入特征数量非常大时，这变得尤为重要。
- en: 'Why do we need to prioritize the proper set of input features? Why not use
    all the features? Here are the top benefits of feature selection:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要优先考虑适当的输入特征集？为什么不使用所有特征？以下是特征选择的顶级好处：
- en: Faster training
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更快的训练
- en: Simpler model, easier to interpret
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更简单的模型，更容易解释
- en: Reduces overfitting
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少过拟合
- en: Improved model accuracy
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高模型准确性
- en: 'Let’s go through some different feature selection techniques. Keep in mind
    that you can automate all of these techniques:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一些不同的特征选择技术。请记住，您可以自动化所有这些技术：
- en: Filters
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器
- en: According to this technique, the selection of features is independent of any
    machine learning algorithms. Features are selected based on their correlation
    with the outcome variable, as measured by statistical tests. Because the selection
    process is agnostic of the model, this method might not select the most useful
    features but is robust against overfitting. As shown in [Figure 2-6](#feature_selection),
    selecting the best subset of features happens before model training.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这种技术，特征的选择与任何机器学习算法无关。特征的选择基于它们与结果变量的相关性，由统计测试来衡量。因为选择过程与模型无关，这种方法可能不会选择最有用的特征，但能有效防止过拟合。如图[2-6](#feature_selection)所示，选择最佳特征子集发生在模型训练之前。
- en: Wrappers
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 包装器
- en: According to this technique, a subset of features is used to train a model.
    Based on the performance of the model, we decide to add or remove features from
    the subset and train the model again with the updated subset. This process continues
    until the model’s performance is satisfactory. However, this technique can be
    computationally expensive due to multiple back-and-forth iterations. Because the
    selection process is tied to the model, it tends to produce more accurate results
    than filter methods but is more prone to overfitting. [Figure 2-6](#feature_selection)
    demonstrates wrapper methods.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这种技术，使用特征子集来训练模型。基于模型的性能，我们决定从子集中添加或删除特征，并使用更新的子集再次训练模型。这个过程持续进行，直到模型的性能令人满意。然而，由于多次来回迭代，这种技术可能计算开销很大。由于选择过程与模型绑定，它倾向于产生比过滤方法更准确的结果，但更容易过拟合。如图[2-6](#feature_selection)所示，展示了包装器方法。
- en: Embedded methods
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入方法
- en: Embedded methods combine the qualities of filter and wrapper methods. Implemented
    by algorithms that have their own built-in feature selection methods, embedded
    methods are like wrappers but are less computationally expensive because there
    are no back-and-forth iterations. This technique is also less prone to overfitting.
    [Figure 2-6](#feature_selection) demonstrates embedded methods.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入方法结合了过滤器和包装器方法的优点。由具有其自身内置特征选择方法的算法实现，嵌入方法类似于包装器，但计算开销较小，因为没有来回迭代。这种技术也较少容易过拟合。如图[2-6](#feature_selection)所示，展示了嵌入方法。
- en: '![paml 0206](assets/paml_0206.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0206](assets/paml_0206.png)'
- en: Figure 2-6\. Feature selection
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6\. 特征选择
- en: Selecting a Model
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择模型
- en: 'As discussed in the previous chapter, a machine learning model is represented
    by a combination of an algorithm and associated hyperparameter values. Automated
    Machine Learning systems follow different approaches for model selection. In this
    section, we discuss two categories of approaches: brute-force approaches and smarter
    approaches.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所讨论的，机器学习模型由算法及其关联的超参数值组合表示。自动化机器学习系统采用不同的方法进行模型选择。在本节中，我们讨论了两类方法：暴力方法和更智能的方法。
- en: Brute-force approaches
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 暴力方法
- en: This is the naïve approach of trying out all possible combinations of algorithms
    and hyperparameter values to find the one that produces the best model as measured
    by the evaluation metric. This is typically achieved by picking algorithms at
    random and applying *grid search* to figure out the right set of hyperparameters.
    One major drawback of grid search is that dimensionality suffers when the number
    of hyperparameters grows exponentially. With as few as four parameters, this problem
    can become impracticable because the number of evaluations required for this approach
    increases exponentially with each additional parameter, due to the curse of dimensionality.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是尝试所有可能的算法和超参数值组合的天真方法，以找到根据评估度量得出的最佳模型。通常是随机选择算法，并应用*网格搜索*来找出正确的超参数集。网格搜索的一个主要缺点是，当超参数数量呈指数增长时，维度灾难会显现出来。仅有四个参数，这个问题就变得不切实际，因为这种方法所需的评估次数随每个额外参数的增加而指数增加。
- en: '*Random search* is a technique by which random combinations of the hyperparameters
    are used to find the best solution. In this search pattern, random combinations
    of parameters are considered in every iteration. Because random values are selected
    at each iteration, it is highly likely that the whole space has been covered due
    to randomness; hence the chances of finding the best model are comparatively higher
    than grid search. It takes a huge amount of time to cover every aspect of the
    combination during grid search. Random search works best if all hyperparameters
    are not equally important.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机搜索*是一种技术，通过随机组合超参数来寻找最佳解决方案。在这种搜索模式下，每次迭代都考虑随机组合的参数。由于每次迭代都选择随机值，很有可能整个空间都被覆盖，因此找到最佳模型的机会比网格搜索要高得多。在网格搜索期间，覆盖每个组合的每个方面需要大量时间。如果所有超参数不是同等重要，随机搜索效果最好。'
- en: '[Figure 2-7](#grid_search_versus_random_search) shows how grid search and random
    search work. In this example, nine sets of parameter combinations are being tried.
    Notice how random search manages to reach much better model performance, as shown
    by the dots on the “hills” at top. The topmost point on the “hill” represents
    the best parameter combination.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-7](#grid_search_versus_random_search)展示了网格搜索和随机搜索的工作原理。在这个例子中，正在尝试九组参数组合。请注意，随机搜索成功达到了更好的模型性能，如顶部的“山丘”上的点所示。在“山丘”的最高点表示了最佳参数组合。'
- en: '![paml 0207](assets/paml_0207.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0207](assets/paml_0207.png)'
- en: Figure 2-7\. Grid search versus random search
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7\. 网格搜索与随机搜索比较
- en: Smarter approaches
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更智能的方法
- en: 'For real-world problems, the search space is very large, and brute-force approaches
    will not be effective. This has led to the emergence of smarter selection and
    optimizations approaches, mostly powered by advanced statistics and machine learning
    techniques. Some of these approaches include Bayesian optimization, multiarmed
    bandit, and meta-learning. Here, we describe some of these at a high level (details
    require digging deeper and are beyond the scope of this book):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于现实世界的问题，搜索空间非常大，暴力方法将不会有效。这导致了更智能的选择和优化方法的出现，主要由先进的统计学和机器学习技术驱动。其中一些方法包括贝叶斯优化、多臂赌博机和元学习。在这里，我们以较高层次介绍其中一些方法（详细内容需要更深入挖掘，超出本书的范围）：
- en: Bayesian optimization
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: This method uses approximation to guess an unknown function with some prior
    knowledge. The goal here is to train the model based on available observations.
    The trained model will map to a function, which we don’t know. Our task is to
    find the hyperparameters that maximize the learning function.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使用近似来猜测一个未知函数，并带有一些先验知识。这里的目标是基于可用的观察结果训练模型。训练好的模型将映射到一个我们不知道的函数上。我们的任务是找到最大化学习函数的超参数。
- en: Bayesian optimization can help you find the best model among many, speeding
    up the model selection process by reducing the computation task and not requiring
    help from a human to guess the values. This optimization technique is based on
    randomness and probability distributions. [Figure 2-8](#bayesian_optimization)
    provides a visual description of how it works.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化能帮助您在众多模型中找到最佳模型，通过减少计算任务并不需要人类帮助来猜测数值来加快模型选择过程。这种优化技术基于随机性和概率分布。[图 2-8](#bayesian_optimization)展示了其工作原理的视觉描述。
- en: '![paml 0208](assets/paml_0208.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0208](assets/paml_0208.png)'
- en: Figure 2-8\. Bayesian optimization
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-8\. 贝叶斯优化
- en: The dotted line is our True Objective function curve. Unfortunately, we don’t
    know this function and its equation. We are trying to approximate it by using
    a Gaussian process. Throughout our sample space, we draw an intuitive curve (the
    solid line) that fits with our observed samples (the solid dots). *t* represents
    different time points when we have a new observation sample. The shaded region
    is the Confidence region, where the point could exist. From the preceding prior
    knowledge, we can determine that second point as the maximum observed value. The
    next maximum point should be above it or equal. If we draw a horizontal line through
    the second point, the next maximum point should fall above this line. From the
    intersecting points of this line and the Confidence region, we can discard the
    curve samples below the line to find the maximum. In so doing, we have narrowed
    down our area of investigation. This same process continues with the next sampled
    points.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虚线是我们真实目标函数曲线。不幸的是，我们不知道这个函数及其方程式。我们试图通过使用高斯过程来近似它。在我们的样本空间中，我们绘制一个符合观察到的样本（实心点）的直观曲线（实线）。*t*代表我们有新观察样本时的不同时间点。阴影区域是置信区域，点可能存在的区域。通过前述的先验知识，我们可以确定第二个点为最大观察值。下一个最大点应在其上方或相等。如果我们通过第二个点绘制一条水平线，那么下一个最大点应落在这条线上方。从这条线与置信区域的交点，我们可以舍弃曲线样本低于该线以找到最大值。这样做，我们已经缩小了我们的调查范围。这个相同的过程随后的采样点继续进行。
- en: Multiarmed bandit
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂赌博机
- en: A multiarmed bandit is a problem in which a limited set of resources must be
    allocated between competing choices in a way that maximizes their expected gain
    when each choice’s properties are only partially known at the time of allocation
    and might become better understood as time passes or by allocating resources to
    the choice.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂赌博机是一个问题，其中必须在有限的资源集之间分配，以最大化预期收益，当每个选择的特性在分配时仅部分知道，并且随着时间的推移或通过将资源分配给选择可以更好地理解。
- en: This is a classic [reinforcement learning](https://oreil.ly/anwMJ) problem covering
    the *exploration*–*exploitation* trade-off dilemma, modeling an agent that simultaneously
    attempts to acquire new knowledge (called *exploration*) and optimize its decisions
    based on existing knowledge (called *exploitation*). As shown in [Figure 2-9](#explore_versus_exploit),
    the agent attempts to balance these competing tasks to maximize its total value
    over the period considered.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个经典的[强化学习](https://oreil.ly/anwMJ)问题，涵盖了*探索*与*利用*的权衡困境，建模一个同时试图获取新知识（称为*探索*）并基于现有知识优化其决策的代理。正如[图 2-9](#explore_versus_exploit)所示，代理试图平衡这些竞争任务，以在考虑期内最大化其总价值。
- en: '![paml 0209](assets/paml_0209.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0209](assets/paml_0209.png)'
- en: Figure 2-9\. Explore versus exploit
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9\. 探索与利用
- en: Meta-learning
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习
- en: This “learning to learn.” Think of this as applying machine learning to build
    machine learning models; hence, the term “meta” in the name. The goal of meta-learning
    is to train a model on a variety of learning tasks, such that it can solve new
    learning tasks with only a small number of training samples. Not only does this
    dramatically speed up and improve the design of machine learning pipelines, but
    also allows us to replace a fixed set of manually chosen models with novel approaches
    learned in a data-driven way.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是“学习如何学习”。将其视为应用机器学习来构建机器学习模型；因此，“元”在名称中。元学习的目标是在各种学习任务上训练模型，使其能够仅通过少量训练样本解决新的学习任务。这不仅极大地加快和改进了机器学习流水线的设计，还允许我们用数据驱动的方式替换固定的手动选择模型。
- en: With neural networks gaining popularity, meta-learning approaches have been
    applied to automatically design optimal neural network architectures. Known as
    neural architecture search (NAS), this is a popular area of research. NAS has
    been used to design networks that are on par with or outperform hand-designed
    architectures. Methods for NAS can be categorized according to the *search space*,
    *search strategy*, and *performance estimation strategy* used.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 随着神经网络的普及，元学习方法已被应用于自动设计最优神经网络架构。这被称为神经架构搜索（NAS），是一个研究热点领域。NAS 已被用来设计与手动设计架构相媲美或优于其的网络。NAS
    的方法可以根据使用的*搜索空间*、*搜索策略*和*性能估计策略*进行分类。
- en: The search space defines the type(s) of neural networks that can be designed
    and optimized. The search strategy defines the approach used to explore the search
    space. The performance estimation strategy evaluates the performance of a possible
    neural network from its design (without constructing and training it).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索空间定义了可以设计和优化的神经网络类型。搜索策略定义了探索搜索空间的方法。性能估计策略从设计角度评估可能的神经网络的性能（无需构建和训练它）。
- en: Monitoring and Retraining
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控和重新训练
- en: So far, we have covered the stages leading up to building a good model and how
    Automated Machine Learning can help with each of those stages. The last and final
    stage in the machine learning workflow is monitoring and retraining your model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了建立良好模型的阶段以及自动化机器学习如何在每个阶段帮助。机器学习工作流的最后一个阶段是监控和重新训练您的模型。
- en: Model performance during training can be very different from its performance
    after deployment on real data. Thus, it is important to continuously measure model
    performance even after deployment. Poor model performance is typically caused
    by change in characteristics of input data over time, which is known as *data
    drift*. Techniques exist to automatically monitor data drift and model performance
    over time.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在训练期间的表现可能与在实际数据部署后的表现有很大不同。因此，即使在部署后也要持续测量模型性能非常重要。模型性能不佳通常是由于时间内输入数据特征的变化引起的，这被称为*数据漂移*。存在技术可以自动监控数据漂移和模型性能随时间的变化。
- en: 'As soon as poor model performance is detected, corrective actions can be taken
    to minimize the damage. Corrective actions could include the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦检测到模型性能下降，可以立即采取纠正措施以最小化损害。纠正措施可能包括以下内容：
- en: Immediately take the model offline (and disable the corresponding user experience)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 立即将模型下线（并禁用相应的用户体验）
- en: Retrain the model with the latest data and deploy the retrained model
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最新数据重新训练模型并部署更新后的模型
- en: This stage is particularly critical for companies that have production dependency
    on machine learning models. Hence, a good Automated Machine Learning solution
    should have support for monitoring and training.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于依赖于生产环境的机器学习模型的公司来说，这个阶段尤为关键。因此，一个好的自动化机器学习解决方案应该支持监控和训练。
- en: Bringing It All Together
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全面总结
- en: Automated Machine Learning empowers users (with or without machine learning
    expertise) to identify an end-to-end machine learning pipeline for any problem,
    achieving higher accuracy while spending far less of their time. And it enables
    a significantly larger number of experiments to be run, resulting in faster iteration
    toward production-ready intelligent experiences. Given input data, it can automate
    the process of feature engineering, model selection, and hyperparameter tuning,
    as shown in [Figure 2-10](#automated_machine_learning_on_microsoft).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习使用户（无论是否具有机器学习专业知识）能够为任何问题识别一个端到端的机器学习管道，实现更高的准确性，同时花费更少的时间。它能够运行更多实验，加速向生产就绪的智能体验迭代。根据输入数据，它可以自动化特征工程、模型选择和超参数调优的过程，如图
    [2-10](#automated_machine_learning_on_microsoft) 所示。
- en: '![paml 0210](assets/paml_0210.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0210](assets/paml_0210.png)'
- en: Figure 2-10\. Automated Machine Learning
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-10\. 自动化机器学习
- en: Automated ML
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化 ML
- en: '*Automated ML* is a capability available within the Microsoft Azure Machine
    Learning service. This section provides an overview of how automated ML works,
    whereas subsequent chapters will go into more details on how to use automated
    ML for your scenarios.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*自动化 ML* 是 Microsoft Azure 机器学习服务中提供的一种能力。本节概述了自动化 ML 的工作原理，后续章节将详细介绍如何在您的场景中使用自动化
    ML。'
- en: How Automated ML Works
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化 ML 的工作原理
- en: Automated ML is based on a [breakthrough from the Microsoft Research division](https://arxiv.org/abs/1705.05355).
    The approach combines ideas from collaborative filtering and Bayesian optimization
    to search an enormous space of possible machine learning pipelines intelligently
    and efficiently. It’s essentially a recommender system for machine learning pipelines.
    Similar to how streaming services recommend movies for users, automated ML recommends
    machine learning pipelines for datasets. Figures [2-11](#streaming_service_colon_movie_recommenda)
    and [2-12](#automated_ml_colon_machine_learning_pipe) demonstrate this analogy.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化 ML 基于微软研究部门的[突破性成果](https://arxiv.org/abs/1705.05355)。该方法将协同过滤和贝叶斯优化的思想结合起来，智能高效地搜索大量可能的机器学习流水线。本质上，它是机器学习流水线的推荐系统。与流媒体服务为用户推荐电影类似，自动化
    ML 为数据集推荐机器学习流水线。图 [2-11](#streaming_service_colon_movie_recommenda) 和 [2-12](#automated_ml_colon_machine_learning_pipe)
    展示了这个类比。
- en: '![paml 0211](assets/paml_0211.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0211](assets/paml_0211.png)'
- en: 'Figure 2-11\. Streaming service: movie recommendation'
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-11\. 流媒体服务：电影推荐
- en: '![paml 0212](assets/paml_0212.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0212](assets/paml_0212.png)'
- en: 'Figure 2-12\. Automated ML: machine learning pipeline recommendation'
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-12\. 自动化 ML：机器学习流水线推荐
- en: As indicated by the distributions shown on the right side of Figures [2-11](#streaming_service_colon_movie_recommenda)
    and [2-12](#automated_ml_colon_machine_learning_pipe), automated ML also takes
    uncertainty into account, incorporating a probabilistic model to determine the
    best pipeline to try next. This approach allows automated ML to explore the most
    promising possibilities without exhaustive search, and to converge on the best
    pipelines for the user’s data faster than competing brute-force approaches.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2-11](#streaming_service_colon_movie_recommenda) 和 [2-12](#automated_ml_colon_machine_learning_pipe)
    右侧所示的分布所示，自动化 ML 还考虑了不确定性，采用概率模型确定下一步尝试的最佳流水线。这种方法允许自动化 ML 在不进行穷举搜索的情况下探索最有前途的可能性，并更快地收敛于用户数据的最佳流水线，胜过竞争对手的蛮力方法。
- en: Preserving Privacy
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保护隐私
- en: Automated ML accomplishes all this without having to see the users’ data, preserving
    privacy. As shown in [Figure 2-13](#preserving_privacy-id1), users’ data and execution
    of the machine learning pipeline both reside in the users’ cloud subscription
    (or their local machine), for which they have complete control. Only the model
    performance metrics of each pipeline run are sent back to the automated ML service,
    which then makes an intelligent, probabilistic choice of which pipelines should
    be tried next.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化 ML 实现了所有这些，而无需查看用户的数据，从而保护隐私。如 [图 2-13](#preserving_privacy-id1) 所示，用户的数据和机器学习流水线的执行都驻留在用户的云订阅中（或其本地计算机），用户完全控制。只有每个流水线运行的模型性能指标被发送回自动化
    ML 服务，然后做出智能的、概率性的选择，确定下一步应该尝试哪些流水线。
- en: Automated ML’s probabilistic model has been trained by running hundreds of millions
    of experiments, each involving evaluation of a specific pipeline on a given dataset.
    This training now allows the automated ML service to find good solutions quickly
    for new problems. And the model continues to learn and improve as it runs on new
    machine learning tasks—even though, as just mentioned, it does not see users’
    data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化 ML 的概率模型经过数亿次实验训练，每次实验都涉及在特定数据集上评估特定流水线。现在，这种训练使得自动化 ML 服务能够快速为新问题找到良好的解决方案。并且模型在运行新的机器学习任务时继续学习和改进，尽管如前所述，它不会看到用户的数据。
- en: '![paml 0213](assets/paml_0213.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0213](assets/paml_0213.png)'
- en: Figure 2-13\. Preserving privacy
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-13\. 保护隐私
- en: Enabling Transparency
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现透明度
- en: Transparency is important for data scientists as well as other users so that
    they can understand what’s going on, and trust the output. This is especially
    crucial for enterprises to use in business-critical scenarios in production.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家以及其他用户都认为透明度非常重要，这样他们才能理解发生的情况并信任输出结果。这对企业在生产中使用关键业务场景尤为关键。
- en: Automated ML’s heavy focus on transparency makes it easy to understand the produced
    machine learning pipelines, including all of the stages discussed in the previous
    section (e.g., data understanding, feature engineering, model selection/optimization).
    Users can either directly use the machine learning pipeline produced or they can
    customize it further.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习（Automated ML）强调透明度，使得理解生成的机器学习流水线变得轻松，包括前一节中讨论的所有阶段（例如数据理解、特征工程、模型选择/优化）。用户可以直接使用生成的机器学习流水线，或者进一步进行定制。
- en: Another aspect of transparency is to understand how the input features contribute
    to the outcome of the model, also known as *model explainability* or *interpretability*.
    Automated ML makes this easy by offering a feature importance capability. [Figure 2-14](#feature_importance-id1)
    shows an example of a customer churn model for which the `SupportIncidents` count
    is the top contributing feature. This makes sense because if a customer has had
    a lot of support issues, the likelihood of them churning is much higher.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度的另一个方面是理解输入特征如何影响模型的结果，也称为*模型可解释性*或*解释性*。自动化ML通过提供特征重要性功能使这一过程变得简单。[图 2-14](#feature_importance-id1)展示了一个客户流失模型的示例，其中`SupportIncidents`计数是最重要的特征贡献因素。这是有道理的，因为如果客户遇到了很多支持问题，他们流失的可能性就更高。
- en: '![paml 0214](assets/paml_0214.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0214](assets/paml_0214.png)'
- en: Figure 2-14\. Feature importance
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-14\. 特征重要性
- en: Guardrails
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 守护栏
- en: In addition to providing transparency, automated ML on Azure also offers guardrails
    to help users understand potential issues with their data (e.g., missing values,
    class imbalance) or models and help take corrective actions for improved results.
    We go into more detail about this in [Chapter 7](ch07.html#model_interpretability_and_transparency).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提供透明度外，Azure上的自动化ML还提供守护栏，帮助用户了解数据（例如缺失值、类别不平衡）或模型可能存在的问题，并帮助采取改进结果的纠正措施。我们将在[第7章](ch07.html#model_interpretability_and_transparency)中详细介绍这一点。
- en: End-to-End Model Life-Cycle Management
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 端到端模型生命周期管理
- en: Automated ML, being a capability of Azure Machine Learning, offers end-to-end
    (E2E) model life-cycle management, including easy deployment, monitoring, drift
    analysis, and retraining through integration with ML operationalization (MLOps)
    capability of Azure Machine Learning. [Figure 2-15](#e2e_model_life_cycle_management)
    shows this E2E flow.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化ML作为Azure机器学习的一个能力，提供端到端（E2E）模型生命周期管理，包括通过与Azure机器学习的ML运营能力集成，简单部署、监控、漂移分析和重新训练。[图 2-15](#e2e_model_life_cycle_management)展示了这一端到端的流程。
- en: '![paml 0215](assets/paml_0215.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![paml 0215](assets/paml_0215.png)'
- en: Figure 2-15\. E2E model life-cycle management
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-15\. E2E模型生命周期管理
- en: Conclusion
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, you learned what Automated Machine Learning is, generally speaking,
    and how it can help with every stage of building a good machine learning model
    to solve real-world problems. We also provided a brief overview of how the Azure
    Machine Learning capability, automated ML, works behind the scenes to build good
    machine learning models and enable trust by allowing transparency and preserving
    data privacy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了什么是自动化机器学习，以及它如何帮助解决实际问题中建立良好机器学习模型的每个阶段。我们还简要介绍了Azure机器学习能力自动化ML在幕后如何构建良好的机器学习模型，并通过允许透明度和保护数据隐私来建立信任的简要概述。
- en: In subsequent chapters, we’ll cover different aspects of what we touched upon
    here, and provide hands-on practice and sample scenarios to help you use automated
    ML in your work.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续章节中，我们将涵盖我们在这里提到的不同方面，并提供实践和示例场景，帮助您在工作中使用自动化机器学习。
