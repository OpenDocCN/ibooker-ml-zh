- en: Chapter 2\. Introduction to TensorFlow Extended
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第2章\. TensorFlow Extended简介
- en: In the previous chapter, we introduced the concept of machine learning pipelines
    and discussed the components that make up a pipeline. In this chapter, we introduce
    TensorFlow Extended (TFX). The TFX library supplies all the components we will
    need for our machine learning pipelines. We define our pipeline tasks using TFX,
    and they can then be executed with a pipeline orchestrator such as Airflow or
    Kubeflow Pipelines. [Figure 2-1](#filepos83972) gives an overview of the pipeline
    steps and how the different tools fit together.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了机器学习流水线的概念，并讨论了构成流水线的各个组件。在本章中，我们介绍TensorFlow Extended（TFX）。TFX库提供了我们机器学习流水线所需的所有组件。我们使用TFX定义我们的流水线任务，然后可以使用流水线编排器（如Airflow或Kubeflow
    Pipelines）执行这些任务。[图2-1](#filepos83972)概述了流水线步骤及不同工具如何配合使用。
- en: '![](images/00012.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00012.jpg)'
- en: Figure 2-1\. TFX as part of ML pipelines
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-1\. TFX作为ML流水线的一部分
- en: 'In this chapter, we will guide you through the installation of TFX, explaining
    basic concepts and terminology that will set the stage for the following chapters.
    In those chapters, we take an in-depth look at the individual components that
    make up our pipelines. We also introduce [Apache Beam](https://beam.apache.org)
    in this chapter. Beam is an open source tool for defining and executing data-processing
    jobs. It has two uses in TFX pipelines: first, it runs under the hood of many
    TFX components to carry out processing steps like data validation or data preprocessing.
    Second, it can be used as a pipeline orchestrator, as we discussed in [Chapter 1](index_split_006.html#filepos46283).
    We introduce Beam here because it will help you understand TFX components, and
    it is essential if you wish to write custom components, as we discuss in [Chapter 10](index_split_017.html#filepos1073133).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将指导您安装TFX，并解释将为后续章节铺垫的基本概念和术语。在这些章节中，我们深入探讨了构成我们流水线的各个组件。在本章中，我们还介绍了[Apache
    Beam](https://beam.apache.org)。Beam是一个用于定义和执行数据处理作业的开源工具。在TFX流水线中，它有两个用途：首先，它在许多TFX组件的幕后运行，执行诸如数据验证或数据预处理之类的处理步骤。其次，它可以用作流水线编排器，正如我们在[第1章](index_split_006.html#filepos46283)中讨论的那样。我们在这里介绍Beam，因为它将帮助您理解TFX组件，并且如果您希望编写自定义组件（正如我们在[第10章](index_split_017.html#filepos1073133)中讨论的那样），它是必不可少的。
- en: What Is TFX?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是TFX？
- en: Machine learning pipelines can become very complicated and consume a lot of
    overhead to manage task dependencies. At the same time, machine learning pipelines
    can include a variety of tasks, including tasks for data validation, preprocessing,
    model training, and any post-training tasks. As we discussed in [Chapter 1](index_split_006.html#filepos46283),
    the connections between tasks are often brittle and cause the pipelines to fail.
    These connections are also known as the glue code from the publication [“Hidden
    Technical Debt in Machine Learning Systems”](https://oreil.ly/SLttH). Having brittle
    connections ultimately means that production models will be updated infrequently,
    and data scientists and machine learning engineers loathe updating stale models.
    Pipelines also require well-managed distributed processing, which is why TFX leverages
    Apache Beam. This is especially true for large workloads.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习流水线可能变得非常复杂，并消耗大量开销来管理任务依赖关系。同时，机器学习流水线可以包括各种任务，包括数据验证、预处理、模型训练以及任何训练后任务。正如我们在[第1章](index_split_006.html#filepos46283)中讨论的那样，任务之间的连接通常很脆弱，导致流水线失败。这些连接也被称为来自于出版物[“机器学习系统中的隐藏技术债务”](https://oreil.ly/SLttH)的胶水代码。连接脆弱最终意味着生产模型更新不频繁，数据科学家和机器学习工程师厌恶更新过时模型。流水线还需要良好管理的分布式处理，这就是为什么TFX利用Apache
    Beam。对于大型工作负载来说，这一点尤为重要。
- en: Google faced the same problem internally and decided to develop a platform to
    simplify the pipeline definitions and to minimize the amount of task boilerplate
    code to write. The open source version of Google’s internal ML pipeline framework
    is TFX.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Google在内部面临相同问题，并决定开发一个平台来简化流水线定义，减少编写任务样板代码的工作量。Google内部ML流水线框架的开源版本是TFX。
- en: '[Figure 2-2](#filepos87364) shows the general pipeline architecture with TFX.
    Pipeline orchestration tools are the foundation for executing our tasks. Besides
    the orchestration tools, we need a data store to keep track of the intermediate
    pipeline results. The individual components communicate with the data store to
    receive their inputs, and they return the results to the data store. These results
    can then be inputs to following tasks. TFX provides the layer that combines all
    of these tools, and it provides the individual components for the major pipeline
    tasks.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-2](#filepos87364) 展示了带有 TFX 的通用流水线架构。流水线编排工具是执行我们任务的基础。除了编排工具外，我们还需要一个数据存储来跟踪中间流水线结果。各个组件通过数据存储进行通信以获取其输入，并将结果返回给数据存储。这些结果随后可以作为后续任务的输入。TFX
    提供了将所有这些工具组合在一起的层，并提供了主要流水线任务的各个组件。'
- en: '![](images/00037.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00037.jpg)'
- en: Figure 2-2\. ML pipeline architecture
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-2\. 机器学习流水线架构
- en: Initially, Google released some of the pipeline functionality as open source
    TensorFlow libraries (e.g., TensorFlow Serving is discussed in [Chapter 8](index_split_013.html#filepos764992))
    under the umbrella of TFX libraries. In 2019, Google then published the open source
    glue code containing all the required pipeline components to tie the libraries
    together and to automatically create pipeline definitions for orchestration tools
    like Apache Airflow, Apache Beam, and Kubeflow Pipelines.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，Google 将一些流水线功能作为开源 TensorFlow 库发布（例如，在[第 8 章](index_split_013.html#filepos764992)中讨论了
    TensorFlow Serving），并纳入了 TFX 库的范畴。2019 年，Google 发布了包含所有必要流水线组件的开源粘合代码，用于将这些库连接在一起，并自动创建流水线定义，以供
    Apache Airflow、Apache Beam 和 Kubeflow Pipelines 等编排工具使用。
- en: 'TFX provides a variety of pipeline components that cover a good number of use
    cases. At the time of writing, the following components were available:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: TFX 提供了多种流水线组件，涵盖了许多使用案例。在编写本文时，以下组件可用：
- en: Data ingestion with `ExampleGen`
  id: totrans-13
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 `ExampleGen` 进行数据摄取
- en: Data validation with `StatisticsGen`, `SchemaGen`, and the `ExampleValidator`
  id: totrans-14
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 `StatisticsGen`、`SchemaGen` 和 `ExampleValidator` 进行数据验证
- en: Data preprocessing with `Transform`
  id: totrans-15
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 `Transform` 进行数据预处理
- en: Model training with `Trainer`
  id: totrans-16
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 `Trainer` 进行模型训练
- en: Checking for previously trained models with `ResolverNode`
  id: totrans-17
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 `ResolverNode` 检查先前训练的模型
- en: Model analysis and validation with `Evaluator`
  id: totrans-18
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 `Evaluator` 进行模型分析和验证
- en: Model deployments with `Pusher`
  id: totrans-19
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 `Pusher` 进行模型部署
- en: '[Figure 2-3](#filepos90357) shows how the components of the pipeline and the
    libraries fit together.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-3](#filepos90357) 展示了流水线组件和库的结合方式。'
- en: '![](images/00105.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00105.jpg)'
- en: Figure 2-3\. TFX components and libraries
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-3\. TFX 组件和库
- en: We will discuss the components and libraries in greater detail in the following
    chapters. In case you need some nonstandard functionality, in [Chapter 10](index_split_017.html#filepos1073133)
    we discuss how to create custom pipeline components.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中详细讨论组件和库。如果需要一些非标准功能，在[第 10 章](index_split_017.html#filepos1073133)中我们将讨论如何创建自定义流水线组件。
- en: STABLE RELEASE OF TFX
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: TFX 的稳定版本
- en: At the time of writing this chapter, a stable 1.X version of TFX hasn’t been
    released. The TFX API mentioned in this and the following chapters might be subject
    to future updates. To the best of our knowledge, all the examples in this book
    will work with TFX version 0.22.0.
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在编写本章时，尚未发布稳定的 TFX 1.X 版本。本书中提到的 TFX API 可能会在未来更新中进行更新。据我们所知，本书中的所有示例将与 TFX
    版本 0.22.0 兼容。
- en: Installing TFX
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 TFX
- en: 'TFX can easily be installed by running the following Python installer command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下 Python 安装程序命令可轻松安装 TFX：
- en: '`$` `pip install tfx`'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `$ pip install tfx`'
- en: The `tfx` package comes with a variety of dependencies that will be installed
    automatically. It installs not only the individual TFX Python packages (e.g.,
    TensorFlow Data Validation), but also their dependencies like Apache Beam.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`tfx` 软件包附带多种自动安装的依赖项。它不仅安装各个 TFX Python 软件包（例如 TensorFlow 数据验证），还包括诸如 Apache
    Beam 等的依赖项。'
- en: 'After installing TFX, you can import the individual Python packages. We recommend
    taking this approach if you want to use the individual TFX packages (e.g., you
    want to validate a dataset using TensorFlow Data Validation, see [Chapter 4](index_split_009.html#filepos295199)):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完 TFX 后，您可以导入各个 Python 软件包。如果想使用单独的 TFX 软件包（例如，使用 TensorFlow 数据验证验证数据集，请参见[第
    4 章](index_split_009.html#filepos295199)）推荐采用这种方式：
- en: '`import``tensorflow_data_validation``as``tfdv``import``tensorflow_transform``as``tft``import``tensorflow_transform.beam``as``tft_beam``...`'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``tensorflow_data_validation``as``tfdv``import``tensorflow_transform``as``tft``import``tensorflow_transform.beam``as``tft_beam``...`'
- en: 'Alternatively, you can import the corresponding TFX component (if using the
    components in the context of a pipeline):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以导入相应的TFX组件（如果在管道上下文中使用组件）：
- en: '`from``tfx.components``import``ExampleValidator``from``tfx.components``import``Evaluator``from``tfx.components``import``Transform``...`'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tfx.components``import``ExampleValidator``from``tfx.components``import``Evaluator``from``tfx.components``import``Transform``...`'
- en: Overview of TFX Components
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: TFX组件概述
- en: 'A component handles a more complex process than just the execution of a single
    task. All machine learning pipeline components read from a channel to get input
    artifacts from the metadata store. The data is then loaded from the path provided
    by the metadata store and processed. The output of the component, the processed
    data, is then provided to the next pipeline components. The generic internals
    of a component are always:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个组件处理的过程比仅执行单个任务更复杂。所有机器学习管道组件从通道读取以获取来自元数据存储的输入工件。然后从元数据存储提供的路径加载数据并进行处理。组件的输出，即处理后的数据，然后提供给下一个管道组件。组件的通用内部始终是：
- en: Receive some input
  id: totrans-36
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 接收一些输入
- en: Perform an action
  id: totrans-37
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行一个操作
- en: Store the final result
  id: totrans-38
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 存储最终结果
- en: In TFX terms, the three internal parts of the component are called the driver,
    executor, and publisher. The driver handles the querying of the metadata store.
    The executor performs the actions of the components. And the publisher manages
    the saving of the output metadata in the MetadataStore. The driver and the publisher
    aren’t moving any data. Instead, they read and write references from the MetadataStore.
    [Figure 2-4](#filepos97391) shows the structure of a TFX component.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在TFX术语中，组件的三个内部部分分别称为驱动程序、执行程序和发布者。驱动程序处理元数据存储的查询。执行程序执行组件的操作。发布者管理将输出元数据保存到MetadataStore中。驱动程序和发布者不会移动任何数据。相反，它们从MetadataStore中读取和写入引用。[图2-4](#filepos97391)显示了TFX组件的结构。
- en: '![](images/00114.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00114.jpg)'
- en: Figure 2-4\. Component overview
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-4\. 组件概述
- en: The inputs and outputs of the components are called artifacts. Examples of artifacts
    include raw input data, preprocessed data, and trained models. Each artifact is
    associated with metadata stored in the MetadataStore. The artifact metadata consists
    of an artifact type as well as artifact properties. This artifact setup guarantees
    that the components can exchange data effectively. TFX currently provides ten
    different types of artifacts, which we review in the following chapters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 组件的输入和输出称为工件。工件的示例包括原始输入数据、预处理数据和训练模型。每个工件与存储在MetadataStore中的元数据相关联。工件元数据包括工件类型和工件属性。这种工件设置确保组件可以有效地交换数据。TFX目前提供了十种不同类型的工件，我们将在接下来的章节中进行审查。
- en: What Is ML Metadata?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是ML Metadata？
- en: The components of TFX “communicate” through metadata; instead of passing artifacts
    directly between the pipeline components, the components consume and publish references
    to pipeline artifacts. An artifact could be, for example, a raw dataset, a transform
    graph, or an exported model. Therefore, the metadata is the backbone of our TFX
    pipelines. One advantage of passing the metadata between components instead of
    the direct artifacts is that the information can be centrally stored.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: TFX的组件“通过元数据进行通信”；而不是直接在管道组件之间传递工件，组件使用和发布管道工件的引用。例如，一个工件可以是原始数据集、转换图或导出模型。因此，元数据是我们TFX管道的支柱。通过在组件之间传递元数据而不是直接传递工件的一个优点是信息可以集中存储。
- en: 'In practice, the workflow goes as follows: when we execute a component, it
    uses the ML Metadata (MLMD) API to save the metadata corresponding to the run.
    For example, the component driver receives the reference for a raw dataset from
    the metadata store. After the component execution, the component publisher will
    store the references of the component outputs in the metadata store. MLMD saves
    the metadata consistently to a MetadataStore, based on a storage backend. Currently,
    MLMD supports three types of backends:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，工作流程如下：当我们执行一个组件时，它使用ML Metadata（MLMD）API来保存与运行相对应的元数据。例如，组件驱动从元数据存储中接收原始数据集的引用。在组件执行之后，组件发布者将组件输出的引用存储到元数据存储中。MLMD基于存储后端一致地保存元数据到MetadataStore中。目前，MLMD支持三种类型的后端：
- en: In-memory database (via SQLite)
  id: totrans-46
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 内存数据库（通过SQLite）
- en: SQLite
  id: totrans-47
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SQLite
- en: MySQL
  id: totrans-48
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MySQL
- en: Because the TFX components are so consistently tracked, ML Metadata provides
    a variety of useful functions. We can compare two artifacts from the same component.
    For example, we see this in [Chapter 7](index_split_012.html#filepos624151) when
    we discuss model validation. In this particular case, TFX compares the model analysis
    results from a current run with the results from the previous run. This checks
    whether the more recently trained model has a better accuracy or loss compared
    to the previous model. The metadata can also be used to determine all the artifacts
    that have been based on another, previously created artifact. This creates a kind
    of audit trail for our machine learning pipelines.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TFX组件始终如一地跟踪，ML Metadata提供了多种有用的功能。例如，我们可以在第[7](index_split_012.html#filepos624151)章讨论模型验证时进行比较。在这种特定情况下，TFX比较当前运行的模型分析结果与先前运行的结果。该元数据还可以用于确定所有基于另一个先前创建的工件的工件。这为我们的机器学习流水线创建了一种审计跟踪方式。
- en: '[Figure 2-5](#filepos101137) shows that each component interacts with the MetadataStore,
    and the MetadataStore stores the metadata on the provided database backend.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-5](#filepos101137)显示每个组件与MetadataStore交互，而MetadataStore则将元数据存储在提供的数据库后端上。'
- en: '![](images/00001.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00001.jpg)'
- en: Figure 2-5\. Storing metadata with MLMD
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-5\. 使用MLMD存储元数据
- en: Interactive Pipelines
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式流水线
- en: Designing and implementing machine learning pipelines can be frustrating at
    times. It is sometimes challenging to debug components within a pipeline, for
    example. This is why the TFX functionality around interactive pipelines is beneficial.
    In fact, in the following chapters, we will implement a machine learning pipeline
    step by step and demonstrate its implementations through an interactive pipeline.
    The pipeline runs in a Jupyter Notebook, and the components’ artifacts can be
    immediately reviewed. Once you have confirmed the full functionality of your pipeline,
    in Chapters [11](index_split_018.html#filepos1264016) and [12](index_split_019.html#filepos1378763),
    we discuss how you can convert your interactive pipeline to a production-ready
    pipeline, for example, for execution on Apache Airflow.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 设计和实现机器学习流水线有时可能令人沮丧。例如，有时在流水线中调试组件是具有挑战性的。这就是为什么围绕交互式流水线的TFX功能如此有益。事实上，在接下来的章节中，我们将逐步实现一个机器学习流水线，并通过交互式流水线演示其实现。该流水线在Jupyter
    Notebook中运行，并且可以立即查看组件的工件。一旦确认了流水线的全部功能，在第[11](index_split_018.html#filepos1264016)章和第[12](index_split_019.html#filepos1378763)章，我们将讨论如何将交互式流水线转换为生产就绪的流水线，例如在Apache
    Airflow上执行。
- en: Any interactive pipeline is programmed in the context of a Jupyter Notebook
    or a Google Colab session. In contrast to the orchestration tools we will discuss
    in Chapters [11](index_split_018.html#filepos1264016) and [12](index_split_019.html#filepos1378763),
    interactive pipelines are orchestrated and executed by the user.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 任何交互式流水线都是在Jupyter Notebook或Google Colab会话的环境中编程的。与我们将在第[11](index_split_018.html#filepos1264016)章和第[12](index_split_019.html#filepos1378763)章讨论的编排工具相比，交互式流水线由用户编排和执行。
- en: 'You can start an interactive pipeline by importing the required packages:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过导入所需的包可以启动交互式流水线：
- en: '`import``tensorflow``as``tf``from``tfx.orchestration.experimental.interactive.interactive_context``import`
    `\` `InteractiveContext`'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``tensorflow``as``tf``from``tfx.orchestration.experimental.interactive.interactive_context``import`
    `\` `InteractiveContext`'
- en: 'Once the requirements are imported, you can create a `context` object. The
    context object handles component execution and displays the component’s artifacts.
    At this point, the `InteractiveContext` also sets up a simple in-memory ML MetadataStore:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 导入要求后，可以创建`context`对象。`context`对象处理组件执行并显示组件的工件。此时，`InteractiveContext`还设置了一个简单的内存中ML
    MetadataStore：
- en: '`context``=``InteractiveContext``()`'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`context``=``InteractiveContext``()`'
- en: 'After setting up your pipeline component(s) (e.g., `StatisticsGen`), you can
    then execute each component object through the `run` function of the `context`
    object, as shown in the following example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置流水线组件（例如`StatisticsGen`）之后，您可以通过`context`对象的`run`函数执行每个组件对象，如下例所示：
- en: '`from``tfx.components``import``StatisticsGen``statistics_gen``=``StatisticsGen``(``examples``=``example_gen``.``outputs``[``''examples''``])``context``.``run``(``statistics_gen``)`'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tfx.components``import``StatisticsGen``statistics_gen``=``StatisticsGen``(``examples``=``example_gen``.``outputs``[``''examples''``])``context``.``run``(``statistics_gen``)`'
- en: 'The component itself receives the outputs of the previous component (in our
    case, the data ingestion component `ExampleGen`) as an instantiation argument.
    After executing the component’s tasks, the component automatically writes the
    metadata of the output artifact to the metadata store. The output of some components
    can be displayed in your notebook. The immediate availability of the results and
    the visualizations is very convenient. For example, you can use the `StatisticsGen`
    component to inspect the features of the dataset:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 组件本身将前一组件的输出（在我们的案例中是数据摄取组件`ExampleGen`）作为实例化参数接收。在执行组件的任务后，组件会自动将输出工件的元数据写入元数据存储。一些组件的输出可以在你的笔记本中显示。结果和可视化的即时可用性非常方便。例如，你可以使用`StatisticsGen`组件来检查数据集的特征：
- en: '`context``.``show``(``statistics_gen``.``outputs``[``''statistics''``])`'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`context``.``show``(``statistics_gen``.``outputs``[``''statistics''``])`'
- en: After running the previous `context` function, you can see a visual overview
    of the statistics of the dataset in your notebook, as shown in [Figure 2-6](#filepos110240).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的`context`函数后，你可以在你的笔记本中看到数据集统计的视觉概述，如[图 2-6](#filepos110240)所示。
- en: 'Sometimes it can be advantageous to inspect the output artifacts of a component
    programmatically. After a component object has been executed, we can access the
    artifact properties, as shown in the following example. The properties depend
    on the specific artifact:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，通过程序检查组件的输出工件可能是有利的。在组件对象执行后，我们可以访问工件属性，如以下示例所示。属性取决于特定的工件：
- en: '`for``artifact``in``statistics_gen``.``outputs``[``''statistics''``]``.``get``():``print``(``artifact``.``uri``)`'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`for``artifact``in``statistics_gen``.``outputs``[``''statistics''``]``.``get``():``print``(``artifact``.``uri``)`'
- en: 'This gives the following result:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下结果：
- en: '`''/tmp/tfx-interactive-2020-05-15T04_50_16.251447/StatisticsGen/statistics/2''`'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`''/tmp/tfx-interactive-2020-05-15T04_50_16.251447/StatisticsGen/statistics/2''`'
- en: '![](images/00025.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00025.jpg)'
- en: Figure 2-6\. Interactive pipelines allow us to visually inspect our dataset
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-6\. 交互式流水线允许我们直观地检查数据集
- en: Throughout the following chapters, we will show how each component can be run
    in an interactive context. Then in Chapters [11](index_split_018.html#filepos1264016)
    and [12](index_split_019.html#filepos1378763), we will show the full pipeline
    and how it can be orchestrated by both Airflow and Kubeflow.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将展示如何在交互式环境中运行每个组件。然后在第[11](index_split_018.html#filepos1264016)和[12](index_split_019.html#filepos1378763)章中，我们将展示完整的流水线以及如何通过Airflow和Kubeflow进行编排。
- en: Alternatives to TFX
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: TFX的替代方案
- en: 'Before we take a deep dive into TFX components in the following chapters, let’s
    take a moment to look at alternatives to TFX. The orchestration of machine learning
    pipelines has been a significant engineering challenge in the last few years,
    and it should come as no surprise that many major Silicon Valley companies have
    developed their own pipeline frameworks. In the following table, you can find
    a small selection of frameworks:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解接下来的TFX组件之前，让我们花点时间看看TFX的替代方案。在过去几年中，机器学习流水线的编排一直是一个重要的工程挑战，毫无疑问，许多主要的硅谷公司都开发了自己的流水线框架。在以下表格中，你可以找到一些流行的框架：
- en: '|  Company  |  Framework  |  Link  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '-   |  公司  |  框架  |  链接  |'
- en: '|  AirBnb  |  AeroSolve  |   [https://github.com/airbnb/aerosolve](https://github.com/airbnb/aerosolve)
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '-   |  AirBnb  |  AeroSolve  |   [https://github.com/airbnb/aerosolve](https://github.com/airbnb/aerosolve)
    |'
- en: '|  Stripe  |  Railyard  |   [https://stripe.com/blog/railyard-training-models](https://stripe.com/blog/railyard-training-models)
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '-   |  条纹  |  铁路场  |   [https://stripe.com/blog/railyard-training-models](https://stripe.com/blog/railyard-training-models)
    |'
- en: '|  Spotify  |  Luigi  |   [https://github.com/spotify/luigi](https://github.com/spotify/luigi)
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '-   |  Spotify  |  Luigi  |   [https://github.com/spotify/luigi](https://github.com/spotify/luigi)
    |'
- en: '|  Uber  |  Michelangelo  |   [https://eng.uber.com/michelangelo-machine-learning-platform/](https://eng.uber.com/michelangelo-machine-learning-platform/)
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '-   |  Uber  |  Michelangelo  |   [https://eng.uber.com/michelangelo-machine-learning-platform/](https://eng.uber.com/michelangelo-machine-learning-platform/)
    |'
- en: '|  Netflix  |  Metaflow  |   [https://metaflow.org/](https://metaflow.org/)
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '-   |  Netflix  |  Metaflow  |   [https://metaflow.org/](https://metaflow.org/)
    |'
- en: Since the frameworks originated from corporations, they were designed with specific
    engineering stacks in mind. For example, AirBnB’s AeroSolve focuses on Java-based
    inference code, and Spotify’s Luigi focuses on efficient orchestration. TFX is
    no different in this regard. At this point, TFX architectures and data structures
    assume that you are using TensorFlow (or Keras) as your machine learning framework.
    Some TFX components can be used in combination with other machine learning frameworks.
    For example, data can be analyzed with TensorFlow Data Validation and later consumed
    by a scikit-learn model. However, the TFX framework is closely tied to TensorFlow
    or Keras models. Since TFX is backed by the TensorFlow community and more companies
    like Spotify are adopting TFX, we believe it is a stable and mature framework
    that will ultimately be adopted by a broader base of machine learning engineers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些框架起源于各大公司，它们设计时考虑了特定的工程堆栈。例如，AirBnB的AeroSolve专注于基于Java的推断代码，而Spotify的Luigi则专注于高效的编排。TFX在这方面也不例外。此时，TFX的架构和数据结构假设你正在使用TensorFlow（或Keras）作为机器学习框架。某些TFX组件可以与其他机器学习框架结合使用。例如，可以使用TensorFlow
    Data Validation分析数据，然后由scikit-learn模型使用。然而，TFX框架与TensorFlow或Keras模型密切相关。由于TFX由TensorFlow社区支持，并且像Spotify这样的公司正在采用TFX，我们相信它是一个稳定且成熟的框架，最终将被更广泛的机器学习工程师采纳。
- en: Introduction to Apache Beam
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Beam简介
- en: 'A variety of TFX components and libraries (e.g., TensorFlow Transform) rely
    on Apache Beam to process pipeline data efficiently. Because of the importance
    for the TFX ecosystem, we would like to provide a brief introduction into how
    Apache Beam works behind the scenes of the TFX components. In [Chapter 11](index_split_018.html#filepos1264016),
    we will then discuss how to use Apache Beam for a second purpose: as a pipeline
    orchestrator tool.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 多种TFX组件和库（例如TensorFlow Transform）依赖于Apache Beam来高效处理管道数据。由于在TFX生态系统中的重要性，我们想简要介绍Apache
    Beam在TFX组件背后的工作原理。在[第11章](index_split_018.html#filepos1264016)中，我们将讨论如何将Apache
    Beam用于第二个目的：作为管道编排工具。
- en: Apache Beam offers you an open source, vendor-agnostic way to describe data
    processing steps that then can be executed on various environments. Since it is
    incredibly versatile, Apache Beam can be used to describe batch processes, streaming
    operations, and data pipelines. In fact, TFX relies on Apache Beam and uses it
    under the hood for a variety of components (e.g., TensorFlow Transform or TensorFlow
    Data Validation). We will discuss the specific use of Apache Beam in the TFX ecosystem
    when we talk about TensorFlow Data Validation in [Chapter 4](index_split_009.html#filepos295199)
    and TensorFlow Transform in [Chapter 5](index_split_010.html#filepos397186).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Beam为你提供了一种开源、供应商无关的方式来描述数据处理步骤，然后可以在各种环境中执行。由于其极大的灵活性，Apache Beam可以用于描述批处理过程、流处理操作和数据管道。事实上，TFX依赖于Apache
    Beam，并在幕后使用它来支持各种组件（例如TensorFlow Transform或TensorFlow Data Validation）。我们将在[第4章](index_split_009.html#filepos295199)中讨论Apache
    Beam在TFX生态系统中的具体用法，以及在[第5章](index_split_010.html#filepos397186)中讨论TensorFlow Transform。
- en: While Apache Beam abstracts away the data processing logic from its supporting
    runtime tools, it can be executed on multiple distributed processing runtime environments.
    This means that you can run the same data pipeline on Apache Spark or Google Cloud
    Dataflow without a single change in the pipeline description. Also, Apache Beam
    was not just developed to describe batch processes but to support streaming operations
    seamlessly.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Apache Beam将数据处理逻辑与支持运行时工具分离，但它可以在多个分布式处理运行时环境上执行。这意味着你可以在Apache Spark或Google
    Cloud Dataflow上运行相同的数据管道，而不需要改变管道描述。此外，Apache Beam不仅仅是为了描述批处理过程，还能无缝支持流处理操作。
- en: Setup
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 设置
- en: 'The installation of Apache Beam is straightforward. You can install the latest
    version with:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Apache Beam非常简单。你可以通过以下命令安装最新版本：
- en: '`$` `pip install apache-beam`'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `pip install apache-beam`'
- en: 'If you plan to use Apache Beam in the context of Google Cloud Platform—for
    example, if you want to process data from Google BigQuery or run our data pipelines
    on Google Cloud Dataflow (as described in [“Processing Large Datasets with GCP”](index_split_009.html#filepos370559))—you
    should install Apache Beam as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划在Google Cloud Platform的背景下使用Apache Beam，例如处理来自Google BigQuery的数据或在Google
    Cloud Dataflow上运行我们的数据管道（如[“使用GCP处理大型数据集”](index_split_009.html#filepos370559)中描述的），你应该按以下方式安装Apache
    Beam：
- en: '`$` `pip install` `''apache-beam[gcp]''`'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `pip install` `''apache-beam[gcp]''`'
- en: 'If you plan to use Apache Beam in the context of Amazon Web Services (AWS)
    (e.g., if you want to load data from S3 buckets), you should install Apache Beam
    as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您计划在Amazon Web Services（AWS）的上下文中使用Apache Beam（例如，如果您想要从S3存储桶加载数据），则应按以下方式安装Apache
    Beam：
- en: '`$` `pip install` `''apache-beam[boto]''`'
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `pip install` `''apache-beam[boto]''`'
- en: If you install TFX with the Python package manager `pip`, Apache Beam will be
    automatically installed.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用Python包管理器`pip`安装TFX，Apache Beam将自动安装。
- en: Basic Data Pipeline
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基本数据流水线
- en: 'Apache Beam’s abstraction is based on two concepts: collections and transformations.
    On the one hand, Beam’s collections describe operations where data is being read
    or written from or to a given file or stream. On the other hand, Beam’s transformations
    describe ways to manipulate the data. All collections and transformations are
    executed in the context of a pipeline (expressed in Python through the context
    manager command `with`). When we define our collections or transformations in
    our following example, no data is actually being loaded or transformed. This only
    happens when the pipeline is executed in the context of a runtime environment
    (e.g., Apache Beam’s DirectRunner, Apache Spark, Apache Flink, or Google Cloud
    Dataflow).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Beam的抽象基于两个概念：集合和转换。一方面，Beam的集合描述了从给定文件或流中读取或写入数据的操作。另一方面，Beam的转换描述了如何操作数据。所有的集合和转换都在流水线的上下文中执行（在Python中通过上下文管理器命令`with`表达）。当我们在下面的示例中定义我们的集合或转换时，并没有实际加载或转换任何数据。这只有在流水线在运行时环境（例如Apache
    Beam的DirectRunner、Apache Spark、Apache Flink或Google Cloud Dataflow）的上下文中执行时才会发生。
- en: Basic collection example
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基本集合示例
- en: Data pipelines usually start and end with data being read or written, which
    is handled in Apache Beam through collections, often called `PCollections`. The
    collections are then transformed, and the final result can be expressed as a collection
    again and written to a filesystem.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流水线通常从读取或写入数据开始和结束，在Apache Beam中通过集合（通常称为`PCollections`）处理。然后对集合进行转换，并且最终结果可以再次表达为集合并写入文件系统。
- en: 'The following example shows how to read a text file and return all lines:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何读取文本文件并返回所有行：
- en: '`import``apache_beam``as``beam``with``beam``.``Pipeline``()``as``p``:`![](images/00002.jpg)`lines``=``p``|``beam``.``io``.``ReadFromText``(``input_file``)`![](images/00075.jpg)'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``apache_beam``as``beam``with``beam``.``Pipeline``()``as``p``:`![](images/00002.jpg)`lines``=``p``|``beam``.``io``.``ReadFromText``(``input_file``)`![](images/00075.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Use the context manager to define the pipeline.
  id: totrans-100
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用上下文管理器来定义流水线。
- en: '![](images/00075.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Read the text into a `PCollection`.
  id: totrans-102
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将文本读入`PCollection`。
- en: 'Similar to the `ReadFromText` operation, Apache Beam provides functions to
    write collections to a text file (e.g., `WriteToText`). The write operation is
    usually performed after all transformations have been executed:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`ReadFromText`操作，Apache Beam提供了将集合写入文本文件的函数（例如`WriteToText`）。写操作通常在所有转换执行后进行：
- en: '`with``beam``.``Pipeline``()``as``p``:``...``output``|``beam``.``io``.``WriteToText``(``output_file``)`![](images/00002.jpg)'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`with``beam``.``Pipeline``()``as``p``:``...``output``|``beam``.``io``.``WriteToText``(``output_file``)`![](images/00002.jpg)'
- en: '![](images/00002.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Write the `output` to the file output_file.
  id: totrans-106
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将`output`写入文件`output_file`。
- en: Basic transformation example
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 基本转换示例
- en: 'In Apache Beam, data is manipulated through transformations. As we see in this
    example and later in [Chapter 5](index_split_010.html#filepos397186), the transformations
    can be chained by using the pipe operator `|`. If you chain multiple transformations
    of the same type, you have to provide a name for the operation, noted by the string
    identifier between the pipe operator and the right-angle brackets. In the following
    example, we apply all transformations sequentially on our lines extracted from
    the text file:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在Apache Beam中，数据通过转换进行操作。正如我们在本例中看到的，并且稍后在[第5章](index_split_010.html#filepos397186)中看到的，可以通过使用管道操作符`|`将转换链接在一起。如果您链式地应用同一类型的多个转换，则必须为操作提供一个名称，由管道操作符和尖括号之间的字符串标识符表示。在以下示例中，我们将所有转换顺序应用于从文本文件中提取的行：
- en: '`counts``=``(``lines``|``''Split''``>>``beam``.``FlatMap``(``lambda``x``:``re``.``findall``(``r``''[A-Za-z``\''``]+''``,``x``))``|``''PairWithOne''``>>``beam``.``Map``(``lambda``x``:``(``x``,``1``))``|``''GroupAndSum''``>>``beam``.``CombinePerKey``(``sum``))`'
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`counts``=``(``lines``|``''Split''``>>``beam``.``FlatMap``(``lambda``x``:``re``.``findall``(``r``''[A-Za-z``\''``]+''``,``x``))``|``''PairWithOne''``>>``beam``.``Map``(``lambda``x``:``(``x``,``1``))``|``''GroupAndSum''``>>``beam``.``CombinePerKey``(``sum``))`'
- en: Let’s walk through this code in detail. As an example, we’ll take the phrases
    “Hello, how do you do?” and “I am well, thank you.”
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细地分析这段代码。例如，我们将采用短语“Hello, how do you do?”和“I am well, thank you.”作为示例。
- en: 'The `Split` transformation uses `re.findall` to split each line into a list
    of tokens, giving the result:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`Split` 转换使用 `re.findall` 将每行分割成一个标记列表，结果如下：'
- en: '`[``"Hello"``,``"how"``,``"do"``,``"you"``,``"do"``]``[``"I"``,``"am"``,``"well"``,``"thank"``,``"you"``]`'
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`[``"Hello"``,``"how"``,``"do"``,``"you"``,``"do"``]``[``"I"``,``"am"``,``"well"``,``"thank"``,``"you"``]`'
- en: '`beam.FlatMap` maps the result into a `PCollection`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`beam.FlatMap` 将结果映射到 `PCollection`：'
- en: '`"Hello" "how" "do" "you" "do" "I" "am" "well" "thank" "you"`'
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`"Hello" "how" "do" "you" "do" "I" "am" "well" "thank" "you"`'
- en: 'Next, the `PairWithOne` transformation uses `beam.Map` to create a tuple out
    of every token and the count (1 for each result):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`PairWithOne` 转换使用 `beam.Map` 来创建每个标记和计数（每个结果为1）的元组：
- en: '`("Hello", 1) ("how", 1) ("do", 1) ("you", 1) ("do", 1) ("I", 1) ("am", 1)
    ("well", 1) ("thank", 1) ("you", 1)`'
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`"Hello", 1 "how", 1 "do", 1 "you", 1 "do", 1 "I", 1 "am", 1 "well", 1 "thank",
    1 "you"`'
- en: 'Finally, the `GroupAndSum` transformation sums up all individual tuples for
    each token:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`GroupAndSum` 转换会对每个标记的所有单独元组进行求和：
- en: '`("Hello", 1) ("how", 1) ("do", 2) ("you", 2) ("I", 1) ("am", 1) ("well", 1)
    ("thank", 1)`'
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`"Hello", 1 "how", 1 "do", 2 "you", 2 "I", 1 "am", 1 "well", 1 "thank", 1`'
- en: 'You can also apply Python functions as part of a transformation. The following
    example shows how the function `format_result` can be applied to earlier produced
    summation results. The function converts the resulting tuples into a string that
    then can be written to a text file:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以将 Python 函数应用作为转换的一部分。下面的例子展示了如何将函数 `format_result` 应用于先前生成的求和结果。该函数将生成的元组转换为字符串，然后可以写入文本文件：
- en: '`def``format_result``(``word_count``):``"""Convert tuples (token, count) into
    a string"""``(``word``,``count``)``=``word_count``return``"{}: {}"``.``format``(``word``,``count``)``output``=``counts``|``''Format''``>>``beam``.``Map``(``format_result``)`'
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`def``format_result``(``word_count``):``"""将元组（标记，计数）转换为字符串"""``(``word``,``count``)``=``word_count``return``"{}:
    {}"``.``format``(``word``,``count``)``output``=``counts``|``''Format''``>>``beam``.``Map``(``format_result``)`'
- en: Apache Beam provides a variety of predefined transformations. However, if your
    preferred operation isn’t available, you can write your own transformations by
    using the `Map` operators. Just keep in mind that the operations should be able
    to run in a distributed way to fully take advantage of the capabilities of the
    runtime environments.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Beam 提供了各种预定义的转换。然而，如果您的首选操作不可用，可以通过使用 `Map` 操作符来编写自己的转换。只需记住，操作应该能够以分布式方式运行，以充分利用运行时环境的能力。
- en: Putting it all together
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'After discussing the individual concepts of Apache Beam’s pipelines, let’s
    put them all together in one example. The previous snippets and following examples
    are a modified version of the [Apache Beam introduction](https://oreil.ly/e0tj-).
    For readability, the example has been reduced to the bare minimum Apache Beam
    code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了 Apache Beam 流水线的各个概念之后，让我们通过一个例子将它们整合起来。前面的片段和接下来的示例是修改版的[Apache Beam介绍](https://oreil.ly/e0tj-)。为了可读性，例子已经简化到最基本的
    Apache Beam 代码：
- en: '`import``re``import``apache_beam``as``beam``from``apache_beam.io``import``ReadFromText``from``apache_beam.io``import``WriteToText``from``apache_beam.options.pipeline_options``import``PipelineOptions``from``apache_beam.options.pipeline_options``import``SetupOptions``input_file``=``"gs://dataflow-samples/shakespeare/kinglear.txt"`![](images/00002.jpg)`output_file``=``"/tmp/output.txt"``#
    Define pipeline options object.``pipeline_options``=``PipelineOptions``()``with``beam``.``Pipeline``(``options``=``pipeline_options``)``as``p``:`![](images/00075.jpg)`#
    Read the text file or file pattern into a PCollection.``lines``=``p``|``ReadFromText``(``input_file``)`![](images/00064.jpg)`#
    Count the occurrences of each word.``counts``=``(`![](images/00055.jpg)`lines``|``''Split''``>>``beam``.``FlatMap``(``lambda``x``:``re``.``findall``(``r``''[A-Za-z``\''``]+''``,``x``))``|``''PairWithOne''``>>``beam``.``Map``(``lambda``x``:``(``x``,``1``))``|``''GroupAndSum''``>>``beam``.``CombinePerKey``(``sum``))``#
    Format the counts into a PCollection of strings.``def``format_result``(``word_count``):``(``word``,``count``)``=``word_count``return``"{}:
    {}"``.``format``(``word``,``count``)``output``=``counts``|``''Format''``>>``beam``.``Map``(``format_result``)``#
    Write the output using a "Write" transform that has side effects.``output``|``WriteToText``(``output_file``)`'
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``re``import``apache_beam``as``beam``from``apache_beam.io``import``ReadFromText``from``apache_beam.io``import``WriteToText``from``apache_beam.options.pipeline_options``import``PipelineOptions``from``apache_beam.options.pipeline_options``import``SetupOptions``input_file``=``"gs://dataflow-samples/shakespeare/kinglear.txt"`![](images/00002.jpg)`output_file``=``"/tmp/output.txt"``#
    定义管道选项对象。``pipeline_options``=``PipelineOptions``()``with``beam``.``Pipeline``(``options``=``pipeline_options``)``as``p``:`![](images/00075.jpg)`#
    将文本文件或文件模式读取到PCollection中。``lines``=``p``|``ReadFromText``(``input_file``)`![](images/00064.jpg)`#
    计算每个单词的出现次数。``counts``=``(`![](images/00055.jpg)`lines``|``''Split''``>>``beam``.``FlatMap``(``lambda``x``:``re``.``findall``(``r``''[A-Za-z``\''``]+''``,``x``))``|``''PairWithOne''``>>``beam``.``Map``(``lambda``x``:``(``x``,``1``))``|``''GroupAndSum''``>>``beam``.``CombinePerKey``(``sum``))``#
    将计数格式化为字符串的PCollection。``def``format_result``(``word_count``):``(``word``,``count``)``=``word_count``return``"{}:
    {}"``.``format``(``word``,``count``)``output``=``counts``|``''Format''``>>``beam``.``Map``(``format_result``)``#
    使用具有副作用的“Write”转换写入输出。``output``|``WriteToText``(``output_file``)'
- en: '![](images/00002.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: The text is stored in a Google Cloud Storage bucket.
  id: totrans-126
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 文本存储在Google Cloud Storage存储桶中。
- en: '![](images/00075.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Set up the Apache Beam pipeline.
  id: totrans-128
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 设置Apache Beam管道。
- en: '![](images/00064.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: Create a data collection by reading the text file.
  id: totrans-130
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 通过读取文本文件创建数据集。
- en: '![](images/00055.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00055.jpg)'
- en: Perform the transformations on the collection.
  id: totrans-132
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对集合执行转换。
- en: The example pipeline downloads Shakespeare’s King Lear and performs the token
    count pipeline on the entire corpus. The results are then written to the text
    file located at /tmp/output.txt.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 示例管道下载了莎士比亚的《李尔王》，并在整个语料库上执行了标记计数管道。然后将结果写入位于 `/tmp/output.txt` 的文本文件。
- en: Executing Your Basic Pipeline
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 执行您的基本管道
- en: 'As an example, you can run the pipeline with Apache Beam’s DirectRunner by
    executing the following command (assuming that the previous example code was saved
    as `basic_pipeline.py`). If you want to execute this pipeline on different Apache
    Beam runners like Apache Spark or Apache Flink, you will need to set the pipeline
    configurations through the `pipeline_options` object:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以通过执行以下命令（假设前面的示例代码已保存为 `basic_pipeline.py`）使用Apache Beam的DirectRunner运行管道。如果要在不同的Apache
    Beam运行程序（如Apache Spark或Apache Flink）上执行此管道，则需要通过 `pipeline_options` 对象设置管道配置：
- en: '`python basic_pipeline.py`'
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`python basic_pipeline.py`'
- en: 'The results of the transformations can be found in the designated text file:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 可在指定的文本文件中找到转换的结果：
- en: '`$``head``/``tmp``/``output``.``txt``*``KING``:``243``LEAR``:``236``DRAMATIS``:``1``PERSONAE``:``1``king``:``65``...`'
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$``head``/``tmp``/``output``.``txt``*``KING``:``243``LEAR``:``236``DRAMATIS``:``1``PERSONAE``:``1``king``:``65``...`'
- en: Summary
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we presented a high-level overview of TFX and discussed the
    importance of a metadata store as well as the general internals of a TFX component.
    We also introduced Apache Beam and showed you how to carry out a simple data transformation
    using Beam.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们提供了TFX的高级概述，并讨论了元数据存储的重要性以及TFX组件的一般内部。我们还介绍了Apache Beam，并向您展示了如何使用Beam进行简单的数据转换。
- en: Everything we discussed in this chapter will be useful to you as you read through
    Chapters [3](index_split_008.html#filepos156116)–[7](index_split_012.html#filepos624151)
    on the pipeline components and the pipeline orchestration expalined in Chapters
    [11](index_split_018.html#filepos1264016) and [12](index_split_019.html#filepos1378763).
    The first step is to get your data into the pipeline, and we will cover this in
    [Chapter 3](index_split_008.html#filepos156116).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的所有内容都   这一章讨论的所有内容都将对你阅读第 [3](index_split_008.html#filepos156116)–[7](index_split_012.html#filepos624151)
    章节，特别是管道组件和管道编排（详见第 [11](index_split_018.html#filepos1264016) 和 [12](index_split_019.html#filepos1378763)
    章节）时有所帮助。第一步是将你的数据送入管道，我们将在[第 _split_008.html#filepos156116)中详细介绍。
