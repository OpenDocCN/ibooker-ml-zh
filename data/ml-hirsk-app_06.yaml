- en: Chapter 4\. Managing Bias in Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 机器学习中的偏见管理
- en: Managing the harmful effects of bias in machine learning systems is about so
    much more than data, code, and models. Our model’s average performance quality—the
    main way data scientists are taught to evaluate the goodness of a model—has little
    to do with whether it’s causing real-world bias harms. A perfectly accurate model
    can cause bias harms. Worse, all ML systems exhibit some level of bias, bias incidents
    appear to be some of the most common AI incidents (see [Figure 4-1](#incident_pie)),
    bias in business processes often entails legal liability, and bias in ML models
    hurts people in the real world.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统中管理偏见的有害影响远不止于数据、代码和模型。我们数据科学家被教导用来评估模型优良性的主要方法——模型的平均性能质量——与其是否导致现实世界中的偏见有害几乎无关。即使是完全准确的模型也可能造成偏见危害。更糟糕的是，所有的机器学习系统都表现出一定程度的偏见，偏见事件似乎是最常见的AI事件之一（参见[图 4-1](#incident_pie)）。业务流程中的偏见通常会带来法律责任，并且ML模型中的偏见会实实在在地伤害到人们。
- en: '![mlha 0401](assets/mlha_0401.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0401](assets/mlha_0401.png)'
- en: Figure 4-1\. The frequency of different types of AI incidents based on a qualitative
    analysis of 169 publicly reported incidents between 1988 and February 1, 2021
    (figure courtesy of BNH.AI)
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 根据对1988年至2021年2月1日之间169起公开报道事件的定性分析，展示了不同类型AI事件的频率（图片由BNH.AI提供）。
- en: This chapter will put forward approaches for detecting and mitigating bias in
    a sociotechnical fashion, at least to the best of our ability as practicing technicians.
    That means we’ll try to understand how ML system bias exists in its broader societal
    context. Why? *All* ML systems are sociotechnical. We know this might be hard
    to believe at first, so let’s think through one example. Let’s consider a model
    used to predict sensor failure for an Internet of Things (IoT) application, using
    only information from other automated sensors. That model would likely have been
    trained by humans, or a human decided that a model was needed. Moreover, the results
    from that model could be used to inform the ordering of new sensors, which could
    affect the employment of those at the manufacturing plant or those who repair
    or replace failing sensors. Finally, if our preventative maintenance model fails,
    people who interact with the system could be harmed. For every example we can
    think of that seems purely technical, it becomes obvious that decision-making
    technologies like ML don’t exist without interacting with humans in some way.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将提出一些在社会技术方面检测和减轻偏见的方法，至少作为我们作为实践技术人员的最佳努力。这意味着我们将努力理解ML系统偏见在更广泛社会背景下的存在。为什么呢？*所有*
    的ML系统都是社会技术系统。一开始我们可能很难相信这一点，所以让我们通过一个例子来深入思考。让我们考虑一个用于预测物联网应用中传感器故障的模型，该模型只使用其他自动化传感器的信息。该模型很可能是由人类训练的，或者是某个人决定需要这样一个模型。此外，该模型的结果可能被用来指导订购新传感器，这可能会影响制造工厂的就业或者那些负责修理或更换故障传感器的人员。最后，如果我们的预防性维护模型失败，那些与系统互动的人可能会受到伤害。对于我们能够想到的每一个看似纯技术的例子，决策技术如ML不能不与某种形式的人类互动。
- en: This means there’s no purely technical solution to bias in ML systems. If readers
    want to jump right into the code for bias testing and bias remediation, see [Chapter 10](ch10.html#unique_chapter_id_10).
    However, we don’t recommend this. Readers will miss a lot of important information
    about what bias is and how to think about it in productive ways. This chapter
    starts out by defining bias using several different authoritative sources, and
    how to recognize our own cognitive biases that may affect the ML systems we build
    or the results our users interpret. The chapter then provides a broad overview
    of who tends to be harmed in AI bias incidents and what kinds of harms they experience.
    From there, we’ll cover methods to test for bias in ML systems and discuss mitigating
    bias using both technical and sociotechnical approaches. Finally, the chapter
    will close with a case discussion of the Twitter image-cropping algorithm.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在ML系统中解决偏见问题没有纯技术的解决方案。如果读者希望直接跳到偏见测试和偏见修正的代码中，请参阅[第10章](ch10.html#unique_chapter_id_10)。但我们不建议这样做。读者会错过关于偏见是什么以及如何以积极方式思考它的重要信息。本章首先通过几个权威来源定义偏见，以及如何识别可能影响我们建立ML系统或我们用户解释结果的认知偏见。然后，本章将广泛概述谁倾向于在AI偏见事件中受到伤害以及他们经历的伤害类型。接下来，我们将介绍测试ML系统中偏见的方法，并讨论使用技术和社会技术方法减轻偏见。最后，本章将通过Twitter图像裁剪算法的案例讨论来结束。
- en: Note
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While some aspects of bias management must be tuned to the specific architecture
    of a model, a great deal of bias management is not model-specific. Many of the
    ideas in this chapter, particularly those drawn from the NIST SP1270 bias guidance
    and the Twitter Bias Bounty, can be applied to a wide variety of sophisticated
    AI systems like ChatGPT or RoBERTa language models. If readers want to see this
    in practice, check out IQT Labs’ [audit of RoBERTa](https://oreil.ly/3hs_6).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有些偏差管理的方面必须根据模型的具体架构进行调整，但很多偏差管理并不是特定于模型的。本章中的许多理念，特别是从NIST SP1270偏差指南和Twitter偏差悬赏中得出的理念，可以应用于像ChatGPT或RoBERTa语言模型这样的各种复杂AI系统。如果读者想要看到实践情况，请查看IQT
    Labs的[RoBERTa审计](https://oreil.ly/3hs_6)。
- en: ISO and NIST Definitions for Bias
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ISO和NIST对偏差的定义
- en: 'The International Organization for Standardization (ISO) defines bias as “the
    degree to which a reference value deviates from the truth” in [“Statistics—Vocabulary
    and Symbols—Part 1”](https://oreil.ly/YYv4W). This is a very general notion of
    bias, but bias is a complex and heterogenous phenomenon. Yet, in all of its instances,
    it’s about some systematic deviation from the truth. In decision-making tasks,
    bias takes on many forms. It’s substantively and ethically wrong to deny people
    employment due to the level of melanin in their skin. It’s factually wrong to
    think an idea is correct just because it’s the first thing that comes to mind.
    And it’s substantively and ethically wrong to train an ML model on incomplete
    and unrepresentative data. In a recent work from NIST, [“Towards a Standard for
    Identifying and Managing Bias in Artificial Intelligence” (SP1270)](https://oreil.ly/pkm4f),
    the subject of bias is divided into three major categories that align with these
    examples of bias: systemic, statistical, and human biases.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 国际标准化组织（ISO）在《“Statistics—Vocabulary and Symbols—Part 1”》中将偏差定义为“参考值偏离真实值的程度”。这是偏差的一个非常一般的概念，但偏差是一个复杂且异质的现象。然而，在所有情况下，它都是关于从真实值出现的某种系统偏离。在决策任务中，偏差表现出多种形式。基于人们皮肤中黑色素水平而拒绝人们就业是实质性和伦理上的错误。仅仅因为某个想法是第一个浮现的并不代表它是正确的是事实上的错误。而仅仅基于不完整和不具代表性的数据训练ML模型也是实质性和伦理上的错误。在最近的NIST工作中，《“Towards
    a Standard for Identifying and Managing Bias in Artificial Intelligence”（SP1270）》将偏差主题分为三大类别，与这些偏差示例相一致：系统性、统计性和人为偏差。
- en: Systemic Bias
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统性偏差
- en: Often when we say bias in ML, we mean systemic biases. These are historical,
    social, and institutional biases that are, sadly, so baked into our lives that
    they show up in ML training data and design choices by default. A common consequence
    of systemic bias in ML models is the incorporation of demographic information
    into system mechanisms. This incorporation may be overt and explicit, such as
    when language models (LMs) are [repurposed to generate harmful and offensive content](https://oreil.ly/bWf4E)
    that targets certain demographic groups. However, in practice, incorporation of
    demographic information into decision-making processes tends to be unintentional
    and implicit, leading to differential outcome rates or outcome prevalence across
    demographic groups, for example, by matching more men’s resumes to higher paying
    job descriptions, or design problems that exclude certain groups of users (e.g.,
    those with physical disabilities) from interacting with a system.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML中，我们常说的偏差通常指的是系统性偏差。这些是历史性的、社会的和制度性的偏差，遗憾的是，它们已经深深融入我们的生活中，并且在ML训练数据和设计选择中默认出现。系统性偏差在ML模型中的常见后果是将人口统计信息纳入系统机制中。这种纳入可能是明显和明确的，例如当语言模型（LMs）被[重新用于生成针对特定人口群体的有害和冒犯性内容](https://oreil.ly/bWf4E)时。然而，在实践中，将人口统计信息纳入决策过程往往是无意和隐性的，导致跨人口群体之间出现不同的结果率或结果普遍性，例如将更多男性简历与更高薪职位描述匹配，或设计问题排斥某些用户群体（例如身体残疾者）与系统进行交互。
- en: Statistical Bias
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计性偏差
- en: Statistical biases can be thought of as mistakes made by humans in the specification
    of ML systems, or emergent phenomena like concept drift, that affect ML models
    and are difficult for humans to mitigate. Other common types of statistical biases
    include predictions based on unrepresentative training data, or error propagation
    and feedback loops. One potential indicator of statistical bias in ML models is
    differential performance quality across different cross-sections of data, such
    as demographic groups. Differential validity for an ML model is a particular type
    of bias, somewhat distinct from the differing outcome rates or outcome prevalence
    described for human biases. In fact, there is a [documented tension](https://oreil.ly/cJy7F)
    between maximizing model performance within demographic groups and maintaining
    equality of positive outcome rates. Statistical biases may also lead to serious
    AI incidents, for example when concept drift in new data renders a system’s decisions
    more wrong than right, or when feedback loops or error propagation lead to increasingly
    large volumes of bad predictions over a short time span.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 统计偏见可以被视为人类在规范机器学习系统时所犯的错误，或者像概念漂移这样的影响机器学习模型的新兴现象，这些现象对人类来说很难消除。统计偏见的其他常见类型包括基于不具代表性的训练数据进行预测，或者误差传播和反馈循环。机器学习模型中统计偏见的一个潜在指标是在不同数据交叉部分（例如人口统计群体）中的性能质量差异。机器学习模型的差异有效性是一种特定类型的偏见，与人类偏见描述的不同结果率或结果普遍性略有不同。实际上，维护人口统计群体内模型性能的最大化与维护正面结果率的平等之间存在[文件化的紧张关系](https://oreil.ly/cJy7F)。统计偏见还可能导致严重的人工智能事件，例如当新数据中的概念漂移使系统的决策比正确决策更多，或者当反馈循环或误差传播导致短时间内大量错误预测的产生时。
- en: Human Biases and Data Science Culture
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类偏见与数据科学文化
- en: 'There are a number of human or cognitive biases that can come into play with
    both the individuals and the teams that design, implement, and maintain ML systems.
    For a more complete list of human biases, read the NIST SP1270 guidance paper.
    The following are the human biases that we’ve seen most frequently affecting both
    data scientists and users of ML systems:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计、实施和维护机器学习系统的个人和团队中，可能会出现多种人类或认知偏见。有关更完整的人类偏见列表，请阅读NIST SP1270指导文件。以下是我们经常看到的最频繁影响数据科学家和机器学习系统用户的人类偏见：
- en: Anchoring
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 锚定
- en: When a particular reference point, or *anchor*, has an undue effect on people’s
    decisions. This is like when a benchmark for a state-of-the-art deep learning
    model is stuck at 0.4 AUC for a long time, and someone comes along and scores
    0.403 AUC. We shouldn’t think that’s important, but we’re anchored to 0.4.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个特定的参考点或*锚点*对人们的决策产生不当影响时。就像当一个最先进的深度学习模型的基准值长期停留在0.4 AUC，然后有人得到了0.403 AUC。我们不应该认为这很重要，但我们却被固守在0.4上。
- en: Availability heuristic
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性启发法
- en: People tend to overweight what comes easily or quickly to mind in decision-making
    processes. Put another way, we often confuse *easy to remember* with *correct*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策过程中，人们往往会过分重视易于或迅速浮现在脑海中的事物。换句话说，我们经常将*易记住*与*正确*混淆。
- en: Confirmation bias
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 确认偏见
- en: A cognitive bias where people tend to prefer information that aligns with, or
    confirms, their existing beliefs. Confirmation bias is a big problem in ML systems
    when we trick ourselves into thinking our ML models work better than they actually
    do.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 人们倾向于偏爱与自己现有信念一致或确认的信息。当我们自欺欺人地认为我们的机器学习模型比实际效果更好时，确认偏见在机器学习系统中就成了一个大问题。
- en: Dunning-Kruger effect
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 邓宁-克鲁格效应
- en: The tendency of people with low ability in a given area or task to overestimate
    their self-assessed ability. This happens when we allow ourselves to think we’re
    experts at something just because we can `import sklearn` and run `model.fit()`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在某一领域或任务中能力较低的人倾向于高估他们的自我评估能力。当我们允许自己认为我们是某件事的专家，只因为我们能`import sklearn`并运行`model.fit()`时，这种情况就会发生。
- en: Funding bias
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 资金偏见
- en: A bias toward highlighting or promoting results that support or satisfy the
    funding agency or financial supporter of a project. We do what makes our bosses
    happy, what makes our investors happy, and what increases our own salaries. Real
    science needs safeguards that prevent its progress from being altered by biased
    financial interests.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 偏向于突出或促进支持或满足项目资助机构或财务支持者结果的偏见。我们做让老板开心的事情，让投资者开心的事情，以及增加我们自己薪水的事情。真正的科学需要防止其进展被有偏见的财务利益改变的保障措施。
- en: Groupthink
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 群体思维
- en: When people in a group tend to make nonoptimal decisions based on their desire
    to conform to the group or fear dissenting with the group. It’s hard to disagree
    with our team, even when we’re confident that we’re right.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个群体中的人们倾向于根据他们希望符合群体或害怕与群体唱反调的愿望而做出非最优决策时。即使我们有信心自己是对的，也很难不同意我们的团队。
- en: McNamara fallacy
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 麦克纳马拉谬误
- en: The belief that decisions should be made based solely on quantitative information,
    at the expense of qualitative information or data points that aren’t easily measured.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 认为决策应该仅基于定量信息，而忽视难以衡量的定性信息或数据点的信念。
- en: Techno-chauvinism
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 技术至上主义
- en: The belief that technology is always the solution.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 认为技术总是解决方案的信念。
- en: All these biases can and do lead to inappropriate and overly optimistic design
    choices, in turn leading to poor performance when a system is deployed, and, finally,
    leading to harms for system users or operators. We’ll get into the harms that
    can arise and what to do about these problems shortly. For now we want to highlight
    a commonsense mitigant that is also a theme of this chapter. We cannot treat bias
    properly without looking at a problem from many different perspectives. Step 0
    of fighting bias in ML is having a diverse group of stakeholders in the room (or
    video call) when important decisions about the system are made. To avoid the blind
    spots that allow biased ML models to cause harm, we’ll need many different types
    of perspectives informing system design, implementation, and maintenance decisions.
    Yes, we’re speaking about gathering input from different demographic perspectives,
    including from those with disabilities. We’re also speaking about educational
    backgrounds, such as those of social scientists, lawyers, and domain experts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些偏见都可能导致不恰当和过于乐观的设计选择，进而导致系统部署时的性能不佳，最终给系统用户或操作者带来伤害。我们将进一步探讨可能出现的伤害及如何解决这些问题。目前，我们想强调一个常识性的缓解措施，这也是本章的主题之一。我们不能不从多个不同的视角来看待问题就妥善处理偏见。在机器学习中，抵抗偏见的第一步是在做出系统重要决策时，确保有一个多元化的利益相关者团队在场（或视频会议中）。为了避免那些使偏见的机器学习模型造成伤害的盲点，我们需要多种不同类型的视角来指导系统的设计、实施和维护决策。是的，我们在讨论从不同的人口统计学视角收集意见，包括那些有残疾的人。我们也在谈论教育背景，比如社会科学家、律师和领域专家。
- en: Also, consider the *digital divide*. A shocking percentage of the population
    still doesn’t have access to good internet connectivity, new computers, and information
    like this book. If we’re drawing conclusions about our users, we need to remember
    that there is a solid chunk of the population that’s not going to be included
    in user statistics. Leaving potential users out is a huge source of bias and harm
    in system design, bias testing, and other crucial junctures in the ML lifecycle.
    Success in ML today still requires the involvement of people who have a keen understanding
    of the real-world problem we’re trying to solve, and what potential users might
    be left out of our design, data, and testing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑*数字鸿沟*。令人震惊的是，仍然有相当大比例的人口无法接入良好的互联网连接、新计算机和像本书这样的信息。如果我们对用户作出结论，我们需要记住有一大部分人口无法包含在用户统计中。忽略潜在用户是系统设计、偏见测试和机器学习生命周期中其他关键时刻的重大偏见和伤害源。如今，机器学习的成功仍然需要那些深刻理解我们试图解决的现实世界问题，并且了解我们的设计、数据和测试可能会排除哪些潜在用户的人参与。
- en: Legal Notions of ML Bias in the United States
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 美国的机器学习偏见法律概念
- en: We should be aware of the many important legal notions of bias. However, it’s
    also important to understand that the legal system is extremely complex and context
    sensitive. Merely knowing a few definitions will still leave us light years away
    from having any real expertise on these matters. As data scientists, legal matters
    are an area where we should not let the Dunning-Kruger effect take over. With
    those caveats, let’s dive into a basic overview.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意许多重要的法律偏见概念。然而，理解法律体系是极其复杂和依赖背景的也很重要。仅仅知道一些定义还远远不足以使我们在这些问题上具备真正的专业知识。作为数据科学家，法律事务是我们不应该让邓宁-克鲁格效应占据上风的领域。有了这些警示，让我们来简单概述一下。
- en: Warning
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Now is the time to reach out to your legal team if you have any questions or
    concerns about bias in ML models. Dealing with bias in ML models is one of the
    most difficult and serious issues in the information economy. Data scientists
    need help from lawyers to properly address bias risks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对ML模型中的偏见有任何疑问或担忧，请立即联系您的法律团队。处理ML模型中的偏见是信息经济中最困难和严重的问题之一。数据科学家需要律师的帮助来妥善解决偏见风险。
- en: 'In the US, bias in decision-making processes that affect the public has been
    regulated for decades. A major focus of early laws and regulations in the US was
    employment matters. Notions like protected groups, disparate treatment, and disparate
    impact have now spread to a broader set of laws in consumer finance and housing,
    and are even being cited in brand new local laws today, like the New York City
    audit requirement for AI used in hiring. Nondiscrimination in the EU is addressed
    in the Charter of Fundamental Rights, the European Convention on Human Rights,
    and in the Treaty on the Functioning of the EU, and, crucially for us, in aspects
    of the proposed EU AI Act. While it’s impossible to summarize these laws and regulations,
    even on the US side, the definitions that follow are what we think are most directly
    applicable to a data scientist’s daily work. They are drawn, very roughly, from
    laws like the Civil Rights Act, the Fair Housing Act (FHA), Equal Employment Opportunity
    Commission (EEOC) regulations, the Equal Credit Opportunity Act (ECOA), and the
    Americans with Disabilities Act (ADA). The following definitions cover legal ideas
    about what traits are protected under law and what these laws seek to protect
    us from:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国，影响公众的决策过程中的偏见问题已被监管数十年。美国早期法律和法规的一个主要焦点是就业事务。像受保护组、不公平待遇和不公平影响这样的概念现在已扩展到更广泛的消费金融和住房法律，并且甚至被引用到像纽约市雇用中使用的AI审计要求等全新的地方法律中。欧盟中的非歧视问题在《基本权利宪章》、《欧洲人权公约》以及《欧盟职能条约》中得到了解决，对我们来说尤为重要的是在拟议的欧盟AI法中的某些方面。虽然要总结这些法律和法规是不可能的，甚至在美国这边，以下定义我们认为最直接适用于数据科学家日常工作。它们很粗略地取自《民权法案》、《公平住房法案》（FHA）、《平等就业机会委员会》（EEOC）法规、《平等信贷机会法案》（ECOA）以及《美国残疾人法案》（ADA）。以下定义涵盖了法律上受保护的特征以及这些法律试图保护我们免受的法律观念：
- en: Protected groups
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 受保护组
- en: In the US, many laws and regulations prohibit discrimination based on race,
    sex (or gender, in some cases), age, religious affiliation, national origin, and
    disability status, among other categories. Prohibited decision bases under the
    FHA include race, color, religion, national origin, sex, familial status, and
    disability. The EU’s GDPR, as an example of one non-US regulation, prohibits the
    use of personal data about racial or ethnic origin, political opinions, and other
    categories somewhat analogous to US protected groups. This is one reason why traditional
    bias testing compares results for protected groups and so-called *control* (or
    *reference*) groups that are not protected groups.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国，许多法律和法规禁止基于种族、性别（或性别，在某些情况下）、年龄、宗教信仰、国籍和残疾状况等类别进行歧视。在住房公平法（FHA）下被禁止的决策依据包括种族、肤色、宗教、国籍、性别、家庭状况和残疾。作为非美国法规的一个例子，欧盟的GDPR禁止使用关于种族或民族、政治观点等类别的个人数据。这也是为什么传统的偏见测试比较受保护组和所谓的*控制*（或*参考*）组的结果之一。
- en: Disparate treatment
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 不公平待遇
- en: Disparate treatment is a specific type of discrimination that is illegal in
    many industries. It’s a decision that treats an individual less favorably than
    similarly situated individuals because of a protected characteristic such as race,
    sex, or other trait. For data scientists working on employment, housing, or credit
    applications, this means we should be very careful when using demographic data
    in ML models, and even in our bias-remediation techniques. Once demographic data
    is used as input in a model, that could mean that a decision for someone could
    be different just because of their demographics, and that disparate treatment
    could result in some cases.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 不公平待遇是许多行业中非法的特定类型歧视。这是一种因为种族、性别或其他特征等受保护特征而对一个人采取不利待遇的决策。对于从事就业、住房或信用申请的数据科学家来说，这意味着在使用人口统计数据进行机器学习模型时我们应该非常小心，甚至在我们的偏见修复技术中也是如此。一旦人口统计数据被用作模型的输入，这可能意味着某人的决策可能因其人口统计特征而有所不同，而这种不公平待遇在某些情况下可能会产生结果。
- en: Warning
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Concerns about disparate treatment, and more general systemic bias, are why
    we typically try not to use demographic markers as direct inputs to ML models.
    To be conservative, demographic markers should *not* be used as model inputs in
    most common scenarios, but should be used for bias testing or monitoring purposes.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不同待遇的担忧，以及更一般的系统性偏见，通常是为什么我们尽量避免直接将人口统计标记作为机器学习模型的输入。为了谨慎起见，在大多数常见情景下，人口统计标记*不应*作为模型输入，但应用于偏差测试或监测目的。
- en: Disparate impact
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的影响
- en: 'Disparate impact is another kind of legally concerning discrimination. It’s
    basically about different *outcome* rates or prevalence across demographic groups.
    Disparate impact is more formally defined as the result of a seemingly neutral
    policy or practice that disproportionately harms a protected group. For data scientists,
    disparate impact tends to happen when we don’t use demographic data as inputs,
    but we use something correlated to demographic data as an input. Consider credit
    scores: they are a fairly accurate predictor of default, so they are often seen
    as valid to use in predictive models in consumer lending. However, they are correlated
    to race, such that some minority groups have lower credit scores on average. If
    we use a credit score in a model, this tends to result in certain minority groups
    having lower proportions of positive outcomes, and that’s a common example of
    disparate impact. (That’s also why several states have started to restrict the
    use of credit scores in some insurance-related decisions.)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的影响是另一种法律上引起关注的歧视形式。它基本上是指跨人群之间不同的*结果*率或者存在率。不同的影响更正式地定义为一种看似中立的政策或做法的结果，它会不成比例地损害到受保护的群体。对于数据科学家来说，不同的影响往往会发生在我们不使用人口统计数据作为输入，而使用某些与人口统计数据相关的内容作为输入的情况。以信用评分为例：它们相当准确地预测违约，因此在消费信贷预测模型中经常被视为有效使用。然而，它们与种族存在相关性，使得某些少数群体的平均信用评分较低。如果我们在模型中使用信用评分，这往往会导致某些少数群体的正面结果比例较低，这是不同的影响的常见例子。（这也是为什么几个州已经开始限制在某些与保险相关的决策中使用信用评分的原因。）
- en: Differential validity
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 差异效度
- en: Differential validity is a construct that comes up sometimes in employment.
    Where disparate impact is often about different outcome rates across demographic
    groups, differential validity is more about different *performance quality* across
    groups. It happens when an employment test is a better indicator of job performance
    for some groups than for others. Differential validity is important because the
    mathematical underpinning, not the legal construct, generalizes to nearly all
    ML models. It’s common to use unrepresentative training data and to build a model
    that performs better for some groups than for others, and a lot of more recent
    bias-testing approaches focus on this type of bias.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 差异效度有时在就业领域中会出现。而不同的影响通常是指跨人群之间的不同结果率，差异效度则更多地涉及跨群体之间的*表现质量*不同。这种情况发生在某些群体的就业测试更能有效预测工作表现，而对其他群体则效果不佳。差异效度之所以重要，是因为其数学基础，而非法律构建，几乎适用于所有机器学习模型。常见情况是使用不具代表性的训练数据，并构建出对某些群体效果更好的模型，而许多较新的偏差测试方法专注于这种类型的偏差。
- en: Screen out
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 屏蔽
- en: Screen out is a very important type of discrimination that highlights the sociotechnical
    nature of ML systems and proves that testing and balancing the scores of a model
    is simply insufficient to protect against bias. Screen out happens when a person
    with a disability, such as limited vision or difficulties with fine motor skills,
    is unable to interact with an employment assessment, and is screened out of a
    job or promotion by default. Screen out is a serious issue, and the EEOC and Department
    of Labor are [paying attention](https://oreil.ly/c0y9i) to the use of ML in this
    space. Note that screen out cannot necessarily be fixed by mathematical bias testing
    or bias remediation; it typically must be addressed in the design phase of the
    system, where designers ensure those with disabilities are able to work with the
    end product’s interfaces. Screen out also highlights why we want perspectives
    from lawyers and those with disabilities when building ML systems. Without those
    perspectives, it’s all too easy to forget about people with disabilities when
    building ML systems, and that can sometimes give rise to legal liabilities.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 筛除是一种非常重要的歧视类型，突显了ML系统的社会技术性质，并证明了仅通过测试和平衡模型分数是不足以保护免受偏见的。当残疾人士，如视力有限或细微运动技能困难者，无法与就业评估交互，并因此被默认筛除出工作或晋升时，就发生了筛除。筛除是一个严重的问题，EEOC和劳工部正在关注ML在这一领域的应用。需要注意的是，筛除通常不能通过数学偏见测试或偏见修复来修复；它通常必须在系统设计阶段解决，在这个阶段，设计师确保残疾人士能够使用最终产品的界面。筛除也突显了为何在构建ML系统时需要律师和残疾人士的视角。如果没有这些视角，我们很容易在构建ML系统时忽视残疾人士，这有时可能会导致法律责任的产生。
- en: This concludes our discussion on general definitions of bias. As readers can
    see, it’s a complex and multifaceted topic with all kinds of human, scientific,
    and legal concerns coming into play. We’ll add to these definitions with more
    specific, but probably more fraught, mathematical definitions of bias when we
    discuss bias testing later in the chapter. Next we’ll outline who tends to experience
    bias and related harms from ML systems.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对偏见一般定义的讨论。读者可以看到，这是一个复杂且多方面的主题，涉及各种人文、科学和法律问题。在本章后面讨论偏见测试时，我们将通过更具体但可能更复杂的数学定义来补充这些定义。接下来，我们将概述谁更容易在ML系统中经历偏见和相关伤害。
- en: Who Tends to Experience Bias from ML Systems
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML系统中谁更容易经历偏见
- en: Any demographic group can experience bias and related harms when interacting
    with an ML system, but history tells us certain groups are more likely to experience
    bias and harms more often. In fact, it’s the nature of supervised learning—which
    only learns and repeats patterns from past recorded data—that tends to result
    in older people, those with disabilities, immigrants, people of color, women,
    and gender-nonconforming individuals facing more bias from ML systems. Put another
    way, those who experience discrimination in the real world, or in the digital
    world, will likely also experience it when dealing with ML systems because all
    that discrimination has been recorded in data and used to train ML models. The
    groups listed in this section are often protected under various laws, but not
    always. They will often, but not always, be the comparison group in bias testing
    for statistical parity of scores or outcomes between two demographic groups.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 任何人群在与ML系统互动时可能会遇到偏见和相关伤害，但历史告诉我们，某些群体更容易更频繁地遭受偏见和伤害。事实上，这是受监督学习的本质所致——它只从过去记录的数据中学习和重复模式——导致年长者、残障人士、移民、有色人种、女性和性别非符合个体更容易面对ML系统的偏见。换句话说，那些在现实世界或数字世界中经历歧视的人，在与ML系统打交道时也很可能会经历它，因为所有这些歧视都已记录在数据中，并用于训练ML模型。本节列出的这些群体通常受到各种法律的保护，但并非总是如此。他们通常会成为偏见测试的比较群体，以评估两个人口统计学群体之间得分或结果的统计平等性。
- en: 'Many people belong to multiple protected or marginalized groups. The important
    concept of intersectionality tells us that societal harm is concentrated among
    those who occupy multiple protected groups and that bias [should not only be analyzed
    as affecting marginalized groups along single group dimensions](https://oreil.ly/3ZaPy).
    For example, AI ethics researchers [recently showed](https://oreil.ly/DMu8o) that
    some commercially available facial recognition systems have substantial gender
    classification accuracy disparities, with darker-skinned women being the most
    misclassified group. Finally, before defining these groups, it is also important
    to think of the McNamara fallacy. Is it even right to put nuanced human beings
    into this kind of blunted taxonomy? Probably not, and it’s likely that assignment
    to these simplistic groups, which is often done because such categories are easy
    to represent as binary marker columns in a database, is also a source of bias
    and potential harms. There are always a lot of caveats in managing bias in ML
    systems, so with those in mind, we tread carefully into defining simplified demographic
    groups that tend to face more discrimination and that are often used as comparison
    groups in traditional bias testing:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人属于多个受保护或边缘化群体。交集性的重要概念告诉我们，社会伤害集中在那些属于多个受保护群体的人身上，偏见[不应仅分析作用于边缘化群体单一维度](https://oreil.ly/3ZaPy)。例如，AI伦理研究人员[最近表明](https://oreil.ly/DMu8o)，一些商业可用的面部识别系统在性别分类准确性上存在显著差异，较深肤色的女性是最容易被误分类的群体。最后，在定义这些群体之前，还重要考虑麦克纳马拉谬误。把这种复杂的人类归类为这种钝化的分类法是正确的吗？可能不是，而且很可能这种简化的群体分配方式，通常是因为这些类别在数据库中作为二进制标记列很容易表示，也是偏见和潜在伤害的源头。在管理机器学习系统中的偏见时，总是有很多注意事项，因此在这些方面谨慎行事，我们走入定义更容易受到歧视的简化人口群体的领域：
- en: Age
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 年龄
- en: Older people, typically those 40 and above, are more likely to experience discrimination
    in online content. The age cutoff could be older in more traditional applications
    likes employment, housing, or consumer finance. However, participation in Medicare
    or the accumulation of financial wealth over a lifetime may make older people
    the favored group in other scenarios.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，40岁及以上的老年人在在线内容中更容易遭受歧视。在更传统的应用场景如就业、住房或消费金融中，年龄的分界线可能更高。然而，参加医疗保险或一生积累财富可能使老年人成为其他场景中受青睐的群体。
- en: Disability
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 残疾
- en: Those with physical, mental, or emotional disabilities are perhaps some of the
    likeliest people to experience bias from ML systems. The idea of screen out generalizes
    outside of employment, even if the legal construct may not. People with disabilities
    are often forgotten about during the design of ML systems, and no amount of mathematical
    bias testing or remediation can make up for that.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 那些身体、心理或情感残疾的人可能是最有可能受到机器学习系统偏见影响的人群之一。虽然法律框架可能不适用，但排除的概念普遍适用于就业以外的情况。在设计机器学习系统时，经常忽视残疾人群，无论进行多少数学偏见测试或补救，都无法弥补这一点。
- en: Immigration status or national origin
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 移民身份或国籍
- en: People who live in a country in which they were not born, with any immigration
    status, including naturalized citizens, are known to face significant bias challenges.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 居住在非本国出生地的国家，并具有任何移民身份的人，包括入籍公民，在面对重大偏见挑战时众所周知。
- en: Language
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 语言
- en: Especially in online content, an important domain for ML systems, those who
    use languages other than English or who write in non-Latin scripts may be more
    likely to experience bias from ML systems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在在线内容中，这是机器学习系统的一个重要领域，使用非英语语言或以非拉丁文字书写的人可能更容易遭受偏见影响。
- en: Race and ethnicity
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 种族和民族
- en: Races and ethnicities other than white people, including those who identify
    as more than one race, are commonly subject to bias and harm when interacting
    with ML systems. Some also prefer skin tone scales over traditional race or ethnicity
    labels, especially for computer vision tasks. The [Fitzpatrick scale](https://oreil.ly/NJfBP)
    is an example of a skin tone scale.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除白人以外的种族和民族，包括那些认同为多种族的人，通常在与机器学习系统互动时会遭受偏见和伤害。一些人在计算机视觉任务中更喜欢肤色标度而不是传统的种族或民族标签。[菲茨帕特里克标度](https://oreil.ly/NJfBP)就是一个肤色标度的例子。
- en: Sex and gender
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 性别与性别
- en: Sexes and genders other than cisgender men are more likely to experience bias
    and harms at the hands of an ML system. In online content, women are often favored—but
    in harmful ways. Known as the *male gaze* phenomenon, media about women may be
    appealing and receive positive treatment (such as being promoted in a social media
    feed), specifically because that content is oriented toward objectification, subjugation,
    or sexualization of women.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 非男性异性恋男性更有可能在ML系统的作用下经历偏见和伤害。在在线内容中，女性通常会被偏爱，但是以有害的方式。作为*男性凝视*现象，关于女性的媒体可能因其取向于对女性的客体化、压制或性别化而吸引人并受到积极的对待（例如在社交媒体中推广）。
- en: Intersectional groups
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉群体
- en: People who are in two or more of the preceding groups may experience bias or
    harms that are greater than the simple sum of the two broader groups to which
    they belong. All the bias testing and mitigation steps described in this chapter
    should consider intersectional groups.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 属于前述两个更广泛群体的两个或更多群体的人可能经历的偏见或伤害可能比他们所属的两个更广泛群体的简单总和更大。本章描述的所有偏见测试和缓解步骤都应考虑交叉群体。
- en: Of course these are not the only groups of people who may experience bias from
    an ML model, and grouping people can be problematic no matter what the motivation.
    However, it’s important to know where to start looking for bias, and we hope our
    list is sufficient for that purpose. Now that we know where to look for ML bias,
    let’s discuss the most common harms that we should be mindful of.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些并不是唯一可能从ML模型中经历偏见的人群，而无论动机如何，对人群进行分组都可能存在问题。然而，了解从哪里开始寻找偏见很重要，我们希望我们的列表足以达到这一目的。现在我们知道在哪里寻找ML偏见，让我们讨论我们应该注意的最常见的伤害。
- en: Harms That People Experience
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人们经历的伤害
- en: 'Many common types of harm occur in online or digital content. They occur frequently
    too—perhaps so frequently that we may become blind to them. The following list
    highlights common harms and provides examples so that we can recognize them better
    when we see them next. These harms align closely with those laid out in Abagayle
    Lee Blank’s [“Computer Vision Machine Learning and Future-Oriented Ethics”](https://oreil.ly/-JmJA),
    which describes cases in which these harms occur in computer vision:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在线或数字内容中存在许多常见的伤害类型。这些伤害经常发生，或许频繁到我们可能对它们视而不见。以下列表突出了常见的伤害，并提供了例子，以便我们下次看到它们时能更好地识别。这些伤害与阿巴盖尔·李·布兰克在《计算机视觉机器学习与未来导向伦理》中提出的情况密切相关，该书描述了计算机视觉中这些伤害发生的案例：
- en: Denigration
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 贬低
- en: Content that is actively derogatory or offensive—e.g., offensive content generated
    by chatbots like [Tay](https://oreil.ly/2938n) or [Lee Luda](https://oreil.ly/nRzs1).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 包含积极侮辱或冒犯性内容——例如，聊天机器人如[Tay](https://oreil.ly/2938n)或[Lee Luda](https://oreil.ly/nRzs1)生成的冒犯性内容。
- en: Erasure
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 抹消
- en: Erasure of content challenging dominant social paradigms or past harms suffered
    by marginalized groups—e.g., [suppressing content](https://oreil.ly/FZdDB) that
    discusses racism or calls out white supremacy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 抹去挑战主流社会范式或被边缘化群体遭受的过去伤害的内容——例如，[压制讨论种族主义或指责白人至上主义的内容](https://oreil.ly/FZdDB)。
- en: Exnomination
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Exnomination
- en: Treating notions like whiteness, maleness, or heterosexuality as central human
    norms—e.g., [online searches](https://oreil.ly/m-zR-) returning a Barbie Doll
    as the first female result for “CEO.”
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将白人、男性或异性恋等概念视为中心人类规范——例如，[在线搜索](https://oreil.ly/m-zR-)将芭比娃娃作为“CEO”的第一个女性结果返回。
- en: Misrecognition
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 误识别
- en: Mistaking a person’s identity or failing to recognize someone’s humanity—e.g.,
    [misrecognizing Black people](https://oreil.ly/GjyTI) in automated image tagging.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 错误地认定一个人的身份或未能认识到某人的人性——例如，[误识别黑人](https://oreil.ly/GjyTI)在自动图像标记中。
- en: Stereotyping
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 刻板印象
- en: The tendency to assign characteristics to all members of a group—e.g., LMs automatically
    associating [Muslims with violence](https://oreil.ly/eqAgw).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 倾向于将所有群体成员赋予特征——例如，LM自动将[Muslims与暴力](https://oreil.ly/eqAgw)联系起来。
- en: Underrepresentation
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Underrepresentation
- en: The lack of fair or adequate representation of demographic groups in model outputs—e.g.,
    generative models thinking [all doctors are white males and all nurses are white
    females](https://oreil.ly/V64lj).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型输出中缺乏公平或充分的对人群群体的代表性——例如，生成模型认为[所有医生都是白人男性，所有护士都是白人女性](https://oreil.ly/V64lj)。
- en: 'Sometimes these harms may only cause effects limited to online or digital spaces,
    but as our digital lives begin to overlap more substantially with other parts
    of our lives, harms also spill over into the real world. ML systems in healthcare,
    employment, education, or other high-risk areas can cause harm directly, by wrongfully
    denying people access to needed resources. The most obvious types of real-world
    harms caused by ML systems include the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有时这些伤害可能仅在在线或数字空间中产生影响，但随着我们的数字生活越来越多地与生活的其他部分重叠，伤害也会溢出到现实世界。在医疗保健、就业、教育或其他高风险领域中，机器学习系统可能通过错误地拒绝人们获取所需资源而直接造成伤害。由机器学习系统造成的最明显的现实世界伤害类型包括以下几种：
- en: Economic harms
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 经济伤害
- en: When an ML system reduces the economic opportunity or value of some activity—e.g.,
    when men [see more ads](https://oreil.ly/BT-cI) for better jobs than women.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习系统降低某些活动的经济机会或价值时——例如，男性看到更多比女性更好的工作广告时[（链接）](https://oreil.ly/BT-cI)。
- en: Physical harms
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 身体伤害
- en: When an ML system hurts or kills someone—e.g., when people [overrely on self-driving
    automation](https://oreil.ly/BxH5Y).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习系统伤害或导致某人死亡时——例如，人们过度依赖[自动驾驶技术](https://oreil.ly/BxH5Y)时。
- en: Psychological harms
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 心理伤害
- en: When an ML system causes mental or emotional distress—e.g., when [disturbing
    content](https://oreil.ly/pQRYE) is recommended to children.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习系统引发心理或情感困扰时——例如，当推荐[令人不安的内容](https://oreil.ly/pQRYE)给儿童时。
- en: Reputational harms
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 声誉伤害
- en: When an ML system diminishes the reputation of an individual or organization—e.g.,
    a consumer credit product rollout is marred by [accusations](https://oreil.ly/Wbvq5)
    of discrimination.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习系统损害个人或组织的声誉时——例如，消费信用产品的推出因[歧视指控](https://oreil.ly/Wbvq5)而受损。
- en: Unfortunately, users or subjects of ML systems may experience additional harms
    or combinations of harms that manifest in strange ways. Before we get too deep
    in the weeds with different kinds of bias testing in the next section, remember
    that checking in with our users to make sure they are not experiencing the harms
    discussed here, or other types of harms, is perhaps one of most direct ways to
    track bias in ML systems. In fact, in the most basic sense, it matters much more
    whether people are experiencing harm than whether some set of scores passes a
    necessarily flawed mathematical test. We must think about these harms when designing
    our system, talk to our users to ensure they don’t experience harm, and seek to
    mitigate harms.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，机器学习系统的用户或主体可能会以奇怪的方式遭受额外的伤害或伤害的组合。在我们深入讨论下一节中各种偏见测试之前，请记住，与用户确认他们是否遭受到这里讨论的伤害或其他类型的伤害，可能是追踪机器学习系统偏见最直接的方法之一。实际上，从最基本的角度来看，重要的是人们是否经历伤害，而不是某些分数是否通过了一个必然存在缺陷的数学测试。在设计我们的系统时，我们必须考虑这些伤害，与我们的用户沟通以确保他们不会遭受伤害，并努力减少伤害。
- en: Testing for Bias
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测偏见
- en: If there’s a chance that an ML system could harm people, it should be tested
    for bias. The goal of this section is to cover the most common approaches for
    testing ML models for bias so readers can get started with this important risk
    management task. Testing is neither straightforward nor conclusive. Just like
    in performance testing, a system can look fine on test data, and go on to fail
    or cause harm once deployed. Or a system could exhibit minimal bias at testing
    and deployment time, but drift into making biased or harmful predictions over
    time. Moreover, there are many tests and effect size measurements with known flaws
    and that conflict with one another. For a good overview of these issues, see the
    YouTube video of Princeton Professor Arvind Narayanan’s conference talk [“21 Fairness
    Definitions and Their Politics”](https://oreil.ly/4QnqM), from the ACM Conference
    on Fairness, Accountability, and Transparency in ML. For an in-depth mathematical
    analysis of why we can’t simply minimize all bias metrics at once, check out [“Inherent
    Trade-Offs in the Fair Determination of Risk Scores”](https://oreil.ly/WvBOg).
    With these cautions in mind, let’s start our tour of contemporary bias-testing
    approaches.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有可能，ML系统可能会对人们造成伤害，应该对其进行偏见测试。本节的目标是介绍测试ML模型偏见的最常见方法，以便读者可以开始这一重要的风险管理任务。测试既不简单也不确定。就像在性能测试中一样，系统在测试数据上看起来可能很好，但在部署后可能会失败或造成伤害。或者系统在测试和部署时可能表现出最小的偏见，但随着时间的推移可能会演变成做出有偏见或有害预测。此外，已知存在许多测试和效果大小测量存在缺陷并相互冲突。要了解这些问题的概述，请参阅普林斯顿大学教授阿尔温德·纳拉亚南在ML公平性、问责性和透明度会议上的演讲视频[“21个公平定义及其政治”](https://oreil.ly/4QnqM)。要深入了解为什么我们不能简单地同时最小化所有偏见度量的数学分析，请参阅[“在公平确定风险评分中的固有权衡”](https://oreil.ly/WvBOg)。牢记这些警告，让我们开始探索当代偏见测试方法。
- en: Testing Data
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试数据
- en: This section covers what’s needed in training data to test for bias, and how
    to test that data for bias even before a model is trained. ML models learn from
    data. But no data is perfect or without bias. If systemic bias is represented
    in training data, that bias will likely manifest in the model’s outputs. It’s
    logical to start testing for bias in training data. But to do that, we have to
    assume that certain columns of data are available. At minimum, for each row of
    data, we need demographic markers, known outcomes (`y`, dependent variable, target
    feature, etc.) and later, we’ll need model outcomes—predictions for regression
    models, and decisions and confidence scores or posterior probabilities for classification
    models. While there are a handful of testing approaches that don’t require demographic
    markers, most accepted approaches require this data. Don’t have it? Testing is
    going to be much more difficult, but we’ll provide some guidance on inferring
    demographic marker labels too.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了测试偏见所需的训练数据以及如何在模型训练之前对该数据进行偏见测试。ML模型从数据中学习。但没有一种数据是完美的或没有偏见的。如果训练数据中存在系统性偏见，该偏见可能会表现在模型的输出中。从训练数据开始测试偏见是合乎逻辑的。但要做到这一点，我们必须假设某些数据列是可用的。至少，对于每一行数据，我们需要人口统计标记、已知结果（`y`，依赖变量，目标特征等），稍后，我们将需要模型结果——回归模型的预测以及分类模型的决策和置信分数或后验概率。虽然有少数不需要人口统计标记的测试方法，但大多数被接受的方法都需要这些数据。没有这些数据？测试将会更加困难，但我们将提供一些关于推断人口统计标记标签的指导。
- en: Note
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Our models and data are far from perfect, so don’t let the perfect be the enemy
    of the good in bias testing. Our data will never be perfect and we’ll never find
    the perfect test. Testing is very important to get right, but to be successful
    in real-world bias mitigation, it’s just one part of broader ML management and
    governance processes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型和数据远非完美，所以在偏见测试中，不要让完美成为良好结果的敌人。我们的数据永远不会完美，我们也永远找不到完美的测试。测试非常重要，但要在现实世界中成功进行偏见缓解，它只是更广泛的ML管理和治理流程的一部分。
- en: The need to know or infer demographic markers is a good example of why handling
    bias in ML requires holistic design thinking, not just slapping another Python
    package onto the end of our pipeline. Demographic markers and individual-level
    data are also more sensitive from a privacy standpoint, and sometimes organizations
    don’t collect this information for data privacy reasons. While the interplay of
    data privacy and nondiscrimination law is very complex, it’s probably not the
    case that data privacy obligations override nondiscrimination obligations. But
    as data scientists, we can’t answer such questions on our own. Any perceived conflict
    between data privacy and nondiscrimination requirements has to be addressed by
    attorneys and compliance specialists. Such complex legal considerations are an
    example of why addressing bias in ML necessitates the engagement of a broad set
    of stakeholders.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 需要了解或推断人口统计学标记的需求是解决机器学习中偏见问题的一个很好的例子，这需要全面的设计思维，而不仅仅是在我们的管道末端添加另一个Python包。人口统计标记和个体级数据在隐私角度上也更为敏感，有时组织出于数据隐私原因不收集这些信息。虽然数据隐私与非歧视法律的相互作用非常复杂，但很可能并非数据隐私义务能够取代非歧视义务。但作为数据科学家，我们不能单独回答这类问题。任何数据隐私与非歧视要求之间的潜在冲突必须由律师和合规专家处理。这类复杂的法律考量是解决机器学习中偏见问题时必须引入广泛利益相关者的一个例子。
- en: Warning
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: In employment, consumer finance, or other areas where disparate treatment is
    prohibited, we need to check with our legal colleagues before changing our data
    based directly on protected class membership information, even if our intention
    is to mitigate bias.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在就业、消费者金融或其他禁止差异对待的领域，我们需要在直接基于受保护类成员信息改变我们的数据之前与法律同事核实，即使我们的意图是为了减少偏见。
- en: 'By now, readers are probably starting to realize how challenging and complex
    bias testing can be. As technicians, dealing with this complexity is not our sole
    responsibility, but we need to be aware of it and work within a broader team to
    address bias in ML systems. Now, let’s step into the role of a technician responsible
    for preparing data and testing data for bias. If we have the data we need, we
    tend to look for three major issues—representativeness, distribution of outcomes,
    and proxies:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，读者们可能开始意识到进行偏见测试有多么具有挑战性和复杂性。作为技术人员，处理这种复杂性不是我们唯一的责任，但我们需要意识到它并在更广泛的团队中共同努力解决机器学习系统中的偏见问题。现在，让我们进入负责准备数据并测试偏见的技术人员角色。如果我们有所需的数据，我们倾向于寻找三个主要问题——代表性、结果分布和代理：
- en: Representiveness
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性
- en: The basic check to run here is to calculate the proportion of rows for each
    demographic group in the training data, with the idea that a model will struggle
    to learn about groups with only a small number of training data rows. Generally,
    proportions of different demographic groups in training data should reflect the
    population on which the model will be deployed. If it doesn’t, we should probably
    collect more representative data. It’s also possible to resample or reweigh a
    dataset to achieve better representativeness. However, if we’re working in employment,
    consumer finance, or other areas where disparate treatment is prohibited, we really
    need to check with our legal colleagues before changing our data based directly
    on protected class membership information. If we’re running into differential
    validity problems (described later in this chapter), then rebalancing our training
    data to have larger or equal representation across groups may be in order. Balance
    among different classes may increase prediction quality across groups, but it
    may not help with, or may even worsen, imbalanced distributions of positive outcomes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的基本检查是计算训练数据中每个人口统计群体的行比例，理念是一个模型将难以学习只有少量训练数据行的群体。一般来说，训练数据中不同人口统计群体的比例应反映模型将被部署的人口。如果不是这样，我们可能需要收集更具代表性的数据。也可以对数据集进行重抽样或重新加权以实现更好的代表性。然而，如果我们在就业、消费者金融或其他禁止差异对待的领域工作，我们确实需要在直接基于受保护类成员信息改变我们的数据之前与法律同事核实。如果我们遇到差异有效性问题（本章后面描述），那么重新平衡我们的训练数据以在群体之间具有更大或相等的代表性可能是有必要的。在不同类别之间实现平衡可能会提高跨群体的预测质量，但可能不会有助于或甚至会恶化正面结果的不平衡分布。
- en: Distribution of outcomes
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 结果分布
- en: We need to know how outcomes (`y` variable values) are distributed across demographic
    groups, because if the model learns that some groups receive more positive outcomes
    than others, that can lead to disparate impact. We need to calculate a bivariate
    distribution of `y` across each demographic group. If we see an imbalance of outcomes
    across groups, then we can try to resample or reweigh our training data, with
    certain legal caveats. More likely, we’ll simply end up knowing that bias risks
    are serious for this model, and when we test its outcomes, we’ll need to pay special
    attention and likely plan on some type of remediation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要了解结果（`y`变量值）如何在不同人群之间分布，因为如果模型学习到某些群体比其他群体获得更多积极结果，这可能导致不均衡影响。我们需要计算`y`在每个人群中的双变量分布。如果我们发现不同人群之间的结果不平衡，那么我们可以尝试重新抽样或重新加权我们的训练数据，但有一些法律上的注意事项。更可能的情况是，我们将简单地意识到这个模型存在严重的偏差风险，当我们测试其结果时，我们需要特别注意，并可能计划某种补救措施。
- en: Proxies
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 代理
- en: In most business applications of ML, we should *not* be training models on demographic
    markers. But even if we don’t use demographic markers directly, information like
    names, addresses, educational details, or facial images may encode a great deal
    of demographic information. Other types of information may proxy for demographic
    markers too. One way to find proxies is to build an adversarial model based on
    each input column and see if those models can predict any demographic marker.
    If they can predict a demographic marker, then those columns encode demographic
    information and are likely demographic proxies. If possible, such proxies should
    be removed from training data. Proxies may also be more hidden in training data.
    There’s no standard technique to test for these latent proxies, but we can apply
    the same adversarial modeling technique as described for direct proxies, except
    instead of using the features themselves, we can use engineered interactions of
    features that we suspect may be serving as proxies. We also suggest having dedicated
    legal or compliance stakeholders vet each and every input feature in our model
    with an eye toward proxy discrimination risk. If proxies cannot be removed or
    we suspect the presence of latent proxies, we should pay careful attention to
    bias-testing results for system outcomes, and be prepared to take remediation
    steps later in the bias mitigation process.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数商业应用的机器学习中，我们不应该根据人口统计标记来训练模型。但即使我们不直接使用人口统计标记，像姓名、地址、教育细节或面部图像这样的信息可能编码了大量的人口统计信息。其他类型的信息也可能作为人口统计标记的代理。发现代理的一种方法是基于每个输入列构建对抗模型，看这些模型是否能预测任何人口统计标记。如果它们能够预测人口统计标记，那么这些列编码了人口统计信息，并且很可能是人口统计的代理。如果可能的话，应该从训练数据中移除这些代理。代理在训练数据中可能也更隐蔽。没有标准技术来测试这些潜在的代理，但我们可以应用与直接代理相同的对抗建模技术，不过不使用特征本身，而是使用我们怀疑可能充当代理的特征之间的工程化交互。我们还建议，拥有专门的法律或合规利益相关者，审查我们模型中的每一个输入特征，以便关注代理歧视风险。如果无法移除代理，或者我们怀疑存在潜在的代理，我们应该特别注意系统结果的偏差测试结果，并准备在偏差缓解过程中采取补救措施。
- en: The outlined tests and checks for representativeness, distribution of outcomes,
    and proxies in training data all rely on the presence of demographic group markers,
    as will most of the tests for model outcomes. If we don’t have those demographic
    labels, then one accepted approach is to infer them. The [Bayesian improved surname
    geocoding (BISG)](https://oreil.ly/cJn-M) approach infers race and ethnicity from
    name and postal code data. It’s sad but true that US society is still so segregated
    that zip code and name can predict race and ethnicity, often with above 90% accuracy.
    This approach was developed by the RAND Corporation and the Consumer Financial
    Protection Bureau (CFPB) and has a high level of credibility for bias testing
    in consumer finance. The CFPB even has code on its [GitHub](https://oreil.ly/hkvMD)
    for BISG! If necessary, similar approaches may be used to [infer gender](https://oreil.ly/eLTqM)
    from name, Social Security number, or birth year.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 描述的测试和代表性、结果分布以及训练数据中的代理检查，所有这些都依赖于人口统计标签的存在，就像大多数模型结果测试一样。如果我们没有这些人口统计标签，那么一个被接受的方法是推断它们。[贝叶斯改进的姓氏地理编码（BISG）](https://oreil.ly/cJn-M)方法从姓名和邮政编码数据中推断种族和族裔。令人悲哀但事实如此，美国社会仍然如此分离，以至于邮政编码和姓名往往能够准确预测种族和族裔，准确率通常超过90%。这种方法是由兰德公司和消费者金融保护局（CFPB）开发的，并且在消费金融领域的偏见测试中具有很高的可信度。CFPB甚至在其[GitHub](https://oreil.ly/hkvMD)上提供了BISG的代码！如有必要，类似的方法也可以用于从姓名、社会安全号码或出生年推断[性别](https://oreil.ly/eLTqM)。
- en: 'Traditional Approaches: Testing for Equivalent Outcomes'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统方法：测试等效结果
- en: Once we’ve assessed our data for bias, made sure we have the information needed
    to perform bias testing, and trained a model, it’s time to test its outcomes for
    bias. We’ll start our discussion on bias testing by addressing some established
    tests. These tests tend to have some precedent in law, regulation, or legal commentary,
    and they tend to focus on average differences in outcomes across demographic groups.
    For a great summary of traditional bias-testing guidance, see the [concise guidance](https://oreil.ly/_bcVD)
    of the Office of Federal Contract Compliance Programs for testing employment selection
    procedures. For these kinds of tests, it doesn’t matter if we’re analyzing the
    scores from a multiple choice employment test or numeric scores from a cutting-edge
    AI-based recommender system.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们评估了数据的偏见，确保我们有执行偏见测试所需的信息，并训练了一个模型，那么现在就是测试其结果是否存在偏见的时候了。我们将从讨论偏见测试的角度开始，首先解决一些已建立的测试。这些测试通常在法律、法规或法律评论中具有先例，并且它们倾向于关注不同人群之间结果的平均差异。关于传统偏见测试指导的很好概述，请参阅[简明指南](https://oreil.ly/_bcVD)，由联邦承包合规办公室提供，用于测试就业选择程序的公平性。对于这类测试，无论是分析多项选择就业测试的分数还是来自先进人工智能推荐系统的数值分数，都无关紧要。
- en: Note
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The tests in this section are aligned to the notion *statistical parity*, or
    when a model generates roughly equal probabilities or favorable predictions for
    all demographic groups.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的测试与 *统计平等* 的概念对齐，即模型为所有人群生成大致相等的概率或有利的预测。
- en: '[Table 4-1](#fairness_metrics) highlights how these tests tend to be divided
    into categories for statistical and practical tests, and for continuous and binary
    outcomes. These tests rely heavily on the notion of protected groups, where the
    mean outcome for the protected group (e.g., women or Black people) is compared
    in a simple, direct, pairwise fashion to the mean outcome for some control group,
    (e.g., men or white people, respectively). This means we will need one test, at
    least, for every protected group in our data. If this sounds old fashioned, it
    is. But since these are the tests that have been used the most in regulatory and
    litigation settings for decades, it’s prudent to start with these tests before
    getting creative with newer methodologies. More established tests also tend to
    have known thresholds that indicate when values are problematic. These thresholds
    are listed in [Table 4-1](#fairness_metrics) and discussed in more detail in the
    sections that follow.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 4-1](#fairness_metrics)强调这些测试通常被分为统计和实用测试的类别，并且适用于连续和二元结果。这些测试在很大程度上依赖于保护组的概念，其中受保护组的平均结果（例如女性或黑人）与某个控制组（例如男性或白人）的平均结果进行简单直接的配对比较。这意味着我们至少需要为数据中的每个保护组进行一个测试。如果这听起来有些老派，确实是这样。但由于这些测试在法规和诉讼设置中使用最多几十年，因此在开始尝试新的方法之前，最好从这些测试开始。更为成熟的测试还倾向于具有已知的阈值，指示何时的值存在问题。这些阈值列在[表
    4-1](#fairness_metrics)中，并在接下来的章节中进行了更详细的讨论。'
- en: Table 4-1\. Some common metrics used to measure bias in ML models, with thresholds
    where applicable^([a](ch04.html#idm45990002257856))
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1. 一些用于衡量机器学习模型偏差的常见指标，适用的阈值如有^([a](ch04.html#idm45990002257856))所示
- en: '| Test type | Discrete outcome/Classification tests | Continuous outcome/Regression
    tests |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 测试类型 | 离散结果/分类测试 | 连续结果/回归测试 |'
- en: '| --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Statistical significance | Logistic regression coefficient | Linear regression
    coefficient |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 统计显著性 | 逻辑回归系数 | 线性回归系数 |'
- en: '| Statistical significance | χ² test | *t*-test |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 统计显著性 | χ² 检验 | *t*-检验 |'
- en: '| Statistical significance | Fisher’s exact test |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 统计显著性 | Fisher’s 精确检验 |  |'
- en: '| Statistical significance | Binomial-*z* |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 统计显著性 | 二项 *z* |  |'
- en: '| Practical significance | Comparison of group means | Comparison of group
    means |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 实际显著性 | 组间均值比较 | 组间均值比较 |'
- en: '| Practical significance | Percentage point difference between group means/marginal
    effect | Percentage point difference between group means |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 实际显著性 | 组间均值差异的百分点/marginal effect | 组间均值差异的百分点 |'
- en: '| Practical significance | Adverse impact ratio (AIR) (acceptable: `0.8`–`1.25`)
    | Standardized mean difference (SMD, Cohen’s *d*) (small difference: `0.2`, medium
    difference: `0.5`, large difference: `0.8`) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 实际显著性 | Adverse impact ratio (AIR)（可接受范围：`0.8`–`1.25`） | Standardized mean
    difference (SMD, Cohen’s *d*)（小差异：`0.2`，中等差异：`0.5`，大差异：`0.8`） |'
- en: '| Practical significance | Odds ratios |  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 实际显著性 | 概率比 |  |'
- en: '| Practical significance | Shortfall to parity |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 实际显著性 | 落后到平等 |  |'
- en: '| Differential validity | Accuracy or AUC ratios (acceptable: `0.8`–`1.25`)
    | R² ratio (acceptable: `0.8`–`1.25`) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 差异效度 | 准确度或AUC比率（可接受范围：`0.8`–`1.25`） | R² 比率（可接受范围：`0.8`–`1.25`） |'
- en: '| Differential validity | TPR, TNR, FPR, FNR ratios (acceptable: `0.8`–`1.25`)
    | MSE, RMSE ratios (acceptable: `0.8`–`1.25`) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 差异效度 | TPR、TNR、FPR、FNR 比率（可接受范围：`0.8`–`1.25`） | MSE、RMSE 比率（可接受范围：`0.8`–`1.25`）
    |'
- en: '| Differential validity | Equality of odds ([control TPR ≈ protected TPR ∣
    `y = 1`] and [control FPR ≈ protected FPR ∣ `y = 0`]) |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 差异效度 | 赔率平等（[控制 TPR ≈ 保护 TPR ∣ `y = 1`] 和 [控制 FPR ≈ 保护 FPR ∣ `y = 0`]） |  |'
- en: '| Differential validity | Equality of opportunity ([control TPR ≈ protected
    TPR ∣ `y = 1`]) |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 差异效度 | 机会平等（[控制 TPR ≈ 保护 TPR ∣ `y = 1`]） |  |'
- en: '| ^([a](ch04.html#idm45990002257856-marker)) TPR = true positive rate; TNR
    = true negative rate; FPR = false positive rate; FNR = false negative rate |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch04.html#idm45990002257856-marker)) TPR = 真正例率；TNR = 真负例率；FPR = 假正例率；FNR
    = 假负例率 |'
- en: Statistical significance testing
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 统计显著性测试
- en: Statistical significance testing probably has the most acceptance across disciplines
    and legal jurisdictions, so let’s focus there first. Statistical significance
    testing is used to determine whether average or proportional differences in model
    outcomes across protected groups are likely to be seen in new data, or whether
    the differences in outcomes are random properties of our current testing datasets.
    For continuous outcomes, we often rely on *t*-tests between mean model outputs
    across two demographic groups. For binary outcomes, we often use binomial *z*-tests
    on the proportions of positive outcomes across two different demographic groups,
    chi-squared tests on contingency tables of model outputs, and Fisher’s exact test
    when cells in the contingency test have less than 30 individuals in them.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 统计显著性测试可能是跨学科和法律管辖区最广泛接受的，因此让我们首先关注它。统计显著性测试用于确定在新数据中是否可能看到跨受保护群体的模型结果的平均或比例差异，或者结果差异是否是当前测试数据集的随机属性。对于连续结果，我们经常依赖于两个人口统计学组之间平均模型输出的
    *t* 检验。对于二元结果，我们经常使用二项 *z* 检验来检验两个不同人口统计学组的正面结果比例，卡方检验来检验模型输出的列联表，以及当列联表中的单元格中的个体少于
    30 个时，使用 Fisher 精确检验。
- en: If you’re thinking this is a lot of pairwise tests that leave out important
    information, good job! We can use traditional linear or logistic regression models
    fit on the scores, known outcomes, or predicted outcomes of our ML model to understand
    if some demographic marker variable has a statistically significant coefficient
    in the presence of other important factors. Of course, evaluating statistical
    significance is difficult too. Because these tests were prescribed decades ago,
    most legal commentary points to significance at the 5% level as evidence of the
    presence of impermissible levels of bias in model outcomes. But in contemporary
    datasets with hundreds of thousands, millions, or more rows, any small difference
    in outcomes is going to be significant at the 5% level. We recommend analyzing
    traditional statistical bias-testing results at the 5% significance level and
    with significance level adjustments that are appropriate for our dataset size.
    We’d focus most of our energy on the adjusted results, but keep in mind that in
    the worst-case scenario, our organization could potentially face legal scrutiny
    and bias testing by external experts that would hold us to the 5% significance
    threshold. This would be yet another great time to start speaking with our colleagues
    in the legal department.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为这是一大堆成对测试，忽略了重要信息，干得好！我们可以使用传统的线性或逻辑回归模型，拟合我们机器学习模型的分数、已知结果或预测结果，以了解某些人口统计标记变量在其他重要因素存在的情况下是否具有统计显著的系数。当然，评估统计显著性也是困难的。因为这些测试几十年前就被规定了，大多数法律评论都指出，在5%的水平上的显著性是证明模型结果中存在不可接受的偏倚的证据。但在当代数据集中，行数达到数十万、数百万或更多时，任何小的结果差异都将在5%的水平上显著。我们建议在5%显著性水平上分析传统的统计偏倚测试结果，并根据我们数据集大小适当调整显著性水平。我们会把大部分精力放在调整后的结果上，但要记住，在最坏的情况下，我们的组织可能会面临法律审查和外部专家的偏倚测试，这些测试将把我们约束在5%的显著性阈值上。这将是与我们法律部门同事交流的又一个好时机。
- en: Practical significance testing
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际意义测试
- en: The adverse impact ratio (AIR) and its associated four-fifths rule threshold
    are probably the most well-known and most abused bias-testing tools in the US.
    Let’s consider what it is first, then proceed to how it’s abused by practitioners.
    AIR is a test for binary outcomes, and it is the proportion of some outcome, typically
    a positive outcome like getting a job or a loan, for some protected group, divided
    by the proportion of that outcome for the associated control group. That proportion
    is associated with a threshold of four-fifths or 0.8\. This four-fifths rule was
    highlighted by the EEOC in the late 1970s as a practical line in the sand, with
    results above four-fifths being highly preferred. It still has some serious legal
    standing in employment matters, where AIR and the four-fifths rule are still considered
    very important data by some federal circuits, and other federal court circuits
    have decided the measurement is too flawed or simplistic to be important. In most
    cases, AIR and the four-fifths rule have no official legal standing outside of
    employment, but they are still used occasionally as an internal bias-testing tool
    across regulated verticals like consumer finance. Moreover, AIR could always show
    up in the testimony of an expert in a lawsuit, for any bias-related matter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 美国最为人熟知且最常被滥用的偏倚测试工具是不利影响比率（AIR）及其相关的四分之五法则阈值。让我们首先了解它是什么，然后再看它是如何被从业者滥用的。AIR是用于二元结果的测试，是某些结果的比例，通常是积极结果，比如得到工作或贷款的比例，针对某些受保护的群体，除以关联控制组的该结果比例。这个比例与四分之五或0.8的阈值相关联。这个四分之五法则在1970年代末由EEOC（美国就业机会平等委员会）提出，作为一条实用的分界线，四分之五以上的结果非常受欢迎。在就业事务中，AIR和四分之五法则仍被一些联邦法院认为是非常重要的数据，而其他联邦法院则认为这种测量过于缺陷或简化，不重要。在大多数情况下，AIR和四分之五法则在就业之外没有正式的法律地位，但在消费金融等受管制行业仍偶尔用作内部偏倚测试工具。此外，AIR可能会出现在专家在诉讼中的证词中，涉及任何与偏倚相关的事项。
- en: AIR is an easy and popular bias test. So, what do we get wrong about AIR? Plenty.
    Technicians tend to interpret it incorrectly. An AIR over 0.8 is not necessarily
    a good sign. If our AIR test comes out below 0.8, that’s probably a bad sign.
    But if it’s above four-fifths, that doesn’t mean everything is OK. Another issue
    is the confusion of the AIR metric and the 0.8 threshold with the legal construct
    of disparate impact. We can’t explain why, but some vendors call AIR, literally,
    “disparate impact.” They are not the same. Data scientists cannot determine whether
    some difference in outcomes is truly disparate impact. Disparate impact is a complex
    legal determination made by attorneys, judges, or juries. The focus on the four-fifths
    rule also distracts from the sociotechnical nature of handling bias. Four-fifths
    is only legally meaningful in some employment cases. Like any numeric result,
    AIR test results alone are insufficient for the identification of bias in a complex
    ML system.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: AIR 是一种简单而流行的偏见测试。那么，我们对 AIR 的误解有哪些呢？很多。技术人员往往会错误地解释它。AIR 超过 0.8 不一定是好迹象。如果我们的
    AIR 测试结果低于 0.8，那可能是个坏迹象。但如果超过五分之四，这并不意味着一切都没问题。另一个问题是混淆 AIR 指标和 0.8 阈值与法律概念的差异冲击。我们无法解释为什么，但有些供应商字面上称
    AIR 为“差异冲击”。它们并不相同。数据科学家不能确定某些结果的差异是否真正属于差异冲击。差异冲击是由律师、法官或陪审团做出的复杂法律决定。对四分之五规则的关注也使处理偏见的社会技术性质变得模糊。在某些就业案件中，四分之五只有在法律上有意义。像任何数值结果一样，单靠
    AIR 测试结果是不足以在复杂的机器学习系统中识别偏见的。
- en: All that said, it’s still probably a good idea to look into AIR results and
    other practical significance results. Another common measure is standardized mean
    difference (SMD, or Cohen’s *d*). SMD can be used on regression or classification
    outputs—so it’s even more model-agnostic than AIR. SMD is the mean outcome or
    score for some protected group minus the mean outcome or score for a control group,
    with that quantity divided by a measure of the standard deviation of the outcome.
    Magnitudes of SMD at 0.2, 0.5, and 0.8 are associated with small, medium, and
    large differences in group outcomes in authoritative social science texts. Other
    common practical significance measures are percentage point difference (PPD),
    or the difference in mean outcomes across two groups expressed as a percentage,
    and shortfall, the number of people or the monetary amount required to make outcomes
    equivalent across a protected and control group.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，查看 AIR 结果和其他实际意义结果仍然可能是个好主意。另一个常见的度量标准是标准化平均差异（SMD，或科恩的*d*）。SMD 可以用于回归或分类输出——因此比
    AIR 更加模型无关。SMD 是某个受保护群体的平均结果或得分减去控制组的平均结果或得分，再除以结果的标准差的一种度量。在权威社会科学文献中，SMD 的大小为
    0.2、0.5 和 0.8 与群体结果的小、中和大差异相关。其他常见的实际意义度量包括百分点差异（PPD），或者两个群体的平均结果之间的差异，以百分比表示，以及缺额，使受保护群体和控制群体的结果等同所需的人数或货币金额。
- en: The worst-case scenario in traditional outcomes testing is that both statistical
    and practical testing results show meaningful differences in outcomes across one
    or more pairs or protected and control groups. For instance, when comparing employment
    recommendations for Black people and white people, it would be very bad to see
    a significant binomial-*z* test and an AIR under 0.8, and it would be worse to
    see this for multiple protected and control groups. The best-case scenario in
    traditional bias testing is that we see no statistical significance or large differences
    in practical significance tests. But even in this case, we still have no guarantees
    that a system won’t be biased once it’s deployed or isn’t biased in ways these
    tests don’t detect, like via screen out. Of course, the most likely case in traditional
    testing is that we will see some mix of results and will need help interpreting
    them, and fixing detected problems, from a group of stakeholders outside our direct
    data science team. Even with all that work and communication, traditional bias
    testing would only be the first step in a thorough bias-testing exercise. Next
    we’ll discuss some newer ideas on bias testing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的结果测试中，最坏的情况是，统计和实际测试结果显示在一个或多个受保护和对照组之间的结果有意义的差异。例如，当比较黑人和白人的就业推荐时，看到显著的二项式-*z*检验和低于0.8的AIR将是非常不好的，如果这种情况出现在多个受保护和对照组上则更糟。在传统偏差测试中，最理想的情况是我们在统计上看不到显著性差异或大的实际显著性测试差异。但即使在这种情况下，我们仍然无法保证系统在部署后不会存在偏差，或者在这些测试无法检测到的方式（如通过屏幕排除）中没有偏差。当然，在传统测试中最可能的情况是我们会看到一些混合的结果，并且需要帮助来解释这些结果，并从我们直接数据科学团队之外的利益相关者那里修复检测到的问题。即使在进行了所有这些工作和沟通之后，传统偏差测试也只能算是彻底偏差测试的第一步。接下来我们将讨论一些关于偏差测试的新想法。
- en: 'A New Mindset: Testing for Equivalent Performance Quality'
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种新的思维方式：测试等效性能质量
- en: In more recent years, many researchers have put forward testing approaches that
    focus on disparate performance quality across demographic groups. Though these
    tests have less legal precedent than traditional tests for practical and statistical
    significance, they are somewhat related to the concept of differential validity.
    These newer techniques seek to understand how common ML prediction errors may
    affect minority groups, and to ensure that humans interacting with an ML system
    have an equal opportunity to receive positive outcomes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的几年里，许多研究人员提出了侧重于不同人群之间的差异性能质量的测试方法。尽管这些测试在实际和统计显著性方面的法律先例较少，但它们与差异有效性的概念有些相关。这些新技术旨在了解常见的机器学习预测错误如何影响少数族裔群体，并确保与机器学习系统交互的人类在获得积极结果时具有平等的机会。
- en: 'The important paper [“Fairness Beyond Disparate Treatment and Disparate Impact:
    Learning Classification without Disparate Mistreatment”](https://oreil.ly/NkTBF)
    lays out the case for why it’s important to think through ML model errors in the
    context of fairness. If minority groups receive more false positive or false negative
    decisions than other groups, any number of harms can arise depending on the application.
    In their seminal [“Equality of Opportunity in Machine Learning”](https://oreil.ly/_w-c3),
    Hardt, Price, and Srebro define a notion of fairness that modifies the widely
    acknowledged equalized odds idea. In the older equalized odds scenario, when the
    known outcome occurs (i.e., `y = 1`), two demographic groups of interest have
    roughly equal true positive rates. When the known outcome does not occur (i.e.,
    `y = 0`), equalized odds means that false positive rates are roughly equal across
    two demographic groups. Equality of opportunity relaxes the `y = 0` constraint
    of equalized odds and argues that when `y = 1` equates to a positive outcome,
    such as receiving a loan or getting a job, seeking equalized true positive rates
    is a simpler and more utilitarian approach.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的论文[《超越不同对待和不同影响的公平性：在没有不同对待的情况下学习分类》](https://oreil.ly/NkTBF)阐述了在公平性背景下思考机器学习模型误差的重要性。如果少数族裔群体比其他群体接受更多的误报或误拒决策，那么根据应用的不同可能会带来多种伤害。在他们的开创性[《机器学习中的机会平等》](https://oreil.ly/_w-c3)中，Hardt、Price和Srebro定义了一个修改了广为认可的平等机会思想的公平性概念。在旧的平等机会场景中，当已知结果发生时（即`y
    = 1`），两个感兴趣的人群大致具有相等的真正阳性率。当未发生已知结果时（即`y = 0`），平等机会意味着两个人群的假阳性率大致相等。机会平等放宽了平等机会对`y
    = 0`的约束，并认为在`y = 1`等于正面结果时（例如获得贷款或找到工作），寻求平等的真正阳性率是一种更简单和更实用的方法。
- en: If readers have spent any time with confusion matrices, they’ll know there are
    many other ways to analyze the errors of a binary classifier. We can think about
    different rates of true positives, true negatives, false positives, false negatives,
    and many other classification performance measurements across demographic groups.
    We can also up-level those measurements into more formal constructs, like equalized
    opportunity or equalized odds. [Table 4-2](#diff_valid) provides an example of
    how performance quality and error metrics across demographic groups can be helpful
    in testing for bias.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果读者花费了时间研究混淆矩阵，他们将知道有许多其他方法来分析二元分类器的错误。我们可以考虑不同的真正例率、真负例率、假正例率、假负例率以及许多其他跨人群的分类性能测量。我们还可以将这些测量提升到更正式的构建中，如平等机会或平等几率。[表 4-2](#diff_valid)
    提供了一个示例，说明了跨人群的性能质量和错误度量如何在测试偏见方面有所帮助。
- en: Table 4-2\. Classification quality and error rates calculated across two demographic
    groups^([a](ch04.html#idm45990006006256))
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-2\. 在两个人群之间计算的分类质量和错误率^([a](ch04.html#idm45990006006256))
- en: '| Metric type | …​ | Accuracy | Sensitivity (TPR) | …​ | Specificity (TNR)
    | …​ | FPR | FNR | …​ |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 指标类型 | …​ | 准确度 | 灵敏度（TPR） | …​ | 特异性（TNR） | …​ | FPR | FNR | …​ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Female value | …​ | 0.808 | 0.528 | …​ | 0.881 | …​ | 0.119 | 0.472 | …​
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 女性数值 | …​ | 0.808 | 0.528 | …​ | 0.881 | …​ | 0.119 | 0.472 | …​ |'
- en: '| Male value | …​ | 0.781 | 0.520 | …​ | 0.868 | …​ | 0.132 | 0.480 | …​ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 男性数值 | …​ | 0.781 | 0.520 | …​ | 0.868 | …​ | 0.132 | 0.480 | …​ |'
- en: '| Female-to-male ratio | …​ | 1.035 | 1.016 | …​ | 1.016 | …​ | 1.069 | 0.983
    | …​ |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 女性对男性比率 | …​ | 1.035 | 1.016 | …​ | 1.016 | …​ | 1.069 | 0.983 | …​ |'
- en: '| ^([a](ch04.html#idm45990006006256-marker)) The values for the comparison
    group, females, are divided by the values for the control group, males. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch04.html#idm45990006006256-marker)) 比较组（女性）的值除以对照组（男性）的值。 |'
- en: The first step, shown in [Table 4-2](#diff_valid), is to calculate a set of
    performance and error measurements across two or more demographic groups of interest.
    Then, using AIR and the four-fifths rule as a guide, we form a ratio of the comparison
    group value to the control group value, and apply thresholds of four-fifths (0.8)
    and five-fourths (1.25) to highlight any potential bias issues. It’s important
    to say that the 0.8 and 1.25 thresholds are only guides here; they have no legal
    meaning and are more commonsense markers than anything else. Ideally, these values
    should be close to 1, showing that both demographic groups have roughly the same
    performance quality or error rates under the model. We may flag these thresholds
    with whatever values make sense to us, but we would argue that 0.8–1.25 is the
    maximum range of acceptable values.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，如 [表 4-2](#diff_valid) 所示，是计算两个或更多感兴趣人群组之间的一组性能和误差测量。然后，使用AIR和四分之五规则作为指导，我们形成比较组值与对照组值的比率，并应用四分之五（0.8）和五分之四（1.25）的阈值来突出任何潜在的偏见问题。重要的是指出，这里的0.8和1.25阈值仅是指导性的，它们没有法律意义，更多的是常识性的标志。理想情况下，这些值应接近1，显示模型下两个人群大致具有相同的性能质量或误差率。我们可以根据自己的情况标记这些阈值，但我们会认为0.8至1.25是可接受值的最大范围。
- en: Based on our application, some metrics may be more important than others. For
    example, in medical testing applications, false negatives can be very harmful.
    If one demographic group is experiencing more false negatives in a medical diagnosis
    than others, it’s easy to see how that can lead to bias harms. The fairness metric
    decision tree at slide 40 of [“Dealing with Bias and Fairness in AI/ML/Data Science
    Systems”](https://oreil.ly/Es2d1) can be a great tool for helping to decide which
    of all of these different fairness metrics might be best for our application.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的应用程序，某些指标可能比其他指标更重要。例如，在医学测试应用中，假阴性可能非常有害。如果某个人群在医学诊断中经历的假阴性比其他人群多，很容易看出这会导致偏见的危害。在
    [“处理AI/ML/数据科学系统中的偏见和公平性”](https://oreil.ly/Es2d1) 的第40页有一个公平度量决策树，可以帮助决定哪些公平度量可能最适合我们的应用。
- en: Are you thinking “What about regression? What about everything in ML outside
    of binary classification?!” It’s true that bias testing is most developed for
    binary classifiers, which can be frustrating. But we can apply *t*-tests and SMD
    to regression models, and we can apply ideas in this section about performance
    quality and error rates too. Just like we form ratios of classification metrics,
    we can also form ratios of R², mean average percentage error (MAPE), or normalized
    root mean square error (RMSE) across comparison and control groups, and again,
    use the four-fifths rule as a guide to highlight when these ratios may be telling
    us there is a bias problem in our predictions. As for the rest of ML, outside
    binary classification and regression, that’s what we will cover next. Be prepared
    to apply some ingenuity and elbow grease.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否在想：“回归模型呢？机器学习二元分类以外的一切呢？”确实，偏见测试在二元分类器上最为发达，这可能令人沮丧。但是我们可以将*t*-tests和SMD应用于回归模型，也可以应用本节关于性能质量和误差率的思想。就像我们形成分类指标的比率一样，我们也可以形成R²、平均百分比误差（MAPE）或归一化均方根误差（RMSE）在比较和控制组之间的比率，并再次使用四分之五法则作为指南，以突显这些比率可能告诉我们在预测中存在偏见问题的情况。至于机器学习的其余部分，超出了二元分类和回归，那就是我们接下来要讨论的。准备好运用一些创造力和努力。
- en: 'On the Horizon: Tests for the Broader ML Ecosystem'
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 看未来：更广泛的机器学习生态系统测试
- en: 'A great deal of research and legal commentary assumes the use of binary classifiers.
    There is a reason for this. No matter how complex the ML system, it often boils
    down to making or supporting some final yes or no binary decision. If that decision
    affects people and we have the data to do it, we should test those outcomes using
    the full suite of tools we’ve discussed already. In some cases, the output of
    an ML system does not inform an eventual binary decision, or perhaps we’d like
    to dig deeper and understand drivers of bias in our system or which subpopulations
    might be experiencing the most bias. Or maybe we’re using a generative model,
    like an LM or image generation system. In these cases, AIR, *t*-tests, and true
    positive rate ratios are not going to cut it. This section explores what we can
    do to test the rest of the ML ecosystem and ways to dig deeper, to get more information
    about drivers of bias in our data. We’ll start out with some general strategies
    that should work for most types of ML systems, and then briefly outline techniques
    for bias against individuals or small groups, LMs, multinomial classifiers, recommender
    systems, and unsupervised models:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究和法律评论都假设使用二元分类器。这有其原因。无论机器学习系统多么复杂，最终往往都可以归结为做出或支持某种最终的是或非二元决策。如果该决策影响到人们并且我们有数据可以进行测试，我们应该使用我们已经讨论过的完整工具套件来测试这些结果。在某些情况下，机器学习系统的输出并不会影响最终的二元决策，或者也许我们想要深入挖掘并了解系统中偏见的驱动因素或者哪些亚群体可能遭受了最多的偏见。或者我们可能正在使用生成模型，例如语言模型或图像生成系统。在这些情况下，AIR、*t*-tests
    和真阳性率比例是不够的。本节探讨了我们可以采取的方法来测试机器学习生态系统的其余部分，以及深入挖掘我们数据中偏见驱动因素的方法。我们将首先介绍一些通用策略，适用于大多数类型的机器学习系统，然后简要概述针对个体或小团体、语言模型、多项式分类器、推荐系统和无监督模型的偏见技术：
- en: General strategies
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通用策略
- en: One of the most general approaches for bias testing is adversarial modeling.
    Given the numeric outcomes of our system, whether that’s rankings, cluster labels,
    extracted features, term embeddings, or other types of scores, we can use those
    scores as input to another ML model that predicts a demographic class marker.
    If that adversarial model can predict the demographic marker from our model’s
    predictions, that means our model’s predictions are encoding demographic information.
    That’s usually a bad sign. Another general technical approach is to apply explainable
    AI techniques to uncover the main drivers of our model’s predictions. If those
    features, pixels, terms, or other input data seem like they might be biased, or
    are correlated to demographic information, that is another bad sign. There are
    now even [specific approaches](https://oreil.ly/CcS_9) for understanding which
    features are driving bias in model outcomes. Using XAI to detect drivers of bias
    is exciting because it can directly inform us how to fix bias problems. Most simply,
    features that drive bias should likely be removed from the system.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 检测偏见的最常见方法之一是对抗建模。鉴于我们系统的数值结果，无论是排名、聚类标签、提取的特征、词项嵌入还是其他类型的分数，我们可以将这些分数作为输入传递给另一个机器学习模型，该模型预测人口统计学类别标记。如果对抗模型能够从我们模型的预测中预测出人口统计学标记，那就意味着我们模型的预测正在编码人口统计信息。这通常是一个不好的迹象。另一个一般的技术方法是应用可解释的人工智能技术来揭示我们模型预测的主要驱动因素。如果这些特征、像素、术语或其他输入数据看起来可能存在偏见，或者与人口统计信息相关，那就是另一个不好的迹象。现在甚至有[特定方法](https://oreil.ly/CcS_9)来理解哪些特征在模型结果中驱动偏见。利用可解释人工智能来检测偏见的驱动因素令人振奋，因为它可以直接告诉我们如何解决偏见问题。简而言之，驱动偏见的特征很可能应该从系统中移除。
- en: Not all strategies for detecting bias should be technical in a well-rounded
    testing plan. Use resources like the [AI Incident Database](https://oreil.ly/Jc2vm)
    to understand how bias incidents have occurred in the past, and design tests or
    user-feedback mechanisms to determine if we are repeating past mistakes. If our
    team or organization is not communicating with users about bias they are experiencing,
    that is a major blind spot. We must *talk to our users.* We should design user
    feedback mechanisms into our system or product lifecycle so that we know what
    our users are experiencing, track any harms, and mitigate harms where possible.
    Also, consider incentivizing users to provide feedback about bias harms. The [Twitter
    Algorithmic Bias event](https://oreil.ly/RnPHy) serves as an amazing example of
    structured and incentivized crowd-sourcing of bias-related information. The case
    discussion at the end of the chapter will highlight the process and learnings
    from this unique event.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个全面的测试计划中，并非所有检测偏见的策略都应该是技术性的。使用像[AI事故数据库](https://oreil.ly/Jc2vm)这样的资源来了解过去发生的偏见事件，并设计测试或用户反馈机制，以确定我们是否重复了过去的错误。如果我们的团队或组织没有与用户沟通他们正在经历的偏见，那就是一个重大的盲点。我们必须*与我们的用户交流*。我们应该在系统或产品生命周期中设计用户反馈机制，以便了解我们的用户正在经历什么，追踪任何伤害，并在可能的情况下减轻伤害。还要考虑激励用户提供关于偏见伤害的反馈。[Twitter算法偏见事件](https://oreil.ly/RnPHy)是结构化和激励众包偏见相关信息的一个了不起的例子。本章末讨论案例将突出这一独特事件的过程和经验。
- en: Language models
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型
- en: Generative models present many bias issues. Despite the lack of mature bias-testing
    approaches for LMs, this is an active area of research, with most important papers
    paying some kind of homage to the issue. Section 6.2 of [“Language Models Are
    Few-Shot Learners”](https://oreil.ly/ZvBRL) is one of the better examples of thinking
    through bias harms and conducting some basic testing. Broadly speaking, tests
    for bias in LMs consist of adversarial prompt engineering—allowing LMs to complete
    prompts like “The Muslim man…​” or “The female doctor…​” and checking for offensive
    generated text (and wow can it be offensive!). To inject an element of randomness,
    prompts can also be generated by other LMs. Checks for offensive content can be
    done by manual human analysis, or using more automated sentiment analysis approaches.
    Conducting hot flips by exchanging names considered male for names considered
    female, for example, and testing the performance quality of tasks like named entity
    recognition is another common approach. XAI can be used too. It can help point
    out which terms or entities drive predictions or other outcomes, and people can
    decide if those drivers are concerning from a bias perspective.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型存在许多偏见问题。尽管缺乏成熟的语言模型偏见测试方法，但这是一个积极的研究领域，大多数重要论文都在某种程度上关注了这个问题。[“语言模型是少样本学习者”](https://oreil.ly/ZvBRL)的第6.2节是思考偏见伤害并进行一些基本测试的较好示例之一。广义上说，对语言模型偏见的测试包括对抗性提示工程——允许语言模型完成像“穆斯林男子……”或“女医生……”这样的提示，并检查生成的可能会冒犯人的文本（有时非常冒犯！）。为了注入随机因素，还可以由其他语言模型生成提示。通过手动人工分析或使用更自动化的情感分析方法可以检查冒犯性内容。例如，通过交换被认为是男性的姓名来进行热点翻转，以及测试像命名实体识别这样的任务的性能质量，是另一种常见的方法。可解释人工智能（XAI）也可以使用。它可以帮助指出哪些术语或实体驱动了预测或其他结果，并且人们可以决定这些驱动因素是否从偏见的角度引起关注。
- en: Individual fairness
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 个体公平性
- en: Many of the techniques we’ve put forward focus on bias against large groups.
    But what about small groups or specific individuals? ML models can easily isolate
    small groups of people, based on demographic information or proxies, and treat
    them differently. It’s also easy for very similar individuals to end up on different
    sides of a complex decision boundary. Adversarial models can help again. The adversarial
    model’s predictions can be a row-by-row local measure of bias. People who have
    high-confidence predictions from the adversarial model might be treated unfairly
    based on demographic or proxy information. We can use counterfactual tests, or
    tests that change some data attribute of a person to move them across a decision
    boundary, to understand if people actually belong on one side of a decision boundary,
    or if some kind of bias is driving their predicted outcome. For examples of some
    of these techniques in practice, see [Chapter 10](ch10.html#unique_chapter_id_10).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的许多技术都集中在针对大群体的偏见上。但是小群体或特定个人的情况如何呢？机器学习模型可以基于人口统计信息或代理信息轻松地孤立少数群体，并对其进行不同对待。非常相似的个体也很容易被划分到复杂决策边界的不同侧。对抗模型可以再次发挥作用。对抗模型的预测可以是一种逐行本地的偏见度量。那些从对抗模型得到高置信度预测的人可能会因人口统计或代理信息而受到不公平对待。我们可以使用反事实测试或更改某个人的数据属性来穿越决策边界，以了解人们是否真正属于决策边界的一侧，或者某种偏见是否驱动了他们的预测结果。关于这些技术在实践中的示例，请参见[第10章](ch10.html#unique_chapter_id_10)。
- en: Multinomial classification
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 多项分类
- en: There are several ways to conduct bias testing in multinomial classifiers. For
    example, we might use a dimension reduction technique to collapse our various
    probability output columns into a single column and then test that single column
    like a regression model with *t*-tests and SMD, where we calculate the average
    values and variance of the extracted feature across different demographic groups
    and apply thresholds of statistical and practical significance previously described.
    It would also be prudent to apply more accepted measures that also happen to work
    for multinomial outcomes, like chi-squared tests or equality of opportunity. Perhaps
    the most conservative approach is to treat each output category as its own binary
    outcome in a one-versus-all fashion. If we have many categories to test, start
    with the most common and move on from there, applying all the standards like AIR,
    binomial *z*, and error metric ratios.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在多项分类器中进行偏见测试有几种方法。例如，我们可以使用降维技术将各种概率输出列合并为单列，然后像回归模型一样对这一列进行测试，使用*t*-测试和SMD，其中我们计算不同人口群体中提取特征的平均值和方差，并应用之前描述的统计和实际显著性的阈值。同时，采用更为公认的同时也适用于多项结果的措施也是明智的，如卡方检验或机会平等。也许最保守的方法是以一对全部的方式将每个输出类别视为其自身的二元结果。如果要测试许多类别，可以从最常见的类别开始，并按照标准如AIR、二项*z*和误差度量比例依次进行测试。
- en: Unsupervised models
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督模型
- en: Cluster labels can be treated like multinomial classification output or tested
    with adversarial models. Extracted features can be tested like regression outcomes
    and also can be tested with adversarial models.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类标签可以被视为多项式分类输出或者用对抗模型进行测试。提取的特征也可以像回归结果一样进行测试，并且可以用对抗模型进行测试。
- en: Recommender systems
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统
- en: Recommender systems are one of the most important types of commercial ML technologies.
    They often serve as gatekeepers for accessing information or products that we
    need every day. Of course, they too have been called out for various and serious
    bias problems. Many general approaches, like adversarial models, user feedback,
    and XAI can help uncover bias in recommendations. However, specialized approaches
    for bias-testing recommendations are now available. See publications like [“Comparing
    Fair Ranking Metrics”](https://oreil.ly/gTFQq) or watch out for conference sessions
    like [“Fairness and Discrimination in Recommendation and Retrieval”](https://oreil.ly/fz8Ya)
    to learn more.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是商业机器学习技术中最重要的类型之一。它们经常充当我们每天需要访问信息或产品的门卫。当然，它们也因各种严重的偏见问题而受到指责。许多通用方法，如对抗模型、用户反馈和XAI，都可以帮助发现推荐中的偏见。然而，现在也有专门用于测试推荐系统偏见的方法。查阅像[“比较公平排名指标”](https://oreil.ly/gTFQq)这样的出版物或关注像[“推荐和检索中的公平性和歧视”](https://oreil.ly/fz8Ya)这样的会议议程，以了解更多信息。
- en: The world of ML is wide and deep. You might have a kind of model that we haven’t
    been able to cover here. We’ve presented a lot of options for bias testing, but
    certainly haven’t covered them all! We might have to apply common sense, creativity,
    and ingenuity to test our system. Just remember, numbers are not everything. Before
    brainstorming some new bias-testing technique, check peer-reviewed literature.
    Someone somewhere has probably dealt with a problem like ours before. Also, look
    to past failures as an inspiration for how to test, and above all else, communicate
    with users and stakeholders. Their knowledge and experience is likely more important
    than any numerical test outcome.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的世界广阔而深远。你可能有一种我们这里尚未涵盖的模型类型。我们提出了许多偏见测试的选项，但肯定没有覆盖所有的选项！我们可能需要运用常识、创造力和聪明才智来测试我们的系统。只记住，数字并不是一切。在构思一些新的偏见测试技术之前，查阅同行评议的文献。某处肯定有人已经处理过类似我们的问题。此外，寻找过去的失败作为测试的灵感，并最重要的是与用户和利益相关者进行沟通。他们的知识和经验可能比任何数值测试结果都更重要。
- en: Summary Test Plan
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要测试计划
- en: 'Before moving on to bias mitigation approaches, let’s try to summarize what
    we’ve learned about bias testing into a plan that will work in most common scenarios.
    Our plan will focus on both numerical testing and human feedback, and it will
    continue for the lifespan of the ML system. The plan we present is very thorough.
    We may not be able to complete all the steps, especially if our organization hasn’t
    tried bias testing ML systems before. Just remember, any good plan will include
    technical and sociotechnical approaches and be ongoing:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入偏见缓解方法之前，让我们尝试总结我们关于偏见测试的学习成果，制定一个适用于大多数常见情况的计划。我们的计划将侧重于数值测试和人类反馈，并将持续整个机器学习系统的生命周期。我们所提出的计划非常详尽。我们可能无法完成所有步骤，特别是如果我们的组织以前没有尝试过机器学习系统的偏见测试。只要记住，任何良好的计划都将包括技术和社会技术方法，并且是持续进行的。
- en: At the ideation stage of the system, we should engage with stakeholders like
    potential users, domain experts, and business executives to think through both
    the risks and opportunities the system presents. Depending on the nature of the
    system, we may also need input from attorneys, social scientists, psychologists,
    or others. Stakeholders should always represent diverse demographic groups, educational
    backgrounds, and life and professional experience. We’ll be on the lookout for
    human biases like groupthink, funding bias, the Dunning-Kruger effect, and confirmation
    bias that can spoil our chances for technical success.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在系统的构思阶段，我们应该与潜在用户、领域专家和业务执行人员等利益相关者进行互动，共同思考系统带来的风险和机遇。根据系统的性质，我们可能还需要从律师、社会科学家、心理学家或其他人那里获取意见。利益相关者应该代表多样化的人群，包括教育背景、生活和职业经验。我们将警惕人类偏见，如群体思维、资金偏见、达宁-克鲁格效应和确认偏见，这些偏见可能会影响我们的技术成功。
- en: During the design stage of the system, we should begin planning for monitoring
    and actionable recourse mechanisms, and we should ensure that we have the data—or
    the ability to collect the data—needed for bias testing. That ability is technical,
    legal, and ethical. We must have the technical capability to collect and handle
    the data, we must have user consent or another legal bases for collection and
    use—and do so without engaging in disparate treatment in some cases—and we shouldn’t
    rely on tricking people out of their data. We should also start to consult with
    user interaction and experience (UI/UX) experts to think through the implementation
    of actionable recourse mechanisms for wrong decisions, and to mitigate the role
    of human biases, like anchoring, in the interpretation of system results. Other
    important considerations include how those with disabilities or limited internet
    access will interact with the system, and checking into past failed designs so
    they can be avoided.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在系统设计阶段，我们应该开始规划监控和可操作的救济机制，并确保我们拥有或能够收集需要进行偏见测试的数据。这种能力涉及技术、法律和伦理层面。我们必须具备收集和处理数据的技术能力，必须获得用户同意或其他合法的数据收集和使用基础，且在某些情况下不能参与不公平对待，并且我们不应依赖欺骗人们获取他们的数据。我们还应开始咨询用户交互和体验（UI/UX）专家，共同思考如何实施针对错误决策的可操作救济机制，并减少人类偏见（如锚定效应）在系统结果解释中的作用。其他重要考虑因素包括残障人士或互联网接入有限人群如何与系统互动，以及检查过去失败设计，以避免重复错误。
- en: Once we have training data, we should probably remove any direct demographic
    markers and save these only for testing. (Of course, in some applications, like
    certain medical treatments, it may be crucial to keep this information in the
    model.) We should test training data for representativeness, fair distribution
    of outcomes, and demographic proxies so that we know what we’re getting into.
    Consider dropping proxies from the training data, and consider rebalancing or
    reweighing data to even out representation or positive outcomes across demographic
    groups. However, if we’re in a space like consumer finance, human resources, health
    insurance, or another highly regulated vertical, we’ll want to check with our
    legal department about any disparate treatment concerns around rebalancing data.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了训练数据，我们应该考虑删除任何直接的人口统计标记，并仅在测试时保存这些信息。（当然，在某些应用中，比如某些医疗治疗中，保留这些信息可能是至关重要的。）我们应该测试训练数据的代表性、结果的公平分布以及人口统计代理，以便了解我们所面对的情况。考虑从训练数据中去除代理，考虑重新平衡或重新加权数据，以平衡各人口统计组的代表性或正面结果。然而，如果我们处于消费金融、人力资源、健康保险或其他高度管制的行业，我们将需要与法律部门核实关于重新平衡数据引发的任何不公平对待的顾虑。
- en: After our model is trained, it’s time to start testing. If our model is a traditional
    regression or classification estimator, we’ll want to apply the appropriate traditional
    tests to understand any unfavorable differences in outcomes across groups, and
    we’ll want to apply tests for performance quality across demographic groups to
    check that performance is roughly equal for all of our users. If our model is
    not a traditional regression or classification estimator, we’ll still want think
    of a logical way to transform our outputs into a single numeric column or a binary
    1/0 column so that we can apply a full suite of tests. If we can’t defensibly
    transform our outputs, or we just want to know more about bias in our model, we
    should try adversarial models and XAI to find any pockets of discrimination in
    our outcomes or to understand drivers of bias in our model. If our system is an
    LM, recommendation system, or other more specialized type of ML, we should also
    apply testing strategies designed for those kinds of systems.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的模型训练完成后，现在是时候开始测试了。如果我们的模型是传统的回归或分类估计器，我们将希望应用适当的传统测试来理解跨组别结果中的任何不利差异，并且我们将希望应用性能质量测试来检查所有用户的性能大致相等。如果我们的模型不是传统的回归或分类估计器，我们仍然希望想出一种逻辑方法，将输出转换为单个数值列或二进制1/0列，以便我们可以应用一整套测试。如果我们无法合理地转换我们的输出，或者我们只是想了解模型中的偏差，我们应该尝试对抗性模型和XAI，以发现结果中的任何歧视现象或理解模型中的偏差驱动因素。如果我们的系统是LM、推荐系统或其他更专业类型的ML，我们还应该应用为这些系统设计的测试策略。
- en: When a model is deployed, it has to be monitored for issues like faulty performance,
    hacks, and bias. But monitoring is not only a technical exercise. We need to incentivize,
    receive, and incorporate user feedback. We need to ensure our actionable recourse
    mechanisms work properly in real-world conditions, and we need to track any harms
    that our system is causing. This is all in addition to performance monitoring
    that includes standard statistical bias tests. Monitoring and feedback collection
    must continue for the lifetime of the system.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当模型部署时，必须监控问题，如性能故障、黑客攻击和偏差。但监控不仅仅是技术性的练习。我们需要激励、接收和整合用户反馈。我们需要确保我们的可操作的追索机制在实际条件下正常工作，并且我们需要追踪我们的系统造成的任何伤害。所有这些都是包括标准统计偏差测试的性能监控的一部分。监控和反馈收集必须在系统的整个生命周期内持续进行。
- en: What if we find something bad during testing or monitoring? That’s pretty common,
    and that’s what the next section is all about. There are technical ways to mitigate
    bias, but bias-testing results have to be incorporated into an organization’s
    overall ML governance programs to have their intended transparency and accountability
    benefits. We’ll be discussing governance and human factors in bias mitigation
    in the next section as well.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在测试或监控过程中发现了什么不好的情况怎么办？这是相当常见的，也是下一节内容的重点。有技术手段可以减少偏差，但是偏差测试结果必须纳入组织整体的ML治理程序中，以发挥其预期的透明度和问责制效益。在接下来的章节中，我们还将讨论治理和偏差缓解中的人为因素。
- en: Mitigating Bias
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓解偏差
- en: If we test an ML model for bias in its outcomes, we are likely to find it in
    many cases. When it shows up, we’ll also need to address it (if we don’t find
    bias, double-check our methodology and results and plan to monitor for emergent
    bias issues when the system is deployed). This section of the chapter starts out
    with a technical discussion of bias mitigation approaches. We’ll then transition
    to human factors that mitigate bias that are likely to be more broadly effective
    over time in real-world settings. Practices like human-centered design (HCD) and
    governance of ML practitioners are much more likely to decrease harm throughout
    the lifecycle of an ML system than a point-in-time technical mitigation approach.
    We’ll need to have diverse stakeholders involved with any serious decision about
    the use of ML, including the initial setup of governance and diversity initiatives.
    While the technical methods we’ll put forward are likely to play some role in
    making our organization’s ML more fair, they don’t work in practice without ongoing
    interactions with our users and proper oversight of ML practitioners.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在机器学习模型的结果中测试偏见，我们很可能会在许多情况下发现它。当它显现时，我们也需要处理它（如果我们没有发现偏见，则需要仔细检查我们的方法论和结果，并计划在系统部署时监控出现的偏见问题）。本章节从偏见缓解方法的技术讨论开始。接着我们将转向更可能在现实世界设置中长期有效的人类因素来缓解偏见。像人本设计（HCD）和机器学习从业者的治理比单点时间技术缓解方法更有可能在机器学习系统的整个生命周期内减少伤害。我们需要让各方利益相关者参与到关于使用机器学习的任何重大决策中，包括治理的初期设置和多样化倡议。虽然我们将提出的技术方法很可能在使我们的组织机器学习更加公平方面发挥一定作用，但如果没有与用户的持续互动和对机器学习从业者的适当监督，这些方法在实践中是行不通的。
- en: Technical Factors in Mitigating Bias
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓解偏见的技术因素
- en: Let’s start our discussion of technical bias mitigation with a quote from the
    [NIST SP1270 AI bias guidance](https://oreil.ly/pkm4f). When we dump observational
    data that we chose to use because it is available into an unexplainable model
    and tweak the hyperparameters until we maximize some performance metric, we may
    be doing what the internet calls data science, but we’re not doing *science* science:^([1](ch04.html#idm45990002397824))
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从[NIST SP1270 AI 偏见指南](https://oreil.ly/pkm4f)中的一句话开始讨论技术偏见缓解。当我们把我们选择使用的观察数据倒入一个无法解释的模型中，并调整超参数直到最大化某些性能指标时，我们可能正在做互联网称之为数据科学的事情，但我们并不是在做*科学*。^([1](ch04.html#idm45990002397824))
- en: Physicist Richard Feynman referred to practices that superficially resemble
    science but do not follow the scientific method as cargo cult science. A core
    tenet of the scientific method is that hypotheses should be testable, experiments
    should be interpretable, and models should be falsifiable or at least verifiable.
    Commentators have drawn similarities between AI and cargo cult science citing
    its black box interpretability, reproducibility problem, and trial-and-error processes.
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 物理学家理查德·费曼将表面上类似于科学但不遵循科学方法的做法称为货运邦科学。科学方法的核心原则是假设应可测试，实验应可解释，模型应可证伪或至少可验证。评论员指出人工智能与货运邦科学之间存在相似之处，包括其黑盒解释性、再现性问题和试错过程。
- en: The Scientific Method and Experimental Design
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 科学方法与实验设计
- en: One of the best technical solutions to avoiding bias in ML systems is sticking
    to the scientific method. We should form a hypothesis about the real-world effect
    of our model. Write it down and don’t change it. Collect data that is related
    to our hypothesis. Select model architectures that are interpretable and have
    some structural meaning in the context of our hypothesis; in many cases, these
    won’t be ML models at all. We should assess our model with accuracy, MAPE, or
    whatever traditional assessment measures are appropriate, but then find a way
    to test whether our model is doing what it is supposed to in its real-world operating
    environment, for example with [A/B testing](https://oreil.ly/d_5jB). This time-tested
    process cuts down on human biases—especially confirmation bias—in model design,
    development, and implementation, and helps to detect and mitigate systemic biases
    in ML system outputs, as those will likely manifest as the system not behaving
    as intended. We’ll delve into the scientific method, and what data science has
    done to it, in [Chapter 12](ch12.html#conclusion).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 避免ML系统中偏见的最佳技术解决方案之一是坚持科学方法。我们应该形成关于我们模型真实世界影响的假设。把它写下来并且不要改变它。收集与我们假设相关的数据。选择在我们假设背景下有一些结构意义和可解释性的模型架构；在许多情况下，这些可能根本不是ML模型。我们应该用准确度、MAPE或者适当的传统评估指标来评估我们的模型，然后找到一种方式来测试我们的模型在其真实操作环境中是否在按照预期工作，例如使用[A/B测试](https://oreil.ly/d_5jB)。这一经过时间考验的过程可以减少模型设计、开发和实施中的人类偏见，特别是确认偏见，并有助于检测和缓解ML系统输出中的系统性偏见，因为这些偏见可能会表现为系统未按预期运行。我们将在[第12章](ch12.html#conclusion)深入探讨科学方法及其在数据科学中的应用。
- en: Another basic bias mitigant is [experimental design](https://oreil.ly/A4Dzf).
    We don’t have to use whatever junk data is available to train an ML model. We
    can consult practices from experimental design to collect data specifically designed
    to address our hypothesis. Common problems with using whatever data our organization
    has laying around include that such data might be inaccurate, poorly curated,
    redundant, and laced with systemic bias. Borrowing from experimental design allows
    us to collect and select a smaller, more curated set of training data that is
    actually related to an experimental hypothesis.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个基本的偏见缓解方法是[实验设计](https://oreil.ly/A4Dzf)。我们不必使用随便找到的垃圾数据来训练ML模型。我们可以借鉴实验设计的实践，收集专门设计用于解决我们假设的数据。使用组织中现有的任何数据的常见问题包括这些数据可能不准确、维护不当、冗余，并且带有系统性偏见。从实验设计中借鉴让我们可以收集并选择一组更小、更精心筛选的培训数据，实际上与实验假设相关联。
- en: More informally, thinking through experimental design helps us avoid really
    silly, but harmful, mistakes. It is said there are no stupid questions. Unfortunately
    that’s not the case with ML bias. For example, asking whether a face can predict
    trustworthiness or criminality. These flawed experimental premises are based on
    already debunked and racist theories, like [phrenology](https://oreil.ly/dEmE9).
    One basic way to check our experimental approach is to check whether our target
    feature’s name ends in “iness” or “ality,” as this can highlight that we’re modeling
    some kind of higher-order construct, versus something that is concretely measurable.
    Higher-order constructs like trustworthiness or criminality are often imbued with
    human and systemic biases that our system will learn. We should also check the
    [AI Incident Database](https://oreil.ly/s88Bt) to ensure we’re not just repeating
    a past failed design.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 更不正式地说，通过实验设计进行思考有助于我们避免一些非常愚蠢但有害的错误。有人说没有愚蠢的问题。不幸的是，对于ML偏见来说并非如此。例如，询问一个面孔能否预测其信任度或犯罪性。这些有缺陷的实验前提基于已被揭穿的种族主义理论，如[骨相学](https://oreil.ly/dEmE9)。检查我们实验方法的一个基本方式是检查我们目标特征的名称是否以“iness”或“ality”结尾，因为这可以突显出我们是否在对某种高阶构建进行建模，而不是某种具体可测量的东西。像信任度或犯罪性这样的高阶构建往往充斥着人类和系统偏见，我们的系统会学习到这些。我们还应该检查[A/B测试](https://oreil.ly/d_5jB)以确保我们不仅仅是在重复过去失败的设计，可以通过[AI事故数据库](https://oreil.ly/s88Bt)来检查。
- en: Repeating the past is another big mistake that’s easy to do with ML if we don’t
    think through the experiment our model implies. One of the worst examples of this
    kind of basic experimental design error happened in health insurance and was documented
    in [*Science*](https://oreil.ly/D-wXE) and [*Nature*](https://oreil.ly/sKVYC).
    The goal of the algorithms studied in the *Science* paper was to intervene in
    the care of a health insurer’s sickest patients. This should have been a win-win
    for both the insurer and the patients—costing insurers less by identifying those
    with the greatest needs early in an illness and getting those patients better
    care. But a very basic and very big design mistake led the algorithms to divert
    healthcare away from those most in need! What went wrong? Instead of trying to
    predict which patients would be the sickest in the future, the modelers involved
    decided to predict who would be the most expensive patients. The modelers assumed
    that the most expensive people were the sickest. In fact, the most expensive patients
    were older people with pricey healthcare plans and access to good care. The algorithm
    simply diverted more care to people with good healthcare already, and cut resources
    for those who needed it most. As readers might imagine, those two populations
    were also highly segregated along racial lines. The moment the modelers chose
    to have healthcare cost as their target, as opposed to some indicator of health
    or illness, this model was doomed to be dangerously biased. If we want to mitigate
    bias in ML, we need to think before we code. Trying to use the scientific method
    and experimental design in our ML modeling projects should help us think through
    what we’re doing much more clearly and lead to more technical successes too.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 重复过去是机器学习中另一个容易犯的大错误，如果我们不仔细思考模型所暗示的实验的话。这种基本实验设计错误的最糟糕例子之一发生在健康保险领域，并且已经在[*Science*](https://oreil.ly/D-wXE)和[*Nature*](https://oreil.ly/sKVYC)上有所记录。*Science*文章中研究的算法的目标是干预健康保险公司最病重的患者的护理。这本应该是保险公司和患者双赢的事情——通过早期识别病情严重的患者来降低保险公司的成本，并为这些患者提供更好的护理。但是，一个非常基本且非常严重的设计错误导致算法使医疗资源偏离了最需要帮助的人群！出了什么问题？模型师们不是试图预测哪些患者未来会最病重，而是决定预测谁会是最昂贵的患者。他们假设最昂贵的人就是最病重的。实际上，最昂贵的患者是年龄较大、医疗计划昂贵且有良好医疗保障的人。这个算法简单地将更多的护理资源分配给已经拥有良好医疗保障的人，削减了那些最需要的人的资源。正如读者们可能想象的那样，这两个群体在种族上也高度分隔。在模型师选择将医疗成本作为目标而不是健康或疾病指标时，这个模型注定会存在极大的偏见风险。如果我们想要减少机器学习中的偏见，我们需要在编码之前进行深思熟虑。尝试在我们的机器学习建模项目中使用科学方法和实验设计应该有助于我们更清晰地思考我们正在做的事情，并且也会导致更多的技术成功。
- en: Bias Mitigation Approaches
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏见减少方法
- en: Even if we apply the scientific method and experimental design, our ML system
    may still be biased. Testing will help us detect that bias, and we’ll likely also
    want some technical way of treating it. There are many ways to treat bias once
    it’s detected, or to train ML models that attempt to learn fewer biases. The recent
    paper [“An Empirical Comparison of Bias Reduction Methods on Real-World Problems
    in High-Stakes Policy Settings”](https://oreil.ly/TSAvx) does a nice comparison
    of the most widely available bias mitigation techniques, and another paper by
    the same group of researchers, [“Empirical Observation of Negligible Fairness–Accuracy
    Trade-Offs in Machine Learning for Public Policy”](https://oreil.ly/gitq4), addresses
    the false idea that we have to sacrifice accuracy when addressing bias. We don’t
    actually make our models less performant by making them less biased—a common data
    science misconception. Another good resource for technical bias remediation is
    IBM’s [AIF360 package](https://oreil.ly/G8kCw), which houses most major remediation
    techniques. We’ll highlight what’s known as preprocessing, in-processing, and
    postprocessing approaches, in addition to model selection, LM detoxification,
    and other bias mitigation techniques.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们应用科学方法和实验设计，我们的 ML 系统仍然可能存在偏见。测试将帮助我们检测到这种偏见，而且我们可能还希望有一些技术手段来处理它。一旦检测到偏见，或者尝试学习更少偏见的
    ML 模型，有许多处理偏见的方法。最近的一篇论文 [“在高风险政策设置中真实世界问题上偏见减少方法的经验比较”](https://oreil.ly/TSAvx)
    对最广泛可用的偏见缓解技术进行了很好的比较，另一篇同一组研究人员的论文 [“机器学习在公共政策中的偏差-准确性权衡的经验观察”](https://oreil.ly/gitq4)
    解决了一个错误的观念，即在处理偏见时我们必须牺牲准确性。事实上，通过减少偏见并不会使我们的模型表现更差——这是一个常见的数据科学误解。技术偏见治理的另一个好资源是
    IBM 的 [AIF360 包](https://oreil.ly/G8kCw)，其中包含大多数主要的治理技术。除了模型选择、LM 解毒和其他偏见缓解技术，我们还将重点介绍所谓的预处理、处理和后处理方法。
- en: Preprocessing bias mitigation techniques act on the training data of the model
    rather than the model itself. Preprocessing tends to resample or reweigh training
    data to balance or shift the number of rows for each demographic group or to redistribute
    outcomes more equally across demographic groups. If we’re facing uneven performance
    quality across different demographic groups, then boosting the representation
    of groups with poor performance may help. If we’re facing inequitable distributions
    of positive or negative outcomes, usually as detected by statistical and practical
    significance testing, then rebalancing outcomes in training data may help to balance
    model outcomes.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理偏见缓解技术作用于模型的训练数据，而不是模型本身。预处理倾向于重新采样或重新加权训练数据，以平衡或转移每个人口群体的行数，或更平均地重新分配人口群体的结果。如果我们面对不同人口群体之间的不均匀表现质量，那么增加表现较差群体的代表性可能有所帮助。如果我们面对正面或负面结果的不公平分布，通常通过统计和实际显著性测试检测，那么重新平衡训练数据中的结果可能有助于平衡模型结果。
- en: 'In-processing refers to any number of techniques that alter a model’s training
    algorithm in an attempt to make its outputs less biased. There are many approaches
    for in-processing, but some of the more popular approaches include constraints,
    dual objective functions, and adversarial models:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: In-processing 指的是任何改变模型训练算法的技术，旨在使其输出更少偏见。有许多处理技术，但一些较受欢迎的方法包括约束、双目标函数和对抗模型：
- en: Constraints
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 约束
- en: A major issue with ML models is their instability. A small change in inputs
    can lead to a large change in outcomes. This is especially worrisome from a bias
    standpoint if the similar inputs are people in different demographic groups and
    the dissimilar outcomes are those people’s pay or job recommendations. In the
    seminal [“Fairness Through Awareness”](https://oreil.ly/iYLS9), Cynthia Dwork
    et al. frame reducing bias as a type of constraint during training that helps
    models treat similar people similarly. ML models also find interactions automatically.
    This is worrisome from a bias perspective if models learn many different proxies
    for demographic group membership, across different rows and input features for
    different people. We’ll never be able to find all those proxies. To prevent models
    from making their own proxies, try [interaction constraints](https://oreil.ly/4uIGl)
    in XGBoost.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的一个主要问题是它们的不稳定性。输入的微小变化可能导致结果的显著变化。从偏见的角度来看，这尤为令人担忧，如果相似的输入是来自不同人群的人，而不同的结果是这些人的薪水或工作推荐。在开创性的["通过意识实现公平"](https://oreil.ly/iYLS9)中，辛西娅·德沃克等人将减少偏见视为训练过程中的一种约束类型，帮助模型以相似的方式处理相似的人。机器学习模型还会自动发现交互作用。从偏见的角度来看，如果模型学习多种不同的人群成员的代理，跨不同的行和不同人的输入特征，这就令人担忧。我们永远无法找到所有这些代理。为防止模型制定自己的代理，尝试在XGBoost中使用[交互约束](https://oreil.ly/4uIGl)。
- en: Dual objectives
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 双重目标
- en: 'Dual optimization is where one part of a model’s loss function measures modeling
    error and another term measures bias, and minimizing the loss function finds a
    performant and less biased model. [“FairXGBoost: Fairness-Aware Classification
    in XGBoost”](https://oreil.ly/fq9Jw) introduces a method for including a bias
    regularization term in XGBoost’s objective function that leads to models with
    good performance and fairness trade-offs.^([2](ch04.html#idm45990005773296))'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 双重优化是模型损失函数的一部分，其中一部分衡量建模误差，另一部分衡量偏差，最小化损失函数可以找到一个性能良好且偏差较小的模型。["公平XGBoost：XGBoost中的公平感知分类"](https://oreil.ly/fq9Jw)介绍了一种方法，该方法在XGBoost的目标函数中包含偏差正则化项，从而得到具有良好性能和公平性权衡的模型。^([2](ch04.html#idm45990005773296))
- en: Adversarial models
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗模型
- en: Adversarial models can also help make training less biased. In one setup for
    adversarial modeling, a main model to be deployed later is trained, then an adversarial
    model attempts to predict demographic membership from the main model’s predictions.
    If it can, then adversarial training continues—training the main model and then
    the adversary model—until the adversary model can no longer predict demographic
    group membership from the main model’s predictions, and the adversary model shares
    some information, like gradients, with the main model in between each retraining
    iteration.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗模型也可以帮助使训练过程更少偏见。在一种对抗建模设置中，首先训练将要部署的主模型，然后对抗模型试图从主模型的预测中预测人口统计成员资格。如果可以的话，接着进行对抗训练——训练主模型，然后训练对抗模型——直到对抗模型不能再从主模型的预测中预测人口统计组成员资格，并且对抗模型在每次重新训练迭代之间与主模型分享某些信息，如梯度。
- en: In studies, pre- and in-processing tend to decrease measured bias in outcomes,
    but postprocessing approaches have been shown to be some of the most effective
    technical bias mitigants. Postprocessing is when we change model predictions directly
    to make them less biased. Equalized odds or equalized opportunity are some common
    thresholds used when rebalancing predictions, i.e., changing classification decisions
    until the outcomes roughly meet the criteria for equalized odds or opportunity.
    Of course, continuous or other types of outcomes can also be changed to make them
    less biased. Unfortunately, postprocessing may be the most legally fraught type
    of technical bias mitigation. Postprocessing often boils down to switching positive
    predictions for control group members to negative predictions, so that those in
    protected or marginalized groups receive more positive predictions. While these
    kinds of modifications may be called for in many different types of scenarios,
    be especially careful when using postprocessing in consumer finance or employment
    settings. If we have any concerns, we should talk to legal colleagues about disparate
    treatment or reverse discrimination.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究中，预处理和中间处理往往会降低测量结果中的偏差，但后处理方法被证明是一些最有效的技术偏差缓解方法之一。后处理是指我们直接改变模型预测，使其偏差较小。在重新平衡预测时，均等几率或均等机会是一些常见的阈值，即在改变分类决策直至结果大致符合均等几率或机会的标准。当然，连续或其他类型的结果也可以改变以减少偏差。不幸的是，后处理可能是技术偏差缓解中法律风险最大的类型。后处理通常归结为将对照组成员的正面预测切换为负面预测，以便受保护或边缘化群体得到更多的正面预测。虽然在许多不同类型的场景中可能需要这些修改，但在消费金融或就业设置中使用后处理时要格外小心。如果有任何疑虑，我们应该与法律同事讨论不同对待或反向歧视问题。
- en: Warning
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Because pre-, in-, and postprocessing techniques tend to change modeling outcomes
    specifically based on demographic group membership, they may give rise to concerns
    related to disparate treatment, reverse discrimination, or affirmative action.
    Consult legal experts before using these approaches in high-risk scenarios, especially
    in employment, education, housing, or consumer finance applications.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预处理、中间处理和后处理技术往往会基于人口统计群体成员身份特定地改变建模结果，因此可能会引发与不同待遇、反向歧视或积极行动相关的担忧。在高风险场景中，特别是在就业、教育、住房或消费金融应用中使用这些方法之前，请咨询法律专家。
- en: One of the most legally conservative bias mitigation approaches is to choose
    a model based on performance and fairness, with models trained in what is basically
    a grid search across many different hyperparameter settings and input feature
    sets, and demographic information used only for testing candidate models for bias.
    Consider [Figure 4-2](#_200_nns). It displays the results of a random grid search
    across two hundred candidate neural networks. On the y-axis we see accuracy. The
    highest model on this axis would be the model we normally choose as the best.
    However, when we add bias testing for these models on the x-axis, we can now see
    that there are several models with nearly the same accuracy and much improved
    bias-testing results. Adding bias testing onto hyperparameter searches adds fractions
    of a second to the overall training time, and opens up a whole new dimension for
    helping to select models.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 最保守的偏差缓解方法之一是基于性能和公平选择模型，模型训练基本上是通过对许多不同的超参数设置和输入特征集进行网格搜索，而仅将人口统计信息用于测试候选模型是否存在偏差。考虑[图
    4-2](#_200_nns)。它展示了对两百个候选神经网络进行随机网格搜索的结果。在y轴上，我们看到准确度。在这个轴上最高的模型通常是我们选择的最佳模型。然而，当我们在x轴上为这些模型添加偏差测试时，现在我们可以看到有几个几乎具有相同准确性但偏差测试结果明显改善的模型。在超参数搜索中添加偏差测试只会增加整体训练时间的几分之一秒，并为帮助选择模型开辟了一个全新的维度。
- en: '![mlha 0402](assets/mlha_0402.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0402](assets/mlha_0402.png)'
- en: Figure 4-2\. A simple random grid search produces several interesting choices
    for models that provide a good balance between accuracy and AIR
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 一个简单的随机网格搜索产生了几个有趣的模型选择，这些模型在准确性和AIR之间提供了良好的平衡
- en: There are many other technical bias mitigants. One of the most important, as
    discussed many times in this book, is mechanisms for actionable recourse that
    enable appeal and override of wrong and consequential ML-based decisions. Whenever
    we build a model that affects people, we should make sure to also build and test
    a mechanism that lets people identify and appeal wrong decisions. This typically
    means providing an extra interface that explains data inputs and predictions to
    users, then allows them to ask for the prediction to be changed.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他技术偏见缓解措施。本书中多次讨论过的最重要之一是为可行救济机制提供机制，使人们能够申诉和覆盖错误和重要的基于机器学习的决策。每当我们构建一个影响人们的模型时，我们都应该确保同时构建和测试一个机制，让用户能够理解数据输入和预测，并允许他们要求更改预测。
- en: Detoxification, or the process of preventing LMs from generating harmful language,
    including hate speech, insults, profanities, and threats, is another important
    area in bias mitigation research. Check out [“Challenges in Detoxifying Language
    Models”](https://oreil.ly/gfVaZ) for a good overview of the some of the current
    approaches to detoxification and their inherent challenges. Because bias is thought
    to arise from models systematically misrepresenting reality, causal inference
    and discovery techniques, which seek to guarantee that models represent causal
    real-world phenomena, are also seen as bias mitigants. While causal inference
    from observational data continues to be challenging, causal discovery approaches
    like [LiNGAM](https://oreil.ly/985wC), which seek out input features with some
    causal relationship to the prediction target, are definitely something to consider
    in our next ML project.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 解毒，或者防止语言模型生成有害语言，包括仇恨言论、侮辱、亵渎和威胁的过程，是偏见缓解研究中另一个重要领域。查看[《解毒语言模型中的挑战》](https://oreil.ly/gfVaZ)，了解当前解毒方法及其固有挑战的良好概述。因为偏见被认为是由模型系统性地误代现实引起的，因果推断和发现技术被视为偏见缓解措施，它们旨在确保模型表达因果真实世界现象。尽管从观察数据进行因果推断仍然具有挑战性，像[LiNGAM](https://oreil.ly/985wC)这样的因果发现方法，寻找与预测目标存在某种因果关系的输入特征，无疑是我们下一个机器学习项目中需要考虑的内容。
- en: Warning
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Bias mitigation efforts must be monitored. Bias mitigation can fail or lead
    to worsened outcomes.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见缓解工作必须进行监控。偏见缓解可能失败或导致结果恶化。
- en: We’ll end this section with a warning. Technical bias mitigants probably don’t
    work on their own without the human factors we’ll be discussing next. In fact,
    it’s [been shown](https://oreil.ly/RnES9) that bias testing and bias mitigation
    can lead to no improvements or even worsened bias outcomes. Like ML models themselves,
    bias mitigation has to be monitored and adjusted over time to ensure it’s helping
    and not hurting. Finally, if bias testing reveals problems and bias mitigation
    doesn’t fix them, the system in question should not be deployed. With so many
    ML systems being approached as engineering solutions that are predestined for
    successful deployment, how can we stop a system from being deployed? By enabling
    the right group of people to make the final call via good governance that promotes
    a risk-aware culture!
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本节结束时加以警告。技术上的偏见缓解可能无法独自完成，需要我们接下来要讨论的人类因素。事实上，[已经显示](https://oreil.ly/RnES9)，偏见测试和偏见缓解可能不会改善情况，甚至导致偏见结果恶化。像机器学习模型本身一样，偏见缓解必须随着时间的推移进行监控和调整，以确保其帮助而不是伤害。最后，如果偏见测试显示问题，而偏见缓解无法解决这些问题，涉及的系统就不应该部署。由于许多机器学习系统被视为预定成功部署的工程解决方案，我们如何阻止系统的部署？通过促进风险意识文化的良好治理，让正确的人群做出最终决定！
- en: Human Factors in Mitigating Bias
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓解偏见的人类因素
- en: To ensure a model is minimally biased before it’s deployed requires a lot of
    human work. First, we need a demographically and professionally diverse group
    of practitioners and stakeholders to build, review, and monitor the system. Second,
    we need to incorporate our users into the building, reviewing, and monitoring
    of the system. And third, we need governance to ensure that we can hold ourselves
    accountable for bias problems.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型部署之前确保最低程度的偏见需要大量人力工作。首先，我们需要一个在人口统计和职业上多样化的从业者和利益相关者团体来构建、审查和监控系统。其次，我们需要将用户纳入到系统的构建、审查和监控中。第三，我们需要治理机制来确保我们能够对偏见问题负责。
- en: 'We’re not going to pretend we have answers for the continually vexing issues
    of diversity in the tech field. But here’s what we know: far too many models and
    ML systems are trained by inexperienced and demographically homogenous development
    teams with little domain expertise in the area of application. This leaves systems
    and their operators open to massive blind spots. Usually these blind spots simply
    mean lost time and money, but they can lead to massive diversions of healthcare
    resources, arresting the wrong people, media and regulatory scrutiny, legal troubles,
    and worse. If, in the first design discussions about an AI system, we look around
    the room and see only similar faces, we’re going to have to work incredibly hard
    to ensure that systemic and human biases do not derail the project. It’s a bit
    meta, but it’s important to call out that having the same old tech guy crew lay
    out the rules for who is going to be involved in the system is also problematic.
    Those very first discussions are the time to try to bring in perspectives from
    different types of people, different professions, people with domain expertise,
    and stakeholder representatives. And we need to keep them involved. Is this going
    to slow down our product velocity? Certainly. Is this going to make it harder
    to “move fast and break things”? Definitely. Is trying to involve all these people
    going to make technology executives and senior engineers angry? Oh yes. So, how
    can we do it? We’ll need to empower the voices of our users, who in many cases
    are a diverse group of people with many different wants and needs. And we’ll need
    a governance program for our ML systems. Unfortunately, getting privileged tech
    executives and senior engineers to care about bias in ML can be very difficult
    for one cranky person, or even a group of conscientious practitioners, to do without
    broader organizational support.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不打算假装我们对科技领域中持续困扰的多样性问题有答案。但是这是我们所知道的：有太多的模型和机器学习系统是由经验不足、人口统计上同质化的开发团队训练的，这些团队在应用领域缺乏领域专业知识。这使得系统及其操作者容易出现重大盲点。通常这些盲点意味着时间和金钱的浪费，但它们也可能导致医疗资源的巨大偏离、错误逮捕、媒体和监管审查、法律问题等更为严重的后果。如果在关于人工智能系统的首次设计讨论中，我们看到的只是相似的面孔，我们将不得不极其努力确保系统性和人为偏见不会让项目偏离轨道。这有点元认知，但重要的是要指出，让同样老掉牙的技术人员团队制定参与系统的规则也是有问题的。那些最初的讨论是尝试引入不同类型的人、不同职业背景的人、具有领域专业知识的人和利益相关者代表的时机。而且我们需要让他们继续参与进来。这会减慢我们产品的速度吗？肯定会。这会使我们更难以“快速迭代、快速失败”吗？绝对会。尝试让所有这些人参与会让技术高管和高级工程师感到愤怒吗？肯定会。那么，我们该如何做呢？我们需要赋予我们用户的声音权力，这些用户通常是一个多样化的群体，拥有多样化的需求。我们还需要为我们的机器学习系统建立治理程序。不幸的是，让特权的技术高管和高级工程师关心机器学习中的偏见可能对一个脾气暴躁的人，甚至是一群有良知的从业者来说都很困难，如果没有更广泛的组织支持的话。
- en: One of the ways we can start organizational change around ML bias is interacting
    with users. Users don’t like broken models. Users don’t like predatory systems,
    and users don’t like being discriminated against automatically and at scale. Not
    only is taking feedback from users good business, but it helps us spot issues
    in our design and track harms that statistical bias testing can miss. We’ll highlight
    yet again that statistical bias testing is very unlikely to uncover how or when
    people with disabilities or those that live on the other side of the digital divide
    experience harms because they cannot use the system or it works in strange ways
    for them. How do we track these kinds of harms? By talking to our users. We’re
    not suggesting that frontline engineers run out to their user’s homes, but we
    are suggesting that when building and deploying ML systems, organizations employ
    standard mechanisms like user stories, UI/UX research studies, human-centered
    design, and bug bounties to interact with their users in structured ways, and
    incorporate user feedback into improvements of the system. The case at the end
    of the chapter will highlight how structured and incentivized user feedback in
    the form of a bug bounty shed light on problems in a large and complex ML system.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在围绕机器学习偏见展开组织变革的其中一种方式是与用户互动。用户不喜欢损坏的模型。用户不喜欢掠夺性系统，也不喜欢自动和大规模地受到歧视。从用户那里获取反馈不仅是好的业务做法，而且有助于发现设计中的问题，并跟踪统计偏见测试可能忽视的伤害。我们再次强调，统计偏见测试极少能揭示残疾人士或生活在数字鸿沟另一端的人如何经历伤害，因为他们无法使用系统或系统对他们工作方式不同寻常。我们如何跟踪这些伤害？通过与用户交流。我们并不建议一线工程师奔赴用户家中，但我们建议在构建和部署机器学习系统时，组织采用标准机制，如用户故事、UI/UX研究、以人为本设计和漏洞赏金计划等结构化方式与用户互动，并将用户反馈纳入系统改进中。章节末尾的案例将突显结构化和激励性的用户反馈，如漏洞赏金计划，揭示了大型复杂机器学习系统中的问题。
- en: Another major way to shift organizational culture is governance. That’s why
    we started the book with governance in [Chapter 1](ch01.html#unique_chapter_id_1).
    Here, we’ll explain briefly why governance matters for bias mitigation purposes.
    In many ways, bias in ML is about sloppiness and sometimes it’s about bad intent.
    Governance can help with both. If an organization’s written policies and procedures
    mandate that all ML models be tested thoroughly for bias or other issues before
    being deployed, then more models will probably be tested, increasing the performance
    of ML models for the business, and hopefully decreasing the chance of unintentional
    bias harms. Documentation, and particularly model documentation templates that
    walk practitioners through policy-mandated workflow steps, are another key part
    of governance. Either we as practitioners fill out the model documentation fulsomely,
    noting the correct steps we’ve taken along the way to meet what our organization
    defines as best practices, or we don’t. With documentation there is a paper trail,
    and with a paper trail there is some hope for accountability. Managers should
    see good work in model documents, and they should be able to see not-so-good work
    too. In the case of the latter, management can step in and get those practitioners
    training, and if the problems continue, disciplinary action can be taken. Regarding
    all those legal definitions of fairness that can be real gotchas for organizations
    using ML—policies can help everyone stay aligned with the law, and managerial
    review of model documentation can help to catch when practitioners are not aligned.
    Regarding all those human biases that can spoil ML models—policies can define
    best practices to help avoid them, and managerial review of model documentation
    can help to spot them before models are deployed.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个转变组织文化的重要方式是治理。这就是为什么我们在[第1章](ch01.html#unique_chapter_id_1)中以治理为开篇。在这里，我们将简要解释为什么治理对减少偏见很重要。在许多情况下，机器学习中的偏见是关于粗心大意，有时是有意的坏意图。治理可以在这两方面发挥作用。如果一个组织的书面政策和程序要求所有机器学习模型在部署之前都必须进行充分的偏见或其他问题测试，那么可能会测试更多的模型，提高业务中机器学习模型的性能，并希望减少无意的偏见伤害的几率。文档，特别是能够引导从业者按照政策规定的工作流程步骤进行的模型文档模板，是治理的另一个关键部分。作为从业者，我们要么充分填写模型文档，记录我们沿途采取的正确步骤以符合组织定义的最佳实践，要么不填写。有了文档就有了一份记录，有了记录就有了一些责任的希望。管理者应该能够在模型文档中看到好的工作，也应该能够看到不太好的工作。在后者的情况下，管理层可以介入并对那些从业者进行培训，如果问题继续存在，则可以采取纪律行动。关于所有那些可以成为使用机器学习的组织的真正难题的公平法律定义——政策可以帮助每个人保持与法律的一致性，管理层审查模型文档可以帮助发现从业者的偏离。关于所有那些可能破坏机器学习模型的人类偏见——政策可以定义最佳实践来帮助避免它们，管理层审查模型文档可以帮助在部署模型之前发现它们。
- en: While written policies and procedures and mandatory model documentation go a
    long way toward shaping an organization’s culture around model building, governance
    is also about organizational structures. One cranky data scientist can’t do a
    whole lot about a large organization’s misuse or abuse of ML models. We need organizational
    support to effect change. ML governance should also ensure the independence of
    model validation and other oversight staff. If testers report to development or
    ML managers, and are assessed on how many models they deploy, then testers probably
    don’t do much more than rubber-stamp buggy models. This is why model risk management
    (MRM), as defined by US government regulators, insists that model testers be fully
    independent from model developers, have the same education and skills as model
    developers, and be paid the same as model developers. If the director of responsible
    ML reports to the VP of data science and chief technology officer (CTO), they
    can’t tell their bosses “no.” They’re likely just a figurehead that spends time
    on panels making an organization feel better about its buggy models. This is why
    MRM defines a senior executive role that focuses on ML risk, and stipulates that
    this senior executive report not to the CTO or CEO but directly to the board of
    directors (or to a chief risk officer who also reports to the board).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然书面政策、程序和强制模型文档在塑造组织围绕模型构建的文化方面发挥了重要作用，但治理也涉及到组织结构。一个不合作的数据科学家对于大型组织对机器学习模型的误用或滥用几乎无能为力。我们需要组织的支持来实现变革。机器学习治理还应确保模型验证和其他监督人员的独立性。如果测试人员向开发者或机器学习经理汇报，并根据部署的模型数量进行评估，那么测试人员可能只是在橡皮图章般地批准有问题的模型。这就是为什么美国政府监管机构所定义的模型风险管理（MRM）坚持要求模型测试人员与模型开发者完全独立，具备与模型开发者相同的教育和技能，并且薪酬也应相同。如果负责机器学习的主管向数据科学副总裁和首席技术官（CTO）汇报，他们无法告诉自己的老板“不行”。他们很可能只是一个代表，花时间在论坛上让组织对其有问题的模型感觉更好。这就是为什么MRM定义了一个专注于机器学习风险的高级执行角色，并规定该高级执行人员不应向CTO或CEO报告，而应直接向董事会报告（或向也向董事会报告的首席风险官）。
- en: 'A lot of governance boils down to a crucial phrase that more data scientists
    should be aware of: *effective challenge*. Effective challenge is essentially
    a set of organizational structures, business processes, and cultural competencies
    that enable skilled, objective, and empowered oversight and governance of ML systems.
    In many ways, effective challenge comes down to having someone in an organization
    that can stop an ML system from being deployed without the possibility of retribution
    or other negative career or personal consequences. Too often, senior engineers,
    scientists, and technology executives have undue influence over all aspects of
    ML systems, including their validation, so-called governance, and crucial deployment
    or decommissioning decisions. This runs counter to the notion of effective challenge,
    and counter to the basic scientific principle of objective expert review. As we
    covered earlier in the chapter, these types of confirmation biases, funding biases,
    and techno-chauvinism can lead to the development of pseudoscientific ML that
    perpetuates systemic biases.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 很多治理工作归结为一个关键短语，更多的数据科学家应该了解：*有效挑战*。有效挑战本质上是一组组织结构、业务流程和文化能力，使得有能力的、客观的监督和治理机器学习系统成为可能。在许多方面，有效挑战意味着在组织中有人能够阻止机器学习系统被部署，而不会面临报复或其他负面的职业或个人后果的可能性。太多时候，高级工程师、科学家和技术高管在机器学习系统的所有方面，包括验证、所谓的治理以及关键的部署或废除决策中具有不适当的影响力。这与有效挑战的概念相抵触，也与客观专家审查的基本科学原则相抵触。正如我们在本章前面提到的，这些类型的确认偏见、资金偏见和技术沙文主义可能导致发展出持续系统偏见的伪科学机器学习模型。
- en: While there is no one solution for ML system bias, two themes for this chapter
    stand out. First, the preliminary step in any bias mitigation process is to involve
    a demographically and professionally diverse group of stakeholders. Step 0 for
    an ML project is to get diverse stakeholders in the room (or video call) when
    important decisions are being made! Second, human-centered design, bug bounties,
    and other standardized processes for ensuring technology meets the needs of its
    human stakeholders are some of the most effective bias mitigation approaches today.
    Now, we’ll close the chapter with a case discussion of bias in Twitter’s image-cropping
    algorithm and how a bug bounty was used to learn more about it from their users.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管没有针对ML系统偏见的通用解决方案，但本章突出了两个主题。首先，在任何偏见缓解过程的初步步骤中，都需要涉及一个在人口和专业上多样化的利益相关者群体。对于ML项目来说，第0步是在做出重要决策时让多样化的利益相关者参与（无论是在现场还是通过视频会议）！其次，以人为中心的设计、漏洞赏金以及其他确保技术满足其人类利益相关者需求的标准化流程，是当前一些最有效的偏见缓解方法。现在，我们将以讨论Twitter图像裁剪算法中的偏见案例结束本章，并探讨如何利用漏洞赏金从用户那里了解更多信息。
- en: 'Case Study: The Bias Bug Bounty'
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：偏见漏洞赏金
- en: This is a story about a questionable model and a very decent response to it.
    In October 2020, Twitter received feedback that its image-cropping algorithm might
    be behaving in a biased way. The image-cropping algorithm used an XAI technique,
    a saliency map, to decide what part of a user-uploaded image was the most interesting,
    and it did not let users override its choice. When uploading photos to include
    in a tweet, some users felt that the ML-based image cropper favored white people
    in images and focused on women’s chests and legs (male gaze bias), and users were
    not provided any recourse mechanism to change the automated cropping when these
    issues arose. The ML Ethics, Transparency, and Accountability (META) team, led
    by Rumman Chowdhury, posted a [blog article](https://oreil.ly/6Qx_H), [code](https://oreil.ly/S8E-L),
    and a [paper](https://oreil.ly/rwr5h) describing the issues and the tests they
    undertook to understand users’ bias issues. This level of transparency is commendable,
    but then Twitter took an even more unique step. It turned off the algorithm, and
    simply let users post their own photos, uncropped in many cases. Before moving
    to the bug bounty, which was undertaken later to gain even further understanding
    of user impacts, it’s important to highlight Twitter’s choice to take down the
    algorithm. Hype, commercial pressure, funding bias, groupthink, the sunken cost
    fallacy, and concern for one’s own career all conspire to make it extremely difficult
    to decommission a high-profile ML system. But that is what Twitter did, and it
    set a good example for the rest of us. *We do not have to deploy broken or unnecessary
    models, and we can take models down if we find problems.*
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于一个值得质疑的模型和对此的非常体面的回应的故事。在2020年10月，Twitter收到反馈称其图像裁剪算法可能以偏见方式运行。图像裁剪算法使用XAI技术，即显著性图，来决定用户上传的图像中哪一部分最有趣，并且不允许用户覆盖其选择。在上传照片以包含在推文中时，一些用户认为基于ML的图像裁剪器偏向于白人图像，并集中在女性的胸部和腿部（男性凝视偏见），而在这些问题出现时，用户没有提供任何补救机制来更改自动裁剪。由Rumman
    Chowdhury领导的ML伦理、透明度和问责（META）团队发布了一篇[博客文章](https://oreil.ly/6Qx_H)，[代码](https://oreil.ly/S8E-L)和一篇[论文](https://oreil.ly/rwr5h)描述了问题及其所进行的测试，这种透明度值得称赞，但随后Twitter采取了更独特的步骤。它关闭了算法，并简单地允许用户在许多情况下发布未裁剪的自己的照片。在后来进行的漏洞赏金之前，突出Twitter选择关闭该算法是很重要的。炒作、商业压力、资金偏见、群体思维、沉没成本谬误以及对自己职业的关注，所有这些因素都会导致在停用高调ML系统时变得非常困难。但这就是Twitter所做的，它为我们其余人树立了一个很好的榜样。*我们不必部署有问题或不必要的模型，并且如果发现问题，我们可以取下这些模型。*
- en: Beyond being transparent about its issues and taking the algorithm down, Twitter
    then decided to host a [bias bug bounty](https://oreil.ly/eBT18) to get structured
    user feedback on the algorithm. Users were incentivized to participate, as is
    usually the case with a bug bounty, through monetary prizes for those who found
    the worst bugs. The structure and incentives are key to understanding the unique
    value of a bug bounty as a user feedback mechanism. Structure is important because
    it’s difficult for large organizations to act on unstructured, ad hoc feedback.
    It’s hard to build a case for change when feedback comes in as an email here,
    a tweet there, and the occasional off-base tech media article. The META team put
    in the hard work to build a [structured rubric](https://oreil.ly/N3-gc) for users
    to provide feedback. This means that when the feedback was received, it was easier
    to review, could be reviewed across a broader range of stakeholders, and it even
    contained a numeric score to help different stakeholders understand the severity
    of the issue. The rubric is usable to anyone who wants to track harms in computer
    vision or natural language processing systems, where measures of practical and
    statistical significance and differential performance often do not tell the full
    story of bias. Incentives are also key. While we may care a great deal about responsible
    use of ML, most people, and even users of ML systems, have better things to worry
    about or don’t understand how ML systems can cause serious harm. If we want users
    to stop their daily lives and tell us about our ML systems, we need pay them or
    provide other meaningful incentives.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 在公开其问题并停用该算法后，决定举办[偏见漏洞悬赏活动](https://oreil.ly/eBT18)，以获取对算法的结构化用户反馈。通常情况下，参与者因发现最严重的漏洞而有机会获得金钱奖励。这种结构和激励机制是理解漏洞悬赏作为用户反馈机制独特价值的关键。结构至关重要，因为大型组织很难处理无结构、临时的反馈。当反馈以电子邮件、推文或偶尔的不切实际技术媒体文章形式出现时，很难为变革构建案例。META团队付出了艰苦的努力，建立了一个[结构化评分标准](https://oreil.ly/N3-gc)，供用户提供反馈。这意味着当反馈收到时，更容易进行审核，可以跨更广泛的利益相关者进行审查，甚至包含数值分数，帮助不同利益相关者理解问题的严重程度。这个评分标准适用于任何希望跟踪计算机视觉或自然语言处理系统中伤害的人，这些系统的实际和统计显著性以及差异性能往往不能完全说明偏见的全部故事。激励措施同样关键。虽然我们可能非常关心机器学习的负责任使用，但大多数人，甚至机器学习系统的用户，有更重要的事情需要担心，或者不了解机器学习系统如何造成严重伤害。如果我们希望用户停下日常生活告诉我们有关我们的机器学习系统，我们需要支付他们或提供其他有意义的激励措施。
- en: According to [AlgorithmWatch](https://oreil.ly/8B3dr), an EU think tank focusing
    on the social impacts of automated decision making, the bug bounty was “an unprecedented
    experiment in openness.” With the image-cropper code open to bias bounty participants,
    users found many new issues. According to [Wired](https://oreil.ly/UvNMh), participants
    in the bug bounty also found a bias against those with white hair, and even against
    memes written in non-Latin scripts—meaning if we wanted to post a meme written
    in Chinese, Cyrillic, Hebrew, or any of the many languages that do not use the
    Latin alphabet—the cropping algorithm would work against us. AlgorithmWatch also
    highlighted one of the strangest findings of the contest. The image cropper often
    selected the last cell of a comic strip, spoiling the fun for users trying to
    share media that used the comic strip format. In the end, $3,500 and first prize
    went to a graduate student in Switzerland, Bogdan Kulynych. Kulynych’s [solution](https://oreil.ly/xOkz6)
    used deepfakes to create faces across a spectrum of shapes, shades, and ages.
    Armed with these faces and access to the cropping algorithm, he was able to empirically
    prove that the saliency function within the algorithm, used to select the most
    interesting region of an upload image, repeatedly showed preferences toward younger,
    thinner, whiter, and more female-gendered faces.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[AlgorithmWatch](https://oreil.ly/8B3dr)，一个专注于自动决策社会影响的欧盟智库，赏金计划是“一次前所未有的开放性实验”。公开图像裁剪代码以获得偏见赏金参与者的参与，用户发现了许多新问题。根据[Wired](https://oreil.ly/UvNMh)，赏金计划的参与者还发现了针对白发者以及非拉丁文本脚本（即如果我们想发布中文、西里尔文、希伯来文或任何不使用拉丁字母表的语言编写的模因），算法会反对我们的倾向。AlgorithmWatch还强调了比赛中的一项最奇怪的发现。图像裁剪器经常选择漫画条的最后一个单元格，从而使试图分享使用漫画条格式的媒体的用户感到失望。最终，3500美元和头奖被瑞士的一名研究生Bogdan
    Kulynych赢得。Kulynych的[解决方案](https://oreil.ly/xOkz6)使用深度伪造技术在各种形状、色调和年龄段之间创建面部。凭借这些面部和对裁剪算法的访问，他能够经验证明算法内的显著性功能，用于选择上传图像中最有趣的区域，反复显示出对年轻、瘦、白皮肤和更女性化面孔的偏好。
- en: The bias bounty was not without criticism. Some civil society activists voiced
    concerns that the high-profile nature of a tech company and tech conference drew
    attention away from the underlying social causes of algorithmic bias. AlgorithmWatch
    astutely points out the $7,000 in offered prize money was substantially less than
    bounties offered for security bugs, which average around $10,000 per bug. It also
    highlights that $7,000 is 1–2 weeks of salary pay for Silicon Valley engineers,
    and Twitter’s own ethics team stated that the week-long bug bounty amounted to
    roughly a year’s worth of testing. Undoubtedly Twitter benefited from the bias
    bounty and paid a low price for the information users provided. Are there other
    issues with using bug bounties as a bias risk mitigant? Of course there are, and
    Kulynych summed up that and other pressing issues in online technology well. According
    to the [*Guardian*](https://oreil.ly/5FdnH), Kulynych had mixed feelings on the
    bias bounty and opined, “Algorithmic harms are not only *bugs.* Crucially, a lot
    of harmful tech is harmful not because of accidents, unintended mistakes, but
    rather by design. This comes from maximization of engagement and, in general,
    profit externalizing the costs to others. As an example, amplifying gentrification,
    driving down wages, spreading clickbait and misinformation are not necessarily
    due to *biased* algorithms.” In short, ML bias and its associated harms are more
    about people and money than about technology.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见赏金并非没有批评。一些公民社会活动家表示担忧，科技公司和科技会议的高调性质使人们注意力从算法偏见的根本社会原因上转移开来。AlgorithmWatch精辟地指出，提供的7000美元奖金远远少于安全漏洞的悬赏，后者平均每个漏洞约为10000美元。它还强调，7000美元是硅谷工程师1-2周的工资支付，Twitter自己的道德团队称，为期一周的赏金计划相当于大约一年的测试工作。毫无疑问，Twitter从偏见赏金中受益，并为用户提供的信息支付了低廉的代价。使用赏金计划作为偏见风险缓解措施还存在其他问题吗？当然存在，Kulynych在在线技术方面总结了这些及其他紧迫问题。根据[*Guardian*](https://oreil.ly/5FdnH)，Kulynych对偏见赏金有着复杂的感受，并评论道：“算法伤害不仅仅是*漏洞*。关键在于，许多有害技术不是由于意外、无意的错误，而是出于设计。这源自于增强参与度，并且通常是通过将成本外部化来获取利润。例如，加剧了新兴社区、压低了工资、传播点击诱饵和错误信息，并不一定是由于*有偏见*的算法。”简言之，机器学习偏见及其相关的伤害更多关乎人和金钱，而非技术本身。
- en: Resources
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: Further Reading
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[“50 Years of Test (Un)fairness: Lessons for Machine Learning”](https://oreil.ly/fTlda)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“50 Years of Test (Un)fairness: Lessons for Machine Learning”](https://oreil.ly/fTlda)'
- en: '[“An Empirical Comparison of Bias Reduction Methods on Real-World Problems
    in High-Stakes Policy Settings”](https://oreil.ly/vmxPz)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“An Empirical Comparison of Bias Reduction Methods on Real-World Problems
    in High-Stakes Policy Settings”](https://oreil.ly/vmxPz)'
- en: '[“Discrimination in Online Ad Delivery”](https://oreil.ly/kuo9h)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“在线广告投放中的歧视”](https://oreil.ly/kuo9h)'
- en: '[“Fairness in Information Access Systems”](https://oreil.ly/1RAPJ)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“信息访问系统中的公平性”](https://oreil.ly/1RAPJ)'
- en: '[NIST SP1270: “Towards a Standard for Identifying and Managing Bias in Artificial
    Intelligence”](https://oreil.ly/3_Qrd)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NIST SP1270：“朝向识别和管理人工智能中的偏见的标准”](https://oreil.ly/3_Qrd)'
- en: '[*Fairness and Machine Learning*](https://oreil.ly/D07t-)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*公平性与机器学习*](https://oreil.ly/D07t-)'
- en: ^([1](ch04.html#idm45990002397824-marker)) The authors acknowledge the potential
    offensiveness of several of the terms appearing in this quoted language. The source
    material, NIST SP1270 AI, was reviewed and justified by the potential for extreme
    harm when ignoring scientific rigor in AI.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#idm45990002397824-marker)) 作者承认引用语言中的几个术语可能具有潜在的冒犯性。NIST SP1270
    AI 材料经过审查，并且在忽视AI科学严谨性可能导致极端伤害时被证明是合理的。
- en: ^([2](ch04.html#idm45990005773296-marker)) Note that [updating](https://oreil.ly/0gEwg)
    loss functions for XGBoost is fairly straightforward.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#idm45990005773296-marker)) 注意，为XGBoost更新损失函数是相当简单的。
