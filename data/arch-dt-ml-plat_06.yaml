- en: Chapter 6\. Innovating with an Enterprise Data Warehouse
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。通过企业数据仓库进行创新
- en: In [Chapter 3](ch03.html#designing_your_data_team), you learned that the choice
    between a data lake and a data warehouse as the central component of your cloud
    data platform comes down to whether your organization is engineering/science-first
    (choose a data lake) or analytics-first (choose a DWH). In [Chapter 5](ch05.html#architecting_a_data_lake),
    we focused on the concept of data lake as a central element in the design of a
    data platform. In this chapter, you will learn how to address the same problems
    of cost, democratization, flexibility, and governance using a modern data warehouse
    as the central element instead.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#designing_your_data_team)中，您了解到选择作为云数据平台中心组件的数据湖或数据仓库的选择取决于您的组织是以工程/科学为先（选择数据湖）还是以分析为先（选择数据仓库）。在[第5章](ch05.html#architecting_a_data_lake)中，我们专注于将数据湖作为数据平台设计的中心要素的概念。在本章中，您将学习如何使用现代数据仓库作为中心要素来解决成本、民主化、灵活性和治理等问题。
- en: We will start with a quick recap of the problems being addressed by building
    a data platform and discuss the technology trends that make a cloud DWH an appealing
    solution. Then we will do a deep dive into what a modern DWH architecture looks
    like and how you can effectively enable data analysts and data scientists with
    it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从快速回顾构建数据平台所解决的问题开始，并讨论使云数据仓库成为一个吸引人解决方案的技术趋势。然后我们将深入探讨现代数据仓库架构的外观以及如何有效地使用它来启用数据分析师和数据科学家。
- en: A Modern Data Platform
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代数据平台
- en: Whenever you undertake a large technology project, you should first ask yourself
    what business goals are you trying to accomplish, what are your current technology
    challenges, and what technology trends do you want to leverage. In this section
    we will focus on helping you understand how to address these questions when building
    a modern data platform and how an enterprise DWH approach can steer the focus
    in your data platform design. Many of these concepts have already been touched
    on in the previous chapters, but it is useful to reframe them here as it will
    help you connect the design of a modern DWH with the problems the architecture
    is solving.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你开始一个大型技术项目时，你应该首先问自己，你试图实现什么业务目标，你当前的技术挑战是什么，你想利用什么技术趋势。在本节中，我们将重点帮助您理解在构建现代数据平台时如何解决这些问题，以及企业数据仓库方法如何引导您的数据平台设计。这些概念在前几章中已经被提及，但在这里重新框架它们是有用的，因为它将帮助您将现代数据仓库的设计与架构解决的问题联系起来。
- en: Organizational Goals
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组织目标
- en: 'In our customer interviews, CTOs repeatedly raised these organization goals
    as being important:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的客户访谈中，CTO们反复提到这些组织目标是非常重要的：
- en: No silos
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 无信息孤岛
- en: Data has to be activated across the entire enterprise because users in one part
    of the business need access to data that other departments create. For example,
    a product manager determining how to design next year’s product might need access
    to transaction data created and managed by the retail operations team.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据必须在整个企业中激活，因为业务的一个部分的用户需要访问其他部门创建的数据。例如，确定如何设计明年产品的产品经理可能需要访问由零售运营团队创建和管理的交易数据。
- en: Democratization
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 民主化
- en: The data platform has to support domain experts and other nontechnical users
    who can access the data without going through technical intermediaries but who
    should be able to rely on the quality and consistency of the data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平台必须支持领域专家和其他非技术用户，他们可以直接访问数据，而无需通过技术中介，但应该能够依赖数据的质量和一致性。
- en: Discoverability
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 可发现性
- en: The data platform has to support data engineers and other technical users who
    need access to data at different stages of processing. For example, if we have
    a dataset in which raw, incoming transactions have been reconciled, a data scientist
    needs to be able to get the reconciled dataset. If they can’t discover it, they
    will rebuild a reconciliation routine. Therefore, it should be possible to discover
    all these “intermediate” datasets so that processing steps are not duplicated
    across the organization.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据平台必须支持数据工程师和其他需要在不同处理阶段访问数据的技术用户。例如，如果我们有一个数据集，其中原始的传入交易已经被对账，数据科学家需要能够获取对账后的数据集。如果他们找不到它，他们将重建一个对账例程。因此，应该能够发现所有这些“中间”数据集，以确保处理步骤不会在整个组织中重复。
- en: Stewardship
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 管理
- en: Datasets should be under the control of teams that understand what they are.
    For example, financial data should be under the control of the finance department,
    not of IT.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集应该由了解其内容的团队控制。例如，财务数据应该由财务部门控制，而不是由IT部门控制。
- en: Single source
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 单一数据源
- en: Data should be read in place. Minimize the copying and extracting of data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据应该在原地读取。尽量减少数据的复制和提取。
- en: Security and compliance
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性和合规性
- en: IT should serve as a broker for data, ensuring that only people with the right
    permissions can access it. It is imperative to implement all compliance checks
    required by regulations (e.g., GDPR, CCPA, Gramm-Leach-Bliley Act). Make sure
    to implement solutions for classifying data into sensitive/restricted data versus
    open or industry-specific data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: IT部门应作为数据的经纪人，确保只有具有正确权限的人可以访问数据。必须实施符合法规要求的所有合规性检查（例如GDPR、CCPA、格拉姆-利奇-布莱利法案）。确保实施将数据分类为敏感/受限数据与开放或行业特定数据的解决方案。
- en: Ease of use
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用便捷性
- en: Make reporting easier since there are hundreds of analysts building reports
    to support a variety of functions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让报告更容易，因为有数百分析师在创建支持各种功能的报告。
- en: Data science
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学
- en: Make data science teams more productive since these roles tend to be expensive
    and difficult to hire.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 提高数据科学团队的生产力，因为这些角色往往成本高且难以招聘。
- en: Agility
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 敏捷性
- en: Make insights available to decision makers faster.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让决策者更快地获得洞见。
- en: While the relative order of these goals varies between organizations, all of
    these goals figure in one way or another in every organization we spoke to. Therefore,
    a modern data platform should enable CTOs to achieve these goals.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些目标的相对顺序在组织之间有所不同，但我们与每个组织交谈时所有这些目标都以某种方式存在。因此，现代数据平台应该使CTO们能够实现这些目标。
- en: Technological Challenges
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术挑战
- en: 'What prevents CTOs from accomplishing these goals with the technologies that
    they have already deployed within the organization? CTOs tend to mention these
    technological challenges:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: CTO们在组织内部已经部署的技术中实现这些目标有什么阻碍？CTO们往往会提到以下技术挑战：
- en: Size and scale
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 规模和规模
- en: The quantity of data their organization is collecting has dramatically increased
    over time and is expected to continue to increase. Their current systems are unable
    to scale and remain within the speed and cost constraints of their business, leading
    to compromises such as sampling the incoming data or heavily prioritizing new
    data projects.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 组织收集的数据量随时间急剧增加，并预计将继续增加。他们当前的系统无法扩展，并且无法在业务的速度和成本约束内保持，导致妥协，如对进入的数据进行抽样或大量优先考虑新数据项目。
- en: Complex data and use cases
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的数据和使用案例
- en: Increasingly, the data being collected is unstructured data—images, videos,
    and natural language text. Their current systems for managing structured and unstructured
    data do not intersect. However, there is increasingly a need to use structured
    data (e.g., product catalog details) and unstructured data (e.g., catalog images,
    user reviews) together in use cases such as recommendations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的数据是非结构化数据——图片、视频和自然语言文本。他们当前管理结构化和非结构化数据的系统没有交集。然而，在推荐等使用案例中，使用结构化数据（例如产品目录详细信息）和非结构化数据（例如目录图片、用户评论）的需求越来越多。
- en: Integration
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 集成
- en: Over time, we have seen the availability of many new sources and sinks of data
    to the technology landscape that organizations should and want to leverage. For
    example, they would love to manage sales information in Salesforce, ads campaigns
    in Adobe, and web traffic in Google Analytics. There is a need to analyze and
    make decisions on all this data in tandem.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们看到技术领域出现了许多新的数据源和数据接收端，组织应该和希望利用这些数据。例如，他们希望在Salesforce中管理销售信息，在Adobe中管理广告活动，在Google
    Analytics中管理网站流量。需要同时分析和决策所有这些数据。
- en: Real time
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 实时性
- en: Much of the new data is collected in real time, and there is a competitive advantage
    to being able to process and make decisions on the data as it arrives. However,
    organizations do not have a data platform that seamlessly supports streaming.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分新数据是实时收集的，能够在数据到达时处理并做出决策具有竞争优势。然而，组织缺乏无缝支持流式处理的数据平台。
- en: 'These are, of course, just a more nuanced version of the traditional big data
    challenges: volume, variety (of data and systems), and velocity.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些只是传统大数据挑战的更细化版本：数据量、数据和系统的多样性以及数据的速度。
- en: Technology Trends and Tools
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术趋势和工具
- en: 'To enable their organization to achieve these business and technological goals,
    a cloud architect can leverage the trends and tools described in the previous
    chapters. For convenience, we’ve summarized them here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助组织实现这些业务和技术目标，云架构师可以利用前面章节中描述的趋势和工具。为方便起见，我们在此处对它们进行了总结：
- en: Separation of compute and storage
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 计算与存储分离
- en: Public cloud providers allow you to store data on blob storage and access it
    from ephemeral computational resources. These computational resources consist
    of software such as Google BigQuery, AWS Redshift, Snowflake, Amazon EMR, Google
    Dataproc, Cloud Dataflow, or Databricks that were custom-built or adapted to take
    advantage of this separation of computing and distribute data processing over
    multiple workers. They span both structured and unstructured data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 公共云提供者允许您在 Blob 存储上存储数据，并从临时计算资源中访问它。这些计算资源包括 Google BigQuery、AWS Redshift、Snowflake、Amazon
    EMR、Google Dataproc、Cloud Dataflow 或 Databricks 等软件，这些软件是定制或适应了这种计算与数据处理分离的优势，并将数据处理分布到多个工作节点上。它们涵盖结构化和非结构化数据。
- en: Multitenancy
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 多租户
- en: Cloud computing resources are built to allow multiple tenants. Therefore, there
    is no need to create distinct clusters or storage arrays for each department in
    an organization. Thus, two separate departments can store their data in a single
    DWH and access each other’s data from compute resources that they each pay for—each
    team can spin up their own analytics on the common, shared dataset. Similarly,
    an organization can use its computing resources to access data from multiple organizations
    and do analytics on the joined datasets. Unlike traditional Hadoop clusters, it
    is not necessary to run the compute workload collocated with the data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算资源被设计成允许多个租户使用。因此，组织无需为每个部门创建单独的集群或存储阵列。因此，两个不同的部门可以将它们的数据存储在单个数据仓库中，并从它们各自支付的计算资源中访问对方的数据，每个团队可以在共享的常用数据集上启动自己的分析。类似地，组织可以利用自己的计算资源访问多个组织的数据，并对联合数据集进行分析。与传统的
    Hadoop 集群不同，不需要在数据的同一位置运行计算工作负载。
- en: Separation of authentication and authorization
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 认证与授权分离
- en: Cloud IAM solutions can ensure that a central IT team can secure identities
    while the owners of the data control access. In fact, by providing access to groups,
    it is possible to allow the accessing organization to manage the membership while
    the data owners manage only business logic of which teams are provided what access.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 云 IAM 解决方案可以确保中央 IT 团队安全管理身份，同时数据所有者控制访问权限。实际上，通过提供对组的访问权限，允许访问组织管理成员资格，而数据所有者仅管理指定团队的业务逻辑以提供访问权限。
- en: Analytics hubs
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 分析中心
- en: Serverless DWHs (and data lakes, as we have seen in the previous chapter) allow
    the architect to break down data silos even outside the boundaries of the organization.
    While the data owner pays for storage, the data consumer pays for querying. Also,
    while the data owner decides which groups have access, membership of the group
    can be managed by the data consumer. Thus, partners and suppliers can share data
    without having to worry about querying costs or group memberships.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器数据仓库（以及我们在前一章节中看到的数据湖）允许架构师在组织边界之外打破数据孤岛。数据所有者支付存储费用，而数据使用者支付查询费用。此外，数据所有者决定哪些组有权限访问，而组的成员资格可以由数据使用者管理。因此，合作伙伴和供应商可以共享数据，而无需担心查询成本或组成员资格。
- en: Multicloud semantic layer
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 多云语义层
- en: Tools such as Informatica or Looker make it possible to create a semantic layer
    that stretches across hyperscalers (such as AWS, GCP, or Azure), multicloud data
    platforms (such as Snowflake or Databricks), and on-premises environments. The
    semantic layer can rewrite queries on the fly to provide a consistent and coherent
    data dictionary. (Please note that semantic layers are covered in more detail
    later in the chapter.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如 Informatica 或 Looker 的工具使得可以创建跨超大规模云（如 AWS、GCP 或 Azure）、多云数据平台（如 Snowflake
    或 Databricks）以及本地环境的语义层。语义层可以即时重写查询，提供一致和连贯的数据字典。（请注意，语义层将在本章后面更详细地介绍。）
- en: Consistent admin interface
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一致的管理界面
- en: Data fabric solutions provide a consistent administration experience on the
    public cloud no matter where the data is stored, whether in a DWH or in data lake
    formats such as Parquet files on blob storage.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据布局解决方案在公共云上提供一致的管理体验，无论数据存储在数据仓库还是数据湖格式（如 Blob 存储上的 Parquet 文件）中。
- en: Cross-cloud control pane
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 跨云控制面板
- en: Tools such as BigQuery Omni provide a consistent control pane and query layer
    regardless of which hyperscaler (AWS, GCP, or Azure) your organization uses to
    store the data. These are useful if the concern is to ensure that the same tooling
    can be used regardless of which hyperscaler’s storage a particular dataset lives
    in. The trade-off is an increased dependency on the GCP control pane.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如 BigQuery Omni 这样的工具提供了一个一致的控制面板和查询层，无论您的组织使用哪个超大规模云平台（AWS、GCP 或 Azure）存储数据。这些工具在确保可以不受特定数据集存储在哪个超大规模云存储中影响时非常有用。但这也增加了对
    GCP 控制面板的依赖性。
- en: Multicloud platforms
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 多云平台
- en: Snowflake, Confluent, and Databricks provide for the ability to run the same
    software on any hyperscaler. However, unlike in the previous bullet point, the
    runtimes on different clouds remain distinct. These are useful if the concern
    is to ensure that it is possible to move from one hyperscaler to another. The
    trade-off is an increased dependency on the single-source software vendor.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake、Confluent 和 Databricks 提供了在任何超大规模云上运行相同软件的能力。然而，与上述不同的是，不同云上的运行时仍然是独立的。这些工具在确保能够从一个超大规模云平台迁移到另一个平台时非常有用。但这也增加了对单一软件供应商的依赖性。
- en: Converging of data lakes and DWHs
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖与数据仓库的融合
- en: Federated and external queries make it possible to run Spark jobs on data in
    the DWH and SQL queries on data in the data lake. We will expand on this topic
    in the next chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用联合查询和外部查询可以在数据仓库（DWH）中运行 Spark 作业，并在数据湖中运行 SQL 查询。我们将在下一章节中详细讨论这个主题。
- en: Built-in ML
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 内置机器学习
- en: Enterprise DWHs like AWS Redshift and Google BigQuery provide the ability to
    train and run ML without having to move data out of the DWH. Spark has an ML library
    (Spark MLlib), and ML frameworks such as TensorFlow are supported in Hadoop systems.
    Thus, ML can be carried out on the data platform without having to extract data
    to take to a separate ML platform.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 企业数据仓库如 AWS Redshift 和 Google BigQuery 提供了在不移出数据的情况下训练和运行机器学习的能力。Spark 拥有 ML
    库（Spark MLlib），而像 TensorFlow 这样的 ML 框架也受到 Hadoop 系统的支持。因此，可以在数据平台上执行机器学习，而不必将数据提取到单独的
    ML 平台。
- en: Streaming ingest
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 流式摄取
- en: Tools such as Kafka, AWS Kinesis, Azure Event Hub, and Google Cloud Pub/Sub
    support the ability to land data into hyperscalers’ data platforms in real time.
    Tools such as AWS Lambda, Azure Functions, Google Cloud Run, and Google Cloud
    Dataflow also support transforming the data as it arrives so as to quality control,
    aggregate, or semantically correct data before it is written out.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 工具如 Kafka、AWS Kinesis、Azure Event Hub 和 Google Cloud Pub/Sub 支持实时将数据落入超大规模云数据平台。AWS
    Lambda、Azure Functions、Google Cloud Run 和 Google Cloud Dataflow 等工具还支持在数据到达时进行数据转换，以进行质量控制、聚合或语义修正后再写出。
- en: Streaming analytics
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 流式分析
- en: DWHs support streaming SQL, so that as long as data is landed in the DWH in
    real time, queries reflect the latest data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库支持流式 SQL，因此只要数据实时落入数据仓库，查询就反映最新数据。
- en: Change data capture
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 变更数据捕获
- en: Tools such as Qlik, AWS Database Migration Service, and Google Datastream provide
    the ability to capture changes to an operational relational database (such as
    Postgres running on AWS Relational Database Service [RDS] or MySQL running on
    Google Cloud SQL) and stream them in real time to a DWH.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 工具如 Qlik、AWS 数据库迁移服务 和 Google Datastream 提供捕获操作关系数据库（如运行在 AWS 关系数据库服务 [RDS]
    上的 Postgres 或运行在 Google Cloud SQL 上的 MySQL）变更并实时流式传输到数据仓库的能力。
- en: Embedded analytics
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式分析
- en: It is possible to use modern visualization tools such as Power BI to embed analytics
    into the tools (mobile phones or websites) that end users use—it is not necessary
    to make end users operate dashboards.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用现代可视化工具如 Power BI 将分析嵌入到最终用户使用的工具（手机或网站）中——不需要让最终用户操作仪表板。
- en: The *hub-and-spoke architecture* provides a proven way to achieve the CTOs’
    desired goals and take advantage of the above technological capabilities.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*中心集线架构* 提供了实现 CTO 所期望目标并利用上述技术能力的可靠方法。'
- en: Hub-and-Spoke Architecture
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 中心集线架构
- en: When designing a modern cloud data platform centered around a DWH, the hub and
    spoke is the ideal architecture. In this architecture, the DWH acts as a *hub*
    that collects all the data needed for business analysis. *Spokes*, which are custom
    applications, dashboards, ML models, recommendation systems, and so on, interact
    with the DWH via standard interfaces (i.e., APIs). Tools such as Sigma Computing,
    SourceTable, and Connected Sheets even provide a spreadsheet interface that simulates
    Excel running on top of the DWH. All of these spokes can access data directly
    from the DWH without having to make a copy.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计以现代云数据平台为中心的DWH时，集线器和辐式架构是理想的架构。在这种架构中，DWH充当了收集业务分析所需所有数据的*集线器*。*辐*是定制应用程序、仪表盘、ML模型、推荐系统等，通过标准接口（即API）与DWH交互。工具如Sigma
    Computing、SourceTable和Connected Sheets甚至提供了一种电子表格界面，模拟在DWH顶部运行的Excel。所有这些辐可以直接从DWH访问数据，而无需复制。
- en: We suggest this approach to startups with no legacy technologies to accommodate,
    organizations that want a complete do-over to achieve a full-scale transformation,
    and even large enterprises because it is scalable, flexible, and resilient.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议这种方法适用于没有遗留技术需要适应的初创公司，希望进行全面重建以实现全面转型的组织，甚至是大型企业，因为它具有可扩展性、灵活性和弹性。
- en: It is scalable because new (modern) DWHs can easily integrate new data sources
    and use cases to the existing infrastructure without having to reconfigure the
    entire system. It is flexible because you can customize the overall architecture
    to meet the specific needs of an enterprise (e.g., enabling streaming). And it
    is resilient because it can withstand more failures than other architectures.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有可扩展性，因为新的（现代的）DWH可以轻松集成新的数据源和用例到现有的基础设施，而无需重新配置整个系统。它灵活，因为可以根据企业的特定需求定制整体架构（例如启用流式处理）。而且它具有弹性，因为它可以承受比其他架构更多的故障。
- en: A modern cloud native enterprise DWH forms the hub of the hub-and-spoke architecture,
    and the spokes are data providers and data consumers, as depicted in [Figure 6-1](#hub_and_spoke_architecturesemicolon_a_m).
    Please refer to components of the diagram as you read the following paragraphs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现代云原生企业DWH形成集线器和辐式架构的集线器，辐是数据提供者和数据消费者，如[图6-1](#hub_and_spoke_architecturesemicolon_a_m)所示。请在阅读以下段落时参考图表的组成部分。
- en: '![Hub-and-spoke architecture; a modern enterprise DWH forms the hub](assets/adml_0601.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![集线器和辐式架构；现代企业DWH形成集线器](assets/adml_0601.png)'
- en: Figure 6-1\. Hub-and-spoke architecture; a modern enterprise DWH forms the hub
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. 集线器和辐式架构；现代企业DWH形成集线器
- en: An analytics-first data and ML capability involves loading raw data into a DWH
    (*enterprise DWH*) and then carrying out transformations as needed to support
    various needs. Because of the separation of compute and storage, there is only
    one unique copy of the data. Different compute workloads such as querying (*Query
    API*), reporting/interactive dashboards (*business and intelligence reporting
    tools*), and ML (*data science and ML tools*) work on top of this data that sits
    in the DWH. Because you can leverage SQL for all the transformations, you can
    use views and materialized views to carry out elaborations, making ETL pipelines
    unnecessary. These views can invoke external functions, thus allowing for the
    enrichment of data in the DWH using ML APIs and models. In some cases, you can
    even train ML models using just SQL syntax and you can schedule complex batch
    pipelines via simple SQL commands. Modern DWHs support directly ingesting (*Load
    API*) even streaming data (*Streaming API*), and so you have to leverage streaming
    pipelines only when you need to perform low-latency, windowed aggregations analysis
    on data as it comes in. Always remember to assess the final cost of your solution
    and compare it with the benefits you will receive. Sometimes one strategy (batch)
    may be better than the other (streaming), and vice versa.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个以分析为先的数据和 ML 能力包括将原始数据加载到企业数据仓库（*企业数据仓库*），然后根据需要进行转换以支持各种需求。由于计算和存储分离，数据只有一个唯一的副本。不同的计算工作负载，如查询（*查询
    API*）、报告/交互式仪表板（*商业智能报告工具*）和 ML（*数据科学和 ML 工具*），都可以在这些数据之上进行操作，这些数据位于数据仓库中。由于可以利用
    SQL 进行所有转换，您可以使用视图和物化视图来进行详细说明，从而使 ETL 管道变得不必要。这些视图可以调用外部函数，因此可以使用 ML API 和模型丰富数据仓库中的数据。在某些情况下，甚至可以仅使用
    SQL 语法训练 ML 模型，并且可以通过简单的 SQL 命令调度复杂的批处理管道。现代数据仓库支持直接摄取（*加载 API*）甚至流数据（*流式 API*），因此您必须在需要对数据进行低延迟、窗口聚合分析时利用流式管道。始终记住评估您解决方案的最终成本，并将其与您将获得的收益进行比较。有时一个策略（批处理）可能比另一个策略（流处理）更好，反之亦然。
- en: The key idea behind the hub-and-spoke architecture is that you land all the
    data into the enterprise DWH as efficiently as possible. When data is coming from
    SaaS software (such as Salesforce), you can load it through a scheduled, automated
    export mechanism. When instead it is coming from *operational databases* such
    as Postgres, you can land it in near real time through CDC tools. Meanwhile, *real-time
    data sources* are expected to publish new data to the DWH when new events happen.
    Some data sources are federated (*federated datasets)*, which means that you will
    dynamically query and treat them as part of the DWH itself. With all the enterprise
    data now logically part of the DWH, data science and reporting tools can act on
    the data (*Storage API/Query API)*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 轮毂和辐条结构背后的关键思想是尽可能高效地将所有数据落入企业数据仓库。当数据来自 SaaS 软件（例如 Salesforce）时，您可以通过计划的自动导出机制加载它。而当数据来自像
    PostgreSQL 这样的*运营数据库*时，您可以通过 CDC 工具实时地将其落入数据仓库。同时，*实时数据源*预期在发生新事件时向数据仓库发布新数据。某些数据源是联邦的（*联邦数据集*），这意味着您可以动态查询并将它们视为数据仓库的一部分。现在所有企业数据都逻辑上属于数据仓库，数据科学和报告工具可以对数据进行操作（*存储
    API/查询 API*）。
- en: If you don’t have preexisting ETL pipelines that you need to port or preexisting
    end users whose tool choices you have to support, the hub and spoke is simple,
    powerful, fast, and cost-effective. An example manifestation of this architecture
    on Google Cloud is shown in [Figure 6-2](#hub_and_spoke_architecture_as_manifeste).
    The other hyperscalers offer more options (e.g., Athena, Redshift, Snowflake on
    AWS); we’ll cover the variations in [Chapter 7](ch07.html#converging_to_a_lakehouse).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有需要迁移的预先存在的 ETL 管道，或者需要支持的预先存在的最终用户的工具选择，那么轮毂和辐条结构就是简单、强大、快速和经济高效的选择。在 Google
    Cloud 上的一个示例展示了这种架构，如 [图 6-2](#hub_and_spoke_architecture_as_manifeste) 所示。其他超大规模云服务提供商提供更多选项（例如
    AWS 上的 Athena、Redshift、Snowflake）；我们将在 [第 7 章](ch07.html#converging_to_a_lakehouse)
    中涵盖这些变化。
- en: '![Hub-and-spoke architecture as manifested on Google Cloud](assets/adml_0602.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![在 Google Cloud 上展现的轮毂和辐条结构](assets/adml_0602.png)'
- en: Figure 6-2\. Hub-and-spoke architecture as manifested on Google Cloud
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 在 Google Cloud 上展现的轮毂和辐条结构
- en: In its fully automated form, the hub and spoke is a great choice for enterprises
    without strong engineering skills and for small departments doing shadow IT, because
    there is very little code to maintain when it comes to data ingestion. Also, you
    can complete data science and reporting activities with only a knowledge of SQL.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在其完全自动化的形式中，集线器和轮辐对于没有强大工程技能的企业以及进行影子IT的小部门是一个很好的选择，因为在数据摄入时几乎没有代码需要维护。此外，您只需具备SQL知识就可以完成数据科学和报告活动。
- en: 'It is worth noting that the capabilities that enable an analytics-first DWH
    are all recent developments. Separation of compute and storage came about with
    the public cloud: previously, the big data paradigm was dominated by MapReduce,
    which required sharding data to local storage. Analytics workloads required the
    building of data marts that were business specific. Due to performance constraints,
    it was necessary to carry out transformations before loading the data into these
    data marts (i.e., ETL). Stored procedures were DWHs, not external functions, which
    themselves relied on developments in autoscaling, stateless microservices. ML
    deployments required bundling and distributing libraries, not stateless ML APIs.
    ML workflows were based on self-contained training code, not ML pipelines consisting
    of containerized components. Streaming involved separate codebases, not unified
    batch and stream processing.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要注意的是，使分析优先的数据仓库（DWH）具备这些能力都是最近的发展。计算和存储的分离是公共云带来的：之前，大数据范式主要由需要将数据分片到本地存储的MapReduce主导。分析工作负载要求构建业务特定的数据集市。由于性能限制，需要在将数据加载到这些数据集市之前进行转换（即ETL）。存储过程是数据仓库，而不是外部函数，它们本身依赖于自动缩放、无状态微服务的发展。机器学习部署需要打包和分发库，而不是无状态机器学习API。机器学习工作流基于自包含的训练代码，而不是由容器化组件构成的机器学习流水线。流处理涉及单独的代码库，而不是统一的批处理和流处理。
- en: 'Now that you have seen the basic concepts of the hub-and-spoke architecture,
    let’s deep dive into its main components: ingest, business intelligence, transformations,
    and ML.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了集线器和轮辐架构的基本概念，请深入了解其主要组成部分：摄入、业务智能、转换和机器学习。
- en: Data Ingest
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄入
- en: 'One set of spokes in the hub-and-spoke architecture (the boxes gravitating
    around the enterprise DWH in [Figure 6-1](#hub_and_spoke_architecturesemicolon_a_m))
    corresponds to various ways to land (or ingest) data into the DWH. There are three
    ingest mechanisms: prebuilt connectors, real-time data capture, and federated
    querying. We will look at each of these respectively in this section.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 集线器和轮辐架构中的一个集轮辐（围绕企业DWH的盒子在[图6-1](#hub_and_spoke_architecturesemicolon_a_m)中）对应于将数据（或摄入）导入DWH的各种方式。有三种摄入机制：预构建连接器、实时数据捕获和联合查询。我们将在本节分别讨论每一种。
- en: Prebuilt connectors
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预构建连接器
- en: Landing data into the DWH can be extremely easy when leveraging popular SaaS
    platforms because they make available connectors that automatically ingest data
    with a few clicks. Every cloud native DWH (Google BigQuery, AWS Redshift, Snowflake,
    Azure Synapse, etc.) typically supports SaaS software such as Salesforce, Google
    Marketing Platform, and Marketo. If you happen to be using software that your
    choice of DWH doesn’t support, look at whether the software vendor provides a
    connector to your desired DWH—for example, Firebase (a mobile applications platform)
    can directly export crash reports from mobile applications into BigQuery for analytics
    (“crashlytics”).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当利用流行的SaaS平台时，将数据着陆到DWH可能非常简单，因为它们提供可以通过几次点击自动摄入数据的连接器。每个云原生DWH（Google BigQuery、AWS
    Redshift、Snowflake、Azure Synapse等）通常支持Salesforce、Google营销平台和Marketo等SaaS软件。如果您使用的软件不受您选择的DWH支持，请查看软件供应商是否提供连接器以将数据导入所需的DWH——例如，Firebase（一种移动应用平台）可以直接将移动应用程序的崩溃报告导出到BigQuery进行分析（“崩溃分析”）。
- en: You can set up these SaaS services to push data into common DWHs (e.g., Salesforce
    will automatically push data into Snowflake) or set up the import of these datasets
    into the DWH using services like the BigQuery Data Transfer Service. This is often
    termed *Zero ETL*—just as serverless doesn’t mean that there are no servers, only
    that the servers are managed by someone else, Zero ETL means that the ETL process
    is managed by your SaaS vendor or your DWH vendor.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以设置这些SaaS服务，将数据推送到通用的数据仓库（例如，Salesforce将自动将数据推送到Snowflake），或者使用BigQuery数据传输服务将这些数据集导入到数据仓库。这通常被称为*零ETL*——正如无服务器并不意味着没有服务器，只是服务器由他人管理一样，零ETL意味着ETL过程由您的SaaS供应商或您的DWH供应商管理。
- en: A third option is to use a third-party provider of connectors such as Fivetran.
    Their prebuilt connectors provide a turnkey ability to integrate data from marketing,
    product, sales, finance, and other applications (see [Figure 6-3](#third_party_vendors_such_as_fivetran_ca)).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种选择是使用像Fivetran这样的第三方连接器提供商。它们预构建的连接器提供了一种即插即用的能力，可以整合来自营销、产品、销售、财务等应用的数据（见[图 6-3](#third_party_vendors_such_as_fivetran_ca)）。
- en: '![Third-party vendors such as Fivetran can automatically handle landing data
    from a wide range of sources into a cloud DWH such as BigQuery, Redshift, or Snowflake](assets/adml_0603.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![像Fivetran这样的第三方供应商可以自动处理从各种来源到云数据仓库（如BigQuery、Redshift或Snowflake）的数据着陆](assets/adml_0603.png)'
- en: Figure 6-3\. Third-party vendors such as Fivetran can automatically handle landing
    data from a wide range of sources into a cloud DWH such as BigQuery, Redshift,
    or Snowflake
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 像Fivetran这样的第三方供应商可以自动处理从各种来源到云数据仓库（如BigQuery、Redshift或Snowflake）的数据着陆
- en: Between the transfer services of the cloud provider, software vendors that support
    cloud connectors, and third-party connector providers, you can buy (rather than
    build) the ability to export data routinely from your SaaS systems and load them
    to the enterprise DWH.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在云提供商的传输服务、支持云连接器的软件供应商以及第三方连接器提供商之间，你可以购买（而不是构建）定期从你的SaaS系统导出数据并将其加载到企业数据仓库的能力。
- en: Real-time data
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实时数据
- en: What if you want your DWH to reflect changes as they happen? You need to leverage
    a smaller set of tools called CDC tools. Operational databases (Oracle, MySQL,
    Postgres) are typically part of the support, as are enterprise resource planning
    (ERP) tools like SAP. Make sure that these tools use the Streaming API of the
    DWH to load data in near real time. On Google Cloud, Datastream is the recommended
    CDC tool, and on AWS, it is Database Migration Service (DMS).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望你的数据仓库能够反映数据发生变化的情况，你需要利用一组被称为CDC工具的小型工具集。操作数据库（Oracle、MySQL、Postgres）通常是支持的一部分，企业资源计划（ERP）工具如SAP也是如此。确保这些工具使用数据仓库的流式API以近乎实时的方式加载数据。在Google
    Cloud上，Datastream是推荐的CDC工具，在AWS上则是数据库迁移服务（DMS）。
- en: If you have real-time data sources, such as clickstream data or data from IoT
    devices, look for a capability to publish the events as they happen using the
    Streaming API of the DWH. Because the Streaming API can be accessed through HTTPS,
    all you need is a way to invoke an HTTPS service every time an event happens.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有实时数据源，比如点击流数据或来自物联网设备的数据，请寻找一种能够在事件发生时即时发布事件的能力，使用数据仓库的流式API。由于流式API可以通过HTTPS访问，你只需一种方法来在每次事件发生时调用一个HTTPS服务。
- en: If the provider of your IoT devices doesn’t support push notifications, then
    look for a way to publish events into a message queue (for example, using Message
    Queuing Telemetry Transport or MQTT) and use a stream processor (Dataflow on GCP,
    Kinesis on AWS) to write those events into the DWH (see [Figure 6-4](#landing_real_time_data_from_an_iot_devi)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的物联网设备供应商不支持推送通知，那么寻找一种将事件发布到消息队列的方法是一个好办法（例如使用消息队列遥测传输或MQTT），并使用流处理器（GCP上的Dataflow、AWS上的Kinesis）将这些事件写入数据仓库（见[图 6-4](#landing_real_time_data_from_an_iot_devi)）。
- en: '![Landing real-time data from an IoT device into a DWH](assets/adml_0604.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![将来自物联网设备的实时数据着陆到数据仓库](assets/adml_0604.png)'
- en: Figure 6-4\. Landing real-time data from an IoT device into a DWH
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. 将来自物联网设备的实时数据着陆到数据仓库
- en: Federated data
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联合数据
- en: 'You may not even need to land the data into the DWH to use it. Modern cloud
    DWHs are able to run queries on datasets in standard formats such as Avro, CSV,
    Parquet, and JSONL (line-oriented JavaScript Object Notation) without moving the
    data into the DWH. These are called *federated* queries and often require either
    that the data format be self-describing or that the schema be prespecified. For
    example, getting Google BigQuery to perform federated queries on Avro files, a
    self-describing format, involves three steps:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可能无需将数据着陆到数据仓库中来使用它。现代云数据仓库能够对标准格式的数据集（如Avro、CSV、Parquet和JSONL）运行查询，而无需将数据移动到数据仓库中。这些被称为*federated*查询，通常要求数据格式是自描述的，或者模式是预先指定的。例如，让Google
    BigQuery对Avro文件执行federated查询，一个自描述的格式，需要三个步骤：
- en: Create a table definition file using `bq mkdef --source_format=AVRO gs://filename`
    and edit the defaults if necessary. For example, you might change a field that,
    in Avro, is an integer to be treated as a real number.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`bq mkdef --source_format=AVRO gs://filename`创建表定义文件，并根据需要编辑默认设置。例如，你可能会将Avro格式中的整数字段更改为处理为实数字段。
- en: Use the resulting table definition file to create a BigQuery dataset using `bq
    mk --external_table_definition mydataset.mytablename`.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成的表定义文件使用`bq mk --external_table_definition mydataset.mytablename`创建BigQuery数据集。
- en: Query the dataset with SQL as normal.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SQL正常查询数据集。
- en: Note that the data remains on Cloud Storage in Avro format. This is what makes
    this a federated query. If the data format is not self-describing, the `mkdef`
    command allows you to specify a schema.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意数据保留在云存储中以Avro格式。这是使其成为联合查询的原因。如果数据格式不是自描述的，则`mkdef`命令允许您指定模式。
- en: 'It is possible even to combine these steps and apply a *schema on read* so
    that the schema definition is only for the duration of the query. For example,
    to have Azure Synapse query Parquet files in an Azure data lake, you can query
    as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至可以结合这些步骤，并应用*按读模式读取*，使得模式定义仅在查询持续期间有效。例如，要使Azure Synapse查询Azure数据湖中的Parquet文件，可以如下查询：
- en: '[PRE0]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the case of federated queries, the querying engine is the DWH. It is also
    possible to use an external querying engine (such as a PostgreSQL relational database)
    to carry out the queries. Such queries are called *external* queries (see [Figure 6-5](#example_of_federated_versus_external_qu)).
    For example, to get Amazon Redshift to query a Postgres database, follow these
    three steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在联合查询的情况下，查询引擎是数据仓库。也可以使用外部查询引擎（如PostgreSQL关系数据库）执行查询。这些查询称为*外部*查询（参见[图6-5](#example_of_federated_versus_external_qu)）。例如，要让Amazon
    Redshift查询Postgres数据库，按以下三个步骤操作：
- en: Make sure that your RDS PostgreSQL instance can accept connections from your
    Amazon Redshift cluster. To do this, you need to set up physical networking and
    ensure that the Redshift cluster is assigned an IAM role authorized in the PostgreSQL
    instance.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您的RDS PostgreSQL实例可以接受来自Amazon Redshift集群的连接。为此，您需要设置物理网络，并确保为Redshift集群分配了在PostgreSQL实例中授权的IAM角色。
- en: In Redshift, create an external schema using `CREATE EXTERNAL SCHEMA FROM POSTGRES`
    passing in the database, schema, host, IAM, and secret.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Redshift中，使用`CREATE EXTERNAL SCHEMA FROM POSTGRES`创建外部模式，传入数据库、模式、主机、IAM和秘密信息。
- en: Query the schema as normal.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正常查询模式下查询架构。
- en: '![Example of federated versus external query approach, both of which are available
    in AWS and GCP](assets/adml_0605.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图6-5。联合查询与外部查询方法示例，这两种方法在AWS和GCP中都可用](assets/adml_0605.png)'
- en: Figure 6-5\. Example of federated versus external query approach, both of which
    are available in AWS and GCP
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5。联合查询与外部查询方法示例，这两种方法在AWS和GCP中都可用。
- en: In all these instances, the key thing to note is that data remains in place
    and is queried from there—it is not loaded into the DWH. Because the opportunities
    for optimization are more limited when data remains in place (and cannot be partitioned,
    clustered, etc.), federated and external queries tend to be slower than native
    queries.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，需要注意的关键是数据保持在原地并从那里查询 - 不会加载到数据仓库中。因为数据保持在原地时（不能进行分区、聚集等），优化机会更有限，联合和外部查询往往比本地查询更慢。
- en: 'Given that federated and external queries are slower, why use them? Why not
    simply load the data into the DWH and treat the DWH as the source of truth? There
    are a few situations where it can be advantageous to avoid moving the data:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于联合查询和外部查询较慢，为什么要使用它们？为什么不简单地将数据加载到数据仓库并将数据仓库视为事实源？有一些情况下避免数据移动可能是有利的：
- en: In some cases, storage within the DWH is more expensive than storage outside
    of it. It may be more cost-effective to keep the data in a federated data source
    for very rarely queried data. When you need the best possible performance, use
    the native storage system offered by the DWH solution. If instead flexibility
    is more important, try to leverage federated data sources.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，数据仓库内部的存储比外部存储更昂贵。对于很少查询的数据，保持在联合数据源中可能更具成本效益。当需要最佳性能时，请使用数据仓库解决方案提供的本地存储系统。如果灵活性更为重要，则尝试利用联合数据源。
- en: If the data is frequently updated in a relational database, it may be advantageous
    to treat the relational database as the golden source. Doing CDC from the operational
    database to the DWH may introduce unacceptable latency.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果关系数据库中的数据经常更新，将关系数据库视为黄金源可能是有利的。从操作数据库到数据仓库的CDC可能会引入不可接受的延迟。
- en: The data may be created or needed by workloads (such as Spark). Because of this,
    it may be necessary to maintain Parquet files. Using federated/external queries
    limits the movement of data. This is the most common use case when you already
    have a data lake.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可能由工作负载（例如Spark）创建或需要。因此，可能需要维护Parquet文件。使用联合/外部查询限制了数据的移动。当您已经有一个数据湖时，这是最常见的用例。
- en: The data may belong to an organization that is different from the one that is
    querying it. Federation neatly solves the problem. However, suppose you are using
    a fully serverless DWH such as Google BigQuery that is not cluster based. In that
    case, it is possible to provide direct access to partners and suppliers even to
    native storage.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可能属于与查询它的组织不同的组织。联合查询巧妙地解决了这个问题。然而，如果您使用的是完全无服务器的DWH，如Google BigQuery，并非基于集群，那么甚至可以直接访问合作伙伴和供应商的原生存储。
- en: The last situation is one of the reasons that we recommend a fully serverless
    DWH that does not expect you to move data to a cluster, create data extracts,
    or provide specific applications (rather than specific users) access to the data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种情况是我们推荐使用完全无服务器的DWH的原因之一，它不期望您将数据移动到集群，创建数据提取或向数据提供特定应用（而不是特定用户）的访问权限。
- en: Now that you have a better knowledge of the available options for data ingestion,
    let’s deep dive into how to make the data speak by having a look at the various
    capabilities we can develop on the BI side.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您对数据摄取的可用选项有了更好的了解，让我们深入探讨如何通过查看我们可以在BI方面开发的各种能力，使数据发挥作用。
- en: Business Intelligence
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 商业智能
- en: 'Data analysts need to be able to rapidly obtain insights from data. The tool
    they use for this purpose needs to be self-service, support ad hoc analysis, and
    provide a degree of trust in the data being used (*business and intelligence reporting
    tools* in the hub-and-spoke architecture). It needs to provide several capabilities:
    SQL analytics, data visualization, embedded analytics, and a semantic layer, all
    of which we will cover throughout this section.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析师需要能够快速从数据中获得洞见。他们用于此目的的工具需要支持自助服务，支持临时分析，并对使用的数据提供一定程度的信任（*业务和智能报告工具*在轮毂与辐条结构中）。它需要提供多种能力：SQL分析、数据可视化、嵌入式分析和语义层，我们将在本节中涵盖所有这些内容。
- en: SQL analytics
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SQL分析
- en: As highlighted in the previous sections, SQL is the main language when querying
    the DWH. SQL is the *lingua franca* of data analysts and business analysts within
    an organization. These analysts will often carry out ad hoc queries on the DWH
    to help answer questions that come up (such as, “How many liters of ice cream
    were sold in Romania during the last heatwave?”). But in many cases, the questions
    become routine, and users operationalize them in the form of reports leveraging
    ad hoc tools like Power BI or Looker.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前几节所强调的，SQL是查询DWH时的主要语言。SQL是组织内数据分析师和业务分析师的*通用语言*。这些分析师通常会对DWH执行临时查询，以帮助回答出现的问题（例如，“罗马尼亚在最近的热浪期间卖了多少升冰淇淋？”）。但在许多情况下，这些问题变得常规化，用户会利用诸如Power
    BI或Looker等的临时工具将其操作化为报告的形式。
- en: These reporting tools, commonly known as BI tools, aim to provide a view of
    the entire data estate of the organization by connecting to a DWH (*business and
    intelligence reporting tools* in the hub-and-spoke architecture in [Figure 6-1](#hub_and_spoke_architecturesemicolon_a_m))
    so that analysts can make data-driven decisions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这些报告工具，通常称为BI工具，旨在通过连接到DWH（*业务和智能报告工具*在[图6-1](#hub_and_spoke_architecturesemicolon_a_m)中的轮毂与辐条结构）提供对组织整个数据资产的视图，以便分析师可以做出数据驱动的决策。
- en: Considering that it is not practical to expect an analyst to collect, clean,
    and load some piece of data at the time that they need it, the data needs to already
    be there. That’s the reason why the hub-and-spoke model with a central enterprise
    DWH is so popular.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到不实际期望分析师在需要时收集、清洗和加载某些数据，数据需要已经存在。这就是为什么以中央企业DWH为中心的轮毂与辐条模型如此受欢迎的原因。
- en: However, you should make sure that the BI tool is cloud ready and capable of
    dealing with large volumes of fast-arriving data. Early-generation BI tools would
    require that the data be extracted into online analytical processing (OLAP) cubes
    for performance reasons (see [Figure 6-6](#make_sure_that_the_bi_tool_pushes_all_q)).
    This simply won’t do—it leads to a proliferation of stale, exported data or huge
    maintenance burdens to support separate OLAP cubes for every possible use case.
    You want the BI tool to transparently delegate queries onto the DWH and retrieve
    the results. This is the best way to take advantage of the scale of the DWH and
    the timeliness of its streaming interfaces. SQL engines in the enterprise DWH
    have been upgraded to a level that they are able to handle multiple parallel queries
    or maintain in memory a huge amount of data, which allows organizations to get
    rid of the OLAP approach.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你应确保BI工具具备云端就绪，并能处理大量快速到达的数据。早期的BI工具会要求将数据提取到在线分析处理（OLAP）立方体中以提升性能（参见[图6-6](#make_sure_that_the_bi_tool_pushes_all_q)）。这种方法不可取，因为它会导致陈旧的导出数据的增加或者为支持各种用例而维护庞大的OLAP立方体带来巨大的负担。你希望BI工具能够透明地将查询委托给DWH并检索结果。这是充分利用DWH规模和其流式接口及时性的最佳方式。企业DWH中的SQL引擎已经升级到能够处理多个并行查询或在内存中维护大量数据的水平，这使得组织能够摆脱OLAP方法。
- en: '![Make sure that the BI tool pushes all queries to the enterprise DWH and doesn’t
    do them on OLAP cubes (extracts of the database/DWH)](assets/adml_0606.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![确保BI工具将所有查询推送到企业DWH，而不是在OLAP立方体上执行（数据库/DWH的提取）](assets/adml_0606.png)'
- en: Figure 6-6\. Make sure that the BI tool pushes all queries to the enterprise
    DWH and doesn’t do them on OLAP cubes (extracts of the database/DWH)
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6\. 确保BI工具将所有查询推送到企业DWH，而不是在OLAP立方体上执行（数据库/DWH的提取）
- en: Visualization
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化
- en: Your SQL queries will produce tables. However, it can be difficult to gain understanding
    and insights from raw tables alone. Instead, you will often plot your results
    as graphs, charts, or maps. Visualizing the results of SQL queries is often what
    leads to insight.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你的SQL查询将生成表格。然而，仅凭原始表格往往难以获得理解和洞察力。因此，你通常会将结果绘制为图形、图表或地图。将SQL查询结果可视化通常是洞察力的来源。
- en: In exploratory data analysis, visualization is ad hoc. However, the visualization
    has to help frame answers to common questions using common charting elements.
    This is the remit of dashboarding tools like Tableau, Looker, Power BI, and Looker
    Studio.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索性数据分析中，可视化是临时的。然而，可视化必须通过常见的图表元素帮助解答常见问题。这是像Tableau、Looker、Power BI和Looker
    Studio这样的仪表板工具的职责所在。
- en: Good dashboards keep the audience in mind and tell stories. They work both as
    a high-level overview of the current state but also as a launching point for further
    interactive analysis. They display contextually relevant, important metrics using
    the appropriate form of a chart.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 优秀的仪表板考虑受众并讲述故事。它们既可以作为当前状态的高级概览，也可以作为进一步互动分析的起点。它们使用适当的图表形式显示上下文相关的重要指标。
- en: Embedded analytics
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入式分析
- en: Visualization through traditional dashboard tools is sufficient if you wish
    to share the analytics results with a handful of internal people. Such users will
    appreciate the rich controls and interactivity that a dashboard provides. What
    if you are a crafts marketplace or telecom operator, and you want each artist
    or each kiosk to have access to near-real-time graphs of the performance of their
    own store?
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望与少数内部人员分享分析结果，则通过传统的仪表板工具进行可视化已足够。这些用户将欣赏仪表板提供的丰富控制和互动性。但如果你是一个手工艺品市场或电信运营商，并且希望每位艺术家或每个亭子都能访问其店铺性能的近实时图表，会怎样呢？
- en: When you are providing customized reports to thousands of users, you don’t want
    to provide end users with a feature-rich dashboard interface that can be hard
    to support and maintain. Instead, what you need is a lightweight graphics visual
    layer that can be embedded within the tool that the user is already using. It
    is common to embed the analytics within the website or mobile app that artists
    visit to list items for sale or that kiosk operations visit to order new items.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当你为成千上万的用户提供定制报告时，你不希望为最终用户提供一个功能丰富的仪表板界面，这会使支持和维护变得困难。相反，你需要的是一个轻量级的图形可视化层，可以嵌入到用户已经在使用的工具中。通常将分析嵌入到艺术家访问以列出销售物品或亭子运营访问以订购新物品的网站或移动应用程序中是很常见的。
- en: Providing consistent, live metrics of their store performance can significantly
    enhance the ability of artists and operators to make money in your marketplace.
    It is also possible to better connect workflows—for example, using analytics,
    sellers might be able to change the price of frequently back-ordered goods easily.
    Providing ML capabilities such as forecasting the demand of products might also
    provide the marketplace with new revenue streams.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 提供其商店表现的一致性、实时指标可以显著增强艺术家和经营者在您的市场中赚钱的能力。还可以更好地连接工作流程——例如，使用分析工具，卖家可能能够轻松地更改经常缺货商品的价格。提供预测产品需求等ML功能还可能为市场带来新的收入流。
- en: Semantic layer
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义层
- en: There is a tension between self-service analytics and consistency. It is important
    to give every business analyst in your company the ability to rapidly create dashboards
    without waiting on a central IT team. At the same time, it is imperative to maintain
    consistency in the way dashboards perform calculations (e.g., shipping costs have
    to be calculated in the same way across dashboards). While it is important that
    business analysts can build complex dashboards themselves, it is also important
    that analysts reuse existing visualizations as much as possible. The traditional
    approach to providing consistency and reuse has been to centralize calculations
    and foundational capabilities within IT. However, such centralized, IT-centric
    solutions are typically too brittle and too slow to satisfy business users in
    a data-driven environment.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 自助分析和一致性之间存在紧张关系。在您的公司中，赋予每位业务分析师快速创建仪表板的能力非常重要，而不必等待中央IT团队。同时，保持仪表板计算方式的一致性至关重要（例如，所有仪表板中的运输成本必须采用相同的计算方式）。尽管业务分析师能够自行构建复杂的仪表板至关重要，但尽可能地重用现有的可视化也很重要。提供一致性和重用的传统方法是将计算和基础功能集中在IT部门内。然而，在数据驱动环境中，这种中央化的IT中心化解决方案通常过于脆弱和过慢，无法满足业务用户的需求。
- en: Modern BI tools like Looker or MicroStrategy offer a *semantic layer* (see [Chapter 2](ch02.html#strategic_steps_to_innovate_with_data))
    to help address this tension. A semantic layer is an additional layer that allows
    you to facilitate an end user’s access to data autonomously using common business
    terms; it works via a decoupling between the nomenclature adopted by the table
    creator and the names adopted by the business users. LookML, which Looker calls
    its semantic model layer, is a data language based on SQL (see [Example 6-1](#example_six_onedot_the_semantic_layer_c)).
    It provides data stewards the ability to create reusable dimensions or measures
    that business analysts can reuse and extend. These definitions are made available
    in a data dictionary for easy discovery and management.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 像Looker或MicroStrategy这样的现代BI工具提供*语义层*（参见[第2章](ch02.html#strategic_steps_to_innovate_with_data)），帮助解决这种紧张关系。语义层是一个额外的层，允许您使用常见的业务术语，促进最终用户自主访问数据；它通过表创建者采用的命名方式与业务用户采用的名称之间的解耦来工作。Looker称其为语义模型层的LookML是基于SQL的数据语言（参见[示例6-1](#example_six_onedot_the_semantic_layer_c)）。它为数据管理员提供了创建可重用维度或度量的能力，业务分析师可以重用和扩展这些定义。这些定义在数据字典中可供轻松发现和管理。
- en: Example 6-1\. The semantic layer consists of centralized definitions of metrics
    and dimensions; this example from Looker shows a metric (overall health score)
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. 语义层由指标和维度的集中定义组成；Looker的这个示例显示了一个指标（总体健康评分）。
- en: '[PRE1]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: These semantic layers function as a BI data source in and of themselves. Tableau,
    for example, can connect to Looker’s semantic layer instead of directly to the
    DWH.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些语义层本身作为BI数据源进行功能。例如，Tableau可以连接到Looker的语义层，而不是直接连接到数据仓库。
- en: Although business users will typically not see or interact with LookML-like
    tools directly, they get to build dashboards that use the defined dimensions and
    metrics. This helps foster reuse so that each analyst doesn’t have to define from
    base table columns every dimension or metric that they employ. Centralizing the
    definition of a metric also decreases the likelihood of human error and provides
    a single point at which definitions can be updated. Having such a centralized
    definition exist in a text file permits easy version control as well.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管业务用户通常不会直接看到或与类似LookML的工具进行交互，他们可以构建使用定义的维度和指标的仪表板。这有助于促进重用，使每个分析师无需为每个使用的维度或指标从基础表列定义。集中定义指标还能减少人为错误的可能性，并提供更新定义的单一点。这种集中定义存在于文本文件中，便于版本控制。
- en: You have seen how you can dive into the data with the help of the BI tools and,
    via a semantic layer, facilitate business users managing the data by themselves.
    Sometimes this approach is not enough, and you need to prepare the data before
    starting with the ingestion into the DWH. In the next section we will focus on
    this topic.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经看到如何借助BI工具深入数据，并通过语义层使业务用户自行管理数据。有时这种方法还不够，您需要在将数据摄入数据仓库之前准备数据。在接下来的部分，我们将专注于这个主题。
- en: Transformations
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换
- en: Suppose you land raw data from ERP systems into a DWH. If the ERP is SAP, it’s
    likely that the column names are in German and reflect the application state,
    not what we would consider data that is useful to persist. We don’t want to force
    all our users to have to transform the data into a usable form every time they
    need it. So where should the transformation take place?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您从ERP系统将原始数据加载到数据仓库中。如果ERP是SAP，列名很可能是德语，并反映应用状态，而不是我们认为有用以保持的数据。我们不希望所有用户每次需要数据时都必须将数据转换为可用形式。那么转换应该在哪里进行呢？
- en: One option is to define the way columns have to be transformed in a semantic
    layer that is part of your BI tool as discussed in the previous section. However,
    this limits the definitions to dimensions and metrics, and accessing these definitions
    will be difficult from non-BI tools.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选项是在BI工具的语义层中定义列转换的方式，如前一节所讨论的。然而，这限制了定义为维度和度量，并且从非BI工具访问这些定义将会很困难。
- en: 'A better approach is to define transformations in SQL and create views, tables,
    or materialized views. In this section we will have a look at the common options
    available to handle transformations within a DWH. This is another advantage of
    the hub-and-spoke architecture: when data transformations are implemented in the
    DWH, the results are immediately available to all use cases that require them.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是在SQL中定义转换并创建视图、表格或物化视图。在本节中，我们将看一下数据仓库内处理转换的常见选项。这也是轮式结构的另一个优点：当数据转换在数据仓库中实现时，其结果立即对所有需要它们的用例可用。
- en: A third approach is to take advantage of the application development platform
    available on the hyperscaler. Use the event mechanism (Eventarc, EventBridge,
    Event Grid) to launch a serverless function (Lambda, Cloud Functions) whenever
    a new table partition gets created. The function can then transform and “push”
    the modified data into backend systems by invoking their APIs, thus initiating
    business actions (such as shipping notifications). This is called *reverse ETL*
    because the direction of data flow is away from the DWH.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法是利用超大规模应用开发平台。使用事件机制（Eventarc、EventBridge、Event Grid）在创建新表分区时启动无服务器函数（Lambda、Cloud
    Functions）。然后，函数可以转换并通过调用其API将修改后的数据“推送”到后端系统，从而启动业务操作（如发货通知）。这被称为*反向ETL*，因为数据流的方向远离数据仓库。
- en: ELT with views
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带视图的ELT
- en: Instead of ETL, it is possible to load data into the DWH as-is and transform
    it on the fly when reading it through views (see [Example 6-2](#example_six_twodot_in_this_example_from)).
    Because the views carry out the transformation on the fly after the data is loaded
    into the DWH (no transformation is carried out before loading), this is commonly
    called extract-load-transform (ELT) in contrast to the typical ETL workflow.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与ETL不同，可以将数据加载到数据仓库中并在通过视图读取时即时转换（见[示例 6-2](#example_six_twodot_in_this_example_from)）。因为视图在数据加载到数据仓库后即时进行转换（在加载之前不进行转换），这通常称为提取-加载-转换（ELT）工作流的常见方式。
- en: Example 6-2\. In this example from Azure Synapse Analytics, the SQL code creates
    a view by joining two tables
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-2\. 在Azure Synapse Analytics中的此示例中，SQL代码通过连接两个表格创建视图。
- en: '[PRE2]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Instead of querying the raw tables, users query the view. The SQL that creates
    the view can select specific columns, apply operations such as masking to the
    columns, and join multiple tables. Thus, ELT provides a consistent, governed view
    of the raw data to business users. Because the final query runs the *view query*
    before it further aggregates or selects, all queries reflect up-to-date information
    based on whatever data is present in the DWH.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是查询原始表格，用户查询视图。创建视图的SQL可以选择特定列，对列应用操作（如掩码），并连接多个表格。因此，ELT为业务用户提供了一致且受控的原始数据视图。因为最终查询在进一步聚合或选择之前运行*视图查询*，所以所有查询都基于数据仓库中存在的数据反映最新信息。
- en: Views, however, can become quite expensive in terms of computational resources,
    time, and/or money. Consider, for example, the view shown in [Example 6-2](#example_six_twodot_in_this_example_from)
    that joins the sales orders tables with the sales territory. All sales orders
    from all the territories over the entire lifetime of the business are being queried
    in this view. This is the case even if the analyst querying the view is interested
    in only a specific year or region (and in that case, it makes a lot of sense to
    filter out the data that is not relevant before performing the join).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，视图可能在计算资源、时间和/或金钱方面变得非常昂贵。例如，考虑到在 [例子 6-2](#example_six_twodot_in_this_example_from)
    中显示的视图，该视图将销售订单表与销售地域表进行连接。该视图会查询企业整个生命周期内所有地域的所有销售订单。即使分析人员只对特定年份或地区感兴趣（在这种情况下，过滤掉不相关的数据再执行连接操作是非常有意义的）。
- en: Scheduled queries
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计划查询
- en: If the tables are updated infrequently, it can be far more efficient to extract
    the data into tables periodically. For example, in Google BigQuery, as reported
    in [Example 6-3](#example_six_threedot_in_this_google_big), we specify the destination
    table, how often to run the query, and the query itself.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果表格的更新频率较低，则周期性地将数据提取到表格中可能会更加高效。例如，在 Google BigQuery 中，如 [例子 6-3](#example_six_threedot_in_this_google_big)
    所述，我们指定目标表、查询频率和查询内容。
- en: Example 6-3\. In this Google BigQuery example, raw data from the Orders table
    is extracted and aggregated, and aggregated results stored in a destination table,
    every 24 hours
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例 6-3\. 在这个 Google BigQuery 的示例中，从订单表中提取原始数据并进行聚合，然后将聚合结果存储在目标表中，每 24 小时执行一次
- en: '[PRE3]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The raw table is queried only once every 24 hours in the example shown. While
    there is an increase in storage cost associated with the destination table, storage
    costs are typically orders of magnitude cheaper than computational costs. Therefore,
    the cost of creating the destination table is quite manageable.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，原始表每 24 小时只查询一次。虽然与目标表格相关的存储成本会增加，但存储成本通常比计算成本便宜几个数量级。因此，创建目标表的成本是相当可控的。
- en: The key advantage of scheduled queries is that analysts query the destination
    table and not the raw data. Therefore, analyst queries are relatively inexpensive.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 计划查询的关键优势在于分析人员查询的是目标表而不是原始数据。因此，分析人员的查询相对较为廉价。
- en: The drawback of scheduled queries is the results returned to analysts can be
    up to 24 hours out of date. The degree to which queries are out of date can be
    reduced by running the scheduled query more frequently. Of course, the more often
    the extraction query is scheduled, the more the cost advantages start to dissipate.
    Another drawback of scheduled queries is that they can be wasteful if the resulting
    destination table is never queried.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 计划查询的缺点是返回给分析人员的结果可能落后于最多 24 小时。可以通过更频繁地运行计划查询来减少查询落后的程度。当然，如果调度查询的频率越高，成本优势就越容易消失。计划查询的另一个缺点是，如果生成的目标表从未被查询过，那么这些查询可能会造成浪费。
- en: Materialized views
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 材料化视图
- en: It is clear that the most efficient way to balance obsolete data and cost is
    to extract raw data into a destination table when you request a view for the first
    time. Subsequent queries can be fast because they can retrieve data from the destination
    table without doing any extraction. Meanwhile, the system needs to monitor the
    underlying tables and reextract the data when the original tables change. You
    can make the system even more efficient by updating the destination table with
    new rows of raw data rather than requerying the entire table.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，在第一次请求视图时，将原始数据提取到目标表中是平衡过时数据与成本的最有效方法。随后的查询可以很快，因为可以直接从目标表中检索数据，无需再进行提取操作。与此同时，系统需要监控底层表，并在原始表更改时重新提取数据。可以通过向目标表添加新的原始数据行来使系统更加高效，而不是重新查询整个表格。
- en: While you can build a data engineering pipeline to all this, modern cloud DWHs
    support fully managed materialized views out of the box. Creating a materialized
    view in these systems is analogous to creating a live view (see [Example 6-4](#example_six_fourdot_creating_a_material)),
    and you can query it just like any other view. The DWH takes care of ensuring
    that queries return up-to-date data. The DWH vendor charges a few costs for managing
    the materialized view.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你可以构建一个数据工程管道来处理所有这些问题，现代化的云数据仓库（DWHs）支持开箱即用的全面管理的物化视图。在这些系统中创建物化视图类似于创建实时视图（见[示例 6-4](#example_six_fourdot_creating_a_material)），你可以像查询任何其他视图一样查询它。数据仓库负责确保查询返回最新数据。数据仓库供应商会为管理物化视图收取一些费用。
- en: Example 6-4\. Creating a materialized view in Snowflake with automatic data
    update
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-4\. 在Snowflake中创建自动数据更新的物化视图
- en: '[PRE4]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Be careful, though—some DWHs (like Amazon Redshift, at the time of writing)
    do not automatically manage the materialized view for you; you are expected to
    set up a schedule or trigger to `REFRESH` the materialized view. In that sense,
    what they term a materialized view is actually just a table extract.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 但要小心——某些数据仓库系统（例如，撰写时的Amazon Redshift）并不会自动为您管理物化视图；你需要设置一个调度或触发器来`REFRESH`物化视图。从这个意义上说，他们所谓的物化视图实际上只是一个表的提取。
- en: Google BigQuery, Snowflake, Databricks, and Azure Synapse do transparently maintain
    the materialized view content. The view content is automatically updated as data
    is added to the underlying tables.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Google BigQuery、Snowflake、Databricks和Azure Synapse会透明地维护物化视图内容。视图内容会随着数据添加到底层表中而自动更新。
- en: Security and lineage
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全性和血统
- en: Data governance best practice is to ensure that an organization keeps track
    of all data transformations from its ingestion up to its usage.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 数据治理的最佳实践是确保组织跟踪从数据摄取到使用的所有数据转换过程。
- en: 'An important aspect to consider is related to the identification of the resources
    (e.g., entire datasets or some fields within a table) that have to be considered
    sensitive and need to be protected by design. It is pivotal not only to prevent
    access to information that should be considered secret within a company but even
    to be ready to correctly manage the data in light of compliance with government
    regulations that in some cases are becoming more and more stringent (e.g., GDPR
    in Europe, Health Insurance Portability and Accountability Act or HIPAA in the
    United States, etc.). It is important to note that, when talking about security
    and compliance, your focus should not be limited to who is accessing what (that
    can be generally addressed via a fine-grained access control lists [ACLs] policy
    management approach and data encryption/masquerading techniques); it should also
    be on:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一个需要考虑的重要方面是与资源标识相关的问题（例如，整个数据集或表中的某些字段），这些资源必须被视为敏感，并且需要通过设计进行保护。不仅仅是防止公司内应视为机密的信息的访问，还需要在正确管理数据方面做好准备，以符合某些情况下变得越来越严格的政府法规（例如，欧洲的GDPR，美国的健康保险可携带性和责任法案或HIPAA等）。需要注意的是，在谈论安全性和合规性时，你的关注点不应仅限于谁访问了什么（这可以通过细粒度的访问控制列表[ACL]策略管理方法和数据加密/伪装技术通常解决）；它还应该关注：
- en: The origin of the data
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的起源
- en: The different transformations that have been applied before its consumption
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其消费之前应用的不同转换
- en: The current physical data location (e.g., data lake in Germany or DWH in the
    United Kingdom)
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前物理数据位置（例如，德国的数据湖或英国的数据仓库）
- en: 'The tracking of this sort of metadata is called *data lineage*, and it helps
    ensure that accurate, complete, and trustworthy data is being used to drive business
    decisions. Looking at data lineage is also helpful in situations where you need
    to guarantee data locality because of some local legislation (e.g., telecommunication
    metadata in Germany): if you can track *where* the data is during its lifecycle,
    then you can put in place automations to prevent the access, usage, and movement
    of that information by people who do not meet the minimum requirements.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据血统*追踪这种元数据的方式有助于确保使用准确、完整和可信任的数据来支持业务决策。查看数据血统在需要保证数据位置的情况下也是有帮助的（例如，德国的电信元数据）：如果你能追踪数据在其生命周期中的*位置*，那么你可以采取自动化措施，防止未达到最低要求的人员访问、使用和移动这些信息。'
- en: 'As you have seen, metadata plays a central role in helping companies organize
    their data and govern its access. It is also crucial for evaluating the *quality*
    of the collected data in terms of accuracy, completeness, consistency, and freshness:
    poor-quality data might lead to wrong business decisions and potential security
    issues. There are several techniques, and related tools, that can support data
    quality activities, but the most common ones are:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，元数据在帮助公司组织其数据并管理其访问方面发挥着核心作用。它也对评估所收集数据的*质量*（如准确性、完整性、一致性和新鲜度）至关重要：低质量的数据可能导致错误的业务决策和潜在的安全问题。有几种技术和相关工具可以支持数据质量活动，但最常见的是：
- en: Standardization
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化
- en: The process of putting data coming from different sources into a consistent
    format (e.g., fix inconsistencies in capitalizations, characters, updating values
    in the wrong fields, data formats, etc.)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 将来自不同来源的数据放入一致的格式中的过程（例如，修正大小写不一致、字符、错误字段中的数据格式等）
- en: Deduplication
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 去重
- en: The process of identifying duplicate records (leveraging a similarity score)
    and then proceeding with the deletion of the duplicated values
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 识别重复记录（利用相似度分数），然后删除重复值的过程。
- en: Matching
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配
- en: The process of finding similarities or links between datasets to discover potential
    dependencies
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 查找数据集之间的相似性或链接，以发现潜在的依赖关系的过程
- en: Profiling and monitoring
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 分析和监控
- en: The process of identifying a range of data values (e.g., min, max, mean), revealing
    outliers and anomalies that could imply, for example, the need for a data fix
    or a schema evolution
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 识别一系列数据值（例如，最小值、最大值、均值），揭示可能暗示需要数据修复或模式演变的异常和异常值的过程
- en: If you use the native managed services of a cloud vendor, when you perform data
    transformations the tools will typically manage and carry along metadata. Thus,
    if you use, for example, Data Fusion, views, materialized views, etc., on Google
    Cloud, the Data Catalog is updated and lineage maintained. If you build your transformation
    pipeline using Dataflow, you should update the Data Catalog. Similarly, the crawlers
    will automatically update the Data Catalog on AWS, but you need to invoke the
    Glue API to add to the catalog if you implement your own transformations.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用云供应商的原生托管服务，在进行数据转换时，工具通常会管理并携带元数据。因此，例如在Google Cloud上使用Data Fusion、视图、物化视图等，Data
    Catalog会得到更新并保持血统。如果您使用Dataflow构建转换管道，则应更新Data Catalog。类似地，在AWS上，爬虫会自动更新Data Catalog，但如果您实施自己的转换，则需要调用Glue
    API将其添加到目录中。
- en: You have seen how the DWH (the *hub* in our architecture) can drive data transformation
    to make it available to all use cases you want to implement, keeping track at
    the same time of all the metadata you need to maintain a robust lineage of all
    the elaborations. Now let’s have a look at how you can craft the structure of
    the organization to match the hub-and-spoke architecture.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经看到了DWH（我们架构中的*中心*）如何驱动数据转换，使其可以供您希望实现的所有用例使用，并同时跟踪您需要维护的所有元数据，以保持强大的所有加工的血统。现在让我们看看如何设计组织结构来匹配枢纽和辐条架构。
- en: Organizational Structure
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组织结构
- en: In many organizations, there are many more business analysts than engineers.
    Often, this ratio is 100:1\. A hub-and-spoke architecture is ideal for organizations
    that wish to build a data and ML platform that primarily serves business analysts.
    Because the hub-and-spoke architecture assumes that business analysts are capable
    of writing ad hoc SQL queries and building dashboards, some training may be necessary.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多组织中，商业分析师比工程师多得多。通常，这个比例是100:1。枢纽和辐条架构非常适合希望构建主要服务于商业分析师的数据和机器学习平台的组织。因为枢纽和辐条架构假定商业分析师能够编写临时SQL查询和构建仪表板，可能需要进行一些培训。
- en: 'In an analyst-first system, the central data engineering team is responsible
    for (see the filled shapes in [Figure 6-7](#the_central_data_engineering_team_is_re)):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在以分析师为先导的系统中，中心数据工程团队负责（见[图6-7](#the_central_data_engineering_team_is_re)中的填充形状）：
- en: Landing the raw data from a variety of sources into the DWH. Many sources can
    be configured to directly publish to modern DWHs.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将来自各种来源的原始数据着陆到DWH中。许多源可以配置为直接发布到现代DWH。
- en: Ensuring data governance, including a semantic layer and the protection of PII.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保数据治理，包括语义层和个人身份信息（PII）的保护。
- en: Workloads that cross business units (e.g., activation), involve data across
    business units (e.g., identity resolution), or require specialized engineering
    skills (e.g., ML).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨业务单元的工作负载（例如激活），涉及跨业务单元的数据（例如身份解析），或需要专业工程技能（例如ML）。
- en: Common artifact repositories such as data catalog, source code repository, secret
    store, feature store, and model registries.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的工件存储库，例如数据目录，源代码存储库，秘密存储，特征存储和模型注册表。
- en: 'The business unit is responsible for (see the unfilled shapes in [Figure 6-7](#the_central_data_engineering_team_is_re)):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 业务单元负责（参见[图6-7](#the_central_data_engineering_team_is_re)中的未填充形状）：
- en: Landing data from business-specific sources into the DWH
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将来自特定业务源的数据引入DWH
- en: Transforming the raw data into a form usable for downstream analysis
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将原始数据转换为可供下游分析使用的形式
- en: Populating the governance catalogs and artifact registries with business-specific
    artifacts
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用业务特定工件填充治理目录和工件注册表
- en: Reports, ad hoc analytics, and dashboards for business decision making
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告，特定分析和用于业务决策的仪表板
- en: '![The central data engineering team is responsible for the filled shapes, while
    the business unit is responsible for the unfilled shapes](assets/adml_0607.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![中央数据工程团队负责填充的形状，而业务单元负责未填充的形状](assets/adml_0607.png)'
- en: Figure 6-7\. The central data engineering team is responsible for the filled
    shapes, while the business unit is responsible for the unfilled shapes
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7. 中央数据工程团队负责填充的形状，而业务单元负责未填充的形状
- en: '[Figure 6-7](#the_central_data_engineering_team_is_re) shows Google Analytics,
    Finance, and Salesforce as illustrative of data sources. In our example, Google
    Analytics data may be needed across multiple business units and is therefore ingested
    by the central data engineering team. Finance and Salesforce data is needed only
    by specific business units and is therefore ingested by that business unit.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-7](#the_central_data_engineering_team_is_re)显示Google Analytics，Finance和Salesforce作为数据源的示例。在我们的例子中，Google
    Analytics数据可能需要跨多个业务单元使用，因此由中央数据工程团队摄入。Finance和Salesforce数据仅需要特定的业务单元使用，因此由该业务单元摄入。'
- en: Each business unit manages its own deployment. It is the responsibility of the
    central team to land data into the DWH that the business team uses. This often
    means that the software that the central team uses needs to be portable across
    different clouds and the data format that is produced by the pipelines is a portable
    format. For this reason, Apache Spark and Parquet are common choices for building
    the ETL pipelines.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 每个业务单元都管理其自己的部署。将数据引入业务团队使用的DWH是中央团队的责任。这通常意味着中央团队使用的软件需要在不同云之间可移植，并且由管道生成的数据格式是可移植的格式。因此，Apache
    Spark和Parquet是构建ETL管道的常见选择。
- en: If two business units choose to use the same DWH technology, data sharing across
    those two business units becomes simpler, so, where possible, we recommend using
    the same DWH technology (BigQuery, Snowflake, Redshift, etc.) across the organization.
    However, in businesses that grow via acquisitions, this is not always possible.
    Based on our experience, it is better to use a single cloud technology for your
    DWH to get the most out of its capabilities across all departments, even if you
    have a multicloud strategy. We have worked with organizations that were leveraging
    two or even more technologies, and the effort they put into maintaining alignment
    among all the systems was not worth the benefits.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个业务单元选择使用相同的DWH技术，那么这两个业务单元之间的数据共享变得更简单，因此在可能的情况下，我们建议整个组织使用相同的DWH技术（BigQuery，Snowflake，Redshift等）。但是，在通过收购进行业务增长的公司中，这并不总是可行的。根据我们的经验，最好为您的DWH使用单一的云技术，以充分利用其在所有部门中的能力，即使您采用多云战略也是如此。我们曾与利用两种甚至更多技术的组织合作过，他们在维护所有系统之间的一致性方面付出的努力并不值得其带来的好处。
- en: In this section you have seen how you can modernize your data platform via the
    implementation of a hub-and-spoke architecture, putting at the center your DWH.
    You have understood how multiple spokes can gravitate around the central hub,
    your modern DWH, to enable whatever use case you want to implement both batch
    and streaming, leveraging pure SQL language, and to be compliant with data governance
    requirements. In the next section we discuss how the DWH can enable data scientists
    to carry out their activities.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您已经了解了如何通过实施轮毂和辐条架构来现代化您的数据平台，将您的DWH置于中心位置。您已经了解到多个辐条可以围绕中心轮毂——您的现代DWH——旋转，以实现您想要实现的任何用例，无论是批处理还是流处理，都可以利用纯SQL语言，并符合数据治理要求。在下一节中，我们将讨论DWH如何使数据科学家能够开展他们的活动。
- en: DWH to Enable Data Scientists
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DWH以支持数据科学家
- en: Data analysts support data-driven decision making by carrying out ad hoc analysis
    of data to create reports and then operationalize the reports through BI. Data
    scientists aim to automate and scale data-driven decisions using statistics, ML,
    and AI. What do data scientists and data science tools need from a modern cloud
    DWH? As you saw in [Figure 6-1](#hub_and_spoke_architecturesemicolon_a_m), they
    need to interact with the DWH in various ways to execute queries or to simply
    get access to the low-level data. In this section we will have a look at the most
    common tools they can leverage to achieve this goal.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析师通过对数据进行即席分析创建报告，并通过BI将报告操作化，支持数据驱动决策。数据科学家旨在使用统计学、机器学习和人工智能自动化和扩展数据驱动决策。现代云DWH需要从数据科学家和数据科学工具中获得什么？正如您在[Figure
    6-1](#hub_and_spoke_architecturesemicolon_a_m)中所看到的那样，他们需要以各种方式与DWH交互，执行查询或仅仅获取低级别数据访问。在本节中，我们将看看他们可以利用的最常见的工具，以实现这一目标。
- en: As you saw in the previous chapter, data scientists need to carry out experiments
    to try out different forms of automation and learn how the automation will work
    on various slices of historical data. The primary development tools that data
    scientists use for their experimentation, as we have seen earlier, are *notebooks*.
    Therefore, they need to have ways to access data in the DWH efficiently. This
    has to be for both exploratory data analysis through the query interface of the
    DWH and operationalization through the storage interface of the DWH (please refer
    to [Figure 6-1](#hub_and_spoke_architecturesemicolon_a_m)). It is important to
    ensure that your cloud DWH supports both these mechanisms. Let’s have a look at
    how these mechanisms work.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所示，数据科学家需要进行实验，尝试不同形式的自动化，并了解这些自动化在历史数据的各个切片上的工作方式。正如我们之前看到的那样，数据科学家主要使用的开发工具是*笔记本*。因此，他们需要有效地访问DWH中的数据。这不仅限于通过DWH的查询界面进行探索性数据分析，还包括通过DWH的存储界面进行操作化（请参考[Figure
    6-1](#hub_and_spoke_architecturesemicolon_a_m)）。确保您的云DWH支持这两种机制非常重要。让我们看看这些机制是如何运作的。
- en: Query Interface
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询界面
- en: Before automating decision making, data scientists need to carry out copious
    amounts of exploratory data analysis and experimentation. This needs to be done
    interactively. Therefore, there needs to be a fast way to invoke SQL queries from
    the notebook.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动化决策之前，数据科学家需要进行大量的探索性数据分析和实验。这需要以交互方式进行。因此，需要一种快速的方法从笔记本中调用SQL查询。
- en: A Jupyter magic (such as the `%%bigquery` line in the cell in [Figure 6-8](#from_a_notebookcomma_you_can_invoke_sql))
    provides a way to invoke SQL queries from the notebook without boilerplate code.
    The result comes back as a native Python object, called a DataFrame, that can
    be acted upon using pandas, a library of data analysis functions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter魔法（例如在[Figure 6-8](#from_a_notebookcomma_you_can_invoke_sql)中单元格中的`%%bigquery`行）提供了一种从笔记本中调用SQL查询的方法，无需样板代码。结果作为本地Python对象返回，称为DataFrame，可以使用数据分析函数库pandas进行操作。
- en: '![From a notebook, you can invoke SQL queries on the DWH without boilerplate
    code and get the result back as an object that is easy to work with; in this example
    we are invoking BigQuery](assets/adml_0608.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![从笔记本中，您可以调用DWH上的SQL查询，无需样板代码，并将结果返回为易于处理的对象；在此示例中，我们正在调用BigQuery](assets/adml_0608.png)'
- en: Figure 6-8\. From a notebook, you can invoke SQL queries on the DWH without
    boilerplate code and get the result back as an object that is easy to work with;
    in this example we are invoking BigQuery
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8. 从笔记本中，您可以调用DWH上的SQL查询，无需样板代码，并将结果返回为易于处理的对象；在此示例中，我们正在调用BigQuery
- en: It is important to note that this is done without creating in-memory extracts
    of the data. The cloud DWH carries out the SQL queries in a distributed way. The
    notebook server doesn’t need to run on the same computational infrastructure as
    the DWH.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这是在不创建数据的内存提取的情况下完成的。云 DWH 以分布式方式执行 SQL 查询。笔记本服务器无需运行在与 DWH 相同的计算基础设施上。
- en: This combination of using the DWH backend for large-scale data processing and
    the programmatic and visualization capabilities of the notebook frontend is potent
    and necessary for data scientists to be productive.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DWH 后端进行大规模数据处理，并结合笔记本前端的编程和可视化能力，对于数据科学家来说是强大且必要的。
- en: Storage API
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储 API
- en: While the interactive capabilities of the notebook-DWH connection are important
    for exploration and experimentation, they are not what you need for automation.
    For automation, the speed of data access is paramount. It should be possible for
    ML frameworks to bypass the query API and directly access the storage layer of
    the DWH. The storage access should support parallel reading from multiple background
    threads because the common scenario is for the reading of one batch of data to
    happen while the ML accelerator (GPU or TPU) is carrying out heavy computations
    on the previous batch.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管笔记本-DWH连接的交互能力对于探索和实验很重要，但对于自动化而言，数据访问速度至关重要。ML 框架应该能够绕过查询 API，直接访问 DWH 的存储层。存储访问应支持从多个后台线程并行读取，因为常见情况是在
    ML 加速器（GPU 或 TPU）对前一批数据进行大量计算时读取下一批数据。
- en: Instead of using the Query API, therefore, use the Storage API that ML frameworks
    support to read data efficiently and in parallel out of the DWH. Reading data
    from Google BigQuery using the Storage API from Spark and TensorFlow is shown
    in [Example 6-5](#example_six_fivedot_reading_data_direct).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，不再使用查询 API，而是使用 ML 框架支持的存储 API 从 DWH 高效并行地读取数据。使用 Spark 和 TensorFlow 从 Google
    BigQuery 读取数据的存储 API 的示例见[示例 6-5](#example_six_fivedot_reading_data_direct)。
- en: Example 6-5\. Reading data directly from Google BigQuery using Spark (top) or
    TensorFlow (bottom) without going through the query layer
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-5\. 直接使用 Spark（顶部）或 TensorFlow（底部）从 Google BigQuery 读取数据，而无需经过查询层。
- en: '[PRE5]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Query interface and Storage API are the two methods that data scientists use
    to access the data in the DWH to carry out their analysis. Now there is a new
    trend to consider—the ability to implement, train, and use an ML algorithm directly
    in the DWH without having to pull out the data. In the next section we will have
    a look at how it works.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 查询接口和存储 API 是数据科学家用来访问 DWH 中数据以进行分析的两种方法。现在有一个新趋势需要考虑——在 DWH 中直接实现、训练和使用 ML
    算法，而无需提取数据。在下一节中，我们将看看它是如何工作的。
- en: ML Without Moving Your Data
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不移动数据的 ML
- en: Some modern cloud DWHs (BigQuery and Redshift, at the time of writing) also
    support the ability to train ML models on data in the DWH without having to first
    extract data out. They do this by training simple models in SQL and more complex
    models by delegating the job to Vertex AI and SageMaker, respectively. Let’s have
    a look at how you can leverage both training and serving (known as *activation*)
    of your ML algorithm directly from your DWH.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，一些现代化的云 DWH（如 BigQuery 和 Redshift）还支持在 DWH 中训练 ML 模型，而无需首先提取数据。它们通过在
    SQL 中训练简单模型，并通过 Vertex AI 和 SageMaker 分别委托任务来训练更复杂的模型。让我们看看如何直接从您的 DWH 中利用训练和服务（称为
    *激活*）您的 ML 算法。
- en: Training ML models
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练 ML 模型
- en: 'Let’s imagine you want to train an ML model to predict whether or not a user
    will churn given the characteristics of an account and charges on the account:
    it is possible to do everything leveraging the historical data, as shown in [Example 6-6](#example_six_sixdot_develop_and_train_a).
    The types of models trained can be quite sophisticated.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要训练一个 ML 模型，以预测用户是否会流失，给定账户的特征和账户上的费用：可以利用历史数据完成所有操作，如[示例 6-6](#example_six_sixdot_develop_and_train_a)所示。训练的模型类型可以非常复杂。
- en: Example 6-6\. Develop and train a classification ML model in AWS RedShift, leveraging
    historical data you already have in the DWH
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-6\. 在 AWS RedShift 中开发和训练分类 ML 模型，利用您已有的 DWH 中的历史数据。
- en: '[PRE6]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can train a recommender system using historical data on what visitors actually
    purchased, as shown in [Example 6-7](#example_six_sevendot_develop_and_train).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用访客实际购买的历史数据训练推荐系统，如[示例 6-7](#example_six_sevendot_develop_and_train)所示。
- en: Example 6-7\. Develop and train a recommendation ML model in Google BigQuery,
    leveraging historical data you already have in the DWH
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-7\. 在 Google BigQuery 中开发和训练推荐 ML 模型，利用您已有的 DWH 中的历史数据
- en: '[PRE7]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Example 6-8](#example_six_eightdot_anomaly_detection) shows an anomaly detection
    system written using just two SQL statements.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-8](#example_six_eightdot_anomaly_detection) 展示了使用仅两条 SQL 语句编写的异常检测系统。'
- en: Example 6-8\. Anomaly detection model developed in SQL in BigQuery
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-8\. 在 BigQuery 中用 SQL 开发的异常检测模型
- en: '[PRE8]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Being able to do ML using just SQL without having to set up data movement opens
    up predictive analytics to the larger organization. It democratizes ML because
    data analysts leveraging typical BI tools can now implement predictions. It also
    dramatically improves productivity—it is more likely that the organization will
    be able to identify anomalous activity if it requires only two lines of SQL than
    if it requires a six-month project by data scientists proficient in TensorFlow
    or PyTorch.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用 SQL 就能进行 ML，而不必设置数据移动，使得更多组织能够进行预测分析。这使得 ML 民主化，因为利用典型的 BI 工具的数据分析师现在可以实现预测。这也极大地提高了生产力—如果仅需两行
    SQL 就能识别异常活动，那么组织更有可能实现这一目标，而不是依赖熟练掌握 TensorFlow 或 PyTorch 的数据科学家进行为期六个月的项目。
- en: ML training and serving
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ML 训练和服务
- en: 'Training an ML model is not the only ML activity that modern DWHs support.
    They also support the ability to:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ML 训练不是现代 DWH 支持的唯一 ML 活动。它们还支持以下能力：
- en: Export trained ML models in standard ML formats for deployment elsewhere
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练好的 ML 模型导出为标准 ML 格式，以在其他地方部署
- en: Incorporate ML training within the DWH as part of a larger ML workflow
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 ML 训练整合到 DWH 中作为更大 ML 工作流的一部分
- en: Invoke ML models as external functions
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为外部函数调用 ML 模型
- en: Load ML models directly into the DWH to invoke ML predictions efficiently in
    a distributed way
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接将 ML 模型加载到 DWH 中，以分布式方式有效调用 ML 预测
- en: Let’s look at all four of these activities, why they are important, and how
    a modern DWH supports them.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起看看这四种活动，它们为何如此重要，以及现代 DWH 如何支持它们。
- en: Exporting trained ML models
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导出训练好的 ML 模型
- en: ML models trained on data in the DWH can be invoked directly on historical data
    using `ML.PREDICT` in a *batch* mode. However, such a capability is not enough
    for modern applications that require real-time results (e.g., an ecommerce application
    that, based on the connected user, has to decide which ads to display in the page).
    It is necessary to be able to invoke the model *online*—that is, as part of a
    synchronous request for single input or a small set of inputs.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DWH 中训练的 ML 模型可以使用 `ML.PREDICT` 批处理模式直接调用历史数据。然而，对于需要实时结果的现代应用来说（例如基于连接用户的电子商务应用，需要决定在页面中显示哪些广告），这样的能力是不够的。必须能够在线调用模型—也就是作为同步请求的一部分，用于单个输入或少量输入。
- en: You can accomplish this in Redshift by allowing the model to be deployed to
    a SageMaker endpoint and in Google Cloud by exporting the model in SavedModel
    format. From there, you can deploy it to any environment that supports this standard
    ML format. Vertex AI endpoints are supported, of course, but so are SageMaker,
    Kubeflow Pipelines, iOS and Android phones, and Coral Edge TPUs. Vertex AI and
    SageMaker support deployment as microservices and are often used in server-based
    applications, including websites. Deployment to pipelines supports use cases such
    as stream data processing, automated trading, and monitoring. Deployment to iOS
    and Android supports mobile phones, and deployment to Edge TPUs supports custom
    devices such as the dashboards of cars and kiosks.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过允许模型部署到 SageMaker 端点来在 Redshift 中完成此操作，并在 Google Cloud 中以 SavedModel 格式导出模型。从那里，您可以将其部署到支持此标准
    ML 格式的任何环境。当然，支持 Vertex AI 端点，但也支持 SageMaker、Kubeflow Pipelines、iOS 和 Android
    手机以及 Coral Edge TPUs。Vertex AI 和 SageMaker 支持部署为微服务，通常用于包括网站在内的服务器应用程序。部署到管道支持流数据处理、自动交易和监控等用例。部署到
    iOS 和 Android 支持移动电话，部署到 Edge TPU 支持自定义设备，例如汽车和亭子的仪表板。
- en: Using your trained model in ML pipelines
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 ML 流水线中使用您的训练模型
- en: It is rare that a data scientist’s experiment consists of just training an ML
    model. Typically, an experiment will consist of some data preparation, training
    multiple ML models, doing testing and evaluation, choosing the best model, creating
    a new version of the model, setting up an A/B test on a small fraction of the
    traffic, and monitoring the results. Only a few of these operations are done in
    the DWH. Therefore, the steps that are done in the DWH have to be part of a larger
    ML workflow.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有数据科学家的实验仅涉及训练一个 ML 模型。通常，一个实验包括一些数据准备、训练多个 ML 模型、测试和评估、选择最佳模型、创建模型的新版本、在一小部分流量上设置
    A/B 测试以及监控结果。这些操作中只有少数在数据仓库中完成。因此，数据仓库中完成的步骤必须是较大 ML 工作流的一部分。
- en: ML experiments and their workflows are captured in ML pipelines. These pipelines
    consist of a number of containerized steps, a few of which will involve the DWH.
    Therefore, data preparation, model training, model evaluation, etc., have to be
    invocable as containers when they are part of an ML pipeline.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ML 实验及其工作流程都被记录在 ML 管道中。这些管道包含多个容器化步骤，其中少数步骤会涉及数据仓库（DWH）。因此，数据准备、模型训练、模型评估等在成为
    ML 管道的一部分时，必须作为容器调用。
- en: Pipeline frameworks offer convenience functions to invoke operations on a cloud
    DWH. For example, to invoke BigQuery as a containerized operation from Kubeflow
    Pipelines, you can use a BigQuery operator.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 管道框架提供了方便的函数来在云数据仓库上调用操作。例如，要从 Kubeflow Pipelines 中作为容器化操作调用 BigQuery，可以使用 BigQuery
    运算符。
- en: Invoking external ML models
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调用外部 ML 模型
- en: ML is the common way nowadays to make sense of unstructured data such as images,
    video, and natural language text. In many cases, pretrained ML models already
    exist for many kinds of unstructured content and use cases—for example, pretrained
    ML models are available to detect whether some review text contains toxic speech.
    You don’t need to train your own toxic speech detection model. Consequently, if
    your dataset includes unstructured data, it is important to be able to invoke
    an ML model on it.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 当今，ML 是处理非结构化数据（例如图像、视频和自然语言文本）的常见方法。在许多情况下，已存在许多种类非结构化内容和用例的预训练 ML 模型——例如，可以使用预训练
    ML 模型来检测某些评论文本是否含有有毒言论。您无需训练自己的有毒言论检测模型。因此，如果您的数据集包含非结构化数据，能够调用 ML 模型对其进行处理非常重要。
- en: In Snowflake, for example, it is possible to invoke Google Cloud’s Translate
    API using an `EXTERNAL FUNCTION`. You can do this by creating an API integration
    through a gateway and then configuring Snowflake as demonstrated in [Example 6-9](#example_six_ninedot_snowflake_third_par).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Snowflake 中，例如，可以使用 `EXTERNAL FUNCTION` 调用 Google Cloud 的 Translate API。您可以通过通过网关创建
    API 集成，然后按照示例 [Example 6-9](#example_six_ninedot_snowflake_third_par) 配置 Snowflake
    来实现这一点。
- en: Example 6-9\. Snowflake third-party APIs integration example
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-9\. Snowflake 第三方 API 集成示例
- en: '[PRE9]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Given this, it is possible to invoke the Translate API on a column from within
    a `SELECT` statement:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，可以在 `SELECT` 语句中对列调用 Translate API 是可能的：
- en: '[PRE10]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Loading pretrained ML models
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载预训练的 ML 模型
- en: Invoking ML models using external functions can be inefficient because the computation
    does not take advantage of the distributed nature of the DWH. A better approach
    is to load the trained ML model and have the DWH invoke the model within its own
    computing environment.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用外部函数调用 ML 模型可能效率低，因为计算没有充分利用数据仓库的分布式特性。更好的方法是加载已训练的 ML 模型，并在其自身的计算环境中让数据仓库调用模型。
- en: In BigQuery, you can do this by first loading the TensorFlow model and then
    invoking it by using `ML.PREDICT` from within a `SELECT` statement as presented
    in [Example 6-10](#example_six_onezerodot_bigquery_loading).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BigQuery 中，您可以通过首先加载 TensorFlow 模型，然后在 `SELECT` 语句中使用 `ML.PREDICT` 来调用它，如
    [Example 6-10](#example_six_onezerodot_bigquery_loading) 所示。
- en: Example 6-10\. BigQuery loading TensorFlow model stored in Google Cloud Storage
    and predicting results
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-10\. 在 BigQuery 中加载存储在 Google Cloud Storage 中的 TensorFlow 模型并预测结果
- en: '[PRE11]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Because `ML.PREDICT` does not require an external call from the DWH to a deployed
    model service, it is usually much faster. It is also much more secure since the
    model is already loaded and cannot be tampered with.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 `ML.PREDICT` 不需要数据仓库从已部署的模型服务中进行外部调用，通常速度更快。它也更安全，因为模型已加载且无法被篡改。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter we focused on describing how a DWH can be the central core
    of a modern data platform, analyzing how to leverage a hub-and-spoke architecture
    to enable data analysis and ML training and activation. The key takeaways are
    as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于描述数据仓库如何成为现代数据平台的核心，分析如何利用中枢和辐射架构来实现数据分析和机器学习的培训和激活。主要收获如下：
- en: A modern data platform needs to support making insights available to decision
    makers faster.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代数据平台需要支持更快地将洞察力提供给决策者。
- en: The hub-and-spoke architecture is the ideal architecture for organizations with
    no legacy technologies to accommodate. All data is landed into the enterprise
    DWH in as automated a manner as possible.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于没有遗留技术需要适应的组织来说，中枢和辐射架构是理想的架构。所有数据以尽可能自动化的方式落入企业数据仓库。
- en: Data from a large number of SaaS software can be ingested using prebuilt connectors.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用预构建的连接器摄取大量 SaaS 软件的数据。
- en: A DWH can be made to reflect changes made in an OLTP database or ERP system
    using a CDC tool.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CDC 工具可以使数据仓库反映在 OLTP 数据库或 ERP 系统中所做的更改。
- en: It is possible to federate data sources such as blob storage, ensuring that
    some data does not need to be moved into the DWH.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以联合数据源，如 blob 存储，确保某些数据无需移到数据仓库。
- en: It is possible to run external queries on SQL-capable relational databases.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在支持 SQL 的关系数据库上运行外部查询。
- en: To assist business analysis, invest in a SQL tool that pushes operations to
    a database instead of relying on extracting OLAP cubes.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了支持业务分析，投资于一个将操作推送到数据库而不是依赖提取 OLAP 立方体的 SQL 工具。
- en: When you have thousands to millions of customers, it’s more scalable to provide
    a lightweight graphics visual layer that can be embedded within the tool that
    the user is already using.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您有数千到数百万客户时，更可扩展的方式是提供一个轻量级的图形可视化层，可以嵌入用户已经使用的工具内。
- en: Build a semantic layer to capture KPIs, metrics, and dimensions to foster consistency
    and reuse.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立语义层来捕获关键绩效指标、度量和维度，促进一致性和重复利用。
- en: Views provide a way to make the raw data landed in the DWH more usable. Scheduled
    queries to extract view content to tables can be more cost-effective. Materialized
    views provide the best of both worlds.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视图提供了一种使落入数据仓库的原始数据更易用的方法。定期查询以将视图内容提取到表格可能更具成本效益。物化视图集成了两者的优点。
- en: Data governance and security are becoming more crucial because data is exploding,
    architectures are becoming more complex, the number of users accessing and leveraging
    the data is increasing, and legislators are introducing more stringent rules to
    handle the data.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据治理和安全性变得更加重要，因为数据爆炸，架构变得更加复杂，访问和利用数据的用户数量增加，立法者正在引入更严格的规则来处理数据。
- en: Data scientists need to be able to interactively access the DWH from notebooks,
    run queries without boilerplate code, and access data in bulk directly from the
    warehouse’s storage.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学家需要能够从笔记本交互式地访问数据仓库，运行无样板代码的查询，并直接从仓库存储中批量访问数据。
- en: Being able to do ML without any data movement and have the resulting ML model
    be usable in many environments helps foster the use of AI in many parts of the
    business.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够在不移动任何数据的情况下进行机器学习，并使生成的机器学习模型在多个环境中可用，有助于推动业务各个部分的 AI 使用。
- en: Organizationally, some data engineering and governance functions are centralized,
    but different business units transform the data in different ways, and only as
    and when it is needed.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织上，一些数据工程和治理功能是集中管理的，但不同的业务部门以不同方式转换数据，只在需要时才进行。
- en: In the next chapter we will discuss the convergence of the data lake and DWH
    worlds and see how modern platforms can leverage the best of the both paradigms,
    giving end users maximum flexibility and performance.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论数据湖和数据仓库世界的融合，并看看现代平台如何利用两种范式的优势，为最终用户提供最大的灵活性和性能。
