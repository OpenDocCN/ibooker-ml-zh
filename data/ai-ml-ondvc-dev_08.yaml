- en: 'Chapter 8\. Going Deeper: Understanding TensorFlow Lite'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。深入了解 TensorFlow Lite
- en: Underlying all of the machine learning technology that you’ve seen so far in
    this book is TensorFlow. This is a framework that allows you to architect, train,
    and test machine learning models; we had an introduction to this in [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning)
    and [Chapter 2](ch02.html#introduction_to_computer_vision).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中你所见过的所有机器学习技术的基础是 TensorFlow。这是一个允许你设计、训练和测试机器学习模型的框架；我们在[第1章](ch01.html#introduction_to_ai_and_machine_learning)和[第2章](ch02.html#introduction_to_computer_vision)中对此进行了介绍。
- en: TensorFlow models are usually *not* designed for mobile scenarios where one
    has to consider size, battery consumption, and everything else that can impact
    the mobile user experience. To that end, TensorFlow Lite was created with two
    main aims. The first—it could be used to *convert* existing TensorFlow models
    into a format that was smaller and more compact, with an eye on optimizing them
    for mobile. The second is to have an efficient runtime for various mobile platforms
    that could be used for model inference. In this chapter, we’ll explore TensorFlow
    Lite and take a deeper look at the tooling that’s available for you to convert
    models trained with TensorFlow as well as how to use tools to optimize them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 模型通常*不*设计为在移动场景下使用，需要考虑大小、电池消耗以及其他可能影响移动用户体验的因素。为此，TensorFlow Lite
    有两个主要目标。第一个是能够*转换*现有的 TensorFlow 模型为更小更紧凑的格式，并优化其在移动设备上的表现。第二个目标是为各种移动平台提供高效的运行时，用于模型推断。在本章中，我们将探讨
    TensorFlow Lite，并深入了解可用于转换使用 TensorFlow 训练的模型以及如何使用工具来优化它们的工具。
- en: We’ll start with a brief tour of why it’s important, and then we can roll our
    sleeves up and get down to the bits and bytes…
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从简要介绍为什么重要开始，然后我们可以动手去了解其细节和字节...
- en: What Is TensorFlow Lite?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow Lite？
- en: The rationale behind the need for something like TensorFlow Lite was driven
    by several factors. The first is the explosion in the number of personal devices.
    Mobile devices running iOS or Android already outnumber traditional desktop or
    laptops as primary computing devices, and embedded systems outnumber mobile devices.
    The need for these to run machine learning models grows with them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 TensorFlow Lite 的原因是由多个因素驱动的。首先是个人设备数量的激增。运行 iOS 或 Android 的移动设备已经超过了传统的台式机或笔记本电脑，成为主要的计算设备，而嵌入式系统数量更是超过了移动设备。随之而来的是这些设备需要运行机器学习模型的需求也在增长。
- en: But let’s focus on the smartphone here, and the user experience for the smartphone.
    If machine learning models are in the realm of the server, and there is no mobile
    runtime, one would have to wrap their functionality in some form of interface
    that the mobile device could call. For example, for an image classifier, the mobile
    device would have to send the picture to the server, have the server do the inference,
    and return the results. There’s an obvious *latency* issue here beyond the fact
    that *connectivity* is essential. Not everywhere may have connectivity that would
    allow for a quick and easy upload of an image, which can be several megabytes
    of data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们在这里专注于智能手机，以及智能手机的用户体验。如果机器学习模型在服务器端，而没有移动运行时，那么用户必须以某种形式封装其功能，以便移动设备调用。例如，对于图像分类器，移动设备必须将图片发送到服务器，服务器进行推断，然后返回结果。这里显然存在一个*延迟*问题，除了*连接性*至关重要外。并非每个地方都有足够快速和便捷上传数兆字节数据的连接。
- en: And of course, there’s *privacy*. Many scenarios involve using very personal
    data—such as the aforementioned photos—and requiring them to be uploaded to a
    server in order for functionality to work can violate the user’s privacy, and
    they probably would reject using your app as a result.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有*隐私*。许多场景涉及使用非常个人化的数据，比如前面提到的照片，要求它们上传到服务器以便功能正常运行可能会侵犯用户的隐私权，因此他们可能会拒绝使用您的应用。
- en: Having a framework that allows models to work on the device so that there’s
    no lag in transferring the data to a third party and no connectivity dependency,
    while also maintaining the user’s privacy, is paramount for machine learning to
    be a viable scenario on mobile.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个框架，使模型能够在设备上运行，这样数据就不会延迟传输到第三方，也不会依赖连接性，同时保护用户的隐私，对于机器学习在移动设备上成为可行方案至关重要。
- en: Enter TensorFlow Lite. As mentioned in the introduction, it’s designed for you
    to convert your TensorFlow model to a compact format for mobile, as well as the
    runtime for inference on that mobile platform.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 进入TensorFlow Lite。如介绍中所述，它专为您将TensorFlow模型转换为移动设备的紧凑格式以及在该移动平台上进行推理的运行时而设计。
- en: What’s exciting about this in particular is that it can allow for a whole new
    generation of products and scenarios. Think about what happens when new platforms
    appear, and the innovation that follows. Consider, for example, when the smartphone
    first came on the scene—a device that was loaded with sensors like GPS or a camera
    and connected to the internet. Think about how difficult it was to find your way
    around a new location, particularly one that primarily used different languages,
    using a paper map! Now, you can use your device to pinpoint your location, give
    it your destination, and it can smartly route you to the quickest way there with
    step-by-step instructions—even if you are walking, using an augmented reality
    interface that shows you the route to follow. While you *could* have done that
    with your laptop if you were able to connect it to the internet somehow, it wasn’t
    really feasible. With the emergence of ML models on mobile, a whole new platform
    awaits for interesting scenarios to be implemented—these may be things that *could*
    be implemented without ML, but are likely too difficult to do when not using models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 特别令人兴奋的是，这种方法可以为全新的产品和场景带来全新的可能性。想象一下当新平台出现时会发生什么，以及随之而来的创新。例如，当智能手机首次登场时——一种装载了GPS或相机等传感器并连接到互联网的设备。想象一下在使用纸质地图时在一个主要使用不同语言的新地点四处寻找方向是多么困难！现在，您可以使用您的设备精确定位您的位置，告诉它您的目的地，它可以智能地为您规划到达那里的最快路径，并提供逐步指引——即使您在步行，使用增强现实界面来显示您要遵循的路线。虽然如果您能够以某种方式将笔记本电脑连接到互联网的话，您*可能*也能做到这一点，但实际上是不可行的。随着移动设备上ML模型的出现，一个全新的平台正在等待着有趣的场景实施——这些可能是*可以*不使用ML实现的事情，但在不使用模型时很可能过于困难。
- en: Consider, for example, [Figure 8-1](#live_translation_on_a_camera_screen), where
    there’s text in Chinese that I can’t read. I’m in a restaurant and have some food
    allergies. With Google Translate, using ML models on-device for translation, and
    with another model able to do text recognition for what’s in the camera’s field
    of view, I can now have a live, visual, translation of what’s in front of me.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，请考虑[图8-1](#live_translation_on_a_camera_screen)，摄像头屏幕上有些我看不懂的中文文字。我在一家餐馆里，有些食物过敏。通过使用设备上的ML模型进行翻译，并且使用另一个模型来对摄像头视野中的文字进行识别，现在我可以实时、视觉地翻译我面前的内容了。
- en: '![](assets/aiml_0801.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0801.png)'
- en: Figure 8-1\. Live translation on a camera screen
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. 摄像头屏幕上的实时翻译
- en: Consider how difficult it would be to do something like this *without* machine
    learning. How to write code to do optical character recognition of any character
    in any language. And then, once you’ve done that, how to then translate those
    without a round trip to a translation server. It simply wouldn’t be feasible.
    But ML, and in particular ML on the device, is making this possible.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下如果没有机器学习，要做类似这样的事情是多么困难。如何编写代码来进行任何语言中任何字符的光学字符识别。然后，一旦你完成了这个，如何在不经过翻译服务器的情况下翻译它们。这简直是不可行的。但是机器学习，特别是设备上的机器学习，正在使这成为可能。
- en: Getting Started with TensorFlow Lite
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Lite 入门指南
- en: To understand TensorFlow Lite, I think it’s easiest to get hands-on and work
    with it right away. Let’s start by exploring the *converter* that takes a TensorFlow
    model and converts it to the TensorFlow Lite format. We’ll then take the model
    it creates and implement it in a simple Android app.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解TensorFlow Lite，我认为最简单的方法是立即动手并开始使用它。让我们首先探索将TensorFlow模型转换为TensorFlow Lite格式的*转换器*。然后，我们将使用它创建的模型在一个简单的Android应用程序中实施它。
- en: 'In [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning), we did some
    code that we called the “Hello World” of machine learning, with a very simple
    linear regression to build a model that could predict the relationship between
    two numbers x and y as y = 2x − 1\. As a recap, here’s the Python code for training
    the model in TensorFlow:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.html#introduction_to_ai_and_machine_learning)中，我们进行了一些称为机器学习的“Hello
    World”代码，使用非常简单的线性回归来建立一个模型，该模型可以预测两个数字x和y之间的关系，即y = 2x − 1\. 简而言之，以下是在TensorFlow中训练模型的Python代码：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After training for 500 epochs, it gave the following output on the print statements:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练了500个时期之后，它在打印语句上产生了以下输出：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So it predicted that if x is 10, then y is 18.984955, which is very close to
    the 19 we’d expect with the formula y = 2x − 1\. This is because the single neuron
    in the neural network learned a weight of 1.9978193, and a bias of −0.99323905\.
    So, given a very small amount of data, it inferred a relationship of y = 1.9978193x
    − 0.99323905, which is pretty close to our desired y = 2x − 1.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，预测如果x为10，则y为18.984955，这非常接近于我们使用y = 2x − 1公式预期的19。这是因为神经网络中的单个神经元学习了权重为1.9978193和偏差为−0.99323905。因此，基于非常少量的数据，它推断出了y
    = 1.9978193x − 0.99323905的关系，这非常接近我们期望的y = 2x − 1。
- en: So, can we now get this working on Android, instead of running it in the cloud
    or on our developer workstation? The answer, of course, is yes. And the first
    step is to save our model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，现在我们能在Android上使其工作，而不是在云端或开发者工作站上运行吗？答案当然是肯定的。第一步是保存我们的模型。
- en: Save the Model
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存模型
- en: TensorFlow uses several different methods of saving models, but the most standardized
    across the TensorFlow ecosystem is the SavedModel format. This will save the model
    in the *.pb* (for protobuf) file format, as a representation of the frozen model,
    with associated directories containing any model assets or variables. This has
    the decided advantage of separation of architecture from state, so that other
    states could be added later if we wanted, or updates to the model could be shipped
    without needing to reship any on-model assets, which themselves may be quite large.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow使用多种不同的方法来保存模型，但在TensorFlow生态系统中最标准的是SavedModel格式。这将以*.pb*（protobuf）文件格式保存模型，作为冻结模型的表示，附带包含任何模型资产或变量的相关目录。这种方式的明显优势在于将架构与状态分离，因此如果需要，可以随时添加其他状态，或者更新模型而无需重新发送任何模型资产，这些资产本身可能相当庞大。
- en: 'To save with this format, you just specify the output directory and then call
    `tf.saved_model.save()` like this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这种格式保存，只需指定输出目录，然后像这样调用`tf.saved_model.save()`：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can see the directory structure that gets saved out in [Figure 8-2](#the_directory_structure_after_saving_a).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到保存的目录结构在[图8-2](#the_directory_structure_after_saving_a)中。
- en: '![](assets/aiml_0802.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0802.png)'
- en: Figure 8-2\. The directory structure after saving a model
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2。保存模型后的目录结构
- en: As this is a very simple model, the *variables* file has only one shard. Larger
    models will be split into multiple ones, hence the naming *variables.data-00000-of-00001*.
    This model doesn’t use any assets, so that folder will be empty.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个非常简单的模型，*variables*文件只有一个分片。更大的模型将被分割为多个分片，因此命名为*variables.data-00000-of-00001*。此模型不使用任何资产，因此该文件夹将为空。
- en: Convert the Model
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换模型
- en: Converting the model is as easy as creating an instance of the converter from
    the saved model, and calling its convert method. This will give you a model in
    the TFLite format that you can then save out by writing its bytes to a file stream.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型转换为TFLite格式只需创建保存模型的转换器实例，并调用其转换方法即可。然后，通过将其字节写入文件流来保存模型。
- en: 'Here’s the code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The process of writing the bytes will return a number, and this is the number
    of bytes written. With different versions of the converter this might change,
    but the one at the time of writing wrote out a file called *model.tflite* that
    was 896 bytes in size. This encapsulates the entire trained model, including architecture
    and learned weights.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 写入字节的过程将返回一个数字，这是写入的字节数。不同版本的转换器可能会更改这个数字，但在撰写本文时，写出了一个名为*model.tflite*的文件，大小为896字节。这包括整个训练过的模型，包括架构和学习的权重。
- en: Note
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While using the Python APIs as shown earlier is the recommended way to perform
    the model conversion, the TensorFlow team also offers a command-line interpreter
    should you want to use that instead. You can learn more about it at [*https://www.tensorflow.org/lite/convert*](https://www.tensorflow.org/lite/convert).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用如前所示的Python API是执行模型转换的推荐方法，但TensorFlow团队还提供了命令行解释器，如果您愿意，也可以使用该解释器。您可以在[*https://www.tensorflow.org/lite/convert*](https://www.tensorflow.org/lite/convert)了解更多信息。
- en: Testing the Model with a Standalone Interpreter
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用独立解释器测试模型
- en: Before trying the model on iOS or Android, you should use the standalone interpreter
    from TensorFlow Lite to see if it works well. This interpreter runs in the Python
    environment, so it can also be used in embedded systems that can run Python, such
    as the Linux-based Raspberry Pi!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试在iOS或Android上使用该模型之前，您应该先使用TensorFlow Lite的独立解释器来检查它是否运行良好。这个解释器在Python环境中运行，因此也可以在可以运行Python的嵌入式系统上使用，比如基于Linux的树莓派！
- en: The next step is to load the model into the interpreter, allocate tensors that
    will be used for inputting data to the model for prediction, and then read the
    predictions that the model outputs. This is where using TensorFlow Lite, from
    a programmer’s perspective, greatly differs from using TensorFlow. With TensorFlow
    you can just say `model.predict(`*`something`*`)` and get the results, but because
    TensorFlow Lite won’t have many of the dependencies that TensorFlow does, particularly
    in non-Python environments, you now have to get a bit more low level and deal
    with the input and output tensors, formatting your data to fit them and parsing
    the output in a way that makes sense for your device.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将模型加载到解释器中，分配张量以用于将数据输入模型进行预测，并读取模型输出的预测结果。这是在使用 TensorFlow Lite 时，从程序员的角度来看，与使用
    TensorFlow 有很大不同的地方。在 TensorFlow 中，您可以直接使用 `model.predict(`*`something`*`)` 并获取结果，但是由于
    TensorFlow Lite 在非 Python 环境中不会有许多 TensorFlow 的依赖项，因此现在您需要更低级别地处理输入和输出张量，格式化您的数据以适应它们，并以对您的设备有意义的方式解析输出。
- en: 'First, load the model and allocate the tensors:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载模型并分配张量：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then you can get the input and output details from the model, so you can begin
    to understand what data format it expects and what data format it will provide
    back to you:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以从模型获取输入和输出详细信息，从而开始理解它期望的数据格式以及它将向您提供的数据格式：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You’ll get a lot of output!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您将会得到大量的输出！
- en: 'First, let’s inspect the input parameter. Note the shape setting, which is
    an array of type [1,1]. Also note the class, which is `numpy.float32`. These settings
    will dictate the shape of the input data and its format:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查输入参数。注意形状设置，它是一个类型为 [1,1] 的数组。还请注意类别，它是 `numpy.float32`。这些设置将决定输入数据的形状及其格式：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'So, in order to format the input data, you’ll need to use code like this to
    define the input array shape and type if you want to predict the y for x = 10.0:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了格式化输入数据，您需要使用以下代码来定义输入数组的形状和类型，如果您想预测 x = 10.0 时的 y：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The double brackets around the 10.0 can cause a little confusion—the mnemonic
    I use for the `array[1,1]` here is to say that there is one list, giving us the
    first set of [], and that list contains just one value, which is `[10.0]`, thus
    giving `[[10.0]]`. It can also be confusing that the shape is defined as `dtype=int32`,
    whereas you’re using `numpy.float32`. The `dtype` parameter is the datatype defining
    the shape, not the contents of the list that is encapsulated in that shape. For
    that, you’ll use the class.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕 10.0 的双括号可能会引起一些混淆——我在这里使用的记忆方法是说，这里有一个列表，给我们了第一个 [] 的设置，并且该列表只包含一个值，即 `[10.0]`，因此得到
    `[[10.0]]`。也可能令人困惑的是，形状被定义为 `dtype=int32`，而您使用的是 `numpy.float32`。`dtype` 参数是定义形状的数据类型，而不是封装在该形状中的列表内容。对于后者，您将使用类别。
- en: You can also print the output details with `print(output_details)`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 `print(output_details)` 打印输出详细信息。
- en: 'These are very similar, and what you want to keep an eye on here is the shape.
    As it’s also an array of type `[1,1]`, you can expect the answer to be `[[y]]`
    in much the same way as the input was `[[x]]`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些非常相似，您需要关注的是形状。因为它也是一个类型为 `[1,1]` 的数组，您可以期望答案会以类似于输入为 `[[x]]` 的方式为 `[[y]]`：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To get the interpreter to do the prediction, you set the input tensor with
    the value to predict, telling it what input value to use:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要让解释器进行预测，您需要将输入张量设置为要预测的值，并告知它要使用的输入值：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The input tensor is specified using the index of the array of input details.
    In this case, you have a very simple model that only has a single input option,
    so it’s `input_details[0]`, and you’ll address it at the index. Input details
    item 0 has only one index, indexed at 0, and it expects a shape of `[1,1]` as
    defined earlier. So, you put the `to_predict` value in there. Then you invoke
    the interpreter with the `invoke` method.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输入张量是使用输入详细信息数组的索引来指定的。在这种情况下，您有一个非常简单的模型，只有一个输入选项，因此它是 `input_details[0]`，您将在索引处引用它。输入详细信息项
    0 只有一个索引，索引为 0，并且它期望的形状如前所述为 `[1,1]`。因此，您将 `to_predict` 值放在其中。然后，使用 `invoke` 方法调用解释器。
- en: 'You can then read the prediction by calling `get_tensor` and supplying it with
    the details of the tensor you want to read:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过调用 `get_tensor` 并提供您想要读取的张量的详细信息来读取预测值：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Again, there’s only one output tensor, so it will be `output_details[0]`, and
    you specify the index to get the details beneath it, which will have the output
    value.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，只有一个输出张量，因此它将是 `output_details[0]`，您指定索引以获取其下的详细信息，其中将包含输出值。
- en: 'So, for example, assume you run this code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，举例来说，假设你运行以下代码：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You should see output like:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到如下输出：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: where 10 is the input value and 18.97 is the predicted value, which is very
    close to 19, which is 2x – 1 when x = 10\. For the reasons why it’s not 19, look
    back to [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning)!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中10是输入值，18.97是预测值，非常接近于19，即当x=10时2x-1。为什么不是19的原因，请回顾[第1章](ch01.html#introduction_to_ai_and_machine_learning)！
- en: Note that you might have seen slightly different results (such as 18.984) in
    [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning), and these will
    occur for two main reasons. First, the neurons start from a different random initialization
    state, so their final value will be slightly different. Also, when compressing
    to a TFLite model, optimizations will be made that impact the final results. Keep
    this in mind when creating more complex models later—it’s important to keep an
    eye on any accuracy impact by mobile conversion.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可能已经在[第1章](ch01.html#introduction_to_ai_and_machine_learning)中看到了稍微不同的结果（例如18.984），这些结果由于两个主要原因而会发生。首先，神经元从不同的随机初始化状态开始，因此它们的最终值会略有不同。此外，在将模型压缩为
    TFLite 模型时，会进行影响最终结果的优化。以后创建更复杂模型时，请记住这一点——重要的是要注意移动转换可能对准确性的影响。
- en: Now that we’ve tested the model using the standalone interpreter, and it looks
    to be performing as expected, for the next step, let’s build a simple Android
    application and see what using the model looks like there!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经使用独立解释器测试了模型，并且它看起来表现如预期一样。下一步，让我们构建一个简单的 Android 应用程序，看看在那里使用模型是什么样子！
- en: Create an Android App to Host TFLite
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个 Android 应用程序来托管 TFLite
- en: Create an Android app using Android Studio using the single activity template.
    If you’re not familiar with this, go through all the steps in [Chapter 3](ch03.html#introduction_to_ml_kit).
    Check them out there!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Android Studio 使用单个活动模板创建一个 Android 应用程序。如果您对此不熟悉，请阅读[第3章](ch03.html#introduction_to_ml_kit)中的所有步骤。在那里查找它们！
- en: 'Edit your *build.gradle* file to include the TensorFlow Lite runtime:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑您的*build.gradle*文件以包含 TensorFlow Lite 运行时：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here I’ve used version 2.4.0\. To get the latest version, you can check out
    the current versions available on the [Bintray website](https://oreil.ly/Y3kb0).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我使用了版本2.4.0。要获取最新版本，您可以查看[Bintray 网站](https://oreil.ly/Y3kb0)上提供的当前版本。
- en: 'You’ll also need a new setting within the `android{}` section, as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要在`android{}`部分中添加一个新的设置，如下所示：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This step prevents the compiler from compressing your *.tflite* file. Android
    Studio compiles assets to make them smaller, so that the download time from the
    Google Play Store will be reduced. However, if the *.tflite* file is compressed,
    the TensorFlow Lite interpreter won’t recognize it. To ensure that it doesn’t
    get compressed, you need to set `aaptOptions` to `noCompress` for *.tflite* files.
    If you used a different extension (some people just use *.lite*), make sure you
    have that here.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤可防止编译器压缩您的*.tflite*文件。 Android Studio 会编译资源以减小其大小，从而减少从 Google Play 商店下载的时间。但是，如果压缩*.tflite*文件，则
    TensorFlow Lite 解释器将无法识别它。为确保它不会被压缩，您需要为*.tflite*文件设置`aaptOptions`为`noCompress`。如果您使用了不同的扩展名（有些人只使用*.lite*），请确保在此处配置。
- en: You can now try building your project. The TensorFlow Lite libraries will be
    downloaded and linked.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以尝试构建您的项目。 TensorFlow Lite 库将被下载和链接。
- en: 'Next, update your activity file (you can find this in your layout directory)
    to create a simple UI. This will have an edit text in which you can type a value,
    and a button that you press in order to trigger the inference:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，更新您的活动文件（您可以在布局目录中找到它）以创建一个简单的用户界面。这将包含一个编辑文本框，您可以在其中输入一个值，并且一个按钮，用于触发推断：
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Before you start coding you’ll need to import the TFLite file to your app. You
    can see how to do that next.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始编码之前，您需要将 TFLite 文件导入到您的应用程序中。接下来您可以看到如何做到这一点。
- en: Import the TFLite File
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入 TFLite 文件
- en: The first thing to do is create an *assets* folder in your project. To do this,
    navigate to the *app/src/main* folder in the project explorer, right-click on
    the *main* folder and select New Directory. Call it *assets*. Drag the *.tflite*
    file that you downloaded after training the model into that directory. If you
    didn’t create this file earlier, you can find it in the book’s GitHub repository.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是在您的项目中创建一个*assets*文件夹。为此，请在项目资源管理器中导航至*app/src/main*文件夹，在*main*文件夹上右键单击，然后选择新建文件夹。将其命名为*assets*。将您在训练模型后下载的*.tflite*文件拖放到该目录中。如果您之前没有创建此文件，您可以在书籍的
    GitHub 代码库中找到它。
- en: 'If you get a warning about the file not being in the right directory, so Model
    Binding is disabled, it’s safe to ignore it. Model Binding is something we’ll
    explore later that works for a number of fixed scenarios: it allows you to easily
    import a *.tflite* model without many of the manual steps illustrated in this
    example. Here we’re going a little lower level into the nuts and bolts of how
    to use a TFLite file within Android Studio.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你收到关于文件不在正确目录中的警告，即模型绑定被禁用，请安全地忽略它。模型绑定是我们稍后将探索的内容，它适用于许多固定场景：它允许你轻松地导入 *.tflite*
    模型，而不需要本示例中所示的许多手动步骤。在这里，我们将深入到如何在 Android Studio 中使用 TFLite 文件的基本操作中。
- en: After adding the asset to Android Studio, your project explorer should look
    a little like [Figure 8-3](#adding_a_tflite_file_as_an_assets_file).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将资产添加到 Android Studio 后，你的项目资源管理器应该看起来像图 [8-3](#adding_a_tflite_file_as_an_assets_file)
    一样。
- en: '![](assets/aiml_0803.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0803.png)'
- en: Figure 8-3\. Adding a TFLite file as an assets file
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 将 TFLite 文件添加为资产文件
- en: Now that everything is set up, we can start coding!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切准备就绪，我们可以开始编码了！
- en: Write Kotlin Code to Interface with the Model
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写 Kotlin 代码与模型接口
- en: Despite the fact that you’re using Kotlin, your source files are in the *java*
    directory!. Open this, and you’ll see a folder with your package name. Within
    that you should see your *MainActivity.kt* file. Double-click this file to open
    it in the code editor.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你正在使用 Kotlin，但你的源文件位于 *java* 目录下！打开它，你会看到一个与你的包名相对应的文件夹。在这个文件夹中，你会看到 *MainActivity.kt*
    文件。双击此文件以在代码编辑器中打开它。
- en: 'First, you’ll need a helper function that loads the TensorFlow Lite model from
    the *assets* directory:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要一个帮助函数，从 *assets* 目录加载 TensorFlow Lite 模型：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As the *.tflite* file is effectively a binary blob of weights and biases that
    the interpreter will use to build an internal neural network model, it’s a `ByteBuffer`
    in Android terms. This code will load the file at `modelPath` and return it as
    a `ByteBuffer`. (Note that earlier you made sure not to compress this file type
    at compile time so that Android could recognize the contents of the file.)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *.tflite* 文件实际上是解释器将用于构建内部神经网络模型的权重和偏置的二进制 blob，在 Android 术语中它是一个 `ByteBuffer`。此代码将从
    `modelPath` 加载文件，并将其作为 `ByteBuffer` 返回。（请注意，早些时候你确保在编译时不压缩此文件类型，以便 Android 能够识别文件的内容。）
- en: 'Then, within your activity, at the class level (i.e., just below the class
    declaration, not within any class functions), you can add the declarations for
    the model and interpreter:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，在你的活动中，在类级别（即在类声明下方，不在任何类函数内部），你可以添加模型和解释器的声明：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So, in this case, the interpreter object that does all the work will be called
    `tflite` and the model that you’ll load into the interpreter as a `ByteBuffer`
    is called `tflitemodel`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，执行所有工作的解释器对象将被称为 `tflite`，而加载到解释器中作为 `ByteBuffer` 的模型将被称为 `tflitemodel`。
- en: 'Next, in the `onCreate` method, which gets called when the activity is created,
    add some code to instantiate the interpreter and load `model.tflite` into it:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在 `onCreate` 方法中，当活动创建时调用，添加一些代码来实例化解释器，并加载 `model.tflite`：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Also, while you’re in `onCreate`, add the code for the two controls that you’ll
    interact with—the `EditText` where you’ll type a value, and the `Button` that
    you’ll press to get an inference:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在 `onCreate` 中，还要添加两个你将与之交互的控件的代码——`EditText` 用于输入数值，和 `Button` 用于执行推断：
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You’ll also need to declare the `EditText` at the class level alongside `tflite`
    and `t⁠f​l⁠i⁠t⁠e⁠m⁠o⁠d⁠e⁠l`, as it will be referred to within the next function.
    You can do that with:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在下一个函数中将需要在类级别声明 `EditText`，以及 `tflite` 和 `tflitemodel`，因为它将在其中使用：
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, it’s time to do the inference. You can do this with a new function
    called `doInference`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候进行推断了。你可以使用一个名为 `doInference` 的新函数来完成：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Within this function you can gather the data from the input, pass it to TensorFlow
    Lite to get an inference, and then display the returned value.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，你可以收集输入数据，传递给 TensorFlow Lite 进行推断，然后显示返回的值。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In this case, the inference is very simple. For complex models it can be a long-running
    process that can block the UI thread, which you’ll need to keep in mind when building
    your own apps.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，推断非常简单。对于复杂模型，可能会是一个长时间运行的过程，可能会阻塞UI线程，这是在构建你自己的应用程序时需要牢记的事项。
- en: 'The `EditText` control, where you’ll enter the number, will provide you with
    a string, which you’ll need to convert to a float:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`EditText` 控件用于输入数字，将会提供一个字符串，你需要将其转换为浮点数：'
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As you’ll recall from [Chapter 1](ch01.html#introduction_to_ai_and_machine_learning)
    and [Chapter 2](ch02.html#introduction_to_computer_vision), when feeding data
    into the model you’ll typically need to format it as a NumPy array. Being a Python
    construct, NumPy isn’t available in Android, but you can just use a `FloatArray`
    in this context. Even though you’re only feeding in one value, it still needs
    to be in an array, roughly approximating a tensor:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第1章](ch01.html#introduction_to_ai_and_machine_learning)和[第2章](ch02.html#introduction_to_computer_vision)中回顾的，当将数据输入模型时，通常需要将其格式化为NumPy数组。作为Python的构造，NumPy在Android上不可用，但在这种情况下您可以使用`FloatArray`。即使您只输入一个值，它仍然需要在一个数组中，粗略地近似一个张量：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The model will return a stream of bytes to you that will need to be interpreted.
    As you know you’re getting a float value out of the model, and given that a float
    is 4 bytes, you can set up a `ByteBuffer` of 4 bytes to receive the output. There
    are several ways that bytes can be ordered, but you just need the default, native
    order:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将向您返回一系列字节，需要解释。如您所知，模型输出一个浮点值，考虑到浮点数占据4字节，您可以设置一个4字节的`ByteBuffer`来接收输出。字节可以按多种方式排序，但您只需要默认的本地顺序：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To perform the inference, you call the run method on the interpreter, passing
    it the input and output values. It will then read from the input value and write
    to the output value:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行推断，您需要在解释器上调用运行方法，传递输入和输出值。然后它将从输入值读取并写入输出值：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is written to the `ByteBuffer`, whose pointer is now at the end
    of the buffer. To read it, you have to reset it to the beginning of the buffer:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 输出被写入到`ByteBuffer`中，其指针现在位于缓冲区的末尾。要读取它，您必须将其重置为缓冲区的开头：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'And now you can read the contents of the `ByteBuffer` as a float:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以将`ByteBuffer`的内容作为一个浮点数读取：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If you want to display this to the user, you can then use an `AlertDialog`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要向用户显示这个信息，可以使用`AlertDialog`：
- en: '[PRE28]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can now run the app and try it for yourself! You can see the results in
    [Figure 8-4](#running_the_inference), where I entered a value of 10 and the model
    gave me an inference of 18.984955, which is displayed in an alert box. Note that
    your value may be different for the reasons discussed earlier. When you train
    a model, the neural network begins by being randomly initialized, so when it converges
    it may be doing so from a different starting point, and as such your model may
    have slightly different results.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以运行应用并尝试自己！您可以在[图8-4](#running_the_inference)中看到结果，我输入了一个值为10，模型给出了18.984955的推断结果，显示在一个警报框中。请注意，由于前面讨论的原因，您的值可能会有所不同。当训练模型时，神经网络开始时是随机初始化的，因此当它收敛时可能是从不同的起点开始的，因此您的模型可能会有稍微不同的结果。
- en: '![](assets/aiml_0804.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0804.png)'
- en: Figure 8-4\. Running the inference
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4\. 运行推断
- en: Going Beyond the Basics
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越基础知识
- en: This example was very trivial—you had a model that took a single input value
    and provided a single output value. Both of these were floats, which take 4 bytes
    to store, so you could create byte buffers with just 4 bytes in each and know
    that they contained the single value. So, when using more complex data, you’ll
    have to work to get your data into the format that the model expects, which will
    require a lot of engineering on your part. Let’s look at an example using an image.
    In [Chapter 9](ch09.html#creating_custom_models) we’ll look at Model Maker, which
    is a really useful tool for abstracting the complexity of using TFLite on Android
    or iOS for common scenarios—including image classification like this—but I think
    it’s still a useful exercise to explore an under-the-hood scenario for how to
    manage data in and out of a model for when you go outside the common scenarios!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例非常简单——您有一个模型，它接受单个输入值并提供单个输出值。这两个值都是浮点数，占据4个字节的存储空间，因此您可以仅创建每个字节缓冲区中的4个字节，并知道它们包含单个值。因此，在使用更复杂的数据时，您需要努力将数据格式化为模型期望的格式，这将需要您进行大量的工程工作。让我们看一个使用图像的示例。在[第9章](ch09.html#creating_custom_models)中，我们将看到模型制造器，这是一个在Android或iOS上使用TFLite进行常见场景——包括像这样的图像分类的复杂性抽象的非常有用的工具，但我认为，探索如何管理模型的数据进出对于您超出常见情景的需求仍然是一种有用的练习！
- en: For example, let’s start with an image like that in [Figure 8-5](#image_of_a_dog_to_interpret),
    which is a simple image of a dog that happens to be 395 × 500 pixels. This is
    used in a model that can tell the difference between cats and dogs. I won’t go
    into detail on how to *create* that model, but there’s a notebook in the repository
    for this book that does that for you, as well as a sample app to handle the inference.
    You can find the training code in [*Chapter8_Lab2.ipynb*](https://oreil.ly/mohak),
    and the app is called “cats_vs_dogs.”
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们从像[图8-5](#image_of_a_dog_to_interpret)中的图片开始，这是一张狗的简单图片，尺寸是395 × 500像素。这张图片用于一个可以区分猫和狗的模型。我不会详细介绍如何*创建*这个模型，但是这本书的仓库中有一个笔记本可以为你完成这部分工作，还有一个样例应用来处理推断。你可以在[*Chapter8_Lab2.ipynb*](https://oreil.ly/mohak)中找到训练代码，应用程序名为“cats_vs_dogs”。
- en: '![](assets/aiml_0805.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0805.png)'
- en: Figure 8-5\. Image of a dog to interpret
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5\. 解释狗的图片
- en: 'The first thing you need to do is resize it to 224 × 224 pixels, the image
    dimensions that the model was trained on. This can be done in Android using the
    Bitmap libraries. For example, you can create a new 224 × 224 bitmap with:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要做的事情是将其调整为224 × 224像素，即模型训练时使用的图片尺寸。在Android中可以使用Bitmap库来完成这个操作。例如，你可以创建一个新的224
    × 224位图：
- en: '[PRE29]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: (In this case, bitmap contains the raw image loaded as a resource by the app.
    The full app is available in the book’s GitHub repo.)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: （在这种情况下，位图包含由应用程序加载为资源的原始图片。完整的应用程序可以在本书的GitHub仓库中找到。）
- en: 'Now that it’s the right size, you have to reconcile how the image is structured
    in Android with how the model expects it to be structured. If you recall, when
    training models in earlier chapters in the book, you fed in images as normalized
    tensors of values. For example, an image like this would be (224, 224, 3): 224
    × 224 is the image size, and 3 is the color depth. The values were also all normalized
    to between 0 and 1.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，图片尺寸已经调整到合适大小，接下来需要调整Android中的图片结构，使其与模型期望的结构一致。如果你回忆一下，本书前几章在训练模型时，输入的是归一化后的数值张量作为图片。例如，这样一张图片的尺寸为（224,
    224, 3）：224 × 224表示图片尺寸，3表示颜色深度。这些数值也都归一化到了0到1之间。
- en: 'So, in summary, you need 224 × 224 × 3 float values between 0 and 1 to represent
    the image. To store that in a `ByteArray`, where 4 bytes make a float, you can
    use this code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结一下，你需要224 × 224 × 3个介于0和1之间的浮点值来表示这张图片。为了将其存储在一个`ByteArray`中，其中4个字节表示一个浮点数，你可以使用以下代码：
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Our Android image, on the other hand, has each pixel stored as a 32-bit integer
    in an ARGB value. This might look something like 0x0010FF10 for a particular pixel.
    The first two values are the transparency, which you can ignore, and the rest
    are for RGB (i.e., 0x10 for red, 0xFF for green, and 0x10 for blue). The simple
    normalization you’ve been doing to this point is just to divide the R, G, B channel
    values by 255, which would give you .06275 for red, 1 for green, and .06275 for
    blue.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们的Android图片中，每个像素都以32位整数的ARGB值存储。一个特定像素可能看起来像0x0010FF10。前两个值是透明度，可以忽略，其余的用于RGB（例如，红色为0x10，绿色为0xFF，蓝色为0x10）。到目前为止，你一直在做的简单归一化是将R、G、B通道值除以255，这将得到红色的.06275，绿色的1，和蓝色的.06275。
- en: 'So, to do this conversion, let’s first turn our bitmap into an array of 224
    × 224 integers, and copy the pixels in. You can do this using the `getPixels`
    API:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了进行这种转换，首先将我们的位图转换为一个224 × 224整数数组，并将像素复制进去。你可以使用`getPixels` API来完成这个操作：
- en: '[PRE31]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You can find details on the getPixels API explaining these parameters in the
    [Android developer documentation](https://oreil.ly/EFs1Q).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[Android开发者文档](https://oreil.ly/EFs1Q)中找到有关getPixels API的详细信息，解释了这些参数。
- en: 'Now you’ll need to iterate through this array, reading the pixels one by one
    and converting them into normalized floats. You’ll use bit shifting to get the
    particular channels. For example, consider the value 0x0010FF10 from earlier.
    If you shift that by 16 bits to the right, you’ll get 0x0010 (with the FF10 being
    “lost”). If you then “and” that by 0xFF, you’ll get 0x10, keeping just the bottom
    two numbers. Similarly, if you had shifted by 8 bits to the right you’d have 0x0010FF,
    and performing an “and” on that would give you 0xFF. This technique (typically
    called *masking*) allows you to quickly and easily strip out the relevant bits
    that make up the pixels. You can use the `shr` operation on an integer for this,
    with `input.shr(16)` reading “shift input 16 pixels to the right”:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您需要遍历此数组，逐个读取像素并将其转换为归一化浮点数。您将使用位移操作获取特定的通道。例如，考虑之前的值0x0010FF10。如果您将其向右移动16位，您将得到0x0010（FF10被“丢弃”）。然后，“与”0xFF，您将得到0x10，仅保留底部两个数字。类似地，如果您向右移动8位，您将得到0x0010FF，并在其上执行“和”操作将给出0xFF。这种技术（通常称为*掩码*）允许您快速轻松地剥离构成像素的相关位。您可以对整数使用`shr`操作来执行此操作，其中`input.shr(16)`读取“将输入向右移动16像素”：
- en: '[PRE32]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As before, when it comes to the output, you need to define an array to hold
    the result. It doesn’t *have* to be a `ByteArray`; indeed, you can define something
    like a `FloatArray` if you know the results are going to be floats, as they usually
    are. In this case, with the dogs versus cats model, you have two labels, and the
    model architecture is defined with two neurons in the output layer, containing
    the respective properties for the classes `cat` and `dog`. Reading back the results,
    you can define a structure to contain the output tensor like this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，在处理输出时，您需要定义一个数组来保存结果。它不一定要是`ByteArray`；实际上，如果您知道结果通常是浮点数，您可以定义像`FloatArray`这样的东西。在这种情况下，使用猫与狗模型，您有两个标签，并且模型体系结构在输出层中定义了两个神经元，包含类别`cat`和`dog`的相应属性。回读结果时，您可以定义一个结构来包含输出张量，如下所示：
- en: '[PRE33]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note that it’s a single array that contains an array of two items. Remember
    back when using Python you might see a value like `[[1.0 0.0]]`—it’s the same
    here. The `Array(1)` is defining the containing array `[]`, while the `FloatArray(2)`
    is the `[1.0 0.0]`. It can be a little confusing, for sure, but it’s something
    that I hope you’ll get used to as you write more TensorFlow apps!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，它是包含两个项目数组的单个数组。还记得在使用Python时，您可能会看到像`[[1.0 0.0]]`这样的值——在这里也是一样的。`Array(1)`正在定义包含数组`[]`，而`FloatArray(2)`是`[1.0
    0.0]`。这可能会有点令人困惑，但当您编写更多TensorFlow应用程序时，我希望您会习惯的！
- en: 'As before, you interpret using `interpreter.run`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，您使用`interpreter.run`进行解释：
- en: '[PRE34]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: And now your result will be an array, containing two values (one for each of
    the probabilities that the image is a cat or a dog). You can see what it looks
    like in the Android debugger in [Figure 8-6](#parsing_the_output_value)—where
    you can see this image had a 0.01 probability of being a cat and a 0.99 one of
    being a dog!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您的结果将是一个包含两个值的数组（分别是图像是猫或狗的概率）。您可以在Android调试器中查看它的外观，见[图 8-6](#parsing_the_output_value)——在这里，您可以看到这张图片有0.01的概率是猫，0.99的概率是狗！
- en: '![](assets/aiml_0806.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0806.png)'
- en: Figure 8-6\. Parsing the output value
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-6\. 解析输出值
- en: As you create mobile apps with Android, this is the most complex part—other
    than creating the model, of course—that you’ll have to take into account. How
    Python represents values, particularly with NumPy, can be very different from
    how Android does. You’ll have to create converters to reformat your data for how
    neural networks expect the data to be input, and you’ll have to understand the
    output schema that the neural network uses so that you can parse the results.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用Android创建移动应用程序时，这是最复杂的部分之一——除了创建模型之外，当然——您必须考虑到。特别是在NumPy的情况下，Python如何表示值可能与Android的方式非常不同。您将不得不创建转换器来重新格式化数据，以便神经网络期望数据输入，并且您必须理解神经网络使用的输出模式，以便解析结果。
- en: Create an iOS App to Host TFLite
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个iOS应用程序来托管TFLite
- en: Earlier we explored creating an app to host the simple y = 2x − 1 model in Android.
    Let’s see how to do the same in iOS next. You’ll need a Mac if you want to follow
    along with this example, as the development tool to use is Xcode (which is only
    available on Mac). If you don’t have it already, you can install it from the App
    Store. It will give you everything you need, including an iOS Simulator on which
    you can run iPhone and iPod apps without the need for a physical device.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候我们探索了在Android上创建一个用于托管简单y = 2x − 1模型的应用程序。接下来让我们看看如何在iOS上完成相同的操作。如果您想要跟随这个示例，您需要一台Mac电脑，因为开发工具是仅在Mac上可用的Xcode。如果您还没有安装它，您可以从App
    Store安装。它将为您提供一切所需的内容，包括可以在上面运行iPhone和iPod应用程序的iOS模拟器，而无需实际设备。
- en: 'Step 1: Create a Basic iOS App'
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步：创建一个基本的iOS应用程序
- en: Open Xcode and select File → New Project. You’ll be asked to pick the template
    for your new project. Choose Single View App, which is the simplest template ([Figure 8-7](#creating_a_new_ios_application_in_xcode)),
    and click Next.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 打开Xcode并选择文件 → 新建项目。您将被要求选择新项目的模板。选择单视图应用程序，这是最简单的模板（参见[图 8-7](#creating_a_new_ios_application_in_xcode)），然后点击下一步。
- en: '![](assets/aiml_0807.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0807.png)'
- en: Figure 8-7\. Creating a new iOS application in Xcode
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-7\. 在Xcode中创建一个新的iOS应用程序
- en: After that you’ll be asked to choose options for your new project, including
    a name for the app. Call it *firstlite*, and make sure that the language is Swift
    and the user interface is Storyboard ([Figure 8-8](#choosing_options_for_your_new_project)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将被要求选择新项目的选项，包括应用程序的名称。将其命名为*firstlite*，确保语言为Swift，用户界面为Storyboard（参见[图 8-8](#choosing_options_for_your_new_project)）。
- en: '![](assets/aiml_0808.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0808.png)'
- en: Figure 8-8\. Choosing options for your new project
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-8\. 选择新项目的选项
- en: Click Next to create a basic iOS app that will run on an iPhone or iPad simulator.
    The next step is to add TensorFlow Lite to it.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 点击下一步创建一个基本的iOS应用程序，该应用程序将在iPhone或iPad模拟器上运行。接下来的步骤是将TensorFlow Lite添加到其中。
- en: 'Step 2: Add TensorFlow Lite to Your Project'
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：向您的项目添加TensorFlow Lite
- en: 'To add dependencies to an iOS project, you can use a technology called [CocoaPods](https://cocoapods.org),
    a dependency management project with many thousands of libraries that can be easily
    integrated into your app. To do so, you create a specification called a Podfile,
    which contains details about your project and the dependencies you want to use.
    This is a simple text file called *Podfile* (no extension), and you should put
    it in the same directory as the *firstlite.xcodeproj* file that was created for
    you by Xcode. Its contents should be as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要向iOS项目添加依赖项，您可以使用名为[CocoaPods](https://cocoapods.org)的技术，这是一个具有数千个库的依赖管理项目，可以轻松集成到您的应用程序中。为此，您需要创建一个称为Podfile的规范文件，其中包含关于您的项目和要使用的依赖项的详细信息。这是一个名为*Podfile*的简单文本文件（没有扩展名），应放置在与Xcode为您创建的*firstlite.xcodeproj*文件相同的目录中。其内容应如下所示：
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The important part is the line that reads `pod 'TensorFlowLiteSwift'`, which
    indicates that the TensorFlow Lite Swift libraries need to be added to the project.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的部分是这一行：`pod 'TensorFlowLiteSwift'`，它表示需要将TensorFlow Lite Swift库添加到项目中。
- en: 'Next, using Terminal, change to the directory containing the Podfile and issue
    the following command:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在Terminal中，切换到包含Podfile的目录，并输入以下命令：
- en: '[PRE36]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The dependencies will be downloaded and added to your project, stored in a new
    folder called *Pods*. You’ll also have an *.xcworkspace* file added, as shown
    in [Figure 8-9](#your_file_structure_after_running_pod_i). Use this one in the
    future to open your project, and not the *.xcodeproj* file.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖项将被下载并添加到您的项目中，存储在一个名为*Pods*的新文件夹中。您还将添加一个*.xcworkspace*文件，如[图 8-9](#your_file_structure_after_running_pod_i)所示。将来打开项目时，请使用这个文件而不是*.xcodeproj*文件。
- en: '![](assets/aiml_0809.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0809.png)'
- en: Figure 8-9\. Your file structure after running pod install
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-9\. 运行pod install后的文件结构
- en: You now have a basic iOS app, and you have added the TensorFlow Lite dependencies.
    The next step is to create your user interface.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在拥有一个基本的iOS应用程序，并已添加了TensorFlow Lite的依赖项。下一步是创建用户界面。
- en: 'Step 3: Create the User Interface'
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步：创建用户界面
- en: The Xcode storyboard editor is a visual tool that allows you to create a user
    interface. After opening your workspace, you’ll see a list of source files on
    the left. Select *Main.storyboard*, and using the controls palette, drag and drop
    controls onto the view for an iPhone screen ([Figure 8-10](#adding_controls_to_the_storyboard)).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Xcode故事板编辑器是一个可视化工具，允许您创建用户界面。打开工作区后，您会在左侧看到源文件列表。选择*Main.storyboard*，并使用控件面板，将控件拖放到iPhone屏幕的视图上（在[图8-10](#adding_controls_to_the_storyboard)中可见）。
- en: If you can’t find the controls palette, you can access it by clicking the +
    at the top right of the screen (highlighted in [Figure 8-10](#adding_controls_to_the_storyboard)).
    Using it, add a Label, and change the text to “Enter a Number.” Then add another
    one with the text “Result goes here.” Add a Button and change its caption to “Go,”
    and finally add a TextField. Arrange them similarly to what you can see in [Figure 8-10](#adding_controls_to_the_storyboard).
    It doesn’t have to be pretty!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找不到控件面板，可以通过点击屏幕右上角的+号（在[图8-10](#adding_controls_to_the_storyboard)中突出显示）来访问它。使用它，添加一个标签，并将文本更改为“输入一个数字”。然后再添加一个标签，文本为“结果显示在此处”。添加一个按钮，将其标题更改为“Go”，最后再添加一个文本字段。排列它们的方式类似于[图8-10](#adding_controls_to_the_storyboard)中显示的样子。它不一定要很漂亮！
- en: '![](assets/aiml_0810.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0810.png)'
- en: Figure 8-10\. Adding controls to the storyboard
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10\. 向故事板添加控件
- en: Now that the controls are laid out, you want to be able to refer to them in
    code. In storyboard parlance, you do this using either *outlets* (when you want
    to address the control to read or set its contents) or *actions* (when you want
    to execute some code when the user interacts with the control).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在控件已布局好，您希望能够在代码中引用它们。在故事板术语中，您可以使用*出口*（当您希望访问控件以读取或设置其内容时）或*操作*（当您希望在用户与控件交互时执行一些代码时）来实现此目的。
- en: The easiest way to wire this up is to have a split screen, with the storyboard
    on one side and the *ViewController.swift* code that underlies it on the other.
    You can achieve this by selecting the split screen control (highlighted in [Figure 8-11](#splitting_the_screen)),
    clicking on one side and selecting the storyboard, and then clicking on the other
    side and selecting *ViewController.swift*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将此连接起来的最简单方法是使用分割屏幕，将故事板放在一侧，底层代码*ViewController.swift*放在另一侧。您可以通过选择分割屏幕控件（在[图8-11](#splitting_the_screen)中突出显示），点击一侧选择故事板，然后点击另一侧选择*ViewController.swift*来实现此目的。
- en: '![](assets/aiml_0811.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0811.png)'
- en: Figure 8-11\. Splitting the screen
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-11\. 分割屏幕
- en: Once you’ve done this, you can start creating your outlets and actions by dragging
    and dropping. This app will have the user type a number into the text field and
    press the Go button, and then run inference on the value that they typed. The
    result will be rendered in the label that says “Result goes here.”
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这一步，您可以开始通过拖放来创建您的出口和操作。该应用程序将让用户在文本字段中输入一个数字，然后按“Go”按钮，然后对他们输入的值进行推断。结果将呈现在标签中，标签上写着“结果显示在此处”。
- en: This means you’ll need to read or write to two controls, reading the contents
    of the text field to get what the user typed in and writing the result to the
    “Results goes here” label. Thus, you’ll need two outlets. To create them, hold
    down the Ctrl key and drag the control on the storyboard onto the *ViewController.swift*
    file, dropping it just below the class definition. A pop-up will appear asking
    you to define it ([Figure 8-12](#creating_an_outlet)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着您需要读取或写入两个控件，读取文本字段的内容以获取用户输入的内容，并将结果写入“结果显示在此处”的标签中。因此，您需要两个出口。要创建它们，按住Ctrl键并将控件拖放到*ViewController.swift*文件中，将其放置在类定义的正下方。会弹出一个窗口询问您要定义它（见[图8-12](#creating_an_outlet)）。
- en: '![](assets/aiml_0812.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0812.png)'
- en: Figure 8-12\. Creating an outlet
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-12\. 创建一个出口
- en: Ensure the connection type is Outlet, and create an outlet for the text field
    called `txtUserData` and one for the label called `txtResult`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 确保连接类型为出口，并创建一个称为`txtUserData`的文本字段的出口，以及一个称为`txtResult`的标签的出口。
- en: Next, drag the button over to the *ViewController.swift* file. In the pop-up,
    ensure that the connection type is Action and the event type is Touch Up Inside.
    Use this to define an action called `btnGo` ([Figure 8-13](#adding_an_action)).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将按钮拖放到*ViewController.swift*文件中。在弹出窗口中，确保连接类型为操作，事件类型为“Touch Up Inside”。使用此方法来定义一个名为`btnGo`的操作（见[图8-13](#adding_an_action)）。
- en: '![](assets/aiml_0813.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0813.png)'
- en: Figure 8-13\. Adding an action
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-13\. 添加一个操作
- en: 'At this point your *ViewController.swift* file should look like this—note the
    `IBOutlet` and `IBAction` code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您的 *ViewController.swift* 文件应该是这样的——请注意 `IBOutlet` 和 `IBAction` 代码：
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now that the UI is squared away, the next step will be to create the code that
    will handle the inference. Instead of having this in the same Swift file as the
    `ViewController` logic, you’ll place it in a separate code file.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 UI 部分已经搞定，下一步将是创建处理推断的代码。您将其放在与 `ViewController` 逻辑不同的 Swift 文件中。
- en: 'Step 4: Add and Initialize the Model Inference Class'
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 4：添加和初始化模型推断类
- en: To keep the UI separate from the underlying model inference, you’ll create a
    new Swift file containing a `ModelParser` class. This is where all the work of
    getting the data into the model, running the inference, and then parsing the results
    will happen. In Xcode, choose File → New File and select Swift File as the template
    type ([Figure 8-14](#adding_a_new_swift_file)).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将 UI 与底层模型推断分离，您将创建一个新的 Swift 文件，其中包含一个 `ModelParser` 类。这是将数据输入模型、运行推断并解析结果的所有工作都将在这里进行。在
    Xcode 中，选择 文件 → 新建文件，选择 Swift 文件作为模板类型（[图 8-14](#adding_a_new_swift_file)）。
- en: '![](assets/aiml_0814.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0814.png)'
- en: Figure 8-14\. Adding a new Swift file
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-14\. 添加一个新的 Swift 文件
- en: Call this *ModelParser* and ensure that the checkbox targeting it to the *firstlite*
    project is checked ([Figure 8-15](#adding_modelparserdotswift_to_your_proj)).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此 *ModelParser* 并确保将其指向 *firstlite* 项目的复选框已选中（[图 8-15](#adding_modelparserdotswift_to_your_proj)）。
- en: '![](assets/aiml_0815.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0815.png)'
- en: Figure 8-15\. Adding ModelParser.swift to your project
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-15\. 将 ModelParser.swift 添加到您的项目
- en: 'This will add a *ModelParser.swift* file to your project that you can edit
    to add the inference logic. First, ensure that the imports at the top of the file
    include `TensorFlowLite`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在您的项目中添加一个 *ModelParser.swift* 文件，您可以编辑以添加推断逻辑。首先确保文件顶部的导入包括 `TensorFlowLite`：
- en: '[PRE38]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You’ll pass a reference to the model file, *model.tflite*, to this class—you
    haven’t added it yet, but you will soon:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 您将把一个指向模型文件 *model.tflite* 的引用传递给这个类——尽管您还没有添加它，但很快会添加：
- en: '[PRE39]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This `typealias` and `enum` make the code a little more compact. You’ll see
    them in use in a moment. Next you’ll need to load the model into an interpreter,
    so first declare the interpreter as a private variable to the class:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `typealias` 和 `enum` 使得代码更加简洁。稍后您将看到它们的使用。接下来，您需要将模型加载到解释器中，因此首先将解释器声明为类的私有变量：
- en: '[PRE40]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Swift requires variables to be initialized, which you can do within an `init`
    function. The following function will take two input parameters. The first, `modelFileInfo`,
    is the `FileInfo` type you just declared. The second, `threadCount`, is the number
    of threads to use to initialize the interpreter, which we’ll set to 1\. Within
    this function, you’ll create a reference to the model file that you described
    earlier (*model.tflite*):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Swift 要求变量进行初始化，在 `init` 函数中可以执行此操作。下面的函数将接受两个输入参数。第一个参数 `modelFileInfo` 是您刚刚声明的
    `FileInfo` 类型。第二个参数 `threadCount` 是用于初始化解释器的线程数，我们将其设置为 1。在此函数中，您将创建对之前描述的模型文件的引用（*model.tflite*）：
- en: '[PRE41]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'When you compile your app and assets into a package that gets deployed to the
    device, the term used in iOS parlance is a “bundle.” So your model needs to be
    in the bundle, and once you have the path to the model file, you can load it:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将应用程序和资源编译成一个部署到设备上的包时，在 iOS 术语中使用的术语是“bundle”。因此，您的模型需要在 bundle 中，一旦获得模型文件的路径，您就可以加载它：
- en: '[PRE42]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Step 5: Perform the Inference'
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 5：执行推断
- en: Within the `ModelParser` class, you can then do the inference. The user will
    type a string value in the text field, which will be converted to a float, so
    you’ll need a function that takes a float, passes it to the model, runs the inference,
    and parses the return value.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `ModelParser` 类内部，然后可以进行推断。用户将在文本字段中输入一个字符串值，该值将转换为浮点数，因此您需要一个接受浮点数的函数，将其传递给模型，运行推断并解析返回值。
- en: 'Start by creating a function called `runModel`. Your code will need to catch
    errors, so start it with a `do{`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个名为 `runModel` 的函数。您的代码将需要捕获错误，所以请从 `do{` 开始：
- en: '[PRE43]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, you’ll need to allocate tensors on the interpreter. This initializes
    the model and prepares it for inference:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要在解释器上分配张量。这将初始化模型并准备进行推断：
- en: '[PRE44]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Then you’ll create the input tensor. As Swift doesn’t have a `Tensor` datatype,
    you’ll need to write the data directly to memory in an `UnsafeMutableBufferPointer`.
    These are covered in [Apple’s developer documentation](https://oreil.ly/EfDus).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将创建输入张量。由于Swift没有`Tensor`数据类型，您需要将数据直接写入`UnsafeMutableBufferPointer`中的内存。这些内容在[苹果的开发者文档](https://oreil.ly/EfDus)中有详细介绍。
- en: 'You can specify the type of this, which will be Float, and write one value
    (as you only have one float), starting from the address of the variable called
    data. This will effectively copy all the bytes for the float into the buffer:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以指定其类型为Float，并写入一个值（因为只有一个float），从名为data的变量地址开始。这将有效地将浮点数的所有字节复制到缓冲区中：
- en: '[PRE45]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'With the data in the buffer, you can then copy it to the interpreter at input
    0\. You only have one input tensor, so you can specify it as the buffer:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 有了缓冲区中的数据，您可以将其复制到输入 0 的解释器中。您只有一个输入张量，因此可以将其指定为缓冲区：
- en: '[PRE46]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'To execute the inference, you invoke the interpreter:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行推断，您需要调用解释器：
- en: '[PRE47]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'There’s only one output tensor, so you can read it by taking the output at
    0:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个输出张量，因此您可以通过获取索引为0的输出来读取它：
- en: '[PRE48]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Similar to when inputting the values, you’re dealing with low-level memory,
    which is referred to as *unsafe* data. When using typical datatypes, their location
    in memory is tightly controlled by the operating system so that you don’t overflow
    and overwrite other data. When doing this, you are writing the data to memory
    directly yourself, and as such are taking the risk that bounds may not be respected
    (hence the term *unsafe*).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 与输入值时类似，您处理的是低级别内存，被称为*unsafe*数据。当使用典型数据类型时，它们在内存中的位置受操作系统严格控制，以防止溢出和覆盖其他数据。但是，在这种情况下，您自己直接将数据写入内存，因此存在违反边界的风险（因此称为*unsafe*）。
- en: 'It’s in an array of `Float32` values (it only has one element but still needs
    to be treated as an array), which can be read like this:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 它是由`Float32`值组成的数组（虽然只有一个元素，但仍需视为数组），可以像这样读取：
- en: '[PRE49]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: If you’re not familiar with the `??` syntax, this says to make the results an
    array of `Float32` by copying the output tensor into it, and if that fails (usually
    a null pointer error), then create an empty array instead. For this code to work,
    you’ll need to implement an `Array` extension; the full code for that will be
    shown in a moment.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对`??`语法不熟悉，请注意，此语法用于通过将输出张量复制到其中来使结果成为`Float32`数组，并在失败时（通常是空指针错误）创建一个空数组。为了使此代码正常工作，您需要实现一个`Array`扩展；其完整代码稍后将会展示。
- en: 'Once you have the results in an array, the first element will be your result.
    If this fails, just return `nil`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您将结果存储在数组中，第一个元素将是您的结果。如果失败，只需返回`nil`：
- en: '[PRE50]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The function began with a `do{`, so you’ll need to catch any errors, print
    them, and return `nil` in that event:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 函数以`do{`开头，因此您需要捕获任何错误，并在这种情况下打印错误消息并返回`nil`：
- en: '[PRE51]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Finally, still in *ModelParser.swift*, you can add the `Array` extension that
    handles the unsafe data and loads it into an array:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在*ModelParser.swift*中，您可以添加处理不安全数据并将其加载到数组中的`Array`扩展：
- en: '[PRE52]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This is a handy helper that you can use if you want to parse floats directly
    out of a TensorFlow Lite model. Now that the class for parsing the model is done,
    the next step is to add the model to your app.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个便捷的助手，如果您想直接从TensorFlow Lite模型中解析浮点数，您可以使用它。现在，解析模型的类已完成，下一步是将模型添加到您的应用程序中。
- en: 'Step 6: Add the Model to Your App'
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 6：将模型添加到您的应用程序
- en: To add the model to your app, you’ll need a *models* directory within the app.
    In Xcode, right-click on the *firstlite* folder and select New Group ([Figure 8-16](#adding_a_new_group_to_your_app)).
    Call the new group *models*.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 要将模型添加到您的应用程序中，您需要在应用程序中创建一个*models*目录。在Xcode中，右键单击*firstlite*文件夹，然后选择新建组（[图 8-16](#adding_a_new_group_to_your_app)）。将新组命名为*models*。
- en: You can get the model by training the simple y = 2x – 1 sample from earlier
    in this chapter. If you don’t have it already, you can use the Colab in the book’s
    GitHub repository.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过训练本章早些时候的简单y = 2x – 1样本来获取该模型。如果您尚未拥有它，可以使用书籍GitHub存储库中的Colab。
- en: Once you have the converted model file (called *model.tflite*), you can drag
    and drop it into Xcode on the models group you just added. Select “Copy items
    if needed” and ensure you add it to the target *firstlite* by checking the box
    beside it ([Figure 8-17](#adding_the_model_to_your_project)).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有转换后的模型文件（称为*model.tflite*），您可以将其拖放到刚刚添加的模型组中的Xcode中。选择“复制项目（如有需要）”，并确保通过选中旁边的复选框将其添加到*firstlite*目标中（[图 8-17](#adding_the_model_to_your_project)）。
- en: '![](assets/aiml_0816.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0816.png)'
- en: Figure 8-16\. Adding a new group to your app
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-16\. 向应用程序添加一个新组
- en: '![](assets/aiml_0817.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0817.png)'
- en: Figure 8-17\. Adding the model to your project
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-17\. 将模型添加到你的项目中
- en: The model will now be in your project and available for inference. The final
    step is to complete the user interface logic—then you’ll be ready to go!
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 模型现在将会在你的项目中，并且可用于推理。最后一步是完成用户界面逻辑，然后你就可以开始了！
- en: 'Step 7: Add the UI Logic'
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 7 步：添加 UI 逻辑
- en: Earlier, you created the storyboard containing the UI description and began
    editing the *ViewController.swift* file containing the UI logic. As most of the
    work of inference has now been offloaded to the `ModelParser` class, the UI logic
    should be very light.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，你创建了包含 UI 描述的 storyboard，并开始编辑包含 UI 逻辑的 *ViewController.swift* 文件。由于推理工作的大部分现在已经被转移到
    `ModelParser` 类中，UI 逻辑应该非常轻量级。
- en: 'Start by adding a private variable declaring an instance of the `ModelParser`
    class:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 首先添加一个私有变量，声明 `ModelParser` 类的一个实例：
- en: '[PRE53]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Previously, you created an action on the button called `btnGo`. This will be
    called when the user touches the button. Update that to execute a function called
    `doInference` when the user takes that action:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，你在按钮上创建了一个名为 `btnGo` 的操作。当用户触摸按钮时，将调用它。更新为在用户执行该操作时执行名为 `doInference` 的函数：
- en: '[PRE54]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, you’ll construct the `doInference` function:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将构建 `doInference` 函数：
- en: '[PRE55]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The text field that the user will enter data into is called `txtUserData.`
    Read this value, and if it’s empty just set the result to 0.00 and don’t bother
    with any inference:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 用户将输入数据的文本字段称为 `txtUserData.` 读取这个值，如果它为空，只需将结果设置为 0.00，并且不需要进行任何推理：
- en: '[PRE56]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Otherwise, convert it to a float. If this fails, exit the function:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，将其转换为浮点数。如果失败，则退出函数：
- en: '[PRE57]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'If the code has reached this point, you can now run the model, passing it that
    input. The `ModelParser` will do the rest, returning you either a result or `nil`.
    If the return value is `nil`, then you’ll exit the function:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代码已经执行到这一点，你现在可以运行模型，将输入传递给它。`ModelParser` 将处理剩余的部分，返回一个结果或 `nil`。如果返回值是 `nil`，那么你将退出该函数：
- en: '[PRE58]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Finally, if you’ve reached this point, you have a result, so you can load it
    into the label (called `txtResult`) by formatting the float as a string:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你已经到达这一点，你有一个结果，所以你可以将它加载到标签中（称为 `txtResult`），通过将浮点数格式化为字符串：
- en: '[PRE59]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'That’s it! The complexity of the model loading and inference has been handled
    by the `ModelParser` class, keeping your `ViewController` very light. For convenience,
    here’s the complete listing:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！模型加载和推理的复杂性已由 `ModelParser` 类处理，使你的 `ViewController` 非常轻量级。为了方便起见，这里是完整的列表：
- en: '[PRE60]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: You’ve now done everything you need to get the app working. Run it, and you
    should see it in the simulator. Type a number in the text field, press the button,
    and you should see a result in the results field, as shown in [Figure 8-18](#running_the_app_in_the_iphone_simulator).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经完成了使应用程序运行所需的一切。运行它，你应该可以在模拟器中看到它。在文本字段中输入一个数字，按按钮，你应该在结果字段中看到一个结果，如图
    [8-18](#running_the_app_in_the_iphone_simulator) 所示。
- en: 'While this was a long journey for a very simple app, it should provide a good
    template to help you understand how TensorFlow Lite works. In this walkthrough,
    you saw how to:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这只是一个非常简单的应用程序的漫长旅程，但它应该为你提供一个很好的模板，帮助你理解 TensorFlow Lite 的工作原理。在这个步骤中，你看到了如何：
- en: Use pods to add the TensorFlow Lite dependencies
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pods 添加 TensorFlow Lite 依赖项
- en: Add a TensorFlow Lite model to your app
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 TensorFlow Lite 模型添加到你的应用程序中
- en: Load the model into an interpreter
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型加载到解释器中
- en: Access the input tensors and write directly to their memory
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问输入张量并直接写入它们的内存
- en: Read the memory from the output tensors and copy that to high-level data structures
    like float arrays
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从输出张量中读取内存，并将其复制到像浮点数组这样的高级数据结构中
- en: Wire it all up to a user interface with a storyboard and view controler.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 storyboard 和 view controller 将所有内容连接起来。
- en: '![](assets/aiml_0818.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0818.png)'
- en: Figure 8-18\. Running the app in the iPhone Simulator
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-18\. 在 iPhone 模拟器中运行应用程序
- en: In the next section, you’ll move beyond this simple scenario and look at handling
    more complex data.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将超越这个简单的场景，看看如何处理更复杂的数据。
- en: 'Moving Beyond “Hello World”: Processing Images'
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越“Hello World”：处理图像
- en: In the previous example, you saw how to create a full app that uses TensorFlow
    Lite to do very simple inferences. However, despite the simplicity of the app,
    the process of getting data into the model and parsing data out of the model can
    be a little unintuitive because you’re handling low-level bits and bytes. As you
    get into more complex scenarios, such as managing images, the good news is that
    the process isn’t that much more complicated.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，您看到了如何创建一个使用 TensorFlow Lite 进行非常简单推断的完整应用程序。然而，尽管应用程序简单，但是将数据传入模型和解析模型输出的过程可能有些不直观，因为您正在处理低级的位和字节。随着您涉及更复杂的场景，例如管理图像，好消息是该过程并不那么复杂。
- en: Consider a model that you’d create to distinguish between cats versus dogs.
    In this section you’ll see how to create an iOS app in Swift with a trained model
    that, given an image of a cat or a dog, will be able to infer what is in the picture.
    The full app code is available in the GitHub repo for this book, as well as a
    Colab notebook to train and convert the model to TFLite format.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑创建一个模型来区分猫和狗。在本节中，您将看到如何使用经过训练的模型创建一个 iOS 应用程序（使用 Swift），该应用程序可以根据猫或狗的图像推断出图片内容。完整的应用程序代码可以在本书的
    GitHub 存储库中找到，以及一个 Colab 笔记本来训练和将模型转换为 TFLite 格式。
- en: 'First, recall that the tensor for an image has three dimensions: width, height,
    and color depth. So, for example, when using the MobileNet architecture that the
    dogs versus cats mobile sample is based on, the dimensions are 224 × 224 × 3—each
    image is 224 × 224 pixels and has three channels for color depth. Note that each
    pixel, after normalization, will be represented by a value between 0 and 1 in
    each channel, indicating the intensity of that pixel on the red, green, and blue
    channels.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，回顾一下图像的张量具有三个维度：宽度、高度和颜色深度。例如，使用基于 MobileNet 架构的猫狗移动样本时，尺寸为 224 × 224 × 3——每个图像为
    224 × 224 像素，并具有三个颜色深度通道。请注意，每个像素在归一化后，每个通道的值都介于 0 和 1 之间，表示该像素在红色、绿色和蓝色通道上的强度。
- en: In iOS, images are typically represented as instances of the `UIImage` class,
    which has a useful `pixelBuffer` property that returns a buffer of all the pixels
    in the image.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在 iOS 中，图像通常表示为 `UIImage` 类的实例，该类具有一个有用的 `pixelBuffer` 属性，返回图像中所有像素的缓冲区。
- en: 'Within the `CoreImage` libraries, there’s a `CVPixelBufferGetPixelFormatType`
    API that will return the type of the pixel buffer:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `CoreImage` 库中，有一个 `CVPixelBufferGetPixelFormatType` API 可以返回像素缓冲区的类型：
- en: '[PRE61]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This will typically be a 32-bit image with channels for alpha (aka transparency),
    red, green, and blue. However, there are multiple variants, generally with these
    channels in different orders. You’ll want to ensure that it’s one of these formats,
    as the rest of the code won’t work if the image is stored in a different format:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是一个 32 位图像，具有 alpha（即透明度）、红色、绿色和蓝色通道。然而，有多种变体，通常这些通道的顺序不同。您需要确保它是这些格式之一，因为如果图像存储在不同的格式中，其余的代码将无法正常工作：
- en: '[PRE62]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'As the desired format is 224 × 224, which is square, the best thing to do next
    is to crop the image to the largest square in its center, using the `centerThumbnail`
    property, and then scale this down to 224 × 224:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所需格式是 224 × 224，这是一个方形，接下来最好的做法是使用 `centerThumbnail` 属性将图像裁剪为其中心的最大方形，然后将其缩放到
    224 × 224：
- en: '[PRE63]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Now that you have the image resized to 224 × 224, the next step is to remove
    the alpha channel. Remember that the model was trained on 224 × 224 × 3, where
    the 3 is the RGB channels, so there is no alpha.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已将图像调整为 224 × 224，接下来的步骤是移除 alpha 通道。请记住，模型是在 224 × 224 × 3 上进行训练的，其中 3 表示
    RGB 通道，因此没有 alpha 通道。
- en: 'Now that you have a pixel buffer, you need to extract the RGB data from it.
    This helper function achieves that for you by finding the alpha channel and slicing
    it out:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了像素缓冲区，需要从中提取RGB数据。这个辅助函数可以帮助您通过找到 alpha 通道并将其切片来实现这一目标：
- en: '[PRE64]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'This code uses an extension called `Data` that copies the raw bytes into an
    array:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用了一个名为 `Data` 的扩展，它将原始字节复制到数组中：
- en: '[PRE65]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now you can pass the thumbnail pixel buffer you just created to `rgbDataFromBuffer`:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以将刚创建的缩略图像素缓冲区传递给 `rgbDataFromBuffer`：
- en: '[PRE66]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'At this point you have the raw RGB data that is in the format the model expects,
    and you can copy it directly to the input tensor:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了模型期望的原始RGB数据格式，可以直接将其复制到输入张量中：
- en: '[PRE67]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'You can then invoke the interpreter and read the output tensor:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 您随后可以调用解释器并读取输出张量：
- en: '[PRE68]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'In the case of dogs versus cats, you have as output a float array with two
    values, the first being the probability that the image is of a cat and the second
    that it’s a dog. This is the same results code that you saw earlier, and it uses
    the same array extension from the previous example:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在狗与猫的情况下，输出为一个包含两个值的浮点数组，第一个值是图片为猫的概率，第二个值是为狗的概率。这与您之前看到的结果代码相同，并且使用了前面示例中的相同数组扩展：
- en: '[PRE69]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: As you can see, although this is a more complex example, the same design pattern
    holds. You must understand your model’s architecture, and the raw input and output
    formats. You must then structure your input data in the way the model expects—which
    often means getting down to raw bytes that you write into a buffer, or at least
    simulate using an array. You then have to read the raw stream of bytes coming
    out of the model and create a data structure to hold them. From the output perspective,
    this will almost always be something like we’ve seen in this chapter—an array
    of floats. With the helper code you’ve implemented, you’re most of the way there!
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，尽管这是一个更复杂的例子，但相同的设计模式适用。您必须理解您的模型架构，原始输入和输出格式。然后，您必须按照模型预期的方式结构化输入数据——这通常意味着将原始字节写入缓冲区，或者至少模拟使用数组。然后，您必须读取从模型出来的原始字节流，并创建一个数据结构来保存它们。从输出的角度来看，这几乎总是像我们在本章中看到的那样——一个浮点数数组。借助您实施的辅助代码，您已经完成了大部分工作！
- en: We’ll look into this example in more detail in [Chapter 11](ch11.html#using_custom_models_in_ios).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第11章](ch11.html#using_custom_models_in_ios)中更详细地讨论这个例子。
- en: Exploring Model Optimization
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索模型优化
- en: TensorFlow Lite includes tools to optimize your model using representative data
    as well as processes such as quantization. We’ll explore those in this section.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite包含工具，可以使用代表性数据来优化您的模型，以及通过量化等过程。我们将在本节中探讨这些内容。
- en: Quantization
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: The idea of quantization comes from understanding that neurons in a model default
    to using float32 as representation, but often their values fall within a much
    smaller range than the range of a float32\. Consider [Figure 8-19](#quantizing_values)
    as an example.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 量化的概念来自于理解模型中的神经元默认使用float32作为表示，但通常它们的值落在比float32范围小得多的范围内。以[图8-19](#quantizing_values)为例。
- en: '![](assets/aiml_0819.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0819.png)'
- en: Figure 8-19\. Quantizing values
  id: totrans-301
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-19. 量化值
- en: In this case, at the bottom of the diagram is a histogram of the possible values
    that a particular neuron might have. They’re normalized, so they’re spread around
    0, but the min is far greater than the min of a float32, and the max value is
    far less than the max of a float32\. What if, instead of having all of this “empty
    space,” the histogram could be converted into a much smaller range—say −127 to
    +127, with the values mapped accordingly. Note that by doing this you are significantly
    reducing how many *possible* values can be represented, so you will risk a loss
    in precision. Studies of this method have shown that while precision can be lost,
    the loss is often small, but the benefits in model size, as well as inference
    time, because of the simpler data structure, greatly outweigh the risks.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，图表底部是特定神经元可能具有的可能值的直方图。它们被归一化，所以它们围绕0分布，但最小值远大于float32的最小值，最大值远小于float32的最大值。如果，而不是拥有所有这些“空间”，直方图可以转换为一个更小的范围——比如从−127到+127，然后相应地映射值。请注意，通过这样做，您显著减少了可以表示的*可能*值的数量，因此会冒降低精度的风险。研究表明，尽管可能会损失精度，但损失通常很小，但由于更简单的数据结构，模型大小以及推理时间的好处大大超过风险。
- en: Then, the range of values could be implemented in a single byte (256 values)
    instead of the 4 bytes of a float32\. And given that neural networks can have
    many neurons, often using hundreds of thousands or millions of parameters, using
    only one-quarter of the space to store their parameters can have significant time
    savings.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，值的范围可以在一个字节（256个值）中实现，而不是float32的4个字节。考虑到神经网络可能有许多神经元，通常使用数十万或数百万个参数，仅使用四分之一的空间来存储它们的参数可以显著节省时间。
- en: Note
  id: totrans-304
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This can also be coupled with hardware that’s optimized to use data storage
    in int8 instead of float32, giving even further benefits of hardware accelerated
    inference.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以与优化为使用int8而不是float32数据存储的硬件结合使用，从而提供进一步的硬件加速推理优势。
- en: This process is called *quantization* and is available in TensorFlow Lite. Let’s
    explore how it works.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为*量化*，并且在TensorFlow Lite中可用。让我们来探索它是如何工作的。
- en: Note that there are multiple methods for quantization, including quantization-aware
    training, where you take it into account while the model learns; pruning, which
    is used to reduce connectivity in a model to simplify it; and post-training quantization,
    which we’ll explore here, where you reduce the data storage in a model for weights
    and biases as described earlier.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有多种量化方法，包括量化感知训练，在模型学习时考虑其中；修剪，用于减少模型中的连接以简化模型；以及我们将在这里探讨的后训练量化，在此过程中，您可以减少模型中权重和偏差的数据存储，如前所述。
- en: In the source code for this chapter, there’s a notebook called *Chapter8Lab2*
    that trains a neural network to recognize the difference between cats and dogs.
    I strongly recommend you work through it as you follow along here. You can access
    it using [Colab](https://oreil.ly/xpEd0) if it’s available in your country; otherwise,
    you’ll need a Python environment or Jupyter notebook environment to work in.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的源代码中，有一个名为*Chapter8Lab2*的笔记本，用于训练神经网络识别猫和狗的差异。我强烈建议您在跟随本节内容时一起进行。如果在您的国家可以访问，您可以使用[Colab](https://oreil.ly/xpEd0)；否则，您需要在Python环境或Jupyter笔记本环境中进行工作。
- en: 'You may notice as you’re working through it that you get a summary of the model
    that looks like this:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在您进行工作时，您可能会注意到您会得到一个看起来像这样的模型摘要：
- en: '[PRE70]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Note the number of parameters: over two million! If you’re reducing the storage
    of these by 3 bytes apiece, you can reduce your model size by over 6 MB with quantization!'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意参数的数量：超过两百万！如果每个参数的存储减少3字节，通过量化可以将模型大小减少超过6 MB！
- en: 'So let’s explore how to do quantization on the model—it’s very easy to do!
    Earlier you saw how to instantiate the converter from a saved model like this:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们探索如何对模型进行量化——这非常容易！您之前看到如何从保存的模型实例化转换器如下：
- en: '[PRE71]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: But before converting, you can set an optimizations parameter on the converter,
    which accepts a parameter specifying the type of optimization. It was originally
    designed so that there were three different types of optimization (default or
    balanced, for size, and for speed), but the option to choose a type of quantization
    has been deprecated, so for now, you can only use “default” quantization.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 但在转换之前，您可以在转换器上设置一个优化参数，该参数接受指定优化类型的参数。最初设计时有三种不同类型的优化（默认或平衡，大小和速度），但选择量化类型的选项已经被弃用，因此目前您只能使用“默认”量化。
- en: 'For compatibility reasons, and future flexibility, the method signature remains,
    but you only have one option (default) that you can use like this:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 为了兼容性和未来的灵活性，方法签名仍然保留，但您只有一个选项（默认选项），可以像这样使用：
- en: '[PRE72]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: When I created the cats versus dogs model *without* optimization, I had a model
    that was 8.8 Mb. When the optimization was applied, it shrank to 2.6 Mb, a significant
    savings!
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 当我创建了未经优化的猫狗模型时，模型大小为8.8 Mb。应用优化后，模型缩小为2.6 Mb，节省了大量空间！
- en: You might wonder about any impact this has on accuracy. Given the change in
    size of the model, it’s good to investigate.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道这对准确性有什么影响。鉴于模型大小的变化，进行调查是很好的。
- en: There’s code in the notebook that you can try for yourself, but when I investigated,
    I found that the unoptimized version of the model would do about 37 iterations
    per second in Colab (which uses a GPU, so it’s optimized for floating point operations!),
    whereas the shrunken version did about 16 iterations per second. Without the GPU,
    the performance degraded by about half, but importantly, the speed is still terrific
    for image classification, and it’s not likely you’ll need this kind of performance
    when classifying images on a device!
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中有一些您可以自己尝试的代码，但当我调查时，我发现在Colab中未优化版本的模型每秒大约可以进行37次迭代（使用GPU，因此针对浮点运算进行了优化！），而缩小版本每秒大约进行16次迭代。没有GPU时，性能下降了约一半，但重要的是，速度对于图像分类仍然很好，并且在设备上分类图像时不太可能需要这种性能！
- en: More important is the accuracy—on a set of 100 images I tested, the unoptimized
    model got 99 of them correct, while the optimized one got 94\. Here’s where you’ll
    have a trade-off decision about whether or not you want to optimize the model
    at the cost of accuracy. Experiment with this for your own models! Do note that
    in this case I just did basic quantization. There are other methods to reduce
    the size of your model that may have less of an impact, and all should be explored.
    Let’s look next at using representative data.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是精度——在我测试的100张图像集中，未优化的模型正确识别了99张，而优化后的模型识别了94张。这里您将需要在是否以精度为代价优化模型之间做出权衡决定。尝试为自己的模型进行实验！请注意，在这种情况下，我只是进行了基本的量化。还有其他减小模型大小的方法，可能影响较小，所有方法都应该探索。接下来我们来看看使用代表性数据。
- en: Using Representative Data
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用代表性数据
- en: The preceding example showed quantization by reducing values from float32 to
    int8 by effectively removing “white space” from the data, but the algorithm generally
    assumed that the data was uniformly spread around 0, because that’s what it learned
    from the training data, which could lead to a loss of accuracy if your test or
    real-world data isn’t represented like this. We saw that the accuracy dropped
    from 99/100 to 94/100 with a small set of test images.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例展示了通过将浮点数（float32）的值有效地减少到整数（int8），从数据中有效去除“空白”，来进行量化。但是，该算法通常假设数据在0周围均匀分布，因为这是它从训练数据中学到的，如果您的测试或实际数据不是这样表示，可能会导致精度下降。我们看到，用少量测试图像，精度从99/100下降到94/100。
- en: You can give the optimization process a helping hand by providing some representative
    data from your dataset so it can predict better how to optimize for the type of
    data that the neural network will be expected to “see” going forward. This will
    give you a trade-off of size against accuracy—the size might be a bit larger,
    because the optimization process won’t convert all the values from float32 to
    int8, detecting, from the dataset, that some might involve too much lost data.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过提供来自数据集的一些代表性数据来帮助优化过程，以便它更好地预测神经网络将来“看到”的数据类型。这将使您在大小与精度之间进行权衡——由于优化过程不会将所有值从float32转换为int8，检测到数据集中一些值可能涉及过多的丢失数据，因此大小可能会稍大一些。
- en: 'Doing this is pretty simple—simply take a subset of your data as the representative
    data:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做非常简单——只需将您的数据子集作为代表性数据：
- en: '[PRE73]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Then, specify this as the representative dataset for the convertor:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将此指定为转换器的代表性数据集：
- en: '[PRE74]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Finally, specify the desired ops to target. You’ll generally use the built-in
    `INT8` ones like this:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，指定所需的目标操作。通常会使用内置的`INT8`操作，如下所示：
- en: '[PRE75]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Note that at the time of writing, the full set of supported ops selections is
    experimental. You can find details on the [TensorFlow website](https://oreil.ly/Kn1Xj).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在撰写本文时，全面支持的操作选择是实验性的。您可以在[TensorFlow网站](https://oreil.ly/Kn1Xj)上找到详细信息。
- en: After converting with this process, the model size increased slightly (to 2.9
    Mb, but still significantly smaller than the original 8.9 Mb), but the iteration
    speed dropped sharply (to about one iteration per second). However the accuracy
    improved to 98/100, much closer to the original model.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个过程转换后，模型大小略有增加（至2.9 Mb，但仍远小于原始的8.9 Mb），但迭代速度急剧下降（约每秒一个迭代）。然而，准确性提高到98/100，接近原始模型的水平。
- en: For more techniques, and to explore results in model optimization, check out
    [*https://www.tensorflow.org/lite/performance/model_optimization*](https://www.tensorflow.org/lite/performance/model_optimization).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多技术，并探索模型优化的结果，请查看[*https://www.tensorflow.org/lite/performance/model_optimization*](https://www.tensorflow.org/lite/performance/model_optimization)。
- en: Try it out for yourself, but do note that the Colab environment can change a
    lot, and your results may be different from mine, in particular if you use different
    models. I’d strongly recommend seeing what the inference speed and accuracy look
    like on a device to get more appropriate results.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 请自行尝试，但请注意，Colab环境可能会有很大变化，您的结果可能与我的不同，特别是如果您使用不同的模型。我强烈建议查看在设备上推断速度和准确性如何，以获取更合适的结果。
- en: Summary
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter provided an introduction to TensorFlow Lite, and how it works to
    bring models trained using Python to mobile devices such as Android or iOS. You
    saw the toolchain and the converter scripts that let you shrink your model and
    optimize for mobile, before exploring some scenarios in how to code Android/Kotlin
    or iOS/Swift applications to use these models. Going beyond a simple model, you
    also began to see some of the considerations you need to take on as an app developer
    when converting data from the internal representation of a mobile environment
    to the tensor-based one in a TensorFlow model. Finally, you explored some scenarios
    for optimizing and shrinking your model further. In [Chapter 9](ch09.html#creating_custom_models),
    you’ll study some scenarios to create more sophisticated models than “Hello World,”
    and in Chapters [10](ch10.html#using_custom_models_in_android) and [11](ch11.html#using_custom_models_in_ios),
    you’ll bring those models into Android and iOS!
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 TensorFlow Lite，以及它如何将使用 Python 训练的模型带入到诸如 Android 或 iOS 等移动设备中的工作原理。您看到了工具链和转换脚本，这些工具能够帮助您缩小模型并优化以适配移动设备，然后探索了如何编写
    Android/Kotlin 或 iOS/Swift 应用程序来使用这些模型的一些场景。在超越简单模型的同时，您还开始了解作为应用程序开发者在将数据从移动环境的内部表示转换为
    TensorFlow 模型中的基于张量的表示时需要考虑的一些因素。最后，您还探索了进一步优化和缩小模型的一些场景。在[第九章](ch09.html#creating_custom_models)中，您将学习一些用于创建比“Hello
    World”更复杂模型的场景，并在第[十](ch10.html#using_custom_models_in_android)和[十一](ch11.html#using_custom_models_in_ios)章中，您将把这些模型引入到
    Android 和 iOS 中！
