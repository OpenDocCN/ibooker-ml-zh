- en: Chapter 6\. Data Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 数据处理
- en: In the trivial recommender that we defined in [Chapter 1](ch01.html#CH0), we
    used the method `get_availability`; and in the MPIR, we used the method `get_item_popularities`.
    We hoped the choice of naming would provide sufficient context about their function,
    but we did not focus on the implementation details. Now we will start unpacking
    the details of some of this complexity and present the toolsets for online and
    offline collectors.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在[第1章](ch01.html#CH0)中定义的简单推荐器中，我们使用了`get_availability`方法；而在MPIR中，我们使用了`get_item_popularities`方法。我们希望这些命名选择能提供足够的上下文来说明它们的功能，但我们并未关注实现细节。现在我们将开始分解一些这种复杂性的细节，并呈现在线和离线收集工具集。
- en: Hydrating Your System
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统水合
- en: Getting data into the pipeline is punnily referred to as *hydration*. The ML
    and data fields have a lot of water-themed naming conventions; [“(Data ∩ Water)
    Terms”](https://oreil.ly/XVlzd) by Pardis Noorzad covers this topic.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据导入管道中幽默地称为*水合*。ML和数据领域有很多与水相关的命名惯例；[《《(数据 ∩ 水) 术语》》](https://oreil.ly/XVlzd)由帕迪斯·努尔扎德涵盖了这个主题。
- en: PySpark
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySpark
- en: Spark is an extremely general computing library, with APIs for Java, Python,
    SQL, and Scala. PySpark’s role in many ML pipelines is for data processing and
    transforming the large-scale datasets.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个非常通用的计算库，提供Java、Python、SQL和Scala的API。PySpark在许多ML流水线中用于数据处理和转换大规模数据集。
- en: 'Let’s return to the data structure we introduced for our recommendation problem;
    recall that the user-item matrix is the linear-algebraic representation of all
    the triples of users, items, and the user’s rating of the item. These triples
    are not naturally occurring in the wild. Most commonly, you begin with log files
    from your system; for example, Bookshop.org may have something that looks like
    this:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们为推荐问题引入的数据结构；回想一下用户-物品矩阵是所有用户、物品及用户对物品评分的线性代数表示。这些三元组在野外并不自然存在。最常见的情况是，你从系统的日志文件开始；例如，Bookshop.org可能有类似以下内容的东西：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is a made-up log file that may look similar to the backend data for Bookshop.org’s
    best sellers of the week. These are the kinds of events that you consume from
    engineering and are likely stored in your columnar database. For data like this,
    utilizing SQL syntax will be our entry point.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个捏造的日志文件，它可能类似于Bookshop.org本周畅销书的后端数据。这些是你从工程部门获取的事件，并且很可能存储在你的列式数据库中。对于这样的数据，使用SQL语法将是我们的入口点。
- en: PySpark provides a convenient SQL API. Based on your infrastructure, this API
    will allow you to write what looks like SQL queries against a potentially massive
    dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark提供了一个方便的SQL API。基于你的基础设施，这个API将允许你编写看起来像对大规模数据集的SQL查询。
- en: Example Schemas
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例架构
- en: These example database schemas are only guesses at what Bookshop.org may use,
    but they are modeled on the authors’ experience of looking at hundreds of database
    schemas at multiple companies over many years. Additionally, we attempt to distill
    these schemas to the components relevant to our topic. In real systems, you’d
    expect much more complexity but the same essential parts. Each data warehouse
    and event stream will have its own quirks. Please consult a data engineer near
    you.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例数据库架构仅仅是对Bookshop.org可能使用的猜测，但它们是基于作者多年来查看多家公司数百个数据库架构经验的建模。此外，我们试图将这些模式提炼到与我们主题相关的组件。在实际系统中，你会期望有更多的复杂性，但同样的基本部分。每个数据仓库和事件流都有其独特的特点。请咨询你附近的数据工程师。
- en: 'Let’s use Spark to query the preceding logs:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Spark查询前面的日志：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is a simple SQL query, assuming the preceding log schema, that would allow
    us to see, for each user-item pair, how many times that user has viewed that pair.
    The convenience of writing pure SQL here means that we can use our experience
    in columnar databases to quickly ramp up on Spark.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的SQL查询，假设前面的日志模式，它可以让我们看到每个用户-物品对被查看的次数。在这里纯粹使用SQL的便利性意味着我们可以利用我们在列式数据库上的经验快速上手Spark。
- en: The major advantage of Spark, however, is not yet on display. When executing
    the preceding code in a Spark session, this query will not be immediately run.
    It will be staged for execution, but Spark waits until you use this data downstream
    in a way that *requires immediate execution* before it begins doing so. This is
    called *lazy evaluation*, and it allows you to work on your data object without
    every change and interaction immediately being applied. For more details, it’s
    worth consulting a more in-depth guide like [*Learning Spark*](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)
    by Jules Damji et al. (O’Reilly), but there’s one more important characteristic
    of the Spark paradigm that is essential to discuss.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Spark 的主要优势尚未展现出来。在 Spark 会话中执行前述代码时，此查询不会立即运行。它将被准备好以执行，但 Spark 将等待直到您在下游使用此数据，并且需要*立即执行*时才开始执行。这被称为*惰性评估*，它允许您在不立即应用每个更改和交互的情况下操作数据对象。有关更多详细信息，请参阅像
    [*Learning Spark*](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)
    这样更深入的指南，由 Jules Damji 等人（O'Reilly）编写，但 Spark 范式的另一个重要特征值得讨论。
- en: Spark is natively a distributed computing language. In particular, this means
    that the preceding query—even after we force it to execute—will store its data
    on multiple computers. Spark works via a *driver program* in your program or notebook,
    which drives a *cluster manager*, which in turn coordinates *executors* on *worker
    nodes.* When we query data with Spark, instead of all that data being returned
    into a DataFrame in memory on the computer we’re using, parts of that data are
    sent to memory on the executors. And when we do a transformation on the DataFrame,
    it is applied appropriately on the pieces of the DataFrame that are stored on
    each of the executors.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 本质上是一种分布式计算语言。具体来说，这意味着即使在强制执行之后，前述查询仍将数据存储在多台计算机上。Spark 通过程序或笔记本中的*驱动程序*工作，该驱动程序驱动*集群管理器*，后者进一步协调*工作节点*上的*执行程序*。当我们使用
    Spark 查询数据时，数据并非全部返回到我们使用的计算机上的内存中的 DataFrame 中，而是将部分数据发送到执行程序的内存中。当我们对 DataFrame
    进行转换时，它将适用于存储在每个执行程序上的 DataFrame 片段。
- en: If this sounds a bit like magic, that’s because it’s obscuring a lot of technical
    details behind several convenience layers. Spark is a layer of technology that
    allows the ML engineer to program as if they’re working on one machine, and have
    those changes take effect on an entire cluster of machines. It’s not important
    to understand the network structure when querying, but it is important to be aware
    of some of these details in case things go wrong; the ability to understand what
    the error output is referring to is crucial in troubleshooting. This is all summarized
    in [Figure 6-1](#fig:sparkitecture), which is a diagram from the [Spark documentation](https://oreil.ly/89kAm).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来有点像魔术，那是因为它在几个便利层后面隐藏了许多技术细节。Spark 是一种技术层，允许机器学习工程师编程时像在一台机器上工作一样，并使这些更改在整个机器群集上生效。在查询时理解网络结构并不重要，但如果出了问题，了解一些细节是很重要的；在故障排除时理解错误输出所指的内容至关重要。这一切都在
    [图 6-1](#fig:sparkitecture) 中总结，这是来自 [Spark 文档](https://oreil.ly/89kAm) 的一张图。
- en: '![Sparkitecture](assets/brpj_0601.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Sparkitecture](assets/brpj_0601.png)'
- en: Figure 6-1\. Component architecture of Spark 3.0
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. Spark 3.0 的组件架构
- en: It’s important to note that all this does not come for free; both lazy evaluation
    and distributed DataFrames come at the cost of needing additional thought when
    writing programs. Even though Spark makes a lot of this work far easier, understanding
    how to write efficient code in this paradigm that works with the architecture
    but still achieves complicated goals can require a year’s worth of experience.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这一切并非免费；惰性评估和分布式 DataFrame 需要在编写程序时额外思考。尽管 Spark 让许多工作变得更容易，但理解如何在这种范式中编写与体系结构兼容但仍能实现复杂目标的高效代码，可能需要一年的经验积累。
- en: Returning to recommendation systems—and in particular, the offline collector—we
    want to use PySpark to build the types of datasets needed to train our models.
    One simple thing to do with PySpark is to transform our logs data into the appropriate
    form for training a model. In our simple query, we applied a few filters to our
    data and grouped by user and item to get the number of views. A variety of other
    tasks may fit naturally into this paradigm—perhaps adding user or item features
    stored in other databases, or high-level aggregations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 回到推荐系统，特别是离线收集器，我们希望使用 PySpark 构建训练模型所需的数据集。使用 PySpark 可以轻松将日志数据转换为训练模型所需的适当形式。在我们的简单查询中，我们对数据应用了一些过滤器，并按用户和项目分组以获取观看次数。许多其他任务可能自然而然地适合这种范例，例如添加存储在其他数据库中的用户或项目特征，或者进行高级聚合。
- en: 'In our MPIR, we asked for `get_item_popularities`; and we sort of assumed a
    few things:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 MPIR 中，我们要求使用 `get_item_popularities`；我们有点假设了一些事情：
- en: This would return the number of times each item was chosen.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将返回每个项目被选择的次数。
- en: This method would be fast.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法将会很快。
- en: The second point is important if the endpoint is going to be called in real
    time. So how might Spark come into play?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要实时调用终端节点，则第二点非常重要。那么 Spark 可能如何发挥作用呢？
- en: 'First, let’s assume we have a lot of data, enough that we can’t get it all
    to fit into our little MacBook Pro’s memory. Additionally, let’s continue to use
    the preceding schema. We can write an even simpler query:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们假设我们有大量数据，足以使我们无法将其全部适应我们的小 MacBook Pro 的内存中。此外，让我们继续使用前面的架构。我们可以编写一个更简单的查询：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can now write this aggregated list of `(item, count)` pairs to an app database
    to serve `get_item_popularities` (something that doesn’t require us to do any
    parsing when this is called), or potentially we can take a subset of the top-
    <math alttext="upper N"><mi>N</mi></math> of this list and store it in memory
    to get the best items with respect to a particular ranking. Either way, we’ve
    separated concerns of parsing all our log data, and doing aggregation, from the
    `get_item_popularities` function call in real time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这个聚合的`(item, count)`对列表写入应用程序数据库以提供`get_item_popularities`（当调用时不需要我们进行任何解析），或者我们可以获取此列表的前<math
    alttext="upper N"><mi>N</mi></math>个子集，并将其存储在内存中，以根据特定排名获取最佳项目。无论哪种方式，我们都已经将解析所有日志数据和进行聚合的任务与实时调用中的`get_item_popularities`函数调用分离开来。
- en: This example used an overly simple data aggregation, one just as easy to do
    in something like PostgreSQL, so why bother? The first reason is scalability.
    Spark is really built to scale horizontally, which means that as the data we need
    to access grows, we merely add more worker nodes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例使用了一个过于简单的数据聚合，可以在诸如 PostgreSQL 等数据库中轻松完成，那么为什么还要费这个劲呢？第一个原因是可伸缩性。Spark 真的是为水平扩展而构建的，这意味着随着我们需要访问的数据增长，我们只需添加更多的工作节点。
- en: The second reason is that PySpark is more than just SparkSQL; anyone who’s done
    complicated SQL queries can probably agree that the power and flexibility of SQL
    is enormous, but frequently certain tasks that you want to achieve require a lot
    of creativity to carry out in the fully SQL environment. PySpark gives you all
    the expressiveness of pandas DataFrames, Python functions and classes, and a simple
    interface to apply Python code to the PySpark data structure’s user-defined functions
    (UDFs). UDFs are similar to lambda functions that you’d use in pandas, but they’re
    built and optimized for PySpark DataFrames. As you’ve probably experienced when
    writing ML programs in smaller data regimes, at some point you switch away from
    using only SQL to using pandas API functions to perform data transformations—so
    too will you appreciate this power at the Spark data scale.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是 PySpark 不仅仅是 SparkSQL；任何完成复杂 SQL 查询的人都可能同意 SQL 的强大和灵活性是巨大的，但是经常需要在完全
    SQL 环境中执行一些你想要的任务需要很多创造力。PySpark 为您提供了 pandas DataFrames、Python 函数和类的所有表现力，以及将
    Python 代码应用于 PySpark 数据结构的用户定义函数（UDFs）的简单接口。UDFs 类似于您在 pandas 中使用的 lambda 函数，但它们是为
    PySpark DataFrames 构建和优化的。正如您在较小的数据范围内编写 ML 程序时可能遇到的情况一样，有一天您会从仅使用 SQL 切换到使用 pandas
    API 函数执行数据转换，同样您将欣赏到在 Spark 数据规模上拥有的这种功能。
- en: PySpark allows you to write what looks very much like Python and pandas code
    and have that code executed in a distributed fashion! You don’t need to write
    code to specify which worker nodes operations should happen; that’s handled for
    you by PySpark. This framework isn’t perfect; some things you expect to work may
    require a bit of care, and optimization of your code can require an additional
    level of abstraction, but generally, PySpark gives you a rapid way to move your
    code from one node to a cluster and utilize that power.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark允许您编写看起来非常像Python和pandas代码的代码，并以分布式方式执行该代码！您不需要编写代码来指定应在哪些工作节点执行操作；PySpark会为您处理这些工作。这个框架并不完美；一些您期望能够正常工作的事情可能需要一些小心，而且对代码的优化可能需要额外的抽象级别，但总的来说，PySpark为您提供了一种快速将代码从一个节点移动到一个集群并利用该能力的方法。
- en: To illustrate something a bit more useful in PySpark, let’s return to collaborative
    filtering (CF) and compute some features more relevant for ranking.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 PySpark 中更实用地说明一些内容，让我们回到协同过滤（CF）并计算一些更适合排名的特征。
- en: 'Example: User Similarity in PySpark'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：PySpark 中的用户相似度
- en: A user similarity table allows you to map a user to other users who are relevant
    to the recommender. This recalls the assumption that two similar users like similar
    things, and thus you can recommend to both users the items that one hasn’t seen.
    Constructing this user similarity table is an example of a PySpark job that you
    might see as part of the offline collector’s responsibility. Even though in many
    cases ratings would continue to stream in all the time, for the purposes of large
    offline jobs, we often think of a daily batch to update the essential tables for
    our model. In practice, in many cases this daily batch job suffices to provide
    features that are good enough for most of the ML work downstream. Other important
    paradigms exist, but those frequently *marry* the more frequent updates with these
    daily batch jobs, instead of totally eliminating them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 用户相似度表允许您将用户映射到与推荐系统相关的其他用户。这提醒了一个假设，即两个相似的用户喜欢相似的事物，因此您可以向这两个用户推荐其中一个尚未看过的项目。构建这个用户相似度表是一个PySpark作业的示例，您可能会在离线收集器的职责范围内看到。尽管在许多情况下，评分将继续不断流入，但为了大型离线作业的目的，我们通常考虑每日批处理以更新我们模型的基本表。实际上，在许多情况下，这种每日批处理作业足以提供足够好的特性，以满足大多数ML工作的需求。其他重要的范例存在，但这些范例通常将更频繁的更新与这些每日批处理作业结合起来，而不是完全消除它们。
- en: This architecture of daily batch jobs with smaller, more frequent batch jobs
    is called the *lambda architecture*, and we’ll get more into the details of how
    and why later. In brief, the two layers—batch and speed—which are distinguished
    (inversely) by the frequency of processing and the volume per run of data they
    process. Note that the speed layer may have varying frequencies associated with
    it, and it’s possible to have different speed layers for hourly, and another speed
    layer for minute-frequency jobs that do different things. [Figure 6-2](#fig:lamb-architecture)
    provides an overview of the architecture.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种每日批处理作业与更小、更频繁的批处理作业的架构称为*lambda架构*，我们将在稍后更详细地讨论如何以及为什么这样做。简言之，这两个层次——批处理和速度——通过它们处理数据的处理频率和每次运行的数据量（反向）进行区分。请注意，速度层可能具有与之关联的不同频率，并且可能会有不同的速度层，用于执行不同操作的小时和分钟频率作业。[图6-2](#fig:lamb-architecture)概述了架构。
- en: '![LambdaArchitecture](assets/brpj_0602.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![LambdaArchitecture](assets/brpj_0602.png)'
- en: Figure 6-2\. Overview of a lambda architecture
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2\. Lambda 架构概览
- en: 'In the case of user similarity, let’s work on a batch job implementation of
    computing a daily table. First we’ll need to get ratings from our schema before
    today. We’ll also include a few other filters that simulate how this query might
    look in real life:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户相似度的情况下，让我们着手实现一个计算每日表格的批处理作业。首先，我们需要从昨天之前的架构中获取评分。我们还将包括一些其他模拟这个查询在现实生活中可能看起来如何的过滤器：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As before, utilizing the SQL syntax to get the dataset into a Spark DataFrame
    is the first step, but now we have additional work on the PySpark side. A common
    pattern is to get the dataset you want to work with via simple SQL syntax and
    logic, and then use the PySpark API to do more detailed data processing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，利用SQL语法将数据集导入Spark DataFrame是第一步，但现在我们在PySpark方面有更多的工作。一个常见的模式是通过简单的SQL语法和逻辑获取要处理的数据集，然后使用PySpark
    API进行更详细的数据处理。
- en: 'Let’s first observe that we have no assumptions about uniqueness of a user-item
    rating. For the sake of this table, let’s decide that we’ll use the most recent
    rating for a pair:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们观察一下，我们对用户-项目评分的唯一性没有任何假设。为了这个表格，让我们决定使用最近的评分对：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We’ll now use `current_rating` as our ratings column for the purpose of downstream
    calculation. Recall from before our ratings-based definition of user similarity:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`current_rating`作为我们的评分列，用于下游计算。回顾之前我们基于评分定义的用户相似度：
- en: <math alttext="normal upper U normal upper S normal i normal m Subscript upper
    A comma upper B Baseline equals StartFraction sigma-summation Underscript x element-of
    script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis
    r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline
    right-parenthesis left-parenthesis r Subscript upper B comma x Baseline minus
    r overbar Subscript upper B Baseline right-parenthesis Over StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar
    Subscript upper A Baseline right-parenthesis squared EndRoot StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper B comma x Baseline minus r overbar
    Subscript upper B Baseline right-parenthesis squared EndRoot EndFraction" display="block"><mrow><msub><mi>USim</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo>
    <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi></msub>
    <mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow></mrow> <mrow><msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub> <msup><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt>
    <msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow></math>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper U normal upper S normal i normal m Subscript upper
    A comma upper B Baseline equals StartFraction sigma-summation Underscript x element-of
    script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis
    r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline
    right-parenthesis left-parenthesis r Subscript upper B comma x Baseline minus
    r overbar Subscript upper B Baseline right-parenthesis Over StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar
    Subscript upper A Baseline right-parenthesis squared EndRoot StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper B comma x Baseline minus r overbar
    Subscript upper B Baseline right-parenthesis squared EndRoot EndFraction" display="block"><mrow><msub><mi>USim</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo>
    <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi></msub>
    <mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow></mrow> <mrow><msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub> <msup><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt>
    <msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow></math>
- en: 'The important values we’ll need are as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的重要值如下：
- en: <math alttext="r Subscript left-parenthesis minus comma minus right-parenthesis"><msub><mi>r</mi>
    <mrow><mo>(</mo><mo>-</mo><mo>,</mo><mo>-</mo><mo>)</mo></mrow></msub></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="r Subscript left-parenthesis minus comma minus right-parenthesis"><msub><mi>r</mi>
    <mrow><mo>(</mo><mo>-</mo><mo>,</mo><mo>-</mo><mo>)</mo></mrow></msub></math>
- en: The rating corresponding to a user-item pair
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 用户-项目对应的评分
- en: <math alttext="r overbar Subscript left-parenthesis minus right-parenthesis"><msub><mover
    accent="true"><mi>r</mi> <mo>¯</mo></mover> <mrow><mo>(</mo><mo>-</mo><mo>)</mo></mrow></msub></math>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="r overbar Subscript left-parenthesis minus right-parenthesis"><msub><mover
    accent="true"><mi>r</mi> <mo>¯</mo></mover> <mrow><mo>(</mo><mo>-</mo><mo>)</mo></mrow></msub></math>
- en: The average rating across all items for a user
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 用户的所有项目的平均评分
- en: 'The rows are already the <math alttext="r Subscript left-parenthesis minus
    comma minus right-parenthesis"><msub><mi>r</mi> <mrow><mo>(</mo><mo>-</mo><mo>,</mo><mo>-</mo><mo>)</mo></mrow></msub></math>
    values, so let’s compute user average ratings, <math alttext="r overbar Subscript
    left-parenthesis minus right-parenthesis"><msub><mover accent="true"><mi>r</mi>
    <mo>¯</mo></mover> <mrow><mo>(</mo><mo>-</mo><mo>)</mo></mrow></msub></math> ,
    and the rating deviations:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 行已经是<math alttext="r Subscript left-parenthesis minus comma minus right-parenthesis"><msub><mi>r</mi>
    <mrow><mo>(</mo><mo>-</mo><mo>,</mo><mo>-</mo><mo>)</mo></mrow></msub></math>值，所以让我们计算用户平均评分<math
    alttext="r overbar Subscript left-parenthesis minus right-parenthesis"><msub><mover
    accent="true"><mi>r</mi> <mo>¯</mo></mover> <mrow><mo>(</mo><mo>-</mo><mo>)</mo></mrow></msub></math>和评分偏差：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now our schema should look like this (we’ve formatted it slightly nicer than
    the default Spark output):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模式应该如下所示（我们对其进行了比默认Spark输出稍微优化的格式化）：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s finish creating a dataset that contains our User Similarity calculations:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们完成创建一个包含我们用户相似度计算的数据集：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In constructing this dataset, we begin by taking a self-join, which avoids matching
    the same users with themselves but rather joins on books that match. As we do
    this join, we take the rating deviation from the user’s mean ratings that we computed
    previously. We also use this opportunity to multiply them together for the numerator
    in our user similarity function. In the last step, we’ll `groupBy` again so that
    we can sum over all matching book IDs (by `groupBy` on `user_id_1` and `user_id_2`);
    we sum the product and the powers of each set of deviations so that we can finally
    divide and generate a new column for our user similarity.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建这个数据集时，我们首先进行自连接，避免将相同用户与自身匹配，而是根据匹配的书籍进行连接。在进行此连接时，我们使用之前计算得出的用户平均评分偏差值。同时，我们利用这个机会将它们相乘，作为用户相似度函数中的分子。在最后一步，我们再次使用`groupBy`，以便对所有匹配的书籍ID（通过对`user_id_1`和`user_id_2`进行`groupBy`）进行求和；我们对每组偏差值的产品和幂进行求和，以便最终进行除法，并生成新的用户相似度列。
- en: While this computation isn’t particularly complex, let’s take note of a few
    things that we might appreciate. First, we built our user similarity matrix in
    full from our records. This matrix may now be stored in a faster-access format
    so that if we wish to do operations in real time, it’s ready to go. Second, we
    did all these data transformations in Spark, so we can run these operations on
    massive datasets and let Spark handle the parallelization onto the cluster. We
    even were able to do this while writing code that looks a lot like pandas and
    SQL. Finally, all our operations were columnar and required no iteration-based
    calculation. This means this code will scale much better than some approaches.
    This also ensures that Spark can parallelize our code well, and we can expect
    good performance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个计算并不特别复杂，但让我们注意一些我们可能会欣赏的事情。首先，我们从记录中完整构建了用户相似度矩阵。现在可以将这个矩阵存储在更快的访问格式中，因此如果我们希望进行实时操作，它已经准备好了。其次，我们在Spark中完成了所有这些数据转换，因此可以在大数据集上运行这些操作，并让Spark处理并行化到集群上。我们甚至能够编写类似于pandas和SQL的代码。最后，所有我们的操作都是基于列的，不需要迭代计算。这意味着这段代码将比某些其他方法更好地扩展。这还确保了Spark能够很好地并行化我们的代码，并且我们可以期望良好的性能。
- en: We’ve seen how PySpark can be used to prepare our user similarity matrix. We
    have this definition of affinity estimating the appropriateness of an item for
    a user; we can collect each of those scores into a tabular form—user rows and
    item columns—to yield a matrix. As an exercise, can you take this matrix and generate
    the affinity matrix?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了PySpark如何用于准备我们的用户相似度矩阵。我们有这样一个定义的亲和力，用于评估一个项目对于用户的适宜性；我们可以将这些分数收集到一个表格中——用户行和项目列——以生成一个矩阵。作为一项练习，你能否拿这个矩阵并生成亲和力矩阵？
- en: <math alttext="normal upper A normal f normal f Subscript upper A comma i Baseline
    equals r overbar Subscript upper A Baseline plus StartFraction sigma-summation
    Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis
    Endscripts normal upper U normal upper S normal i normal m Subscript upper A comma
    upper U Baseline asterisk left-parenthesis r Subscript upper U comma i Baseline
    minus r overbar Subscript upper A Baseline right-parenthesis Over sigma-summation
    Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis
    Endscripts normal upper U normal upper S normal i normal m Subscript upper A comma
    upper U Baseline EndFraction" display="block"><mrow><msub><mi>Aff</mi> <mrow><mi>A</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>=</mo> <msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi></msub>
    <mo>+</mo> <mfrac><mrow><msub><mo>∑</mo> <mrow><mi>U</mi><mo>∈</mo><mi>𝒩</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></msub>
    <msub><mi>USim</mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow></msub> <mo>*</mo><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>U</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow></mrow> <mrow><msub><mo>∑</mo>
    <mrow><mi>U</mi><mo>∈</mo><mi>𝒩</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></msub>
    <msub><mi>USim</mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow></msub></mrow></mfrac></mrow></math>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper A normal f normal f Subscript upper A comma i Baseline
    equals r overbar Subscript upper A Baseline plus StartFraction sigma-summation
    Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis
    Endscripts normal upper U normal upper S normal i normal m Subscript upper A comma
    upper U Baseline asterisk left-parenthesis r Subscript upper U comma i Baseline
    minus r overbar Subscript upper A Baseline right-parenthesis Over sigma-summation
    Underscript upper U element-of script upper N left-parenthesis upper A right-parenthesis
    Endscripts normal upper U normal upper S normal i normal m Subscript upper A comma
    upper U Baseline EndFraction" display="block"><mrow><msub><mi>Aff</mi> <mrow><mi>A</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>=</mo> <msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi></msub>
    <mo>+</mo> <mfrac><mrow><msub><mo>∑</mo> <mrow><mi>U</mi><mo>∈</mo><mi>𝒩</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></msub>
    <msub><mi>USim</mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow></msub> <mo>*</mo><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>U</mi><mo>,</mo><mi>i</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow></mrow> <mrow><msub><mo>∑</mo>
    <mrow><mi>U</mi><mo>∈</mo><mi>𝒩</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></msub>
    <msub><mi>USim</mi> <mrow><mi>A</mi><mo>,</mo><mi>U</mi></mrow></msub></mrow></mfrac></mrow></math>
- en: Feel free to assume that <math alttext="script upper N left-parenthesis upper
    A right-parenthesis"><mrow><mi>𝒩</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo></mrow></math>
    is just the five nearest neighbors to <math alttext="upper A"><mi>A</mi></math>
    with respect to user similarity.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请随意假设<math alttext="script upper N left-parenthesis upper A right-parenthesis"><mrow><mi>𝒩</mi>
    <mo>(</mo> <mi>A</mi> <mo>)</mo></mrow></math>只是关于用户相似性的<math alttext="upper A"><mi>A</mi></math>的五个最近邻。
- en: DataLoaders
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载器
- en: DataLoaders is a programming paradigm originating from PyTorch, but it has been
    embraced in other gradient-optimized ML workflows. As we begin to integrate gradient-based
    learning into our recommendation system architectures, we will face challenges
    in our MLOps tooling. The first is related to training data size and available
    memory. DataLoaders are a way to prescribe how data is batched and sent to the
    training loop efficiently; as datasets get large, careful scheduling of these
    training sets can have major effects on learning. But why must we think about
    *batches* of data? That’s because we’ll use a variant of gradient descent appropriate
    for large amounts of data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载器是源自PyTorch的编程范式，但已经在其他梯度优化的ML工作流中得到了应用。随着我们开始将基于梯度的学习整合到我们的推荐系统架构中，我们将在MLOps工具中面临挑战。第一个与训练数据大小和可用内存有关。数据加载器是一种指定数据如何被批处理并有效地发送到训练循环中的方式；随着数据集变大，这些训练集的谨慎调度可能会对学习产生重大影响。但为什么我们必须考虑数据的*批次*？那是因为我们将使用一种适用于大量数据的梯度下降的变体。
- en: 'First, let’s review the basics of *mini-batched gradient descent*. During training
    via gradient descent, we make a forward pass of our training sample through our
    model to yield a prediction, and we then compute the error and the appropriate
    gradient backward through our model to update parameters. Batched gradient descent
    takes all our data in a single pass to compute the gradient for the training set
    and push it back through; this implies you have the entire training dataset in
    memory. As the dataset scales, this ranges from expensive to impossible; to avoid
    this, we can instead compute gradients of the loss function for only a subset
    of the dataset at a time. The simplest paradigm for this, called *stochastic gradient
    descent* (SGD), computes these gradients and parameter updates one sample at a
    time. The mini-batched version performs our batched gradient descent, but over
    a series of subsets to form a partition of our dataset. In mathematical notation,
    we write the update rule in terms of Jacobians on the smaller batches:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下*小批量梯度下降*的基础知识。在通过梯度下降进行训练期间，我们对训练样本进行前向传播，得出预测结果，然后通过我们的模型计算错误和适当的梯度向后传播以更新参数。批量梯度下降在单次传递中获取所有数据以计算训练集的梯度并将其传回；这意味着您在内存中拥有整个训练数据集。随着数据集的扩大，这从昂贵到不可能；为了避免这种情况，我们可以只计算一次数据集的一部分的损失函数的梯度。这种最简单的范式称为*随机梯度下降*（SGD），它逐个样本计算这些梯度和参数更新。小批量版本执行我们的批量梯度下降，但是在一系列子集上进行，以形成数据集的分区。在数学表示中，我们根据较小的批次编写更新规则：
- en: <math alttext="theta equals theta minus eta asterisk nabla Subscript theta Baseline
    upper J left-parenthesis theta semicolon x Superscript left-parenthesis i colon
    i plus n right-parenthesis Baseline semicolon y Superscript left-parenthesis i
    colon i plus n right-parenthesis Baseline right-parenthesis" display="block"><mrow><mi>θ</mi>
    <mo>=</mo> <mi>θ</mi> <mo>-</mo> <mi>η</mi> <mo>*</mo> <msub><mi>∇</mi> <mi>θ</mi></msub>
    <mi>J</mi> <mfenced close=")" open="(" separators=""><mi>θ</mi> <mo>;</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>;</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>n</mi><mo>)</mo></mrow></msup></mfenced></mrow></math>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="theta equals theta minus eta asterisk nabla Subscript theta Baseline
    upper J left-parenthesis theta semicolon x Superscript left-parenthesis i colon
    i plus n right-parenthesis Baseline semicolon y Superscript left-parenthesis i
    colon i plus n right-parenthesis Baseline right-parenthesis" display="block"><mrow><mi>θ</mi>
    <mo>=</mo> <mi>θ</mi> <mo>-</mo> <mi>η</mi> <mo>*</mo> <msub><mi>∇</mi> <mi>θ</mi></msub>
    <mi>J</mi> <mfenced close=")" open="(" separators=""><mi>θ</mi> <mo>;</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>n</mi><mo>)</mo></mrow></msup>
    <mo>;</mo> <msup><mi>y</mi> <mrow><mo>(</mo><mi>i</mi><mo>:</mo><mi>i</mi><mo>+</mo><mi>n</mi><mo>)</mo></mrow></msup></mfenced></mrow></math>
- en: This optimization serves a few purposes. First, it requires only potentially
    small subsets of our data held in memory during the steps. Second, it requires
    far fewer passes than the purely iterative version in SGD. Third, the gradient
    operating on these mini-batches can be organized as a Jacobian, and thus we have
    linear-algebraic operations that may be highly optimized.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优化有几个目的。首先，在这些步骤期间，它仅需要可能很小的数据子集保存在内存中。其次，它比SGD中的纯迭代版本需要的传递要少得多。第三，对这些小批量的梯度操作可以组织为雅可比矩阵，因此我们有可能高度优化的线性代数运算。
- en: Jacobians
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 雅可比矩阵
- en: The mathematical notion of a Jacobian in the simplest sense is an organizational
    tool for a set of vector derivatives with relevant indexes. You may recall that
    for functions of several variables, you can take the derivative *with respect
    to* each of those variables. For a single multivariable scalar function, the Jacobian
    is simply the row vector of first derivatives of the function—which happens to
    be the transpose of the gradient.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数学概念中的雅可比矩阵最简单的意义是一种用于一组具有相关索引的向量导数的组织工具。您可能还记得，对于多个变量的函数，您可以相对于每个变量进行导数计算。对于单个多变量标量函数，雅可比矩阵简单地是该函数的一阶导数的行向量——恰好是梯度的转置。
- en: This is the simplest case; the gradient of a multivariable scalar function may
    be written as a Jacobian. However, once we have a vector of (vector) derivatives,
    we can write that as a matrix; the utility here is really only in the notation,
    though. When you collect a series of multivariable scalar functions into a vector
    of functions, the associated vector of gradients is a vector of vectors of derivatives.
    This is called a *Jacobian matrix*, and it generalizes the gradient to vector-valued
    functions. As you’ve likely realized, layers of neural networks are a great source
    of vector-valued functions for which you’d like to derivate.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的情况；多变量标量函数的梯度可以写成雅可比矩阵。然而，一旦我们有了（向量）导数的向量，我们可以将其写成矩阵；这里的实用性实际上只在于符号，虽然如此。当您将一系列多变量标量函数收集成函数向量时，相关的梯度向量是导数的向量。这称为
    *雅可比矩阵*，它将梯度推广到矢量值函数。正如您可能已经意识到的那样，神经网络层是希望进行导数推导的矢量值函数的绝佳来源。
- en: 'If you’re convinced mini-batches are useful, it’s time to discuss *DataLoaders*—a
    simple PyTorch API for facilitating mini-batch access from a large dataset. The
    key parameters for a DataLoader are `batch_size`, `shuffle`, and `num_workers`.
    The batch size is easy to understand: it’s the number of samples included in each
    batch (often an integer factor of the total size of the dataset). Often a shuffle
    operation is applied in serving up these batches; the shuffle allows batches in
    each epoch to be shown to the network in a randomized order; this is intended
    to improve robustness. Finally, `num_workers` is a parallelization parameter for
    the CPU’s batch generation.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您确信小批量很有用，现在是讨论 *DataLoaders* 的时候了——这是一个简单的 PyTorch API，用于从大型数据集中提取小批量。DataLoader
    的关键参数包括 `batch_size`、`shuffle` 和 `num_workers`。批次大小很容易理解：每个批次中包含的样本数量（通常是数据集总大小的整数因子）。通常会对这些批次应用随机顺序操作；这旨在提高模型的稳健性。最后，`num_workers`
    是用于 CPU 批次生成的并行化参数。
- en: 'The utility of a DataLoader is really best understood via demonstration:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: DataLoader 的效用最好通过演示来理解：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first important detail in this code is that any of its generators will be
    reading in mini-batches from your total dataset and can be instructed to load
    those batches in parallel. Note also that any differential steps in the model
    computations will now be operating on these mini-batches.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的第一个重要细节是，它的任何生成器都将从您的总数据集中读取小批量，并可以指示并行加载这些批次。还要注意，模型计算中的任何差分步骤现在将在这些小批量上运行。
- en: It’s easy to think of DataLoaders as merely a tool for code cleanliness (which,
    admittedly, it does improve), but it’s important to not underestimate how the
    control of batch order, parallelization, and shape are significant features for
    training your model. Lastly, the structure of your code now looks like batch gradient
    descent, but it is taking advantage of mini-batching, further exposing what your
    code actually does instead of the steps necessary to do it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 容易认为 DataLoaders 只是用于提高代码清洁度的工具（诚然，它确实改进了），但重要的是不要低估批次顺序、并行化和形状控制对模型训练的重要性。最后，您的代码结构现在看起来像是批量梯度下降，但它利用了小批量，进一步暴露了代码实际执行的内容而非所需步骤。
- en: Database Snapshots
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库快照
- en: 'Let’s round out this section by stepping back from these fancy technologies
    to discuss something important and classic: snapshotting a production database.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过远离这些高级技术来结束这一部分，讨论一些重要而经典的事情：对生产数据库进行快照。
- en: An extremely likely scenario is that the engineers (potentially also you) who
    have built the recommendations server are writing their logs and other application
    data to an SQL database. More likely than not, this database architecture and
    deployment are optimized for fast querying by the application across its most
    common use cases. As we’ve discussed, those logs may be in an event-style schema,
    and there are other tables that may require aggregation and roll-up to make any
    sense. For example, a *current inventory* table may require knowledge of start-of-day
    inventory and then aggregate a list of purchase events.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 极有可能的情况是，建立推荐服务器的工程师们（可能还包括您）正在将其日志和其他应用程序数据写入 SQL 数据库。很可能，这种数据库架构和部署是针对应用程序跨其最常见使用情况进行快速查询进行了优化。正如我们讨论过的那样，这些日志可能处于事件样式架构中，还有其他可能需要聚合和汇总以得出任何意义的表格。例如，*当前库存*
    表可能需要了解每天开始的库存，然后聚合一系列购买事件列表。
- en: 'All told, the production SQL database is usually a crucial component in the
    stack that’s geared to specific use. As the downstream consumer of this data,
    you may find yourself wanting different schemas, wanting lots of access to this
    database, and performing serious operations on this data. The most common paradigm
    is *database snapshotting*. Snapshotting is a functionality provided by various
    flavors of SQL to performantly make a clone of a database. While this snapshotting
    may take form in a variety of ways, let’s focus on a few that serve to simplify
    our systems and ensure they have the necessary data on hand:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，生产SQL数据库通常是针对特定用途设计的堆栈中的关键组件。作为这些数据的下游消费者，您可能希望拥有不同的模式，对这些数据库有大量的访问，并对这些数据执行重要的操作。最常见的范式是*数据库快照*。快照是由各种SQL版本提供的功能，用于高效地创建数据库的克隆。虽然快照可以采用各种形式，但让我们专注于一些简化系统并确保其具备所需数据的方式：
- en: A daily table snapshot may be tied to an `as_of` field, or *the state of this
    table on this day*.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每日表快照可能与`as_of`字段相关联，或者*这个表在这一天的状态*。
- en: A daily table snapshot may be limited by time to see *what records have been
    added today*.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每日表快照可能受时间限制，只能查看*今天新增了哪些记录*。
- en: An event table snapshot may be used to feed a set of events into an event stream
    processor like Segment (note that you may also set up live event streams like
    Kafka).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件表快照可用于将一组事件馈送到像Segment这样的事件流处理器（请注意，您也可以设置像Kafka这样的实时事件流）。
- en: An hourly aggregated table can be used for status logging or monitoring.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每小时汇总的表可用于状态记录或监视。
- en: In general, the paradigm is usually to operate on snapshots for downstream data
    processing. Many of the kinds of data processing we mentioned earlier—like computing
    user similarity—are operations that may require significant data reads. *It’s
    important to not build ML applications that require extensive querying on the
    production database*, because doing so would likely decrease performance of the
    app and result in a slower user experience. This decrease will undermine the improvement
    made possible by your recommendations.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，范式通常是操作下游数据处理的快照。我们前面提到的许多数据处理种类，如计算用户相似性，都是可能需要大量数据读取的操作。*重要的是不要构建需要在生产数据库上进行大量查询的ML应用程序*，因为这样做可能会降低应用程序的性能，并导致用户体验变慢。这种降低将会损害推荐系统可能实现的改进。
- en: Once you’ve snapshotted the tables you’re interested in, you can often find
    a collection of data pipelines useful to transform that data into even more specific
    tables in your *data warehouse* (where you should be doing most of your work anyway).
    Tools like Dagster, dbt, Apache Airflow, Argo, and Luigi are popular data-pipeline
    and workflow orchestration tools for extract, transform, load (ETL) operations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您对感兴趣的表进行了快照，通常可以找到一系列数据管道，有助于将数据转换为更具体的表格，存放在*数据仓库*中（您应该在这里完成大部分工作）。像Dagster、dbt、Apache
    Airflow、Argo和Luigi这样的工具是流行的数据管道和工作流编排工具，用于提取、转换和加载（ETL）操作。
- en: Data Structures for Learning and Inference
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于学习和推断的数据结构
- en: This section introduces three important data structures that will enable our
    recommendation system to perform complex operations quickly. The goal of each
    structure is to sacrifice precision as little as possible, while speeding up access
    to the data in real time. As you’ll see, these data structures form the backbone
    of the real-time inference pipeline and approximate what takes place in the batch
    pipeline as accurately as possible.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了三种重要的数据结构，这些结构将使我们的推荐系统能够快速执行复杂操作。每种结构的目标是尽可能少地牺牲精度，同时加速对实时数据的访问。正如您将看到的，这些数据结构构成了实时推断管道的核心，并尽可能精确地近似了批处理管道的运行过程。
- en: 'The three data structures are as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种数据结构如下：
- en: Vector search/ANN index
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量搜索/ANN索引
- en: Bloom filters for candidate filtering
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布隆过滤器用于候选过滤
- en: Feature stores
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征存储
- en: So far, we’ve discussed the necessary components for getting data flowing in
    your system. These help organize data to make it more accessible during the learning
    and inference processes. Also, we’ll find some shortcuts to speed up inference
    during retrieval. Vector search will allow us to identify similar items at scale.
    Bloom filters will allow us to rapidly evaluate many criteria for excluding results.
    Feature stores will provide us with necessary data about users for recommendation
    inference.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了在系统中使数据流动所必需的组件。这些帮助组织数据，使其在学习和推断过程中更易于访问。此外，我们还将找到一些快捷方式来加速检索过程中的推断。向量搜索将允许我们在规模上识别相似的项目。布隆过滤器将允许我们快速评估许多排除结果的标准。特征存储将为我们提供有关用户的推荐推断所需的必要数据。
- en: Vector Search
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量搜索
- en: We have discussed user similarity and item similarity in terms of understanding
    the relationships between those entities, but we haven’t talked about any *acceleration
    structures* for these processes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 就理解这些实体之间的关系而言，我们已经讨论了用户相似性和项目相似性，但我们还没有谈论这些过程的任何*加速结构*。
- en: First let’s discuss a bit of terminology;. If we think of a collection of vectors
    that represent entities with a similarity metric provided by a distance function,
    we refer to this as a *latent space.* The simple goal is to utilize our latent
    space and its associated similarity metric (or complementary distance metric)
    to be able to retrieve *similar* items quickly. In our previous examples with
    similarity, we talked about neighborhoods of users and how they can be utilized
    to build an affinity score between users and unseen items. But how do you find
    the neighborhood?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们讨论一些术语；如果我们将表示具有由距离函数提供的相似度度量的实体的向量集合视为一个*潜空间*。简单的目标是利用我们的潜空间及其相关的相似度度量（或补充距离度量），以便能够快速检索*相似的*项目。在我们以前的相似性示例中，我们谈到了用户邻域及其如何被利用来建立用户与未见项目之间的亲和分数。但是你如何找到这个邻域呢？
- en: To understand this, recall that we defined neighborhoods of an element <math
    alttext="x"><mi>x</mi></math> , written <math alttext="script upper N left-parenthesis
    x right-parenthesis"><mrow><mi>𝒩</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    , as the set of <math alttext="k"><mi>k</mi></math> elements in the latent space
    with the maximum similarity; or said differently, the set of <math alttext="j"><mi>j</mi></math>
    th order statistics for <math alttext="j less-than-or-equal-to k"><mrow><mi>j</mi>
    <mo>≤</mo> <mi>k</mi></mrow></math> from the sample of item similarities to <math
    alttext="x"><mi>x</mi></math> . These *<math alttext="k"><mi>k</mi></math> -nearest
    neighbors*, as they’re often called, will be used as the set of elements considered
    similar to <math alttext="x"><mi>x</mi></math> .
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这一点，请回想一下我们定义了一个元素<math alttext="x"><mi>x</mi></math>的邻域，写成<math alttext="script
    upper N left-parenthesis x right-parenthesis"><mrow><mi>𝒩</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math>，作为潜空间中与最大相似度的<math alttext="k"><mi>k</mi></math>个元素的集合；或者换句话说，项目相似性样本的<math
    alttext="j"><mi>j</mi></math>阶统计的集合小于或等于<math alttext="k"><mi>k</mi></math>。这些*<math
    alttext="k"><mi>k</mi></math> -最近邻居*，通常被称为，将用作被视为与<math alttext="x"><mi>x</mi></math>相似的元素集合。
- en: 'These vectors from CF yield a few other useful side effects:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从CF得出的这些向量还产生了一些其他有用的副作用：
- en: A simple recommender that randomly samples unseen items from a user neighborhood’s
    liked items
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的推荐系统，从用户邻域的喜欢项目中随机抽取未见项目
- en: Predictions about features of a user, from known features of users in the neighborhood
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于用户特征的预测，从邻域中已知用户的已知特征
- en: User segmentation via taste similarity
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过口味相似性进行用户分割
- en: So how can we speed up these processes? One of the first significant improvements
    in this area came from inverted indices. Utilizing inverted indices is at its
    core carefully constructing a large hash between tokens of the query (for text-based
    search) and the candidates.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何加速这些过程呢？这个领域的第一个重大改进之一来自倒排索引。利用倒排索引的核心是在查询的标记之间（用于基于文本的搜索）和候选项之间谨慎地构建一个大哈希。
- en: 'This approach is great for tokenizable entities like sentences or small-lexicon
    collections. Given the ability to look up items that share one or many tokens
    with the query, you can even use a general latent embedding to rank the candidate
    responses by similarity. This approach deserves extra consideration as you scale:
    it incurs a speed cost because it entails two steps, and because the similarity
    distribution may not be well correlated with the token similarity required to
    return many more candidates than we need.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法非常适合像句子或小词典集合这样的可标记化实体。由于能够查找与查询共享一个或多个标记的项目，您甚至可以使用一般的潜在嵌入来按相似性对候选响应进行排名。随着规模化，这种方法值得额外考虑：它会产生速度成本，因为它需要两个步骤，并且因为相似性分布可能与返回比我们需要的更多候选者所需的标记相似性不相关。
- en: Classic approaches to building a search system are based on large lookup tables
    and feel deterministic. As we move toward ANN lookup, we want to relax some of
    that strong deterministic behavior and introduce data structures that make assumptions
    to *prune* these large indices. Instead of building indices for only tokenizable
    components of your elements, you could precompute the *k*-d tree and use the indices
    as the index. The *k*-d tree would precompute the nearest neighbors in a batch
    process (which may be slow), to populate a top-*k* response for fast lookup. *k*-d
    trees are an efficient data structure for encoding the preceding neighborhoods
    but are notoriously slow to read from in higher dimensions. Using them instead
    to build inverted indices, though, can be a great improvement.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 构建搜索系统的经典方法基于大型查找表，并具有确定性的感觉。随着我们转向ANN查找，我们希望放松一些强大的确定性行为，并引入能够“修剪”这些大索引的数据结构的假设。与仅为元素的可标记化组件构建索引不同，您可以预先计算*k*-d树并使用索引作为索引。
    *k*-d树将以批处理过程预计算最近邻居（这可能很慢），以填充用于快速查找的前*k*个响应。 *k*-d树是一种有效的数据结构，用于编码前述邻域，但在更高维度中读取时已众所周知较慢。然而，使用它们来构建倒排索引可以带来很大的改进。
- en: More recently, explicitly using vector databases with vector search is becoming
    much more possible and feasible. Elasticsearch has added this capability; [Faiss](https://oreil.ly/AZ-Ai)
    is a Python library that helps you implement this functionality in your systems;
    [Pinecone](https://oreil.ly/LSaos) is a vector-database system explicitly targeting
    this goal; and [Weaviate](https://oreil.ly/Z6la_) is a native vector-database
    architecture that allows you to layer the previous token-based inverted indices
    and vector similarity search.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，明确使用向量数据库进行向量搜索变得越来越可能和可行。Elasticsearch已添加了这一功能；[Faiss](https://oreil.ly/AZ-Ai)是一个Python库，可以帮助您在系统中实现此功能；[Pinecone](https://oreil.ly/LSaos)是一种专门针对此目标的向量数据库系统；而[Weaviate](https://oreil.ly/Z6la_)是一种本地向量数据库架构，允许您在前述基于标记的倒排索引和向量相似性搜索之上构建层。
- en: Approximate Nearest Neighbors
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 近似最近邻
- en: What are this element’s *k*-nearest neighbors? Incredibly, approximate nearest
    neighbors (ANN) can get very high accuracy compared to the actual nearest neighbors,
    and you get there faster with head-spinning speedups. You often are satisfied
    with approximate solutions to these problems.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个元素的*k*个最近邻是什么？令人惊讶的是，近似最近邻（ANN）与实际最近邻相比可以达到非常高的准确性，并且通过令人眼花缭乱的加速技术更快地到达那里。对于这些问题，您经常对近似解决方案感到满意。
- en: 'One open source library that specializes in these approximations is [PyNNDescent](https://oreil.ly/i5LyM),
    which uses clever speedups via both optimized implementation and careful mathematical
    tricks. With ANN, you are opened up to two strategies as discussed:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 专门处理这些近似值的开源库之一是[PyNNDescent](https://oreil.ly/i5LyM)，它通过优化的实现和精心设计的数学技巧实现了巧妙的加速。使用ANN，您可以采用如下讨论的两种策略：
- en: The pre-index can be dramatically improved.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以显著改善预索引。
- en: On queries without a pre-indexing option, you can still expect good performance.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在没有预先索引选项的查询中，您仍可以期望良好的性能。
- en: 'In practice, these similarity lookups are incredibly important for making your
    applications actually work. While we’ve mostly talked about recommendations for
    full known catalogs of items, we cannot assume this in other recommendation contexts.
    These include the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这些相似性查找对于使您的应用程序实际工作非常重要。虽然我们大多数时候都在讨论已知项目目录的推荐，但我们不能在其他推荐上下文中做出这种假设。这些上下文包括以下内容：
- en: Query-based recommendations (like search)
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于查询的推荐（如搜索）
- en: Contextual recommendations
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文推荐
- en: Cold-starting new items
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冷启动新项目
- en: 'As we go, you will see more and more references to similarity in spaces and
    nearest neighbors; at each of those moments, think: “I know how to make this fast!”'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入研究相似性和最近邻时，你会越来越多地看到这些参考，每当这些时刻到来时，请想一想：“我知道如何使这个过程变得快速！”
- en: Bloom Filters
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 布隆过滤器
- en: '*Bloom filters* are probabilistic data structures that allow us to test for
    set inclusion very efficiently but with a downside: set exclusion is deterministic,
    but set inclusion is probabilistic. *In practice, this means that asking the question
    “Is <math alttext="x"><mi>x</mi></math> in this set” never results in a false
    negative but may result in a false positive!* Note that this type-I error increases
    as the size of the bloom increases.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*布隆过滤器*是一种概率数据结构，允许我们非常高效地测试集合的包含性，但有一个缺点：集合的排除是确定的，但集合的包含是概率性的。*实际上，这意味着询问“<math
    alttext="x"><mi>x</mi></math>是否在这个集合中”永远不会产生假阴性，但可能会产生假阳性！* 请注意，这种类型-I错误随着布隆大小的增加而增加。'
- en: Via vector search, we have identified a large pool of potential recommendations
    for the user. From this pool, we need to do some immediate elimination. The most
    obvious type of high-level filtering that’s essential is to remove those items
    that the *user has previously not shown interest in or has already purchased.*
    You’ve probably had the experience of being recommended the same item, over and
    over, and thinking, “I don’t want this; stop showing me this.” From the simple
    CF models we’ve introduced, you may now see why this could happen.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向量搜索，我们已经确定了用户的大量潜在推荐项目。从这些项目中，我们需要立即进行一些排除。最明显的高级过滤类型是删除*用户以前没有显示兴趣或已经购买的商品*。你可能经历过被反复推荐同一件商品的经历，然后想：“我不想要这个，请别再给我看了。”通过我们介绍的简单协同过滤模型，你现在可能会明白为什么会出现这种情况。
- en: 'The system has identified a set of items via CF that you’re more likely to
    pick. Without any outside influence, those computations will continue to return
    the same results, and you’ll never escape those recommendations. As the system
    designer, you may start with a heuristic:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 系统已经通过CF识别出了一组更有可能被你选择的项目。在没有任何外部影响的情况下，这些计算将继续返回相同的结果，你将永远无法摆脱这些推荐。作为系统设计师，你可以从一个启发式开始：
- en: If the user has seen this item recommended three times and never clicked, let’s
    not show it to them anymore.
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果用户看到这个推荐的商品三次都没有点击，我们就不再向他们展示了。
- en: This is a totally reasonable strategy to improve *freshness* (the idea of ensuring
    users see new item recommendations) in your recommendation system. While this
    is a simple strategy to improve your recommendations, how might you implement
    this at scale?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种完全合理的策略，旨在改善你推荐系统中的*新鲜度*（确保用户看到新项目推荐的想法）。虽然这是改善推荐的简单策略，但你如何在规模上实现它呢？
- en: 'A bloom filter may be used by defining the sets in question with the following:
    “Has this user seen this item recommended three times and never clicked?” Bloom
    filters have a caveat that they’re additive only: once something is in the bloom,
    you can’t remove it. This is not a problem when observing a binary state like
    this heuristic.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过定义以下的集合，可以使用布隆过滤器：“这个用户看到这个推荐的商品三次但从未点击过吗？”布隆过滤器的一个注意事项是它们仅能进行添加操作：一旦某物被加入布隆中，就无法将其移除。当观察这种启发式的二进制状态时，这并不是一个问题。
- en: Let’s construct a user-item ID to use as our hash in the bloom. Remember that
    the key feature of the bloom filter is to quickly determine whether the hashed
    item is in the bloom. When we observe a user-item pair that satisfies the preceding
    criteria, take that pair as an ID and hash it. Now, because that hashed pair can
    be easily reconstructed from a list of items for a user, we have a very fast way
    to filter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个用户-商品ID，用作我们在布隆中的哈希。请记住，布隆过滤器的关键特性是快速确定哈希后的项目是否在布隆中。当我们观察到满足上述标准的用户-商品对时，请将该对作为ID并进行哈希。现在，由于可以从用户的项目列表中轻松重建这个哈希对，我们有了一种非常快速的过滤方式。
- en: Let’s discuss a few technical details on this topic. First, you might want to
    do a variety of kinds of filtering—maybe freshness is one, and another may be
    items the user has already bought, and a third could exclude items that have sold
    out.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下这个话题的一些技术细节。首先，你可能希望进行各种类型的过滤 —— 也许新鲜度是其中之一，另一个可能是用户已经购买的商品，第三个可能是排除已售罄的商品。
- en: Here it would be good to implement each of these filters independently; the
    first two can follow our user-item ID hashing as before, and the third one can
    be a hash only on item IDs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，独立实施每个过滤器将是很好的；前两个可以像以前一样遵循我们的用户-商品ID哈希，第三个可以仅对商品ID进行哈希。
- en: Another consideration is populating the bloom filters. It’s best practice to
    build these blooms from a database during the offline batch jobs. On whatever
    schedule your batch training is run, rebuild your blooms from the records storage
    to ensure you’re keeping your blooms accurate. Remember that blooms don’t allow
    deletion, so in the previous example, if an item goes from sold out to restocked,
    your batch refresh of your blooms can pick up the availability again. In between
    batch retraining, adding to a bloom is also very performant, so you can continue
    to add to the bloom as you observe more data that needs to be considered for the
    filtering in real time. Be sure these transactions are logged to a table, though!
    That logging will be important when you want to refresh.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个考虑因素是填充布隆过滤器。最佳实践是在离线批处理作业期间从数据库构建这些布隆过滤器。无论您的批处理训练按什么时间表运行，都要从记录存储重新构建布隆过滤器，以确保保持布隆过滤器的准确性。请记住，布隆过滤器不允许删除，因此在前面的示例中，如果某个项目从售罄状态变为补货状态，您的批量刷新操作可以重新捕捉其可用性。在批量重新训练之间，向布隆过滤器添加内容也非常高效，因此您可以在实时过滤需要考虑的更多数据时继续向布隆过滤器添加内容。但务必将这些事务记录到表中！当您想要刷新时，这些日志记录将非常重要。
- en: 'Fun Aside: Bloom Filters as the Recommendation System'
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有趣的一点：布隆过滤器作为推荐系统
- en: Bloom filters not only provide an effective way to eliminate some recommendations
    based on conditions for inclusion, but can also be used to do the recommending
    itself! In particular, [“An Item/User Representation for Recommender Systems Based
    on Bloom Filters”](https://oreil.ly/VsvN2) by Manuel Pozo et al. shows that for
    high-dimensional feature sets with a lot of sparsity (as we discussed in [Chapter 3](ch03.html#ch:math)),
    the type of hashing bloom filters do can help overcome some of the key challenges
    in defining good similarity functions!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 布隆过滤器不仅提供了一种有效的方式来根据包含条件消除某些推荐，而且还可以用来执行推荐本身！特别是，Manuel Pozo 等人在 [“An Item/User
    Representation for Recommender Systems Based on Bloom Filters”](https://oreil.ly/VsvN2)
    中指出，对于具有高维特征集和大量稀疏性的推荐系统（正如我们在 [第 3 章](ch03.html#ch:math) 中讨论的那样），布隆过滤器所做的哈希类型可以帮助克服定义良好相似性函数的关键挑战！
- en: 'Let’s observe that we can do two natural operations on sets via the bloom filter
    data structures. First, consider two sets <math alttext="upper A"><mi>A</mi></math>
    and <math alttext="upper B"><mi>B</mi></math> , and associate to them bloom filters
    <math alttext="script upper B script upper F Subscript upper A"><msub><mi>ℬℱ</mi>
    <mi>A</mi></msub></math> and <math alttext="script upper B script upper F Subscript
    upper B"><msub><mi>ℬℱ</mi> <mi>B</mi></msub></math> . Then what’s the definition
    of <math alttext="upper A intersection upper B"><mrow><mi>A</mi> <mo>∩</mo> <mi>B</mi></mrow></math>
    ? Can we come up with a bloom filter for this intersection? Yep! Recall that our
    bloom filters are guaranteed to tell us when an element is not contained in the
    set, but if an element is in the set, the bloom filter can respond with only a
    certain probability. In this case, we’d simply look for elements that are *in*
    according to <math alttext="script upper B script upper F Subscript upper A"><msub><mi>ℬℱ</mi>
    <mi>A</mi></msub></math> *AND* *in* according to <math alttext="script upper B
    script upper F Subscript upper B"><msub><mi>ℬℱ</mi> <mi>B</mi></msub></math> .
    Of course, the set of items returned as *in* each set is larger than the actual
    set (i.e., <math alttext="upper A subset-of script upper B script upper F Subscript
    upper A"><mrow><mi>A</mi> <mo>⊂</mo> <msub><mi>ℬℱ</mi> <mi>A</mi></msub></mrow></math>
    ), so the intersection will also be larger:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们观察到，通过布隆过滤器数据结构，我们可以对集合执行两种自然操作。首先，考虑两个集合 <math alttext="upper A"><mi>A</mi></math>
    和 <math alttext="upper B"><mi>B</mi></math>，并为它们关联布隆过滤器 <math alttext="script
    upper B script upper F Subscript upper A"><msub><mi>ℬℱ</mi> <mi>A</mi></msub></math>
    和 <math alttext="script upper B script upper F Subscript upper B"><msub><mi>ℬℱ</mi>
    <mi>B</mi></msub></math>。那么 <math alttext="upper A intersection upper B"><mrow><mi>A</mi>
    <mo>∩</mo> <mi>B</mi></mrow></math> 的定义是什么？我们能为这个交集设计一个布隆过滤器吗？当然可以！回想一下，我们的布隆过滤器保证能告诉我们元素不在集合中，但如果元素在集合中，则布隆过滤器只能以一定的概率回应。在这种情况下，我们只需查找根据
    <math alttext="script upper B script upper F Subscript upper A"><msub><mi>ℬℱ</mi>
    <mi>A</mi></msub></math> 和 <math alttext="script upper B script upper F Subscript
    upper B"><msub><mi>ℬℱ</mi> <mi>B</mi></msub></math> 声明为“在”的元素。当然，每个集合返回的“在”元素集合都比实际集合更大（即
    <math alttext="upper A subset-of script upper B script upper F Subscript upper
    A"><mrow><mi>A</mi> <mo>⊂</mo> <msub><mi>ℬℱ</mi> <mi>A</mi></msub></mrow></math>
    ），因此交集也会更大。
- en: <math alttext="upper A intersection upper B subset-of script upper B script
    upper F Subscript upper A intersection script upper B script upper F Subscript
    upper B" display="block"><mrow><mi>A</mi> <mo>∩</mo> <mi>B</mi> <mo>⊂</mo> <msub><mi>ℬℱ</mi>
    <mi>A</mi></msub> <mo>∩</mo> <msub><mi>ℬℱ</mi> <mi>B</mi></msub></mrow></math>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A intersection upper B subset-of script upper B script
    upper F Subscript upper A intersection script upper B script upper F Subscript
    upper B" display="block"><mrow><mi>A</mi> <mo>∩</mo> <mi>B</mi> <mo>⊂</mo> <msub><mi>ℬℱ</mi>
    <mi>A</mi></msub> <mo>∩</mo> <msub><mi>ℬℱ</mi> <mi>B</mi></msub></mrow></math>
- en: Note that you can compute the exact difference in cardinality via information
    about your choice of hashing functions. Also note that the equation is an abuse
    of notation by calling <math alttext="script upper B script upper F Subscript
    upper A"><msub><mi>ℬℱ</mi> <mi>A</mi></msub></math> the set of things returned
    by the bloom filter corresponding to <math alttext="upper A"><mi>A</mi></math>
    .
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可以通过有关哈希函数选择的信息计算基数的确切差异。还请注意，方程式是滥用符号的，通过将<math alttext="script upper
    B script upper F Subscript upper A"><msub><mi>ℬℱ</mi> <mi>A</mi></msub></math>称为对应于<math
    alttext="upper A"><mi>A</mi></math>的布隆过滤器返回的事物集。
- en: 'Second, we also need to construct the union. This is similarly easy by considering
    elements that are *in* according to <math alttext="script upper B script upper
    F Subscript upper A"><msub><mi>ℬℱ</mi> <mi>A</mi></msub></math> *OR* *in* according
    to <math alttext="script upper B script upper F Subscript upper B"><msub><mi>ℬℱ</mi>
    <mi>B</mi></msub></math> . And so, similarly:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们还需要构建联合。通过考虑根据<math alttext="script upper B script upper F Subscript upper
    A"><msub><mi>ℬℱ</mi> <mi>A</mi></msub></math> *OR* *in*根据<math alttext="script
    upper B script upper F Subscript upper B"><msub><mi>ℬℱ</mi> <mi>B</mi></msub></math>。因此，类似地：
- en: <math alttext="upper A union upper B subset-of script upper B script upper F
    Subscript upper A union script upper B script upper F Subscript upper B" display="block"><mrow><mi>A</mi>
    <mo>∪</mo> <mi>B</mi> <mo>⊂</mo> <msub><mi>ℬℱ</mi> <mi>A</mi></msub> <mo>∪</mo>
    <msub><mi>ℬℱ</mi> <mi>B</mi></msub></mrow></math>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A union upper B subset-of script upper B script upper F
    Subscript upper A union script upper B script upper F Subscript upper B" display="block"><mrow><mi>A</mi>
    <mo>∪</mo> <mi>B</mi> <mo>⊂</mo> <msub><mi>ℬℱ</mi> <mi>A</mi></msub> <mo>∪</mo>
    <msub><mi>ℬℱ</mi> <mi>B</mi></msub></mrow></math>
- en: 'Now, if we consider items <math alttext="upper X"><mi>X</mi></math> and <math
    alttext="upper Y"><mi>Y</mi></math> as concatenated vectors of potentially many
    features, and hash those concatenated features, we are representing each of them
    as the bitwise vectors of our bloom. From before, we saw that the intersection
    of two blooms makes sense, and in fact is equivalent to the bitwise *AND* of their
    bloom representations. This means two items’ feature similarities can be expressed
    by the bitwise *and* similarity of their bloom hashes:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们将项目<math alttext="upper X"><mi>X</mi></math>和<math alttext="upper Y"><mi>Y</mi></math>作为可能具有许多特征的串联向量，并对这些串联特征进行哈希处理，我们将它们表示为我们的布隆的位向量。我们之前看到两个布隆的交集是有意义的，事实上等价于它们的布隆表示的位*AND*。这意味着两个项目的特征相似性可以通过它们的布隆哈希的位*and*相似性来表示：
- en: <math alttext="normal s normal i normal m left-parenthesis upper X comma upper
    Y right-parenthesis equals StartAbsoluteValue script upper B script upper F left-parenthesis
    upper X right-parenthesis intersection script upper B script upper F left-parenthesis
    upper Y right-parenthesis EndAbsoluteValue equals script upper B script upper
    F left-parenthesis upper X right-parenthesis asterisk Subscript normal b normal
    i normal t normal w normal i normal s normal e Baseline script upper B script
    upper F left-parenthesis upper X right-parenthesis" display="block"><mrow><mi>sim</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mrow><mo>|</mo> <mi>ℬℱ</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>∩</mo>
    <mi>ℬℱ</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>|</mo></mrow> <mo>=</mo>
    <mi>ℬℱ</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <msub><mo>*</mo> <mi>bitwise</mi></msub>
    <mi>ℬℱ</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal s normal i normal m left-parenthesis upper X comma upper
    Y right-parenthesis equals StartAbsoluteValue script upper B script upper F left-parenthesis
    upper X right-parenthesis intersection script upper B script upper F left-parenthesis
    upper Y right-parenthesis EndAbsoluteValue equals script upper B script upper
    F left-parenthesis upper X right-parenthesis asterisk Subscript normal b normal
    i normal t normal w normal i normal s normal e Baseline script upper B script
    upper F left-parenthesis upper X right-parenthesis" display="block"><mrow><mi>sim</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>,</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mrow><mo>|</mo> <mi>ℬℱ</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>∩</mo>
    <mi>ℬℱ</mi> <mrow><mo>(</mo> <mi>Y</mi> <mo>)</mo></mrow> <mo>|</mo></mrow> <mo>=</mo>
    <mi>ℬℱ</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <msub><mo>*</mo> <mi>bitwise</mi></msub>
    <mi>ℬℱ</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></mrow></math>
- en: For static datasets, this method has real advantages, including speed, scalability,
    and performance. Limitations are based on a variety of features and on the ability
    to change the set of possible items. Later we will discuss *locally sensitive
    hashing*, which further iterates on lookup speed with lower risks of collision
    in high-dimensional spaces, and some similar ideas will reemerge.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于静态数据集，这种方法具有真正的优势，包括速度、可伸缩性和性能。限制因素基于各种特征以及改变可能项目集的能力。稍后我们将讨论*局部敏感哈希*，它进一步迭代查找速度，并降低在高维空间中碰撞风险，一些类似的想法也将重新出现。
- en: Feature Stores
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征存储
- en: So far, we have focused on recommendation systems that we might call *pure collaborative
    filtering*. We’ve made use of the user- or item-similarity data only when attempting
    to make good recommendations. If you’ve been wondering, “Hey, what about information
    about the actual users and items?” your curiosity will now be sated.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们专注于我们可能称为*纯协同过滤*的推荐系统。我们只在试图做出好的推荐时使用了用户或项目相似性数据。如果您一直在想，“嘿，实际用户和项目的信息呢？”您现在将满足您的好奇心。
- en: 'There are a huge variety of reasons you could be interested in features in
    addition to your previous CF methods. Let’s list a few high-level concerns:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除了以前的 CF 方法外，您可能对特征感兴趣的原因有很多。让我们列举一些高级别的关注点：
- en: You may wish to show new users a specific set of items first.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能希望首先向新用户展示特定的一组项目。
- en: You may wish to consider geographic boundaries in your recommendations.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能希望在推荐中考虑地理边界。
- en: Distinguishing between children and adults may be important for the types of
    recommendations they’re given.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分儿童和成年人可能对给予的推荐类型很重要。
- en: Item features may be used to ensure high-level diversity in the recommendations
    (more to come in [Chapter 15](ch15.html#Diversity)).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目特征可用于确保推荐中的高级别多样性（更多内容请参见[第 15 章](ch15.html#Diversity)）。
- en: User features can enable various kinds of experimental testing.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户特征可以启用各种类型的实验测试。
- en: Item features could be used to group items into sets for contextual recommendations
    (more to come in [Chapter 15](ch15.html#Diversity)).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目特征可用于将项目分组为上下文推荐的集合（更多内容请参见[第 15 章](ch15.html#Diversity)）。
- en: 'In addition to these issues, another kind of feature is often essential: real-time
    features. While the point of feature stores is to provide real-time access to
    all the necessary features, it’s worthwhile to distinguish stable features that
    change infrequently from real-time features that we anticipate will change often.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些问题，另一种重要的特征通常是必不可少的：实时特征。虽然特征存储的目的是提供对所有必要特征的实时访问，但值得注意的是要区分那些变化不频繁的稳定特征和我们预计会经常变化的实时特征。
- en: Some important examples of a real-time feature store are dynamic prices, current
    item availability, *trending* status, wish-list status, and so on. These features
    may change throughout the day, and we want their values in the feature store to
    be mutable in real-time via other services and systems. Therefore, the real-time
    feature store will need to provide API access for feature mutation. This is something
    you may not want to provide for *stable* features.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一些实时特征存储的重要示例包括动态价格、当前商品可用性、*趋势*状态、愿望清单状态等。这些特征可能会在一天之中发生变化，我们希望通过其他服务和系统的实时方式对特征存储中的值进行可变更。因此，实时特征存储将需要提供API访问以进行特征的变更。这是你可能不想为*稳定*特征提供的功能。
- en: When we design our feature store, we’re likely to want the stable features to
    be built from data warehouse tables via ETLs and transformations, and we likely
    want the real-time features to be built this way as well, but on a faster schedule
    or allowing API access for mutation. In either case, the key quality of a feature
    store is *very fast read access*. It’s often a good idea to separately build feature
    stores for offline training of models that can be built in test to ensure support
    for new models.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们设计我们的特征存储时，我们可能希望稳定的特征通过数据仓库表格通过ETL和转换构建，并且我们可能也希望实时特征以相同的方式构建，但是在更快的时间表上或允许API访问进行变更。无论哪种情况，特征存储的关键质量是*非常快的读取访问*。通常建议为离线模型训练单独构建特征存储，以便在测试中构建以确保支持新模型。
- en: So how might the architecture and implementation look? See [Figure 6-3](#fig:feat-store).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 那么架构和实施可能是什么样子呢？参见[图6-3](#fig:feat-store)。
- en: '![Feature Store](assets/brpj_0603.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![特征存储](assets/brpj_0603.png)'
- en: Figure 6-3\. Demonstration of a feature store
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 特征存储的演示
- en: 'Designing a feature store involves designing pipelines that define and *transform
    the features into that store* (coordinated via things like Airflow, Luigi, Argo,
    etc.) and often look similar to the type of data pipelines used in building our
    collector. One additional complication that the feature store needs to concern
    itself with is a speed layer. During our discussion of the lambda architecture
    earlier in this chapter, we mentioned that we can think of batch data processing
    for the collector and a more rapid speed layer for intermediary updates, but this
    is even more important for the feature store. The feature store may also need
    a *streaming layer*. This layer operates on continuous streams of data and can
    perform data transformations on those; it then writes the appropriate output to
    the online feature store in real time. This adds complexity because data transformations
    on streaming data present a very different set of challenges and often require
    different algorithmic strategies. Some technologies that help here are Spark Streaming
    and Kinesis. You’ll also need to configure the system to properly handle the data
    stream, the most common of which is Kafka. Data streaming layers involve many
    components and architectural considerations that fall outside our scope; if you’re
    considering getting started with Kafka, check out [*Kafka: The Definitive Guide*](https://www.oreilly.com/library/view/kafka-the-definitive/9781492043072/)
    by Gwen Shapira et al. (O’Reilly).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 设计特征存储涉及设计管道，定义并将特征*转换到该存储中*（通过像Airflow、Luigi、Argo等协调的方式），并且通常看起来类似于构建我们收集器使用的数据管道类型。特征存储需要考虑的一个额外复杂因素是速度层。在本章早些时候讨论的Lambda架构中，我们提到可以将批量数据处理用于收集器，并为中间更新提供更快的速度层，但是对于特征存储来说，这更为重要。特征存储可能还需要一个*流处理层*。这一层操作于连续的数据流，并可以对这些数据进行数据转换；然后实时将适当的输出写入在线特征存储。这增加了复杂性，因为流数据的数据转换提出了一组非常不同的挑战，并且通常需要不同的算法策略。在这方面有帮助的一些技术包括Spark
    Streaming和Kinesis。您还需要配置系统以正确处理数据流，其中最常见的是Kafka。数据流处理层涉及许多组件和架构考虑因素，这超出了我们的范围；如果您考虑开始使用Kafka，请查看[*Kafka：权威指南*](https://www.oreilly.com/library/view/kafka-the-definitive/9781492043072/)（Gwen
    Shapira等人编著，O’Reilly）。
- en: A feature store also needs a *storage layer*; many approaches exist here, but
    using a NoSQL database is common, especially in the online feature store. The
    reason is faster retrieval and the nature of the data storage. Feature stores
    for recommendation systems tend to be very key based (i.e., *get the features
    for this user*, or *get the features for this item*), which lend themselves well
    to key-value stores. Some example technologies here are DynamoDB, Redis, and Cassandra.
    The storage layer for an offline feature store may simply be an SQL-style database
    to reduce complexity, but instead you’ll pay a tax of a delta between offline
    and online. This delta and others like it are called [*training-serving skew*](https://oreil.ly/IcE1R).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储还需要一个*存储层*；这里有许多方法，但在在线特征存储中使用NoSQL数据库是常见的。原因是检索更快和数据存储的性质。推荐系统的特征存储往往是非常基于键的（例如，*获取此用户的特征*或*获取此项的特征*），这很适合键值存储。这里的一些示例技术包括DynamoDB、Redis和Cassandra。离线特征存储的存储层可能只是SQL样式数据库，以减少复杂性，但这样做会在离线和在线之间产生差异。这种差异和其他类似的称为[*训练-服务偏差*](https://oreil.ly/IcE1R)。
- en: A unique but essential aspect of feature stores is the *registry*. A registry
    is incredibly useful for a feature store because it coordinates existing features
    and information on how they’re defined. A more sophisticated instance of a registry
    also includes input and output schemas with typing, and distributional expectations.
    These are contracts that the data pipelines must adhere to and satisfy to avoid
    populating your feature store with garbage data. Additionally, the registry’s
    definitions allow parallel data scientists and ML engineers to develop new features,
    use one another’s features, and generally understand the assumptions of features
    their models may utilize.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储的一个独特但至关重要的方面是*注册表*。注册表对于特征存储非常有用，因为它协调了现有特征及其定义方式的信息。一个更复杂的注册表实例还包括输入和输出的模式和类型，并且有分布预期。这些是数据管道必须遵守和满足的合同，以避免在特征存储中填充垃圾数据。此外，注册表的定义允许并行的数据科学家和ML工程师开发新特征，共享彼此的特征，并通常了解模型可能使用的特征假设。
- en: One important advantage of these registries is that they incentivize alignment
    between teams and developers. In particular, if you decide you care about *country*
    for your user, and you see a feature *country* in the registry, you’re more likely
    to use that (or ask the developer who’s assigned to this feature in the registry)
    than to make a new one from scratch. Practically, data scientists make hundreds
    of small decisions and assumptions when defining their models, and this removes
    some of that load that’s relying on the existing resources.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些注册表的一个重要优势是它们激励团队和开发人员之间的对齐。特别是，如果你决定关注用户的*国家*，并在注册表中看到一个名为*国家*的特征，你更有可能使用它（或者询问负责此特征的开发人员），而不是从头开始创建一个新的。实际上，数据科学家在定义模型时会做出数百个小决策和假设，这减轻了依赖现有资源的负担。
- en: Model Registries
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型注册表
- en: 'A closely related concept to feature registries is model registries. The concepts
    have a lot in common, but we caution you to think of them differently. A great
    model registry can have type contracts for the input and output of your models,
    and can serve many of the same benefits around alignment and clarity. A feature
    registry should really be focused on definitions of the business logic and features.
    Because feature engineering can also be model driven, speaking clearly about the
    differences between these two things can be challenging, so to sum it up, we’ll
    focus on what they serve: a model registry concerns itself with ML models and
    the relevant metadata, whereas a feature registry concerns itself with features
    that models will use.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 与特征注册表密切相关的概念是模型注册表。这些概念有很多共同点，但我们提醒您以不同的方式思考它们。一个优秀的模型注册表可以对模型的输入和输出有类型合同，并且可以提供与对齐和清晰度相关的许多相同的好处。特征注册表应该真正关注业务逻辑和特征的定义。因为特征工程也可以是模型驱动的，清楚地表达这两者之间的差异可能是具有挑战性的，因此总结起来，我们将专注于它们的作用：模型注册表关注于ML模型和相关的元数据，而特征注册表关注于模型将使用的特征。
- en: Finally, we need to talk about *serving* these features. Backed by the appropriately
    performant storage layer, we need to serve via API request the necessary feature
    vectors. Those feature vectors are details about the user that the model will
    need when serving recommendations—for example, the user’s location or content
    age restrictions. The API can serve back the entire set of features for the key
    or allow for more specification. Often the responses are JSON serialized for fast
    data transfer. It’s important that the features being served are the *most up-to-date
    set of features*, and latency here is expected to be < 100 ms for more serious
    industrial applications.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要讨论如何*提供*这些特征。通过合适高效的存储层支持，我们需要通过API请求提供必要的特征向量。这些特征向量包括模型在提供推荐时需要的用户详细信息，例如用户的位置或内容的年龄限制。API可以返回指定键的全部特征集，或者更具体的规定。通常，响应以JSON序列化形式进行快速数据传输。重要的是，所提供的特征应是*最新的特征集*，并且此处的延迟预计应小于100毫秒，以满足更严肃的工业应用需求。
- en: One important caveat here is that for offline training, these feature stores
    need to accommodate *time travel*. Because our goal during training is to give
    the model the appropriate data to learn in the *most generalizable way*, when
    training our model, it’s crucial to not give it access to features out of time.
    This is called *data leakage* and can cause massive divergence in performance
    between training and production. The feature store for offline training thus must
    have knowledge of the features through time, so that during training, a time index
    may be provided to get the features as they were then. These `as_of` keys can
    be tied to the historical training data as we *replay* the history of what the
    user-item interactions looked like.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在离线训练中的一个重要注意事项是，这些特征存储需要适应*时间旅行*。因为在训练期间，我们的目标是以*最具普遍性的方式*提供模型所需的数据，因此在训练模型时，关键是不让其访问超出时间范围的特征。这被称为*数据泄漏*，可能导致训练和生产中性能出现严重偏差。因此，离线训练的特征存储必须具备对历史时间段内特征的知识，以便在训练期间提供时间索引，以获取那时的特征。这些`as_of`键可以与历史训练数据关联，就像*重放*用户-项目交互历史一样。
- en: With these pieces in place—and the important monitoring this system needs—you’ll
    be able to serve offline and online features to your models. In [Part III](part03.html#ranking),
    you will see model architectures that make use of them.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些基础设施，并伴随着这个系统所需的重要监控，您将能够向模型提供离线和在线特征。在[第三部分](part03.html#ranking)，您将看到利用这些特征的模型架构。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We’ve discussed not only the crucial components necessary to hydrate your system
    and serve recommendations, but also some of the engineering building blocks needed
    to make those components a reality. Equipped with data loaders, embeddings, feature
    stores, and retrieval mechanisms, we are ready to start constructing our pipeline
    and system topology.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅讨论了为了滋养您的系统和提供推荐所必需的关键组件，还探讨了使这些组件成为现实所需的一些工程基础构件。配备了数据加载器、嵌入、特征存储和检索机制，我们已准备好开始构建我们的管道和系统拓扑。
- en: In the next chapter, we’ll focus our sights on MLOps and the rest of the engineering
    work required to build and iterate on these systems. It’s going to be important
    for us to think carefully about deployment and monitoring so our recommendation
    systems are constrained to life in IPython Notebooks.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将把目光集中在MLOps上，以及构建和迭代这些系统所需的其他工程工作。对我们来说，认真考虑部署和监控是至关重要的，以便我们的推荐系统能在IPython笔记本中运行。
- en: Continue onward to see the architectural considerations to move to production.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 继续向前看，了解到向生产环境迁移的架构考虑因素。
