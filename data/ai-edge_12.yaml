- en: 'Chapter 11\. Use Case: Wildlife Monitoring'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。使用案例：野生动物监测
- en: Now that we understand the basics of developing machine learning models for
    edge applications, the first realm of use cases we will cover is related to wildlife
    conservation and monitoring. We will explore possible problems and their associated
    solutions for each use case chapter in this book via the development workflow
    outlined in [Chapter 9](ch09.html#developing_edge_ai_applications).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了为边缘应用开发机器学习模型的基础知识，我们将首先介绍与野生动物保护和监测相关的使用案例领域。我们将通过本书中第9章中概述的开发工作流程来探讨每个使用案例章节中可能遇到的问题及其相关解决方案。
- en: There is a rapid decline of threatened species worldwide due to various human
    civilization impacts and environmental reasons or disasters. The primary drivers
    of this decline are habitat loss, degradation, and fragmentation.^([1](ch11.html#idm45988812226016))
    The causes of these drivers are human activity, such as urbanization, agriculture,
    and resource extraction. As a result of this decline, many species are at risk
    of extinction.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 全球威胁物种的迅速减少是由于各种人类文明影响以及环境原因或灾害。这种减少的主要驱动因素是栖息地的丧失、退化和碎片化^([1](ch11.html#idm45988812226016))。这些驱动因素的原因是人类活动，如城市化、农业和资源开采。由于这种减少，许多物种面临灭绝的风险。
- en: A growing number of AI and edge AI applications are being developed with the
    aim of helping to protect wildlife. These applications range from early detection
    of illegal wildlife trade to monitoring of endangered species to automated identification
    of poachers. As previously discussed in this book, edge AI is used to process
    data locally on the device instead of in the cloud. This is important for wildlife
    conservation purposes because it can be used to process data in remote locations
    without the need for an internet connection. This means that data can be processed
    quickly and without the need for expensive infrastructure, helping prevent future
    poaching and thus protecting our planet’s most vulnerable species.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的人工智能和边缘人工智能应用正在开发中，旨在帮助保护野生动物。这些应用程序涵盖从早期检测非法野生动物贸易到监测濒危物种和自动识别偷猎者的各个方面。正如本书中先前讨论的那样，边缘人工智能用于在设备上本地处理数据，而不是在云端处理。这对于野生动物保护至关重要，因为它可以用于在无需互联网连接的偏远地区处理数据。这意味着数据可以快速处理，无需昂贵的基础设施，有助于防止未来的偷猎，从而保护我们星球上最脆弱的物种。
- en: When used responsibly, edge AI can and will have an extremely positive impact
    on society and our planet. However, technology and AI are what their developers
    make of them. They can thus be used for good, or sometimes be used for harmful
    and unethical purposes. It is therefore important to be thoughtful about how they
    are developed and used to ensure that their benefits outweigh their risks. The
    United Nations^([2](ch11.html#idm45988812223184)) and various major technology
    companies like Google,^([3](ch11.html#idm45988812221760)) Microsoft,^([4](ch11.html#idm45988812220336))
    etc., are creating initiatives to utilize their resources for AI for social and
    environmental good.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当负责任地使用边缘人工智能时，它可以并且将会对社会和我们的星球产生极其积极的影响。然而，技术和人工智能的作用取决于它们的开发者。因此，它们可以用于善良之用，有时也可能被用于有害和不道德的目的。因此，如何开发和使用它们至关重要，以确保它们的利益大于风险。联合国^([2](ch11.html#idm45988812223184))和各大科技公司如谷歌^([3](ch11.html#idm45988812221760))、微软^([4](ch11.html#idm45988812220336))等正在创建倡议，利用他们的资源为社会和环境做出贡献。
- en: One such usage of AI for good is in a well-known and well-researched method
    of protecting, identifying, monitoring and tracking endangered species, the *camera
    trap*. Camera trapping is a powerful tool that can be used for a variety of wildlife
    conservation research and monitoring purposes. It can be used to monitor endangered
    species, study animal behavior, and assess the impact of human activity on wildlife.
    This method can also be used to detect and track poachers, as well as to monitor
    the health and behavior of endangered species. Camera traps are often used in
    conjunction with other methods, such as DNA analysis, to create a more complete
    picture of what is happening in an area.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一个这样的利用人工智能的例子是一种众所周知和深受研究的保护、识别、监测和追踪濒危物种的方法，*摄像机陷阱*。摄像机陷阱是一个强大的工具，可以用于各种野生动物保护研究和监测目的。它可以用于监测濒危物种、研究动物行为，并评估人类活动对野生动物的影响。这种方法还可以用于检测和跟踪偷猎者，以及监测濒危物种的健康和行为。摄像机陷阱通常与DNA分析等其他方法结合使用，以更全面地了解区域内的情况。
- en: What Exactly is a Camera Trap?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是摄像机陷阱？
- en: A camera trap is a remotely activated camera that is used to take photographs
    of animals in their natural habitat. The camera is usually triggered by an infrared
    (IR) sensor that is triggered by the animal’s movement.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 摄像机陷阱是一种远程激活的摄像机，用于拍摄动物在其自然栖息地中的照片。摄像机通常由红外（IR）传感器触发，该传感器由动物的运动触发。
- en: A camera trap is usually confined to a singular location on the ground; camera
    trapping is especially useful for large, ground-dwelling animals. So, this method
    is most appropriate for only a small portion of the Earth’s species, as camera
    traps are not useful for underwater applications, birds in flight, rapidly moving
    small insects, etc.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 摄像机陷阱通常局限于地面上的一个固定位置；摄像机陷阱特别适用于大型地栖动物。因此，这种方法对地球物种的一小部分特别有效，因为摄像机陷阱不适用于水下应用、飞行中的鸟类、迅速移动的小昆虫等。
- en: Problem Exploration
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题探索
- en: The term *wildlife conservation* is too broad of a concept to tackle in this
    one chapter and too large of a problem to be solved with just one machine learning
    model, so for the purposes of this book we will focus on wildlife conservation
    in terms of protecting specific animal species that are on the [IUCN Red List
    of Threatened Species](https://www.iucnredlist.org).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: “野生动物保护”这一术语对于本章来说涉及的概念过于宽泛，对于仅用一个机器学习模型来解决的问题也过于庞大，因此，对于本书的目的，我们将专注于保护[IUCN濒危物种红色名录](https://www.iucnredlist.org)中特定动物物种的野生动物保护。
- en: 'We also need to explore the difficulty of the problem we are trying to solve:
    what are the costs, travel, implementation, and infrastructure or government restrictions
    that will inhibit the creation of machine learning models for a nonprofit purpose?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要探索我们试图解决的问题的难度：什么是成本、旅行、实施、基础设施或政府限制，这些将阻碍为非营利目的创建机器学习模型？
- en: Solution Exploration
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案探索
- en: Since endangered species roam freely, they are difficult to spot by the human
    eye in broad daylight or at night. Camera traps are especially useful tools because
    they allow humans to track, count, and identify both the endangered animal and/or
    the animal’s threats without interference in their natural habitat. Camera traps
    ultimately allow animals to be monitored so that they can be protected remotely
    without drastically affecting their behavior, movements, environment, food sources,
    etc.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于濒危物种在自由漫游，白天或夜晚很难被人眼观察到。摄像机陷阱是特别有用的工具，因为它们允许人类在不干扰其自然栖息地的情况下跟踪、计数和识别濒危动物及/或动物的威胁。摄像机陷阱最终允许动物被监视，从而可以在不大幅度影响它们的行为、移动、环境、食物来源等情况下远程保护它们。
- en: An important step of protecting these endangered wildlife species is to provide
    their human custodians with actionable information. This can come in many different
    forms. Goal-wise, we can both generate a machine learning model that can identify
    threats to these specific species and alert humans of the threat’s location, or
    we can identify, count, and/or track the animal’s location. Both of these approaches
    accomplish the same goal, giving people the necessary information to protect a
    threatened species. However, they require a different combination of machine learning
    classes and sensor inputs to be solved.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 保护这些濒危野生动物物种的一个重要步骤是为它们的人类监护人提供可行的信息。这可以采取许多不同的形式。从目标上来看，我们既可以生成一个能够识别对这些特定物种构成威胁并警示人类威胁位置的机器学习模型，也可以识别、计数和/或跟踪动物的位置。这两种方法都达到了同样的目标，为人们提供必要的信息以保护濒危物种。然而，它们需要不同的机器学习类别组合和传感器输入来解决。
- en: Goal Setting
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标设定
- en: 'Poaching is the illegal hunting, killing, or trapping of animals. Poachers
    often target rare or endangered animals for their meat, horns, tusks, or fur.
    Poaching is a serious problem that threatens the survival of many wildlife species.
    Camera traps can be used to reduce poaching by helping to track the movements
    of poachers and by providing evidence that can be used to prosecute them. Camera
    traps can also be used to deter poachers by making them aware that they are being
    watched:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 盗猎是非法捕猎、杀戮或困禁动物。盗猎者通常以其肉、角、象牙或皮毛为目标瞄准稀有或濒危动物。盗猎是一个严重的问题，威胁着许多野生动物物种的生存。摄像机陷阱可通过帮助追踪盗猎者的活动并提供用于起诉的证据来减少盗猎。摄像机陷阱还可以通过让盗猎者意识到自己被监视来阻止盗猎：
- en: Camera trapping in remote areas can potentially help protected area managers
    to increase rates of detection of IHA (illegal human activity) in their conservation
    landscapes and increase rates of arrests and prosecutions by providing appropriate
    supporting evidence.^([5](ch11.html#idm45988812201184))
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在偏远地区使用相机陷阱有望帮助保护区管理人员提高在其保护景观中检测非法人类活动的率，并通过提供适当的支持证据提高逮捕和起诉的率。^([5](ch11.html#idm45988812201184))
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Biological Conservation* article'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*生物保护* 文章'
- en: Camera traps are also an important tool for studying, conserving, and monitoring
    endangered species. They allow researchers to collect data on the ecology and
    behavior of animals without disturbing them. This information can then be used
    to design conservation plans that protect endangered species and their habitats.
    Camera trapping also represents a unique opportunity for broadscale collaborative
    species monitoring due to its largely nondiscriminatory nature due to the amount
    of camera data that is ingested by the device with no other trigger than IR movement;
    these trigger movements could be from a wide range of species.^([6](ch11.html#idm45988812197536))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 相机陷阱也是研究、保护和监测濒危物种的重要工具。它们使研究人员能够在不干扰动物的情况下收集关于动物生态和行为的数据。这些信息可以用来设计保护濒危物种及其栖息地的保护计划。由于相机陷阱数据的广泛获取并不特定于某一物种，这些触发运动可能来自多种不同的物种。^([6](ch11.html#idm45988812197536))
- en: Solution Design
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案设计
- en: 'In order to avoid many ethical dilemmas by creating a machine learning model
    that will be used in a camera trap system to monitor endangered species, instead
    we can promote the conservation and welfare of endangered species by tracking
    as well as monitoring their environment’s other invasive species. Monitoring the
    location and abundance of invasive animals in the device’s environment with a
    camera trap and relaying this information to the environment’s human custodians
    promotes the conservation of endangered animals: the local resources and unnatural
    species intrusions or unnatural predators will be reduced, allowing the endangered
    animal’s population to recover and thrive.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免通过创建一个用于监视濒危物种的相机陷阱系统的机器学习模型而产生许多伦理困境，我们可以通过追踪及监控其环境中的其他入侵物种来促进濒危物种的保护和福利。通过在设备环境中使用相机陷阱监测入侵动物的位置和数量，并将这些信息传达给环境的人类监护者，可以促进濒危动物的保护：当地资源及非自然物种入侵或非自然捕食者将减少，从而使濒危动物种群得以恢复和茁壮成长。
- en: In this book, we are choosing to design and implement a low-cost, efficient,
    and easy-to-train camera trap to monitor an invasive animal species of your choice.
    However, a conservation and monitoring trap does not always need to be a camera-based
    solution, and by using the principles and design workflow presented in this chapter
    and throughout the book, many other types of machine learning models and applications
    can be implemented for conservation and monitoring purposes, including using audio
    data to classify animal calls or birdcalls, underwater audio/radar to listen to
    ocean sounds and track and identify whales, and more.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们选择设计和实施一种低成本、高效且易于培训的相机陷阱，以监测您选择的一种入侵动物物种。然而，保护和监测陷阱并不总是需要基于相机的解决方案，通过使用本章和整本书中提出的原则和设计工作流程，可以实施许多其他类型的机器学习模型和应用于保护和监测目的，包括使用音频数据来分类动物呼叫或鸟类叫声，水下音频/雷达来监听海洋声音并跟踪和识别鲸类，等等。
- en: What Solutions Already Exist?
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现有的解决方案有哪些？
- en: Camera traps are already being used for commercial and conservation/monitoring
    purposes and have been extensively used since the 1990s. By integrating a movement
    sensor onto a camera setup, an outdoor wildlife camera is triggered when any movement
    is detected by the integrated movement sensor, resulting in thousands of images
    from the viewpoint of the camera’s fixed location over many days or months.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自1990年代以来，相机陷阱已被广泛用于商业和保护/监测目的。通过在相机设置上集成运动传感器，当检测到任何运动时，室外野生动物相机将被触发，从相机固定位置的视角获得数千张图片，持续数天或数月。
- en: As on-device networking capabilities used to be too power intensive to integrate
    into a remote field device, researchers would need to go into the environment
    where the device is located to manually retrieve the images from the camera, a
    sometimes labor-intensive task depending on where the camera was placed in the
    wild and how remote the location was. Once the images were retrieved, it would
    take weeks or months for researchers with trained eyes to comb through the images
    by hand to find their target species in the photographs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于设备本身的网络能力曾经过于耗电，无法集成到远程场地设备中，研究人员需要进入设备所在环境手动检索相机的图像，这有时是一个劳动密集型的任务，取决于相机在野外放置的位置以及位置的偏远程度。一旦图像被检索出来，研究人员需要几周甚至几个月的时间用训练有素的眼睛手动筛查图像，以找到其中的目标物种。
- en: By integrating AI into the camera device itself, researchers are now able to
    dramatically reduce the time needed to locate their target animal/species since
    the device now has a probability reading of animals present for each and every
    image that is captured after the movement sensor is triggered. Only the highest
    probability images are sent over a network to the researcher’s lab, eliminating
    the need for a human to physically go into the field to retrieve the camera’s
    images manually (a potentially dangerous task as well, considering the environment)
    and reducing the man-hours required to sift through the captured images.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 将AI整合到相机设备本身中后，研究人员现在能够显著减少定位目标动物/物种所需的时间，因为设备现在在触发移动传感器后捕获的每张图像都有动物出现的概率读数。只有概率最高的图像才会通过网络发送到研究人员的实验室，消除了人类需要亲自进入野外检索相机图像的需求（考虑到环境可能是一个潜在的危险任务），并减少了筛选捕获图像所需的人工工时。
- en: There are specific AI tools that already exist for the purpose of camera trapping,
    from automatic specific detection in your unlabeled images or video feeds to data
    ingestion tools for the purpose of postprocessing, tracking, and counting species
    in the cloud. These tools are very valuable for researchers, and as camera trapping
    is a well-researched and widely adapted method there is an abundance of these
    solutions available; a simple [web search](https://oreil.ly/RSnfF) can help you
    find all of these prebuilt solutions. These prebuilt devices each have their own
    positives and negatives, considering that it is not yet possible to have a singular
    model that can identify and track all living animal species on our planet in all
    environments. We will not go into too much depth on these preexisting solutions
    in this chapter, and instead go through the process of designing and deploying
    a camera trap on our own for our own local habitats.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有专门用于相机陷阱的具体AI工具，从自动检测未标记图像或视频数据流中特定对象到用于后处理、跟踪和计数物种的数据摄入工具。这些工具对研究人员非常有价值，由于相机陷阱是一个经过深入研究和广泛适用的方法，因此有大量这些解决方案可用；只需进行简单的[网络搜索](https://oreil.ly/RSnfF)就可以帮助您找到所有这些预构建解决方案。这些预构建设备各有其优缺点，考虑到尚无可能有一个单一模型能够在我们星球上所有环境中识别和跟踪所有生物物种，我们在本章节不会过多深入讨论这些现有解决方案，而是会通过设计和部署我们自己的相机陷阱来介绍这个过程。
- en: Solution Design Approaches
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案设计方法
- en: 'We can take our problem statement and design a solution many different ways,
    with the following pros and cons for each approach:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采用多种不同方式来解决我们的问题陈述，并针对每种方法列出以下的优缺点：
- en: Identify an endangered animal.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 识别濒危动物。
- en: If there is a sufficiently big enough dataset available, or enough publicly
    available labeled images of the animal, the training/testing datasets will be
    easy to collate and the subsequent model will have high enough accuracy for the
    device’s environment. However, framing the problem this way will potentially allow
    poachers and other human threats to easily create a device that is essentially
    an amazingly accurate hunting tool, especially depending on the quality of the
    data used for the environment the device is in.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有足够大的数据集可用，或者有足够的公开可用的动物标记图像，那么训练/测试数据集将很容易整理，随后的模型在设备环境中将具有足够高的准确性。然而，以这种方式框定问题可能会让偷猎者和其他人类威胁轻易地创建出一种本质上是极其精确的狩猎工具，尤其是取决于用于设备所在环境的数据质量。
- en: Identify an endangered animal’s invasive predators.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 识别濒危动物的侵入性捕食者。
- en: In a well-researched environment, there is usually quite a bit of publicly available
    data for various regions around the world for invasive species, including invasive
    predators, plants, and other wildlife; this type of problem and its solution will
    generally be quite beneficial to people trying to increase the chances of success
    for endangered animals to replenish their numbers as humans can use the trap’s
    data to find and remove invasive threats.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究充分的环境中，通常有大量针对全球各地不同地区的入侵物种的公开数据，包括入侵性捕食者、植物和其他野生动物；这类问题及其解决方案通常对于试图增加濒危动物繁衍成功机会的人们非常有益，因为人类可以利用陷阱数据来发现并清除入侵威胁。
- en: However, it could be hard to determine exactly which invasive species will likely
    be in the environment of the endangered animal at any given time, and invasive
    species that are detrimental to the animal could be any number of threats, from
    humans to other animals or invasive poisonous plant life. So, the problem statement
    here could be a bit too broad to protect the endangered animal from all sides.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，确定在任何给定时间内濒危动物环境中可能出现哪种入侵物种可能是困难的，对动物有害的入侵物种可能是从人类到其他动物或入侵性有毒植物的任何威胁。因此，这里的问题陈述可能有点过于宽泛，无法从各个方面保护濒危动物。
- en: Another con of this approach is that it requires the model creator to be aware
    that an invasive species model is only useful and ethical if used in an environment
    where the species being identified is indeed actually a verified invasive species.
    This requires a good faith effort on the ML model developer’s side to ensure the
    creation of their model is indeed of an invasive species in their target area
    and requires the developer to try and limit model distribution to zones where
    the invasive species is actually not invasive.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的另一个缺点是，它要求模型创建者意识到，入侵物种模型仅在被识别的物种确实是目标区域中已验证的入侵物种的环境中才是有用和道德的。这需要机器学习模型开发者方面的诚意努力，以确保其模型的创建确实是在其目标区域中入侵物种的真实情况，并且要求开发者努力限制模型分发到实际上并非入侵性的区域。
- en: This solution will also require the end user to ensure the model is not being
    used to overhunt the identified threat, and that it complies with a region’s hunting
    rules and seasonal regulations as well as plant foraging/gathering/removal rules,
    if applicable.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此解决方案还要求最终用户确保模型不被用于过度捕猎已识别的威胁，并且符合该地区的狩猎规则和季节性法规以及适用的植物采集/收获/去除规则。
- en: Identify poachers and their associated threats.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 识别偷猎者及其相关威胁。
- en: A human/person image identification approach or even a person/object detection
    model is already a widely established area of machine learning model development.
    Many datasets already exist for the purpose of identifying humans in view of a
    camera’s lens for both low-power and high compute–power computers. However, there
    are many ethical and security obligations related to the solution of this problem.
    The model developer must ensure that the data used in the training and testing
    sets represents the usage context and is permitted to be used under copyright/fair
    use law.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 人类/人物图像识别方法或甚至是人/物体检测模型已经是机器学习模型开发中一个广泛成熟的领域。许多数据集已经存在，用于在低功率和高计算能力计算机上识别摄像头镜头中的人类。然而，与解决此问题相关的许多道德和安全义务。模型开发者必须确保在训练和测试集中使用的数据代表使用情境，并且在版权/公平使用法律下被允许使用。
- en: 'The resulting model must also only be used for binary classification: yes (there
    is a person present in the frame of the camera lens) or no (there is no person
    present in the image). This is similar to object detection of human bodies. It
    requires a good deal of good faith on the development side to ensure that facial
    data, biometric data, and other identifying information is not used or collected.
    The developer also needs to ensure that the model adheres to the many privacy
    and data laws that apply to the region where the model is to be deployed.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 结果模型还必须仅用于二元分类：是（相机镜头的框架中有人）或否（图像中没有人）。这类似于人体物体检测。这需要开发方面的良好诚意，以确保面部数据、生物特征数据和其他识别信息不被使用或收集。开发者还需确保模型遵守适用于部署模型的地区的众多隐私和数据法律。
- en: Identify other invasive species.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 识别其他入侵物种。
- en: This approach provides a lot of options in terms of determining which other
    species could be a threat to endangered species in your selected environment.
    From plants, insects, and other animals, the variations for this type of model
    are endless and all go toward the benefit of protecting and ensuring survival
    of your designated endangered species. However, there are cons similar to those
    encountered when trying to identify an endangered animal’s predators.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在确定哪些其他物种可能对您选择的环境中的濒危物种构成威胁方面提供了许多选项。从植物、昆虫到其他动物，这种模型类型的变化是无穷无尽的，并且全部有助于保护和确保您指定的濒危物种的生存。然而，与尝试识别濒危动物的捕食者时遇到的类似的缺点也是存在的。
- en: There are many pros and cons to each type of approach and their resulting solutions;
    you need to use your own exploratory methods to develop a pros/cons list for your
    chosen solution! A good first step is to brainstorm with a variety of stakeholders
    and people that have firsthand experience with your problem and solution. In addition
    to these pros/cons there are also many considerations we will need to take into
    account to ensure responsible design, which we will discuss later in this chapter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种方法类型及其产生的解决方案方面都存在许多利弊；你需要使用自己的探索方法为你选择的解决方案制定利弊列表！一个好的第一步是与各种利益相关者和具有你问题及其解决方案的第一手经验的人进行头脑风暴。除了这些利弊外，我们还需要考虑许多因素，以确保负责任的设计，这些我们将在本章后面讨论。
- en: Design Considerations
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计考虑因素
- en: To achieve the overarching goal of supporting researchers who study our selected
    wildlife species, and/or identifying and tracking invasive species that are a
    threat to endangered species in our selected areas, from a technological standpoint,
    we can use a wide variety of data sources, including different types of sensors
    and cameras ([Table 11-1](#table-sensors-to-accomplish)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现支持研究我们选择的野生物种的研究人员，和/或识别和跟踪对我们选择的地区的濒危物种构成威胁的入侵物种的总体目标，从技术角度来看，我们可以使用各种数据源，包括不同类型的传感器和相机（参见[表11-1](#table-sensors-to-accomplish)）。
- en: Table 11-1\. Sensors to accomplish various wildlife conservation goals
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表11-1\. 实现各种野生动物保护目标的传感器
- en: '| Goal | Sensor(s) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 传感器 |'
- en: '| --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Counting elephants in the wild | Camera |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 统计野生环境中的大象数量 | 相机 |'
- en: '| Identify a bird based on its call | Microphone |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 根据其呼叫声识别鸟类 | 麦克风 |'
- en: '| Listen for whale calls in the ocean | Microphone, HARPs (high-frequency acoustic
    recording packages)^([a](ch11.html#idm45988811688320)) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 在海洋中聆听鲸鱼的呼叫声 | 麦克风，HARPs（高频声学记录装置）^([a](ch11.html#idm45988811688320)) |'
- en: '| Listen for threats in an environment (poachers, gunshots, etc.) | Microphone
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 在环境中监测威胁（盗猎者，枪声等） | 麦克风 |'
- en: '| Tracking and identifying poachers | Camera, microphone |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 追踪和识别盗猎者 | 相机，麦克风 |'
- en: '| General nonnative / invasive species control & tracking | Camera, microphone,
    accelerometer, Doppler radar |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 控制和跟踪常见的外来物种 / 入侵物种 | 相机，麦克风，加速计，多普勒雷达 |'
- en: '| ^([a](ch11.html#idm45988811688320-marker)) See the NOAA Fisheries article,
    [“Passive Acoustics in the Pacific Islands”](https://oreil.ly/d-yVo). |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch11.html#idm45988811688320-marker)) 参见NOAA渔业文章，《太平洋群岛的被动声学》](https://oreil.ly/d-yVo)。
    |'
- en: In all the preceding use cases, a typical machine learning approach, *classification*,
    is used; or by uploading a machine learning–training dataset containing the information
    you would like to spot in a new, unseen sensor data input stream on the device.
    To refresh your memory on various machine learning algorithms, see [“Algorithm
    Types by Functionality”](ch04.html#algo_types_by_functionality).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有前述的用例中，典型的机器学习方法，*分类*，被用来；或通过上传一个包含您希望在设备上的新的、看不见的传感器数据输入流中发现的信息的机器学习–训练数据集来实现。要了解各种机器学习算法，请参阅[“功能类型的算法类型”](ch04.html#algo_types_by_functionality)。
- en: When choosing your wildlife monitoring goal and use case, you will also need
    to take into consideration how easy it will be to collect a large, robust, and
    high-quality dataset for training your machine learning model. As we found in
    previous chapters (especially in [Chapter 7](ch07.html#how_to_build_a_dataset)),
    your model is only as good as the quality of your input data. If you wish to create
    a model to identify the birdcall of a rare and endangered bird, for example, you
    may not be able to procure a sufficiently large enough dataset to successfully
    train a highly accurate classification model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择您的野生动物监测目标和用例时，您还需要考虑如何轻松收集用于训练机器学习模型的大型、强大且高质量的数据集。正如我们在前几章中发现的（特别是在[第7章](ch07.html#how_to_build_a_dataset)中），您的模型只能与输入数据的质量一样好。例如，如果您希望创建一个识别稀有和濒危鸟类鸣叫的模型，则可能无法获得足够大的数据集来成功训练高度准确的分类模型。
- en: Refer to [“Dataset Gathering”](#wildlife_monitoring_dataset_gathering). Thankfully,
    in the age of the internet and widely available research datasets and collaboration
    projects, model developers are able to use and acquire many preexisting databases
    of images to identify a specific animal species or download freely available research,
    including various sensor or audio datasets of the animal’s call, vocalizations,
    environment chemical footprint, etc. [“Getting Your Hands on Data”](ch07.html#getting_your_hands_on_data)
    discusses some of the pros and cons to this approach of dataset gathering.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 参考[“数据集收集”](#wildlife_monitoring_dataset_gathering)。幸运的是，在互联网时代和广泛可用的研究数据集和合作项目中，模型开发人员能够使用和获取许多现有的图像数据库，以识别特定动物物种，或者下载各种传感器或音频数据集，包括动物鸣叫、语音、环境化学足迹等。[“获取数据”](ch07.html#getting_your_hands_on_data)讨论了这种数据集收集方法的一些利弊。
- en: 'Also, consider where the device will be located and what sensors will be required
    for the desired environment:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，考虑设备的位置以及所需环境的传感器：
- en: Device location during initial data collection phase
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始数据收集阶段的设备位置
- en: Device location postdeployment
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备部署后的位置
- en: Average weather conditions of the device’s location
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备位置的平均天气条件
- en: Battery-powered versus USB-powered versus permanent powerline
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电池供电 vs USB供电 vs 永久电源线
- en: Environmental requirements (i.e., water, fog, dirt, and other environmental
    factors) that could inhibit nominal usage of the sensor or destroy the device
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境要求（例如水、雾、泥土和其他环境因素）可能会阻碍传感器的正常使用或损坏设备
- en: The device could be located in a very remote field; depending on the use case
    it may need more or less processing power, and thus more battery. The device could
    also be affixed to a permanent energy line or could be super low power and thus
    run on batteries that only need to be replaced once a year or every few years.
    A permanent powerline may not be feasible for the use case or target environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 设备可能位于非常偏远的野外；根据用例，它可能需要更多或更少的处理能力，因此需要更多的电池。设备也可以固定在永久电源线上，或者可以是超低功耗，因此可以使用每年或每几年只需更换一次的电池。对于特定的用例或目标环境，永久电源线可能不可行。
- en: Consider also communicating the model’s inferencing results back to some cloud
    platform. This communication could be energy and power limiting depending on the
    type of networking protocol chosen and will affect how long the device can be
    in the field without human intervention, battery replacement, etc. If the device
    is moving all the time, how will the model need to adapt in order to work well
    in all of these environments and situations?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 还要考虑将模型推理结果返回到某些云平台。根据选择的网络协议类型，这种通信可能会受到能源和功率限制，并且会影响设备在无需人为干预、更换电池等情况下在现场运行的时间长短。如果设备一直在移动，模型需要如何调整以在所有这些环境和情况中工作良好？
- en: Environmental Impact
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境影响
- en: Please reread [“Building Applications Responsibly”](ch02.html#building_ethical_applications),
    and then return to this section. We will discuss the specific considerations for
    the solution’s environmental impact.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请重新阅读[“负责任地构建应用程序”](ch02.html#building_ethical_applications)，然后返回本节。我们将讨论解决方案环境影响的具体考虑因素。
- en: Model developers also need to consider how their device will directly impact
    the environment it will be placed in. For example, if you were to put a large
    device in the rainforest just to track human activity, that device is likely to
    be inherently invasive regardless of the measures and attachments used on the
    physical device; however we need to consider how many animals or endangered species
    will be potentially saved from this device and resulting inferencing data, and
    then weigh the pros and cons.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模型开发人员还需要考虑他们的设备将如何直接影响其所放置的环境。例如，如果你把一个大型设备放在雨林中只是为了追踪人类活动，那么无论在物理设备上使用了什么措施和附件，该设备很可能本质上是具有侵入性的；然而，我们需要考虑多少动物或濒危物种有可能会因此设备及其推断数据而得到拯救，并权衡利弊。
- en: 'Other notes and questions to take into consideration include:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要考虑的其他注意事项和问题包括：
- en: Is the target creature itself invasive to the installation environment?
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标生物是否本身对安装环境具有入侵性？
- en: Will the device be invasive in the environment? Device anchoring could inadvertently
    negatively impact other species, bugs, bacteria, etc.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备在环境中会有侵入性吗？设备锚定可能会无意中对其他物种、虫子、细菌等产生负面影响。
- en: How many humans will be required to physically install the device in the environment?
    What travel and installation footprint will result after installation? (human
    litter, tracks, destroying other animal’s habitats, etc.)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要多少人来在环境中进行物理安装设备？安装后会产生什么样的旅行和安装足迹？（人类垃圾、足迹、破坏其他动物栖息地等）
- en: How will the device alert the user or cloud system when it has identified the
    target species?
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备在识别目标物种后如何警示用户或云系统？
- en: Where is the device placed, and how often will humans need to physically traverse
    to the device?
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备放置在哪里，人类需要多频繁地物理穿越到设备位置？
- en: We also need to ensure that the device is not emitting lights, sounds, noises,
    and chemicals that are not natural or native to the environment that it is placed
    in. These factors could cause the animal that you are trying to track to behave
    abnormally, thus skewing your data and inferencing results.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要确保设备不会发出不自然或非本地环境的光线、声音、噪音和化学物质。这些因素可能会导致您试图追踪的动物行为异常，从而扭曲您的数据和推理结果。
- en: Warning! Camera Traps Can be Heard and Seen by Animals^([7](ch11.html#idm45988811651456))
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 警告！相机陷阱可能会被动物听到和看到^([7](ch11.html#idm45988811651456))
- en: 'The developer of the camera trap needs to consider the following ways the device
    can be intrusive in the animal’s environment:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 相机陷阱的开发者需要考虑设备在动物环境中可能具有的以下侵入方式：
- en: Auditory intrusions
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 听觉侵入
- en: Olfactory intrusions
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嗅觉侵入
- en: Learned association
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习关联
- en: Visual (day)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉（白天）
- en: Visual (night)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉（夜间）
- en: In another vein of environmental impact, the camera trap may also cause an ethical
    dilemma if used for detecting poaching activity, and could cause direct, negative
    impacts to the local people of the lands we are trying to protect. There are reports
    that antipoaching initiatives have been used by governments to exclude local minorities
    from areas they’ve traditionally lived and gathered food in.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从环境影响的另一个角度看，如果用于侦测偷猎活动，相机陷阱可能引起伦理困境，并可能对我们试图保护的土地上的当地人造成直接负面影响。有报道称，反偷猎行动已被政府用来排除传统上生活和采集食物的当地少数民族。
- en: Any AI that’s designed to highlight people for punishment has a high risk of
    abuse, because it can be used in ways the model developers didn’t intend, for
    example, a tribe being evicted from their village, and the government placing
    “antipoacher” cameras to make sure they don’t come back, or an authoritarian regime
    using them against rebels. This capability being provided by Western organizations
    also echoes a lot of harmful technology transfers that have happened over the
    years.^([8](ch11.html#idm45988811641664))
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 任何设计用于惩罚人员的人工智能都存在高滥用风险，因为它可能被用于模型开发者未曾预料的方式，例如，部落被驱逐出其村庄，政府安置“反偷猎者”摄像头以确保他们不会返回，或者威权政权将其用于对付叛乱分子。西方组织提供这种能力也会引起多年来发生过的有害技术转移的回响。^([8](ch11.html#idm45988811641664))
- en: Bootstrapping
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引导启动
- en: 'For this chapter, we will implement a solution that is geared toward “identifying
    an endangered animal’s invasive predators” (see [“Solution Design Approaches”](#wildlife_solution_design_pros_cons))
    and design a model that will detect and classify the animal *Callosciurus finlaysonii*
    [(Finlayson’s squirrel)](https://oreil.ly/JRz_2), also known colloquially as the
    “Thai squirrel,” which is a certified invasive species in the Netherlands according
    to the [European Union list as of August 2, 2022](https://oreil.ly/fSbmw). The
    author of this chapter is a resident of the Netherlands; thus, a certified Netherlands
    invasive species has been chosen as an example for this use case. Once we have
    collected our dataset with our target trap animal, we will also add another class
    of data for general environmental images that do not include *Callosciurus finlaysonii*.
    These two classes will allow our image classification machine learning model to
    identify when the camera is triggered by movement in the environment: the camera
    takes an image and the trained machine learning model inferences and determines
    where the *Callosciurus finlaysonii* is present in the environment. The resulting
    image, if it includes our invasive species, will be sent over our selected network
    connection for further processing by a human or in the cloud.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将实施一个解决方案，旨在“识别濒危动物的入侵捕食者”（参见[“解决方案设计方法”](#wildlife_solution_design_pros_cons)），并设计一个模型来检测和分类动物*Callosciurus
    finlaysonii*（[芬利松松鼠](https://oreil.ly/JRz_2)），俗称“泰国松鼠”，根据[2022年8月2日欧盟列表](https://oreil.ly/fSbmw)在荷兰被认定为入侵物种。本章的作者是荷兰的居民，因此选择了荷兰认证的入侵物种作为此用例的示例。一旦我们收集到了目标捕捉动物的数据集，我们还将添加另一类不包括*Callosciurus
    finlaysonii*的一般环境图像数据。这两类数据将使我们的图像分类机器学习模型能够识别摄像头在环境中因移动而触发的时刻：摄像头拍摄一张图像，训练好的机器学习模型推断并确定*Callosciurus
    finlaysonii*在环境中的位置。如果结果图像包含我们的入侵物种，将通过我们选择的网络连接发送给人类或云端进一步处理。
- en: 'According to the [Dutch government and the European Union](https://oreil.ly/v1XZh):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[荷兰政府和欧盟](https://oreil.ly/v1XZh)：
- en: In Italy, the Thai squirrel (*Callosciurus finlaysonii*) strips the bark of
    trees, increasing the chance of infestation by fungi and invertebrates. In its
    native range, the Thai squirrel is considered a frequent predator of bird eggs,
    but there is no information known about such an effect in areas where this squirrel
    has been introduced. Stripping bark from trees is mentioned as a negative effect
    on ecosystem services. This can be significant for both individual trees and entire
    production forests. Stripping the bark can also lead to secondary contamination
    with, for example, fungi. A result of this is felled trees in Italy.^([9](ch11.html#idm45988811626576))
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在意大利，泰国松鼠（*Callosciurus finlaysonii*）啃食树皮，增加了真菌和无脊椎动物感染的几率。在其原产地范围内，泰国松鼠被认为是鸟类蛋的常见捕食者，但在引入区域没有关于这种影响的信息。从树皮上啃食对生态系统服务有负面影响，这对个别树木和整个生产林业都可能具有重要意义。树皮的剥离还可能导致次生污染，例如真菌。这导致在意大利有被砍伐的树木[^9]。
- en: Define Your Machine Learning Classes
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义您的机器学习类别
- en: '[Table 11-2](#table_machine_learning_classes_for_use_case) shows potential
    combinations of use cases, sensor and data input types, and machine learning classes
    one would use to collect and label their training and testing datasets. The use
    cases and their associated class labels are important for the type of machine
    learning algorithm we’re employing in this chapter, specifically “classification.”
    You can learn more about this in [“Classification”](ch04.html#algo_classification).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 11-2](#table_machine_learning_classes_for_use_case) 展示了用例、传感器和数据输入类型以及机器学习分类的潜在组合，用于收集和标记训练和测试数据集。用例及其相关的类别标签对于我们在本章中使用的机器学习算法类型（特别是“分类”）非常重要。更多信息，请参见[“分类”](ch04.html#algo_classification)。'
- en: Table 11-2\. Machine learning classes for various use cases
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Table 11-2\. 各种用例的机器学习分类
- en: '| Use case | Training data | Class labels |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 用例 | 训练数据 | 类别标签 |'
- en: '| --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Camera trap | Images | Target animal, background environment (with or without
    other animals) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 摄像机陷阱 | 图像 | 目标动物、背景环境（有或无其他动物） |'
- en: '| Audio trap | Microphone data | Target animal call, ambient environment noise,
    “other” animal calls that are NOT the target animal’s call |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 音频陷阱 | 麦克风数据 | 目标动物呼叫、环境背景噪音、“其他”动物呼叫（非目标动物呼叫） |'
- en: '| Animal object detection | Images (with bounding boxes) | Target animal |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 动物目标检测 | 图像（带有边界框） | 目标动物 |'
- en: '| Motion trap | Accelerometer, radar, or other spatial signals | Movement of
    desired animal |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 运动陷阱 | 加速度计、雷达或其他空间信号 | 所需动物的运动 |'
- en: '| Chemical trap | Gas signals | Ambient environment, target species chemical
    signature |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 化学诱捕 | 气体信号 | 环境环境，目标物种化学特征 |'
- en: In this chapter, we will select and build upon the traditional camera trap use
    case for machine learning image classification using transfer learning techniques,
    answering the question, “Is my target animal present in the field of view of the
    camera?” Our project’s machine learning classes will be “target animal” and “background
    environment (with or without other animals),” or more simply, “unknown.”
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将选择并建立传统的摄像机陷阱用例，用于机器学习图像分类，使用迁移学习技术回答问题，“我的目标动物是否出现在摄像机的视野中？”我们项目的机器学习类别将是“目标动物”和“背景环境（有或没有其他动物）”，或者更简单地说，“未知”。
- en: Dataset Gathering
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集收集
- en: 'For technical and specific information about how to gather a clean, robust,
    and useful dataset, see [“Getting Your Hands on Data”](ch07.html#getting_your_hands_on_data).
    You can also utilize various strategies on how to collect data from multiple sources
    to create your own unique dataset for your use case:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取干净、稳健和有用数据集的技术和具体信息，请参阅[“获取数据”的方法](ch07.html#getting_your_hands_on_data)。您还可以利用各种策略从多个来源收集数据，以为您的用例创建独特的数据集：
- en: Combining public research datasets
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合公共研究数据集
- en: Combining no-animal-present environment images from multiple public datasets
    with a dataset of labeled images of the target trap animal
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将来自多个公共数据集的无动物出现环境图像与标记的目标陷阱动物图像数据集结合起来
- en: Using existing massive image datasets like COCO (common objects in context)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用现有的大规模图像数据集，如COCO（常见上下文中的常见对象）
- en: Sourcing Publicly Available Image Datasets
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取公开可用的图像数据集
- en: You can always use a dataset from a seemingly nonrelevant source; for example,
    if your target invasive species lives in Portugal but there is not an abundance
    of labeled image datasets for your target species in that environment, you can
    find research datasets for other Portugal species and use their data as “nontarget
    invasive species” in your training/testing dataset. Your target invasive species
    could possibly even be present in those images, which your model can identify
    after training, without the original dataset developer’s knowledge!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您始终可以使用看似不相关来源的数据集；例如，如果您的目标入侵物种生活在葡萄牙，但在那个环境中缺乏标记的目标物种图像数据集，您可以找到其他葡萄牙物种的研究数据集，并将其数据用作您的训练/测试数据集中的“非目标入侵物种”。您的目标入侵物种甚至可能存在于这些图像中，您的模型在训练后可以识别，而不会被原始数据集开发者知晓！
- en: Edge Impulse
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Edge Impulse
- en: The Edge Impulse Studio is a freely available cloud-based platform containing
    all the tools and code required for the entire end-to-end machine learning pipeline,
    including collecting and labeling high-quality training/testing datasets, extracting
    your data’s most important features using various digital signal processing techniques,
    designing and training your machine learning model, testing and validating your
    model for real-world performance/accuracy, and deploying your model in various
    library formats with the easy-to-use Edge Impulse SDK. For this chapter, and the
    subsequent use case chapters in this book, we will use the Edge Impulse Studio
    to reduce our model development time and the amount of code we will need to write
    in order to achieve a full edge machine learning model development pipeline process
    and subsequent deployment.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Edge Impulse Studio是一个免费提供的基于云的平台，包含所有工具和代码，用于整个端到端机器学习流程，包括收集和标记高质量的训练/测试数据集，使用各种数字信号处理技术提取数据的最重要特征，设计和训练您的机器学习模型，测试和验证模型的真实世界性能/准确性，并使用易于使用的Edge
    Impulse SDK在各种库格式中部署模型。在本章及本书后续用例章节中，我们将使用Edge Impulse Studio来减少我们的模型开发时间以及我们需要编写的代码量，以实现完整的边缘机器学习模型开发流程和随后的部署。
- en: For further justification for using Edge Impulse for edge machine learning model
    development, review [“End-to-End Platforms for Edge AI”](ch05.html#end_to_end_platforms).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步证明为何使用Edge Impulse进行边缘机器学习模型开发，请查看[“边缘AI的端到端平台”](ch05.html#end_to_end_platforms)。
- en: To follow along with the rest of the instructions in this chapter, you will
    need to create a [free Edge Impulse account](https://edgeimpulse.com).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随本章节的其余说明，您需要创建一个[免费的 Edge Impulse 账户](https://edgeimpulse.com)。
- en: Public project
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 公共项目
- en: Each use case chapter of this book contains a written tutorial to demonstrate
    and achieve a complete end-to-end machine learning model for the described use
    case. However, if you would like to just get straight to the point and see the
    exact data and model that the authors have developed for the chapter in its final
    state, you may do so by navigating to the public [Edge Impulse project](https://oreil.ly/DP1gJ)
    for this chapter.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本书每个用例章节都包含一个书面教程，以演示和实现所描述用例的完整端到端机器学习模型。但如果您只想直奔主题，查看作者为本章节开发的确切数据和模型的最终状态，可以访问本章节的公共[Edge
    Impulse 项目](https://oreil.ly/DP1gJ)。
- en: You may also directly clone this project, including all of the original training
    and testing data, intermediate model information, resulting trained model results,
    and all deployment options by selecting the Clone button at the top right side
    of the Edge Impulse page (see [Figure 11-1](#figure_clone_edge_impulse_public_project)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过选择 Edge Impulse 页面右上角的克隆按钮直接克隆此项目，包括所有原始训练和测试数据、中间模型信息、训练后模型结果以及所有部署选项（参见[图 11-1](#figure_clone_edge_impulse_public_project)）。
- en: '![Edge Impulse clone project](assets/aiae_1101.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![Edge Impulse 克隆项目](assets/aiae_1101.png)'
- en: Figure 11-1\. Clone Edge Impulse public project
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-1\. 克隆 Edge Impulse 公共项目
- en: Choose Your Hardware and Sensors
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择您的硬件和传感器
- en: In this book, we try to remain as device agnostic as possible, but we also need
    to discuss how one can use an off-the-shelf, easy-to-use development kit to create
    this use case’s solution, since we are under the assumption that the tutorial
    outlined in this chapter will most likely be used for an ethical, nonprofit purpose,
    meaning that the reader’s potential access to embedded engineering funds, resources,
    developers, etc., will be limited. Thus, this book aims to make this hardware
    selection as easy, affordable, and accessible as possible.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们尽可能地保持设备的中立性，但我们还需要讨论如何使用现成、易于使用的开发套件来创建此用例的解决方案，因为我们假设本章节中概述的教程很可能用于道德、非营利目的，这意味着读者可能会受到嵌入式工程资金、资源、开发人员等的限制。因此，本书旨在尽可能地使硬件选择变得简单、经济和易于获取。
- en: For a quick and easy data ingestion and deployment option, without having to
    write any code, we will both ingest new data and deploy the resulting trained
    model to a mobile phone with the Edge Impulse WebAssembly library and mobile client.
    For other equally easy deployment devices, Edge Impulse provides a large array
    of [officially supported platforms](https://oreil.ly/stMSR), from MCUs to GPUs,
    of which all include an open source prewritten firmware available for you to use.
    If you have a device that is not listed as an officially supported platform by
    Edge Impulse, you can still use the device, but you will need to integrate the
    deployed C++ library and your device’s driver code into your application code,
    just as you would normally during a typical embedded firmware development workflow.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速简便地进行数据摄入和部署，而无需编写任何代码，我们将同时使用 Edge Impulse WebAssembly 库和移动客户端将新数据摄入并将训练后的模型部署到移动电话。对于其他同样简便的部署设备，Edge
    Impulse 提供了大量[官方支持的平台](https://oreil.ly/stMSR)，从 MCU 到 GPU，其中所有平台都包括可供您使用的开源预写固件。如果您有一台未列为
    Edge Impulse 官方支持平台的设备，仍然可以使用该设备，但您需要将部署的 C++ 库和您设备的驱动代码集成到您的应用程序代码中，就像在典型的嵌入式固件开发工作流中一样。
- en: Platform selection is not as important to this book because we are trying to
    make sure that every use case chapter can be realistically solved with almost
    any physical device platform (barring any memory or latency constraints). You
    could solve all of the use case chapters with a Raspberry Pi and various sensor
    configurations and still achieve the same goal discussed here.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 平台选择对本书来说并不重要，因为我们试图确保几乎任何物理设备平台都能实际解决每个用例章节（除了任何内存或延迟限制）。您可以使用树莓派和各种传感器配置解决所有用例章节，并实现本文所讨论的相同目标。
- en: However, depending on the use case goal, choosing a Raspberry Pi will force
    you to consider the costly power requirements necessary for the Pi to function,
    but in contrast, this device selection will potentially be lower in cost and have
    dramatically reduced total software development time (for a single field unit;
    of course, if a large number of the same device is required, then a Raspberry
    Pi + sensor/camera configuration will likely be more expensive than an MCU/integrated
    sensor/camera solution, for example).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，根据使用案例的目标，选择树莓派将迫使您考虑必要的高耗电需求以使树莓派正常运行，但与此相反，这种设备选择可能成本更低且大大减少总软件开发时间（对于单个现场单位而言；当然，如果需要大量相同设备，则树莓派+传感器/摄像头配置可能比MCU/集成传感器/摄像头解决方案更昂贵，例如）。
- en: Hardware configuration
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件配置
- en: There are endless combinations to choose from for your main edge device and
    add-on camera attachment. For this chapter, we will remain device agnostic, but
    assume that our target device is similar to that of an [OpenMV Cam H7 Plus](https://oreil.ly/hZddx)
    (with RGB-integrated camera).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您的主要边缘设备和附加摄像头配件，有无限组合可供选择。在本章中，我们将保持设备不可知性，但假设我们的目标设备类似于[OpenMV Cam H7 Plus](https://oreil.ly/hZddx)（带RGB集成摄像头）。
- en: 'This generic setup already implies a few limitations: our camera trap will
    only reliably work in daylight; the quality of the input frame image may be too
    low to accurately spot all instances of our target animal if the animal is too
    far away from the lens; your device may be too battery intensive to live unattended
    in the field for long periods of time; and if you are trying to trap an animal
    with a specific coloring using grayscale, the input image may yield inaccurate
    predictions.^([10](ch11.html#idm45988811564112))'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这种通用设置已经暗示了一些限制：我们的摄像陷阱只能在白天可靠工作；输入帧图像的质量可能太低，无法准确识别目标动物的所有实例，如果动物距离镜头太远；您的设备可能会因电池消耗过高而无法长时间无人看管地在野外生存；如果您试图使用灰度捕捉具有特定颜色的动物，则输入图像可能会产生不准确的预测。^([10](ch11.html#idm45988811564112))
- en: 'Following is a list of some other camera attachment options and requirements
    to ponder in order to improve the accuracy of your wildlife monitoring model for
    your specific environment, use case, project budget, and more:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些其他摄像头附件选项和要考虑的要求清单，以提高特定环境、使用案例、项目预算等方面的野生动物监测模型的准确性：
- en: High-quality cameras
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高质量摄像头
- en: Low-quality cameras
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低质量摄像头
- en: Infrared, thermal cameras
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 红外线、热成像摄像头
- en: Grayscale versus color (RGB) input
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灰度与彩色（RGB）输入
- en: Lens focal length
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 镜头焦距
- en: Input image pixel density
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入图像像素密度
- en: Data Collection
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'Using Edge Impulse, there are many options available to upload and label data
    in your project:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Edge Impulse，您可以在项目中上传和标记数据的多种选项：
- en: '[Edge Impulse Studio uploader](https://oreil.ly/b3url)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[Edge Impulse Studio 上传工具](https://oreil.ly/b3url)'
- en: The web uploader allows you to directly upload files from your computer to your
    Edge Impulse project in a variety of file formats. You can also have the studio
    automatically label your samples based on the filename.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 网页上传工具允许您直接从计算机上传文件到您的Edge Impulse项目中，支持多种文件格式。您还可以根据文件名自动为您的样本标记。
- en: '[CLI uploader](https://oreil.ly/cxdp4)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLI上传工具](https://oreil.ly/cxdp4)'
- en: The CLI uploader allows you to directly upload files locally from your computer’s
    command-line terminal to your Edge Impulse project with a variety of file formats
    and input options. You can also have the studio automatically label your samples
    based on the filename.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: CLI上传工具允许您从计算机的命令行终端直接上传文件到Edge Impulse项目，支持多种文件格式和输入选项。您还可以根据文件名自动为您的样本标记。
- en: '[Ingestion API](https://oreil.ly/myL7K)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[摄入 API](https://oreil.ly/myL7K)'
- en: Write a data collection script connecting your platform over a networking protocol
    to your Edge Impulse project by simply calling the ingestion API. Using the scripting
    language of your choice, you can set timers and triggers to automatically upload
    images to your project using your [Edge Impulse project API key](https://oreil.ly/623ly).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 编写数据收集脚本，通过简单调用摄入API，连接您的平台到Edge Impulse项目。使用您选择的脚本语言，您可以设置定时器和触发器，以便自动上传图像到您的项目，使用您的[Edge
    Impulse项目API密钥](https://oreil.ly/623ly)。
- en: '[Data sources (cloud bucket integration)](https://oreil.ly/1QweQ)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据源（云存储桶集成）](https://oreil.ly/1QweQ)'
- en: Directly pull data from your cloud data bucket and automatically trigger responses
    in your Edge Impulse project (this feature is especially useful for improving
    your model over time with active learning strategies).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 直接从您的云数据存储桶中提取数据，并在您的Edge Impulse项目中自动触发响应（此功能特别适用于使用主动学习策略随时间改进模型）。
- en: Further details regarding the Edge Impulse data acquisition format can be viewed
    in the [Edge Impulse API reference documentation](https://oreil.ly/Z5IzD).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Edge Impulse数据采集格式的更多细节可以在[Edge Impulse API参考文档](https://oreil.ly/Z5IzD)中查看。
- en: Connecting your device directly to Edge Impulse for data collection
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将您的设备直接连接到Edge Impulse进行数据收集
- en: There are many ways to upload data directly from your desired platform to your
    Edge Impulse project.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以直接从您选择的平台上传数据到您的Edge Impulse项目。
- en: If your chosen device platform is *officially supported*, you can follow the
    firmware update guide found for your target within the [Edge Impulse development
    boards documentation](https://oreil.ly/ULIdQ).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择的设备平台*得到官方支持*，您可以根据[Edge Impulse开发板文档](https://oreil.ly/ULIdQ)中找到的您目标设备的固件更新指南进行操作。
- en: If your chosen device platform is *not officially supported*, follow the [development
    platform porting guide](https://oreil.ly/iOo23) to fully integrate the Edge Impulse
    [ingestion API](https://oreil.ly/FsCTx) into your embedded device firmware (note
    that porting is usually time consuming and not necessary for most projects, unless
    you want your target to be featured on the [Edge Impulse community boards page](https://oreil.ly/xxTwr)),
    or use the [Edge Impulse CLI serial data forwarder](https://oreil.ly/c9qb0) to
    quickly and easily ingest data over the serial port or with WebUSB into your Edge
    Impulse project.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择的设备平台*未经官方支持*，请参考[开发平台移植指南](https://oreil.ly/iOo23)，将Edge Impulse的[摄取API](https://oreil.ly/FsCTx)完全集成到您的嵌入式设备固件中（请注意，移植通常耗时且大多数项目不必要，除非您希望您的目标出现在[Edge
    Impulse社区论坛页面](https://oreil.ly/xxTwr)上），或者使用[Edge Impulse CLI串行数据转发器](https://oreil.ly/c9qb0)，快速轻松地通过串行端口或WebUSB将数据注入到您的Edge
    Impulse项目中。
- en: You can also use a mobile phone or your computer to directly upload new images
    from the camera on your device; check out all of the device connection options
    via your project’s Devices tab (see [Figure 11-2](#figure_devices_tab_collect_data_view)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用手机或计算机直接从设备上的摄像头上传新图像；通过项目的设备标签查看所有设备连接选项（参见[图 11-2](#figure_devices_tab_collect_data_view)）。
- en: '![Devices tab - Collect data view](assets/aiae_1102.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![设备标签 - 数据收集视图](assets/aiae_1102.png)'
- en: Figure 11-2\. “Collect data” view in the Devices tab
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2. 设备标签中的“收集数据”视图
- en: iNaturalist
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: iNaturalist
- en: Since most people likely do not have a large dataset of invasive species images
    available at their disposal, a secondary form of data collection is required in
    order to start our dataset of images of our invasive animal. For this tutorial,
    rather than setting up a device in the field to collect new, unlabeled raw images
    of animals, we will instead use images acquired from the community in our desired
    location that have already been (somewhat) reliably labeled with the name of our
    target species. Using [iNaturalist](https://www.inaturalist.org), we will query
    their database for images with our identified species, query on this species name,
    and download a dataset of images with the iNatural image ID and photographer’s
    username attributed to each file download.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数人可能没有大量入侵物种图像数据集可供使用，因此我们需要通过第二种数据收集形式来开始我们的入侵动物图像数据集。在本教程中，我们将不会在野外设置设备来收集动物的新的未标记的原始图像，而是将使用社区中已经标记了我们目标物种名称的图像。使用[iNaturalist](https://www.inaturalist.org)，我们将查询他们的数据库以获取包含我们识别的物种图像的数据集，查询此物种名称，并下载具有iNatural图像ID和每个文件下载的摄影师用户名的数据集。
- en: You will need an iNaturalist account to log into the [iNaturalist export website](https://oreil.ly/u4m7i)
    and process the following queries.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您将需要一个iNaturalist帐户登录[iNaturalist导出网站](https://oreil.ly/u4m7i)，并处理以下查询。
- en: 'First we will query on our desired trap animal species name and retrieve a
    CSV file with the following columns in iNaturalist: id, user_login, quality_grade,
    license, url, image_url (see [Example 11-1](#query_callosciurus) and [Figure 11-3](#select_columns_for_csv_file)).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查询我们期望的捕捉动物物种名称，并在iNaturalist中检索一个包含以下列的CSV文件：id、user_login、quality_grade、license、url、image_url（见[示例 11-1](#query_callosciurus)和[图 11-3](#select_columns_for_csv_file)）。
- en: Example 11-1\. Query for Callosciurus finlaysonii
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例11-1. 查询Callosciurus finlaysonii
- en: '[PRE0]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Select the columns for the CSV file](assets/aiae_1103.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![选择用于 CSV 文件的列](assets/aiae_1103.png)'
- en: Figure 11-3\. Select the columns for the CSV file
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-3\. 选择用于 CSV 文件的列
- en: 'We will also need a dataset of images that include “unknown” species or images
    of the environment in the Netherlands that do not include the *Callosciurus finlaysonii*
    (or any animals at all). This “unknown” data will allow us to train our model
    to more accurately predict when our trap animal has been photographed/captured
    by our device. Query for this data with the following columns in iNaturalist:
    id, user_login, quality_grade, license, url, image_url (see [Example 11-2](#query_unknown)
    and [Figure 11-4](#select_columns_for_the_csv_file2)).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个图像数据集，其中包括“未知”物种的图像或荷兰环境中不包括*Callosciurus finlaysonii*（或任何动物）的图像。这些“未知”数据将帮助我们训练模型，更准确地预测我们的设备是否拍摄/捕捉到了我们设置的目标动物。使用以下列在
    iNaturalist 中查询此类数据：id、user_login、quality_grade、license、url、image_url（参见[示例 11-2](#query_unknown)和[图 11-4](#select_columns_for_the_csv_file2)）。
- en: Example 11-2\. Query for unlabeled images in place ID 7506 (Netherlands)
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-2\. 查询荷兰地区ID 7506（荷兰）的未标记图像
- en: '[PRE1]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Select the columns for the CSV file](assets/aiae_1104.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![选择用于 CSV 文件的列](assets/aiae_1104.png)'
- en: Figure 11-4\. Select the columns for the CSV file
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-4\. 选择用于 CSV 文件的列
- en: Download the CSV files generated from iNaturalist from the preceding queries
    and save the files to your computer.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 下载从上述查询中生成的 CSV 文件，并将文件保存到您的计算机上。
- en: Now, using the CSV files we generated, use the Python code in [Example 11-3](#inaturalist_code)
    to download and save the iNaturalist query images to your computer, while also
    attributing the files we download with the username of the original iNaturalist
    uploader. Run the script twice, once for your trap animal images, and again for
    your “unknown” images. Save these files in two different directories, for example
    */unknown/* and */animal/* (see [Example 11-3](#inaturalist_code)).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用我们生成的 CSV 文件，使用 [示例 11-3](#inaturalist_code) 中的 Python 代码下载和保存 iNaturalist
    查询的图像到您的计算机，同时也将下载的文件归因于原始 iNaturalist 上传者的用户名。运行脚本两次，一次用于您的陷阱动物图像，另一次用于“未知”图像。将这些文件保存在两个不同的目录中，例如
    */unknown/* 和 */animal/*（参见 [示例 11-3](#inaturalist_code)）。
- en: 'You may also need to install the `requests` package via `pip` if you don’t
    already have it: `python -m pip install requests`.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有安装 `requests` 包，您可能需要通过 `pip` 安装它：`python -m pip install requests`。
- en: Example 11-3\. Python code to download images from iNaturalist
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-3\. 从 iNaturalist 下载图像的 Python 代码
- en: '[PRE2]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you’d like to drop query params in image URLs (as explained above), replace
    `Path(url).suffix` with `Path(url.split("?")[0]).suffix`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要删除图像 URL 中的查询参数（如上文所述），请将 `Path(url).suffix` 替换为 `Path(url.split("?")[0]).suffix`。
- en: 'This script may take a while to run depending on how big your CSV file is and
    how many entries resulted from your iNaturalist query. For this use case, I recommend
    keeping your iNaturalist query to under 4,000 results. You can reduce the output
    of your iNaturalist query by changing the query settings to include only research
    quality–grade images, images from a specified place ID, etc. You can find a specific
    place ID by going to [the iNaturalist website](https://oreil.ly/SGCIr) and typing
    in a location in the Place textbox of the Identify search bar, then the place
    ID value will populate in the URL after pressing Go. For example, New York City
    has place ID 674: *[*https://www.inaturalist.org/observations/identify?place_id=674*](https://www.inaturalist.org/observations/identify?place_id=674)*.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的 iNaturalist 查询结果的大小和条目数量，此脚本的运行时间可能会有所不同。对于此用例，建议将 iNaturalist 查询结果限制在
    4,000 条以下。您可以通过更改查询设置，仅包括研究质量的图像、来自特定地点 ID 的图像等方式，减少 iNaturalist 查询的输出。您可以通过访问
    [iNaturalist 网站](https://oreil.ly/SGCIr)，在 Identify 搜索栏的 Place 文本框中输入一个位置，然后在按下
    Go 后，Place ID 值将在 URL 中显示。例如，纽约市的地点 ID 是 674：[*https://www.inaturalist.org/observations/identify?place_id=674*](https://www.inaturalist.org/observations/identify?place_id=674)。
- en: Dataset Limitations
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集限制
- en: Even with a robust dataset acquired from iNaturalist, there are still many limitations
    that arise. When a camera records multiple detections of an unmarked animal, one
    cannot determine whether the images represent multiple mobile individuals or a
    single individual repeatedly entering the camera viewshed.^([11](ch11.html#idm45988802278176))
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 即使从 iNaturalist 获取了一个强大的数据集，仍然会出现许多限制。当摄像机记录到未标记动物的多次检测时，无法确定这些图像是代表多个移动个体还是一个个体重复进入摄像头视野区域。^([11](ch11.html#idm45988802278176))
- en: iNaturalist also tends to prefer images of animals that are close-up/large within
    the image frame. This bias of animal images could reduce the accuracy of the resulting
    machine learning model in the real world as the close-up images tend to not include
    a large background of the surrounding environment, resulting in a model that expects
    every animal to be close to the camera lens.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: iNaturalist也倾向于选择在图像框架内部接近/大的动物图像。这种动物图像的偏差可能会降低实际世界中生成的机器学习模型的准确性，因为近距离的图像倾向于不包括周围环境的大背景，导致模型期望每个动物都靠近摄像头镜头。
- en: In order to counteract this bias, an “active learning” approach may be necessary
    in order to improve the model over time—i.e., initially deploy a subpar model
    to camera trap new images of the target creature, store these new images directly
    on the device or upload them to a cloud bucket, then confirm the animal is located
    in these images, label and upload them to our project’s original training dataset,
    and finally retrain the model and redeploy to the device.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了抵消这种偏差，可能需要采取“主动学习”方法来随着时间的推移改进模型——即，最初部署次优模型以捕捉目标生物的新图像，直接将这些新图像存储在设备上或上传到云存储桶，然后确认图像中的动物位置，标记并将其上传到我们项目的原始训练数据集，最后重新训练模型并重新部署到设备上。
- en: Dataset Licensing and Legal Obligations
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集许可和法律义务
- en: 'Upon Edge Impulse account creation, every Edge Impulse user must abide by the
    following terms of use, licenses, and policies:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建Edge Impulse帐户时，每位Edge Impulse用户必须遵守以下使用条款、许可证和政策：
- en: '[Edge Impulse Privacy Policy](https://oreil.ly/Ud6ja)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Edge Impulse隐私政策](https://oreil.ly/Ud6ja)'
- en: '[Edge Impulse Terms of Service](https://oreil.ly/0y-PK)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Edge Impulse服务条款](https://oreil.ly/0y-PK)'
- en: '[Edge Impulse Responsible AI License](https://oreil.ly/rmeaN)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Edge Impulse负责人工智能许可](https://oreil.ly/rmeaN)'
- en: '[Edge Impulse DMCA (Digital Millennium Copyright Act) Policy](https://oreil.ly/a6SwO)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Edge Impulse DMCA（数字千禧年版权法案）政策](https://oreil.ly/a6SwO)'
- en: Assuming you are abiding by the above rules and conditions, once you create
    and deploy your model to your device, you have no subscription or fees; as of
    writing this book (2022) all free Edge Impulse users are allowed to distribute
    and deploy their model to an endless amount of devices in production, for free.
    If your data was originally your own, you maintain your IP throughout the entire
    life cycle of your edge AI model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您遵守上述规则和条件，一旦您将模型创建并部署到设备上，您无需订阅或支付费用；截至撰写本书时（2022年），所有免费的Edge Impulse用户都可以免费将其模型分发并部署到无数的生产设备上。如果您的数据原本是您自己的，您在整个边缘AI模型的生命周期中都保留您的知识产权。
- en: If you are using a dataset downloaded from a third-party site such as iNaturalist,
    you will need to ensure that the data you have acquired is eligible to be redistributed
    or used for commercial use. More details about iNaturalist’s Terms of Use can
    be [viewed on their website](https://oreil.ly/Thjyc).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用从第三方网站（如iNaturalist）下载的数据集，则需要确保您获取的数据可以重新分发或用于商业用途。有关iNaturalist使用条款的更多详细信息可以[在其网站上查看](https://oreil.ly/Thjyc)。
- en: For any other datasets, please ensure you are acquiring, distributing, and using
    the data legally, fairly, and ethically. Many dataset collection sites will use
    licenses such as [Creative Commons](https://oreil.ly/AyCfy), Apache, etc. You
    will need to use your best judgment when using these datasets for the purposes
    of edge machine learning model training and testing. If you have any doubts, email
    the dataset owner or data collection site support team for more information on
    data usage requirements or attribution obligations and legal clarification.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何其他数据集，请确保您在合法、公正和道德的基础上获取、分发和使用数据。许多数据集收集网站会使用像[知识共享](https://oreil.ly/AyCfy)、Apache等许可证。在用于边缘机器学习模型训练和测试的目的时，您需要在使用这些数据集时做出最佳判断。如果有任何疑问，请通过电子邮件联系数据集所有者或数据收集网站支持团队，以获取有关数据使用要求、归属义务和法律澄清的更多信息。
- en: Cleaning Your Dataset
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理您的数据集
- en: Because we have downloaded our image dataset from iNaturalist and thus have
    already labeled our images with their associated machine learning class, we do
    not need to do much further dataset cleaning before we upload our images into
    our Edge Impulse project.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经从iNaturalist下载了我们的图像数据集，并已经使用与其关联的机器学习类别标记了我们的图像，所以在将图像上传到Edge Impulse项目之前，我们无需进行太多的数据集清理工作。
- en: However, if you have a small dataset of *labeled* images and also a larger dataset
    of associated but *unlabeled* images, Edge Impulse provides a tool called the
    [“data explorer”](https://oreil.ly/uhD9P) to allow you to use a pretrained model
    (see [Figure 11-5](#figure_edge_impulse_studio_data_explorer)), a previously trained
    impulse, or your preprocessing block to bulk label unlabeled images in your training
    or testing datasets. Of course, this tool will not work if you have not already
    trained a model on a smaller subset of your data, as unique species names are
    not pretrained on existing ImageNets like MobileNetV2, for example. You can also
    select between two different types of dimensionality reductions, t-SNE (works
    well with smaller datasets) and PCA (works with any dataset size).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您有一小部分*标记*图像的数据集和一大部分关联但*未标记*的图像数据集，Edge Impulse 提供名为[“数据浏览器”](https://oreil.ly/uhD9P)的工具，允许您使用预训练模型（参见[图 11-5](#figure_edge_impulse_studio_data_explorer)），先前训练过的
    impulse 或您的预处理块来批量标记训练或测试数据集中的未标记图像。当然，如果您尚未在数据的较小子集上训练模型，则此工具将无法工作，例如，像 MobileNetV2
    那样，唯一的物种名称并未预先训练在现有的 ImageNet 上。您还可以选择两种不同类型的降维技术，t-SNE（适用于较小的数据集）和 PCA（适用于任何数据集大小）。
- en: '![Edge Impulse Studio - data explorer](assets/aiae_1105.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![Edge Impulse Studio - 数据浏览器](assets/aiae_1105.png)'
- en: Figure 11-5\. Edge Impulse Studio data explorer
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-5\. Edge Impulse Studio 数据浏览器
- en: Uploading Data to Edge Impulse
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据上传到 Edge Impulse
- en: 'Following the iNaturalist Python data downloading script, upload your images
    to your Edge Impulse project (see [Figure 11-6](#figure_uploading_existing_dataset_edge_web))
    using the Edge Impulse project web GUI, or with the following [Edge Impulse CLI
    uploader](https://oreil.ly/l_OQo) command, and make sure to replace `[your-api-key]`
    with the API key of your Edge Impulse project, `[label]` with “unknown” or the
    name of your trap animal, and `[directory]` with the file directory you specified
    in the iNaturalist Python script:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 iNaturalist Python 数据下载脚本，将图像上传到您的 Edge Impulse 项目（参见[图 11-6](#figure_uploading_existing_dataset_edge_web)），使用
    Edge Impulse 项目的网页 GUI，或使用以下[Edge Impulse CLI 上传工具](https://oreil.ly/l_OQo)命令，确保将
    `[your-api-key]` 替换为您的 Edge Impulse 项目的 API 密钥，`[label]` 替换为“unknown”或陷阱动物的名称，`[directory]`
    替换为您在 iNaturalist Python 脚本中指定的文件目录：
- en: '[PRE3]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Edge Impulse Studio - web uploader](assets/aiae_1106.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![Edge Impulse Studio - 网页上传工具](assets/aiae_1106.png)'
- en: Figure 11-6\. Uploading existing dataset into the Edge Impulse web uploader
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-6\. 将现有数据集上传到 Edge Impulse 网页上传工具
- en: Both the web GUI and the uploader CLI allow you to automatically split the uploaded
    images into both the `training` and `testing` datasets at an 80/20 split (a good
    ratio for most machine learning projects).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 网页 GUI 和上传 CLI 都允许您将上传的图像自动分割为训练和测试数据集，按 80/20 的比例分割（大多数机器学习项目的良好比例）。
- en: DSP and Machine Learning Workflow
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DSP 和机器学习工作流程
- en: Now that we have uploaded all of our images into our training and testing datasets,
    we need to extract the most important features of our raw data using a digital
    signal processing (DSP) approach, and then train our machine learning model to
    identify patterns in our image’s extracted features. Edge Impulse calls the DSP
    and ML training workflow the “Impulse design.”
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已将所有图像上传到我们的训练和测试数据集中，我们需要使用数字信号处理（DSP）方法提取我们原始数据的最重要特征，然后训练我们的机器学习模型以识别图像提取特征中的模式。Edge
    Impulse 将DSP和ML训练工作流程称为“脉冲设计”。
- en: The “Impulse design” tab of your Edge Impulse project allows you to view and
    create a graphical, simple overview of your full end-to-end machine learning pipeline.
    On the far left is the raw data block where the Edge Impulse Studio will ingest
    and preprocess your data; in our case for images it will normalize all our images
    so they have the same dimensions, and if the dimensions are not square, it will
    crop the image via your method of choice.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 您的 Edge Impulse 项目的“脉冲设计”选项卡允许您查看和创建全面的端到端机器学习流水线的图形简单概述。最左边是原始数据块，Edge Impulse
    Studio 将摄取并预处理您的数据；在我们的情况下，对于图像，它将规范化所有图像，使它们具有相同的尺寸，并且如果尺寸不是正方形，则会通过您选择的方法裁剪图像。
- en: Next is the DSP block, where we will extract the most important features of
    our images via an open source digital signal processing script. Once we have generated
    our data’s features, the learning block will train our neural network based on
    our desired architecture and configuration settings.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 DSP 块，我们将通过开源数字信号处理脚本提取图像的最重要特征。一旦生成了我们数据的特征，学习块将根据我们期望的架构和配置设置来训练我们的神经网络。
- en: Finally, we can see the deployment output information, including the desired
    classes we would like our trained machine learning model to classify.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以查看部署输出信息，包括我们希望我们训练过的机器学习模型分类的期望类别。
- en: 'In your Edge Impulse project, set up your “Impulse design” tab the same as
    in [Figure 11-7](#figure_impulse_design_tab_configuration), or as listed by selecting
    from the various block pop-up windows, then click Save Impulse:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在Edge Impulse项目中，设置您的“Impulse design”选项卡与[Figure 11-7](#figure_impulse_design_tab_configuration)相同，或通过从各种块弹出窗口中选择来进行设置，然后点击保存脉冲：
- en: Image data
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据
- en: 'Image width: 160'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像宽度：160
- en: 'Image height: 160'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像高度：160
- en: 'Resize mode: Fit shortest axis'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模式：适合最短轴
- en: Processing block
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 处理块
- en: Image
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像
- en: Learning block
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 学习块
- en: Transfer Learning (Images)
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习（图像）
- en: '![Impulse design tab configuration](assets/aiae_1107.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![脉冲设计选项卡配置](assets/aiae_1107.png)'
- en: Figure 11-7\. Impulse design tab configuration
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-7\. 脉冲设计选项卡配置
- en: Digital Signal Processing Block
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数字信号处理块
- en: For the project presented in this chapter, we will be using the Image DSP algorithm
    that is included by default in the Edge Impulse Studio. This image processing
    block we selected on the “Impulse design” tab is prewritten and available for
    free use and free deployment from the platform. The code used in the Image block
    is available in the [Edge Impulse GitHub repository “processing-blocks”](https://oreil.ly/jjL2E).
    You can also learn more about the specifics of the Spectral Analysis algorithm
    in [“Image feature detection”](ch04.html#image_feature_detection).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章介绍的项目中，我们将使用Edge Impulse Studio默认包含的Image DSP算法。我们在“Impulse design”选项卡上选择的这个图像处理块是预先编写的，并可以在平台上免费使用和部署。图像块中使用的代码可以在[Edge
    Impulse GitHub仓库“processing-blocks”](https://oreil.ly/jjL2E)中找到。您还可以在[“Image feature
    detection”](ch04.html#image_feature_detection)中了解更多有关Spectral Analysis算法的详细信息。
- en: If you would like to write your own custom DSP block for use in the Edge Impulse
    Studio, it’s easy to do so in your language of choice by following the [Edge Impulse
    custom processing blocks tutorial](https://oreil.ly/Dx2KJ).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要为Edge Impulse Studio编写自己的定制DSP块，可以通过遵循[Edge Impulse自定义处理块教程](https://oreil.ly/Dx2KJ)使用您选择的语言轻松完成。
- en: However, if you do decide to write your own custom DSP processing block for
    your application, note that you will need to then write the corresponding C++
    implementation of your custom DSP Python/MATLAB/etc. code in order for your model
    deployment to work as intended within the Edge Impulse SDK. This is a major advantage
    of using a readily available DSP block in the Edge Impulse Studio as it reduces
    the total development time from data collection to feature extraction and then
    deployment—you do not need to write any of your own custom C++ code on the application
    side; everything is already integrated within the deployed library and ready for
    compilation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您决定为应用程序编写自己定制的DSP处理块，请注意，您需要编写相应的C++实现，以便您的模型部署按预期工作在Edge Impulse SDK中。在Edge
    Impulse Studio中使用现成的DSP块是一个重要优势，因为它可以从数据收集到特征提取再到部署的整个开发时间——您不需要在应用程序端编写任何自定义的C++代码；一切都已经集成在部署库中，准备好进行编译。
- en: From the Image tab in the navigation sidebar, leave the color depth as RGB and
    click on “Save parameters.” Now, select “Generate features” to create a view of
    the “Feature explorer” (see [Figure 11-8](#figure_image_dsp_block_feature_explorer)).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在导航侧边栏的图像选项卡中，将颜色深度保持RGB，并点击“保存参数”。现在，选择“生成特征”以创建“Feature explorer”的视图（见[Figure 11-8](#figure_image_dsp_block_feature_explorer)）。
- en: '![Image DSP block and feature explorer](assets/aiae_1108.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图像DSP块和特征探索器](assets/aiae_1108.png)'
- en: Figure 11-8\. Image DSP block and feature explorer
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-8\. 图像DSP块和特征探索器
- en: Machine Learning Block
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习块
- en: We are now ready to train our edge machine learning model! There are multiple
    ways to train your model in Edge Impulse, the easiest of which is the visual (or
    web GUI) editing mode. However, if you are a machine learning engineer, expert,
    or if you already have experiencing coding with TensorFlow/Keras, then you can
    also edit your transfer learning block locally or in expert mode within the Edge
    Impulse Studio.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备训练我们的边缘机器学习模型了！在Edge Impulse中有多种训练模型的方式，最简单的是可视（或Web GUI）编辑模式。但是，如果您是机器学习工程师、专家，或者已经具有使用TensorFlow/Keras编码的经验，那么您也可以在Edge
    Impulse Studio中的本地或专家模式中编辑您的迁移学习块。
- en: We can set the neural network architecture and other training configuration
    settings of our project from the “Transfer learning” tab.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从“转移学习”选项卡设置项目的神经网络架构和其他训练配置设置。
- en: Visual mode
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视觉模式
- en: 'The easiest way to configure and set our machine learning training settings
    and neural network architecture is through the Edge Impulse Visual mode, or the
    default view when you select the “Transfer learning” tab under “Impulse design”
    in the navigation bar. The following settings are automatically applied by default
    when you save an impulse with the transfer learning block (see [Figure 11-9](#figure_default_transfer_learning_neural_network));
    if these settings differ in your project, go ahead and copy these settings into
    your transfer learning block configuration:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 配置和设置我们的机器学习训练设置和神经网络架构的最简单方法是通过 Edge Impulse 的视觉模式，或者在导航栏中选择“冲动设计”下的“转移学习”选项卡时的默认视图。保存带有转移学习块的冲动时，默认应用以下设置（参见图
    [Figure 11-9](#figure_default_transfer_learning_neural_network)）；如果您的项目中这些设置不同，请复制这些设置到您的转移学习块配置中：
- en: 'Number of training cycles: 100'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练周期数：100
- en: 'Learning rate: 0.0005'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：0.0005
- en: 'Validation set size: 20%'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集大小：20%
- en: 'Auto-balance dataset: unchecked'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动平衡数据集：未选中
- en: 'Data augmentation: unchecked'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强：未选中
- en: 'Neural network architecture: MobileNetV2 96x96 0.35 (final layer: 16 neurons,
    0.1 dropout)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络架构：MobileNetV2 96x96 0.35（最终层：16个神经元，0.1的丢弃率）
- en: '![Default transfer learning Neural Network settings](assets/aiae_1109.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![默认迁移学习神经网络设置](assets/aiae_1109.png)'
- en: Figure 11-9\. Default transfer learning Neural Network settings
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-9\. 默认迁移学习神经网络设置
- en: Once you have entered the settings, all you need to do is click “Start training”
    below the neural network architecture configuration to spawn your training job
    on the Edge Impulse servers. The job that is spawned is training your model exactly
    how you would normally train your model if you were running a TensorFlow/Keras
    script locally on your own computer. By using Edge Impulse, we do not need to
    use up local resources on our own computer, and instead are leveraging the cloud
    compute time that Edge Impulse offers for free to all developers. Depending on
    the size of your dataset, this training step may take a while, in which case you
    can select the bell icon on the “Training output” view to get an email notification
    when your job has completed and you can see the output of the training results
    (see Figures [11-10](#figure_training_job_notification_bell_icon) and [11-11](#figure_configure_job_notification_settings)).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您输入了这些设置，您只需点击“开始训练”，在神经网络架构配置下方生成您的训练作业，将其发送到 Edge Impulse 服务器上。生成的作业将会精确地训练您的模型，就像在本地计算机上运行
    TensorFlow/Keras 脚本一样。通过使用 Edge Impulse，我们无需使用本地计算机的资源，而是利用 Edge Impulse 免费为所有开发者提供的云计算时间。根据您的数据集大小，此训练步骤可能需要一些时间，此时您可以选择“训练输出”视图上的铃铛图标，在作业完成时收到电子邮件通知，并查看训练结果的输出（参见图
    [11-10](#figure_training_job_notification_bell_icon) 和 [11-11](#figure_configure_job_notification_settings)）。
- en: '![Training job notification bell icon](assets/aiae_1110.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![训练作业通知铃铛图标](assets/aiae_1110.png)'
- en: Figure 11-10\. Training job notification bell icon
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-10\. 训练作业通知铃铛图标
- en: '![Configure job notification settings](assets/aiae_1111.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![配置作业通知设置](assets/aiae_1111.png)'
- en: Figure 11-11\. Configure job notification settings
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-11\. 配置作业通知设置
- en: Once your model training has completed, you can view the transfer learning results
    in the “Model > Last training performance” view (see [Figure 11-12](#figure_transfer_learning_results_block_configuration)).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的模型训练完成后，您可以在“模型 > 最后训练性能”视图中查看迁移学习结果（参见图 [Figure 11-12](#figure_transfer_learning_results_block_configuration)）。
- en: '![Default transfer learning results](assets/aiae_1112.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![默认迁移学习结果](assets/aiae_1112.png)'
- en: Figure 11-12\. Transfer learning results from default block configuration (76.3%
    accuracy)
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-12\. 默认块配置的迁移学习结果（准确率为76.3%）
- en: Considering that all we have done so far is upload our training and testing
    datasets, extracted the most important features with the image DSP block, and
    trained our model all with the default block configuration settings and without
    writing any code, these results are pretty decent! The result of 76.3% is a fairly
    good initial accuracy considering we haven’t done any custom configuration to
    our neural network architecture, DSP block, etc., for our specific use case. However,
    we can further increase the accuracy of our model by using the other tools available
    in Edge Impulse, such as the EON Tuner, which we will describe in the next section.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到到目前为止我们所做的一切仅仅是上传了训练和测试数据集，用图像 DSP 块提取了最重要的特征，并使用了默认的块配置设置进行了模型训练，而没有编写任何代码，这些结果还是相当不错的！76.3%
    的结果是一个相当不错的初始准确率，考虑到我们对神经网络架构、DSP 块等并没有进行任何自定义配置来适应我们的特定用例。然而，通过使用 Edge Impulse
    提供的其他工具，如 EON 调谐器，我们可以进一步提高模型的准确性，下一节将对此进行描述。
- en: Expert Mode
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专家模式
- en: Are you a machine learning engineer, or do you already know how to write TensorFlow/Keras
    code in Python? Use the Expert mode option in Edge Impulse to upload your own
    code or edit the existing block code locally by clicking the three-dot drop-down
    button to the right of “Neural Network settings” and selecting [“Switch to Expert
    (Keras) mode”](https://oreil.ly/wpEzB) or [“Edit block locally”](https://oreil.ly/sYSIP)
    from the menu.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 您是机器学习工程师，或者已经知道如何在 Python 中编写 TensorFlow/Keras 代码吗？在 Edge Impulse 中使用专家模式选项，通过点击“神经网络设置”右侧的三点下拉按钮，可以上传您自己的代码或本地编辑现有的块代码，选择[“切换到专家（Keras）模式”](https://oreil.ly/wpEzB)或[“本地编辑块”](https://oreil.ly/sYSIP)。
- en: EON Tuner
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EON 调谐器
- en: Auto machine learning tools are valuable tools that can automatically select
    and apply the best machine learning algorithms for your data and automatically
    tune the parameters of your machine learning model, which can further improve
    its performance on the edge device. The Edge Impulse Studio provides an auto machine
    learning tool in your project, called the EON Tuner. The EON Tuner will evaluate
    many candidate model architectures and DSP blocks (selected based on your target
    device and latency requirements) concurrently to help you find the best performing
    architecture for your machine learning application.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 自动机器学习工具是有价值的工具，可以自动选择和应用最佳的机器学习算法来处理您的数据，并自动调整您的机器学习模型的参数，从而进一步提高其在边缘设备上的性能。Edge
    Impulse Studio 在您的项目中提供了一个名为 EON 调谐器的自动机器学习工具。EON 调谐器将并行评估许多候选模型架构和 DSP 块（根据您的目标设备和延迟要求进行选择），以帮助您找到最适合您的机器学习应用的性能最佳架构。
- en: From the EON Tuner tab of your Edge Impulse project, configure the settings
    shown in [Figure 11-13](#figure_eon_tuner_configuration_settings).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的 Edge Impulse 项目的 EON 调谐器选项卡中，配置如 [图 11-13](#figure_eon_tuner_configuration_settings)
    所示的设置。
- en: 'Select the following options from the EON Tuner’s configuration drop-down settings:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 从 EON 调谐器的配置下拉设置中选择以下选项：
- en: 'Dataset category: Vision'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集类别：视觉
- en: 'Target device: Cortex-M7 (or any other supported platform; if you are not using
    an officially supported platform, choose a platform with hardware internals most
    similar to that of your device)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标设备：Cortex-M7（或任何其他支持的平台；如果您不使用官方支持的平台，请选择硬件内部结构与您的设备最相似的平台）
- en: 'Time per inference (ms): 100'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推断时间（毫秒）：100
- en: '![EON Tuner vision settings](assets/aiae_1113.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![EON 调谐器视觉设置](assets/aiae_1113.png)'
- en: Figure 11-13\. EON Tuner configuration settings
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-13\. EON 调谐器配置设置
- en: Then click “Start EON tuner,” as shown in [Figure 11-14](#figure_start_eon_tuner).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 然后点击“启动 EON 调谐器”，如 [图 11-14](#figure_start_eon_tuner) 所示。
- en: '![EON Tuner start button](assets/aiae_1114.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![EON 调谐器启动按钮](assets/aiae_1114.png)'
- en: Figure 11-14\. Start the EON Tuner
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-14\. 启动 EON 调谐器
- en: When comparing the results of the EON Tuner with the default image classification
    blocks included in a default Edge Impulse project, we can see a vast difference
    between them. With auto machine learning tools we can more quickly and efficiently
    determine better performing neural network architectures, DSP blocks, parameters,
    and more for our use case.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 将 EON 调谐器的结果与默认的边缘脉冲项目中包含的默认图像分类块进行比较，我们可以看到它们之间存在显著差异。通过自动机器学习工具，我们可以更快速、更高效地确定适合我们用例的更佳性能的神经网络架构、DSP
    块、参数等。
- en: '[Figure 11-15](#figure_transfer_learning_results_default_block_configuration2)
    shows the default block results with the Image RGB DSP block and the original
    “Transfer learning” neural network block with MobileNetV2 96x96 0.35 (final layer:
    16 neurons, 0.1 dropout), 100 training cycles, and 0.0005 learning rate.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-15](#figure_transfer_learning_results_default_block_configuration2)展示了使用图像RGB
    DSP块和原始“迁移学习”神经网络块的默认块结果，其中MobileNetV2 96x96 0.35（最终层：16个神经元，0.1的丢弃率），100个训练周期和0.0005的学习率。'
- en: '![Transfer learning results](assets/aiae_1115.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![迁移学习结果](assets/aiae_1115.png)'
- en: Figure 11-15\. Transfer learning results with EON Tuner block configuration
    (89.5% accuracy)
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-15\. 使用EON 调谐器块配置的迁移学习结果（准确率89.5%）
- en: Once the EON Tuner auto machine learning job has completed, we can see the results.
    For the EON Tuner results shown in [Figure 11-16](#figure_eon_tuner_results_matrix_best_result),
    the first result achieves an accuracy of 90%; however, we will not select this
    model as the RAM and ROM both exceed our target’s hardware specifications. So,
    the result we will select is the second best option, at 89% accuracy.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦EON 调谐器自动机器学习作业完成，我们可以看到结果。对于[图 11-16](#figure_eon_tuner_results_matrix_best_result)中显示的EON
    调谐器结果，第一个结果达到了90%的准确率；然而，由于RAM和ROM都超出了我们目标硬件规格，我们将选择第二个最佳选项，准确率为89%。
- en: '![EON Tuner results page](assets/aiae_1116.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![EON 调谐器结果页面](assets/aiae_1116.png)'
- en: Figure 11-16\. EON Tuner results matrix (the best result that does not exceed
    target RAM has 89% accuracy)
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-16\. EON 调谐器结果矩阵（不超过目标RAM的最佳结果为89%准确率）
- en: Based on these results, we definitely will want to update the primary block
    information with the blocks generated automatically for our use case with the
    EON Tuner. Next to the configuration with the best accuracy, click the Select
    button and update the primary model, as shown in [Figure 11-17](#figure_eon_tuner_primary_model).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些结果，我们肯定希望使用EON 调谐器为我们的用例自动生成的块更新主块信息。在最佳准确率配置旁边，点击选择按钮并更新主模型，如[图 11-17](#figure_eon_tuner_primary_model)所示。
- en: '![EON Tuner update the primary model button](assets/aiae_1117.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![EON 调谐器更新主模型按钮](assets/aiae_1117.png)'
- en: Figure 11-17\. Updating the primary model with EON Tuner
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-17\. 使用EON 调谐器更新主模型
- en: Wait for the Studio to update the “Impulse design” blocks in your project (see
    [Figure 11-18](#figure_eon_tuner_neural_network_settings)), then click on “Transfer
    learning” and see your updated trained model results, accuracy, and latency calculations,
    as shown in [Figure 11-19](#figure_eon_tuner_primary_transfer_learning_model_updated).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 等待Studio在您的项目中更新“脉冲设计”块（参见[图 11-18](#figure_eon_tuner_neural_network_settings)），然后点击“迁移学习”，查看您更新的训练模型结果、准确率和延迟计算，如[图
    11-19](#figure_eon_tuner_primary_transfer_learning_model_updated)所示。
- en: '![EON Tuner Neural Network settings](assets/aiae_1118.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![EON 调谐器神经网络设置](assets/aiae_1118.png)'
- en: Figure 11-18\. EON Tuner Neural Network settings
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-18\. EON 调谐器神经网络设置
- en: '![EON Tuner primary transfer learning model updated](assets/aiae_1119.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![EON 调谐器主迁移学习模型已更新](assets/aiae_1119.png)'
- en: Figure 11-19\. Primary transfer learning model updated with EON Tuner
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-19\. 使用EON 调谐器更新的主迁移学习模型
- en: Testing the Model
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试模型
- en: Edge Impulse provides multiple types of testing and verification tools to increase
    your confidence in the real-world accuracy of your trained machine learning model,
    or impulse. After you have finished training your impulse, on the navigation bar
    of your project, you can access the [“Live classification”](https://oreil.ly/lBG87)
    and [“Model testing”](https://oreil.ly/gO2EL) tabs.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Edge Impulse提供多种类型的测试和验证工具，以增加您对训练的机器学习模型或脉冲在实际环境准确性的信心。在完成训练脉冲后，您可以访问项目导航栏上的[“实时分类”](https://oreil.ly/lBG87)和[“模型测试”](https://oreil.ly/gO2EL)选项卡。
- en: Testing Your Audio Models with Performance Calibration
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试您的音频模型与性能校准
- en: 'If you developed an audio trap, as described in [“Deep Dive: Bird Sound Classification
    Demo with Lacuna Space”](#deep_dive_bird_sound_classification_demo_with_lacuna_space),
    then you can also use [Performance Calibration](https://oreil.ly/B3eQh) model
    testing and real-world performance tuner in your Edge Impulse project.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您开发了一个音频陷阱，如[“深入探讨：利用Lacuna Space进行鸟类声音分类演示”](#deep_dive_bird_sound_classification_demo_with_lacuna_space)所述，则还可以在您的Edge
    Impulse项目中使用[性能校准](https://oreil.ly/B3eQh)模型测试和真实世界性能调谐器。
- en: Live Classification
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时分类
- en: From the “Live classification” tab, you can test individual test samples from
    your testing dataset against your trained model or connect your device and record
    new images and test samples in real time, then view the image’s extracted features
    and resulting classification result and inferencing predictions (see [Figure 11-20](#figure_live_classification)).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 从“实时分类”选项卡，您可以测试来自测试数据集的单个测试样本与您训练的模型，或者连接您的设备实时录制新图像和测试样本，然后查看图像的提取特征和结果分类结果和推理预测（参见[图 11-20](#figure_live_classification)）。
- en: '![Live classification tab view](assets/aiae_1120.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![实时分类选项卡视图](assets/aiae_1120.png)'
- en: Figure 11-20\. Live classification
  id: totrans-275
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-20\. 实时分类
- en: 'Connect an officially supported device to the “Live classification” tab via
    the installed Edge Impulse device firmware, or via the Edge Impulse CLI data forwarder;
    for example, connect an Arduino Nano 33 BLE Sense to your project to take new
    testing images in the device’s environment via the following CLI command: `edge-impulse-daemon`.
    Follow the CLI prompts to connect your device to your project and record new samples.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 通过安装的Edge Impulse设备固件或通过Edge Impulse CLI数据转发器将官方支持的设备连接到“实时分类”选项卡；例如，连接Arduino
    Nano 33 BLE Sense到您的项目，在设备的环境中通过以下CLI命令拍摄新的测试图像：`edge-impulse-daemon`。按照CLI提示连接您的设备到项目并记录新样本。
- en: Or load an existing testing dataset image in “Classify existing test sample”
    to view this sample’s extracted features and your trained model’s prediction results
    (as shown in [Figure 11-21](#figure_live_classification_result)).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在“分类现有测试样本”中加载现有的测试数据集图像，以查看该样本的提取特征及您训练模型的预测结果（如[图 11-21](#figure_live_classification_result)所示）。
- en: '![Live classification result](assets/aiae_1121.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![实时分类结果](assets/aiae_1121.png)'
- en: Figure 11-21\. Live classification result
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-21\. 实时分类结果
- en: Model Testing
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型测试
- en: You can also bulk classify your testing dataset against your trained model by
    navigating to the [“Model testing”](https://oreil.ly/gPhj3) tab of your project
    (see [Figure 11-23](#figure_model_testing_tab_results1)). From here, you can select
    the “Classify all” button to automatically collect your testing data’s inferencing
    results and model predictions in one long table. You can also set the confidence
    threshold (shown in [Figure 11-22](#figure_set_confidence_thresholds2)) for your
    model’s inferencing results here by clicking on the three-dot drop-down button
    “Set confidence thresholds.” The threshold score determines how to trust the trained
    neural network. If the confidence rating is below the value you set, the sample
    will be tagged as “uncertain.” You can use inferencing results with “uncertain”
    to increase the accuracy of your model even further with an “active learning”
    model development strategy; upload these uncertain images, label them, retrain
    your model, and redeploy to your edge device! See [Figure 11-23](#figure_model_testing_tab_results1)
    for the model testing results.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过导航到您项目的[“模型测试”](https://oreil.ly/gPhj3)选项卡，对您的测试数据集进行批量分类，与您训练的模型进行对比（参见[图 11-23](#figure_model_testing_tab_results1)）。从这里，您可以选择“全部分类”按钮，自动收集测试数据的推理结果和模型预测，并呈现在一个长表格中。您还可以通过单击“设置置信阈值”的三个点下拉按钮在此设置模型推理结果的置信阈值（如[图 11-22](#figure_set_confidence_thresholds2)所示）。阈值分数决定了对训练的神经网络信任程度。如果置信评分低于您设定的值，样本将被标记为“不确定”。您可以使用带有“不确定”标记的推理结果，通过“主动学习”模型开发策略进一步提高模型的准确性；上传这些不确定图像，标记它们，重新训练您的模型，并重新部署到您的边缘设备！查看[图 11-23](#figure_model_testing_tab_results1)以获取模型测试结果。
- en: '![Set confidence thresholds](assets/aiae_1122.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![设置置信阈值](assets/aiae_1122.png)'
- en: Figure 11-22\. Set confidence thresholds
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-22\. 设置置信阈值
- en: '![Model testing tab results](assets/aiae_1123.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![模型测试选项卡结果](assets/aiae_1123.png)'
- en: Figure 11-23\. Model testing tab results
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-23\. 模型测试选项卡结果
- en: Test Your Model Locally
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地测试您的模型
- en: You can also download all of the intermediate block results and trained model
    information to test your model locally through any method you desire—i.e., with
    a Python script to test your model as you normally would for a TensorFlow/Keras
    workflow. Navigate to the Dashboard of your Edge Impulse project to view all of
    the block output files available, as shown in [Figure 11-24](#figure_download_block_output).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以下载所有中间块结果和训练模型信息，通过任何您希望的方法在本地测试您的模型——例如，使用Python脚本进行测试，如您通常为TensorFlow/Keras工作流程所做。导航到您的Edge
    Impulse项目的仪表板，查看所有可用的块输出文件，如[图 11-24](#figure_download_block_output)所示。
- en: '![Download block output](assets/aiae_1124.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![下载块输出](assets/aiae_1124.png)'
- en: Figure 11-24\. Download block output
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-24\. 下载块输出
- en: Deployment
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: Congratulations! You have just finished collecting and labeling your training
    and testing datasets, extracting your data’s features with the DSP block, designing
    and training your machine learning model, and testing your model with your testing
    dataset. Now that we have all of the code and model information needed for inferencing
    on our edge device, we need to flash the prebuilt binary to the device or integrate
    the C++ library into our embedded application code.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您刚刚完成了收集和标记训练和测试数据集，使用DSP块提取数据特征，设计和训练机器学习模型，并使用测试数据集测试模型的过程。现在我们已经拥有了在边缘设备上进行推理所需的所有代码和模型信息，我们需要将预构建的二进制文件刷入设备，或者将C++库集成到我们的嵌入式应用程序代码中。
- en: Select the Deployment tab of your Edge Impulse project and follow the steps
    for one of the many deployment options in the next sections to run your trained
    machine learning model on your edge device.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 选择您的Edge Impulse项目的部署选项卡，并按照下一节中的多个部署选项的步骤之一来在边缘设备上运行您训练好的机器学习模型。
- en: Create Library
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建库
- en: For a simple development experience, Edge Impulse provides many prewritten code
    examples for integrating your deployed model into your embedded application firmware.
    Using an officially supported development board will allow for the quickest deployment
    and least amount of development time, as you will be able to drag-and-drop the
    resulting prebuilt firmware application onto your development board, or clone
    the board’s open source firmware repository from the [Edge Impulse GitHub organization](https://oreil.ly/rH9iO),
    which contains all device firmware and drivers needed in order to get started
    quickly with your embedded application development and debugging process.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化开发体验，Edge Impulse提供了许多预写的代码示例，用于将部署的模型集成到嵌入式应用固件中。使用官方支持的开发板将允许最快的部署和最少的开发时间，因为您可以将预构建的固件应用程序拖放到开发板上，或者从[Edge
    Impulse GitHub组织](https://oreil.ly/rH9iO)克隆板的开源固件存储库，该存储库包含开始嵌入式应用程序开发和调试过程所需的所有设备固件和驱动程序。
- en: 'If you are deploying your model to an “unofficially supported” platform, there
    are many resources available to help you with integrating the Edge Impulse SDK
    into your application code, regardless of library deployment option:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将模型部署到“非官方支持”的平台，有许多资源可帮助您将Edge Impulse SDK集成到您的应用程序代码中，无论库部署选项如何：
- en: '[Prebuilt Edge Impulse firmwares](https://oreil.ly/V3eRI)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[预构建的Edge Impulse固件](https://oreil.ly/V3eRI)'
- en: '[Integrating the Edge Impulse SDK into your application](https://oreil.ly/yAlgD)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将Edge Impulse SDK集成到你的应用程序中](https://oreil.ly/yAlgD)'
- en: '[Understanding the C++ library code and getting model inference results](https://oreil.ly/-gPy_)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解C++库代码并获取模型推理结果](https://oreil.ly/-gPy_)'
- en: A majority of projects utilizing an “unofficially supported” device will deploy
    using the C++ library option available under the “Create library” view of your
    project’s Deployment tab (see [Figure 11-25](#figure_create_open_source_library1)).
    The C++ library is portable, with no external dependencies, and can be compiled
    with any modern C++ compiler.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 多数使用“非官方支持”的设备的项目，将使用位于项目部署选项卡“创建库”视图下的C++库选项部署（见[图 11-25](#figure_create_open_source_library1)）。该C++库是可移植的，没有外部依赖，并且可以使用任何现代C++编译器进行编译。
- en: Custom Processing Blocks
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义处理块
- en: If you decided to use your own custom DSP block in your Edge Impulse Studio
    project, you will need to write the DSP block’s equivalent C++ implementation
    and integrate this into the Edge Impulse SDK code. More information can be found
    in the [Edge Impulse documentation](https://oreil.ly/t1K1_).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定在你的Edge Impulse Studio项目中使用自定义的DSP块，你需要编写DSP块的等效C++实现，并将其集成到Edge Impulse
    SDK代码中。更多信息可以在[Edge Impulse文档](https://oreil.ly/t1K1_)中找到。
- en: '![Create an open source library](assets/aiae_1125.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![创建一个开源库](assets/aiae_1125.png)'
- en: Figure 11-25\. Create an open source library
  id: totrans-303
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-25\. 创建一个开源库
- en: Mobile Phone and Computer
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手机和电脑
- en: Quickly deploy your model to the edge with your computer or mobile phone by
    clicking the Edge Impulse “Computer” and “Mobile phone” deployment options. These
    deployment options utilize an [open source mobile client firmware](https://oreil.ly/4-S9S)
    that builds a WebAssembly library for your trained impulse to classify brand-new
    data directly from the camera on your phone or computer. This option is great
    for quick model prototyping and testing, since you do not need to write any code
    for this deployment option if you are using a default/integrated sensor type in
    your training/testing datasets.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 通过单击Edge Impulse的“计算机”和“手机”部署选项，快速将您的模型部署到边缘。这些部署选项利用一个[开源移动客户端固件](https://oreil.ly/4-S9S)，为您训练的脉冲构建一个WebAssembly库，可以直接从手机或计算机上的摄像头对全新数据进行分类。如果您在训练/测试数据集中使用默认/集成传感器类型，则此选项非常适合快速模型原型设计和测试，因为您不需要为此部署选项编写任何代码。
- en: For this project, because our training and testing data is just images, we can
    use the camera on our phone to test our model directly on the edge through the
    cache of our phone’s web browser and integrated camera data (see [Figure 11-26](#figure_run_impulse_directly_phone_computer)).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，因为我们的训练和测试数据只是图像，我们可以使用手机上的摄像头直接在边缘上测试我们的模型，通过手机的网络浏览器缓存和集成摄像头数据（参见[图11-26](#figure_run_impulse_directly_phone_computer)）。
- en: '![Run your impulse directly (mobile phone and computer)](assets/aiae_1126.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![直接运行您的脉冲（手机和电脑）](assets/aiae_1126.png)'
- en: Figure 11-26\. Run your impulse directly (mobile phone and computer)
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-26\. 直接运行您的脉冲（手机和电脑）
- en: Select the “Computer” or “Mobile phone” icon from the Deployment tab of your
    project, and click Build. If you are using your mobile phone, scan the generated
    QR code with your phone’s camera and open the URL in your phone’s web browser.
    Give the mobile client access to your phone’s camera and wait for your project
    to build. Now you are viewing your trained camera trap model running on the edge
    and printing your inferencing results directly on your mobile phone! See the result
    in [Figure 11-27](#figure_camera_trap_model_phone_deployment).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 从您项目的部署选项卡中选择“计算机”或“手机”图标，然后点击生成。如果您使用的是手机，请使用手机摄像头扫描生成的QR码，并在手机的网络浏览器中打开URL。允许移动客户端访问您手机的摄像头，并等待项目生成。现在，您可以在手机上直接查看您的训练相机陷阱模型在边缘上运行，并打印您的推理结果！在[图11-27](#figure_camera_trap_model_phone_deployment)中查看结果。
- en: '![Camera trap model running on mobile phone deployment](assets/aiae_1127.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![在手机部署上运行的相机陷阱模型](assets/aiae_1127.png)'
- en: Figure 11-27\. Camera trap model running on mobile phone deployment
  id: totrans-311
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-27\. 在手机部署上运行的相机陷阱模型
- en: Prebuilt Binary Flashing
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预构建二进制刷写
- en: From the Deployment tab, select your officially supported Edge Impulse development
    platform under “Build firmware” and then select Build. You also have the option
    to utilize the EON Compiler, which lets you run neural networks using 25–55% less
    RAM, and up to 35% less flash, while retaining the same accuracy, compared to
    TensorFlow Lite for Microcontrollers.^([12](ch11.html#idm45988802021408))
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 从部署选项卡中，在“生成固件”下选择您的官方支持的Edge Impulse开发平台，然后选择生成。您还可以选择使用EON编译器，它使您可以使用25-55%更少的RAM和高达35%更少的闪存来运行神经网络，同时保持与TensorFlow
    Lite for Microcontrollers相同的准确性，详见^([12](ch11.html#idm45988802021408))。
- en: Then, drag-and-drop or flash the resulting firmware application onto your officially
    supported platform by following the instructions shown after clicking Build from
    the Deployment tab. More in-depth instructions for flashing your prebuilt binary
    can be found in the [Edge Impulse documentation for your chosen development platform](https://oreil.ly/llg9B).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，按照部署选项中显示的说明，通过拖放或刷写生成的固件应用程序到您的官方支持平台。关于刷写预构建二进制文件的更深入说明可以在您选择的开发平台的[Edge
    Impulse文档](https://oreil.ly/llg9B)中找到。
- en: For this project, we will select the “OpenMV Library” deployment option to run
    our trained model on the [OpenMV Cam H7 Plus](https://oreil.ly/EfJwe), shown earlier
    in [Figure 11-25](#figure_create_open_source_library1).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们将选择“OpenMV Library”部署选项，在[OpenMV Cam H7 Plus](https://oreil.ly/EfJwe)上运行我们的训练模型，如前文所示[图11-25](#figure_create_open_source_library1)。
- en: Follow the instructions in the [OpenMV deployment documentation on the Edge
    Impulse website](https://oreil.ly/82tKN) to download and install software prerequisites.
    Then extract the downloaded ZIP file of your model firmware and drag-and-drop
    or copy the *labels.txt* and *trained.tflite* files into the filesystem of your
    plugged-in OpenMV Cam H7 Plus. Open the *ei_image_classification.py* script in
    the OpenMV IDE. Connect to your OpenMV Cam board via the USB icon and run the
    Python script to see your model’s inferencing results running on the edge in the
    serial terminal view, as shown in [Figure 11-28](#figure_openmv_ide_model_deployment_cam).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 [Edge Impulse 网站上的 OpenMV 部署文档](https://oreil.ly/82tKN) 的说明下载并安装软件先决条件。然后将您的模型固件的下载
    ZIP 文件解压并将 *labels.txt* 和 *trained.tflite* 文件拖放或复制到您插入的 OpenMV Cam H7 Plus 的文件系统中。在
    OpenMV IDE 中打开 *ei_image_classification.py* 脚本。通过 USB 图标连接到您的 OpenMV Cam 板并运行
    Python 脚本，在串行终端视图中查看在边缘上运行的模型推断结果，如 [图 11-28](#figure_openmv_ide_model_deployment_cam)
    所示。
- en: '![OpenMV IDE model deployment to OpenMV Cam H7 Plus](assets/aiae_1129.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![OpenMV IDE 模型部署到 OpenMV Cam H7 Plus](assets/aiae_1129.png)'
- en: Figure 11-28\. OpenMV IDE model deployment to OpenMV Cam H7 Plus
  id: totrans-318
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-28\. OpenMV IDE 模型部署到 OpenMV Cam H7 Plus
- en: Impulse Runner
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脉冲运行者
- en: You can also use the [Edge Impulse CLI](https://oreil.ly/KVUJf) to download,
    deploy, and run your models continuously through a USB serial connection to an
    officially supported platform of your choosing. Or use the [Edge Impulse Linux
    runner](https://oreil.ly/SJZex) to download, deploy, and run the Edge Impulse
    model on a Raspberry Pi 4 or other Linux device.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 [Edge Impulse CLI](https://oreil.ly/KVUJf) 通过 USB 串行连接到您选择的官方支持平台下载、部署和运行您的模型。或者使用
    [Edge Impulse Linux runner](https://oreil.ly/SJZex) 在 Raspberry Pi 4 或其他 Linux
    设备上下载、部署和运行 Edge Impulse 模型。
- en: GitHub Source Code
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GitHub 源代码
- en: The application source code used in this chapter, including the deployed library
    from the public [Edge Impulse project](https://oreil.ly/I_EIA) and completed application
    code, is available to view and download from the [GitHub repository](https://oreil.ly/rmE7-).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的应用程序源代码，包括来自公共 [Edge Impulse 项目](https://oreil.ly/I_EIA) 的部署库和完成的应用程序代码，可在
    [GitHub 仓库](https://oreil.ly/rmE7-) 上查看和下载。
- en: Iterate and Feedback Loops
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代和反馈循环
- en: 'Now that you have deployed the first iteration of your wildlife monitoring
    model to the edge, you may be satisfied with the results and discontinue development
    here. However, if you wish to further iterate over your model and further improve
    the accuracy over time or with newly acquired equipment upgrades, for example,
    there are many adaptations and variations to consider and improve upon for this
    project:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经将第一次迭代的野生动物监测模型部署到边缘设备，您可能对结果感到满意，并停止在这里进行开发。然而，如果您希望进一步迭代您的模型并随着时间或新获得的设备升级进一步提高准确性，例如，有许多适应和变体需要考虑和改进这个项目：
- en: Add more machine learning classes to your model for different animal(s).
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为不同动物添加更多的机器学习类别到您的模型中。
- en: 'Create a camera trap for invasive plant species instead of animals: for local
    gardening/foraging purposes, etc.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建用于入侵植物物种的相机陷阱而不是动物：用于当地园艺/觅食等目的。
- en: Use different sensors to achieve the same goal—i.e., a wildlife conservation
    trap using a gas sensor or switching your camera training data input from labeled
    images to bounding boxes for species object detection (see [“Object detection
    and segmentation”](ch04.html#algo_object_detection)).
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的传感器实现相同的目标 —— 即，使用气体传感器进行野生动物保护陷阱，或将相机训练数据输入从标记图像转换为物种目标检测的边界框（参见 [“对象检测和分割”](ch04.html#algo_object_detection)）。
- en: Use the same model to achieve a *different* goal or place it in a different
    environment, refining the “unknown” class.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用同样的模型实现*不同*的目标或将其放置在不同的环境中，以改进“未知”类别。
- en: Utilize a combination of sensors to further improve the accuracy of your model—i.e.,
    camera + audio input, audio + gas input, etc.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用多种传感器的组合进一步提高模型的准确性 —— 即，相机 + 音频输入，音频 + 气体输入等。
- en: You can also create multiple projects in Edge Impulse, to create many different
    machine learning models for multiple device locations, multiple datasets, and
    classifying other trap animals. For example, you can use the same Sahara desert
    model for multiple animal species, and just swap out the main species for another
    of your choosing in the initial dataset and then retrain and redeploy. This allows
    you to utilize the same model configuration as you used for one environment on
    another.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在 Edge Impulse 中创建多个项目，为多个设备位置创建多个不同的机器学习模型，并对其他捕捉动物进行分类。例如，您可以在初始数据集中为多个动物物种使用相同的撒哈拉沙漠模型，并仅替换主要物种，然后重新训练和部署。这允许您在另一个环境中使用与您用于一个环境相同的模型配置。
- en: AI for Good
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI for Good
- en: 'Throughout this book, we have discussed the importance of using the machine
    learning tools and knowledge described here in an ethical fashion. Many companies
    have already put the idea of “technology for good” to use—from Edge Impulse to
    Google, many environmental/wildlife conservation efforts and pledges have been
    established:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 本书始终强调了在道德上使用所描述的机器学习工具和知识的重要性。许多公司已经开始将“技术为善”理念付诸实践——从 Edge Impulse 到 Google，已经建立了许多环境/野生动物保护的努力和承诺：
- en: '[1% for the Planet](https://oreil.ly/_xwYK)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1% 为地球计划](https://oreil.ly/_xwYK)'
- en: '[Edge Impulse’s commitment](https://oreil.ly/CRH0m) to 1% for the Planet'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Edge Impulse 的承诺](https://oreil.ly/CRH0m) 给地球 1% 的贡献'
- en: Google, [“Tale of a Whale Song”](https://oreil.ly/wtIpX)
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google, [“鲸鱼歌谣”故事](https://oreil.ly/wtIpX)
- en: Microsoft, [AI for Good](https://oreil.ly/o0TGV)
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft, [AI for Good](https://oreil.ly/o0TGV)
- en: Related Works
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关作品
- en: As stated throughout this chapter, camera traps and conservation traps are an
    established, known, and widely adopted device in research efforts and in ethical
    hunting practices. The next sections describe various devices, datasets, research
    articles, and books on the topic of ethical camera trapping for the problem of
    decreasing populations of various endangered species and the resulting protection
    of existing populations.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章所述，摄像陷阱和保护陷阱已经成为研究和道德狩猎实践中已知并广泛采用的设备。接下来的章节描述了关于道德摄像陷阱的各种设备、数据集、研究文章和书籍，用于解决各种濒危物种减少和保护现有种群的问题。
- en: This book also notes the sources for various applications, methods, devices,
    and quotes from various research and commercial adoption of camera traps throughout
    the chapter in the footnotes on each page.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 本书还在每页脚注中注明了本章节中摄像陷阱的各种应用、方法、设备和引用的来源，包括研究和商业采用。
- en: Datasets
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'There are many existing datasets and dataset collection platforms available
    for this type of use case on the internet. A simple Google search can yield many
    results; however, a few more data collection platforms and research datasets for
    our use case have been listed below:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在互联网上有许多现有的数据集和数据集收集平台可供此类用例使用。通过简单的谷歌搜索可以找到许多结果；然而，以下是几个适用于我们用例的数据收集平台和研究数据集：
- en: '[Kaggle Invasive Species Monitoring Competition](https://oreil.ly/H4Y3N)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle 入侵物种监测竞赛](https://oreil.ly/H4Y3N)'
- en: '[Invasive Alien Plant dataset](https://oreil.ly/xfBKr)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[外来入侵植物数据集](https://oreil.ly/xfBKr)'
- en: '[iWildcam 2021](https://oreil.ly/76OW4)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[iWildcam 2021](https://oreil.ly/76OW4)'
- en: '[Labeled Information Library of Alexandria: Biology and Conservation; list
    of other conservation datasets](https://oreil.ly/-IUvi)'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[亚历山大图书馆标记信息: 生物学与保护; 其他保护数据集列表](https://oreil.ly/-IUvi)'
- en: '[Caltech-UCSD Birds-200-2011, classification of birds by camera](https://oreil.ly/lLU00)'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[加州理工大学-加州大学圣地亚哥分校 200-2011 年鸟类摄影分类](https://oreil.ly/lLU00)'
- en: '[Caltech Camera Traps](https://oreil.ly/boZ8q)'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[加州理工摄像陷阱](https://oreil.ly/boZ8q)'
- en: Again, be mindful that you are using each dataset for ethical purposes and ensure
    that your model’s target species is not considered endangered or threatened in
    your device’s installed location/region.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，请确保您在使用每个数据集时都是出于道德目的，并确保您的模型的目标物种在设备安装的位置/地区中不被视为濒危或受威胁。
- en: Research
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 研究
- en: 'Ahumada, Jorge A. et al. [*Wildlife Insights: A Platform to Maximize the Potential
    of Camera Trap and Other Passive Sensor Wildlife Data for the Planet*](https://doi.org/10.1017/S0376892919000298).
    Cambridge University Press, 2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ahumada, Jorge A. 等人 [*Wildlife Insights: A Platform to Maximize the Potential
    of Camera Trap and Other Passive Sensor Wildlife Data for the Planet*](https://doi.org/10.1017/S0376892919000298).
    剑桥大学出版社, 2019.'
- en: Apps, Peter, and John Weldon McNutt. [“Are Camera Traps Fit for Purpose? A Rigorous,
    Reproducible and Realistic Test of Camera Trap Performance”](https://doi.org/10.1111/aje.12573).
    Wiley Online Library, 2018.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apps, Peter, 和 John Weldon McNutt. [“摄像机陷阱是否适用？摄像机陷阱性能的严格、可重复和真实测试”](https://doi.org/10.1111/aje.12573)。Wiley
    Online Library, 2018年。
- en: Fischer, Johannes H. et al. [“The Potential Value of Camera-Trap Studies for
    Identifying, Ageing, Sexing and Studying the Phenology of Bornean Lophura Pheasants”](https://oreil.ly/udikH).
    ResearchGate, 2017.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fischer, Johannes H. 等人。[“用于鉴定、年龄鉴定、性别鉴定和研究婆罗洲冠雉的摄像机陷阱研究的潜在价值”](https://oreil.ly/udikH)。ResearchGate，2017年。
- en: Jang, Woohyuk, and Eui Chul Lee. [“Multi-Class Parrot Image Classification Including
    Subspecies with Similar Appearance”](https://doi.org/10.3390/biology10111140).
    MDPI, November 5, 2021.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang, Woohyuk, 和 Eui Chul Lee。[“包括外观相似亚种的多类鹦鹉图像分类”](https://doi.org/10.3390/biology10111140)。MDPI，2021年11月5日。
- en: 'O’Brien, Timothy G., and Margaret F. Kinnaird. [*A Picture Is Worth a Thousand
    Words: The Application of Camera Trapping to the Study of Birds*](https://doi.org/10.1017/S0959270908000348).
    Cambridge University Press, 2008.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Brien, Timothy G., 和 Margaret F. Kinnaird。[*一图胜千言：摄像机陷阱在鸟类研究中的应用*](https://doi.org/10.1017/S0959270908000348)。Cambridge
    University Press，2008年。
- en: 'O’Connell, Allan F. et al., eds. [*Camera Traps in Animal Ecology: Methods
    and Analyses*](https://doi.org/10.1007/978-4-431-99495-4). Springer Tokyo, 2011.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'O’Connell, Allan F. 等人，编辑。[*Camera Traps in Animal Ecology: Methods and Analyses*](https://doi.org/10.1007/978-4-431-99495-4)。Springer
    Tokyo, 2011年。'
- en: Rovero, Francesco et al. [“Which Camera Trap Type and How Many Do I Need?”](https://doi.org/10.4404/hystrix-24.2-8789)
    *Hystrix* 24 (2013).
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rovero, Francesco 等人。[“哪种摄像机陷阱类型和数量我需要？”](https://doi.org/10.4404/hystrix-24.2-8789)
    *Hystrix* 24 (2013年)。
- en: Shepley, Andrew et al. [“Automated Location Invariant Animal Detection in Camera
    Trap Images Using Publicly Available Data Sources”](https://oreil.ly/FUEJN). ResearchGate,
    2021.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shepley, Andrew 等人。[“使用公共数据源在摄像机陷阱图像中进行自动位置不变动物检测”](https://oreil.ly/FUEJN)。ResearchGate，2021年。
- en: ^([1](ch11.html#idm45988812226016-marker)) See National Wildlife Federation
    article [“Habitat Loss”](https://oreil.ly/kpOVl).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11.html#idm45988812226016-marker)) 查看国家野生生物联合会的文章，[“栖息地丧失”](https://oreil.ly/kpOVl)。
- en: ^([2](ch11.html#idm45988812223184-marker)) See the website, [“United Nations
    AI for Good”](https://aiforgood.itu.int).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch11.html#idm45988812223184-marker)) 查看网站，[“联合国AI for Good”](https://aiforgood.itu.int)。
- en: ^([3](ch11.html#idm45988812221760-marker)) See Google’s site, [“AI for Social
    Good”](https://oreil.ly/8L3BY).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch11.html#idm45988812221760-marker)) 查看谷歌的网站，[“AI for Social Good”](https://oreil.ly/8L3BY)。
- en: ^([4](ch11.html#idm45988812220336-marker)) See Microsoft’s site, [“AI for Good”](https://oreil.ly/8ZLQI).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch11.html#idm45988812220336-marker)) 查看微软的网站，[“AI for Good”](https://oreil.ly/8ZLQI)。
- en: '^([5](ch11.html#idm45988812201184-marker)) Abu Naser Mohsin Hossain et al.,
    [“Assessing the Efficacy of Camera Trapping as a Tool for Increasing Detection
    Rates of Wildlife Crime in Tropical Protected Areas”](https://doi.org/10.1016/j.biocon.2016.07.023),
    *Biological Conservation* 201 (2016): 314–19.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '^([5](ch11.html#idm45988812201184-marker)) 阿布·纳瑟·莫辛·侯赛因等人，[“评估摄像机陷阱作为提高热带保护区野生动物犯罪检测率工具的效力”](https://doi.org/10.1016/j.biocon.2016.07.023)，《生物保护》201
    (2016): 314–19。'
- en: '^([6](ch11.html#idm45988812197536-marker)) From an article by Abu Naser Mohsin
    Hossain et al., [“Pangolins in Global Camera Trap Data: Implications for Ecological
    Monitoring”](https://doi.org/10.1016/j.gecco.2019.e00769), *Conservation* 201
    (2016): 314–19.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '^([6](ch11.html#idm45988812197536-marker)) 来自阿布·纳瑟·莫辛·侯赛因等人的文章，《全球摄像机陷阱数据中的穿山甲：生态监测的意义》[*Conservation*
    201 (2016): 314–19](https://doi.org/10.1016/j.gecco.2019.e00769)。'
- en: ^([7](ch11.html#idm45988811651456-marker)) See the article by Paul D. Meek et
    al., [“Camera Traps Can Be Heard and Seen by Animals”](https://doi.org/10.1371/journal.pone.0110832).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch11.html#idm45988811651456-marker)) 查看保罗·D·米克等人的文章，[“动物能听到和看到摄像机陷阱”](https://doi.org/10.1371/journal.pone.0110832)。
- en: '^([8](ch11.html#idm45988811641664-marker)) See this article in *The Guardian*:
    [“Report Clears WWF of Complicity in Violent Abuses by Conservation Rangers”](https://oreil.ly/JQ2tE).'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch11.html#idm45988811641664-marker)) 查看《卫报》的这篇文章：[“报告澄清世界自然基金会未参与保护区护林员暴力行为”](https://oreil.ly/JQ2tE)。
- en: ^([9](ch11.html#idm45988811626576-marker)) “Thai Squirrel,” Dutch Food Safety
    Authority, 2022.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch11.html#idm45988811626576-marker)) 《泰国松鼠》，荷兰食品安全局，2022年。
- en: '^([10](ch11.html#idm45988811564112-marker)) This article gives some more information:
    [Fischer et al., “The Potential Value of Camera-Trap Studies for Identifying,
    Ageing, Sexing and Studying the Phenology of Bornean Lophura Pheasants”](https://oreil.ly/id-Bc).'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch11.html#idm45988811564112-marker)) 这篇文章提供了一些更多信息：[Fischer 等人，“用于鉴别、年龄、性别和研究婆罗洲六鸡的摄像机陷阱研究的潜在价值”](https://oreil.ly/id-Bc)。
- en: ^([11](ch11.html#idm45988802278176-marker)) See the article by Neil A. Gilbert
    et al., [“Abundance Estimation of Unmarked Animals Based on Camera-Trap Data”](https://doi.org/10.1111/cobi.13517).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch11.html#idm45988802278176-marker)) 查看 Neil A. Gilbert 等人的文章，[“基于摄像机陷阱数据的未标记动物丰富度估计”](https://doi.org/10.1111/cobi.13517)。
- en: '^([12](ch11.html#idm45988802021408-marker)) See Jan Jongboom’s blog post, [“Introducing
    EON: Neural Networks in Up to 55% Less RAM and 35% Less ROM”](https://oreil.ly/3-kTN)
    (Edge Impulse, 2020).'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch11.html#idm45988802021408-marker)) 查看 Jan Jongboom 的博客文章，[“介绍 EON：神经网络内存减少
    55% 和 ROM 减少 35%”](https://oreil.ly/3-kTN)（Edge Impulse，2020）。
- en: '^([13](ch11.html#idm45988801983776-marker)) See Aurelien Lequertier et al.
    blog post: [“Bird Classification in Remote Areas with Lacuna Space and The Things
    Network”](https://oreil.ly/4Rneh), Edge Impulse, 2021.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch11.html#idm45988801983776-marker)) 查看 Aurelien Lequertier 等人的博客文章：[“利用
    Lacuna Space 和 The Things Network 在偏远地区进行鸟类分类”](https://oreil.ly/4Rneh)，Edge Impulse，2021。
