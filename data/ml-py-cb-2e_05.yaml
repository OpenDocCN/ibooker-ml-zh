- en: Chapter 5\. Handling Categorical Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。处理分类数据
- en: 5.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.0 介绍
- en: 'It is often useful to measure objects not in terms of their quantity but in
    terms of some quality. We frequently represent qualitative information in categories
    such as gender, colors, or brand of car. However, not all categorical data is
    the same. Sets of categories with no intrinsic ordering are called *nominal*.
    Examples of nominal categories include:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通常有用的是，我们不仅仅用数量来衡量物体，而是用某种质量来衡量。我们经常用类别如性别、颜色或汽车品牌来表示定性信息。然而，并非所有分类数据都相同。没有内在排序的类别集称为*名义*。名义类别的例子包括：
- en: Blue, Red, Green
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝色，红色，绿色
- en: Man, Woman
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 男，女
- en: Banana, Strawberry, Apple
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 香蕉，草莓，苹果
- en: 'In contrast, when a set of categories has some natural ordering we refer to
    it as *ordinal*. For example:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，当一组类别具有一些自然顺序时，我们称之为*序数*。例如：
- en: Low, Medium, High
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低，中，高
- en: Young, Old
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年轻，年老
- en: Agree, Neutral, Disagree
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同意，中立，不同意
- en: Furthermore, categorical information is often represented in data as a vector
    or column of strings (e.g., `"Maine"`, `"Texas"`, `"Delaware"`). The problem is
    that most machine learning algorithms require inputs to be numerical values.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，分类信息通常以向量或字符串列（例如`"Maine"`、`"Texas"`、`"Delaware"`）的形式表示在数据中。问题在于，大多数机器学习算法要求输入为数值。
- en: 'The k-nearest neighbors algorithm is an example of an algorithm that requires
    numerical data. One step in the algorithm is calculating the distances between
    observations—​often using Euclidean distance:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: k最近邻算法是需要数值数据的一个例子。算法中的一步是计算观测之间的距离，通常使用欧氏距离：
- en: <math display="block"><msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></math>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></math>
- en: where <math display="inline"><mi>x</mi></math> and <math display="inline"><mi>y</mi></math>
    are two observations and subscript <math display="inline"><mi>i</mi></math> denotes
    the value for the observations’ <math display="inline"><mi>i</mi></math>th feature.
    However, the distance calculation obviously is impossible if the value of <math
    display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> is a string (e.g., `"Texas"`).
    Instead, we need to convert the string into some numerical format so that it can
    be input into the Euclidean distance equation. Our goal is to transform the data
    in a way that properly captures the information in the categories (ordinality,
    relative intervals between categories, etc.). In this chapter we will cover techniques
    for making this transformation as well as overcoming other challenges often encountered
    when handling categorical data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><mi>x</mi></math>和<math display="inline"><mi>y</mi></math>是两个观测值，下标<math
    display="inline"><mi>i</mi></math>表示观测的第<math display="inline"><mi>i</mi></math>个特征的值。然而，如果<math
    display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>的值是一个字符串（例如`"Texas"`），显然是无法进行距离计算的。我们需要将字符串转换为某种数值格式，以便可以将其输入到欧氏距离方程中。我们的目标是以一种能够正确捕捉类别信息（序数性，类别之间的相对间隔等）的方式转换数据。在本章中，我们将涵盖使这种转换以及克服处理分类数据时经常遇到的其他挑战的技术。
- en: 5.1 Encoding Nominal Categorical Features
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.1 编码名义分类特征
- en: Problem
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a feature with nominal classes that has no intrinsic ordering (e.g.,
    apple, pear, banana), and you want to encode the feature into numerical values.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个没有内在排序的名义类别特征（例如苹果，梨，香蕉），并且希望将该特征编码为数值。
- en: Solution
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'One-hot encode the feature using scikit-learn’s `LabelBinarizer`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn的`LabelBinarizer`对特征进行独热编码：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can use the `classes_` attribute to output the classes:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`classes_`属性来输出类别：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we want to reverse the one-hot encoding, we can use `inverse_transform`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要反向进行独热编码，我们可以使用`inverse_transform`：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can even use pandas to one-hot encode the feature:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以使用pandas来进行独热编码：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|  | California | Delaware | Texas |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | 加利福尼亚 | 特拉华州 | 德克萨斯州 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 0 | 0 | 1 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 1 |'
- en: '| 1 | 1 | 0 | 0 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 | 0 |'
- en: '| 2 | 0 | 0 | 1 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 0 | 1 |'
- en: '| 3 | 0 | 1 | 0 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 1 | 0 |'
- en: '| 4 | 0 | 0 | 1 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 | 0 | 1 |'
- en: 'One helpful feature of scikit-learn is the ability to handle a situation where
    each observation lists multiple classes:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的一个有用特性是能够处理每个观测列表包含多个类别的情况：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once again, we can see the classes with the `classes_` method:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以使用`classes_`方法查看类别：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Discussion
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: We might think the proper strategy is to assign each class a numerical value
    (e.g., Texas = 1, California = 2). However, when our classes have no intrinsic
    ordering (e.g., Texas isn’t “less” than California), our numerical values erroneously
    create an ordering that is not present.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能认为正确的策略是为每个类分配一个数值（例如，Texas = 1，California = 2）。然而，当我们的类没有内在的顺序（例如，Texas
    不是比 California “更少”），我们的数值值误创建了一个不存在的排序。
- en: The proper strategy is to create a binary feature for each class in the original
    feature. This is often called *one-hot encoding* (in machine learning literature)
    or *dummying* (in statistical and research literature). Our solution’s feature
    was a vector containing three classes (i.e., Texas, California, and Delaware).
    In one-hot encoding, each class becomes its own feature with 1s when the class
    appears and 0s otherwise. Because our feature had three classes, one-hot encoding
    returned three binary features (one for each class). By using one-hot encoding
    we can capture the membership of an observation in a class while preserving the
    notion that the class lacks any sort of hierarchy.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 适当的策略是为原始特征的每个类创建一个二进制特征。在机器学习文献中通常称为 *独热编码*，而在统计和研究文献中称为 *虚拟化*。我们解决方案的特征是一个包含三个类（即
    Texas、California 和 Delaware）的向量。在独热编码中，每个类都成为其自己的特征，当类出现时为 1，否则为 0。因为我们的特征有三个类，独热编码返回了三个二进制特征（每个类一个）。通过使用独热编码，我们可以捕捉观察值在类中的成员身份，同时保持类缺乏任何层次结构的概念。
- en: Finally, it is often recommended that after one-hot encoding a feature, we drop
    one of the one-hot encoded features in the resulting matrix to avoid linear dependence.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，经常建议在对一个特征进行独热编码后，删除结果矩阵中的一个独热编码特征，以避免线性相关性。
- en: See Also
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Dummy Variable Trap in Regression Models, Algosome](https://oreil.ly/xjBhG)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[回归模型中的虚拟变量陷阱，Algosome](https://oreil.ly/xjBhG)'
- en: '[Dropping one of the columns when using one-hot encoding, Cross Validated](https://oreil.ly/CTdpG)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用独热编码时删除其中一列，Cross Validated](https://oreil.ly/CTdpG)'
- en: 5.2 Encoding Ordinal Categorical Features
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.2 编码序数分类特征
- en: Problem
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have an ordinal categorical feature (e.g., high, medium, low), and you want
    to transform it into numerical values.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个序数分类特征（例如高、中、低），并且希望将其转换为数值。
- en: Solution
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the pandas DataFrame `replace` method to transform string labels to numerical
    equivalents:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas DataFrame 的 `replace` 方法将字符串标签转换为数值等价物：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Discussion
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Often we have a feature with classes that have some kind of natural ordering.
    A famous example is the Likert scale:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 经常情况下，我们有一个具有某种自然顺序的类的特征。一个著名的例子是 Likert 量表：
- en: Strongly Agree
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强烈同意
- en: Agree
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同意
- en: Neutral
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中立
- en: Disagree
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同意
- en: Strongly Disagree
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强烈不同意
- en: When encoding the feature for use in machine learning, we need to transform
    the ordinal classes into numerical values that maintain the notion of ordering.
    The most common approach is to create a dictionary that maps the string label
    of the class to a number and then apply that map to the feature.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在将特征编码用于机器学习时，我们需要将序数类转换为保持排序概念的数值。最常见的方法是创建一个将类的字符串标签映射到数字的字典，然后将该映射应用于特征。
- en: 'It is important that our choice of numeric values is based on our prior information
    on the ordinal classes. In our solution, `high` is literally three times larger
    than `low`. This is fine in many instances but can break down if the assumed intervals
    between the classes are not equal:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对序数类的先前信息，选择数值值是很重要的。在我们的解决方案中，`high` 比 `low` 大三倍。在许多情况下这是可以接受的，但如果假设的类之间间隔不均等，这种方法可能失效：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In this example, the distance between `Low` and `Medium` is the same as the
    distance between `Medium` and `Barely More Than Medium`, which is almost certainly
    not accurate. The best approach is to be conscious about the numerical values
    mapped to classes:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，`Low` 和 `Medium` 之间的距离与 `Medium` 和 `Barely More Than Medium` 之间的距离相同，这几乎肯定不准确。最佳方法是在映射到类的数值值时要注意：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 5.3 Encoding Dictionaries of Features
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.3 编码特征字典
- en: Problem
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a dictionary and want to convert it into a feature matrix.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个字典，并希望将其转换为特征矩阵。
- en: Solution
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use `DictVectorizer`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `DictVectorizer`：
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: By default `DictVectorizer` outputs a sparse matrix that only stores elements
    with a value other than 0\. This can be very helpful when we have massive matrices
    (often encountered in natural language processing) and want to minimize the memory
    requirements. We can force `DictVectorizer` to output a dense matrix using `sparse=False`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`DictVectorizer`输出一个仅存储值非0的稀疏矩阵。当我们遇到大规模矩阵（通常在自然语言处理中）并希望最小化内存需求时，这非常有帮助。我们可以使用`sparse=False`来强制`DictVectorizer`输出一个密集矩阵。
- en: 'We can get the names of each generated feature using the `get_feature_names`
    method:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`get_feature_names`方法获取每个生成特征的名称：
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'While not necessary, for the sake of illustration we can create a pandas DataFrame
    to view the output better:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不必要，为了说明我们可以创建一个pandas DataFrame来更好地查看输出：
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '|  | Blue | Red | Yellow |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | 蓝色 | 红色 | 黄色 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 4.0 | 2.0 | 0.0 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 4.0 | 2.0 | 0.0 |'
- en: '| 1 | 3.0 | 4.0 | 0.0 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 3.0 | 4.0 | 0.0 |'
- en: '| 2 | 0.0 | 1.0 | 2.0 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.0 | 1.0 | 2.0 |'
- en: '| 3 | 0.0 | 2.0 | 2.0 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.0 | 2.0 | 2.0 |'
- en: Discussion
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: A dictionary is a popular data structure used by many programming languages;
    however, machine learning algorithms expect the data to be in the form of a matrix.
    We can accomplish this using scikit-learn’s `DictVectorizer`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 字典是许多编程语言中常用的数据结构；然而，机器学习算法期望数据以矩阵的形式存在。我们可以使用scikit-learn的`DictVectorizer`来实现这一点。
- en: 'This is a common situation when working with natural language processing. For
    example, we might have a collection of documents and for each document we have
    a dictionary containing the number of times every word appears in the document.
    Using `DictVectorizer`, we can easily create a feature matrix where every feature
    is the number of times a word appears in each document:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自然语言处理时常见的情况。例如，我们可能有一系列文档，每个文档都有一个字典，其中包含每个单词在文档中出现的次数。使用`DictVectorizer`，我们可以轻松创建一个特征矩阵，其中每个特征是每个文档中单词出现的次数：
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In our toy example there are only three unique words (`Red`, `Yellow`, `Blue`)
    so there are only three features in our matrix; however, you can imagine that
    if each document was actually a book in a university library our feature matrix
    would be very large (and then we would want to set `sparse` to `True`).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，只有三个唯一的单词（`红色`，`黄色`，`蓝色`），所以我们的矩阵中只有三个特征；然而，如果每个文档实际上是大学图书馆中的一本书，我们的特征矩阵将非常庞大（然后我们将希望将`sparse`设置为`True`）。
- en: See Also
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[How to Create Dictionaries in Python](https://oreil.ly/zu5hU)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在Python中创建字典](https://oreil.ly/zu5hU)'
- en: '[SciPy Sparse Matrices](https://oreil.ly/5nAsU)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SciPy稀疏矩阵](https://oreil.ly/5nAsU)'
- en: 5.4 Imputing Missing Class Values
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.4 填充缺失的类值
- en: Problem
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a categorical feature containing missing values that you want to replace
    with predicted values.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个包含缺失值的分类特征，您希望用预测值替换它。
- en: Solution
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'The ideal solution is to train a machine learning classifier algorithm to predict
    the missing values, commonly a k-nearest neighbors (KNN) classifier:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的解决方案是训练一个机器学习分类器算法来预测缺失值，通常是k近邻（KNN）分类器：
- en: '[PRE24]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'An alternative solution is to fill in missing values with the feature’s most
    frequent value:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决方案是使用特征的最频繁值填充缺失值：
- en: '[PRE26]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Discussion
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: When we have missing values in a categorical feature, our best solution is to
    open our toolbox of machine learning algorithms to predict the values of the missing
    observations. We can accomplish this by treating the feature with the missing
    values as the target vector and the other features as the feature matrix. A commonly
    used algorithm is KNN (discussed in depth in [Chapter 15](ch15.xhtml#k-nearest-neighbors)),
    which assigns to the missing value the most frequent class of the *k* nearest
    observations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当分类特征中存在缺失值时，我们最好的解决方案是打开我们的机器学习算法工具箱，预测缺失观测值的值。我们可以通过将具有缺失值的特征视为目标向量，其他特征视为特征矩阵来实现此目标。常用的算法之一是KNN（在[第15章](ch15.xhtml#k-nearest-neighbors)中详细讨论），它将缺失值分配给*k*个最近观测中最频繁出现的类别。
- en: Alternatively, we can fill in missing values with the most frequent class of
    the feature or even discard the observations with missing values. While less sophisticated
    than KNN, these options are much more scalable to larger data. In any case, it
    is advisable to include a binary feature indicating which observations contain
    imputed values.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用特征的最频繁类别填充缺失值，甚至丢弃具有缺失值的观测。虽然不如KNN复杂，但这些选项在处理大数据时更具可扩展性。无论哪种情况，都建议包含一个二元特征，指示哪些观测包含了填充值。
- en: See Also
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation: Imputation of Missing Values](https://oreil.ly/joZ6J)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn 文档：缺失值的插补](https://oreil.ly/joZ6J)'
- en: '[Overcoming Missing Values in a Random Forest Classifier](https://oreil.ly/TcvOf)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在随机森林分类器中克服缺失值](https://oreil.ly/TcvOf)'
- en: '[A Study of K-Nearest Neighbour as an Imputation Method](https://oreil.ly/kDFEC)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[K 最近邻方法作为插补方法的研究](https://oreil.ly/kDFEC)'
- en: 5.5 Handling Imbalanced Classes
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.5 处理不平衡类别
- en: Problem
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a target vector with highly imbalanced classes, and you want to make
    adjustments so that you can handle the class imbalance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个具有高度不平衡类别的目标向量，并且希望进行调整以处理类别不平衡。
- en: Solution
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Collect more data. If that isn’t possible, change the metrics used to evaluate
    your model. If that doesn’t work, consider using a model’s built-in class weight
    parameters (if available), downsampling, or upsampling. We cover evaluation metrics
    in a later chapter, so for now let’s focus on class weight parameters, downsampling,
    and upsampling.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 收集更多数据。如果不可能，请更改用于评估模型的指标。如果这样做不起作用，请考虑使用模型的内置类权重参数（如果可用），下采样或上采样。我们将在后面的章节中介绍评估指标，因此现在让我们专注于类权重参数、下采样和上采样。
- en: 'To demonstrate our solutions, we need to create some data with imbalanced classes.
    Fisher’s Iris dataset contains three balanced classes of 50 observations, each
    indicating the species of flower (*Iris setosa*, *Iris virginica*, and *Iris versicolor*).
    To unbalance the dataset, we remove 40 of the 50 *Iris setosa* observations and
    then merge the *Iris virginica* and *Iris versicolor* classes. The end result
    is a binary target vector indicating if an observation is an *Iris setosa* flower
    or not. The result is 10 observations of *Iris setosa* (class 0) and 100 observations
    of not *Iris setosa* (class 1):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示我们的解决方案，我们需要创建一些具有不平衡类别的数据。Fisher 的鸢尾花数据集包含三个平衡类别的50个观察，每个类别表示花的物种（*Iris
    setosa*、*Iris virginica* 和 *Iris versicolor*）。为了使数据集不平衡，我们移除了50个 *Iris setosa*
    观察中的40个，并合并了 *Iris virginica* 和 *Iris versicolor* 类别。最终结果是一个二元目标向量，指示观察是否为 *Iris
    setosa* 花。结果是10个 *Iris setosa*（类别 0）的观察和100个非 *Iris setosa*（类别 1）的观察：
- en: '[PRE28]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Many algorithms in scikit-learn offer a parameter to weight classes during
    training to counteract the effect of their imbalance. While we have not covered
    it yet, `RandomForestClassifier` is a popular classification algorithm and includes
    a `class_weight` parameter; learn more about the `RandomForestClassifier` in [Recipe
    14.4](ch14.xhtml#training-a-random-forest-classifier). You can pass an argument
    explicitly specifying the desired class weights:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 中的许多算法在训练期间提供一个参数来加权类别，以抵消其不平衡的效果。虽然我们尚未涵盖它，`RandomForestClassifier`
    是一种流行的分类算法，并包含一个 `class_weight` 参数；在 [14.4 节](ch14.xhtml#training-a-random-forest-classifier)
    中了解更多关于 `RandomForestClassifier` 的信息。您可以传递一个参数显式指定所需的类权重：
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Or you can pass `balanced`, which automatically creates weights inversely proportional
    to class frequencies:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 或者您可以传递 `balanced`，它会自动创建与类别频率成反比的权重：
- en: '[PRE32]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Alternatively, we can downsample the majority class or upsample the minority
    class. In *downsampling*, we randomly sample without replacement from the majority
    class (i.e., the class with more observations) to create a new subset of observations
    equal in size to the minority class. For example, if the minority class has 10
    observations, we will randomly select 10 observations from the majority class
    and use those 20 observations as our data. Here we do exactly that using our unbalanced
    iris data:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以对多数类进行下采样或者对少数类进行上采样。在 *下采样* 中，我们从多数类中无放回随机抽样（即观察次数较多的类别）以创建一个新的观察子集，其大小等于少数类。例如，如果少数类有10个观察，我们将从多数类中随机选择10个观察，然后使用这20个观察作为我们的数据。在这里，我们正是利用我们不平衡的鸢尾花数据做到这一点：
- en: '[PRE34]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Our other option is to upsample the minority class. In *upsampling*, for every
    observation in the majority class, we randomly select an observation from the
    minority class with replacement. The result is the same number of observations
    from the minority and majority classes. Upsampling is implemented very similarly
    to downsampling, just in reverse:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的另一种选择是对少数类进行上采样。在 *上采样* 中，对于多数类中的每个观察，我们从少数类中随机选择一个观察，可以重复选择。结果是来自少数和多数类的相同数量的观察。上采样的实现非常类似于下采样，只是反向操作：
- en: '[PRE38]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Discussion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In the real world, imbalanced classes are everywhere—​most visitors don’t click
    the buy button, and many types of cancer are thankfully rare. For this reason,
    handling imbalanced classes is a common activity in machine learning.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，不平衡的类别随处可见—大多数访问者不会点击购买按钮，而许多类型的癌症又是相当罕见的。因此，在机器学习中处理不平衡的类别是一项常见的活动。
- en: Our best strategy is simply to collect more observations—​especially observations
    from the minority class. However, often this is just not possible, so we have
    to resort to other options.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最好的策略就是简单地收集更多的观察数据—尤其是来自少数类的观察数据。然而，通常情况下这并不可能，所以我们必须求助于其他选择。
- en: A second strategy is to use a model evaluation metric better suited to imbalanced
    classes. Accuracy is often used as a metric for evaluating the performance of
    a model, but when imbalanced classes are present, accuracy can be ill suited.
    For example, if only 0.5% of observations have some rare cancer, then even a naive
    model that predicts nobody has cancer will be 99.5% accurate. Clearly this is
    not ideal. Some better metrics we discuss in later chapters are confusion matrices,
    precision, recall, *F[1]* scores, and ROC curves.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种策略是使用更适合于不平衡类别的模型评估指标。准确率通常被用作评估模型性能的指标，但在存在不平衡类别的情况下，准确率可能并不合适。例如，如果只有0.5%的观察数据属于某种罕见的癌症，那么即使是一个简单的模型预测没有人有癌症，准确率也会达到99.5%。显然，这并不理想。我们将在后面的章节中讨论一些更好的指标，如混淆矩阵、精确度、召回率、*F[1]*分数和ROC曲线。
- en: A third strategy is to use the class weighing parameters included in implementations
    of some models. This allows the algorithm to adjust for imbalanced classes. Fortunately,
    many scikit-learn classifiers have a `class_weight` parameter, making it a good
    option.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种策略是使用一些模型实现中包含的类别加权参数。这使得算法能够调整不平衡的类别。幸运的是，许多scikit-learn分类器都有一个`class_weight`参数，这使得它成为一个不错的选择。
- en: 'The fourth and fifth strategies are related: downsampling and upsampling. In
    downsampling we create a random subset of the majority class of equal size to
    the minority class. In upsampling we repeatedly sample with replacement from the
    minority class to make it of equal size as the majority class. The decision between
    using downsampling and upsampling is context-specific, and in general we should
    try both to see which produces better results.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 第四和第五种策略是相关的：下采样和上采样。在下采样中，我们创建一个与少数类相同大小的多数类的随机子集。在上采样中，我们从少数类中重复有放回地抽样，使其大小与多数类相等。选择使用下采样还是上采样是与上下文相关的决定，通常我们应该尝试两种方法，看看哪一种效果更好。
