- en: Chapter 17\. Support Vector Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第17章 支持向量机
- en: 17.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17.0 引言
- en: To understand support vector machines, we must understand hyperplanes. Formally,
    a *hyperplane* is an *n – 1* subspace in an *n*-dimensional space. While that
    sounds complex, it actually is pretty simple. For example, if we wanted to divide
    a two-dimensional space, we’d use a one-dimensional hyperplane (i.e., a line).
    If we wanted to divide a three-dimensional space, we’d use a two-dimensional hyperplane
    (i.e., a flat piece of paper or a bed sheet). A hyperplane is simply a generalization
    of that concept into *n* dimensions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解支持向量机，我们必须了解超平面。形式上，*超平面*是*n - 1*维空间中的一个* n * -维子空间。尽管听起来复杂，但实际上相当简单。例如，如果我们想要划分一个二维空间，我们会使用一维超平面（即，一条线）。如果我们想要划分一个三维空间，我们会使用二维超平面（即，一张平面或一张床单）。超平面只是将该概念推广到*n*维空间的一种方式。
- en: '*Support vector machines* classify data by finding the hyperplane that maximizes
    the margin between the classes in the training data. In a two-dimensional example
    with two classes, we can think of a hyperplane as the widest straight “band” (i.e.,
    line with margins) that separates the two classes.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*支持向量机*通过找到在训练数据中最大化类之间间隔的超平面来对数据进行分类。在一个二维示例中，我们可以将超平面看作是分开两个类的最宽的直线“带”（即，具有间隔的线）。'
- en: In this chapter, we cover training support vector machines in a variety of situations
    and dive under the hood to look at how we can extend the approach to tackle common
    problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖在各种情况下训练支持向量机，并深入了解如何扩展该方法以解决常见问题。
- en: 17.1 Training a Linear Classifier
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17.1 训练线性分类器
- en: Problem
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to train a model to classify observations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要训练一个模型来对观察结果进行分类。
- en: Solution
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use a *support vector classifier* (SVC) to find the hyperplane that maximizes
    the margins between the classes:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*支持向量分类器*（SVC）找到最大化类之间间隔的超平面：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Discussion
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'scikit-learn’s `LinearSVC` implements a simple SVC. To get an intuition behind
    what an SVC is doing, let’s plot out the data and hyperplane. While SVCs work
    well in high dimensions, in our solution we loaded only two features and took
    a subset of observations so that the data contains only two classes. This will
    let us visualize the model. Recall that SVC attempts to find the hyperplane—​a
    line when we have only two dimensions—​with the maximum margin between the classes.
    In the following code we plot the two classes on a two-dimensional space, then
    draw the hyperplane:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的`LinearSVC`实现了一个简单的SVC。为了理解SVC正在做什么，让我们绘制出数据和超平面的图像。虽然SVC在高维度下工作得很好，但在我们的解决方案中，我们只加载了两个特征并取了一部分观察结果，使得数据只包含两个类。这将让我们可以可视化模型。回想一下，当我们只有两个维度时，SVC试图找到具有最大间隔的超平面—​一条线—​来分离类。在下面的代码中，我们在二维空间中绘制了两个类，然后绘制了超平面：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![mpc2 17in01](assets/mpc2_17in01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 17in01](assets/mpc2_17in01.png)'
- en: 'In this visualization, all observations of class 0 are black and observations
    of class 1 are light gray. The hyperplane is the decision boundary deciding how
    new observations are classified. Specifically, any observation above the line
    will by classified as class 0, while any observation below the line will be classified
    as class 1\. We can prove this by creating a new observation in the top-left corner
    of our visualization, meaning it should be predicted to be class 0:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个可视化中，所有类0的观察结果都是黑色的，而类1的观察结果是浅灰色的。超平面是决策边界，决定了如何对新的观察结果进行分类。具体来说，线上方的任何观察结果将被分类为类0，而线下方的任何观察结果将被分类为类1。我们可以通过在可视化的左上角创建一个新的观察结果来证明这一点，这意味着它应该被预测为类0：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There are a few things to note about SVCs. First, for the sake of visualization,
    we limited our example to a binary example (i.e., only two classes); however,
    SVCs can work well with multiple classes. Second, as our visualization shows,
    the hyperplane is by definition linear (i.e., not curved). This was okay in this
    example because the data was linearly separable, meaning there was a hyperplane
    that could perfectly separate the two classes. Unfortunately, in the real world
    this is rarely the case.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SVC有几点需要注意。首先，为了可视化的目的，我们将示例限制为二元示例（即，只有两个类）；但是，SVC可以很好地处理多类问题。其次，正如我们的可视化所示，超平面在定义上是线性的（即，不是曲线的）。在这个示例中这是可以的，因为数据是线性可分的，意味着有一个能够完美分离两个类的超平面。不幸的是，在现实世界中，这种情况很少见。
- en: More typically, we will not be able to perfectly separate classes. In these
    situations there is a balance between SVC maximizing the margin of the hyperplane
    and minimizing the misclassification. In SVC, the latter is controlled with the
    hyperparameter *C*. *C* is a parameter of the SVC learner and is the penalty for
    misclassifying a data point. When *C* is small, the classifier is okay with misclassified
    data points (high bias but low variance). When *C* is large, the classifier is
    heavily penalized for misclassified data and therefore bends over backward to
    avoid any misclassified data points (low bias but high variance).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 更典型地，我们将无法完全分开类别。在这些情况下，支持向量分类器在最大化超平面间隔和最小化误分类之间存在平衡。在 SVC 中，后者由超参数 *C* 控制。*C*
    是 SVC 学习器的参数，是对误分类数据点的惩罚。当 *C* 较小时，分类器可以接受误分类的数据点（高偏差但低方差）。当 *C* 较大时，分类器对误分类数据点进行严格惩罚，因此竭尽全力避免任何误分类数据点（低偏差但高方差）。
- en: In scikit-learn, *C* is determined by the parameter `C` and defaults to `C=1.0`.
    We should treat *C* has a hyperparameter of our learning algorithm, which we tune
    using model selection techniques in [Chapter 12](ch12.xhtml#model-selection).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，*C* 是由参数 `C` 确定的，默认为 `C=1.0`。我们应该将 *C* 视为我们学习算法的超参数，通过模型选择技术在
    [第12章](ch12.xhtml#model-selection) 中进行调优。
- en: 17.2 Handling Linearly Inseparable Classes Using Kernels
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17.2 使用核处理线性不可分类
- en: Problem
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to train a support vector classifier, but your classes are linearly
    inseparable.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要训练一个支持向量分类器，但你的类别是线性不可分的。
- en: Solution
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Train an extension of a support vector machine using kernel functions to create
    nonlinear decision boundaries:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用核函数训练支持向量机的扩展，以创建非线性决策边界：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Discussion
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'A full explanation of support vector machines is outside the scope of this
    book. However, a short explanation is likely beneficial for understanding support
    vector machines and kernels. For reasons best learned elsewhere, a support vector
    classifier can be represented as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对支持向量机的全面解释超出了本书的范围。但是，简短的解释可能有助于理解支持向量机和核。出于最好在其他地方学习的原因，支持向量分类器可以表示为：
- en: <math display="block"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <munder><mo>∑</mo> <mrow><mi>i</mi><mi>ϵ</mi><mi>S</mi></mrow></munder>
    <msub><mi>α</mi> <mi>i</mi></msub> <mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <munder><mo>∑</mo> <mrow><mi>i</mi><mi>ϵ</mi><mi>S</mi></mrow></munder>
    <msub><mi>α</mi> <mi>i</mi></msub> <mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
- en: 'where <math display="inline"><msub><mi>β</mi><mn>0</mn></msub></math> is the
    bias, <math display="inline"><mi>S</mi></math> is the set of all support vector
    observations, <math display="inline"><mi>α</mi></math> is the model parameters
    to be learned, and <math display="inline"><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>''</mo></mrow></msub> <mo>)</mo></math>
    are pairs of two support vector observations, <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    and <math display="inline"><msub><mi>x</mi> <mrow><mi>i</mi> <mo>''</mo></mrow></msub></math>
    . Most importantly, <math display="inline"><mi>K</mi></math> is a kernel function
    that compares the similarity between <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    and <math display="inline"><msub><mi>x</mi> <mrow><mi>i</mi> <mo>''</mo></mrow></msub></math>
    . Don’t worry if you don’t understand kernel functions. For our purposes, just
    realize that (1) <math display="inline"><mi>K</mi></math> determines the type
    of hyperplane used to separate our classes, and (2) we create different hyperplanes
    by using different kernels. For example, if we want a basic linear hyperplane
    like the one we created in [Recipe 17.1](#training-a-linear-classifier), we can
    use the linear kernel:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><msub><mi>β</mi><mn>0</mn></msub></math> 是偏差，<math
    display="inline"><mi>S</mi></math> 是所有支持向量观测的集合，<math display="inline"><mi>α</mi></math>
    是待学习的模型参数，<math display="inline"><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub> <mo>)</mo></math>
    是两个支持向量观测 <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> 和 <math
    display="inline"><msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub></math>
    的对。最重要的是，<math display="inline"><mi>K</mi></math> 是一个核函数，用于比较 <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    和 <math display="inline"><msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub></math>
    之间的相似性。如果你不理解核函数也不用担心。对于我们的目的，只需意识到：（1）<math display="inline"><mi>K</mi></math>
    决定了用于分离我们类别的超平面类型，（2）我们通过使用不同的核函数创建不同的超平面。例如，如果我们想要一个类似于我们在 [配方 17.1](#training-a-linear-classifier)
    中创建的基本线性超平面，我们可以使用线性核：
- en: <math display="block"><mrow><mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <msub><mi>x</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> <mi>j</mi></mrow></msub></mrow></math>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <msub><mi>x</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo> <mi>j</mi></mrow></msub></mrow></math>
- en: 'where <math display="inline"><mi>p</mi></math> is the number of features. However,
    if we want a nonlinear decision boundary, we swap the linear kernel with a polynomial
    kernel:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><mi>p</mi></math> 是特征数。然而，如果我们想要一个非线性决策边界，我们可以将线性核替换为多项式核：
- en: <math display="block"><mrow><mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mrow><mo>(</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi></msubsup> <msub><mi>x</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo>
    <mi>j</mi></mrow></msub> <mo>)</mo></mrow> <mi>d</mi></msup></mrow></math>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mrow><mo>(</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi></msubsup> <msub><mi>x</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo>
    <mi>j</mi></mrow></msub> <mo>)</mo></mrow> <mi>d</mi></msup></mrow></math>
- en: 'where <math display="inline"><mi>d</mi></math> is the degree of the polynomial
    kernel function. Alternatively, we can use one of the most common kernels in support
    vectors machines, the *radial basis function kernel*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><mi>d</mi></math>是多项式核函数的阶数。或者，我们可以使用支持向量机中最常见的核函数之一，*径向基函数核*：
- en: <math display="block"><mrow><mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mi>e</mi> <mrow><mo>(</mo><mo>-</mo><mi>γ</mi><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo>
    <mi>j</mi></mrow></msub> <mo>)</mo></mrow> <mn>2</mn></msup> <mo>)</mo></mrow></msup></mrow></math>
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>K</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mi>e</mi> <mrow><mo>(</mo><mo>-</mo><mi>γ</mi><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>'</mo>
    <mi>j</mi></mrow></msub> <mo>)</mo></mrow> <mn>2</mn></msup> <mo>)</mo></mrow></msup></mrow></math>
- en: where <math display="inline"><mi>γ</mi></math> is a hyperparameter and must
    be greater than zero. The main point of the preceding explanation is that if we
    have linearly inseparable data, we can swap out a linear kernel with an alternative
    kernel to create a nonlinear hyperplane decision boundary.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><mi>γ</mi></math>是一个超参数，必须大于零。上述解释的主要观点是，如果我们有线性不可分的数据，我们可以用替代核函数替换线性核函数，从而创建非线性超平面决策边界。
- en: 'We can understand the intuition behind kernels by visualizing a simple example.
    This function, based on one by Sebastian Raschka, plots the observations and decision
    boundary hyperplane of a two-dimensional space. You do not need to understand
    how this function works; I have included it here so you can experiment on your
    own:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过可视化一个简单的例子来理解核函数的直觉。这个函数基于Sebastian Raschka的一个函数，绘制了二维空间的观测和决策边界超平面。您不需要理解这个函数的工作原理；我在这里包含它，以便您自己进行实验：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In our solution, we have data containing two features (i.e., two dimensions)
    and a target vector with the class of each observation. Importantly, the classes
    are assigned such that they are *linearly inseparable*. That is, there is no straight
    line we can draw that will divide the two classes. First, let’s create a support
    vector machine classifier with a linear kernel:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，我们有包含两个特征（即两个维度）和一个目标向量的数据。重要的是，这些类别被分配得*线性不可分*。也就是说，我们无法画出一条直线来分隔这两类数据。首先，让我们创建一个带有线性核函数的支持向量机分类器：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, since we have only two features, we are working in a two-dimensional
    space and can visualize the observations, their classes, and our model’s linear
    hyperplane:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，由于我们只有两个特征，我们是在二维空间中工作，可以可视化观测、它们的类别以及我们模型的线性超平面：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![mpc2 17in02](assets/mpc2_17in02.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 17in02](assets/mpc2_17in02.png)'
- en: 'As we can see, our linear hyperplane did very poorly at dividing the two classes!
    Now, let’s swap out the linear kernel with a radial basis function kernel and
    use it to train a new model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的线性超平面在分隔这两类数据时表现非常糟糕！现在，让我们将线性核函数替换为径向基函数核，并用它来训练一个新模型：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And then visualize the observations and hyperplane:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可视化观测和超平面：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![mpc2 17in03](assets/mpc2_17in03.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![mpc2 17in03](assets/mpc2_17in03.png)'
- en: By using the radial basis function kernel we can create a decision boundary
    that is able to do a much better job of separating the two classes than the linear
    kernel. This is the motivation behind using kernels in support vector machines.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用径向基函数核，我们可以创建一个决策边界，它能够比线性核函数更好地分离这两类数据。这就是支持向量机中使用核函数的动机。
- en: In scikit-learn, we can select the kernel we want to use by using the `kernel`
    parameter. Once we select a kernel, we need to specify the appropriate kernel
    options, such as the value of *d* (using the `degree` parameter) in polynomial
    kernels, and the value of γ (using the `gamma` parameter) in radial basis function
    kernels. We will also need to set the penalty parameter, `C`. When training the
    model, in most cases we should treat all of these as hyperparameters and use model
    selection techniques to identify the combination of their values that produces
    the model with the best performance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，我们可以通过使用`kernel`参数来选择要使用的核函数。选择核函数后，我们需要指定适当的核选项，如多项式核中的*阶数*（使用`degree`参数）和径向基函数核中的*γ*值（使用`gamma`参数）。我们还需要设置惩罚参数`C`。在训练模型时，大多数情况下，我们应该将所有这些视为超参数，并使用模型选择技术来确定它们值的组合，以生成性能最佳的模型。
- en: 17.3 Creating Predicted Probabilities
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17.3 创建预测概率
- en: Problem
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to know the predicted class probabilities for an observation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要知道观测的预测类别概率。
- en: Solution
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'When using scikit-learn’s `SVC`, set `probability=True`, train the model, then
    use `predict_proba` to see the calibrated probabilities:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 scikit-learn 的`SVC`时，设置`probability=True`，训练模型，然后使用`predict_proba`来查看校准概率：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Discussion
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Many of the supervised learning algorithms we have covered use probability
    estimates to predict classes. For example, in k-nearest neighbors, an observation’s
    *k* neighbor’s classes were treated as votes to create a probability that an observation
    was of that class. Then the class with the highest probability was predicted.
    SVC’s use of a hyperplane to create decision regions does not naturally output
    a probability estimate that an observation is a member of a certain class. However,
    we can in fact output calibrated class probabilities with a few caveats. In an
    SVC with two classes, *Platt scaling* can be used, wherein first the SVC is trained,
    and then a separate cross-validated logistic regression is trained to map the
    SVC outputs into probabilities:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论过的许多监督学习算法使用概率估计来预测类别。例如，在k最近邻算法中，一个观测的*k*个邻居的类别被视为投票，以创建该观测属于该类别的概率。然后预测具有最高概率的类别。支持向量机使用超平面来创建决策区域，并不会自然输出一个观测属于某个类别的概率估计。但是，事实上我们可以在一些条件下输出校准的类别概率。在具有两个类别的支持向量机中，可以使用*Platt缩放*，首先训练支持向量机，然后训练一个单独的交叉验证逻辑回归，将支持向量机的输出映射到概率：
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo>
    <mn>1</mn> <mo>∣</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>(</mo><mi>A</mi><mo>×</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>B</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo>
    <mn>1</mn> <mo>∣</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>(</mo><mi>A</mi><mo>×</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>B</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
- en: where <math display="inline"><mi>A</mi></math> and <math display="inline"><mi>B</mi></math>
    are parameter vectors, and <math display="inline"><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></math>
    is the <math display="inline"><mi>i</mi></math>th observation’s signed distance
    from the hyperplane. When we have more than two classes, an extension of Platt
    scaling is used.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><mi>A</mi></math>和<math display="inline"><mi>B</mi></math>是参数向量，<math
    display="inline"><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></math>是第<math display="inline"><mi>i</mi></math>个观测到超平面的有符号距离。当我们有超过两个类别时，会使用Platt缩放的扩展。
- en: In more practical terms, creating predicted probabilities has two major issues.
    First, because we are training a second model with cross-validation, generating
    predicted probabilities can significantly increase the time it takes to train
    our model. Second, because the predicted probabilities are created using cross-validation,
    they might not always match the predicted classes. That is, an observation might
    be predicted to be class 1 but have a predicted probability of being class 1 of
    less than 0.5.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 更实际地说，创建预测概率有两个主要问题。首先，因为我们使用交叉验证训练第二个模型，生成预测概率可能会显著增加训练模型的时间。其次，因为预测概率是使用交叉验证创建的，它们可能并不总是与预测类别匹配。也就是说，一个观测可能被预测为类别1，但其预测的概率小于0.5。
- en: In scikit-learn, the predicted probabilities must be generated when the model
    is being trained. We can do this by setting `SVC`’s `probability` to `True`. After
    the model is trained, we can output the estimated probabilities for each class
    using `predict_proba`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，必须在训练模型时生成预测概率。我们可以通过将`SVC`的`probability`设置为`True`来实现这一点。模型训练完成后，可以使用`predict_proba`输出每个类别的估计概率。
- en: 17.4 Identifying Support Vectors
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17.4 识别支持向量
- en: Problem
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to identify which observations are the support vectors of the decision
    hyperplane.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 需要确定哪些观测是决策超平面的支持向量。
- en: Solution
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Train the model, then use `support_vectors_`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型，然后使用`support_vectors_`：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Discussion
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Support vector machines get their name from the fact that the hyperplane is
    being determined by a relatively small number of observations, called the *support
    vectors*. Intuitively, think of the hyperplane as being “carried” by these support
    vectors. These support vectors are therefore very important to our model. For
    example, if we remove an observation that is not a support vector from the data,
    the model does not change; however, if we remove a support vector, the hyperplane
    will not have the maximum margin.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机得名于这个事实：决策超平面由相对较少的被称为*支持向量*的观测决定。直观地说，可以将超平面看作由这些支持向量“支持”。因此，这些支持向量对我们的模型非常重要。例如，如果从数据中删除一个非支持向量的观测，模型不会改变；但是，如果删除一个支持向量，超平面将不会具有最大间隔。
- en: 'After we have trained an SVC, scikit-learn offers a number of options for identifying
    the support vector. In our solution, we used `support_vectors_` to output the
    actual observations’ features of the four support vectors in our model. Alternatively,
    we can view the indices of the support vectors using `support_`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练了支持向量机之后，scikit-learn提供了多种选项来识别支持向量。在我们的解决方案中，我们使用了`support_vectors_`来输出我们模型中四个支持向量的实际观测特征。或者，我们可以使用`support_`查看支持向量的索引：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we can use `n_support_` to find the number of support vectors belonging
    to each class:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`n_support_`来查找属于每个类的支持向量的数量：
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 17.5 Handling Imbalanced Classes
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17.5 处理不平衡类
- en: Problem
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to train a support vector machine classifier in the presence of imbalanced
    classes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在类不平衡的情况下，您需要训练支持向量机分类器。
- en: Solution
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Increase the penalty for misclassifying the smaller class using `class_weight`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`class_weight`增加误分类较小类别的惩罚：
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Discussion
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'In support vector machines, <math display="inline"><mi>C</mi></math> is a hyperparameter
    that determines the penalty for misclassifying an observation. One method for
    handling imbalanced classes in support vector machines is to weight <math display="inline"><mi>C</mi></math>
    by classes, so that:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持向量机中，<math display="inline"><mi>C</mi></math>是一个超参数，用于确定误分类观察的惩罚。处理支持向量机中的不平衡类的一种方法是通过按类别加权<math
    display="inline"><mi>C</mi></math>，使得：
- en: <math display="block"><mrow><msub><mi>C</mi> <mi>k</mi></msub> <mo>=</mo> <mi>C</mi>
    <mo>×</mo> <msub><mi>w</mi> <mi>j</mi></msub></mrow></math>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>C</mi> <mi>k</mi></msub> <mo>=</mo> <mi>C</mi>
    <mo>×</mo> <msub><mi>w</mi> <mi>j</mi></msub></mrow></math>
- en: where <math display="inline"><mi>C</mi></math> is the penalty for misclassification,
    <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is a weight inversely
    proportional to class <math display="inline"><mi>j</mi></math>’s frequency, and
    <math display="inline"><msub><mi>C</mi><mi>k</mi></msub></math> is the <math display="inline"><mi>C</mi></math>
    value for class <math display="inline"><mi>k</mi></math>. The general idea is
    to increase the penalty for misclassifying minority classes to prevent them from
    being “overwhelmed” by the majority class.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><mi>C</mi></math>是错误分类的惩罚，<math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math>是与类<math
    display="inline"><mi>j</mi></math>频率成反比的权重，<math display="inline"><msub><mi>C</mi><mi>k</mi></msub></math>是类<math
    display="inline"><mi>k</mi></math>的<math display="inline"><mi>C</mi></math>值。总体思路是增加对误分类少数类的惩罚，以防止它们被多数类“淹没”。
- en: 'In scikit-learn, when using `SVC` we can set the values for <math display="inline"><msub><mi>C</mi><mi>k</mi></msub></math>
    automatically by setting `class_weight="balanced"`. The `balanced` argument automatically
    weighs classes such that:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，当使用`SVC`时，可以通过设置`class_weight="balanced"`来自动设置<math display="inline"><msub><mi>C</mi><mi>k</mi></msub></math>的值。`balanced`参数会自动加权各个类别，以便：
- en: <math display="block"><mrow><msub><mi>w</mi> <mi>j</mi></msub> <mo>=</mo> <mfrac><mi>n</mi>
    <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi></msub></mrow></mfrac></mrow></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>w</mi> <mi>j</mi></msub> <mo>=</mo> <mfrac><mi>n</mi>
    <mrow><mi>k</mi><msub><mi>n</mi> <mi>j</mi></msub></mrow></mfrac></mrow></math>
- en: where <math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math> is the
    weight to class <math display="inline"><mi>j</mi></math>, <math display="inline"><mi>n</mi></math>
    is the number of observations, <math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>
    is the number of observations in class <math display="inline"><mi>j</mi></math>,
    and <math display="inline"><mi>k</mi></math> is the total number of classes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><msub><mi>w</mi><mi>j</mi></msub></math>是类<math display="inline"><mi>j</mi></math>的权重，<math
    display="inline"><mi>n</mi></math>是观察次数，<math display="inline"><msub><mi>n</mi><mi>j</mi></msub></math>是类<math
    display="inline"><mi>j</mi></math>中的观察次数，<math display="inline"><mi>k</mi></math>是总类数。
