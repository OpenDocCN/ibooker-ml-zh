- en: Chapter 12\. Under the Hood and Beyond
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。在引擎盖和更远的地方
- en: In this chapter, we’re going to touch on some of the approaches we have used
    throughout the previous chapters on simulation.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涉及一些我们在前几章中用于模拟的方法。
- en: 'We’ve covered the gist: in simulation-based agent learning, an agent undergoes
    a *training* process to develop a *policy* for its behavior. The policy acts as
    a mapping from previous *observations* to the *actions* it took in response and
    the corresponding *rewards* it earned for doing so. Training takes place across
    a large number of *episodes* during which the cumulative reward should increase
    as the agent improves at the given task, partially dictated by *hyperparameters*
    that control aspects of agent behavior during training—including the *algorithm*
    used to produce the behavior model.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经概述了这个要点：在基于仿真的代理学习中，代理经历一个*训练*过程，为其行为开发一个*策略*。策略充当了从先前的*观察*到其响应的*动作*及相应*奖励*的映射。训练发生在大量的*回合*中，期间累积奖励应随着代理在给定任务中的改进而增加，部分受*超参数*的控制，这些超参数控制训练期间代理行为的各个方面，包括生成行为模型的*算法*。
- en: Once trained, *inference* is used to query the trained agent model for the appropriate
    behavior (actions) in response to given stimuli (observations), but learning has
    ceased and thus the agent will no longer *improve* at the given task.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，*推理*被用来查询训练代理模型以响应给定刺激（观察）的适当行为（动作），但学习已经停止，因此代理不再会在给定任务上*改进*。
- en: 'We’ve talked about most of these concepts already:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了大部分这些概念：
- en: We know about observations, actions, and rewards, and how the mapping between
    them is used to build up a policy.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们了解观察、动作和奖励以及它们之间映射如何用于建立策略。
- en: We know that a training phase occurs over a large number of episodes, and how
    once this is completed, the agent transitions to inference (only querying the
    model, not updating it any longer).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们知道训练阶段会在大量的回合中进行，一旦完成，代理就会转向推理（仅查询模型，不再更新它）。
- en: We know we pass a file of hyperparameters to the `mlagents -learn` process,
    but we kind of glossed over that part.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们知道我们将超参数文件传递给`mlagents -learn`过程，但我们在这部分有点草率。
- en: We know there are different algorithms to choose from during training, but maybe
    not why you would choose a specific one from among the options.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们知道在训练过程中有不同的算法可供选择，但也许并不清楚为什么会选择其中的特定选项。
- en: So, this section will take a further look at the *hyperparameters* and *algorithms*
    available in ML-Agents, where and why you should choose to use each of them, and
    how they impact other choices you will make in training, such as your choice of
    rewards scheme.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本节将进一步探讨ML-Agents中可用的*超参数*和*算法*，以及何时以及为何选择使用它们中的每一个，以及它们如何影响您在训练中的其他选择，如奖励方案的选择。
- en: Hyperparameters (and Just Parameters)
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数（和参数）
- en: 'When training begins with ML-Agents, a YAML file needs to be passed with the
    necessary *hyperparameters*. This is commonly referred to in the machine learning
    world as a *hyperparameters file*, but that’s not all it contains, so ML-Agents
    documentation prefers to instead call this a *configuration file*. This contains
    variables used in the learning process whose values will do one of the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用ML-Agents开始训练时，需要传递一个YAML文件，其中包含必要的*超参数*。这在机器学习世界通常被称为*超参数文件*，但其中不仅包含这些，因此ML-Agents文档更倾向于将其称为*配置文件*。其中包含在学习过程中使用的变量，其值将执行以下操作之一：
- en: Specify aspects of the training process (“parameters”)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定训练过程的各个方面（“参数”）
- en: Change the behavior of the agent or model itself as it learns (“hyperparameters”)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改代理或模型本身在学习过程中的行为（“超参数”）
- en: Parameters
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数
- en: 'Commonly configured training *parameters* include the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 常见配置的训练*参数*包括以下内容：
- en: '`trainer_type`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`trainer_type`'
- en: The algorithm to use in training, selected from `ppo`, `sac`, or `poca`
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 训练中使用的算法，从`ppo`、`sac`或`poca`中选择
- en: '`max_steps`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_steps`'
- en: The maximum number of observations an agent can receive or actions it can take
    before an episode ends, whether or not it has achieved the objective
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个回合结束之前，代理可以接收的最大观察次数或执行的动作数量，无论是否已实现目标
- en: '`checkpoint_interval` and `keep_checkpoints`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`checkpoint_interval`和`keep_checkpoints`'
- en: How often to output duplicate/backup models during training and how many of
    the most recent ones to keep around at a time
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中输出重复/备份模型的频率以及保留的最新模型数量
- en: '`summary_freq`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary_freq`'
- en: How often to print out (or send to TensorBoard) details of how training is going
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如何定期输出（或发送到TensorBoard）有关训练进展的详细信息。
- en: '`network_settings` and its corresponding subparameters'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`network_settings`及其对应的子参数'
- en: Allow you to specify some of the size, shape, or behaviors of the neural network
    that represents the agent policy
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 允许您指定代表代理策略的神经网络的一些大小、形状或行为。
- en: Your selection of `trainer_type` will depend on aspects of your agent and environment.
    We’ll talk about that in depth in the next sections, where we take a look under
    the hood of each of these algorithms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 选择`trainer_type`将取决于您的代理和环境的各个方面。我们将在接下来的章节中深入讨论这些算法的内部工作原理。
- en: The definition of `max_steps` is important, because if a simulation is run thousands
    or millions of times—as occurs in the model training process—it is highly likely
    that at some point the agent will get stuck in an unrecoverable state. Without
    an enforced limit, the agent would remain in this state and continually pollute
    its own behavior model with irrelevant or unrepresentative data until Unity ran
    out of available memory or a user terminated the process. Not ideal. Instead,
    experience or estimation should be used to derive a number that allows for a slow-moving
    agent on the right track to achieve the objective without being cut off, but disallows
    excessive floundering.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 定义`max_steps`很重要，因为如果模拟运行了数千次或数百万次——如在模型训练过程中发生的那样——很可能在某个时刻代理会陷入无法恢复的状态。如果没有强制限制，代理将保持在这种状态，并持续污染其行为模型，使用不相关或不具代表性的数据，直到Unity耗尽可用内存或用户终止进程。这不是理想的情况。相反，应使用经验或估计来确定一个数字，允许一个缓慢前进的代理实现目标，但不允许过度挣扎。
- en: For example, imagine you are training a self-driving car that needs to traverse
    a circuit and reach a goal location, and a previously trained model or test run
    tells you an ideal run will reach the goal location in around 5,000 steps. If
    `max_steps` were set to `500,000`, a *junk* episode in which the agent achieved
    nothing would result in 10 times the information as an episode with optimal performance—likely
    muddying the model in the process. But if `max_steps` were set to `5,000` or lower,
    the model would never be afforded the chance to achieve middling results, instead
    having its attempts cut short each time before eventually (maybe) it happened
    to achieve a perfect episode with no prior knowledge. Highly unlikely. In between
    these numbers is best; say, around `10,000` for this example. For your own agents,
    the ideal will depend on the complexity of the task it is to perform.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您正在训练一辆自动驾驶车辆，需要穿过一个赛道并达到目标位置，先前训练过的模型或测试运行告诉您，理想的运行需要大约5,000步才能到达目标位置。如果将`max_steps`设置为`500,000`，那么一个*无用*的情节，即代理未能达成任何成就，将导致信息量增加十倍，这很可能会混淆模型的过程。但是，如果将`max_steps`设置为`5,000`或更低，模型将永远没有机会取得中等结果，而是每次都会在很短的时间内结束尝试，直到最终（也许）偶然达到了完美的无先验知识情节。这种可能性极小。在这些数字之间选择最佳的策略；例如，在这个例子中大约为`10,000`步。对于您自己的代理，理想值将取决于其执行任务的复杂性。
- en: The checkpoint functionality allows intermediate models to be saved as every
    `checkpoint_interval` changes, keeping the last `keep_checkpoints` models around
    at all times. This means if you see the agent is performing well long before training
    is set to end, you can terminate the training process and just use the latest
    checkpoint. Training can also be resumed from a checkpoint, allowing training
    to begin with one algorithm and then continue with another—such as the BC to GAIL
    example given in [Chapter 1](ch01.html#introducing-the-tools).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点功能允许在每次`checkpoint_interval`更改时保存中间模型，始终保留最后`keep_checkpoints`个模型。这意味着，如果您发现代理在训练结束前表现良好，您可以终止训练过程，仅使用最新的检查点。还可以从检查点恢复训练，允许从一种算法开始训练，然后切换到另一种——例如在[第1章](ch01.html#introducing-the-tools)中提到的从BC到GAIL的示例。
- en: The `network_settings` accessor can be used to set subparameters that dictate
    the shape of the neural network used as the behavior model. This can be helpful
    for reducing the size of the resulting model if it is to be used in an environment
    where storage or compute resources are limited (e.g., edge devices), such as with
    `network_settings→num_layers` or `network_settings->hidden_units`. Other settings
    can give finer control over specific aspects such as the method used to interpret
    visual input, or whether to normalize continuous observations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 访问`network_settings`可用于设置决定行为模型神经网络形状的子参数。如果要在存储或计算资源有限的环境（例如边缘设备）中使用生成的模型，则这对于减小模型大小非常有帮助，例如通过`network_settings→num_layers`或`network_settings->hidden_units`。其他设置可以更精细地控制特定方面，如用于解释视觉输入的方法，或者是否对连续观测进行归一化。
- en: Reward Parameters
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励参数
- en: Further parameters then need to be defined to specify how rewards are treated
    during training. These fall under `reward_signals`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来需要定义进一步的参数来指定如何在训练期间处理奖励。这些参数属于`reward_signals`。
- en: Where explicit rewards are used, the `reward_signals->extrinsic->strength` and
    `extrinsic->gamma` must be set. Here, `strength` is simply a scale applied to
    the rewards sent by `AddReward` or `SetReward` calls, which you might want to
    reduce if you were experimenting with a hybrid learning approach or some other
    combination of extrinsic and intrinsic rewards.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用明确奖励时，必须设置`reward_signals->extrinsic->strength`和`extrinsic->gamma`。在这里，`strength`只是一个应用于通过`AddReward`或`SetReward`调用发送的奖励的比例，如果您正在尝试混合学习方法或其他外在和内在奖励的组合，则可能希望减少这一比例。
- en: Meanwhile, `gamma` is a scale applied to *estimations* of the reward, based
    on how long it would take to achieve. This is used when the agent is considering
    what it should do next, based on the reward it thinks it will receive for each
    of the options. `gamma` can be viewed as a measure of how much an agent should
    prioritize long-term gain over short-term gain. Should the agent forego rewards
    in the next few steps to hopefully achieve a larger objective and receive a larger
    reward at the end? Or should it do whatever will give it rewards most immediately
    in the moment? The choice will depend on your rewards scheme and the complexity
    of the task the agent is to achieve, but generally higher values (suggesting more
    long-term thinking) tend to produce smarter agents.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，`gamma`是应用于奖励估计的比例，基于达到该奖励所需的时间。当代理考虑下一步该做什么时会使用这一值，基于它认为对每个选项将会收到的奖励。`gamma`可以被视为一个衡量代理应该在长期获益与短期获益之间优先考虑多少的指标。代理是否应该放弃未来几步的奖励，以希望在最后实现更大的目标并获得更大的奖励？还是应该立即做出能立即获得奖励的选择？这个选择将取决于您的奖励方案以及代理要完成任务的复杂性，但通常较高的值（表明更多的长期思考）倾向于产生更智能的代理。
- en: Other types of rewards may be enabled that will come directly from the training
    process, and they are called *intrinsic* rewards—similar to those used by imitation
    learning methods. But where IL rewards the agent for similarity to demonstrated
    behaviors, these rewards encourage general attributes—almost like a certain agent
    *personality*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以启用其他类型的奖励，这些奖励直接来自训练过程，并被称为*内在*奖励——类似于模仿学习方法使用的那些奖励。但是，IL奖励代理与展示行为的相似性，而这些奖励鼓励了一般属性，几乎像某种特定代理的*个性*。
- en: The most generally applicable intrinsic reward is `curiosity`, defined by `reward_signals->curiosity`.
    Curiosity in an agent means level of tendency to prioritize mapping the unknown
    (i.e., try new things and see how many points they get) over known good actions
    (i.e., things that have gotten them points in the past). Curiosity is encouraged
    through rewards that scale based on how new an action is or how unforeseen its
    result is, and this helps avoid the local maxima problem that commonly occurs
    in sparse reward environments.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最普遍适用的内在奖励是`curiosity`，由`reward_signals->curiosity`定义。代理的好奇心意味着优先级别映射未知（即尝试新事物并查看其得分）超过已知良好动作（即过去给他们带来得分的动作）。通过基于动作的新颖程度或其结果的意外性的奖励来鼓励好奇心，这有助于避免稀疏奖励环境中常见的局部最大化问题。
- en: For example, an agent may be designed to seek out and stand on a platform to
    open a door and then move through the open door to reach a goal. To incentivize
    each step and speed up training, you would give the agent a reward for standing
    on the platform and an exponentially larger reward for then reaching the goal.
    But a noncurious agent may recognize that it got a reward for the first step,
    and decide the best course of action is to continually step on and off the platform
    until each episode—and eventually training—ends. This is because it *knows* that
    standing on the platform is good, and the results of any other action (of which
    there could be infinite as far as it knows) are unknown. So, it will just stick
    with what it is good at. This is why multistage objectives will often require
    the introduction of artificial curiosity to make the agent more willing to try
    new things.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个代理可能被设计成寻找并站在一个平台上以打开一个门，然后穿过打开的门到达目标。为了激励每一步并加快训练速度，你可以给予代理站在平台上的奖励，并对达到目标后给予指数级增长的奖励。但一个不好奇的代理可能会意识到它在第一步获得了奖励，并决定最佳行动是不断地在平台上站立直至每一集结束，最终结束训练。这是因为它*知道*站在平台上是好的，而它认为其他任何行动的结果（在它看来可能是无限的）是未知的。因此，它将坚持自己擅长的行为。这就是为什么多阶段目标通常需要引入人工好奇心，使代理更愿意尝试新事物。
- en: 'To enable curiosity, simply pass the same hyperparameters as are required for
    extrinsic rewards:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用好奇心，只需传递与外在奖励所需的相同的超参数：
- en: '`reward_signals->curiosity->strength`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`reward_signals->curiosity->strength`'
- en: A number to scale curiosity rewards by when trying to balance them against other
    rewards such as extrinsic rewards (must be between `0.0` and `1.0`)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当试图平衡好奇奖励与其他奖励（如外在奖励）时，需要按比例缩放好奇奖励的数字（必须在`0.0`和`1.0`之间）。
- en: '`reward_signals->curiosity->gamma`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`reward_signals->curiosity->gamma`'
- en: A second scale to apply to temper the perceived value of rewards based on how
    long they would take to achieve, the same as in `extrinsic->gamma` (also between
    `0.0` and `1.0`)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个尺度用于根据实现所需时间来调节奖励的感知价值，与`extrinsic->gamma`中的相同（同样在`0.0`和`1.0`之间）。
- en: Other, less commonly used intrinsic reward signals can be used to introduce
    other agent tendencies, such as random network distillation, or to enable particular
    learning types, such as GAIL.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其他不太常用的内在奖励信号可以用来引入其他代理倾向，例如随机网络蒸馏，或者启用特定的学习类型，例如GAIL。
- en: Hyperparameters
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: 'Commonly configured model *hyperparameters* include:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 常见配置的模型*超参数*包括：
- en: '`batch_size`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch_size`'
- en: Controls how many iterations the simulation does per update of the model. The
    `batch_size` should be large (in the thousands) if you’re using continuous actions
    at all, and small (in the tens) if you’re only using discrete actions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 控制模型每次更新时模拟执行的迭代次数。如果你完全使用连续动作，`batch_size`应该很大（数千），如果只使用离散动作，则应该很小（十几）。
- en: '`buffer_size`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`buffer_size`'
- en: Controls different things depending on the algorithm. For PPO and MA-POCA, it
    controls the number of experiences to collect before we do any updating of the
    model (it should be multiple times larger than `batch_size`). For SAC, `buffer_size`
    corresponds to the maximum size of the experience buffer, so SAC can learn from
    both old and new experiences.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 取决于算法的不同，控制不同的事物。对于PPO和MA-POCA，它控制在更新模型之前收集的经验数量（应该是`batch_size`的几倍）。对于SAC，`buffer_size`对应于经验缓冲区的最大大小，因此SAC可以从旧的和新的经验中学习。
- en: '`learning_rate`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate`'
- en: Corresponds to how much of an impact each update makes on the model. It will
    typically be between `1e-5` and `1e-3`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型的影响每次更新的程度。通常介于`1e-5`和`1e-3`之间。
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If training is unstable (in other words, the reward does not consistently increase),
    try decreasing the `learning_rate`. A larger `buffer_size` will also correspond
    to more stable training.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练不稳定（换句话说，奖励不一致增加），尝试降低`learning_rate`。较大的`buffer_size`也将对训练的稳定性有所帮助。
- en: 'Some hyperparameters are specific to the trainer being used. Let’s start with
    the important ones for the trainer you’ll probably use most often (PPO):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一些超参数是特定于所使用的训练器的。让我们从您可能经常使用的训练器（PPO）的重要参数开始：
- en: '`beta` incentivizes exploration. Here, high `beta` values will have a result
    similar to the curiosity intrinsic reward, in that it will encourage the agent
    to try new things even once early rewards have been discovered. Lower `beta` values
    are preferred in simple environments, so agents will tend to limit their behaviors
    to those that have been beneficial in the past—which may decrease training time.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta`鼓励探索。在这里，较高的`beta`值将导致与好奇心内在奖励类似的结果，因为它鼓励代理尝试新事物，即使早期已发现了奖励。在简单环境中更倾向于较低的`beta`值，因此代理将倾向于限制其行为在过去曾有益的行为上，这可能会减少训练时间。'
- en: '`epsilon` dictates how receptive the behavior model is to change. Here, high
    `epsilon` values will allow the agent to adopt new behaviors quickly once rewards
    are discovered, but this also allows the agent behavior to change easily and often
    even into late training. Lower `epsilon` values mean an agent will need more attempts
    to learn from an experience, and may result in longer training but will ensure
    consistent behavior in late training.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon`决定了行为模型对变化的接受程度。在这里，较高的`epsilon`值将允许代理快速采纳新行为一旦发现奖励，但这也意味着代理的行为容易改变，甚至在训练后期也是如此。较低的`epsilon`值意味着代理需要更多尝试才能从经验中学习，可能会导致更长的训练时间，但会确保在训练后期行为一致。'
- en: Schedule hyperparameters such as `beta_schedule` and `epsilon_schedule` can
    be used to make the values of other hyperparameters change during training, such
    as to prioritize `beta` (curiosity) more during early training, or to reduce `epsilon`
    (fickleness) during late training.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度超参数如`beta_schedule`和`epsilon_schedule`可用于在训练过程中更改其他超参数的值，例如在早期训练中优先考虑`beta`（好奇心），或在后期减少`epsilon`（善变性）。
- en: Tip
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: POCA/MA-POCA uses the same hyperparameters as PPO, but [SAC has a few dedicated
    hyperparameters of its own](https://oreil.ly/SuApG).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: POCA/MA-POCA使用与PPO相同的超参数，但是[SAC有一些专门的超参数](https://oreil.ly/SuApG)。
- en: For a full list of the parameters and hyperparameters currently supported by
    ML-Agents, see [the ML-Agents documentation](https://oreil.ly/4M3An).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 欲知当前ML-Agents支持的参数和超参数的完整列表，请参阅[ML-Agents文档](https://oreil.ly/4M3An)。
- en: Tip
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you don’t really know (or want to know) what the required hyperparameters
    mean for your chosen model, you can look at the [ML-Agents example files for each
    trainer type](https://oreil.ly/0048b) on GitHub. Once you see how training goes,
    if you encounter some hyperparameter-specific issues as described here, you may
    choose to tweak those specific values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太了解（或者不想了解）你选择的模型所需的超参数是什么意思，你可以查看GitHub上每种训练器类型的[ML-Agents示例文件](https://oreil.ly/0048b)。一旦你看到训练的过程，如果你遇到像这里描述的一些特定于超参数的问题，你可以选择调整这些具体的数值。
- en: Algorithms
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法
- en: 'The Unity ML-Agents framework allows agents to learn behaviors through optimization
    of rewards defined in one of the following ways:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Unity ML-Agents框架允许代理通过优化定义在以下方式之一的奖励来学习行为：
- en: Explicitly (extrinsic rewards) by you
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 明确地（外在奖励）由你
- en: 'RL approaches in which we’ve used the `AddReward` or `SetReward` method: we
    give an agent a reward when we know it did something right (or a penalty when
    it’s wrong).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用`AddReward`或`SetReward`方法的RL方法中：当我们知道代理做对了某事时（或者做错了时）我们给予奖励（或惩罚）。
- en: Implicitly (intrinsic rewards) by selected algorithm
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 隐含地（内在奖励）由选择的算法
- en: 'IL approaches in which rewards are given based on similarity to the provided
    behavior demonstration: we demonstrate behavior to an agent, and it attempts to
    clone it and gets rewarded automatically based on how well it clones it.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在IL方法中，基于与提供的行为演示的相似性来给予奖励：我们向代理展示行为，它试图克隆它，并根据其克隆的质量自动获得奖励。
- en: Implicitly by training process
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 隐含地由训练过程
- en: The hyperparameter settings we discussed, in which the agent can be rewarded
    for exhibiting certain attributes such as curiosity. We haven’t touch on this
    one in this book as much, as it’s a bit out of scope.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的超参数设置，其中代理可以因展现某些属性（如好奇心）而受到奖励。在这本书中，我们并没有深入讨论这个，因为这超出了本书的范围。
- en: But that’s not all that differs between the different algorithms available in
    ML-Agents.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不是ML-Agents中可用算法之间的唯一区别。
- en: '*Proximal policy optimization (PPO)* is probably the most sensible default
    choice for your ML efforts with Unity. [PPO](https://oreil.ly/6rCeP) attempts
    to approximate an ideal function that maps an agent’s observations to the best
    possible action available for a given state. It’s designed to be a general-purpose
    algorithm. It’s unlikely to be the most effective, but it will usually get the
    job done. That’s why Unity ML-Agents ships it as the default.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*近端策略优化（PPO）* 可能是在使用Unity进行ML工作时最明智的默认选择。[PPO](https://oreil.ly/6rCeP) 试图逼近一个理想函数，将代理的观察映射到给定状态下可能的最佳行动。它被设计为通用算法。它可能不是最有效的，但通常可以完成任务。这就是为什么Unity
    ML-Agents将其作为默认选项的原因。'
- en: '*Soft actor-critic (SAC)* is an *off-policy* RL algorithm. This basically means
    that optimal training behavior and optimal resulting agent behavior can be defined
    separately. This can reduce the training time required for an agent to arrive
    at optimal behavior, because you can encourage certain attributes that may be
    desirable during training but not in the final agent behavior model.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*软策演员-评论者（SAC）* 是一种*离策略*的强化学习算法。这基本上意味着可以单独定义最佳训练行为和最佳结果代理行为。这可以减少代理达到最佳行为所需的训练时间，因为可以在训练期间鼓励一些可能在训练中可取但在最终代理行为模型中不可取的特征。'
- en: The best example of such an attribute is *curiosity*. Curious exploration is
    great during training, when you don’t want your agent to discover one thing that
    gives it points and then never try anything else. But it’s not so great once the
    model is trained, because if training has gone as intended, it will already have
    discovered all the desirable behaviors.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这种属性的最佳例子是*好奇心*。在训练期间，好奇心的探索是很好的，因为你不希望你的代理只发现一个给它点数的东西，然后再也不尝试其他任何事物。但是一旦模型训练完毕，这种探索就不那么理想了，因为如果训练如期进行，它已经发现了所有理想的行为。
- en: So, [SAC](https://oreil.ly/jPlxp) can be faster for training but requires more
    memory to hold and update separate behavior models when compared to an *on-policy*
    approach like PPO.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，[SAC](https://oreil.ly/jPlxp) 在训练速度上可能更快，但相比于像PPO这样的*在策略*方法，需要更多的内存来存储和更新单独的行为模型。
- en: Note
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There is debate as to whether PPO is *on-policy* or *off-policy*. We tend to
    think of it as on-policy, since it makes updates based on following the current
    policy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有关于PPO是*在策略*还是*离策略*的争论。我们倾向于将其视为在策略上，因为它基于遵循当前策略进行更新。
- en: '*Multi-Agent POsthumous Credit Assignment (POCA or MA-POCA)* is a multiagent
    algorithm that uses a centralized *critic* to reward and penalize a group of agents.
    The rewards are similar to basic PPO, but are given to the critic. The agents
    should learn how best to contribute to receiving the reward, but can also be individually
    rewarded. It’s considered *posthumous* because an agent can be removed from the
    group of agents during the learning process, but will still learn which of its
    actions contributed to the reward achieved by the group, even after being removed.
    This means agents can take actions that are beneficial to the group, even if they
    result in their own death.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*多代理人死后信用分配（POCA或MA-POCA）* 是一种多代理算法，使用集中的*评论者*来奖励和惩罚一组代理。奖励类似于基本的PPO，但是奖励给评论者。代理应该学会如何最好地贡献以获得奖励，但也可以单独奖励。它被认为是*死后*的，因为代理在学习过程中可以从代理组中移除，但仍然会学习其行为对组获得奖励的贡献，即使在被移除后也是如此。这意味着代理可以采取对组有益的行动，即使这些行动会导致它们自身的死亡。'
- en: We used [MA-POCA](https://oreil.ly/aDvvz) in [Chapter 9](ch09.html#chapter-coop-intro).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第9章](ch09.html#chapter-coop-intro)中使用了[MA-POCA](https://oreil.ly/aDvvz)。
- en: Unity Inference Engine and Integrations
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Unity 推理引擎和集成
- en: During agent training, the neural network that represents agent behavior is
    constantly updated as the agent performs actions and receives feedback in the
    form of rewards. This is often a lengthy process, as a neural network graph can
    be very large, and the calculations required to adjust it between steps or episodes
    scales with its size. Likewise, the number of episodes required for an agent to
    consistently succeed at the desired task is commonly in the hundreds of thousands
    or even millions.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理训练期间，代表代理行为的神经网络会随着代理执行动作并接收奖励形式的反馈而不断更新。这通常是一个漫长的过程，因为神经网络图可能非常庞大，调整所需的计算量会随其大小而增加。同样，使代理在所需任务中持续成功所需的剧集数量通常在数十万甚至数百万。
- en: Thus, training an agent of moderate complexity in ML-Agents will easily occupy
    a personal computer for hours or even days. And yet a *trained* agent can easily
    be included in a Unity game or exported for use in simple applications. So, how
    is it that their use becomes more feasible following training?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 ML-Agents 中训练一个中等复杂度的代理可能会占用个人电脑数小时甚至数天。然而，一个*训练过的*代理可以轻松地包含在 Unity 游戏中或导出用于简单的应用程序中。那么，他们在训练后的使用如何变得更加可行呢？
- en: Tip
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you want to train models for use in ML-Agents outside of ML-Agents, start
    by understanding the [Tensor names](https://oreil.ly/J59hl) and [Barracuda model
    parameters](https://oreil.ly/pu3YM). It’s outside the scope of this book, but
    it’s very interesting!.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要在 ML-Agents 外部训练用于 ML-Agents 的模型，请首先了解[Tensor名称](https://oreil.ly/J59hl)和[Barracuda模型参数](https://oreil.ly/pu3YM)。这超出了本书的范围，但非常有趣！
- en: The answer is the difference between the performance required during training
    versus during *inference*. Following the training phase, the neural network of
    agent behavior is locked in place; it will no longer be updated as the agent performs
    actions, and rewards will no longer be sent as feedback. Instead, the same observations
    will be served to the agent as they were during training, but the rules that define
    which observations will be responded to with which actions are already defined.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 答案在于训练期间所需的性能与*推理*期间的差异。在训练阶段后，代理行为的神经网络被锁定在一个位置；随着代理执行动作，它将不再更新，奖励也不再作为反馈发送。相反，将以与训练期间相同的方式向代理提供相同的观察，但是定义哪些观察将与哪些动作响应的规则已经定义好了。
- en: Figuring out the corresponding reaction is as simple as tracing a graph. For
    this reason, inference is a performant process that can be included even in applications
    where compute resources are limited. All that is needed is an *inference engine*
    that knows how to take input, trace the network graph, and output the appropriate
    action to be performed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 弄清楚对应的反应就像追踪一个图形一样简单。因此，推理是一个高效的过程，即使在计算资源有限的应用程序中也可以包含。所需的只是一个*推理引擎*，它知道如何接受输入，追踪网络图，并输出适当的操作。
- en: Note
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The Unity ML-Agents inference engine is implemented using *compute shaders*,
    which are tiny specialized programs that run on a GPU (also known as a graphics
    card), but are not used for graphics. This means they may not work on all [platforms](https://oreil.ly/Iaj5s).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Unity ML-Agents 推理引擎是使用*计算着色器*实现的，这些着色器是在 GPU 上运行的小型专用程序（也称为图形卡），但不用于图形处理。这意味着它们可能无法在所有[平台](https://oreil.ly/Iaj5s)上工作。
- en: Luckily, Unity ML-Agents comes with one, called the *Unity inference engine*
    (sometimes called Barracuda). So, you don’t need to make your own—or ship around
    the underlying framework you used for training, such as PyTorch or TensorFlow.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Unity ML-Agents 自带一个称为*Unity推理引擎*（有时称为Barracuda）的推理引擎。因此，你不需要制作自己的引擎，或者在训练时使用的底层框架（如PyTorch或TensorFlow）。
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can learn more about Barracuda in the [Unity documentation](https://oreil.ly/0jyye).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[Unity文档](https://oreil.ly/0jyye)中了解更多关于Barracuda的信息。
- en: If you’ve trained a model outside of ML-Agents, you will not be able to use
    it with Unity’s inference engine. You could, in theory, make a model outside of
    ML-Agents that complies with the names of constants and tensors that ML-Agents
    expects, but it’s not officially supported.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 ML-Agents 外部训练了一个模型，你将无法使用它与 Unity 的推理引擎。理论上，你可以创建一个符合 ML-Agents 期望的常量和张量名称的模型，但这并没有得到官方支持。
- en: Warning
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It might be counterintuitive (or really intuitive, depending on your background),
    but for models generated using ML-Agents, running inference using the CPU will
    be faster than using the GPU unless you have a significant number of visual observations
    on your agent.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习代理生成的模型，使用 CPU 运行推理速度可能比使用 GPU 更快，这可能有点违反直觉（或者对于你的背景来说可能很直观），除非你的代理有大量的视觉观察。
- en: Using the ML-Agents Gym Wrapper
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 ML-Agents Gym 包装器
- en: The OpenAI Gym is an (almost de facto standard at this point) open source library
    used for developing and exploring reinforcement learning algorithms. In this section,
    we’re going to take a quick look at using the ML-Agents Gym Wrapper to explore
    reinforcement learning algorithms.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 是一个（几乎成为事实上的标准）用于开发和探索强化学习算法的开源库。在本节中，我们将快速了解使用 ML-Agents Gym 包装器来探索强化学习算法。
- en: 'Before we get started with the ML-Agents Gym Wrapper, you’ll need to have your
    Python and Unity environments set up. So, if you haven’t done so already, create
    a new environment via the steps from [“Setting Up”](ch02.html#ch02-setup). Once
    you’ve done this, continue here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用ML-Agents Gym Wrapper之前，您需要设置好Python和Unity环境。因此，如果您还没有这样做，请通过[“设置”](ch02.html#ch02-setup)的步骤创建一个新的环境。完成这些步骤后，继续执行以下操作：
- en: 'Activate your new environment, and then install the `gym_unity` Python package:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活您的新环境，然后安装`gym_unity` Python包：
- en: '[PRE0]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can then, from any Python script, launch a Unity simulation environment
    *as a gym*:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以从任何Python脚本中启动Unity仿真环境*作为gym*：
- en: '[PRE1]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this case, `unity_env` is a Unity environment to be wrapped and presented
    as a gym. That’s it!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`unity_env`是要包装并作为gym呈现的Unity环境。就是这样！
- en: Unity environments and OpenAI Baselines
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Unity环境和OpenAI Baselines
- en: One of the most interesting components of the OpenAI project is OpenAI Baselines,
    a set of high-quality implementations of reinforcement algorithms. It’s in maintenance
    mode now, but it still provides an incredibly useful collection of algorithms
    with which you can explore reinforcement learning.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI项目中最有趣的组成部分之一是OpenAI Baselines，这是一组高质量的强化学习算法实现。现在处于维护模式，但它仍然提供了一系列非常有用的算法，供您探索强化学习。
- en: Conveniently, you can use OpenAI Baselines together with a Unity simulation
    environment, via the Unity ML-Agents Gym Wrapper.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的是，您可以通过Unity ML-Agents Gym Wrapper与Unity仿真环境一起使用OpenAI Baselines。
- en: As a quick example, we’ll train the GridWorld that we were using in [Chapter 11](ch11.html#chapter-python)
    with the DQN algorithm from OpenAI.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个快速示例，我们将使用OpenAI的DQN算法来训练我们在[第11章](ch11.html#chapter-python)中使用的GridWorld。
- en: 'First, you’ll need to build a copy of the GridWorld environment:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要构建GridWorld环境的一个副本：
- en: Open the Project folder in the copy of the ML-Agents GitHub repository that
    you cloned or downloaded (see [“Setting Up”](ch02.html#ch02-setup)) as a Unity
    project using the Unity Hub, and then use the Project view to open the GridWorld
    scene.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您克隆或下载的ML-Agents GitHub存储库的副本中，打开项目文件夹（参见[“设置”](ch02.html#ch02-setup)），作为Unity项目使用Unity
    Hub打开，并使用项目视图打开GridWorld场景。
- en: Then open File menu → Build settings, and choose your current platform.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后打开“文件”菜单 → “构建设置”，选择您当前的平台。
- en: Make sure the only scene selected in the Scenes in Build list is the GridWorld
    scene.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保“场景构建列表”中仅选中了GridWorld场景。
- en: Click Build and choose to save the build somewhere you’re familiar with on your
    system.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“构建”并选择一个您熟悉的位置保存构建。
- en: Tip
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You might be wondering why we cannot use the default registry to grab a copy
    of GridWorld, since we’re exclusively working in Python here. The reason is that
    the ML-Agents Gym Wrapper only supports environments in which there is a single
    agent. All the prebuilt default registry environments have multiple areas in them,
    for speedier training.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可能会想知道为什么我们不能使用默认的注册表来获取GridWorld的副本，因为我们在这里专门使用Python。原因是ML-Agents Gym Wrapper只支持存在单个代理的环境。所有预构建的默认注册表环境都有多个区域，以加快训练速度。
- en: 'Next, we’ll move to Python:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将转到Python：
- en: 'In your Python environment, you’ll need to install the Baselines package:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的Python环境中，您需要安装Baselines包：
- en: '[PRE2]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Warning
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'You might need to install TensorFlow, via `pip install tensorflow==1.15`, before
    you can do this. You’ll need this specific version of TensorFlow in order to maintain
    compatibility with OpenAI Baselines: specifically, it uses the TensorFlow `contrib`
    module, which is not part of TensorFlow 2.0\. Such is the joy of Python.'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可能需要在执行此操作之前安装TensorFlow，通过`pip install tensorflow==1.15`。您将需要这个特定版本的TensorFlow以保持与OpenAI
    Baselines的兼容性：特别是它使用了TensorFlow的`contrib`模块，该模块不是TensorFlow 2.0的一部分。这就是Python的乐趣所在。
- en: Next, fire up Jupyter Lab, following the process we used in [“Experimenting
    with an Environment”](ch11.html#ch11-python-setup) and create a new notebook.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，启动Jupyter Lab，按照我们在[“尝试环境”](ch11.html#ch11-python-setup)中使用的过程创建一个新的笔记本。
- en: 'Add the following `import` lines:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下`import`行：
- en: '[PRE3]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, get a handle on the Unity environment we built out a moment ago, and
    convert it to a gym:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，获取一下我们刚刚构建的Unity环境，并将其转换为gym环境：
- en: '[PRE4]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that the equivalent of `/Users/parisba/Downloads/GridWorld.app` should
    point to a *.app* or *.exe* or other executable (depending on your platform),
    which is the built copy of GridWorld we made a moment ago.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，`/Users/parisba/Downloads/GridWorld.app`的等效部分应该指向一个*.app*或*.exe*或其他可执行文件（取决于您的平台），这是我们刚刚制作的GridWorld的构建副本。
- en: 'Finally, run the training:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，运行训练：
- en: '[PRE5]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Your environment will launch, and it will be trained using the OpenAI Baselines
    DQN algorithm.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你的环境将启动，并将使用OpenAI Baselines DQN算法进行训练。
- en: Side Channels
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Side Channels
- en: 'Unity’s Python ML-Agents components provide a feature called side channels,
    which allow you to share arbitrary information back and forth between C# code
    running in Unity and Python code. Specifically, ML-Agents supplies two side channels
    for you to use: `EngineConfigurationChannel` and `EnvironmentParametersChannel`.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Unity的Python ML-Agents组件提供了一个名为“side channels”的功能，允许你在运行在Unity中的C#代码和Python代码之间双向共享任意信息。具体来说，ML-Agents提供了两个可供使用的side
    channels：`EngineConfigurationChannel`和`EnvironmentParametersChannel`。
- en: Engine configuration channel
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引擎配置通道
- en: 'The engine configuration channel allows you to vary parameters related to the
    engine: timescale, graphics quality, resolution, and such. It’s intended to be
    used to increase performance during training by varying the quality, or to make
    things prettier and more interesting or useful for human review during inference.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 引擎配置通道允许你变化与引擎相关的参数：时间尺度、图形质量、分辨率等。它旨在通过变化质量来提高训练性能，或者在推断期间使事物更漂亮、更有趣或更有用以供人类审查。
- en: 'Follow these steps to create an `EngineConfigurationChannel`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建一个`EngineConfigurationChannel`：
- en: 'Make sure the following are part of your `import` statements:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保以下内容包含在你的`import`语句中：
- en: '[PRE6]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create an `EngineConfigurationChannel`:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`EngineConfigurationChannel`：
- en: '[PRE7]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Pass the channel into the `UnityEnvironment` that you’re using:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将通道传递给你正在使用的`UnityEnvironment`：
- en: '[PRE8]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Configure the channel as needed:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要配置通道：
- en: '[PRE9]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this case, the configuration for this `EngineConfigurationChannel` sets the
    `time_scale` to `2.0`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这个`EngineConfigurationChannel`的配置将`time_scale`设置为`2.0`。
- en: And that’s it! There is a range of possible arguments that can be used with
    `set_configuration_parameters`, such as `width` and `height`, for resolution control,
    `quality_level`, and `target_frame_rate`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！有一系列可能用于`set_configuration_parameters`的参数，比如用于分辨率控制的`width`和`height`，`quality_level`和`target_frame_rate`。
- en: Environment parameters channel
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境参数通道
- en: The environment parameters channel is more general than the engine configuration
    channel; it allows you to work with any numerical values you need to pass back
    and forth between Python and the simulation environment.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 环境参数通道比引擎配置通道更通用；它允许你在Python和仿真环境之间传递任何需要的数值值。
- en: 'Follow these steps to create an `EnvironmentParametersChannel`:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建一个`EnvironmentParametersChannel`：
- en: 'Ensure that you have the following `import` statements:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你拥有以下的`import`语句：
- en: '[PRE10]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Create an `EnvironmentParametersChannel` and pass it to the `UnityEnvironment`,
    as we did for the engine configuration channel:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`EnvironmentParametersChannel`并将其传递给`UnityEnvironment`，就像我们对引擎配置通道所做的那样：
- en: '[PRE11]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, use the channel to `set_float_parameter` on the Python side, naming a
    parameter:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在Python端使用该通道，命名一个参数为`set_float_parameter`：
- en: '[PRE12]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this case, the parameter is named `myParam`.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，参数被命名为`myParam`。
- en: 'This allows you to get access to the same parameter from C# in Unity:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这允许你从Unity中的C#访问相同的参数：
- en: '[PRE13]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `0.0f` in the call here is a default value.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里调用中的`0.0f`是一个默认值。
- en: And with that, we’re done with this chapter, and more or less done with simulations
    for the book. We’ve provided some next steps in the code download; if you’re curious
    about reinforcement learning and want to explore more, open the Next_Steps folder
    in the resources bundle you can find on the [book’s website](http://secretlab.com.au/books/practical-sims).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们完成了本章的内容，也基本完成了书籍中的模拟。在代码下载中提供了一些下一步操作；如果你对强化学习感兴趣并希望进一步探索，请打开资源包中书籍网站的Next_Steps文件夹。
