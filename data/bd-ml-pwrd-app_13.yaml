- en: Chapter 9\. Choose Your Deployment Option
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。选择您的部署选项
- en: The previous chapters covered the process of going from a product idea to an
    ML implementation, as well as methods to iterate on this application until you
    are ready to deploy it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的章节介绍了从产品创意到ML实现的过程，以及迭代此应用程序直至准备部署的方法。
- en: This chapter covers different deployment options and the trade-offs between
    each of them. Different deployment approaches are suited to different sets of
    requirements. When considering which one to choose, you’ll want to think of multiple
    factors such as latency, hardware and network requirements, as well as privacy,
    cost, and complexity concerns.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了不同的部署选项以及它们之间的权衡。不同的部署方法适合不同的需求集合。在选择时，您需要考虑多个因素，如延迟、硬件和网络要求，以及隐私、成本和复杂性问题。
- en: The goal of deploying a model is to allow users to interact with it. We will
    cover common approaches to achieve this goal, as well as tips to decide between
    approaches when deploying models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 部署模型的目标是允许用户与之交互。我们将介绍实现此目标的常见方法，以及在部署模型时如何在不同方法之间进行选择的技巧。
- en: We will start with the simplest way to get started when deploying models and
    spinning up a web server to serve predictions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从部署模型和启动Web服务器提供预测的最简单方法开始。
- en: Server-Side Deployment
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务器端部署
- en: Server-side deployment consists of setting up a web server that can accept requests
    from clients, run them through an inference pipeline, and return the results.
    This solution fits within a web development paradigm, as it treats models as another
    endpoint in an application. Users have requests that they send to this endpoint,
    and they expect results.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器端部署包括设置一个能够接受客户端请求、通过推断管道运行请求并返回结果的网络服务器。这种解决方案适用于Web开发范式，因为它将模型视为应用程序中的另一个端点。用户向此端点发送请求，并期望得到结果。
- en: There are two common workloads for server-side models, streaming and batch.
    Streaming workflows accept requests as they come and process them immediately.
    Batch workflows are run less frequently and process a large number of requests
    all at once. Let’s start by looking at streaming workflows.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器端模型有两种常见的工作负载，流式处理和批处理。流式处理工作流在接收请求时立即处理它们。批处理工作流则较少频繁运行，一次处理大量请求。让我们首先看看流式处理工作流。
- en: Streaming Application or API
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式应用程序或API
- en: The streaming approach considers a model as an endpoint that users can send
    requests to. In this context, users can be end users of an application or an internal
    service that relies on predictions from a model. For example, a model that predicts
    website traffic could be used by an internal service that is charged with adjusting
    the number of servers to match the predicted amount of users.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 流式处理方法将模型视为用户可以发送请求的端点。在这种情况下，用户可以是应用程序的最终用户，也可以是依赖于模型预测的内部服务。例如，预测网站流量的模型可以被内部服务使用，负责根据预测的用户数量调整服务器数量。
- en: 'In a streaming application, the code path for a request goes through a set
    of steps that are the same as the inference pipeline we covered in [“Start with
    a Simple Pipeline”](ch02.html#pipeline_description). As a reminder, these steps
    are:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在流式应用程序中，请求的代码路径经过我们在[“从简单流水线开始”](ch02.html#pipeline_description)中介绍的一系列步骤。作为提醒，这些步骤包括：
- en: Validate the request. Verify values of parameters passed, and optionally check
    whether the user has the correct permissions for this model to be run.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证请求。验证传递的参数值，并可选择检查用户是否具有运行此模型的正确权限。
- en: Gather additional data. Query other data sources for any additional needed data
    we may need, such as information related to a user, for example.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集额外数据。查询其他数据源，获取可能需要的任何额外数据，例如与用户相关的信息。
- en: Preprocess data.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据。
- en: Run the model.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行模型。
- en: Postprocess the results. Verify that the results are within acceptable bounds.
    Add context to make it understandable to the user, such as explaining the confidence
    of a model.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 后处理结果。验证结果是否在可接受范围内。增加上下文，使用户能够理解，例如解释模型的置信度。
- en: Return a result.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回结果。
- en: You can see this sequence of steps illustrated in [Figure 9-1](#streaming_workflow).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[图 9-1](#streaming_workflow)中看到这些步骤的示例。
- en: '![Streaming API workflow](assets/bmla_0901.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![流式API工作流程](assets/bmla_0901.png)'
- en: Figure 9-1\. Streaming API workflow
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1。流式API工作流程
- en: The endpoint approach is quick to implement but requires infrastructure to scale
    linearly with the current number of users, since each user leads to a separate
    inference call. If traffic increases beyond the capacity of a server to handle
    requests, they will start to be delayed or even fail. Adapting such a pipeline
    to traffic patterns thus requires being able to easily launch and shut down new
    servers, which will require some level of automation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 端点方法实现快速，但需要基础设施按当前用户数量线性扩展，因为每个用户会导致单独的推断调用。如果流量增加超出服务器处理请求的能力，则会开始延迟或甚至失败。因此，根据流量模式调整这样的流水线需要能够轻松启动和关闭新服务器，这将需要一定程度的自动化。
- en: For a simple demo such as the ML Editor, however, which is only meant to be
    visited by a few users at a time, a streaming approach is usually a good choice.
    To deploy the ML Editor, we use a lightweight Python web application such as [Flask](https://oreil.ly/cKLMn),
    which makes it easy to set up an API to serve a model with a few lines of code.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于像ML编辑器这样的简单演示，通常一次只需几个用户，流式方法通常是一个不错的选择。为了部署ML编辑器，我们使用轻量级Python Web应用程序，例如[Flask](https://oreil.ly/cKLMn)，它使得通过几行代码轻松设置一个API来为模型提供服务。
- en: 'You can find the deployment code for the prototype in the book’s [GitHub repository](https://github.com/hundredblocks/ml-powered-applications),
    but I’ll give a high-level overview here. The Flask application consists of two
    parts, an API that takes in requests and sends them to a model for processing
    using Flask, and a simple website built in HTML for users to input their text
    and to display results. Defining such an API does not require much code. Here,
    you can see two functions that handle the bulk of the work to serve the v3 of
    the ML Editor:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在书的[GitHub存储库](https://github.com/hundredblocks/ml-powered-applications)中找到原型的部署代码，但我将在这里进行高层次的概述。Flask应用程序由两部分组成，一个API接收请求并使用Flask将其发送到模型进行处理，另一个是用HTML构建的简单网站，用户可以在其中输入其文本并显示结果。定义这样的API不需要太多代码。在这里，你可以看到两个处理大部分工作的函数，用于为ML编辑器的v3版本提供服务：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The v3 function defines a route, which allows it to determine the HTML to display
    when a user accesses the `/v3` page. It uses the function `handle_text_request`
    to decide what to display. When a user first accesses the page, the request type
    is `GET` and so the function displays an HTML template. A screenshot of this HTML
    page is shown in [Figure 9-2](#flask_screenshot). If a user clicks the “Get recommendation”
    button, the request type is `POST`, so `handle_text_request` retrieves the question
    data, passes it to a model, and returns the model output.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: v3函数定义了一个路由，允许它确定当用户访问`/v3`页面时要显示的HTML。它使用`handle_text_request`函数来决定显示什么内容。当用户首次访问页面时，请求类型为`GET`，因此该函数显示HTML模板。此HTML页面的截图显示在[图9-2](#flask_screenshot)中。如果用户点击“获取推荐”按钮，则请求类型为`POST`，因此`handle_text_request`获取问题数据，将其传递给模型，并返回模型输出。
- en: '![Simple webpage to use a model](assets/bmla_0902A.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![使用模型的简单网页](assets/bmla_0902A.png)'
- en: Figure 9-2\. Simple webpage to use a model
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2\. 使用模型的简单网页
- en: A streaming application is required when strong latency constraints exist. If
    the information a model needs will be available only at prediction time and the
    model’s prediction is required immediately, you will need a streaming approach.
    For example, a model that predicts the price for a specific trip in a ride hailing
    app requires information about the user’s location and the current availability
    of drivers to make a prediction, which is available only at request time. Such
    a model also needs to output a prediction immediately, since it must be displayed
    to the user for them to decide whether to use the service.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在严格的延迟约束时，需要流式应用程序。如果模型需要的信息只在预测时可用，并且需要立即进行模型预测，则需要一种流式方法。例如，在打车应用中预测特定行程的价格时，需要用户位置信息和司机当前的可用性信息来进行预测，这些信息只在请求时可用。这样的模型还需要立即输出预测结果，因为必须显示给用户，以便他们决定是否使用该服务。
- en: In some other cases, the information required to compute predictions is available
    ahead of time. In those cases, it can be easier to process a large number of requests
    at once rather than processing them as they arrive. This is called *batch prediction*,
    and we will cover it next.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他一些情况下，计算预测所需的信息可以提前获得。在这些情况下，一次处理大量请求可能比随时处理更容易。这称为*批处理预测*，我们将在下面进行介绍。
- en: Batch Predictions
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量预测
- en: The batch approach considers the inference pipeline as a job that can be run
    on multiple examples at once. A batch job runs a model on many examples and stores
    predictions so they can be used when needed. Batch jobs are appropriate when you
    have access to the features needed for a model before the model’s prediction is
    required.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理方法将推断管道视为可以一次运行多个示例的作业。批处理作业在许多示例上运行模型并存储预测，以便在需要时使用。当您在模型的预测需要之前就能获得需要的特征时，批处理作业是适当的。
- en: For example, let’s say you’d like to build a model to provide each salesperson
    on your team with a list of companies that are the most valuable prospects to
    contact. This is a common ML problem called *lead scoring*. To train such a model,
    you could use features such as historical email conversations and market trends.
    Such features are available before a salesperson is deciding which prospect to
    contact, which is when a prediction is required. This means you could compute
    a list of prospects in a nightly batch job and have the results ready to be displayed
    by the morning, when they will be needed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您想要建立一个模型，为您团队的每个销售人员提供最有价值的潜在客户列表。这是一个常见的机器学习问题，称为*潜在客户评分*。为了训练这样的模型，您可以使用历史电子邮件对话和市场趋势等特征。在销售人员决定联系哪个潜在客户时，这些特征是可用的，这也是预测所需的时间点。这意味着您可以在夜间批处理作业中计算潜在客户列表，并在早晨准备好显示结果，这时它们将被需要。
- en: Similarly, an app that uses ML to prioritize and rank the most important message
    notifications to read in the morning does not have strong latency requirements.
    An appropriate workflow for this app would be to process all unread emails in
    a batch in the morning and save the prioritized list for when the user needs it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，一个利用机器学习来在早晨优先和排名最重要的消息通知以阅读的应用程序并不需要很强的延迟要求。这种应用程序的适当工作流程是在早晨批处理处理所有未读邮件，并保存优先列表以便用户需要时使用。
- en: A batch approach requires as many inference runs as a streaming approach, but
    it can be more resource efficient. Because predictions are done at a predetermined
    time and the number of predictions is known at the start of a batch, it is easier
    to allocate and parallelize resources. In addition, a batch approach can be faster
    at inference time since results have been precomputed and only need to be retrieved.
    This provides similar gains to caching.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与流式处理相比，批处理方法需要像流式处理一样多次推断运行，但可能更加资源高效。因为预测是在预定的时间进行的，并且在批处理开始时已知预测的数量，因此更容易分配和并行化资源。此外，批处理方法在推断时间上可能更快，因为结果已经预先计算并且只需检索。这提供了类似缓存的收益。
- en: '[Figure 9-3](#batch_workflow) shows the two sides of this workflow. At batch
    time, we compute predictions for all the data points and store the results we
    produce. At inference time, we retrieve the precomputed results.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-3](#batch_workflow)展示了这个工作流程的两个方面。在批处理时，我们计算所有数据点的预测并存储我们产生的结果。在推断时，我们检索预先计算的结果。'
- en: '![Example of batch workflow](assets/bmla_0903.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![批处理工作流程示例](assets/bmla_0903.png)'
- en: Figure 9-3\. Example of batch workflow
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3\. 批处理工作流程示例
- en: It is also possible to use a hybrid approach. Precompute in as many cases as
    possible and at inference time either retrieve precomputed results or compute
    them on the spot if they are not available or are outdated. Such an approach produces
    results as rapidly as possible, since anything that can be computed ahead of time
    will be. It comes with the cost of having to maintain both a batch pipeline and
    a streaming pipeline, which significantly increases the complexity of a system.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用混合方法。在尽可能多的情况下预先计算，在推断时要么检索预先计算的结果，要么在现场计算如果结果不可用或已过时。这种方法可以尽快产生结果，因为可以提前计算的任何内容都会被计算。但同时也需要维护批处理管道和流式处理管道，这显著增加了系统的复杂性。
- en: We’ve covered two common ways of deploying applications on a server, streaming
    and batch. Both of these approaches require hosting servers to run inference for
    customers, which can quickly become costly if a product becomes popular. In addition,
    such servers represent a central failure point for your application. If the demand
    for predictions increases suddenly, your servers may not be able to accommodate
    all of the requests.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了两种在服务器上部署应用程序的常见方式，即流式处理和批处理。这两种方法都需要托管服务器来为客户运行推断，如果产品变得流行起来，这很快会变得昂贵。此外，这些服务器代表了您应用程序的中心故障点。如果预测需求突然增加，您的服务器可能无法容纳所有请求。
- en: Alternatively, you could process requests directly on the devices of the clients
    making them. Having models run on users’ devices reduces inference costs and allows
    you to maintain a constant level of service regardless of the popularity of your
    application, since clients are providing the necessary computing resources. This
    is called *client-side deployment*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以直接在客户端设备上处理客户的请求。在用户设备上运行模型可以降低推断成本，并允许您保持服务水平的恒定，无论应用程序的流行程度如何，因为客户端提供了必要的计算资源。这称为*客户端部署*。
- en: Client-Side Deployment
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端部署
- en: The goal of deploying models on the client side is to run all computations on
    the client, eliminating the need for a server to run models. Computers, tablets,
    modern smartphones, and some connected devices such as smart speakers or doorbells
    have enough computing power to run models themselves.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端部署模型的目标是在客户端上运行所有计算，消除服务器运行模型的需求。计算机、平板电脑、现代智能手机以及一些连接设备如智能音箱或门铃具有足够的计算能力来自行运行模型。
- en: This section only covers *trained models* being deployed on device for inference,
    not training a model on the device. Models are still trained in the same manner
    and are then sent to the device for inference. The model can make its way to the
    device by being included in an app, or it can be loaded from a web browser. See
    [Figure 9-4](#on_device) for an example workflow to package a model in an application.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本节仅涵盖*在设备上部署的经过训练的模型*，而不是在设备上训练模型。模型仍然以相同的方式进行训练，然后发送到设备进行推断。模型可以通过包含在应用程序中的方式或从
    Web 浏览器加载来到达设备。请参见 [图 9-4](#on_device) 以了解在应用程序中打包模型的示例工作流程。
- en: '![A model running inference on device](assets/bmla_0904.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![模型在设备上运行推断](assets/bmla_0904.png)'
- en: Figure 9-4\. A model running inference on device (we can still train on a server)
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. 模型在设备上运行推断（我们仍然可以在服务器上进行训练）
- en: Pocket-sized devices offer more limited compute power than powerful servers,
    so this approach limits the complexity of the models that can be used, but having
    models run on device can offer multiple advantages.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 便携设备的计算能力比强大的服务器更有限，因此这种方法限制了可使用模型的复杂性，但在设备上运行模型可以提供多种优势。
- en: First, this reduces the need to build infrastructure that can run inference
    for every single user. In addition, running models on devices reduces the quantity
    of data that needs to be transferred between the device and the server. This reduces
    network latency and can even allow an application to run with no access to the
    network.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这减少了需要为每个用户运行推断的基础设施的需求。此外，将模型运行在设备上可以减少设备和服务器之间需要传输的数据量。这可以降低网络延迟，甚至允许应用程序在无网络访问的情况下运行。
- en: Finally, if the data required for inference contains sensitive information,
    having a model run on device removes the need for this data to be transferred
    to a remote server. Not having sensitive data on servers lowers the risk of an
    unauthorized third party accessing this data (see [“Data Concerns”](ch08.html#dat_concerns_eight)
    for why this can be a serious risk).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果推断所需的数据包含敏感信息，则在设备上运行模型可以消除将此数据传输到远程服务器的需求。不将敏感数据存储在服务器上可降低未经授权的第三方访问这些数据的风险（参见
    [“数据问题”](ch08.html#dat_concerns_eight) 为什么这可能是一个严重的风险）。
- en: '[Figure 9-5](#server_client_local) compares the workflow for getting a prediction
    to a user for server-side models and client-side models. At the top, you can see
    that the longest delay for a server-side workflow is often the time it takes to
    transfer data to the server. On the bottom, you can see that while client-side
    models incur next to no latency, they often process examples slower than servers
    because of hardware constraints.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-5](#server_client_local) 比较了为用户获取预测的服务器端模型和客户端模型的工作流程。在顶部，您可以看到服务器端工作流程的最长延迟通常是将数据传输到服务器所需的时间。在底部，您可以看到，虽然客户端模型几乎没有延迟，但由于硬件限制，它们处理示例的速度通常比服务器慢。'
- en: '![Running on a server, or locally](assets/bmla_0905.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![在服务器上运行，或本地运行](assets/bmla_0905.png)'
- en: Figure 9-5\. Running on a server, or locally
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. 在服务器上运行，或本地运行
- en: Just like for server-side deployment, there are multiple ways to deploy applications
    client side. In the following sections, we will cover two methods, deploying models
    natively and running them through the browser. These approaches are relevant for
    smartphones and tablets, which have access to an app store and web browser, but
    not for other connected devices such as microcontrollers, which we will not cover
    here.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 就像服务器端部署一样，有多种方法可以在客户端部署应用程序。在接下来的章节中，我们将涵盖两种方法，即本地部署模型和通过浏览器运行模型。这些方法适用于拥有应用商店和Web浏览器访问权限的智能手机和平板电脑，但不适用于其他连接设备，比如微控制器，在此我们将不进行介绍。
- en: On Device
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在设备上
- en: Processors in laptops and phones are not usually optimized to run ML models
    and so will execute an inference pipeline slower. For a client-side model to run
    quickly and without draining too much power, it should be as small as possible.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本电脑和手机中的处理器通常不会针对运行机器学习模型进行优化，因此会较慢地执行推断流水线。为了使客户端模型能够快速运行且不消耗过多电力，它应尽可能小。
- en: Reducing model size can be done by using a simpler model, reducing a model’s
    number of parameters or the precision of calculations. In neural networks, for
    example, weights are often pruned (removing those with values close to zero) and
    quantized (lowering the precision of weights). You may also want to reduce the
    number of features your model uses to further increase efficiency. In recent years,
    libraries such as [Tensorflow Lite](https://oreil.ly/GKYDs) have started providing
    useful tools to reduce the size of models and help make them more easily deployable
    on mobile devices.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 减小模型尺寸可以通过使用更简单的模型、减少模型参数数量或计算精度来实现。例如，在神经网络中，通常会对权重进行修剪（删除接近零值的权重）和量化（降低权重精度）。您可能还希望减少模型使用的特征数量，以进一步提高效率。近年来，诸如[Tensorflow
    Lite](https://oreil.ly/GKYDs)之类的库开始提供有用的工具，用于减小模型尺寸并帮助使其更容易在移动设备上部署。
- en: Because of these requirements, most models will suffer a slight performance
    loss by being ported on device. Products that cannot tolerate model performance
    degradation such as ones that rely on cutting-edge models that are too complex
    to be run on a device such as a smartphone should be deployed on a server. In
    general, if the time it would take to run inference on device is larger than the
    time it would take to transmit data to the server to be processed, you should
    consider running your model in the cloud.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些要求，大多数模型在移植到设备上时性能会稍微下降。那些不能容忍模型性能下降的产品，比如依赖于无法在智能手机等设备上运行的前沿模型的产品，应部署在服务器上。通常来说，如果在设备上运行推断所需的时间大于将数据传输到服务器进行处理所需的时间，您应考虑在云端运行您的模型。
- en: For other applications such as predictive keyboards on smartphones that offer
    suggestions to help type faster, the value of having a local model that does not
    need access to the internet outweighs the accuracy loss. Similarly, a smartphone
    application built to help hikers identify plants by taking a photo of them should
    work offline so that it can be used on a hike. Such an application would require
    a model to be deployed on device, even if it means sacrificing prediction accuracy.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他应用，比如在智能手机上提供帮助快速输入的预测键盘，具有无需访问互联网的本地模型的价值超过了精度损失。类似地，一款通过拍照帮助远足者识别植物的智能手机应用应支持离线工作，以便在远足中使用。这样的应用程序需要在设备上部署模型，即使这意味着牺牲预测精度。
- en: A translation app is another example of an ML-powered product that benefits
    from functioning locally. Such an app is likely to be used abroad where users
    may not have network access. Having a translation model that can run locally becomes
    a requirement, even if it isn’t as precise as a more complex one that could run
    only on a server.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译应用程序是另一个依赖于本地运行的机器学习驱动产品的例子。这样的应用程序可能会在用户无法访问网络的国外地区使用。因此，拥有可以本地运行的翻译模型成为一种需求，即使它不像只能在服务器上运行的更复杂模型那样精确。
- en: In addition to network concerns, running models in the cloud adds a privacy
    risk. Sending user data to the cloud and storing it even temporarily increases
    the odds of an attacker getting access to it. Consider an application as benign
    as superimposing filters on photos. Many users may not feel comfortable with their
    photos being transmitted to a server for processing and stored indefinitely. Being
    able to guarantee to users that their photos never leave the device is an important
    differentiator in an increasingly privacy conscious world. As we saw in [“Data
    Concerns”](ch08.html#dat_concerns_eight), the best way to avoid putting sensitive
    data at risk is making sure it never leaves the device or gets stored on your
    servers.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 除了网络问题外，将模型运行在云端会增加隐私风险。将用户数据发送到云端并临时存储会增加攻击者访问数据的可能性。考虑到一个看似无害的应用，比如在照片上叠加滤镜。许多用户可能不希望他们的照片被传输到服务器进行处理并永久存储。在越来越注重隐私的世界中，向用户保证他们的照片永远不会离开设备是一个重要的差异化点。正如我们在[“数据问题”](ch08.html#dat_concerns_eight)中看到的，避免将敏感数据置于风险之中的最佳方法是确保它永远不离开设备或存储在您的服务器上。
- en: On the other hand, quantizing pruning and simplifying a model is a time-consuming
    process. On-device deployment is only worthwhile if the latency, infrastructure,
    and privacy benefits are valuable enough to invest the engineering effort. For
    the ML Editor, we will limit ourselves to a web-based streaming API.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，量化、修剪和简化模型是一个耗时的过程。只有在延迟、基础设施和隐私好处值得投入工程工作的情况下，设备端部署才是值得的。对于ML编辑器，我们将限制在基于web的流媒体API。
- en: Finally, optimizing models specifically so they run on a certain type of device
    can be time-consuming, as the optimization process may differ between devices.
    More options exist to run models locally, including ones that leverage commonalities
    between devices to reduce required engineering work. An exciting area in this
    domain is ML in the browser.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，专门优化模型以在特定类型设备上运行可能是耗时的，因为优化过程可能因设备而异。有更多选项可以在本地运行模型，包括利用设备之间的共同点来减少所需的工程工作。在这一领域中一个令人兴奋的地方是浏览器中的机器学习。
- en: Browser Side
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 浏览器端
- en: Most smart devices have access to a browser. These browsers have often been
    optimized to support fast graphical calculations. This has led to rising interest
    in libraries that use browsers to have the client perform ML tasks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数智能设备都可以访问浏览器。这些浏览器通常已经经过优化，支持快速的图形计算。这导致了对使用浏览器让客户端执行机器学习任务的库的兴趣日益增加。
- en: The most popular of these frameworks is [Tensorflow.js](https://www.tensorflow.org/js),
    which makes it possible to train and run inference in JavaScript in the browser
    for most differentiable models, even ones that were trained in different languages
    such as Python.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些框架中最流行的是[Tensorflow.js](https://www.tensorflow.org/js)，它使得在浏览器中使用JavaScript进行大多数可微分模型的训练和推断成为可能，甚至可以处理用不同语言如Python训练的模型。
- en: This allows users to interact with models through the browser without needing
    to install any additional applications. In addition, since models run in the browser
    using JavaScript, computations are done on the user’s device. Your infrastructure
    only needs to serve the web page that includes the model weights. Finally, Tensorflow.js
    supports WebGL, which allows it to leverage GPUs on the clients’ device if they
    are available to make computations faster.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用户能够通过浏览器与模型进行交互，而无需安装任何额外的应用程序。此外，由于模型在浏览器中使用JavaScript运行，计算是在用户的设备上完成的。您的基础设施只需提供包含模型权重的网页。最后，Tensorflow.js支持WebGL，这使它能够利用客户端设备上的GPU，从而加快计算速度。
- en: Using a JavaScript framework makes it easier to deploy a model on the client
    side without requiring as much device-specific work as the previous approach.
    This approach does come with the drawback of increasing bandwidth costs, since
    the model will need to be downloaded by clients each time they open the page as
    opposed to once when they install the application.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用JavaScript框架可以更轻松地在客户端部署模型，而无需像以前的方法那样进行太多的设备特定工作。但是，这种方法的缺点是增加了带宽成本，因为每次客户端打开页面时都需要下载模型，而不是在安装应用程序时仅下载一次。
- en: As long as the models you use are a few megabytes or smaller and can be downloaded
    quickly, using JavaScript to run them on the client can be a useful way to lower
    server costs. If server costs ever became an issue for the ML Editor, deploying
    the model using a framework like Tensorflow.js would be one of the first methods
    I would recommend exploring.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 只要您使用的模型几兆字节或更小，并且可以快速下载，使用JavaScript在客户端上运行它们可以是降低服务器成本的有效方法。如果服务器成本对于ML编辑器成为问题，我会建议首先探索使用Tensorflow.js等框架部署模型的方法。
- en: So far, we’ve considered clients purely to deploy models that have already been
    trained, but we could also decide to train models on clients. In the next part,
    we will explore when this could be useful.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们考虑的客户端纯粹是为了部署已经训练过的模型，但我们也可以决定在客户端训练模型。在接下来的部分，我们将探讨这种做法何时会有用。
- en: 'Federated Learning: A Hybrid Approach'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联邦学习：混合方法
- en: We have mostly covered different ways to deploy models that we have already
    trained (ideally by following the guidelines in the previous chapters) and that
    we are now choosing how to deploy. We have looked at different solutions for getting
    a unique model in front of all our users, but what if we wanted each user to have
    a different model?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要涵盖了已经训练的模型的不同部署方式（理想情况下是按照前几章的指南），现在我们正在选择如何部署。我们已经看过了让所有用户面前都有一个独特模型的不同解决方案，但如果我们希望每个用户都有不同的模型怎么办？
- en: '[Figure 9-6](#one_big_many_small) shows the difference between a system at
    the top that has a common trained model for all users and one at the bottom where
    each user has a slightly different version of the model.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-6](#one_big_many_small)展示了顶部系统中所有用户都有一个共同训练模型的区别，以及底部每个用户都有略微不同版本模型的情况。'
- en: '![One big model or many individual ones](assets/bmla_0906.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![一个大模型或许多个个体模型](assets/bmla_0906.png)'
- en: Figure 9-6\. One big model or many individual ones
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-6\. 一个大模型或许多个个体模型
- en: For many applications such as content recommendation, giving writing advice,
    or healthcare, a model’s most important source of information is the data it has
    about the user. We can leverage this fact by generating user-specific features
    for a model, or we can decide that each user should have their own model. These
    models can all share the same architecture, but each user’s model will have different
    parameter values that reflect their individual data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用程序，如内容推荐、提供写作建议或医疗保健，模型最重要的信息来源是其对用户的数据。我们可以通过为模型生成用户特定的特征，或者决定每个用户都应该有自己的模型来利用这一事实。这些模型可以共享相同的架构，但每个用户的模型将具有反映其个体数据的不同参数值。
- en: This idea is at the core of federated learning, an area of deep learning that
    has been getting increasing attention recently with projects such as [OpenMined](https://www.openmined.org/).
    In federated learning, each client has their own model. Each model learns from
    their user’s data and sends aggregated (and potentially anonymized) updates to
    the server. The server leverages all updates to improve its model and distills
    this new model back to individual clients.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是联邦学习的核心，这是一个深度学习领域，近年来引起越来越多的关注，例如[OpenMined](https://www.openmined.org/)项目。在联邦学习中，每个客户端都有自己的模型。每个模型从其用户数据中学习，并将聚合（可能匿名化）的更新发送到服务器。服务器利用所有更新来改进其模型，并将这个新模型提炼回个体客户端。
- en: Each user receives a model that is personalized to their needs, while still
    benefiting from aggregate information about other users. Federated learning improves
    privacy for users because their data is never transferred to the server, which
    only receives aggregated model updates. This stands in contrast to training a
    model the traditional way by collecting data about each user and storing all of
    it on a server.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 每个用户接收到根据其需求个性化的模型，同时仍然从其他用户的汇总信息中受益。联邦学习提升了用户的隐私，因为他们的数据从不传输到服务器，服务器仅接收聚合的模型更新。这与通过收集每个用户数据并将其全部存储在服务器上来训练模型的传统方式形成鲜明对比。
- en: Federated learning is an exciting direction for ML, but it does add an additional
    layer of complexity. Making sure that each individual model is performing well
    and that the data transmitted back to the server is properly anonymized is more
    complicated than training a single model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习是机器学习的一个令人兴奋的方向，但它增加了额外的复杂性。确保每个个体模型表现良好，并且传输回服务器的数据得到适当的匿名化，比训练单一模型更复杂。
- en: Federated learning is already used in practical applications by teams that have
    the resources to deploy it. For example, as described in this article by A. Hard
    et al., [“Federated Learning for Mobile Keyboard Prediction”](https://arxiv.org/abs/1811.03604),
    Google’s GBoard uses federated learning to provide next-word predictions for smartphone
    users. Because of the diversity of writing styles among users, building a unique
    model that performs well for all users proved challenging. Training models at
    the user level allows GBoard to learn about user-specific patterns and to provide
    better predictions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习已经被那些有资源部署它的团队在实际应用中使用。例如，正如A. Hard等人在本文中所述，[“联邦学习用于移动键盘预测”](https://arxiv.org/abs/1811.03604)，Google的GBoard使用联邦学习为智能手机用户提供下一个单词预测。由于用户之间写作风格的多样性，构建一个适合所有用户且性能良好的唯一模型是具有挑战性的。在用户级别训练模型使GBoard能够了解用户特定的模式，并提供更好的预测。
- en: We’ve covered multiple ways to deploy models on servers, on devices, or even
    on both. You should consider each approach and its trade-offs based on the requirements
    of your application. As with other chapters in this book, I encourage you to start
    with a simple approach and move to a more complex one only once you’ve validated
    that it is necessary.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了在服务器、设备甚至两者上部署模型的多种方法。您应根据应用程序的要求考虑每种方法及其权衡。与本书其他章节一样，我建议您从简单的方法开始，并仅在验证必要性后才转向更复杂的方法。
- en: Conclusion
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: There are multiple ways to serve an ML-powered application. You can set up a
    streaming API to allow a model to process examples as they arrive. You can use
    a batch workflow that will process multiple data points at once on a regular schedule.
    Alternatively, you can choose to deploy your models on the client side by either
    packaging them in an application or serving them through a web browser. Doing
    so would lower your inference costs and infrastructure needs but make your deployment
    process more complex.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以为基于机器学习的应用程序提供服务。您可以设置流式 API，使模型能够处理到达的示例。您可以使用批处理工作流，定期一次性处理多个数据点。或者，您可以选择在客户端部署模型，方法是将它们打包到应用程序中或通过
    Web 浏览器提供服务。这样做可以降低推断成本和基础设施需求，但会使部署过程更加复杂。
- en: The right approach depends on your application’s needs, such as latency requirements,
    hardware, network and privacy concerns, and inference costs. For a simple prototype
    like the ML Editor, start with an endpoint or a simple batch workflow and iterate
    from there.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的方法取决于应用程序的需求，例如延迟要求、硬件、网络和隐私问题以及推断成本。对于像ML Editor这样的简单原型，建议从端点或简单的批处理工作流开始，并从中迭代。
- en: Deploying a model comes with more than just exposing it to users, however. In
    [Chapter 10](ch10.html#model_engineering), we will cover methods to build safeguards
    around models to mitigate errors, engineering tools to make the deployment process
    more effective, and approaches to validate that models are performing the way
    they should be.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 部署模型不仅仅是让用户接触到它。在[第10章](ch10.html#model_engineering)中，我们将介绍围绕模型构建保护措施以减少错误的方法、工程工具以提高部署过程的效率，以及验证模型表现是否符合预期的方法。
