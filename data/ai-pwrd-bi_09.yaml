- en: Chapter 9\. Leveraging Unstructured Data with AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 利用AI处理非结构化数据
- en: In the previous chapters, we used AI quite a lot with structured data, or as
    most people call it, tables. However, a lot of data in businesses is not actually
    stored in clean tables, but comes in a plethora of formats such as PDFs, images,
    raw text, websites, and emails. When you consider these formats, the majority
    of data available within organizations is unstructured. With AI, we can unlock
    these treasure troves and get insights from data that has hardly been touched
    before by analysts or data that otherwise needs a lot of manual effort before
    anyone can get insights from it. In this chapter, we’ll explore how AI can help
    us analyze texts, documents, and image files.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们在处理结构化数据时大量使用了AI，或者大多数人称之为表格。然而，企业中的许多数据实际上并未存储在干净的表格中，而是以各种格式存在，如PDF、图像、原始文本、网站和电子邮件。当考虑到这些格式时，组织内可用的大多数数据是非结构化的。借助AI，我们可以解锁这些宝藏，并从以前未被分析师触及或需要大量手动工作才能从中获得见解的数据中获取洞察。在本章中，我们将探讨AI如何帮助我们分析文本、文档和图像文件。
- en: 'Use Case: Getting Insights from Text Data'
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用案例：从文本数据中获取洞察
- en: Written language is one of the biggest and most diverse data sources humanity
    has collected. And businesses are no exception. The biggest creators of data are
    people, either within or outside organizations. Customers become content producers
    and share their opinions about products or services across the web and on various
    channels.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 书面语言是人类收集的最大和最多样化的数据来源之一。企业也不例外。数据的最大创造者是人，无论是在组织内还是外部。顾客成为内容生产者，通过网络和各种渠道分享他们对产品或服务的意见。
- en: In this use case, we are going to deploy an AI service that will help us make
    sense of this data. In the concrete problem at hand, we are going to analyze user
    reviews at scale and communicate key insights through a BI dashboard. Let’s go!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个使用案例中，我们将部署一个AI服务来帮助我们理解这些数据。在具体的问题中，我们将分析用户评论的规模，并通过BI仪表板传达关键见解。让我们开始吧！
- en: Problem Statement
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: Small rooms, unfriendly staff, and horrible breakfast—or not? The management
    of a large hotel is strained by the variety of customer feedback. Are there really
    problems that management needs to address, or are there just sporadic complaints
    that they have to accept, as someone running an accommodation business? The head
    of operations has hired you as an external analyst to find out what customers
    think about the hotel and how this trend has developed over time.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 小房间、不友好的员工和可怕的早餐——或者并非如此？一家大型酒店的管理团队因各种客户反馈的多样性而感到紧张。管理层是否真的需要解决问题，还是只是不得不接受零星的投诉，作为经营住宿业务的一部分？运营负责人已聘请您作为外部分析师，以了解客户对酒店的看法以及这一趋势随时间的发展情况。
- en: As a data source, the company provides a sample of text files of customer feedback
    that they have collected over time through the hotel’s website and gathered from
    booking portals. Since the new season is just about to start, management wants
    the results today rather than tomorrow, so speed is clearly prioritized over accuracy.
    The management staff wants to know whether something is fundamentally wrong and
    to have the capability to dive in deeper if necessary.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据来源，公司提供了通过酒店网站和预订平台收集的客户反馈文本文件样本。由于新季即将开始，管理层希望尽快看到结果，速度明显优先于准确性。管理人员希望了解是否存在根本性问题，并有能力在必要时深入了解。
- en: Solution Overview
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案概述
- en: Take a look at the high-level use case architecture in [Figure 9-1](#use_case_architecture_for_getting_insig).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[图9-1](#use_case_architecture_for_getting_insig)中的高层次使用案例架构。
- en: '![Use case architecture for getting insights from text data](Images/apbi_0901.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![从文本数据中获取洞察的使用案例架构](Images/apbi_0901.png)'
- en: Figure 9-1\. Use case architecture for getting insights from text data
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1 从文本数据中获取洞察的使用案例架构
- en: From a first glance at this architecture, you should immediately recognize that
    the analysis layer seems to be pretty simple. The reason is that, in order to
    analyze many text files automatically, we will use an NLP AI service. As you have
    seen previously, this AI service doesn’t need any training; instead, we can send
    data to the service and get a response right away.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从对这种架构的第一眼看，您应该立即认识到分析层似乎非常简单。原因是，为了自动分析许多文本文件，我们将使用NLP AI服务。正如您之前所见，这种AI服务不需要任何训练；相反，我们可以将数据发送到服务，并立即获得响应。
- en: In this case, our goal is to extract information about whether opinions in customer
    reviews stored as text files are negative or positive, which is also called *sentiment
    analysis*. Furthermore, we want to extract keywords so we can relate back to word
    phrases that carry a positive or negative emotion. The presentation should be
    in the form of a BI dashboard, in our case Power BI (user layer).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的目标是提取关于存储为文本文件的客户评论中的意见是消极还是积极的信息，这也被称为*情感分析*。此外，我们希望提取关键词，以便我们可以关联到带有积极或消极情绪的词组。展示应以
    BI 仪表板的形式呈现，在我们的情况下是 Power BI（用户层）。
- en: The most challenging part of this use case is in the data layer. We need to
    get the data into the right shape, send it to the AI service, and retrieve the
    results in a structured, tabular way so our BI system can handle them. We will
    achieve that by building a small data processing pipeline using a script in Azure
    Notebooks on Azure ML Studio that loads the files, calls the AI service, transforms
    the results, and exports flat CSV files that can be then consumed by our BI system
    (Power BI).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此用例中最具挑战性的部分在于数据层。我们需要将数据整理成正确的格式，将其发送到 AI 服务，并以结构化表格的方式检索结果，以便我们的 BI 系统可以处理它们。我们将通过在
    Azure Notebooks 上使用 Azure ML Studio 中的脚本构建一个小型数据处理流水线来实现这一目标，该流水线加载文件，调用 AI 服务，转换结果，并导出可以被我们的
    BI 系统（Power BI）消费的扁平 CSV 文件。
- en: Setting Up the AI Service
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 AI 服务
- en: First of all, we need to activate the Cognitive Services Text Analytics in our
    Azure subscription. This step is straightforward and similar to what we’ve done
    previously in other chapters. To activate Cognitive Services Text Analytics, go
    to your [Azure portal](https://portal.azure.com) and search for `**cognitive services**`
    in the search bar, as shown in [Figure 9-2](#searching_for_azure_cognitive_services).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在我们的 Azure 订阅中激活认知服务文本分析。这一步骤非常简单，类似于我们在其他章节中做过的操作。要激活认知服务文本分析，请转到您的
    [Azure 门户](https://portal.azure.com)，并在搜索栏中搜索`**cognitive services**`，如 [图 9-2](#searching_for_azure_cognitive_services)
    所示。
- en: '![Searching for Azure Cognitive Services](Images/apbi_0902.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![搜索 Azure 认知服务](Images/apbi_0902.png)'
- en: Figure 9-2\. Searching for Azure Cognitive Services
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. 搜索 Azure 认知服务
- en: Select Cognitive Services from the suggestions list and head over to the corresponding
    resource page. At this page, you can enable Cognitive Services for all kinds of
    data and use cases. Scroll down until you see the Language section, shown in [Figure 9-3](#cognitive_services_for_language).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从建议列表中选择认知服务，然后转到相应的资源页面。在此页面，您可以为各种数据和用例启用认知服务。向下滚动直到看到语言部分，如 [图 9-3](#cognitive_services_for_language)
    所示。
- en: '![Cognitive Services for Language](Images/apbi_0903.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![语言认知服务](Images/apbi_0903.png)'
- en: Figure 9-3\. Cognitive Services for Language
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3\. 语言认知服务
- en: From here, you can deploy a new Language Service resource by clicking the “+
    Create” button. In the following form, which you should be quite used to by now,
    select your Azure subscription and the resource group, and give your Language
    Service a name. Also, make sure that you choose the Free Tier (F0), which allows
    5,000 transactions per 30 days. One transaction is a text record that corresponds
    to the number of 1,000-character units within a document that is provided as input
    to a Language Service API—that’s more than enough for our use case here. You can
    see an example of the filled-in form in [Figure 9-4](#creating_a_text_analytics_service_form).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，您可以通过点击“+ 创建”按钮部署新的语言服务资源。在接下来的表单中，您应该已经非常熟悉了，选择您的 Azure 订阅和资源组，并为您的语言服务命名。此外，请确保选择免费层（F0），该层允许每
    30 天进行 5,000 次交易。一次交易是一个文本记录，对应于作为输入提供给语言服务 API 的文档中的 1,000 个字符单位数量——对于我们这里的用例来说，这已经足够了。您可以在
    [图 9-4](#creating_a_text_analytics_service_form) 中看到填写好的表单示例。
- en: Click “Review + create” and then Create after the automatic review process has
    passed. The deployment will take a few minutes, but after that you should see
    a notification prompting that the new resource is ready.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“审核 + 创建”，然后在自动审核流程通过后点击创建。部署将需要几分钟时间，但之后您应该会看到一条通知，提示新资源已准备就绪。
- en: '![Creating a text analytics service form](Images/apbi_0904.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![创建文本分析服务表单](Images/apbi_0904.png)'
- en: Figure 9-4\. Creating a text analytics service form
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. 创建文本分析服务表单
- en: After the deployment is finished, navigate to the new resource either by clicking
    the notification, or—if you left the page—simply by searching for the resource
    name you provided in the form in the Azure search bar. In both cases, you should
    be greeted with the resource’s overview page ([Figure 9-5](#text_analytics_quick_start)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 部署完成后，导航到新资源，可以通过单击通知或者（如果您离开了页面）只需在 Azure 搜索栏中搜索您在表单中提供的资源名称来实现。 在这两种情况下，您应该会看到资源的概述页面（[图9-5](#text_analytics_quick_start)）。
- en: '![Text analytics overview page](Images/apbi_0905.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![文本分析概述页面](Images/apbi_0905.png)'
- en: Figure 9-5\. Text analytics overview page
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5\. 文本分析概述页面
- en: We can do many things here, but for now we just want to focus on two. First,
    how do we authenticate against the API? Second, where do we find out how the API
    works (which code we need to write—or better, copy and paste)?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里做很多事情，但现在我们只想专注于两件事。 首先，我们如何对 API 进行身份验证？ 其次，我们在哪里找到 API 的工作原理（我们需要编写或更好地复制和粘贴的代码）？
- en: To answer the first question, you can click the Keys and Endpoint option in
    the left menu pane. Here you will find two keys, just as you did in the Azure
    Personalizer example in [Chapter 8](ch08.xhtml#ai_powered_prescriptive_analytics).
    You will need only one of these to call the API. Leave this window open in a new
    tab since we will soon need these resource keys.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答第一个问题，您可以在左侧菜单窗格中单击“键和端点”选项。 在这里，您会找到两个密钥，就像在 [第8章](ch08.xhtml#ai_powered_prescriptive_analytics)
    中的 Azure Personalizer 示例中一样。 您只需要其中一个来调用 API。 在新标签页中保持此窗口打开，因为我们很快将需要这些资源密钥。
- en: To find out how the API works, you can explore the resources under “Develop”
    on the overview page. This will bring you to a site that explains various scenarios
    where you could use the Text Analytics API and the code you would use. This might
    seem like a lot for now, but don’t get intimidated. You won’t need all of this
    information. For now, it is enough to acknowledge that the majority of the code
    we are writing later in the data preparation step can be taken from this documentation
    page and adapted to your own needs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 API 的工作原理，您可以在概述页面的“开发”下探索资源。 这将带您到一个网站，解释了各种情景，您可以在文本分析 API 和您将使用的代码中使用这些情景。
    现在可能会有点多，但不要被吓到。 您不需要所有这些信息。 现在，认识到我们稍后在数据准备步骤中编写的大部分代码可以从此文档页面获取，并根据自己的需求进行调整，就足够了。
- en: Looking through the documentation for text classification, we can discover some
    more useful information regarding our new AI service. Navigate to [Concepts →
    Data limits](https://oreil.ly/9j6lt), and you should find a table ([Figure 9-6](#text_analytics_request_limits)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览文本分类的文档时，我们可以发现有关我们新 AI 服务的一些更有用的信息。 转到 [概念 → 数据限制](https://oreil.ly/9j6lt)，您应该会找到一张表（[图9-6](#text_analytics_request_limits)）。
- en: '![Text analytics request limits](Images/apbi_0906.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![文本分析请求限制](Images/apbi_0906.png)'
- en: Figure 9-6\. Text analytics request limits
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6\. 文本分析请求限制
- en: If we look at this table and locate the row Sentiment Analysis, we can see that
    the API accepts 10 documents per request. We can send up to 10 text files at once
    to the API, which will greatly speed up our inference process, compared to sending
    the text files to the API one by one. We will come back to this during the data
    processing.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看此表，并定位到情绪分析行，我们可以看到 API 接受每次请求的10个文档。 我们可以一次性向 API 发送最多10个文本文件，与逐个发送文本文件到
    API 相比，这将大大加快我们的推理过程。 我们将在数据处理期间回到这一点。
- en: Now, everything has been set up for the AI service. Let’s head over to build
    our mini data pipeline and run some inference on the customer feedback data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，AI 服务的一切都已经准备就绪。 让我们继续构建我们的迷你数据管道，并对客户反馈数据进行一些推理。
- en: Setting Up the Data Pipeline
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置数据管道
- en: To get from raw text files to a beautifully designed BI dashboard, we have to
    solve some intermediate steps. In a production environment, this would be called
    our *extract, transform, load* (*ETL*) process. For our prototype, we won’t go
    as far as naming this a fully fledged ETL process, but in essence we are doing
    just that. Let’s call it our mini ETL job.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要从原始文本文件到设计精美的 BI 仪表板，我们必须解决一些中间步骤。 在生产环境中，这将被称为我们的*提取，转换，加载*（*ETL*）过程。 对于我们的原型，我们不会将其称为完全成熟的
    ETL 过程，但本质上我们正在做同样的事情。 让我们称之为我们的迷你 ETL 作业。
- en: 'We will write a short script to handle the essential parts of an ETL process:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写一个简短的脚本来处理 ETL 过程的基本部分：
- en: Read plain-text files from a file.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文件中读取纯文本文件。
- en: Send the file contents to the AI service API.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件内容发送到 AI 服务 API。
- en: Collect the results, transform them, and store them in a structured data object.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集结果，转换它们，并将它们存储在结构化的数据对象中。
- en: Export the files as flat tables so they can be easily consumed by our BI software.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件导出为扁平表格，以便我们的BI软件可以轻松消费。
- en: For step 1, we need to get the text files and store them someplace where our
    script can access them. We will use Azure Blob Storage for this. Navigate to the
    [book’s website](https://oreil.ly/0uHwu) and download the *reviews.zip* file.
    In your Azure portal, navigate to storage, select the storage account you have
    set up previously, and create a new container named `**texts**`, as shown in [Figure 9-7](#creating_a_new_container_in_azure_blob).
    This container will hold the raw input text files. The container “tables” should
    already be there from [Chapter 4](ch04.xhtml#prototyping) and will receive the
    tabular output of our mini ETL process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一步，我们需要获取文本文件并将它们存储在脚本可以访问的地方。我们将使用Azure Blob Storage进行存储。前往[书籍网站](https://oreil.ly/0uHwu)，下载*reviews.zip*文件。在Azure门户中，导航到存储，选择您之前设置的存储帐户，并创建一个名为`**texts**`的新容器，如[图 9-7](#creating_a_new_container_in_azure_blob)所示。该容器将保存原始输入文本文件。“表”容器应该已经存在于[第4章](ch04.xhtml#prototyping)中，并将接收我们的迷你ETL过程的表格化输出。
- en: '![Creating a new container in Azure Blob Storage](Images/apbi_0907.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![在Azure Blob Storage中创建新容器](Images/apbi_0907.png)'
- en: Figure 9-7\. Creating a new container in Azure Blob Storage
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-7\. 在Azure Blob Storage中创建新容器
- en: After creating the “texts” container, upload all the individual text files here.
    Don’t upload the ZIP file, but its contents. Your container should now look like
    [Figure 9-8](#azure_storage_container_with_review_dat).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 创建“texts”容器后，将所有单独的文本文件上传到这里。不要上传ZIP文件，而是其内容。您的容器现在应如[图 9-8](#azure_storage_container_with_review_dat)所示。
- en: '![Azure storage container with review data](Images/apbi_0908.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![带有审查数据的Azure存储容器](Images/apbi_0908.png)'
- en: Figure 9-8\. Azure storage container with review data
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-8\. 带有审查数据的Azure存储容器
- en: Our input data is now ready to be consumed by a script. You can leave the other
    container “tables” empty for now. We will populate it through our script, which
    we will take a look at next.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的输入数据已准备好供脚本消费。暂时可以将其他“表”容器保持为空。我们将通过下面将要查看的脚本来填充它。
- en: Download the *ETL_For_Text_Analytics.ipynb* file from the [book’s website](https://oreil.ly/0uHwu).
    This notebook contains all the code you will need to execute the mini ETL process.
    If you have a local IDE such as Jupyter Notebooks installed on your computer,
    you can go ahead and use this. If not, I suggest using Azure Notebooks as a hosted
    IDE as it comes as part of your Azure subscription.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从[书籍网站](https://oreil.ly/0uHwu)下载*ETL_For_Text_Analytics.ipynb*文件。该笔记本包含您执行迷你ETL过程所需的所有代码。如果您已经在计算机上安装了本地IDE，如Jupyter笔记本，可以直接使用它。如果没有，我建议使用Azure笔记本作为托管IDE，因为它是Azure订阅的一部分。
- en: I will walk you through the example using Azure Notebooks, but the same steps
    apply when you use your local IDE instead.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我将指导您使用Azure笔记本的示例，但是当您使用本地IDE时，步骤相同。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We will use some Azure packages needed in this code for authentication and blob
    storage that are currently not supported in R. While it is still possible to execute
    this workflow in R, it would be very heavy. So in this case, I provide only the
    Python version. All R users, please follow along here.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一些Azure包进行此代码所需的身份验证和Blob存储，这些包目前不支持R语言。虽然在R中仍然可以执行此工作流程，但会非常耗费资源。因此，在这种情况下，我仅提供Python版本。所有使用R的用户，请跟随这里。
- en: 'Let’s start by creating a new Azure notebook:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个新的Azure笔记本：
- en: Navigate to [Microsoft Azure Machine Learning Studio](https://ml.azure.com)
    and select the workspace you have been using throughout the book.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往[Microsoft Azure机器学习工作室](https://ml.azure.com)，并选择您在整本书中一直使用的工作区。
- en: Choose Notebooks from the menu on the left.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左侧菜单中选择笔记本。
- en: Click the plus icon and choose “Upload files,” as shown in [Figure 9-9](#uploading_files_to_the_notebook_environ).
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击加号图标，并选择“上传文件”，如[图 9-9](#uploading_files_to_the_notebook_environ)所示。
- en: Locate *ETL_For_Text_Analytics.ipynb* on your computer, check the “I trust the
    contents of this file” option, and click Upload.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的计算机上找到*ETL_For_Text_Analytics.ipynb*，选中“我信任该文件的内容”选项，然后点击上传。
- en: '![Uploading files to the notebook environment](Images/apbi_0909.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![将文件上传到笔记本环境](Images/apbi_0909.png)'
- en: Figure 9-9\. Uploading files to the notebook environment
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-9\. 将文件上传到笔记本环境中
- en: You should see the notebook, as shown in [Figure 9-10](#etl_notebook).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到笔记本，如[图 9-10](#etl_notebook)所示。
- en: '![ETL notebook](Images/apbi_0910.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![ETL笔记本](Images/apbi_0910.png)'
- en: Figure 9-10\. ETL notebook
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-10\. ETL笔记
- en: The notebook will be displayed on the right side. While this file contains quite
    a lot of code, don’t get intimidated by it. In fact, the only things you need
    to modify are the custom parameters for your access credentials and AI service
    endpoint in lines 2, 3, and 6 in section 0, Packages and Setup—just as you did
    in [Chapter 8](ch08.xhtml#ai_powered_prescriptive_analytics).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本将显示在右侧。虽然此文件包含大量代码，但不要被吓到。事实上，您唯一需要修改的是第0节“Packages and Setup”中第2、3和6行的访问凭据和AI服务端点的自定义参数，就像您在[第8章](ch08.xhtml#ai_powered_prescriptive_analytics)中所做的那样。
- en: As an intermediate solution between letting you run the whole notebook at once,
    trusting that it will somehow work out, and discussing the code line by line,
    I will give you a high-level overview of what this code is doing by explaining
    the code sections. I recommend following along and executing the code cells.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作为在让您一次运行整个笔记本和逐行讨论代码之间的中间解决方案，我将通过解释代码部分来为您提供此代码正在执行的高级概述。我建议您跟着执行代码单元格。
- en: 'This code has four main parts that roughly align to the four steps I mentioned
    previously during the ETL process:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码分为四个主要部分，大致对应我之前在ETL过程中提到的四个步骤：
- en: 'Download text files from Azure: we will download the raw text files from Azure
    to the local machine that is running the notebook.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Azure下载文本文件：我们将从Azure下载原始文本文件到运行笔记本的本地计算机。
- en: 'Call AI service (sentiment analysis): we will send the text files to the AI
    services and save the results.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用AI服务（情感分析）：我们将发送文本文件到AI服务并保存结果。
- en: 'Transform AI results to CSVs: we will transform the AI results from JSON outputs
    to flat CSV files so they can be consumed easily by our BI.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将AI结果转换为CSV：我们将AI结果从JSON输出转换为平面CSV文件，以便我们的BI系统能够轻松消化。
- en: 'Upload CSVs to Azure Blob Storage: we will upload the files back to Azure Blob
    Storage that is connected to our BI.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将CSV上传到Azure Blob Storage：我们将文件上传回连接到我们的BI系统的Azure Blob Storage中。
- en: Before we start, make sure that the notebook is connected to a compute resource
    that is running. If you have not created a compute resource yet, refer to [“Create
    an Azure Compute Resource”](ch04.xhtml#create_an_azure_compute_resource).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保笔记本连接到正在运行的计算资源。如果尚未创建计算资源，请参考[“创建Azure计算资源”](ch04.xhtml#create_an_azure_compute_resource)。
- en: If you are asked to authenticate again after the compute resource has started,
    just click the button that shows up to authenticate. You should then see a success
    prompt ([Figure 9-11](#authentication_prompt)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在计算资源启动后再次要求进行身份验证，请单击显示身份验证按钮。然后，您应该看到成功提示（[图 9-11](#authentication_prompt)）。
- en: '![Authentication prompt](Images/apbi_0911.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![身份验证提示](Images/apbi_0911.png)'
- en: Figure 9-11\. Authentication prompt
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-11\. 身份验证提示
- en: Now, head over to section 0 of the code. This is the place where we will make
    sure that all packages that we need are installed and loaded and, more importantly,
    your personal Azure credentials are provided. This is the only place in the whole
    notebook where you need to change something. Replace the dummy strings for the
    key, endpoint, and the Azure connection with your custom values from the Azure
    portal of the Text Analytics Cognitive Service and the Storage account, respectively.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请前往代码的第0节。这是我们确保所有需要的包都已安装和加载，更重要的是，提供您的个人Azure凭据的地方。这是整个笔记本中您需要更改内容的唯一位置。用来自Text
    Analytics Cognitive Service和Storage账户的Azure门户自定义值替换密钥、端点和Azure连接的虚拟字符串。
- en: Once you’ve updated your personal credentials, run the first code cell by placing
    the cursor inside it and pressing Shift+Enter. The output of this code should
    be relatively simple. All packages should have been installed on the Azure compute
    resource already, and you should simply see the `Requirement already satisfied`
    notifications, as shown in [Figure 9-12](#notebook_output_for_package_installs).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 更新了个人凭证后，通过将光标放在第一个代码单元格内并按Shift+Enter来运行第一个代码单元格。此代码的输出应该相对简单。所有包应该已经安装在Azure计算资源上，您应该只看到`Requirement
    already satisfied`的通知，如[图 9-12](#notebook_output_for_package_installs)所示。
- en: Note
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While we store our Azure credentials in some plain text here within the code,
    please be aware that this is generally not good coding practice. We are doing
    this to keep the example simple and because the notebook is still hosted in a
    protected Azure environment. If you want to learn best practices around handling
    authentication mechanisms, I recommend checking out the Microsoft resource [“About
    Azure Key Vault”](https://oreil.ly/9U6Z7).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在代码中以明文存储我们的 Azure 凭据，但请注意，这通常不是良好的编码实践。我们之所以这样做是为了保持示例简单，并且因为笔记本仍然托管在受保护的
    Azure 环境中。如果您想了解如何处理身份验证机制的最佳实践，请查阅微软资源 [“关于 Azure Key Vault”](https://oreil.ly/9U6Z7)。
- en: '![Notebook output for package installs](Images/apbi_0912.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![包安装的笔记本输出](Images/apbi_0912.png)'
- en: Figure 9-12\. Notebook output for package installs
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-12\. 包安装的笔记本输出
- en: 'Let’s move on to the first part of our mini ETL process: the file download.
    This first code cell of step 1 calls a class object that we defined in the preceding
    setup section. This code is essentially establishing a connection to your Azure
    storage account by using your connection string and downloading all files from
    the `AZURE_I⁠N⁠P⁠U⁠T⁠_​C⁠O⁠N⁠T⁠A⁠I⁠N⁠E⁠R` that you provided. Run this cell, and
    it should be finished after a minute, printing output as shown in [Figure 9-13](#notebook_output_for_blob_download).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续我们的小型 ETL 过程的第一部分：文件下载。第 1 步的第一个代码单元调用了我们在前面设置部分中定义的一个类对象。这段代码基本上是通过使用您的连接字符串来建立与
    Azure 存储帐户的连接，并从您提供的 `AZURE_INPUT_CONTAINER` 中下载所有文件。运行此代码单元，大约一分钟后应该会完成，并打印如
    [图 9-13](#notebook_output_for_blob_download) 所示的输出。
- en: '![Notebook output for blob download](Images/apbi_0913.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![Blob 下载的笔记本输出](Images/apbi_0913.png)'
- en: Figure 9-13\. Notebook output for blob download
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-13\. Blob 下载的笔记本输出
- en: Note
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you wonder why this download takes so long, rest assured that there are methods
    to speed up the downloading process—for example, running multiple downloads at
    once. In fact, you could also pass over the file URLs of the AI services directly
    without downloading them at all. For our prototype, the slow download method is
    still OK. But if you move to production, you would consider faster alternatives.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想知道为什么这个下载如此耗时，请放心，有方法可以加快下载过程——例如同时运行多个下载任务。事实上，您甚至可以直接传递 AI 服务的文件 URL，而完全不下载它们。对于我们的原型，慢速下载的方法仍然可以接受。但是如果您进入生产阶段，您可能会考虑更快的替代方案。
- en: Next, let’s move on to section 2 of our code, calling the AI service. In this
    section, we will send the texts to the AI service for analysis. As mentioned previously,
    we are sending batches of 10 text files (documents) at once to the API so that
    we get the results faster. Run the code cell. Once it is finished, you should
    see the first result containing the first 10 results of the sentiment analysis,
    as shown in [Figure 9-14](#raw_notebook_output_for_sentiment_analy).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们继续我们代码的第 2 部分，调用 AI 服务。在这一部分，我们将文本发送到 AI 服务进行分析。如前所述，我们一次发送 10 个文本文件（文档）的批次到
    API，以便更快地获得结果。运行代码单元。一旦完成，您应该会看到第一个结果，其中包含情感分析的前 10 个结果，如 [图 9-14](#raw_notebook_output_for_sentiment_analy)
    所示。
- en: '![Raw notebook output for sentiment analysis](Images/apbi_0914.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![情感分析的原始笔记本输出](Images/apbi_0914.png)'
- en: Figure 9-14\. Raw notebook output for sentiment analysis
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-14\. 情感分析的原始笔记本输出
- en: As you might notice from this preview output, the structure of the results is
    nested. The AI service provides not only the sentiment score for each text, but
    also a detailed breakdown for opinions on a word level for each text that was
    analyzed. This will be useful for the interpretation of the data, but on the downside
    it creates some hassle for us to untangle the whole object and convert it back
    into some nice flat tables. This is what section 3 of this code is all about.
    Let’s move on!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您从这个预览输出中所注意到的，结果的结构是嵌套的。AI 服务不仅为每个文本提供情感分数，还为每个分析的文本在词级别上提供了详细的意见分析。这对于数据的解释将会很有用，但不利的一面是，我们需要费些工夫来解开整个对象并将其转换回一些简洁的平面表格。这就是本代码第
    3 部分的内容。让我们继续前进吧！
- en: Section 3 of the data preparation code is the most exhaustive. While the main
    AI workload has been done, a bit of postprocessing remains, to extract the sentiment
    scores for each text and the corresponding opinions discovered in each item.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备代码的第 3 部分是最详尽的部分。虽然主要的 AI 工作负荷已经完成，但还需要进行一些后处理工作，提取每个文本的情感分数和每个条目中发现的相应意见。
- en: Now, if you ask yourself, “How on earth should I ever come up with this code?”
    I have good news for you. Most of this code was borrowed from the AI services’
    documentation page. Do you remember the quick-start reference back in the Azure
    console? This is the place where most of this code is coming from. Once you understand
    the general behavior, making the adjustments you need is pretty straightforward.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你问自己，“我到底该如何编写这段代码？”我有好消息告诉你。大部分这段代码都是从AI服务的文档页面借用来的。你还记得Azure控制台里的快速入门参考吗？这就是这段代码大部分来自的地方。一旦你理解了一般行为，你所需做的调整就相当简单了。
- en: In this example, I made three adjustments. First, I extracted only the high-level
    sentiment scores per text item and saved them as a flat table for better CSV compatibility.
    This table, or dataframe, also contains the original filename, the original text,
    and a datetime column extracted from the texts’ filenames. This will result in
    a nice flat CSV output that we can later display in our BI. And second and third,
    I created a flat CSV each for all positive and negative terms or opinions found
    in the text documents, again stored with the reference to the original filename
    and the extracted datetime column so these terms can be linked back to their original
    context and also possibly filtered by time. If you feel lucky, go through the
    code line by line and see what’s happening. If not, that’s also fine. We’re not
    wanting to become software engineers. In any case, click the Run button for this
    cell to see the output shown in [Figure 9-15](#formatted_notebook_output_for_sentiment).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我进行了三处调整。首先，我只提取每个文本项的高级情感分数，并将它们保存为扁平表以更好地与CSV兼容。这个表或数据框还包含原始文件名、原始文本以及从文本文件名中提取的日期时间列。这将产生一个漂亮的扁平CSV输出，我们稍后可以在我们的BI中显示。其次，我为文本文档中找到的所有正面和负面词汇或意见分别创建了一个扁平CSV文件，同样存储有对原始文件名和提取的日期时间列的引用，以便这些术语可以被链接回其原始上下文，并且也可能按时间进行过滤。如果你感觉幸运，逐行查看代码看看发生了什么。如果没有，也没关系。我们不打算成为软件工程师。无论如何，点击此单元格的运行按钮以查看显示在[图
    9-15](#formatted_notebook_output_for_sentiment)中的输出。
- en: '![Formatted notebook output for sentiment analysis](Images/apbi_0915.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![格式化后的情感分析笔记本输出](Images/apbi_0915.png)'
- en: Figure 9-15\. Formatted notebook output for sentiment analysis
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-15\. 情感分析的格式化笔记本输出
- en: For convenience reasons, the output printed the sentiment and opinions for each
    text document in an easy-to-read format. For example, you can see from [Figure 9-15](#formatted_notebook_output_for_sentiment)
    that the sentence `Basement room are pretty noisy` (despite the grammar mistake)
    was recognized by the AI as a negative sentiment and that the AI service was able
    to identify the words `room` and `noisy` as the main drivers behind this negative
    opinion. Isn’t that pretty?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，输出以易读的格式打印了每个文本文档的情感和意见。例如，你可以从[图 9-15](#formatted_notebook_output_for_sentiment)看到，尽管有语法错误，“地下室很吵”被AI识别为负面情感，而AI服务能够识别“房间”和“吵闹”作为这一负面意见的主要驱动因素。这不是挺好吗？
- en: At the end, this code also created the CSV files we wanted. If you take a look
    at the file explorer on the left and click Refresh, you should see three new CSV
    files, as shown in [Figure 9-16](#csv_outputs_in_notebook_environment).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这段代码还创建了我们想要的CSV文件。如果你查看左侧的文件资源管理器并点击刷新，你应该会看到三个新的CSV文件，就像[图 9-16](#csv_outputs_in_notebook_environment)所示。
- en: '![CSV outputs in notebook environment](Images/apbi_0916.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![笔记本环境中的CSV输出](Images/apbi_0916.png)'
- en: Figure 9-16\. CSV outputs in notebook environment
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-16\. 笔记本环境中的CSV输出
- en: To make these CSV files accessible to our BI, let’s head over to code section
    4, which is all about uploading these files to Azure Blob Storage. Compared to
    section 3, this will be a breeze.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要使这些CSV文件对我们的BI可访问，让我们转到第 4 部分的代码部分，这部分代码专门用于将这些文件上传到Azure Blob Storage。与第 3
    部分相比，这将会轻而易举。
- en: Execute the last code cell. Within a blink of a second, you should see the output
    shown in [Figure 9-17](#notebook_output_for_completed_upload) confirming that
    the files have been uploaded successfully.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 执行最后一个代码单元格。眨眼间，你应该会看到显示在[图 9-17](#notebook_output_for_completed_upload)中的输出，确认文件已成功上传。
- en: '![Notebook output for completed upload](Images/apbi_0917.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![已完成上传的笔记本输出](Images/apbi_0917.png)'
- en: Figure 9-17\. Notebook output for completed upload
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-17\. 已完成上传的笔记本输出
- en: Congratulations! You’ve completed the hardest part and survived the mini ETL
    challenge. Now, as with every ETL process, this process is reproducible. Whenever
    you update the text files in the blob input container and the files keep the same
    structure, you can simply rerun this whole workflow (at once), and the CSV files
    in the output container will be replaced and overwritten with the new results.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已完成最困难的部分，并成功完成了迷你 ETL 挑战。现在，与每个 ETL 过程一样，此流程是可重复的。每当更新 blob 输入容器中的文本文件，并且文件保持相同结构时，您只需重新运行整个工作流程（一次），输出容器中的
    CSV 文件将被替换并覆盖为新结果。
- en: 'But let’s leave the realms of ETL for now and move on to the fun part: visualizing
    and synthesizing our results in our BI.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们暂时离开 ETL 的领域，转向更有趣的部分：在我们的 BI 中可视化和综合我们的结果。
- en: Warning
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Don’t forget to stop your compute resource after this exercise to avoid any
    ongoing charges!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记在此练习后停止计算资源，以避免任何持续费用！
- en: Model Inference with Power BI Walk-Through
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Power BI 进行模型推断演练
- en: To display our results, we will once more come back to Power BI. Create a new
    report in Power BI and select Azure Blob Storage as the data source. Connect with
    your storage account and select the checkbox next to the folder *Tables*. Click
    Transform. In the Power Query Editor, you will find a list of all CSV tables found
    in the folder. We need to go through all three of them one by one.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要显示我们的结果，我们将再次回到 Power BI。在 Power BI 中创建一个新报告，选择 Azure Blob 存储作为数据源。连接到您的存储账户，然后选择
    *Tables* 文件夹旁边的复选框。点击“Transform”。在 Power Query Editor 中，您会找到文件夹中所有 CSV 表格的列表。我们需要逐个处理这三个表格。
- en: Choose the first one and click the Binary link to see a preview of your data,
    as shown in [Figure 9-18](#sentiment_data_in_power_query). Double-check that the
    Datetime column has been converted correctly to a datetime format. Click Close
    & Apply and redo these steps for the other two CSV files.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 选择第一个文件并点击 Binary 链接以查看数据预览，如[图 9-18](#sentiment_data_in_power_query)所示。确保 Datetime
    列已正确转换为日期时间格式。点击“关闭并应用”，然后对其他两个 CSV 文件重复这些步骤。
- en: '![Sentiment data in Power Query](Images/apbi_0918.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![Power Query 中的情感数据](Images/apbi_0918.png)'
- en: Figure 9-18\. Sentiment data in Power Query
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-18\. Power Query 中的情感数据
- en: Your data model should now list three separate tables. We know, however, that
    these tables are related to one another and we want Power BI to know this as well.
    Right-click any table and choose “Edit relationship.” In the editor that pops
    up, create a relationship between the table, “positive_targets,” and “sentiment”
    based on the column Filename. The cardinality should be many to one, as shown
    in [Figure 9-19](#data_relationships_in_power_bi). Click OK and redo this setup
    for the table “negative_targets.”
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的数据模型应该列出三个单独的表。然而，我们知道这些表彼此相关，并且我们希望 Power BI 也知道这一点。右键单击任何表格，选择“编辑关系”。在弹出的编辑器中，基于“文件名”列在“positive_targets”和“sentiment”表之间创建关系。基数应该是多对一，如[图
    9-19](#data_relationships_in_power_bi)所示。点击“确定”，然后为“negative_targets”表重复此设置。
- en: '![Data relationships in Power BI](Images/apbi_0919.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![Power BI 中的数据关系](Images/apbi_0919.png)'
- en: Figure 9-19\. Data relationships in Power BI
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-19\. Power BI 中的数据关系
- en: Afterward, the tables in your data model should be connected, as shown in [Figure 9-20](#data_model_in_power_bi-id000002).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，您的数据模型中的表格应该已连接，如[图 9-20](#data_model_in_power_bi-id000002)所示。
- en: '![](Images/apbi_0920.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/apbi_0920.png)'
- en: Figure 9-20\. Data model in Power BI
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-20\. Power BI 中的数据模型
- en: Now that our data model is completed, it’s time to move on to handling the visuals.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据模型已经完成，是时候继续处理可视化了。
- en: Building the AI-Powered Dashboard in Power BI
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Power BI 中构建 AI 驱动的仪表板
- en: 'Let’s quickly recap the situation: management wants to know if something is
    going on that should be on their radar. To get a high-level overview about the
    customer reviews, we need four elements:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下情况：管理层想知道是否有什么事情需要他们关注。为了对客户评论进行高层次的概述，我们需要四个要素：
- en: The development of customer sentiment over time to check for trends
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随时间发展顾客情感以检查趋势
- en: A list of items that customers complain about (negative targets)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顾客投诉的项目列表（负面目标）
- en: A list of items that customers like (positive targets)
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顾客喜欢的项目列表（正面目标）
- en: A reference back to the original data so we get more context
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回到原始数据以获取更多上下文信息
- en: We could convey this information in many ways. I decided to put a report together
    consisting of a line chart containing the overall trend, two treemaps highlighting
    the positive and negative targets, and a simple table that lists all the customer
    feedback with plain text. You can see the final result in [Figure 9-21](#final_dashboard_for_ai_powered_sentimen).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过多种方式传达这些信息。我决定编制一份报告，包含一个显示总体趋势的折线图，两个突显正面和负面目标的树状图，以及一个简单的表格，列出所有客户反馈的纯文本。你可以在[图 9-21](#final_dashboard_for_ai_powered_sentimen)中看到最终结果。
- en: '![Final dashboard for AI-powered sentiment analysis](Images/apbi_0921.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![AI 动力情感分析的最终仪表板](Images/apbi_0921.png)'
- en: Figure 9-21\. Final dashboard for AI-powered sentiment analysis
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-21\. AI 动力情感分析的最终仪表板
- en: Try to re-create this dashboard by yourself in Power BI or come up with your
    own solution. Or, you can download the complete *User_Reviews_AI-Powered.pbix*
    file from the [book’s website](https://oreil.ly/0uHwu) and connect it to your
    own Azure Blob Storage.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 试着在Power BI中自行重新创建这个仪表板，或者想出你自己的解决方案。或者，你可以从[书籍网站](https://oreil.ly/0uHwu)下载完整的
    *User_Reviews_AI-Powered.pbix* 文件，并将其连接到你自己的Azure Blob Storage。
- en: So what does this dashboard tell us? For one, we can see that both the positive
    and negative sentiment seems to have a somewhat steady trend, with daily ups and
    downs. If we aggregate the visual to a monthly level, the picture becomes a bit
    clearer ([Figure 9-22](#monthly_sentiment_analysis)). From a monthly perspective,
    the negative sentiments seem to have increased a lot. If we explore the reasons,
    we can find out that most complaints are about the rooms, the breakfast, and the
    WiFi. While the rooms might be hard to fix in the short term, the breakfast and
    WiFi provide clear action items that can be addressed quickly by management.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这个仪表板告诉我们什么？首先，我们可以看到正面和负面情感似乎有一个相对稳定的趋势，每天都有起伏。如果我们将可视化数据聚合到月度水平，情况就会变得更加清晰（参见[图 9-22](#monthly_sentiment_analysis)）。从月度的角度来看，负面情感似乎大幅增加。如果我们探究原因，我们会发现大多数投诉是关于房间、早餐和WiFi的。尽管房间可能在短期内难以修复，但早餐和WiFi提供了可以由管理层迅速解决的明确行动项目。
- en: '![Monthly sentiment analysis](Images/apbi_0922.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![每月情感分析](Images/apbi_0922.png)'
- en: Figure 9-22\. Monthly sentiment analysis
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-22\. 每月情感分析
- en: Also, since we have created the relationships among the tables, Power BI allows
    us to filter for certain data points. For example, if we select a date from the
    line chart, the rest of the dashboard will update accordingly and show only the
    relevant data that was collected on this day, and vice versa. What exactly is
    it that customers complain about with regards to the room? Just click Rooms in
    the Negative Targets treemap and see what customers say by exploring the original
    text at the bottom. This provides a great way to explore the dataset and find
    out more about the actual customer feedback.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，由于我们已经在表格之间创建了关系，Power BI允许我们筛选特定的数据点。例如，如果我们从折线图中选择一个日期，仪表板的其余部分将相应更新，并仅显示在这一天收集的相关数据，反之亦然。关于房间，客户投诉了什么？只需点击负面目标树状图中的房间，通过探索底部的原始文本来了解客户的说法。这为探索数据集并更多地了解实际客户反馈提供了一个很好的途径。
- en: While this dashboard only touches the top of the iceberg of text analytics,
    I hope you have seen how powerful it can be to analyze text data at scale. Thanks
    to AI services, this can be done with a few lines of code and integrated in any
    BI system.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个仪表板只触及了文本分析的冰山一角，但希望你已经看到了在大规模文本数据分析中其强大的能力。多亏了AI服务，这可以用几行代码完成，并集成到任何BI系统中。
- en: 'Use Case: Parsing Documents with AI'
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用案例：使用AI解析文档
- en: Have you ever seen a business that does not use forms? Forms are for businesses
    what APIs are for programmers. Forms have a great advantage. They typically work
    well to exchange information between humans. Given a specific form and some descriptions,
    you should pretty quickly be able to figure out how to read the form or how to
    fill it in.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你有见过一个不使用表格的企业吗？对于企业来说，表格就像程序员的API一样重要。表格有一个很大的优势。它们通常很好地在人类之间交换信息。给定一个特定的表格和一些描述，你应该很快能够弄清楚如何阅读表格或如何填写它。
- en: The drawback often comes when forms need to be processed by a computer. Often
    forms still live as paper documents or their electronic counterparts, PDFs. If
    you want to process them at scale, you often don’t have any other choice than
    to look through them manually and extract data from these PDFs by hand for further
    analysis. Receipts can also be considered a form; they provide a recurring data
    structure that is easy for humans to read, but hard for machines to interpret.
    In this use case, we will learn how AI will help us to read documents at scale
    and extract values from it to display them in a BI system.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当表格需要由计算机处理时，常常会出现问题。通常表格仍然以纸质文件或其电子副本形式存在，即 PDF 文件。如果您希望大规模处理它们，往往除了手动查阅并从这些
    PDF 中提取数据进行进一步分析之外别无选择。收据也可以看作是一种表格；它们提供了一种易于人类阅读但难以机器解释的重复数据结构。在这个使用案例中，我们将学习如何利用人工智能帮助我们大规模阅读文件并从中提取值，以便在商业智能系统中显示。
- en: Problem Statement
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: In our next scenario, imagine that we are working for a medium-sized company
    that provides consulting services. The business is located in Germany, where sales
    representatives travel frequently to potential customers to prepare or close deals.
    The travel is mostly done via high-speed trains that connect the big cities of
    Germany, called InterCity Express (ICE).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一个场景中，假设我们正在为一家提供咨询服务的中型公司工作。该公司位于德国，销售代表经常出差到潜在客户那里，以准备或完成交易。这些出差大多通过连接德国大城市的高速列车进行，称为ICE（InterCity
    Express）。
- en: 'Our travel management department keeps control of the overall expenses. The
    process for business travel is as follows: sales reps can book their train tickets
    over a self-service portal from the train operator (Deutsche Bahn). These tickets
    are paid for with a company credit card. The travel management team oversees the
    credit card billing at the end of each month to get an overview of the travel
    expenses. However, the credit card statements do not provide any further detail
    except for the amount spent and the date. To optimize travel expenses and better
    understand which business trips are causing the most costs, the travel management
    team wants more insights about the trips, including a breakdown by popular travel
    routes (train origin and train destinations).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的出差管理部门负责控制总体开支。商务旅行的流程如下：销售代表可以通过火车运营商（德国铁路）的自助门户预订火车票。这些票券使用公司信用卡支付。出差管理团队在每月末监督信用卡账单结算，以便了解出差开支的概况。然而，信用卡对账单除了支出金额和日期外并未提供任何进一步的细节。为了优化出差开支并更好地了解哪些商务旅行造成了最高成本，出差管理团队希望更深入地了解出差情况，包括按流行旅行路线（火车出发地和目的地）进行的细分。
- en: This information can be found in the booking receipt that the sales representatives
    automatically receive via email after every successful booking. [Figure 9-23](#ice_train_online_ticket)
    shows an example of such a receipt.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息可以在销售代表每次成功预订后自动通过电子邮件接收的预订收据中找到。[图 9-23](#ice_train_online_ticket)展示了这种收据的示例。
- en: 'In particular, the team is interested in extracting the following information
    from this form: booking date, trip origin, trip destination, and ticket price.
    The travel management team wants to find out if there is a way to extract this
    information automatically and ideally report it using the existing BI. They have
    provided us with a sample of 162 receipts from a single sales representative to
    work on a first prototype.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，团队对从这份表格中提取以下信息感兴趣：预订日期、出发地、目的地以及票价。出差管理团队希望找到一种自动提取这些信息并理想情况下使用现有的商业智能系统进行报告的方法。他们已经提供了一位销售代表的162张收据样本，用于初版原型的开发。
- en: '![ICE train online ticket](Images/apbi_0923.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![ICE 列车在线车票](Images/apbi_0923.png)'
- en: Figure 9-23\. ICE train online ticket
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-23\. ICE 列车在线车票
- en: Solution Overview
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案概述
- en: In this scenario, we’re going to extract information from a document by using
    optical character recognition (OCR). Now, this technique isn’t new and has been
    around for a while. However, AI will help us with at least two layers of this
    problem. The first layer is to improve the actual OCR—that is, recognizing single
    text characters and converting them into machine-readable form (plain-text string).
    The second part is making sense of the characters, to find out which ones belong
    to a word or a sentence, or even recognizing more complex structures like tables.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将使用光学字符识别（OCR）从文档中提取信息。现在，这项技术并不新鲜，并且已经存在了一段时间。但是，AI将帮助我们解决这个问题的至少两个层面。第一层是改进实际的OCR——即识别单个文本字符并将其转换为可机器读取的形式（纯文本字符串）。第二部分是理解这些字符，找出哪些属于单词或句子，甚至识别更复杂的结构，如表格。
- en: Take a look at [Figure 9-24](#use_case_architecture_for_parsing_docum) to review
    the overall architecture of this use case.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[图 9-24](#use_case_architecture_for_parsing_docum)以审查此使用案例的整体架构。
- en: '![Use case architecture for parsing documents](Images/apbi_0924.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![解析文档的用例架构](Images/apbi_0924.png)'
- en: Figure 9-24\. Use case architecture for parsing documents
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-24\. 解析文档的用例架构
- en: 'To tackle this problem, we will follow a similar approach to that in [“Use
    Case: Getting Insights from Text Data”](#use_case_getting_insights_from_text_dat):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，我们将采用与[“使用案例：从文本数据获取洞察”](#use_case_getting_insights_from_text_dat)类似的方法：
- en: Deploy an out-of-the box AI service on Microsoft Azure—in this case, Cognitive
    Services for Computer Vision (analysis layer).
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署Microsoft Azure中的即用即用AI服务——在本例中，是计算机视觉的认知服务（分析层）。
- en: Load the data into a staging area—in this case, again using Azure Blob Storage
    (data layer).
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据加载到分段区域——再次使用Azure Blob Storage（数据层）。
- en: Prepare a small ETL script that loads the data from the staging area, applies
    the AI service, and transforms it to a flat CSV (data layer).
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个小的ETL脚本，从分段区域加载数据，应用AI服务，并将其转换为平面CSV（数据层）。
- en: Upload the CSV file to a location from where it can be easily accessed and visualized
    with our BI tool (user layer).
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将CSV文件上传到一个可以轻松访问和可视化的位置，并与我们的BI工具（用户层）一起使用。
- en: Let’s go!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧！
- en: Setting Up the AI Service
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置AI服务
- en: Go to your [Azure portal](http://portal.azure.com) and type `**cognitive services**`
    in the search bar. Select Cognitive Services from the suggestion list and proceed
    to the respective resource page. Scroll down to Vision. You should see the available
    services shown in [Figure 9-25](#cognitive_services_for_vision).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 前往你的[Azure 门户](http://portal.azure.com)，在搜索栏中输入`**认知服务**`。从建议列表中选择“认知服务”，然后进入相应的资源页面。向下滚动至Vision，你应该看到显示在[图 9-25](#cognitive_services_for_vision)中的可用服务。
- en: '![Cognitive Services for Vision](Images/apbi_0925.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![视觉认知服务](Images/apbi_0925.png)'
- en: Figure 9-25\. Cognitive Services for Computer Vision
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-25\. 计算机视觉的认知服务
- en: Select “+ Create” from the “Computer vision” box, and you should be greeted
    with the now well-known form to set up a computer vision resource. Again, select
    a resource group, give your resource a name, and select the free F0 pricing tier.
    Don’t forget to select the checkbox at the bottom that you agree to the “Responsible
    AI terms” of use.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从“计算机视觉”框中选择“+ 创建”，你应该会看到设置计算机视觉资源的现在广为人知的表单。再次选择一个资源组，给你的资源命名，并选择免费的F0定价层。不要忘记选择底部的复选框，同意使用“负责任的AI条款”。
- en: Take a minute to acknowledge these terms. They essentially mean that you don’t
    violate personal rights by using the AI. Remember, every image or document that
    you send to the API will be processed by services owned and operated by Microsoft.
    So, especially if you deal with personal data, you have to ensure that you have
    the consent and permission to do this. For our use cases, we are dealing with
    fictional or public domain data so there is no risk involved. However, in a business
    setting, think twice before you start sending documents about your customers or
    employees to a remote AI service.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 花一分钟时间了解这些术语。它们基本上意味着你不会通过使用AI来违反个人权利。请记住，你发送到API的每个图像或文档都将由微软拥有和运营的服务处理。因此，特别是当你处理个人数据时，你必须确保已获得同意和权限。对于我们的用例，我们处理的是虚构或公共领域的数据，因此没有风险。然而，在商业环境中，在开始向远程AI服务发送关于你的客户或员工的文档之前，请三思。
- en: Click “Review + create” once you’ve completed the form, and Create after the
    automatic validation passes. After a few minutes, your service should be deployed,
    and you can access it either by clicking the notification link or by searching
    for your resource name through the search bar in the Azure portal. If you open
    the resource page, you should see a screen that looks similar to [Figure 9-26](#computer_vision_service_overview).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成表单后，点击“审核 + 创建”，并在自动验证通过后点击“创建”。几分钟后，您的服务应该已部署完成，您可以通过点击通知链接或在 Azure 门户中的搜索栏中搜索资源名称来访问它。如果打开资源页面，您应该会看到一个与[图
    9-26](#computer_vision_service_overview)类似的屏幕。
- en: '![Computer vision service overview](Images/apbi_0926.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![计算机视觉服务概述](Images/apbi_0926.png)'
- en: Figure 9-26\. Computer Vision Service overview
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-26\. 计算机视觉服务概述
- en: 'The quick start looks a bit different from the previous AI service but essentially
    contains the same information: you will find a link to your access keys and some
    code examples to get started quickly. For now, you can navigate to Keys and Endpoint,
    where you will find the resource HTTP endpoints and secret access keys. Leave
    this page open, because we will need these keys in a bit.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 快速入门看起来与以前的 AI 服务有些不同，但基本上包含相同的信息：您将找到访问密钥的链接以及一些快速入门的代码示例。目前，您可以导航到“Keys and
    Endpoint”，在那里您将找到资源的 HTTP 终端和秘密访问密钥。保持此页面打开，因为我们稍后将需要这些密钥。
- en: The AI setup is complete, so let’s move on to load and process the PDF files.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: AI 设置已完成，现在我们继续加载和处理 PDF 文件。
- en: Note
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Wait a second, you might say. Why didn’t we use the dedicated Azure service
    Form Recognizer for this? Form Recognizer is a specialized service that tailors
    computer vision to form recognition and value extraction. The service is more
    focused, but also needs a certain degree of custom training and setup before it
    works; it is no out-of-the box service. On the other hand, the computer vision
    API is a multipurpose service that works like Plug and Play. It is easier to implement,
    but you will hit limits when the task gets too specialized. It will also be useful
    for you in scenarios other than form extraction. My recommendation is to start
    with the general computer vision first and switch over to more customized or tailored
    services later, if needed. If you want to learn more about Form Recognizer, see
    [“Azure Form Recognizer Documentation”](https://oreil.ly/XfqAK).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 等一下，你可能会说。为什么我们没有使用专用的 Azure 服务“表单识别器”？表单识别器是一项专门针对表单识别和价值提取的计算机视觉服务。该服务更专注，但在开始工作之前需要一定程度的定制训练和设置；它并非即开即用的服务。另一方面，计算机视觉
    API 是一个多用途的服务，类似即插即用。它更容易实现，但在任务过于专业化时会受到限制。它在除了表单提取之外的场景中也会很有用。我的建议是首先从通用计算机视觉开始，如有需要再转向更定制或特定的服务。如果你想了解更多关于表单识别器的信息，请参阅[“Azure
    表单识别器文档”](https://oreil.ly/XfqAK)。
- en: Setting Up the Data Pipeline
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置数据管道
- en: Let’s first upload our PDF files to an Azure Blob Storage container so we can
    access them easily. Open a new Azure portal window and navigate to Azure Blob
    Storage by using the search bar or looking in your recent items in your dashboard.
    Navigate to Containers and create a new container called `**pdfs**`, as shown
    in [Figure 9-27](#creating_a_new_storage_container_in_azu).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将我们的 PDF 文件上传到 Azure Blob 存储容器中，以便我们可以轻松访问它们。通过搜索栏或查看仪表板中的最近项目，打开新的 Azure
    门户窗口并导航到 Azure Blob 存储。导航到“容器”并创建一个名为`**pdfs**`的新容器，如[图 9-27](#creating_a_new_storage_container_in_azu)所示。
- en: '![Creating a new storage container in Azure Blob Storage](Images/apbi_0927.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![在 Azure Blob 存储中创建新的存储容器](Images/apbi_0927.png)'
- en: Figure 9-27\. Creating a new storage container in Azure Blob Storage
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-27\. 在 Azure Blob 存储中创建新的存储容器
- en: Now, download the *pdfs.zip* file from the [book’s website](https://oreil.ly/0uHwu)
    and unzip it on your local computer. Download all the contents from your local
    *pdfs* folder to the container in Azure Blob Storage. At the end, all PDF files
    should be stored in the “pdfs”' container on Azure Blob Storage without any subfolder
    or any ZIP file, as shown in [Figure 9-28](#pdf_files_in_azure_blob_storage).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在[书籍网站](https://oreil.ly/0uHwu)下载*pdfs.zip*文件，并在本地计算机上解压缩它。从本地的*pdfs*文件夹下载所有内容到
    Azure Blob 存储中的容器。最终，所有 PDF 文件应存储在 Azure Blob 存储的“pdfs”容器中，不包含任何子文件夹或任何 ZIP 文件，如[图
    9-28](#pdf_files_in_azure_blob_storage)所示。
- en: '![PDF files in Azure Blob Storage](Images/apbi_0928.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![Azure Blob 存储中的 PDF 文件](Images/apbi_0928.png)'
- en: Figure 9-28\. PDF files in Azure Blob Storage
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-28\. Azure Blob 存储中的 PDF 文件
- en: Now that the files are ready for analysis, we can move ahead to the ETL process.
    As in the previous example, I will walk you through the code step by step, and
    we will run the script in Azure Notebooks. Of course, you can also use your local
    IDE if you prefer.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在文件已准备好进行分析，我们可以继续进行ETL过程。与之前的示例一样，我将逐步为您讲解代码，并在Azure Notebooks中运行脚本。当然，如果您更喜欢，也可以使用您的本地IDE。
- en: First things first, download the *ETL_For_Document_Analysis.ipynb* file from
    the [book’s website](https://oreil.ly/0uHwu) to your local computer. Open [Azure
    ML Studio](http://ml.azure.com), select your preferred workspace, and click “Get
    started.” Navigate to Notebooks and choose “Upload files,” as shown in [Figure 9-9](#uploading_files_to_the_notebook_environ).
    Select the *ETL_For_Document_Analysis.ipynb* file and upload it to Azure Notebooks.
    After the upload, the notebook shows up on the right side of the screen. Don’t
    forget to connect your notebook to a compute resource as we did in the previous
    use case. When everything is ready, your screen should look similar to [Figure 9-29](#etl_notebook_for_document_analysis).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从[书籍网站](https://oreil.ly/0uHwu)下载*ETL_For_Document_Analysis.ipynb*文件到您的本地计算机。打开[Azure
    ML Studio](http://ml.azure.com)，选择您的首选工作区，然后点击“开始”。导航到笔记本，并选择“上传文件”，如[图 9-9](#uploading_files_to_the_notebook_environ)所示。选择*ETL_For_Document_Analysis.ipynb*文件并将其上传到Azure
    Notebooks。上传后，笔记本将显示在屏幕右侧。不要忘记像在之前的用例中一样将您的笔记本连接到计算资源。当一切准备就绪时，您的屏幕应类似于[图 9-29](#etl_notebook_for_document_analysis)。
- en: '![ETL notebook for document analysis](Images/apbi_0929.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![文档分析的ETL笔记本](Images/apbi_0929.png)'
- en: Figure 9-29\. ETL notebook for document analysis
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-29\. 文档分析的ETL笔记本
- en: I will now walk you through the code and explain what is happening section by
    section. Please follow along by executing the code cells in the order covered
    here.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我将为您详细讲解代码，并逐节解释发生的情况。请按照这里覆盖的顺序执行代码单元格。
- en: First, section 0, Packages and Setup, again is the place where you need to input
    your custom Azure connection string as well as your custom key and endpoint for
    your computer vision service. Do you still have the window open? Good, then you
    can just copy and paste both values here. In the following section, the code ensures
    that all required packages are imported, and we define some custom functions.
    I will explain these functions in more detail once we are going to use them. Now,
    run the first code cell by placing the cursor inside and pressing Shift+Enter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是第0节，包和设置，这里再次是您需要输入自定义的Azure连接字符串以及计算机视觉服务的自定义密钥和端点的地方。您的窗口还打开着吗？很好，那么您只需在此处复制并粘贴这两个值即可。在接下来的部分中，代码确保导入了所有所需的包，并定义了一些自定义函数。一旦我们将要使用它们，我将更详细地解释这些函数。现在，通过将光标放在内部并按Shift+Enter来运行第一个代码单元格。
- en: Section 1 covers the file download from Azure Blob Storage. This one is quite
    straightforward and identical to what we did in the previous use case. Run this
    code cell as well. As a result, you should see a *pdfs* folder in the file browser.
    If not, click the small refresh icon in the file browser.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 第1节涵盖了从Azure Blob存储中下载文件。这个非常简单，与我们在之前的用例中所做的完全相同。也运行这个代码单元格。作为结果，在文件浏览器中您应该看到一个*pdfs*文件夹。如果没有，请点击文件浏览器中的小刷新图标。
- en: Section 2 is again a short one. Here, we are just calling the AI service and
    fetching the raw results from the analysis. We have two important things to note
    here.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分再次很简短。在这里，我们只是调用AI服务并从分析中获取原始结果。这里有两个重要的事情需要注意。
- en: You might notice the `time.sleep(6)` command in the code. I included this line
    to make sure we are not exceeding the free tier limit of 20 requests per minute.
    So why do we wait 6 seconds and not only 3 (3 seconds × 20 requests = 60 seconds)?
    This is because in the case of computer vision, we are running an *asynchronous
    operation*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到代码中的`time.sleep(6)`命令。我包含了这一行以确保我们不会超过每分钟20次请求的免费限制。那么为什么我们等待6秒而不仅仅是3秒（3秒
    × 20次请求 = 60秒）？这是因为在计算机视觉的情况下，我们正在运行一个*异步操作*。
- en: 'Remember that in the previous example for the sentiment analysis, we just sent
    the text to the API and immediately received the response with the AI results.
    In the case of computer vision, the operation is more complex and the results
    can take a while (like seconds). Therefore, for each document, we need to make
    two API calls: one POST request to send the document to the AI service, and one
    GET request to fetch the results. If you look carefully into the function `document_analysis`
    under section 0, you will notice both API calls. And, in fact, we are waiting
    two seconds each time before we make the GET request to increase the chances that
    the result is there for every call that we make.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个情感分析示例中，请记住，我们只是将文本发送到 API，并立即收到带有 AI 结果的响应。而在计算机视觉中，操作更复杂，结果可能需要一段时间（如几秒钟）。因此，对于每个文档，我们需要进行两次
    API 调用：一次 POST 请求将文档发送到 AI 服务，一次 GET 请求获取结果。如果您仔细查看第 0 节下的 `document_analysis`
    函数，您将注意到这两个 API 调用。事实上，我们在进行 GET 请求之前每次等待两秒，以增加结果在每次调用时都能及时返回的机会。
- en: If you move to a paid plan, you can safely delete the `time.sleep(6)` line from
    the loop in step 2\. But I do recommend keeping a short pause between the first
    POST and GET request; otherwise, you will run into a lot of empty (but still billed)
    API calls. Considering the limitations of the free plan, the overall analysis
    for the 162 PDF documents should take around 30 minutes. Run the code cell. And
    now it’s time for a coffee break before we head to the dirty work!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您升级到付费计划，可以安全地从第 2 步循环中删除 `time.sleep(6)` 行。但我建议在第一个 POST 和 GET 请求之间保持短暂的暂停；否则，您将遇到许多空的（但仍计费）API
    调用。考虑到免费计划的限制，对 162 个 PDF 文档的整体分析大约需要 30 分钟。运行代码单元。现在是喝杯咖啡的时间，然后我们开始做一些“脏活”！
- en: 'Calling the AI service so far has been pretty easy. Extracting the information
    from the response can be messy, though. To understand why, let’s take a look at
    what the result from the AI looks like. To do this, we can’t just print the result
    object. The result object is a nested structure that we need to unpack. Fortunately,
    the GitHub [documentation with Python code examples](https://oreil.ly/305e3) shows
    us how to do this:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，调用 AI 服务相当简单。不过，从响应中提取信息可能会很混乱。要理解原因，让我们看看 AI 结果的样子。为此，我们不能只是打印结果对象。结果对象是一个嵌套结构，我们需要解开它。幸运的是，GitHub
    的[带有 Python 代码示例的文档](https://oreil.ly/305e3)向我们展示了如何做到这一点。
- en: '[PRE0]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code will yield the following output (truncated for brevity):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以上代码将产生以下输出（为简洁起见进行了截断）。
- en: '[PRE1]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To make sense of this, let’s compare the output to the original document, as
    shown in [Figure 9-30](#online_train_ticket_left_parenthesisful).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这一点，让我们将输出与原始文档进行比较，如[图 9-30](#online_train_ticket_left_parenthesisful)所示。
- en: '![Online train ticket (full PDF)](Images/apbi_0930.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![在线火车票（完整 PDF）](Images/apbi_0930.png)'
- en: Figure 9-30\. Online train ticket (full PDF)
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-30\. 在线火车票（完整 PDF）
- en: In both the code output and [Figure 9-30](#online_train_ticket_left_parenthesisful),
    I have highlighted the elements of the document that we are interested in (Date,
    Price, Origin, Destination).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码输出和[图 9-30](#online_train_ticket_left_parenthesisful)中，我已突出显示了我们感兴趣的文档元素（日期、价格、起始地点、目的地）。
- en: If we compare the AI output with the original document, we can see that the
    AI parsed the document line by line. It starts with “DB” and “Online-Ticket” from
    the top left and ends with “Seite 1/1” on the bottom right.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 AI 的输出与原始文档进行比较，我们会发现 AI 逐行解析文档。它从左上角的“DB”和“Online-Ticket”开始，到右下角的“Seite
    1/1”结束。
- en: Each line of text is an element of the result object, and the numbers that follow
    are the bounding boxes for these lines, indicating where the line was positioned
    in the document. The six numbers correspond to the *x* and *y* coordinates, as
    shown in [Figure 9-31](#bounding_boxes_used_by_the_computer_vis), with the coordinate
    system starting at the top left of the document with coordinates (*x* = 0, *y*
    = 0). Each list of coordinates is mapped as [*x*1, *y*1, *x*2, *y*2, *x*3, *y*3,
    *x*4, *y*4].
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 每行文本都是结果对象的一个元素，后面的数字是这些行的边界框，指示行在文档中的位置。六个数字对应于 *x* 和 *y* 坐标，如[图 9-31](#bounding_boxes_used_by_the_computer_vis)所示，坐标系统从文档的左上角开始，坐标为
    (*x* = 0, *y* = 0)。每个坐标列表映射为 [*x*1, *y*1, *x*2, *y*2, *x*3, *y*3, *x*4, *y*4]。
- en: '![Bounding boxes used by the computer vision service](Images/apbi_0931.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![计算机视觉服务使用的边界框](Images/apbi_0931.png)'
- en: Figure 9-31\. Bounding boxes used by the computer vision service
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-31\. 计算机视觉服务使用的边界框
- en: To extract just the information we want from the document, we need to rely on
    the assumption that the document was parsed in western reading order (from left
    to right, top to bottom); this can be adjusted for the AI service if needed.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 要从文档中提取我们想要的信息，我们需要依赖于这样一个假设，即文档按西方阅读顺序解析（从左到右，从上到下）；如果需要，可以为 AI 服务进行调整。
- en: Let’s start with the date information by running the next code cell labeled
    `Part 1` of section 3\. In this code cell, most work is done by the custom function
    `get_text_by_keyword(result, "Datum")`. The function, which was defined in section
    0, takes a given keyword argument, in this case `Datum` (German for *Date*) and
    returns the item that follows just after that in the AI response. Since the AI
    parses the text from left to right, the result will be the actual date that we
    are looking for. We do some postprocessing to turn it into a more conventional
    date format, and that’s it. Extracting a form value that has a distinct form label
    next to it is pretty straightforward.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从运行标记为第 3 节的 `Part 1` 代码单元格开始获取日期信息。在这个代码单元格中，大部分工作由自定义函数 `get_text_by_keyword(result,
    "Datum")` 完成。该函数在第 0 节中定义，接受一个给定的关键字参数，此处为 `Datum`（德语中的 *日期*），并返回 AI 响应中紧随其后的项目。由于
    AI 从左到右解析文本，结果将是我们正在寻找的实际日期。我们进行了一些后处理，以将其转换为更常规的日期格式，就这样。提取具有显著表单标签的表单值是非常简单的。
- en: Now we move on to part 2, extracting the origin and destination information.
    This part is a bit trickier since the information is not listed on the same line,
    but on different lines in the document, similar to a table structure. To make
    it even more complicated, this list can be even longer if connecting trains run
    between the origin and final destination. How do we tackle this? We will use a
    combined approach of a keyword search and a bounding box filter. Let’s take a
    look before we run the cell.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向第二部分，提取起始点和目的地信息。这部分有点棘手，因为信息不在同一行上列出，而是在文档中的不同行上，类似于表格结构。为了使事情更加复杂，如果连接的列车在起点和最终目的地之间运行，则该列表可能会更长。我们如何解决这个问题呢？我们将使用关键词搜索和边界框过滤的组合方法。在运行单元格之前，让我们先看一下。
- en: First, we call the function `get_text_between_keywords(result, "Halt", "Wichtige
    Nutzungshinweise:")`, which will give us all text elements between the start and
    stop markers. In our case, that is the first column header of the table (`Halt`,
    German for *Stop*) and the first text object that follows after the table (`Wichtige
    Nutzungshinweise`, meaning *Terms of Use*). This function will extract all text
    elements in the table, as shown in [Figure 9-32](#table_data_contained_in_the_pdf).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们调用函数 `get_text_between_keywords(result, "Halt", "Wichtige Nutzungshinweise:")`，这将给我们提供表格的所有文本元素，这是表格的第一列标题（`Halt`，德语中的
    *停止*）和表格之后的第一个文本对象（`Wichtige Nutzungshinweise`，意为 *使用条款*）。该函数将提取表格中的所有文本元素，如[图
    9-32](#table_data_contained_in_the_pdf)所示。
- en: '![Table data contained in the PDF](Images/apbi_0932.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![包含在 PDF 中的表格数据](Images/apbi_0932.png)'
- en: Figure 9-32\. Table data contained in the PDF
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-32\. 包含在 PDF 中的表格数据
- en: However, we don’t want all text elements, but only the items of the first column.
    How do we get them? There are possibly many ways to approach this, but I found
    it easiest to just locate this information by using the respective bounding boxes.
    The idea is that, from the text fragment we have just collected, we extract only
    the part indicated by the dashed line in [Figure 9-33](#relevant_column_in_pdf_table).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们并不需要所有的文本元素，而只需要第一列的项目。我们该如何获取它们呢？可能有许多方法可以解决这个问题，但我发现最简单的方法是使用相应的边界框来定位这些信息。我们的想法是，从我们刚刚收集的文本片段中，仅提取由虚线框标识的部分，见[图
    9-33](#relevant_column_in_pdf_table)。
- en: '![Relevant column in PDF table](Images/apbi_0933.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![PDF 表格中的相关列](Images/apbi_0933.png)'
- en: Figure 9-33\. Relevant column in PDF table
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-33\. PDF 表格中相关列
- en: So how do we get the coordinates for this dashed box? We take the lower-left
    corner of the first column header `Halt` and define it to be the upper-left corner
    of our dashed box. To get the width of the box, we take the *x* coordinate of
    the lower-left corner of the text `Datum`, because we know this will always be
    the second column. This will become the *x*2 value of our dashed box. And finally,
    to get the height of the dashed box, we search for the highest *y*3 coordinate
    in our extracted text area. These coordinates are all we need to draw a rectangle.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何获取这个虚线框的坐标呢？我们取第一列标题`Halt`的左下角，并定义它为我们虚线框的左上角。要获取框的宽度，我们取文本`Datum`的左下角的*x*坐标，因为我们知道这将始终是第二列。这将成为我们虚线框的*x*2值。最后，要获取虚线框的高度，我们在提取的文本区域中搜索最高的*y*3坐标。这些坐标就是我们绘制矩形所需的一切。
- en: Now that we have defined our “filter” box, we can keep the text findings where
    the bounding box is inside the boundaries of our filters. This is what the function
    `get_text_by_position(result, filter_box)` is handling for us. This function will
    return a clean list with all the train stops. Now, we just need to take the first
    and the last item of this list, and voilá—we have our origin and destination values.
    With the conceptual understanding of what’s going on here, run the code cell for
    part 2.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的“筛选”框，我们可以保留文本发现，其中边界框位于我们筛选器的范围内。这就是函数`get_text_by_position(result,
    filter_box)`为我们处理的内容。这个函数将返回一个干净的列表，其中包含所有的列车停靠站点。现在，我们只需取此列表的第一项和最后一项，voilá——我们得到了起始点和目的地值。理解了这里正在发生的事情后，运行第2部分的代码单元。
- en: Part 3 is again a bit simpler. In this case, we are extracting the price information
    based on identifying the keyword `Betrag` (indicating the price) in the document,
    similar to the way we extracted the date. The only difference is that this time
    we are extracting not only the next item after the keyword, but also the next
    two items after the keyword, as you can see by the parameter `2` in the function
    `get_text_by_keyword(result, "Betrag", 2)`. Why is that? The short answer is,
    it makes our script a bit more robust. We can safely identify the price information
    based on the `€` sign and we know it should come soon after the label `Betrag`,
    but it does not necessarily have to be the first item; it could be the second.
    Go ahead and run the cell.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 第3部分再次简单一些。在这种情况下，我们根据文档中关键字`Betrag`（指示价格）来提取价格信息，类似于我们提取日期的方式。唯一的区别是，这次我们不仅提取关键字后面的下一个项，还提取关键字后面的下两个项，正如您在函数`get_text_by_keyword(result,
    "Betrag", 2)`中看到的那样。为什么会这样呢？简短的答案是，这使我们的脚本更加健壮。我们可以安全地根据`€`符号识别价格信息，并且我们知道它应该很快出现在标签`Betrag`后面，但不一定是第一个项目；它可能是第二个。继续运行这个单元格。
- en: Now only one step is left for the data transformation, and that is bringing
    our results together and writing the CSV table. By now, you should have four list
    objects—`dates`, `prices`, `origins`, and `destinations`—all having the same length.
    In this last step, we are binding them together to a dataframe and exporting this
    as a CSV file. Run this last cell and you should see the new file *public-transportation-costs.csv*
    in the file explorer on the left after a quick refresh. The result table is shown
    in [Figure 9-34](#structured_output_for_extracted_pdf_dat).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据转换只剩下一步，那就是将我们的结果汇总并将CSV表写入。到目前为止，您应该有四个列表对象——`dates`、`prices`、`origins`和`destinations`——它们的长度都相同。在这最后一步中，我们将它们绑定到一个数据框架并将其导出为CSV文件。运行此最后一个单元格，然后您应该在左侧的文件资源管理器中快速刷新后看到新文件*public-transportation-costs.csv*。结果表显示在[图9-34](#structured_output_for_extracted_pdf_dat)中。
- en: '![Structured output for extracted PDF data](Images/apbi_0934.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![提取PDF数据的结构化输出](Images/apbi_0934.png)'
- en: Figure 9-34\. Structured output for extracted PDF data
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-34\. 提取PDF数据的结构化输出
- en: Finally, let’s upload our result to our container called “tables” on Azure Blob
    Storage. The last code cell in section 4 will handle this for you. Run this cell
    and give yourself a pat on the back; the biggest part of the work has been completed.
    We can now move ahead and visualize the results in our BI.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将结果上传到Azure Blob存储上名为“tables”的容器中。第4节中的最后一个代码单元将为您处理此操作。运行此单元格，并为自己鼓掌；大部分工作已经完成。现在我们可以继续并在我们的BI中可视化结果。
- en: Warning
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Don’t forget to stop the Azure compute resource after you finish the script
    to avoid any ongoing charges!
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记在完成脚本后停止Azure计算资源，以避免任何持续的费用！
- en: Model Inference with Power BI Walk-Through
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Power BI 模型推断演示
- en: Let’s move on to the fun part. Open your favorite BI and load the CSV that we
    just created to get more insights. I will walk you through the process again with
    the example of Power BI.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入有趣的部分。打开你喜欢的 BI 工具，并加载我们刚刚创建的 CSV 文件，以获取更多见解。我将再次用 Power BI 的示例来带你完成这个过程。
- en: In Power BI, create a new report and choose Azure Blob Storage from “Get data.”
    Provide the name of your Azure Blob Storage account, as shown in [Figure 9-35](#importing_data_from_azure_blob_storage).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Power BI 中，创建一个新报表，并选择“获取数据”中的 Azure Blob 存储。提供你的 Azure Blob 存储账户名称，如 [图 9-35](#importing_data_from_azure_blob_storage)
    所示。
- en: '![Importing data from Azure Blob Storage](Images/apbi_0935.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![从 Azure Blob 存储导入数据](Images/apbi_0935.png)'
- en: Figure 9-35\. Importing data from Azure Blob Storage
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-35\. 从 Azure Blob 存储导入数据
- en: You should see the file list of your blob storage. Navigate to the folder *tables*
    and click Refresh, as shown in [Figure 9-36](#refreshing_a_data_preview_in_power_quer),
    if you still can’t see the new file *public-transportation-costs.csv*.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到你的 Blob 存储文件列表。导航到 *tables* 文件夹并点击“刷新”，如 [图 9-36](#refreshing_a_data_preview_in_power_quer)
    所示，如果你仍然看不到新文件 *public-transportation-costs.csv*。
- en: Select the checkbox for the *tables* folder and click Transform Data to open
    Power Query. Click the Binary link next to the name of the CSV file *public-transportation-costs.csv*.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 *tables* 文件夹的复选框，然后点击“转换数据”以打开 Power Query。点击 CSV 文件 *public-transportation-costs.csv*
    名称旁边的二进制链接。
- en: '![Refreshing a data preview in Power Query](Images/apbi_0936.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![在 Power Query 中刷新数据预览](Images/apbi_0936.png)'
- en: Figure 9-36\. Refreshing a data preview in Power Query
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-36\. 在 Power Query 中刷新数据预览
- en: You want to make sure of two things here. First, the date column should be correctly
    identified as a Date data type. Second, ensure that the prices are converted to
    a numeric value. If Power BI does not automatically recognize the correct data
    types, right-click the Date column and select Date, as shown in [Figure 9-37](#changing_the_data_type_in_power_query).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你需要确保两件事情。首先，日期列应正确识别为日期数据类型。其次，确保价格转换为数值。如果 Power BI 没有自动识别正确的数据类型，请右键单击日期列并选择日期，如
    [图 9-37](#changing_the_data_type_in_power_query) 所示。
- en: '![Changing the data type in Power Query](Images/apbi_0937.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![在 Power Query 中更改数据类型](Images/apbi_0937.png)'
- en: Figure 9-37\. Changing the data type in Power Query
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-37\. 在 Power Query 中更改数据类型
- en: If the prices are not showing up as numeric values, right-click the column and
    choose Change Type → Using Locale, as shown in [Figure 9-38](#parsing_decimal_numbers_by_using_the_lo).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果价格没有显示为数值，请右键单击列并选择“更改类型” → 使用区域设置，如 [图 9-38](#parsing_decimal_numbers_by_using_the_lo)
    所示。
- en: '![Parsing decimal numbers by using the locale in Power Query](Images/apbi_0938.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![在 Power Query 中使用区域设置解析小数](Images/apbi_0938.png)'
- en: Figure 9-38\. Parsing decimal numbers by using the locale in Power Query
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-38\. 在 Power Query 中使用区域设置解析小数
- en: Change the data type to Decimal Number and set the locale to English (United
    States), as shown in [Figure 9-39](#changing_the_data_type_with_the_locale).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据类型更改为小数，并设置区域设置为英语（美国），如 [图 9-39](#changing_the_data_type_with_the_locale)
    所示。
- en: '![Changing the data type with the locale in Power Query](Images/apbi_0939.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![在 Power Query 中使用区域设置更改数据类型](Images/apbi_0939.png)'
- en: Figure 9-39\. Changing the data type with the locale in Power Query
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-39\. 使用区域设置更改数据类型
- en: Finally, your table should look similar to [Figure 9-40](#transformed_dataset_in_power_query).
    Close Power Query by clicking Close & Apply.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你的表格应类似于 [图 9-40](#transformed_dataset_in_power_query)。通过单击“关闭并应用”关闭 Power
    Query。
- en: '![Transformed dataset in Power Query](Images/apbi_0940.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![转换后的数据集在 Power Query 中](Images/apbi_0940.png)'
- en: Figure 9-40\. Transformed dataset in Power Query
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-40\. 在 Power Query 中转换后的数据集
- en: Building the AI-Powered Dashboard in Power BI
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Power BI 中构建 AI 助力仪表板
- en: Head over to the report and create some new visuals. I decided to show the total
    travel expenses per year in a line chart, the total number of business trips as
    a metric, the top destinations as a horizontal bar chart, and, finally, all routes
    (combinations of origins and destinations) as a treemap in which the size equals
    the money spent. [Figure 9-41](#ai_powered_expense_tracking_dashboard) shows the
    visuals.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 转到报表并创建一些新的可视化内容。我决定用折线图展示每年的总旅行费用，用指标显示总商务旅行次数，用水平条形图展示热门目的地，最后在树状图中显示所有路线（起点和目的地的组合），其中大小等于花费的金额。[图 9-41](#ai_powered_expense_tracking_dashboard)
    展示了这些可视化内容。
- en: '![AI-powered expense-tracking dashboard](Images/apbi_0941.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![AI 助力费用追踪仪表板](Images/apbi_0941.png)'
- en: Figure 9-41\. AI-powered expense-tracking dashboard
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-41\. AI 助力费用追踪仪表板
- en: With these visuals, the travel management team can clearly see the overall cost
    trend and absolute trip numbers. And the team can also identify which routes contribute
    the most to the overall travel expenses. In this example, we can see that the
    connection from Hamburg to Leipzig and back accounted for the largest proportion
    of travel costs. We can also see that most trips started in Hamburg, with Bonn
    and Frankfurt being the second and third most popular destinations from this origin.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些可视化工具，旅行管理团队可以清楚地看到总体成本趋势和绝对出行次数。团队还可以确定哪些路线对整体旅行费用贡献最大。在这个例子中，我们可以看到从汉堡到莱比锡来回的连接占据了旅行费用的最大比例。我们还可以看到大多数出行都始于汉堡，而波恩和法兰克福则是从这个起点出发的第二和第三最受欢迎的目的地。
- en: Feel free to re-create this dashboard on your own or see if you can find even
    better ways to display the data. If you want to see the final dashboard that I
    created, you can download *Document_Analysis_AI-Powered.pbix* from the [book’s
    website](https://oreil.ly/0uHwu).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎您自行重新创建这个仪表板，或者看看是否能找到更好的方式来展示数据。如果您想看到我创建的最终仪表板，可以从[书籍的网站](https://oreil.ly/0uHwu)下载
    *Document_Analysis_AI-Powered.pbix*。
- en: 'Use Case: Counting Objects in Images'
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例：图像中物体计数
- en: With the previous use case, we have only touched the vast capabilities of computer
    vision. In the following scenario, I will show you how to use the existing infrastructure
    even further—namely, to detect objects in images. For this, let’s take a closer
    look at the problem at hand.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的用例中，我们仅仅触及了计算机视觉的广泛能力。在接下来的场景中，我将向您展示如何进一步利用现有基础设施——即在图像中检测物体。为此，让我们更详细地看一下手头的问题。
- en: Problem Statement
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: We are working for a transport and road authority that tries to improve overall
    traffic management and traffic flow to minimize road congestion. One important
    factor is to control the speed limits on highways. To set speed limits, the authority
    needs an ideally constant measurement of the traffic flow on the roads. While
    new roads are equipped with respective measurement sensors and technology, old
    roads often have only closed-circuit television (CCTV) cameras installed that
    were used by traffic managers for manual traffic inspections.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为一个试图改进整体交通管理和交通流量以最小化道路拥堵的交通和道路管理机构工作。一个重要因素是控制高速公路上的限速。为了设定限速，管理机构需要理想情况下对道路上的交通流量进行恒定测量。虽然新建道路配备了相应的测量传感器和技术，但旧道路通常只安装了用于手动交通检查的闭路电视（CCTV）摄像头。
- en: The operations team approached us to check whether it is possible to count the
    traffic by using the existing camera infrastructure and has provided us with sample
    CCTV footage for testing purposes. [Figure 9-42](#cctv_camera_footage) shows an
    example of the CCTV footage.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 运维团队与我们联系，希望通过现有的摄像头基础设施进行交通计数，并为测试目的提供了样本 CCTV 录像。[图 9-42](#cctv_camera_footage)展示了一个
    CCTV 录像的示例。
- en: '![CCTV camera footage](Images/apbi_0942.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![CCTV 摄像头录像](Images/apbi_0942.png)'
- en: 'Figure 9-42\. CCTV camera footage. Source: [Kaggle](https://oreil.ly/yHY0w)'
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-42\. CCTV 摄像头录像。来源：[Kaggle](https://oreil.ly/yHY0w)
- en: The CCTV images for this use case were provided via a dataset on Kaggle ([Highway
    CCTV Footage Images)](https://oreil.ly/yHY0w).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这个用例的 CCTV 图像是通过 Kaggle 提供的数据集提供的（[高速公路 CCTV 录像图像](https://oreil.ly/yHY0w)）。
- en: Solution Overview
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案概述
- en: To tackle the problem at hand, we are building on the existing computer vision
    AI service from the previous use case. Since we are using the same service, the
    overall use case architecture is almost identical to the previous computer vision
    use case, as you can see in [Figure 9-43](#use_case_architecture_for_counting_obje).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为解决手头的问题，我们基于前一个用例中的现有计算机视觉 AI 服务展开工作。由于我们使用了相同的服务，因此整体的用例架构与之前的计算机视觉用例几乎相同，正如您可以在[图
    9-43](#use_case_architecture_for_counting_obje)中看到的那样。
- en: '![Use case architecture for counting objects in images](Images/apbi_0943.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图像中物体计数的用例架构](Images/apbi_0943.png)'
- en: Figure 9-43\. Use case architecture for counting objects in images
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-43\. 图像中物体计数的用例架构
- en: Instead of a Read operation, we are now calling a service called “Detect objects.”
    In this case, we want to analyze data on the fly. So instead of loading a dataset
    to Azure Blob Storage, we are consuming the CCTV footage directly from a web URL
    and will feed this web URL into the AI service. We will still have a small ETL
    job in place that consumes the HTTP URLs, calls the AI service, and writes the
    output as a flat CSV file to Azure Blob Storage so we can consume it with our
    BI tool. Let’s start!
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们不是进行读取操作，而是调用一个名为“检测对象”的服务。在这种情况下，我们希望动态分析数据。因此，我们不会将数据集加载到Azure Blob存储中，而是直接从Web
    URL消耗CCTV录像，并将此Web URL馈送到AI服务中。我们仍然有一个小的ETL作业，消耗HTTP URL，调用AI服务，并将输出写入Azure Blob存储作为平面CSV文件，以便我们可以用BI工具消耗它。让我们开始吧！
- en: Setting Up the AI Service
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置AI服务
- en: 'We are using the same AI service that we deployed previously in [“Use Case:
    Parsing Documents with AI”](#use_case_parsing_documents_with_ai). If you haven’t
    completed it yet, go to the previous section, complete the steps listed under
    “Setting Up the AI Service,” and come back here. If you did it already, just follow
    along; there’s nothing more you need to do.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用与之前部署的AI服务相同的AI服务在[“用例：使用AI解析文档”](#use_case_parsing_documents_with_ai)。如果您还没有完成，请转到前一节，在“设置AI服务”下完成列出的步骤，然后返回此处。如果您已经完成了，只需跟着做，不需要再做其他事情。
- en: As it turns out, computer vision has many use cases that run under the same
    service. With the computer vision service that you have just deployed, you can
    not only extract text and detect objects, but also recognize brand names, popular
    landmarks, tag images, and many more. For a complete guide, check out [“Computer
    Vision Documentation”](https://oreil.ly/m3UD7) for Azure Cognitive Services.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，计算机视觉有许多使用案例都在同一个服务下运行。通过刚刚部署的计算机视觉服务，您不仅可以提取文本和检测对象，还可以识别品牌名称、热门地标、标记图像等等。要获取完整指南，请查看
    [“计算机视觉文档”](https://oreil.ly/m3UD7) ，这是Azure认知服务的一部分。
- en: Setting Up the Data Pipeline
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置数据管道
- en: Download *ETL_For_Image_Analysis.ipynb* from the [book’s website](https://oreil.ly/0uHwu).
    Just as before, open the file in your local IDE or upload it to Azure Notebooks.
    In addition to this file, you will also need *image-urls.txt*, which needs to
    be located in the same directory as the notebook file.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 从[书的网站](https://oreil.ly/0uHwu)下载 *ETL_For_Image_Analysis.ipynb*。与之前一样，将文件打开在本地IDE中或上传到Azure笔记本中。除了这个文件，您还需要
    *image-urls.txt*，它需要位于笔记本文件相同的目录中。
- en: If you open *ETL_For_Image_Analysis.ipynb*, you will see that it looks similar
    to the files that we used previously, as you can see in [Figure 9-44](#etl_notebook_for_image_analysis).
    I’ll quickly walk you through the main steps.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您打开 *ETL_For_Image_Analysis.ipynb*，您会发现它看起来与我们之前使用的文件相似，正如您在[图 9-44](#etl_notebook_for_image_analysis)中所看到的。我将快速为您介绍主要步骤。
- en: '![ETL notebook for image analysis](Images/apbi_0944.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图像分析的ETL笔记本](Images/apbi_0944.png)'
- en: Figure 9-44\. ETL notebook for image analysis
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-44\. 图像分析的ETL笔记本
- en: In section 0, update your Azure credentials. These are the same as those you
    used in the previous use case. After you enter them, run the first code cell.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在第0节中，更新您的Azure凭证。这些与您在先前用例中使用的相同。输入完后，运行第一个代码单元。
- en: In section 1, execute the code cell to import the TXT file with the image URLs.
    Make sure that you downloaded the file from the book’s repository and placed it
    in the same location as the *.ipynb* file.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1节中，执行代码单元以导入包含图像URL的TXT文件。确保您从书的存储库下载了该文件，并将其放置在与 *.ipynb* 文件相同的位置。
- en: Section 2 calls the computer vision AI service. In contrast to the use case
    before, object detection is a synchronous operation, so we need only one API call
    per image and can reduce the wait time to three seconds after each image to stay
    within the 20 images per minute limit of the free tier. If you are on a paid tier,
    you can safely delete the line `time.sleep(3)` from the script.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 第2节调用计算机视觉AI服务。与之前的用例相比，对象检测是同步操作，因此我们每张图片只需一次API调用，并且可以将等待时间减少到每张图片后的三秒钟，以保持在免费套餐的每分钟20张图片的限制内。如果您使用的是付费套餐，可以安全地从脚本中删除`time.sleep(3)`这行。
- en: 'Execute the code cell. While you wait for the results, this is a good time
    to draw your attention to some restrictions or preconditions for the object-detection
    AI service. These also apply, more or less, to other providers, not only Microsoft
    Azure. As we can read in the [Azure documentation](https://oreil.ly/dDToz), the
    following limitations apply:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这个代码单元格。在等待结果的同时，现在是注意一些对象检测AI服务的限制或前提条件的好时机。这些限制或多或少也适用于其他供应商，不仅限于Microsoft
    Azure。正如我们可以在[Azure文档](https://oreil.ly/dDToz)中看到的那样，以下限制适用：
- en: It’s important to note the limitations of object detection so you can avoid
    or mitigate the effects of false negatives (missed objects) and limited detail.
  id: totrans-269
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 需要注意的是对象检测的限制，以避免或减少假阴性（遗漏的对象）和有限的细节。
- en: Objects are generally not detected if they’re small (less than 5% of the image).
  id: totrans-270
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果物体很小（小于图像的5%），通常不会检测到物体。
- en: Objects are generally not detected if they’re arranged closely together (a stack
    of plates, for example).
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果物体排列紧密（例如一堆盘子），通常不会检测到物体。
- en: Objects are not differentiated by brand or product names (different types of
    sodas on a store shelf, for example). However, you can get brand information from
    an image by using the Brand detection feature.
  id: totrans-272
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对象不会根据品牌或产品名称进行区分（例如商店货架上的不同类型的汽水）。但是，您可以通过使用品牌检测功能从图像中获取品牌信息。
- en: Most importantly we have to consider the first point. We basically have to make
    sure that the objects of interest are large enough compared to the overall picture
    size. If you pay close attention to the image URLs in the text file, you will
    note that we are not passing the raw footage to the AI service but instead cropping
    these images on the fly using a content delivery network (CDN). This way, we can
    focus on the parts of the image that actually matter to us and ensure that a potential
    car object covers more than 5% of the image area. So, if you see poor results
    with an object-detection AI service, try cropping the image or splitting it into
    parts.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是我们要考虑第一个要点。我们基本上必须确保感兴趣的对象与整体图像大小相比足够大。如果您仔细注意文本文件中的图像URL，您会注意到我们并没有将原始镜头传递给AI服务，而是在使用内容传递网络（CDN）时即时裁剪这些图像。通过这种方式，我们可以专注于实际对我们有意义的图像部分，并确保潜在的汽车对象覆盖了图像面积的5%以上。因此，如果您在对象检测AI服务中看到效果不佳，请尝试裁剪图像或将其分割为部分。
- en: By now, the AI service should have done its job, and you can run the following
    code cell to print out an example result ([Figure 9-45](#example_output_of_the_computer_vision_s)).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，AI 服务应该已经完成了它的工作，您可以运行下面的代码单元格来打印一个示例结果（[图9-45](#example_output_of_the_computer_vision_s)）。
- en: '![Example output of the computer vision service for one image](Images/apbi_0945.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![一个图像的计算机视觉服务示例输出](Images/apbi_0945.png)'
- en: Figure 9-45\. Example output of the computer vision service for one image
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-45. 一个图像的计算机视觉服务示例输出
- en: As you can see, the AI service provides not only the names of the detected objects
    but also a little more context, such as the position of the object in the image
    and additional semantic information. Let’s move on to section 3, data transformations,
    where we want to parse these results to get the count of vehicles in an image.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，AI 服务不仅提供检测到的对象名称，还提供更多的上下文，例如对象在图像中的位置和额外的语义信息。接下来让我们进入第三部分，数据转换，我们想要解析这些结果以获取图像中车辆的计数。
- en: 'The approach here is rather simple. We are calling a function called `count_objects(result,
    "Car", 0.7)` that is filtering the outputs from the result according to two criteria:
    the object property name and the confidence score of the detection. I chose 0.7,
    which translates to something like “the AI service is 70% sure that the detected
    object is a car.” Feel free to experiment with the value and find out where the
    sweet spot is for your use case. Run this code cell and you will see a list with
    the counts.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的方法相当简单。我们调用一个名为`count_objects(result, "Car", 0.7)`的函数，该函数根据两个标准过滤结果的输出：对象属性名称和检测的置信度。我选择了0.7，这相当于“AI
    服务有70%的把握检测到的对象是一辆车。” 可以自由地尝试不同的值，找出适合你使用情况的最佳值。运行此代码单元格，您将看到一个包含计数的列表。
- en: The only task left is to put this into a nice table format that also includes
    the original image file URL to reference back to the data source and an index
    that makes sorting the data easier. Execute the second code cell of section 3
    and you should see output similar to [Figure 9-46](#tabular_data_output_from_the_image_anal).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的任务是将其放入一个漂亮的表格格式中，还包括原始图像文件URL以引用数据源和一个使数据排序更容易的索引。执行第3节的第二个代码单元格，您应该看到类似于[图9-46](#tabular_data_output_from_the_image_anal)的输出。
- en: '![Tabular data output from the image analysis service](Images/apbi_0946.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图像分析服务输出的表格数据](Images/apbi_0946.png)'
- en: Figure 9-46\. Tabular data output from the image analysis service
  id: totrans-281
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-46\. 图像分析服务输出的表格数据
- en: Finally, run the last code cell in section 4 to upload the CSV file to our Azure
    Blob Storage. And that’s everything we had to do for our prototype ETL job. Let’s
    move on to visualizing the results in BI!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第4节的最后一个代码单元格中运行，将CSV文件上传到我们的Azure Blob Storage。这就是我们原型ETL作业所需做的一切。接下来让我们开始在BI中可视化结果！
- en: Warning
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Don’t forget to stop the Azure compute resource after you finish the script
    to avoid any ongoing charges!
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 完成脚本后，请别忘了停止Azure计算资源，以避免继续产生费用！
- en: Model Inference with Power BI Walk-Through
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Model Inference with Power BI Walk-Through
- en: Since we have stored the AI results in a flat CSV, any BI tool can make use
    of this data. I will show you again how I have done it with Power BI—and there
    will be a little surprise!
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将AI结果存储在扁平化的CSV文件中，任何BI工具都可以利用这些数据。我将再次展示如何在Power BI中使用它——并且会有一个小惊喜！
- en: First, create a new Power BI file. Choose Get Data → Azure Blob Storage, provide
    your storage account name, and select the container called “tables.” Click Transform
    and click the Binary link in the row where the new table name *traffic_counts.csv*
    is shown (see [Figure 9-47](#azure_blob_storage_files_listed_in_powe)). If you
    can’t see this file, refresh the preview in Power Query.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个新的Power BI文件。选择“获取数据”→“Azure Blob Storage”，提供您的存储账户名称，并选择名为“tables”的容器。点击“转换”并点击显示新表名为*traffic_counts.csv*的行上的二进制链接（参见[图9-47](#azure_blob_storage_files_listed_in_powe)）。如果看不到此文件，请在Power
    Query中刷新预览。
- en: '![Azure Blob Storage files listed in Power Query](Images/apbi_0947.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![在Power Query中列出的Azure Blob Storage文件](Images/apbi_0947.png)'
- en: Figure 9-47\. Azure Blob Storage files listed in Power Query
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-47\. 在Power Query中列出的Azure Blob Storage文件
- en: In the Power Query Editor, we don’t have much to do. Just give the index column
    a name by right-clicking the column name and choosing Rename, as shown in [Figure 9-48](#renaming_the_column_in_power_query).
    Rename the column to `**Index**`.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在Power Query Editor中，我们没什么可做的。只需右键点击索引列名并选择重命名，如[图9-48](#renaming_the_column_in_power_query)所示。将列重命名为`**Index**`。
- en: '![Renaming the column in Power Query](Images/apbi_0948.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![在Power Query中重命名列](Images/apbi_0948.png)'
- en: Figure 9-48\. Renaming the column in Power Query
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-48\. 在Power Query中重命名列
- en: 'Double-check that all three columns are in the table: the index (numeric),
    the count (numeric), and the file URL (string). Confirm the query by clicking
    Close & Apply.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 双重检查表格中是否包含所有三列：索引（数字）、计数（数字）和文件URL（字符串）。点击“关闭并应用”以确认查询。
- en: Before we head over to build the report, let’s include one small but powerful
    feature that Power BI is offering for us. Click the small table icon that is located
    between the data model and the report icon on the left side ([Figure 9-49](#setting_the_data_category_to_image_url)).
    In this window, select the URL column and choose “Column tools” from the menu
    pane on the top. Find the field “Data category,” which is a bit hidden in the
    top menu, and change the data type from text to Image URL, as shown in [Figure 9-49](#setting_the_data_category_to_image_url).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建报告之前，让我们包含Power BI为我们提供的一个小而强大的功能。点击左侧（数据模型和报告图标之间）的小表格图标，如[图9-49](#setting_the_data_category_to_image_url)所示。在这个窗口中，选择URL列，并从顶部菜单窗格中选择“列工具”。找到稍微隐藏在顶部菜单中的“数据类别”字段，并将数据类型从文本更改为图像URL，如[图9-49](#setting_the_data_category_to_image_url)所示。
- en: '![Setting the data category to Image URL in Power BI](Images/apbi_0949.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![在Power BI中设置数据类别为图像URL](Images/apbi_0949.png)'
- en: Figure 9-49\. Setting the data category to Image URL in Power BI
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-49\. 在Power BI中设置数据类别为图像URL
- en: Building the AI-Powered Dashboard in Power BI
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Power BI中构建AI驱动的仪表板
- en: 'Now, let’s move on to the report page. In this example, I chose only two components:
    a line chart for the count over time (in our case, the index) and a table showing
    the count, index, and image URL. And because we have formatted the URL field as
    an Image URL data type, Power BI will fetch the image from this URL and show it
    in our dashboard. Isn’t this marvelous? Take a look at [Figure 9-50](#ai_powered_traffic_monitoring_dashboard)
    to see the final dashboard in action.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入报告页面。在此示例中，我仅选择了两个组件：一个用于随时间计数（在我们的案例中是指数）的折线图，以及一个显示计数、指数和图像 URL 的表格。因为我们已将
    URL 字段格式化为图像 URL 数据类型，Power BI 将从此 URL 获取图像并显示在我们的仪表板中。这不是很神奇吗？查看[图 9-50](#ai_powered_traffic_monitoring_dashboard)
    看看最终仪表板的效果。
- en: '![AI-powered traffic-monitoring dashboard](Images/apbi_0950.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![AI 强化的交通监控仪表板](Images/apbi_0950.png)'
- en: Figure 9-50\. AI-powered traffic-monitoring dashboard
  id: totrans-300
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-50\. AI 强化的交通监控仪表板
- en: Of course, this table inherits the full interactivity that you expect from a
    Power BI visual. So when you select, for example, one point with the highest traffic
    count on index 80, the table will show the relevant image for this data point,
    as [Figure 9-51](#selecting_single_data_points_in_the_rep) shows. This way, we
    can also ensure that the AI detection was correct by spotting four cars in the
    image.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，此表格具有您从 Power BI 可视化中期望的完全互动性。因此，当您选择例如在指数 80 上具有最高流量计数的一个点时，表格将显示此数据点的相关图像，就像[图
    9-51](#selecting_single_data_points_in_the_rep) 中展示的那样。这样，我们还可以通过观察图像中的四辆车确保 AI
    检测是正确的。
- en: '![Selecting single data points in the report](Images/apbi_0951.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![在报告中选择单个数据点](Images/apbi_0951.png)'
- en: Figure 9-51\. Selecting single data points in the report
  id: totrans-303
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-51\. 在报告中选择单个数据点
- en: Feel free to re-create this dashboard by yourself or modify it as you like.
    You can find the final version on the [book’s website](https://oreil.ly/0uHwu)
    by downloading the *Traffic-Monitoring_AI-Powered.pbix* file.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎您按照自己的需求重新创建或修改此仪表板。您可以在[书籍网站](https://oreil.ly/0uHwu)下载 *Traffic-Monitoring_AI-Powered.pbix*
    文件的最终版本。
- en: Cleaning Up Resources
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理资源
- en: 'Consider stopping or deleting the following resources we have used in this
    chapter to avoid any ongoing charges:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，考虑停止或删除我们在此章节中使用的以下资源，以避免任何持续的费用：
- en: Stop the compute resource in Azure ML Studio.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停止 Azure ML Studio 中的计算资源。
- en: Delete the Azure Cognitive Services that you have deployed.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除您部署的 Azure 认知服务。
- en: Delete all files from Azure Blob Storage.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除 Azure Blob 存储中的所有文件。
- en: We will still need the Azure resource group for the use case in the final chapter.
    If you don’t plan to continue, you can delete all resources at once. Select “Resource
    groups” in your Azure portal, select the resource group that you created, and
    choose “Delete resource group.” Confirm the resource group name and click Delete.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要 Azure 资源组用于最终章节的使用案例。如果您不打算继续使用，请一次性删除所有资源。在您的 Azure 门户中选择“资源组”，选择您创建的资源组，然后选择“删除资源组”。确认资源组名称，然后单击“删除”。
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter has only touched the surface of what AI services can do with data
    sources that are not in the usual CSV or Excel format. I hope that you had fun
    trying out these AI services, and I wish even more that you gained more inspiration
    on how to use these tools for your own use cases.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 本章只是触及了 AI 服务能够处理非常规 CSV 或 Excel 格式数据源的表面。希望您尝试了这些 AI 服务后感到愉快，并希望您能从中获取更多灵感，以便在您自己的用例中使用这些工具。
- en: This chapter concluded the building blocks of AI services that can empower your
    BI. In the next chapter, we will take a look at how to combine what you’ve learned
    in order to build a fully functional BI dashboard with multiple AI services at
    work.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了可以增强您的 BI 的 AI 服务构建模块。在下一章中，我们将探讨如何结合您学到的内容，构建一个具有多个 AI 服务的完全功能的 BI 仪表板。
