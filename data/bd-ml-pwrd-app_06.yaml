- en: Chapter 4\. Acquire an Initial Dataset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章。获取初始数据集
- en: Once you have a plan to solve your product needs and have built an initial prototype
    to validate that your proposed workflow and model are sound, it is time to take
    a deeper dive into your dataset. We will use what we find to inform our modeling
    decisions. Oftentimes, understanding your data well leads to the biggest performance
    improvements.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您制定了解决产品需求并构建了初始原型以验证您提议的工作流程和模型是否合理的计划，就是深入研究您的数据集的时候了。我们将利用我们发现的内容来指导我们的建模决策。通常情况下，深入了解您的数据会带来最大的性能改进。
- en: In this chapter, we will start by looking at ways to efficiently judge the quality
    of a dataset. Then, we will cover ways to vectorize your data and how to use said
    vectorized representation to label and inspect a dataset more efficiently. Finally,
    we’ll cover how this inspection should guide feature generation strategies.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先看看如何高效评估数据集的质量。然后，我们将涵盖如何对数据进行向量化以及如何使用这种向量化表示更高效地标记和检查数据集。最后，我们将探讨这种检查如何指导特征生成策略。
- en: Let’s start by discovering a dataset and judging its quality.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从发现数据集并评估其质量开始。
- en: Iterate on Datasets
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代数据集
- en: The fastest way to build an ML product is to rapidly build, evaluate, and iterate
    on models. Datasets themselves are a core part of that success of models. This
    is why data gathering, preparation, and labeling should be seen as an *iterative
    process*, just like modeling. Start with a simple dataset that you can gather
    immediately, and be open to improving it based on what you learn.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 构建机器学习产品的最快方法是快速构建、评估和迭代模型。数据集本身是模型成功的核心组成部分。这就是为什么数据收集、准备和标记应该被视为一个*迭代过程*，就像建模一样。从您可以立即获取的简单数据集开始，并根据您所学到的知识进行改进。
- en: This iterative approach to data can seem confusing at first. In ML research,
    performance is often reported on standard datasets that the community uses as
    benchmarks and are thus immutable. In traditional software engineering, we write
    deterministic rules for our programs, so we treat data as something to receive,
    process, and store.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对数据的迭代方法一开始可能会令人困惑。在机器学习研究中，性能通常是报告在社区中作为基准使用的标准数据集上的。因此，这些数据集是不可变的。在传统软件工程中，我们为程序编写确定性规则，因此我们将数据视为接收、处理和存储的内容。
- en: ML engineering combines engineering and ML in order to build products. Our dataset
    is thus just another tool to allow us to build products. In ML engineering, choosing
    an initial dataset, regularly updating it, and augmenting it is often the *majority
    of the work*. This difference in workflow between research and industry is illustrated
    in [Figure 4-1](#iterative_data_collection).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工程结合了工程和机器学习，以便构建产品。因此，我们的数据集只是允许我们构建产品的另一种工具。在机器学习工程中，选择初始数据集、定期更新和增强它通常是*大部分的工作*。这种工作流程在研究和行业之间的差异在[图4-1](#iterative_data_collection)中有所体现。
- en: '![Datasets are fixed in research, but part of a product in industry](assets/bmla_0401.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![研究中的数据集是固定的，但在工业中是产品的一部分](assets/bmla_0401.png)'
- en: Figure 4-1\. Datasets are fixed in research, but part of the product in industry
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1。研究中的数据集是固定的，但在工业中是产品的一部分
- en: Treating data as part of your product that you can (and should) iterate on,
    change, and improve is often a big paradigm shift for newcomers to the industry.
    Once you get used to it, however, data will become your best source of inspiration
    to develop new models and the first place you look for answers when things go
    wrong.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据视为您可以（并且应该）迭代、更改和改进的产品的一部分，对于行业新人来说通常是一种重大范式转变。然而，一旦你习惯了，数据将成为您开发新模型的最佳灵感来源，并且当事情出错时您寻找答案的第一处去处。
- en: Do Data Science
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行数据科学
- en: I’ve seen the process of curating a dataset be the main roadblock to building
    ML products more times than I can count. This is partly because of the relative
    lack of education on the topic (most online courses provide the dataset and focus
    on the models), which leads to many practitioners fearing this part of the work.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我见过整理数据集的过程比我能数的次数更多次成为构建机器学习产品的主要障碍。这部分原因是因为相对缺乏有关该主题的教育（大多数在线课程提供数据集并侧重于模型），这导致许多从业者害怕这部分工作。
- en: It is easy to think of working with data as a chore to tackle before playing
    with fun models, but models only serve as a way to extract trends and patterns
    from existing data. Making sure that the data we use exhibits patterns that are
    predictive enough for a model to leverage (and checking whether it contains clear
    bias) is thus a fundamental part of the work of a data scientist (in fact, you
    may have noticed the name of the role is not model scientist).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易认为处理数据是在玩乐模型之前要解决的烦心事，但模型只是从现有数据中提取趋势和模式的一种方法。确保我们使用的数据展现出足够预测模型利用的模式（并检查它是否包含明显偏差）因此是数据科学家工作的基本部分（事实上，您可能已经注意到角色的名称并不是模型科学家）。
- en: This chapter will focus on this process, from gathering an initial dataset to
    inspecting and validating its applicability for ML. Let’s start with exploring
    a dataset efficiently to judge its quality.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将专注于这一过程，从收集初始数据集到检查和验证其在机器学习中的适用性。让我们从有效地探索数据集开始，以评估其质量。
- en: Explore Your First Dataset
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索你的第一个数据集
- en: So how do we go about exploring an initial dataset? The first step of course
    is to gather a dataset. This is where I see practitioners get stuck the most often
    as they search for a perfect dataset. Remember, our goal is to get a simple dataset
    to extract preliminary results from. As with other things in ML, start simple,
    and build from there.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何开始探索初始数据集呢？当然，第一步是收集数据集。这是我看到从业者最常陷入困境的地方，因为他们寻找完美的数据集。记住，我们的目标是从中获得初步结果的简单数据集。与机器学习中的其他事物一样，从简单开始，逐步扩展。
- en: Be Efficient, Start Small
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高效率，从小处开始
- en: For most ML problems, more data can lead to a better model, but this does not
    mean that you should start with the largest possible dataset. When starting on
    a project, a small dataset allows you to easily inspect and understand your data
    and how to model it better. You should aim for an initial dataset that is easy
    to work with. Only once you’ve settled on a strategy does it make sense to scale
    it up to a larger size.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数机器学习问题，更多的数据可能会导致更好的模型，但这并不意味着你应该从最大可能的数据集开始。在开始项目时，一个小数据集可以让你轻松检查和理解数据及其更好地建模方式。你应该瞄准一个易于处理的初始数据集。只有一旦你制定了策略，扩展到更大的规模才有意义。
- en: If you are working at a company with terabytes of data stored in a cluster,
    you can start by extracting a uniformly sampled subset that fits in memory on
    your local machine. If you would like to start working on a side project trying
    to identify the brands of cars that drive in front of your house, for example,
    start with a few dozens of images of cars on streets.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个存储有数百万兆字节数据的集群中工作，你可以开始提取一个在本地机器内存中适合的均匀抽样子集。例如，如果您想开始一个副业项目，试图识别驶过你家前面的汽车品牌，请从街上几十辆汽车的图像开始。
- en: Once you have seen how your initial model performs and where it struggles, you
    will be able to iterate on your dataset in an informed manner!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你看到你的初始模型表现及其困难之处，你将能够以经过明智决策的方式迭代你的数据集！
- en: You can find many existing datasets online on platforms such as [Kaggle](https://www.kaggle.com/)
    or [Reddit](https://www.reddit.com/r/datasets) or gather a few examples yourself,
    either by scraping the web, leveraging large open datasets such as found on the
    [Common Crawl site](https://commoncrawl.org), or generating data! For more information,
    see [“Open data”](ch02.html#open_datasets).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在诸如[Kaggle](https://www.kaggle.com/)或[Reddit](https://www.reddit.com/r/datasets)等平台上找到许多现有的数据集，或者自行收集一些示例，可以通过网页抓取，利用大型开放数据集（如[Common
    Crawl site](https://commoncrawl.org)）或生成数据！欲了解更多信息，请参阅[“开放数据”](ch02.html#open_datasets)。
- en: Gathering and analyzing data is not only necessary, it will speed you up, especially
    early on in a project’s development. Looking at your dataset and learning about
    its features is the easiest way to come up with a good modeling and feature generation
    pipeline.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 收集和分析数据不仅是必要的，尤其是在项目早期阶段，它还能加速你的进展。查看你的数据集并了解其特征是提出良好建模和特征生成管道的最简单方法。
- en: Most practitioners overestimate the impact of working on the model and underestimate
    the value of working on the data, so I recommend always making an effort to correct
    this trend and bias yourself toward looking at data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数从业者高估了工作模型的影响，低估了处理数据的价值，因此我建议始终努力纠正这种趋势，偏向于查看数据。
- en: When examining data, it is good to identify trends in an exploratory fashion,
    but you shouldn’t stop there. If your aim is to build ML products, you should
    ask yourself what the best way to leverage these trends in an automated fashion
    is. How can these trends help you power an automated product?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查数据时，探索性地识别趋势是一个很好的方法，但你不应该止步于此。如果您的目标是构建机器学习产品，您应该问自己如何以自动化的方式利用这些趋势。这些趋势如何帮助您驱动一个自动化的产品？
- en: Insights Versus Products
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 洞察力与产品
- en: Once you have a dataset, it is time to dive into it and explore its content.
    As we do so, let’s keep in mind the distinction between data exploration for analysis
    purposes and data exploration for product building purposes. While both aim to
    extract and understand trends in data, the former concerns itself with creating
    insights from trends (learning that most fraudulent logins to a website happen
    on Thursdays and are from the Seattle area, for example), while the latter is
    about using trends to build features (using the time of a login attempt and its
    IP address to build a service that prevents fraudulent accounts logins).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您获得了数据集，现在是时候深入探讨其内容了。当我们这样做时，让我们记住数据探索的目的分为分析目的和产品构建目的两种。虽然两者都旨在从数据趋势中提取和理解，但前者关注从趋势中创造洞察力（例如，学习到大多数网站的欺诈登录发生在星期四，并且来自西雅图地区），而后者则是利用趋势来构建功能（使用登录尝试的时间和其IP地址来构建防止欺诈账户登录的服务）。
- en: While the difference may seem subtle, it leads to an extra layer of complexity
    in the product building case. We need to have confidence that the patterns we
    see will apply to data we receive in the future and quantify the differences between
    the data we are training on and the data we expect to receive in production.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管差异可能看似微妙，但在产品构建的情况下，这导致了额外的复杂性层级。我们需要确信我们所看到的模式将适用于将来收到的数据，并量化我们在训练数据和预期收到的生产数据之间的差异。
- en: For fraud prediction, noticing a seasonality aspect to fraudulent logins is
    the first step. We should then use this observed seasonal trend to estimate how
    often we need to train our models on recently gathered data. We will dive into
    more examples as we explore our data more deeply later in this chapter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于欺诈预测来说，注意欺诈登录的季节性是第一步。然后，我们应该利用观察到的季节性趋势来估计我们需要多频繁地基于最近收集到的数据来训练我们的模型。在本章的后续部分，随着我们更深入地探索数据，我们将深入探讨更多示例。
- en: Before noticing predictive trends, we should start by examining quality. If
    our chosen dataset does not meet quality standards, we should improve it before
    moving on to modeling.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意到预测趋势之前，我们应该首先检查质量。如果我们选择的数据集不符合质量标准，我们应该在进行建模之前对其进行改进。
- en: A Data Quality Rubric
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量评估表
- en: In this section, we will cover some aspects to examine when first working with
    a new dataset. Each dataset comes with its own biases and oddities, which require
    different tools to be understood, so writing a comprehensive rubric covering anything
    you may want to look for in a dataset is beyond the scope of this book. Yet, there
    are a few categories that are valuable to pay attention to when first approaching
    a dataset. Let’s start with formatting.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖首次使用新数据集时需要检查的一些方面。每个数据集都带有其自己的偏见和奇怪之处，需要不同的工具来理解，因此编写一份涵盖您可能想要在数据集中查找的任何内容的全面规则已超出本书的范围。然而，在初次接触数据集时，有几个类别是值得注意的。让我们从格式开始。
- en: Data format
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据格式
- en: Is the dataset already formatted in such a way that you have clear inputs and
    outputs, or does it require additional preprocessing and labeling?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是否已经格式化，使您拥有清晰的输入和输出，或者是否需要额外的预处理和标记？
- en: When building a model that attempts to predict whether a user will click on
    an ad, for example, a common dataset will consist of a historical log of all clicks
    for a given time period. You would need to transform this dataset so that it contains
    multiple instances of an ad being presented to a user and whether the user clicked.
    You’d also want to include any features of the user or the ad that you think your
    model could leverage.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当构建一个试图预测用户是否会点击广告的模型时，常见的数据集将包含一个给定时间段内所有点击的历史记录。您需要转换此数据集，使其包含向用户展示广告的多个实例以及用户是否点击了广告。您还希望包括您认为您的模型可以利用的用户或广告的任何特征。
- en: If you are given a dataset that has already been processed or aggregated for
    you, you should validate that you understand the way in which the data was processed.
    If one of the columns you were given contains an average conversion rate, for
    example, can you calculate this rate yourself and verify that it matches with
    the provided value?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你得到了一个已经处理或聚合过的数据集，你应该验证你理解数据处理的方式。比如，如果你得到的列包含了平均转化率，你能计算出这个转化率并验证它与提供的数值是否一致吗？
- en: In some cases, you will not have access to the required information to reproduce
    and validate preprocessing steps. In those cases, looking at the quality of the
    data will help you determine which features of it you trust and which ones would
    be best left ignored.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，你将无法访问到重现和验证预处理步骤所需的信息。在这些情况下，查看数据质量将帮助你确定哪些特征是可信的，哪些是最好忽略的。
- en: Data quality
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据的质量
- en: Examining the quality of a dataset is crucial before you start modeling it.
    If you know that half of the values for a crucial feature are missing, you won’t
    spend hours debugging a model to try to understand why it isn’t performing well.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始建模之前，检查数据集的质量至关重要。如果你知道一个关键特征的一半数值是缺失的，你就不会花几个小时来调试一个模型，试图弄清楚为什么模型表现不佳。
- en: There are many ways in which data can be of poor quality. It can be missing,
    it can be imprecise, or it can even be corrupted. Getting an accurate picture
    of its quality will not only allow you to estimate which level of performance
    is reasonable, it will make it easier to select potential features and models
    to use.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以有许多质量低下的方式。它可能缺失，可能不精确，甚至可能损坏。准确评估其质量不仅能让你估计合理的性能水平，还能更容易地选择潜在的特征和模型使用。
- en: If you are working with logs of user activity to predict usage of an online
    product, can you estimate how many logged events are missing? For the events you
    do have, how many contain only a subset of information about the user?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用用户活动日志来预测在线产品的使用情况，你能估计有多少事件没有被记录下来吗？对于你已经记录的事件，有多少只包含用户信息的一个子集？
- en: If you are working on natural language text, how would you rate the quality
    of the text? For example, are there many incomprehensible characters? Is the spelling
    very erroneous or inconsistent?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在处理自然语言文本，你会如何评价文本的质量？例如，是否有很多难以理解的字符？拼写错误或不一致的情况很多吗？
- en: If you are working on images, are they clear enough that you could perform the
    task yourself? If it is hard for you to detect an object in an image, do you think
    your model will struggle to do so?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处理的是图像，它们的清晰度足够高，你能自己执行任务吗？如果你在图像中难以检测一个物体，你觉得你的模型会遇到困难吗？
- en: In general, which proportion of your data seems noisy or incorrect? How many
    inputs are hard for you to interpret or understand? If the data has labels, do
    you tend to agree with them, or do you often find yourself questioning their accuracy?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，你的数据中有多大比例是嘈杂或不正确的？有多少输入对你来说是难以解释或理解的？如果数据有标签，你是否倾向于同意它们，还是经常质疑它们的准确性？
- en: I’ve worked on a few projects aiming to extract information from satellite imagery,
    for example. In the best cases, these projects have access to a dataset of images
    with corresponding annotations denoting objects of interest such as fields or
    planes. In some cases, however, these annotations can be inaccurate or even missing.
    Such errors have a significant impact on any modeling approach, so it is vital
    to find out about them early. We can work with missing labels by either labeling
    an initial dataset ourselves or finding a weak label we can use, but we can do
    so only if we notice the quality *ahead of time*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我曾经参与过一些从卫星图像中提取信息的项目。在最好的情况下，这些项目可以访问到带有相应注释的图像数据集，标记出感兴趣的对象，如田地或飞机。然而，在某些情况下，这些注释可能是不准确的，甚至缺失的。这些错误对任何建模方法都有显著影响，因此及早了解这些问题非常重要。我们可以通过自己为初始数据集标注标签或找到一个可以使用的弱标签来处理缺失的标签，但前提是我们要在时间*上有所察觉*。
- en: 'After verifying the format and quality of the data, one additional step can
    help proactively surface issues: examining data quantity and feature distribution.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证数据的格式和质量之后，还有一个额外的步骤可以帮助及早发现问题：检查数据的数量和特征分布。
- en: Data quantity and distribution
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据的数量和分布
- en: Let’s estimate whether we have enough data and whether feature values seem within
    a reasonable range.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估一下我们是否有足够的数据，以及特征值是否在合理范围内。
- en: How much data do we have? If we have a large dataset, we should select a subset
    to start our analysis on. On the other hand, if our dataset is too small or some
    classes are underrepresented, models we train would risk being just as biased
    as our data. The best way to avoid such bias is to increase the diversity of our
    data through data gathering and augmentation. The ways in which you measure the
    quality of your data depend on your dataset, but [Table 4-1](#qual_format_quantity)
    covers a few questions to get you started.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多少数据？如果我们有大量数据集，我们应选择一个子集来开始分析。另一方面，如果我们的数据集太小或某些类别代表性不足，那么我们训练的模型可能会像我们的数据一样存在偏差。避免这种偏差的最佳方法是通过数据收集和增强增加数据的多样性。您衡量数据质量的方式取决于您的数据集，但
    [表 4-1](#qual_format_quantity) 涵盖了一些问题，供您开始使用。
- en: Table 4-1\. A data quality rubric
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-1\. 数据质量评分表
- en: '| Quality | Format | Quantity and distribution |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 质量 | 格式 | 数量和分布 |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Are any relevant fields ever empty? | How many preprocessing steps does your
    data require? | How many examples do you have? |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 有任何相关字段是空的吗？ | 您的数据需要多少预处理步骤？ | 您有多少示例？ |  |'
- en: '| Are there potential errors of measurement? | Will you be able to preprocess
    it in the same way in production? | How many examples per class? Are any absent?
    |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 存在测量错误的可能性吗？ | 您能在生产中以相同的方式预处理吗？ | 每个类别有多少示例？有任何缺失的吗？ |  |'
- en: For a practical example, when building a model to automatically categorize customer
    support emails into different areas of expertise, a data scientist I was working
    with, Alex Wahl, was given nine distinct categories, with only one example per
    category. Such a dataset is too small for a model to learn from, so he focused
    most of his effort on a [data generation strategy](https://oreil.ly/KRn0B). He
    used templates of common formulations for each of the nine categories to produce
    thousands more examples that a model could then learn from. Using this strategy,
    he managed to get a pipeline to a much higher level of accuracy than he would
    have had by trying to build a model complex enough to learn from only nine examples.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个实际例子，当构建一个模型自动将客户支持电子邮件分类到不同的专业领域时，我与之合作的数据科学家 Alex Wahl 收到了九个不同类别的示例，每个类别只有一个示例。这样的数据集对于模型学习来说太小了，所以他将大部分精力集中在了一个
    [数据生成策略](https://oreil.ly/KRn0B) 上。他使用常见表述的模板为每个九个类别生成了成千上万个示例，模型可以从中学习。通过这种策略，他成功将流水线的准确性提升到比仅仅从九个示例中尝试构建足够复杂的模型要高得多的水平。
- en: Let’s apply this exploration process to the dataset we chose for our ML editor
    and estimate its quality!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个探索过程应用到我们为 ML 编辑器选择的数据集上，并估计其质量！
- en: ML editor data inspection
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ML 编辑器数据检查
- en: For our ML editor, we initially settled on using the anonymized [Stack Exchange
    Data Dump](https://oreil.ly/6jCGY) as a dataset. Stack Exchange is a network of
    question-and-answer websites, each focused on a theme such as philosophy or gaming.
    The data dump contains many archives, one for each of the websites in the Stack
    Exchange network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 ML 编辑器，我们最初决定使用匿名化的 [Stack Exchange 数据转储](https://oreil.ly/6jCGY) 作为数据集。Stack
    Exchange 是一个以哲学或游戏等主题为中心的问答网站网络。数据转储包含许多存档，每个存档对应 Stack Exchange 网络中的一个网站。
- en: For our initial dataset, we’ll choose a website that seems like it would contain
    broad enough questions to build useful heuristics from. At first glance, the [Writing
    community](https://writing.stackexchange.com/) seems like a good fit.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的初始数据集，我们选择了一个看起来足够广泛的网站，以便从中构建有用的启发式。乍一看，[写作社区](https://writing.stackexchange.com/)
    看起来很合适。
- en: 'Each website archive is provided as an XML file. We need to build a pipeline
    to ingest those files and transform them into text we can then extract features
    from. The following example shows the `Posts.xml` file for *datascience.stackexchange.com*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 每个网站存档都以 XML 文件的形式提供。我们需要建立一个流水线来摄取这些文件，并将其转换为我们可以从中提取特征的文本。以下示例展示了 *datascience.stackexchange.com*
    的 `Posts.xml` 文件：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To be able to leverage this data, we will need to be able to load the XML file,
    decode the HTML tags in the text, and represent questions and associated data
    in a format that would be easier to analyze such as a pandas DataFrame. The following
    function does just this. As a reminder, the code for this function, and all other
    code throughout this book, can be found in [this book’s GitHub repository](https://oreil.ly/ml-powered-applications).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够利用这些数据，我们需要能够加载XML文件，解码文本中的HTML标记，并以更易于分析的格式表示问题和关联数据，例如pandas DataFrame。以下函数就是这样做的。提醒一下，本书中此函数的代码和所有其他代码都可以在[本书的GitHub存储库](https://oreil.ly/ml-powered-applications)中找到。
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Even for a relatively small dataset containing only 30,000 questions this process
    takes more than a minute, so we serialize the processed file back to disk to only
    have to process it once. To do this, we can simply use panda’s `to_csv` function,
    as shown on the final line of the snippet.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于只包含30,000个问题的相对较小的数据集，此过程也需要超过一分钟的时间，因此我们将处理后的文件序列化回磁盘，只需处理一次。为此，我们可以简单地使用pandas的`to_csv`函数，如代码片段的最后一行所示。
- en: This is generally a recommended practice for any preprocessing required to train
    a model. Preprocessing code that runs right before the model optimization process
    can slow down experimentation significantly. As much as possible, always preprocess
    data ahead of time and serialize it to disk.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是训练模型所需的任何预处理的推荐实践。在模型优化过程之前运行的预处理代码可能会显著减慢实验。尽可能在预处理数据并将其序列化到磁盘。
- en: Once we have our data in this format, we can examine the aspects we described
    earlier. The entire exploration process we detail next can be found in the dataset
    exploration notebook in [this book’s GitHub repository](https://oreil.ly/ml-powered-applications).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据格式确定，我们可以检查前面描述的各个方面。我们接下来详细介绍的整个探索过程可以在[本书的GitHub存储库](https://oreil.ly/ml-powered-applications)的数据集探索笔记本中找到。
- en: 'To start, we use `df.info()` to display summary information about our DataFrame,
    as well as any empty values. Here is what it returns:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`df.info()`来显示关于我们的DataFrame的摘要信息，以及任何空值。这是它的返回结果：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can see that we have a little over 31,000 posts, with only about 4,000 of
    them having an accepted answer. In addition, we can notice that some of the values
    for `Body`, which represents the contents of a post, are null, which seems suspicious.
    We would expect all posts to contain text. Looking at rows with a null `Body`
    quickly reveals they belong to a type of post that has no reference in the documentation
    provided with the dataset, so we remove them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们有超过31,000个帖子，其中约4,000个帖子有一个被接受的答案。此外，我们可以注意到`Body`的一些值，代表帖子的内容，是空的，这似乎有些可疑。我们期望所有帖子都包含文本。快速查看具有空`Body`的行，很快发现它们属于数据集提供的文档中没有参考的一种帖子类型，因此我们将其移除。
- en: Let’s quickly dive into the format and see if we understand it. Each post has
    a `PostTypeId` value of 1 for a question, or 2 for an answer. We would like to
    see which type of questions receive high scores, as we would like to use a question’s
    score as a weak label for our true label, the quality of a question.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解一下格式，看看我们是否理解它。每个帖子的`PostTypeId`值为1表示问题，2表示答案。我们希望查看哪种类型的问题获得了高分，因为我们希望将问题的分数作为真正标签的弱标签，即问题的质量。
- en: First, let’s match questions with the associated answers. The following code
    selects all questions that have an accepted answer and joins them with the text
    for said answer. We can then look at the first few rows and validate that the
    answers do match up with the questions. This will also allow us to quickly look
    through the text and judge its quality.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将问题与相关答案进行匹配。以下代码选择所有具有已接受答案的问题，并将它们与所述答案的文本连接起来。然后我们可以查看前几行，并验证答案是否与问题匹配。这还允许我们快速查看文本并评估其质量。
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In [Table 4-2](#q_a), we can see that questions and answers do seem to match
    up and that the text seems mostly correct. We now trust that we can match questions
    with their associated answers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表4-2](#q_a)中，我们可以看到问题和答案似乎是匹配的，并且文本看起来大部分是正确的。我们现在相信我们可以将问题与其关联的答案匹配起来了。
- en: Table 4-2\. Questions with their associated answers
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-2\. 带有其关联答案的问题
- en: '| Id | body_text | body_text_answer |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Id | body_text | body_text_answer |'
- en: '| --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | I’ve always wanted to start writing (in a totally amateur way), but whenever
    I want to start something I instantly get blocked having a lot of questions and
    doubts.\nAre there some resources on how to start becoming a writer?\nl’m thinking
    something with tips and easy exercises to get the ball rolling.\n | When I’m thinking
    about where I learned most how to write, I think that reading was the most important
    guide to me. This may sound silly, but by reading good written newspaper articles
    (facts, opinions, scientific articles, and most of all, criticisms of films and
    music), I learned how others did the job, what works and what doesn’t. In my own
    writing, I try to mimic other people’s styles that I liked. Moreover, I learn
    new things by reading, giving me a broader background that I need when re… |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 我一直想要开始写作（完全是业余的），但每当我想要开始时，我立刻被各种问题和疑虑所困扰。\n有没有一些关于如何开始成为作家的资源？\n我想要一些有关提示和简单练习的东西，以便让我开始写作。\n
    | 当我考虑我是如何学会写作的时候，我觉得阅读对我来说是最重要的指导。这可能听起来很傻，但通过阅读写得好的报纸文章（事实、观点、科学文章，尤其是影视和音乐评论），我学会了别人是如何做这份工作的，什么是有效的，什么是无效的。在我自己的写作中，我尝试模仿我喜欢的其他人的风格。此外，通过阅读，我学到了新的知识，为我提供了更广泛的背景，这在重新…
    |'
- en: '| 2 | What kind of story is better suited for each point of view? Are there
    advantages or disadvantages inherent to them?\nFor example, writing in the first
    person you are always following a character, while in the third person you can
    “jump” between story lines.\n | With a story in first person, you are intending
    the reader to become much more attached to the main character. Since the reader
    sees what that character sees and feels what that character feels, the reader
    will have an emotional investment in that character. Third person does not have
    this close tie; a reader can become emotionally invested but it will not be as
    strong as it will be in first person.\nContrarily, you cannot have multiple point
    characters when you use first person without ex… |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 对于每种视角，哪种故事更适合？它们固有的优势或劣势是什么？\n例如，第一人称写作时，你总是跟随一个角色，而在第三人称写作中，你可以在不同的故事线之间“跳跃”。\n
    | 使用第一人称讲述故事时，你意图让读者更加投入到主角身上。因为读者看到主角看到的东西，感受到主角的感受，读者会对这个角色产生情感投入。第三人称则没有这种紧密联系；读者可以产生情感投入，但不会像第一人称那样强烈。\n与此相反，当你使用第一人称时，你不能有多个视角的主要角色，没有例…
    |'
- en: '| 3 | I finished my novel, and everyone I’ve talked to says I need an agent.
    How do I find one?\n | Try to find a list of agents who write in your genre, check
    out their websites!\nFind out if they are accepting new clients. If they aren’t,
    then check out another agent. But if they are, try sending them a few chapters
    from your story, a brief, and a short cover letter asking them to represent you.\nIn
    the cover letter mention your previous publication credits. If sent via post,
    then I suggest you give them a means of reply, whether it be an email or a stamped,
    addressed envelope.\nAgents… |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 我完成了我的小说，我和所有我谈过的人都说我需要一个代理。我该如何找到一个代理？\n | 尝试找到一份代理人名单，他们在你的体裁中写作，查看他们的网站！\n了解他们是否接受新客户。如果不接受，那么就找另一位代理。但如果接受，尝试向他们发送你小说的几章，简要说明，以及一封简短的求职信，请求他们代表你。\n在求职信中提及你的先前出版成就。如果通过邮件发送，请提供给他们回复的方式，无论是电子邮件还是贴上邮票的地址信封。\n代理…
    |'
- en: As one last sanity check, let’s look at how many questions received no answer,
    how many received at least one, and how many had an answer that was accepted.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一次健全性检查，让我们看看有多少问题没有答案，有多少问题至少有一个答案，以及有多少问题有被接受的答案。
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We have a relatively even split between answered and partially answered and
    unanswered questions. This seems reasonable, so we can feel confident enough to
    carry on with our exploration.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在已答复问题和部分答复问题以及未答复问题之间有一个相对均匀的分布。这看起来是合理的，因此我们可以足够自信地继续我们的探索。
- en: We understand the format of our data and have enough of it to get started. If
    you are working on a project and your current dataset is either too small or contains
    a majority of features that are too hard to interpret, you should gather some
    more data or try a different dataset entirely.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解我们数据的格式，并且有足够的数据可以开始。如果您正在进行一个项目，您当前的数据集要么太小，要么包含大多数难以解释的特征，您应该收集更多数据或尝试完全不同的数据集。
- en: Our dataset is of sufficient quality to proceed. It is now time to explore it
    more in depth, with the goal of informing our modeling strategy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集质量足以继续。现在是时候深入探讨它，以便为我们的建模策略提供信息。
- en: Label to Find Data Trends
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签以查找数据趋势
- en: Identifying trends in our dataset is about more than just quality. This part
    of the work is about putting ourselves in the shoes of our model and trying to
    predict what kind of structure it will pick up on. We will do this by separating
    data into different clusters (I will explain clustering in [“Clustering”](#clustering))
    and trying to extract commonalities in each cluster.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中识别趋势不仅仅是关于质量的问题。这部分工作是要让我们模型的思路和结构预测的能力更强。我们将通过将数据分成不同的聚类（我将在 [“聚类”](#clustering)
    中解释）并尝试提取每个聚类中的共性来实现这一点。
- en: The following is a step-by-step list to do this in practice. We’ll start with
    generating summary statistics of our dataset and then see how to rapidly explore
    it by leveraging vectorization techniques. With the help of vectorization and
    clustering, we’ll explore our dataset efficiently.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是实际操作的逐步列表。我们将从生成数据集的摘要统计开始，然后看看如何通过利用向量化技术快速探索它。借助向量化和聚类的帮助，我们将高效地探索我们的数据集。
- en: Summary Statistics
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要统计
- en: When you start looking at a dataset, it is generally a good idea to look at
    some summary statistics for each of the features you have. This helps you both
    get a general sense for the features in your dataset and identify any easy way
    to separate your classes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开始查看数据集时，通常建议查看每个特征的一些摘要统计数据。这不仅帮助您对数据集中的特征有一个总体感觉，还有助于识别任何简单分离类别的方法。
- en: Identifying differences in distributions between classes of data early is helpful
    in ML, because it will either make our modeling task easier or prevent us from
    overestimating the performance of a model that may just be leveraging one particularly
    informative feature.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 早期识别数据类别之间分布的差异对于机器学习非常有帮助，因为它要么使我们的建模任务更加容易，要么防止我们高估仅仅依赖一个特别信息丰富特征的模型的性能。
- en: For example, if you are trying to predict whether tweets are expressing a positive
    or negative opinion, you could start by counting the average number of words in
    each tweet. You could then plot a histogram of this feature to learn about its
    distribution.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您试图预测推文是否表达了积极或消极的观点，您可以从每个推文的平均字数开始计数。然后，您可以绘制该特征的直方图以了解其分布情况。
- en: A histogram would allow you to notice if all positive tweets were shorter than
    negative ones. This could lead you to add word length as a predictor to make your
    task easier or on the contrary gather additional data to make sure that your model
    can learn about the content of the tweets and not just their length.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图将允许您注意到所有积极推文是否比负面推文更短。这可能导致您添加字长作为预测器以使任务更容易，或者相反，收集额外数据以确保您的模型可以学习推文的内容而不仅仅是它们的长度。
- en: Let’s plot a few summary statistics for our ML editor to illustrate this point.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的 ML 编辑绘制一些摘要统计数据，以说明这一点。
- en: Summary statistics for ML editor
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ML 编辑的摘要统计
- en: 'For our example, we can plot a histogram of the length of questions in our
    dataset, highlighting the different trends between high- and low-score questions.
    Here is how we do this using pandas:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们可以绘制我们数据集中问题长度的直方图，突出显示高分和低分问题之间的不同趋势。以下是使用 pandas 如何实现这一点的方法：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can see in [Figure 4-2](#summary_statistics) that the distributions are mostly
    similar, with high-score questions tending to be slightly longer (this trend is
    especially noticeable around the 800-character mark). This is an indication that
    question length may be a useful feature for a model to predict a question’s score.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 [图 4-2](#summary_statistics) 中看到，分布大部分相似，高分问题倾向于稍长（这种趋势在约 800 字符处尤为明显）。这表明问题长度可能是一个模型预测问题分数的有用特征。
- en: We can plot other variables in a similar fashion to identify more potential
    features. Once we’ve identified a few features, let’s look at our dataset a little
    more closely so that we can identify more granular trends.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以类似的方式绘制其他变量，以识别更多潜在特征。一旦我们确定了一些特征，让我们仔细看一下我们的数据集，以便能够识别更细粒度的趋势。
- en: '![Text length for questions with high or low scores](assets/bmla_0402.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![高分或低分问题的文本长度](assets/bmla_0402.png)'
- en: Figure 4-2\. Histogram of the length of text for high- and low-score questions
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 高分和低分问题文本长度的直方图
- en: Explore and Label Efficiently
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高效探索和标记
- en: You can only get so far looking at descriptive statistics such as averages and
    plots such as histograms. To develop an intuition for your data, you should spend
    some time looking at individual data points. However, going through points in
    a dataset at random is quite inefficient. In this section, I’ll cover how to maximize
    your efficiency when visualizing individual data points.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只看描述性统计数据，如平均值和直方图，您只能走得那么远。要对数据有直觉，您应花一些时间查看单个数据点。但是，随机查看数据集中的点效率低下。在本节中，我将介绍如何在可视化单个数据点时提高效率。
- en: Clustering is a useful method to use here. [Clustering](https://oreil.ly/16f7Z)
    is the task of grouping a set of objects in such a way that objects in the same
    group (called a *cluster*) are more similar (in some sense) to each other than
    to those in other groups (clusters). We will use clustering both for exploring
    our data and for our model predictions later (see [“Dimensionality reduction”](#dim_red)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里使用聚类是一种有用的方法。[聚类](https://oreil.ly/16f7Z)是将一组对象分组的任务，以便同一组（称为*聚类*）中的对象比其他组（聚类）中的对象更相似（某种意义上）。我们将使用聚类来探索我们的数据，以及稍后的模型预测（参见[“降维”](#dim_red)）。
- en: Many clustering algorithms group data points by measuring the distance between
    points and assigning ones that are close to each other to the same cluster. [Figure 4-3](#clustering_image)
    shows an example of a clustering algorithm separating a dataset into three different
    clusters. Clustering is an unsupervised method, and there is often no single correct
    way to cluster a dataset. In this book, we will use clustering as a way to generate
    some structure to guide our exploration.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 许多聚类算法通过测量数据点之间的距离并将距离接近的点分配到同一聚类中来分组数据点。[图4-3](#clustering_image)展示了聚类算法将数据集分成三个不同聚类的示例。聚类是一种无监督方法，通常没有一种单一正确的聚类方法。在本书中，我们将使用聚类作为指导探索的一种方法。
- en: Because clustering relies on calculating the distance between data points, the
    way we choose to represent our data points numerically has a large impact on which
    clusters are generated. We will dive into this in the next section, [“Vectorizing”](#vectorizing).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因为聚类依赖于计算数据点之间的距离，我们选择如何数值化表示数据点对生成的聚类有很大影响。我们将在下一节“[矢量化](#vectorizing)”深入探讨这一点。
- en: '![Generating three clusters from a dataset](assets/bmla_0403.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![从数据集生成三个聚类](assets/bmla_0403.png)'
- en: Figure 4-3\. Generating three clusters from a dataset
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3\. 从数据集生成三个聚类
- en: The vast majority of datasets can be separated into clusters based on their
    features, labels, or a combination of both. Examining each cluster individually
    and the similarities and differences between clusters is a great way to identify
    structure in a dataset.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 绝大多数数据集可以根据它们的特征、标签或二者的组合分为不同的聚类。逐个检查每个聚类以及聚类之间的相似性和差异是识别数据集结构的好方法。
- en: 'There are multiple things to look out for here:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有多个需要注意的事项：
- en: How many clusters do you identify in your dataset?
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您在数据集中识别出多少个聚类？
- en: Do each of these clusters seem different to you? In which way?
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您认为这些聚类各不相同吗？在哪些方面不同？
- en: Are any clusters much more dense than others? If so, your model is likely to
    struggle to perform on the sparser areas. Adding features and data can help alleviate
    this problem.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些聚类比其他聚类密集得多吗？如果是这样，您的模型可能会在较稀疏的区域表现不佳。增加特征和数据可以帮助缓解这个问题。
- en: Do all clusters represent data that seems as “hard” to model? If some clusters
    seem to represent more complex data points, make note of them so you can revisit
    them when we evaluate our model’s performance.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有聚类是否表示看似“难以”建模的数据？如果某些聚类似乎表示更复杂的数据点，请注意它们，以便在评估我们的模型性能时重新审视。
- en: As we mentioned, clustering algorithms work on vectors, so we can’t simply pass
    a set of sentences to a clustering algorithm. To get our data ready to be clustered,
    we will first need to vectorize it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所提到的，聚类算法处理向量，因此我们不能简单地将一组句子传递给聚类算法。为了准备我们的数据进行聚类，我们首先需要将其矢量化。
- en: Vectorizing
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矢量化
- en: Vectorizing a dataset is the process of going from the raw data to a vector
    that represents it. [Figure 4-4](#raw_vs_vector) shows an example of vectorized
    representations for text and tabular data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 矢量化数据集是从原始数据到表示它的向量的过程。[图4-4](#raw_vs_vector)展示了文本和表格数据的矢量化表示示例。
- en: '![Examples of vectorized representations](assets/bmla_0404A.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![矢量化表示的示例](assets/bmla_0404A.png)'
- en: Figure 4-4\. Examples of vectorized representations
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4\. 矢量化表示的示例
- en: There are many ways to vectorize data, so we will focus on a few simple methods
    that work for some of the most common data types, such as tabular data, text,
    and images.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以对数据进行向量化，因此我们将专注于适用于一些最常见数据类型（如表格数据、文本和图像）的几种简单方法。
- en: Tabular data
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 表格数据
- en: For tabular data consisting of both categorical and continuous features, a possible
    vector representation is simply the concatenation of the vector representations
    of each feature.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于既包含分类特征又包含连续特征的表格数据，一种可能的向量表示方法就是简单地将每个特征的向量表示连接起来。
- en: Continuous features should be normalized to a common scale so that features
    with larger scale do not cause smaller features to be completely ignored by models.
    There are various way to normalize data, but starting by transforming each feature
    such that its mean is zero and variance one is often a good first step. This is
    often referred to as a [*standard score*](https://oreil.ly/QTEvI).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 连续特征应该被归一化到一个通用的尺度，以便具有更大尺度的特征不会导致较小的特征被模型完全忽略。有各种方法可以对数据进行归一化，但从使每个特征的均值为零且方差为一的转换开始通常是一个很好的第一步。这通常被称为[*标准化分数*](https://oreil.ly/QTEvI)。
- en: 'Categorical features such as colors can be converted to a one-hot encoding:
    a list as long as the number of distinct values of the feature consisting of only
    zeros and a single one, whose index represents the current value (for example,
    in a dataset containing four distinct colors, we could encode red as [1, 0, 0,
    0] and blue as [0, 0, 1, 0]). You may be curious as to why we wouldn’t simply
    assign each potential value a number, such as 1 for red and 3 for blue. It is
    because such an encoding scheme would imply an ordering between values (blue is
    larger than red), which is often incorrect for categorical variables.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，颜色等分类特征可以转换为独热编码：一个与特征的不同值数量一样长的列表，只包含零和一个单一的一，其索引表示当前值（例如，在包含四种不同颜色的数据集中，我们可以将红色编码为[1,
    0, 0, 0]，将蓝色编码为[0, 0, 1, 0]）。也许你会好奇，为什么我们不简单地为每个可能的值分配一个数字，比如将红色设为1，将蓝色设为3。这是因为这样的编码方案会暗示值之间的排序关系（蓝色大于红色），而这通常对于分类变量是错误的。
- en: A property of one-hot encoding is that the distance between any two given feature
    values is always one. This often provides a good representation for a model, but
    in some cases such as days of the week, some values may be more similar than the
    others (Saturday and Sunday are both in the weekend, so ideally their vectors
    would be closer together than Wednesday and Sunday, for example). Neural networks
    have started proving themselves useful at learning such representations (see the
    paper [“Entity Embeddings of Categorical Variables”](https://arxiv.org/abs/1604.06737),
    by C. Guo and F. Berkhahn). These representations have been shown to improve the
    performance of models using them instead of other encoding schemes.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码的一个特性是任意两个给定特征值之间的距离始终为一。这通常为模型提供了一个良好的表示，但在一些情况下，如星期几，某些值可能比其他值更相似（星期六和星期天都属于周末，因此理想情况下，它们的向量应该比星期三和星期天更接近，例如）。神经网络已经开始证明它们在学习这类表示方面是有用的（参见C.
    Guo和F. Berkhahn的论文["Entity Embeddings of Categorical Variables"](https://arxiv.org/abs/1604.06737)）。已经证明，使用这些表示方法的模型性能比其他编码方案要好。
- en: Finally, more complex features such as dates should be transformed in a few
    numerical features capturing their salient characteristics.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，诸如日期之类的更复杂特征应该转换为几个捕捉其显著特征的数值特征。
- en: Let’s go through a practical example of vectorization for tabular data. You
    can find the code for the example in the tabular data vectorization notebook in
    [this book’s GitHub repository](https://oreil.ly/ml-powered-applications).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个关于表格数据向量化的实际示例来说明。你可以在[这本书的 GitHub 仓库](https://oreil.ly/ml-powered-applications)中找到示例的代码。
- en: Let’s say that instead of looking at the content of questions, we want to predict
    the score a question will get from its tags, number of comments, and creation
    date. In [Table 4-3](#raw_tabular), you can see an example of what this dataset
    would look like for the *writers.stackexchange.com* dataset.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们不看问题的内容，而是想根据其标签、评论数和创建日期来预测问题的评分。在[表 4-3](#raw_tabular)中，你可以看到一个关于*writers.stackexchange.com*数据集的示例。
- en: Table 4-3\. Tabular inputs without any processing
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-3\. 未经任何处理的表格输入
- en: '| Id | Tags | CommentCount | CreationDate | Score |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Id | Tags | CommentCount | CreationDate | Score |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | <resources><first-time-author> | 7 | 2010-11-18T20:40:32.857 | 32 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 1 | <resources><first-time-author> | 7 | 2010-11-18T20:40:32.857 | 32 |'
- en: '| 2 | <fiction><grammatical-person><third-person> | 0 | 2010-11-18T20:42:31.513
    | 20 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 2 | <fiction><grammatical-person><third-person> | 0 | 2010-11-18T20:42:31.513
    | 20 |'
- en: '| 3 | <publishing><novel><agent> | 1 | 2010-11-18T20:43:28.903 | 34 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 3 | <publishing><novel><agent> | 1 | 2010-11-18T20:43:28.903 | 34 |'
- en: '| 5 | <plot><short-story><planning><brainstorming> | 0 | 2010-11-18T20:43:59.693
    | 28 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 5 | <plot><short-story><planning><brainstorming> | 0 | 2010-11-18T20:43:59.693
    | 28 |'
- en: '| 7 | <fiction><genre><categories> | 1 | 2010-11-18T20:45:44.067 | 21 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 7 | <fiction><genre><categories> | 1 | 2010-11-18T20:45:44.067 | 21 |'
- en: 'Each question has multiple tags, as well as a date and a number of comments.
    Let’s preprocess each of these. First, we normalize numerical fields:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 每个问题都有多个标签，还有日期和评论数量。让我们预处理每个问题。首先，我们对数值字段进行归一化：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then, we extract relevant information from the date. We could, for example,
    choose the year, month, day, and hour of posting. Each of these is a numerical
    value our model can use.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从日期中提取相关信息。例如，我们可以选择发布的年份、月份、日期和时间。这些都是我们的模型可以使用的数值。
- en: '[PRE7]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Our tags are categorical features, with each question potentially being given
    any number of tags. As we saw earlier, the easiest way to represent categorical
    inputs is to one-hot encode them, transforming each tag into its own column, with
    each question having a value of 1 for a given tag feature only if that tag is
    associated to this question.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的标签是分类特征，每个问题可能会有任意数量的标签。正如我们之前看到的，表示分类输入的最简单方法是进行独热编码，将每个标签转换为其自己的列，每个问题只有在该标签与此问题相关联时，该标签特征才为1。
- en: Because we have more than three hundred tags in our dataset, here we chose to
    only create a column for the five most popular ones that are used in more than
    five hundred questions. We could add every single tag, but because the majority
    of them appear only once, this would not be helpful to identify patterns.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的数据集中有超过三百个标签，所以我们选择只为使用频率超过五百次的五个最流行标签创建列。我们可以添加每一个标签，但因为大多数标签只出现一次，这并没有帮助我们识别模式。
- en: '[PRE8]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In [Table 4-4](#processed_tabular), you can see that our data is now fully vectorized,
    with each row consisting only of numeric values. We can feed this data to a clustering
    algorithm, or a supervised ML model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表格 4-4](#processed_tabular)中，您可以看到我们的数据现在完全向量化，每行都只包含数值。我们可以将这些数据输入到聚类算法或监督学习模型中。
- en: Table 4-4\. Vectorized tabular inputs
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4-4\. 向量化的表格输入
- en: '| Id | Year | Month | Day | Hour | Norm-Comment | Norm-Score | Creative writing
    | Fiction | Style | Char-acters | Tech-nique | Novel | Pub-lishing |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Id | Year | Month | Day | Hour | Norm-Comment | Norm-Score | Creative writing
    | Fiction | Style | Char-acters | Tech-nique | Novel | Pub-lishing |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
- en: '| 1 | 2010 | 11 | 18 | 20 | 0.165706 | 0.140501 | 0 | 0 | 0 | 0 | 0 | 0 | 0
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2010 | 11 | 18 | 20 | 0.165706 | 0.140501 | 0 | 0 | 0 | 0 | 0 | 0 | 0
    |'
- en: '| 2 | 2010 | 11 | 18 | 20 | -0.103524 | 0.077674 | 0 | 1 | 0 | 0 | 0 | 0 |
    0 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2010 | 11 | 18 | 20 | -0.103524 | 0.077674 | 0 | 1 | 0 | 0 | 0 | 0 |
    0 |'
- en: '| 3 | 2010 | 11 | 18 | 20 | -0.065063 | 0.150972 | 0 | 0 | 0 | 0 | 0 | 1 |
    1 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2010 | 11 | 18 | 20 | -0.065063 | 0.150972 | 0 | 0 | 0 | 0 | 0 | 1 |
    1 |'
- en: '| 5 | 2010 | 11 | 18 | 20 | -0.103524 | 0.119558 | 0 | 0 | 0 | 0 | 0 | 0 |
    0 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 2010 | 11 | 18 | 20 | -0.103524 | 0.119558 | 0 | 0 | 0 | 0 | 0 | 0 |
    0 |'
- en: '| 7 | 2010 | 11 | 18 | 20 | -0.065063 | 0.082909 | 0 | 1 | 0 | 0 | 0 | 0 |
    0 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 2010 | 11 | 18 | 20 | -0.065063 | 0.082909 | 0 | 1 | 0 | 0 | 0 | 0 |
    0 |'
- en: Different types of data call for different vectorization methods. In particular,
    text data often requires more creative approaches.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的数据需要不同的向量化方法。特别是文本数据通常需要更有创意的方法。
- en: Text data
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本数据
- en: The simplest way to vectorize text is to use a count vector, which is the word
    equivalent of one-hot encoding. Start by constructing a vocabulary consisting
    of the list of unique words in your dataset. Associate each word in our vocabulary
    to an index (from 0 to the size of our vocabulary). You can then represent each
    sentence or paragraph by a list as long as our vocabulary. For each sentence,
    the number at each index represents the count of occurrences of the associated
    word in the given sentence.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 文本向量化的最简单方法是使用计数向量，这是词汇版本的一种独热编码。首先构建一个词汇表，包含数据集中唯一词汇的列表。将词汇表中的每个单词与一个索引关联（从0到词汇表大小）。然后，可以用一个与词汇表长度相同的列表来表示每个句子或段落。每个索引处的数字表示给定句子中相关单词的出现次数。
- en: This method ignores the order of the words in a sentence and so is referred
    to as a *bag of words*. [Figure 4-5](#one_hot_bow) shows two sentences and their
    bag-of-words representations. Both sentences are transformed into vectors that
    contain information about the number of times a word occurs in a sentence, but
    not the order in which words are present in the sentence.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法忽略了句子中单词的顺序，因此被称为*词袋*。[图 4-5](#one_hot_bow)显示了两个句子及其词袋表示。这两个句子都被转换为包含关于单词在句子中出现次数信息的向量，但不包括单词出现在句子中的顺序。
- en: '![Getting bag of words vectors from sentences](assets/bmla_0405.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![从句子中获取词袋向量](assets/bmla_0405.png)'
- en: Figure 4-5\. Getting bag-of-words vectors from sentences
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 从句子中获取词袋向量
- en: 'Using a bag-of-words representation or its normalized version TF-IDF (short
    for Term Frequency–Inverse Document Frequency) is simple using scikit-learn, as
    you can see here:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词袋表示或其标准化版本TF-IDF（全称词频-逆文档频率）在scikit-learn中很简单，您可以在这里看到：
- en: '[PRE9]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Multiple novel text vectorization methods have been developed over the years,
    starting in 2013 with [Word2Vec (see the paper, “Efficient Estimation of Word
    Representations in Vector Space,” by Mikolov et al.)](https://oreil.ly/gs-AC)
    and more recent approaches such as [fastText (see the paper, “Bag of Tricks for
    Efficient Text Classification,” by Joulin et al.)](https://arxiv.org/abs/1607.01759).
    These vectorization techniques produce word vectors that attempt to learn a representation
    that captures similarities between concepts better than a TF-IDF encoding. They
    do this by learning which words tend to appear in similar contexts in large bodies
    of text such as Wikipedia. This approach is based on the distributional hypothesis,
    which claims that linguistic items with similar distributions have similar meanings.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 多种新型文本向量化方法已经开发出来，从2013年的[Word2Vec（参见论文，“Efficient Estimation of Word Representations
    in Vector Space” by Mikolov et al.）](https://oreil.ly/gs-AC)开始，到更近期的方法如[fastText（参见论文，“Bag
    of Tricks for Efficient Text Classification” by Joulin et al.）](https://arxiv.org/abs/1607.01759)。这些向量化技术产生的词向量试图学习一种能够更好地捕捉概念间相似性的表示，优于TF-IDF编码。它们通过学习哪些单词倾向于在大量文本（如维基百科）中出现在类似的上下文中来实现这一点。这种方法基于分布假设，声称具有相似分布的语言单元具有相似的含义。
- en: Concretely, this is done by learning a vector for each word and training a model
    to predict a missing word in a sentence using the word vectors of words around
    it. The number of neighboring words to take into account is called the *window
    size*. In [Figure 4-6](#cbow_w2v), you can see a depiction of this task for a
    window size of two. On the left, the word vectors for the two words before and
    after the target are fed to a simple model. This simple model and the values of
    the word vectors are then optimized so that the output matches the word vector
    of the missing word.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，这是通过为每个单词学习一个向量，并训练一个模型来预测句子中缺失的单词，使用周围单词的词向量。考虑的相邻单词数称为*窗口大小*。在[图 4-6](#cbow_w2v)，您可以看到窗口大小为二时此任务的描绘。左侧，目标词前后两个单词的词向量被输入到一个简单的模型中。然后优化这个简单模型和单词向量的值，使得输出匹配缺失单词的词向量。
- en: '![Learning word vectors, from Word2Vec](assets/bmla_0410.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![学习词向量，来自Word2Vec](assets/bmla_0410.png)'
- en: Figure 4-6\. Learning word vectors, from the [Word2Vec paper “Efficient Estimation
    of Word Representations in Vector Space” by Mikolov et al.](https://oreil.ly/gs-AC)
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 学习词向量，来自[Mikolov等人的Word2Vec论文“Efficient Estimation of Word Representations
    in Vector Space”](https://oreil.ly/gs-AC)
- en: Many open source pretrained word vectorizing models exist. Using vectors produced
    by a model that was pretrained on a large corpus (oftentimes Wikipedia or an archive
    of news stories) can help our models leverage the semantic meaning of common words
    better.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多开源预训练的词向量模型。使用在大语料库上预训练的模型生成的向量（通常是维基百科或新闻存档）可以更好地利用我们模型的语义含义。
- en: For example, the word vectors mentioned in the Joulin et al. [fastText](https://fasttext.cc/)
    paper are available online in a standalone tool. For a more customized approach,
    [spaCy](https://spacy.io) is an NLP toolkit that provides pretrained models for
    a variety of tasks, as well as easy ways to build your own.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Joulin等人的[fastText](https://fasttext.cc/)论文中提到的词向量可以在线获取独立工具。对于更定制化的方法，[spaCy](https://spacy.io)是一个自然语言处理工具包，提供预训练模型以及构建自己模型的简便方法。
- en: Here is an example of using spaCy to load pretrained word vectors and using
    them to get a semantically meaningful sentence vector. Under the hood, spaCy retrieves
    the pretrained value for each word in our dataset (or ignores it if it was not
    part of its pretraining task) and averages all vectors in a question to get a
    representation of the question.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用spaCy加载预训练词向量并使用它们获取语义有意义的句子向量的示例。在幕后，spaCy检索我们数据集中每个单词的预训练值（如果它不是其预训练任务的一部分，则忽略它），并平均所有问题中的向量以获取问题的表示。
- en: '[PRE10]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To see a comparison of a TF-IDF model with pretrained word embeddings for our
    dataset, please refer to the vectorizing text notebook in [the book’s GitHub repository](https://oreil.ly/ml-powered-applications).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看我们数据集中使用TF-IDF模型与预训练词嵌入的比较，请参阅[书的GitHub存储库](https://oreil.ly/ml-powered-applications)中的文本向量化笔记本。
- en: 'Since 2018, word vectorization using large language models on even larger datasets
    has started producing the most accurate results (see the papers [“Universal Language
    Model Fine-Tuning for Text Classification”](https://arxiv.org/abs/1801.06146),
    by J. Howard and S. Ruder, and [“BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding”](https://arxiv.org/abs/1810.04805), by J. Devlin et
    al.). These large models, however, do come with the drawback of being slower and
    more complex than simple word embeddings.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '自2018年以来，使用大型语言模型在更大的数据集上进行单词向量化已经开始产生最准确的结果（参见J. Howard和S. Ruder的论文[“Universal
    Language Model Fine-Tuning for Text Classification”](https://arxiv.org/abs/1801.06146)以及J.
    Devlin等人的论文[“BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding”](https://arxiv.org/abs/1810.04805)）。然而，这些大型模型的缺点是比简单的词嵌入更慢更复杂。'
- en: Finally, let’s examine vectorization for another commonly used type of data,
    images.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来看看另一种常用数据类型，即图像的向量化。
- en: Image data
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像数据
- en: Image data is already vectorized, as an image is nothing more but a multidimensional
    array of numbers, often referred to in the ML community as [tensors](https://oreil.ly/w7jQi).
    Most standard three-channel RGB images, for example, are simply stored as a list
    of numbers of length equal to the height of the image in pixels, multiplied by
    its width, multiplied by three (for the red, green, and blue channels). In [Figure 4-7](#image_as_a_vector),
    you can see how we can represent an image as a tensor of numbers, representing
    the intensity of each of the three primary colors.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据已经向量化，因为图像只是一个多维数组的集合，通常在ML社区中称为[tensors](https://oreil.ly/w7jQi)。例如，大多数标准的三通道RGB图像简单地存储为像素高度乘以宽度乘以三（用于红、绿和蓝通道）的数字列表。在[Figure
    4-7](#image_as_a_vector)中，您可以看到我们如何将图像表示为一组数字的张量，表示每个主色的强度。
- en: While we can use this representation as is, we would like our tensors to capture
    a little more about the semantic meaning of our images. To do this, we can use
    an approach similar to the one for text and leverage large pretrained neural networks.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以直接使用这种表示形式，但我们希望我们的张量能够更多地捕捉图像的语义含义。为了做到这一点，我们可以采用类似于文本的方法，并利用大型预训练神经网络。
- en: Models that have been trained on massive classification datasets such as [VGG
    (see the paper by A. Simonyan and A. Zimmerman, “Very Deep Convolutional Networks
    for Large-Scale Image Recognition”)](https://oreil.ly/TVHID) or [Inception (see
    the paper by C. Szegedy et al., “Going Deeper with Convolutions”)](https://oreil.ly/nbetp)
    on the [ImageNet dataset](http://www.image-net.org/) end up learning very expressive
    representations in order to classify well. These models mostly follow a similar
    high-level structure. The input is an image that passes through many successive
    layers of computation, each generating a different representation of said image.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过大规模分类数据集（如[VGG（参见A. Simonyan和A. Zimmerman的论文，“Very Deep Convolutional Networks
    for Large-Scale Image Recognition”](https://oreil.ly/TVHID)）或[Inception（参见C. Szegedy等人的论文，“Going
    Deeper with Convolutions”）](https://oreil.ly/nbetp)在[ImageNet数据集](http://www.image-net.org/)上学习后，为了进行良好的分类，这些模型最终学习到了非常表达丰富的表示形式。这些模型大多遵循相似的高级结构。输入是一张图像，通过许多连续的计算层，每一层生成图像的不同表示。
- en: Finally, the penultimate layer is passed to a function that generates classification
    probabilities for each class. This penultimate layer thus contains a representation
    of the image that is sufficient to classify which object it contains, which makes
    it a useful representation for other tasks.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，倒数第二层被传递给一个函数，为每个类生成分类概率。因此，这个倒数第二层包含了图像的表示，足以对包含的对象进行分类，这使得它成为其他任务的有用表示。
- en: '![An image is simply a tensor](assets/bmla_0411.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![一个图像只是一个张量](assets/bmla_0411.png)'
- en: Figure 4-7\. Representing a 3 as a matrix of values from 0 to 1 (only showing
    the red channel)
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 表示 3 作为从 0 到 1 的值的矩阵（仅显示红色通道）
- en: Extracting this representation layer proves to work extremely well at generating
    meaningful vectors for images. This requires no custom work other than loading
    the pretrained model. In [Figure 4-8](#using_pre_trained_models) each rectangle
    represents a different layer for one of those pretrained models. The most useful
    representation is highlighted. It is usually located just before the classification
    layer, since that is the representation that needs to summarize the image best
    for the classifier to perform well.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 提取此表示层在生成图像有意义的向量方面表现出色。除了加载预训练模型之外，无需任何自定义工作。在 [图 4-8](#using_pre_trained_models)
    中，每个矩形代表其中一个预训练模型的不同层。最有用的表示被突出显示。通常位于分类层之前，因为这是需要最好地总结图像以使分类器表现良好的表示。
- en: '![Using a pre-trained model to vectorize images](assets/bmla_0408.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![使用预训练模型来向量化图像](assets/bmla_0408.png)'
- en: Figure 4-8\. Using a pretrained model to vectorize images
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 使用预训练模型来向量化图像
- en: 'Using modern libraries such as Keras makes this task much easier. Here is a
    function that loads images from a folder and transforms them into semantically
    meaningful vectors for downstream analysis, using a pretrained network available
    in Keras:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像 Keras 这样的现代库可以使这项任务变得更加容易。以下是从文件夹加载图像并使用 Keras 中可用的预训练网络将其转换为语义上有意义的向量的函数：
- en: '[PRE11]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once you have a vectorized representation, you can cluster it or pass your data
    to a model, but you can also use it to more efficiently inspect your dataset.
    By grouping data points with similar representations together, you can more quickly
    look at trends in your dataset. We’ll see how to do this next.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您拥有向量化表示，您可以对其进行聚类或将数据传递给模型，但您也可以使用它更高效地检查数据集。通过将具有相似表示的数据点分组在一起，您可以更快地查看数据集中的趋势。接下来我们将看到如何做到这一点。
- en: Dimensionality reduction
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降维
- en: Having vector representations is necessary for algorithms, but we can also leverage
    those representations to visualize data directly! This may seem challenging, because
    the vectors we described are often in more than two dimensions, which makes them
    challenging to display on a chart. How could we display a 14-dimensional vector?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有向量表示对算法至关重要，但我们还可以利用这些表示直接可视化数据！这可能看起来很具挑战性，因为我们描述的向量通常在两个以上的维度中，这使得它们在图表上显示起来具有挑战性。我们如何显示一个
    14 维向量呢？
- en: 'Geoffrey Hinton, who won a Turing Award for his work in deep learning, acknowledges
    this problem in his lecture with the following tip: “To deal with hyper-planes
    in a 14-dimensional space, visualize a 3D space and say *fourteen* to yourself
    very loudly. Everyone does it.” (See slide 16 from G. Hinton et al.’s lecture,
    “An Overview of the Main Types of Neural Network Architecture” [here](https://oreil.ly/wORb-).)
    If this seems hard to you, you’ll be excited to hear about dimensionality reduction,
    which is the technique of representing vectors in fewer dimensions while preserving
    as much about their structure as possible.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域的图灵奖得主 Geoffrey Hinton 在他的演讲中承认了这个问题，并给出了以下建议：“要处理 14 维空间中的超平面，请在 3D 空间中进行可视化并大声说
    *fourteen*。每个人都这样做。”（参见 G. Hinton 等人的演讲第 16 页，“An Overview of the Main Types of
    Neural Network Architecture” [这里](https://oreil.ly/wORb-)）。如果这对您来说似乎很困难，您将对降维技术感到兴奋，这是在尽可能保持其结构的同时将向量表示为更少维度的技术。
- en: 'Dimensionality reduction techniques such as t-SNE (see the paper by L. van
    der Maaten and G. Hinton, [PCA](https://oreil.ly/kXwvH), [“Visualizing Data Using
    t-SNE”)](https://oreil.ly/x8S2b), and [UMAP (see the paper by L. McInnes et al,
    “UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction”)](https://oreil.ly/IYrHH)
    allow you to project high-dimensional data such as vectors representing sentences,
    images, or other features on a 2D plane.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '降维技术如 t-SNE（参见 L. van der Maaten 和 G. Hinton 的论文，[PCA](https://oreil.ly/kXwvH)，[“Visualizing
    Data Using t-SNE”](https://oreil.ly/x8S2b)）和 [UMAP（参见 L. McInnes 等人的论文，“UMAP:
    Uniform Manifold Approximation and Projection for Dimension Reduction”](https://oreil.ly/IYrHH)）允许您将高维数据（如表示句子、图像或其他特征的向量）投影到二维平面上。'
- en: These projections are useful to notice patterns in data that you can then investigate.
    They are approximate representations of the real data, however, so you should
    validate any hypothesis you make from looking at such a plot by using other methods.
    If you see clusters of points all belonging to one class that seem to have a feature
    in common, check that your model is actually leveraging that feature, for example.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这些投影对于注意数据中的模式非常有用，然后您可以进一步调查它们。然而，它们只是真实数据的近似表示，因此您应该通过使用其他方法来验证您从这样的图中得出的任何假设。例如，如果您看到一类点的集群似乎都有一个共同的特征，请检查您的模型是否实际上正在利用该特征。
- en: To get started, plot your data using a dimensionality reduction technique and
    color each point by an attribute you are looking to inspect. For classification
    tasks, start by coloring each point based on its label. For unsupervised tasks,
    you can color points based on the values of given features you are looking at,
    for example. This allows you to see whether any regions seem like they will be
    easy for your model to separate, or trickier.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，使用降维技术绘制你的数据，并根据你要检查的属性为每个点着色。对于分类任务，首先根据标签为每个点着色。对于无监督任务，你可以根据你正在查看的特征的值来着色点，例如。这可以让你看到是否有任何区域看起来会让你的模型容易分离，或者更棘手。
- en: 'Here is how to do this easily using UMAP, passing it embeddings we generated
    in [“Vectorizing”](#vectorizing):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何在我们在[“向量化”](#vectorizing)中生成的嵌入中轻松使用UMAP来做到这一点：
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As a reminder, we decided to start with using only data from the writers’ community
    of Stack Exchange. The result for this dataset is displayed on [Figure 4-9](#example_umap_plot).
    At first glance, we can see a few regions we should explore, such as the dense
    region of unanswered questions on the top left. If we can identify which features
    they have in common, we may discover a useful classification feature.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，我们决定首先使用来自Stack Exchange作者社区的数据。此数据集的结果显示在[图 4-9](#example_umap_plot)上。乍一看，我们可以看到一些应该探索的区域，比如左上角的未解答问题的密集区域。如果我们能确定它们共有哪些特征，我们可能会发现一个有用的分类特征。
- en: After data is vectorized and plotted, it is generally a good idea to start systematically
    identifying groups of similar data points and explore them. We could do this simply
    by looking at UMAP plots, but we can also leverage clustering.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 数据向量化并绘制后，系统地开始识别相似数据点的群组通常是个好主意。我们可以简单地通过查看UMAP图来做到这一点，但我们也可以利用聚类。
- en: '![UMAP plot colored by whether a given question was successfully answered or
    not](assets/bmla_0413.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![UMAP plot colored by whether a given question was successfully answered or
    not](assets/bmla_0413.png)'
- en: Figure 4-9\. UMAP plot colored by whether a given question was successfully
    answered
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-9\. UMAP plot colored by whether a given question was successfully answered
- en: Clustering
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类
- en: We mentioned clustering earlier as a method to extract structure from data.
    Whether you are clustering data to inspect a dataset or using it to analyze a
    model’s performance as we will do in [Chapter 5](ch05.html#first_model), clustering
    is a core tool to have in your arsenal. I use clustering in a similar fashion
    as dimensionality reduction, as an additional way to surface issues and interesting
    data points.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们前面提到过聚类作为从数据中提取结构的方法。无论您是用于检查数据集还是分析模型性能，正如我们将在[第五章](ch05.html#first_model)中所做的那样，聚类都是您工具库中的核心工具。我使用聚类类似于降维，作为发现问题和有趣数据点的另一种方式。
- en: A simple method to cluster data in practice is to start by trying a few simple
    algorithms such as [k-means](https://oreil.ly/LKdYP) and tweak their hyperparameters
    such as the number of clusters until you reach a satisfactory performance.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，一种简单的聚类数据的方法是从尝试几种简单的算法开始，例如[k-means](https://oreil.ly/LKdYP)，并调整它们的超参数，如聚类数，直到达到令人满意的性能。
- en: Clustering performance is hard to quantify. In practice, using a combination
    of data visualization and methods such as the [elbow method](https://oreil.ly/k98SV)
    or a [silhouette plot](https://oreil.ly/QGky6) is sufficient for our use case,
    which is not to perfectly separate our data but to identify regions where our
    model may have issues.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的表现很难量化。在实践中，使用数据可视化和诸如[肘部法](https://oreil.ly/k98SV)或[轮廓图](https://oreil.ly/QGky6)之类的方法对我们的用例已经足够了，这并不是为了完美地分离我们的数据，而是为了确定我们的模型可能存在问题的区域。
- en: The following is an example snippet of code for clustering our dataset, as well
    as visualizing our clusters using a dimensionality technique we described earlier,
    UMAP.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例代码片段，用于对我们的数据集进行聚类，并使用我们早期描述的降维技术UMAP来可视化我们的聚类。
- en: '[PRE13]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see in [Figure 4-10](#visualize_clusters), the way we would instinctively
    cluster the 2D representation does not always match with the clusters our algorithm
    finds on the vectorized data. This can be because of artifacts in our dimensionality
    reduction algorithm or a complex data topology. In fact, adding a point’s assigned
    cluster as a feature can sometimes improve a model’s performance by letting it
    leverage said topology.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在 [图 4-10](#visualize_clusters) 中所见，我们在2D表示中本能地聚类的方式并不总是与我们的算法在向量化数据上找到的聚类匹配。这可能是由于我们降维算法中的人为因素或复杂的数据拓扑。事实上，有时将点的分配簇作为特征添加到模型中可以提高模型的性能。
- en: Once you have clusters, examine each cluster and try to identify trends in your
    data on each of them. To do so, you should select a few points per cluster and
    act as if you were the model, thus labeling those points with what you think the
    correct answer should be. In the next section, I’ll describe how to do this labeling
    work.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了簇，就检查每个簇并尝试识别数据的每个趋势。为此，您应该选择每个簇的几个点，并像模型一样操作，因此用您认为正确答案的标签来标记这些点。在接下来的章节中，我将描述如何进行这项标注工作。
- en: '![Visualizing our questions, colored by cluster](assets/bmla_0414.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![可视化我们的问题，按簇着色](assets/bmla_0414.png)'
- en: Figure 4-10\. Visualizing our questions, colored by cluster
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. 可视化我们的问题，按簇着色
- en: Be the Algorithm
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成为算法
- en: 'Once you’ve looked at aggregate metrics and cluster information, I’d encourage
    you to follow the advice in [“Monica Rogati: How to Choose and Prioritize ML Projects”](ch01.html#monica_rogati)
    and try to do your model’s job by labeling a few data points in each cluster with
    the results you would like a model to produce.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦您查看了聚合指标和簇信息，我鼓励您遵循[“Monica Rogati: 如何选择和优先处理ML项目”](ch01.html#monica_rogati)中的建议，并尝试通过标记每个簇中的几个数据点来执行您的模型工作，以期望模型产生的结果。'
- en: If you have never tried doing your algorithm’s job, it will be hard to judge
    the quality of its results. On the other side, if you spend some time labeling
    data yourself, you will often notice trends that will make your modeling task
    much easier.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您从未尝试过执行您的算法的工作，那么很难判断其结果的质量。另一方面，如果您花一些时间自己标记数据，您通常会注意到一些趋势，这将使您的建模任务变得更加容易。
- en: You might recognize this advice from our previous section about heuristics,
    and it should not surprise you. Choosing a modeling approach involves making almost
    as many assumptions about our data as building heuristics, so it makes sense for
    these assumptions to be data driven.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会在我们关于启发式的前一节中认出这个建议，这应该不会让您感到意外。选择建模方法涉及对我们的数据几乎与构建启发式方法一样多的假设，因此对这些假设进行数据驱动的选择是有意义的。
- en: You should label data even if your dataset contains labels. This allows you
    to validate that your labels do capture the correct information and that they
    are correct. In our case study, we use a question’s score as a measure of its
    quality, which is a weak label. Labeling a few examples ourselves will allow us
    to validate the assumption that this label is appropriate.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您的数据集包含标签，也应进行标注。这样可以验证您的标签是否捕捉到了正确的信息，并且它们是正确的。在我们的案例研究中，我们使用问题的得分作为其质量的衡量标准，这是一个较弱的标签。自己标注一些示例将帮助我们验证这个标签是否合适。
- en: Once you label a few examples, feel free to update your vectorization strategy
    by adding any features you discover to help make your data representation as informative
    as possible, and go back to labeling. This is an iterative process, as illustrated
    in [Figure 4-11](#labeling).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您标记了一些示例，请随时更新您的向量化策略，通过添加您发现的任何特征来使数据表示尽可能信息丰富，并继续标记。这是一个迭代的过程，正如 [图 4-11](#labeling)
    所示。
- en: '![The process of labeling data.](assets/bmla_0415.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![数据标注过程。](assets/bmla_0415.png)'
- en: Figure 4-11\. The process of labeling data
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-11\. 数据标注过程
- en: To speed up your labeling, make sure to leverage your prior analysis by labeling
    a few data points in each cluster you have identified and for each common value
    in your feature distribution.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快标注速度，请确保利用先前分析的结果，在您已识别的每个簇中标记几个数据点，并针对特征分布中的每个常见值进行标记。
- en: One way to do this is to leverage visualization libraries to interactively explore
    your data. [Bokeh](https://oreil.ly/6eORd) offers the ability to make interactive
    plots. One quick way to label data is to go through a plot of our vectorized examples,
    labeling a few examples for each cluster.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是利用可视化库对数据进行交互式探索。[Bokeh](https://oreil.ly/6eORd) 提供制作交互式图表的功能。快速标记数据的一种方法是通过浏览我们的向量化示例的图表，为每个簇标记几个示例。
- en: '[Figure 4-12](#bokeh_labeling) shows a representative individual example from
    a cluster of mostly unanswered questions. Questions in this cluster tended to
    be quite vague and hard to answer objectively and did not receive answers. These
    are accurately labeled as poor questions. To see the source code for this plot
    and an example of its use for the ML Editor, navigate to the exploring data to
    generate features notebook in [this book’s GitHub repository](https://oreil.ly/ml-powered-applications).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4-12](#bokeh_labeling)显示了一个代表性的个体示例，来自大部分未答复问题的一个聚类。此聚类中的问题往往相当模糊且难以客观回答，因此没有收到答案。这些问题被准确标记为差劲的问题。要查看此图的源代码以及其在ML编辑器中的使用示例，请转到本书的GitHub存储库中的“探索数据生成特征”笔记本。'
- en: '![Using Bokeh to inspect and label data](assets/bmla_0416.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![使用Bokeh检查和标记数据](assets/bmla_0416.png)'
- en: Figure 4-12\. Using Bokeh to inspect and label data
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12\. 使用Bokeh检查和标记数据
- en: When labeling data, you can choose to store labels with the data itself (as
    an additional column in a DataFrame, for example) or separately using a mapping
    from file or identifier to label. This is purely a matter of preference.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在标注数据时，您可以选择将标签与数据本身一起存储（例如，作为DataFrame中的额外列），或者使用文件或标识符到标签的映射来单独存储。这纯粹是个人偏好的问题。
- en: As you label examples, try to notice which process you are using to make your
    decisions. This will help with identifying trends and generating features that
    will help your models.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当您标记示例时，请注意您使用哪种过程来做出决策。这将有助于识别趋势并生成有助于模型的特征。
- en: Data Trends
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据趋势
- en: After having labeled data for a while, you will usually identify trends. Some
    may be informative (short tweets tend to be simpler to classify as positive or
    negative) and guide you to generate useful features for your models. Others may
    be irrelevant correlations because of the way data was gathered.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记数据一段时间后，通常会发现一些趋势。有些可能很有信息量（简短的推文更容易分类为积极或消极），并指导您为模型生成有用的特征。其他可能是无关的相关性，是因为数据收集的方式不同造成的。
- en: Maybe all of the tweets we collected that are in French happen to be negative,
    which would likely lead a model to automatically classify French tweets as negative.
    I’ll let you decide how inaccurate that might be on a broader, more representative
    sample.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 可能我们收集的所有法语推文恰好都是负面的，这可能会导致模型自动将法语推文分类为负面。我让您决定在更广泛、更具代表性的样本上这可能有多不准确。
- en: If you notice anything of the sort, do not despair! These kinds of trends are
    crucial to identify *before* you start building models, as they would artificially
    inflate accuracy on training data and could lead you to put a model in production
    that does not perform well.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您注意到任何此类情况，请不要绝望！在开始构建模型之前，识别这些趋势至关重要，因为它们会人为地提高训练数据的准确性，并可能导致您的模型在生产中表现不佳。
- en: The best way to deal with such biased examples is to gather additional data
    to make your training set more representative. You could also try to eliminate
    these features from your training data to avoid biasing your model, but this may
    not be effective in practice, as models frequently pick up on bias by leveraging
    correlations with other features (see [Chapter 8](ch08.html#final_validation)).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这种有偏见的示例的最佳方法是收集额外的数据，使您的训练集更具代表性。您也可以尝试从训练数据中消除这些特征以避免模型的偏见，但在实践中可能不够有效，因为模型经常通过与其他特征的相关性来利用偏见（参见[第8章](ch08.html#final_validation)）。
- en: Once you’ve identified some trends, it is time to use them. Most often, you
    can do this in one of two ways, by creating a feature that characterizes that
    trend or by using a model that will easily leverage it.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您确定了一些趋势，就是利用它们的时候了。最常见的方法是通过创建表征该趋势的特征或者使用能够轻松利用它的模型之一。
- en: Let Data Inform Features and Models
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让数据指导特征和模型
- en: We would like to use the trends we discover in the data to inform our data processing,
    feature generation, and modeling strategy. To start, let’s look at how we could
    generate features that would help us capture these trends.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望利用我们在数据中发现的趋势来指导我们的数据处理、特征生成和建模策略。首先，让我们看看如何生成能帮助我们捕捉这些趋势的特征。
- en: Build Features Out of Patterns
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用模式构建特征
- en: ML is about using statistical learning algorithms to leverage patterns in the
    data, but some patterns are easier to capture for models than others. Imagine
    the trivial example of predicting a numerical value using the value itself divided
    by 2 as a feature. The model would simply have to learn to multiply by 2 to predict
    the target perfectly. On the other hand, predicting the stock market from historical
    data is a problem that requires leveraging much more complex patterns.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是利用统计学习算法来利用数据中的模式，但有些模式对于模型而言比其他模式更容易捕捉。想象一个简单的例子，用数值本身除以2作为特征来预测数值的情况。模型只需学会乘以2就能完美预测目标值。另一方面，从历史数据预测股市则是一个需要利用更复杂模式的问题。
- en: This is why a lot of the practical gains of ML come from generating additional
    *features* that will help our models identify useful patterns. The ease with which
    a model identifies patterns depends on the way we represent data and how much
    of it we have. The more data you have and the less noisy your data is, the less
    feature engineering work you usually have to do.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么机器学习的许多实际收益来自于生成额外的*特征*，这些特征将帮助我们的模型识别有用的模式。模型识别模式的容易程度取决于我们表示数据的方式以及我们拥有的数据量。数据越多且数据越少噪声，通常需要进行的特征工程工作就越少。
- en: It is often valuable to start by generating features, however; first because
    we will usually be starting with a small dataset and second because it helps encode
    our beliefs about the data and debug our models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，从生成特征开始是有价值的；首先是因为我们通常从一个小数据集开始，其次是因为它有助于编码我们对数据的信念并调试我们的模型。
- en: Seasonality is a common trend that benefits from specific feature generation.
    Let’s say that an online retailer noticed that most of their sales happens on
    the last two weekends of the month. When building a model to predict future sales,
    they want to make sure that it has the potential to capture this pattern.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 季节性是一个常见的趋势，可以通过特定的特征生成来受益。假设在线零售商注意到他们的大多数销售都发生在月底的最后两个周末。在构建预测未来销售的模型时，他们希望确保它能够捕捉到这种模式。
- en: As you’ll see, depending on how they represent dates, the task could prove quite
    difficult for their models. Most models are only able to take numerical inputs
    (see [“Vectorizing”](#vectorizing) for methods to transform text and images into
    numerical inputs), so let’s examine a few ways to represent dates.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，根据日期的表示方式，该任务可能对他们的模型来说非常困难。大多数模型只能接受数值输入（参见[“向量化”](#vectorizing)以了解将文本和图像转换为数值输入的方法），因此让我们来探讨几种表示日期的方法。
- en: Raw datetime
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原始日期时间
- en: The simplest way to represent time is in [Unix time](https://oreil.ly/hMlX3),
    which represents “the number of seconds that have elapsed since 00:00:00 Thursday,
    1 January 1970.”
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 表示时间最简单的方式是使用[Unix时间](https://oreil.ly/hMlX3)，它表示“自1970年1月1日星期四00:00:00以来经过的秒数”。
- en: While this representation is simple, our model would need to learn some pretty
    complex patterns to identify the last two weekends of the month. The last weekend
    of 2018, for example (from 00:00:00 on the 29th to 23:59:59 on the 30th of December),
    is represented in Unix time as the range from 1546041600 to 1546214399 (you can
    verify that if you take the difference between both numbers, which represents
    an interval of 23 hours, 59 minutes, and 59 seconds measured in seconds).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种表示方式很简单，但我们的模型需要学习一些相当复杂的模式来识别月底的最后两个周末。例如，2018年的最后一个周末（从12月29日的00:00:00到12月30日的23:59:59）在Unix时间中表示为从1546041600到1546214399的范围（您可以验证这一点，如果您计算这两个数字之间的差异，该间隔表示为以秒计算的23小时59分钟59秒）。
- en: Nothing about this range makes it particularly easy to relate to other weekends
    in other months, so it will be quite hard for a model to separate relevant weekends
    from others when using Unix time as an input. We can make the task easier for
    a model by generating features.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这个时间范围并没有什么特别容易与其他月份的周末联系起来的地方，所以当使用Unix时间作为输入时，模型将很难将相关的周末与其他周末分开。通过生成特征，我们可以使模型的任务变得更加简单。
- en: Extracting day of week and day of month
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取星期几和月份中的日期
- en: One way to make our representation of dates clearer would be to extract the
    day of the week and day of the month into two separate attributes.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使我们对日期的表示更清晰的一种方法是将星期几和月份中的日期分别提取为两个单独的属性。
- en: The way we would represent 23:59:59 on the 30th of December, 2018, for example,
    would be with the same number as earlier, and two additional values representing
    the day of the week (0 for Sunday, for example) and day of the month (30).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们将表示2018年12月30日23:59:59的方式与前面相同，再加上代表星期几（例如星期日为0）和日期（30）的两个额外值。
- en: This representation will make it easier for our model to learn that the values
    related to weekends (0 and 6 for Sunday and Saturday) and to later dates in the
    month correspond to higher activity.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示方法将使我们的模型更容易学习与周末相关的数值（星期日和星期六的0和6）以及月末较晚日期对应的更高活动度。
- en: It is also important to note that representations will often introduce bias
    to our model. For example, by encoding the day of the week as a number, the encoding
    for Friday (equal to five) will be five times greater than the one for Monday
    (equal to one). This numerical scale is an artifact of our representation and
    does not represent something we wish our model to learn.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，表示通常会向我们的模型引入偏见。例如，通过将星期几编码为数字，星期五的编码（等于五）将比星期一的编码（等于一）大五倍。这种数值尺度是我们表示的产物，不代表我们希望模型学习的内容。
- en: Feature crosses
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征交叉
- en: 'While the previous representation makes the task easier for our models, they
    would still have to learn a complex relationship between the day of the week and
    the day of the month: high traffic does not happen on weekends early in the month
    or on weekdays late in the month.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的表示方法让我们的模型任务更容易，但它们仍然需要学习星期几和日期之间复杂的关系：高流量不会发生在月初的周末或月底的工作日。
- en: Some models such as deep neural networks leverage nonlinear combinations of
    features and can thus pick up on these relationships, but they often need a significant
    amount of data. A common way to address this problem is by making the task even
    easier and introducing *feature crosses*.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型，如深度神经网络，利用特征的非线性组合，因此可以掌握这些关系，但它们通常需要大量数据。解决这个问题的常见方法是通过让任务更容易，并引入 *特征交叉*。
- en: A feature cross is a feature generated simply by multiplying (crossing) two
    or more features with each other. This introduction of a nonlinear combination
    of features allows our model to discriminate more easily based on a combination
    of values from multiple features.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 特征交叉是通过简单地将两个或更多特征相乘（交叉）而生成的特征。这种引入特征的非线性组合使得我们的模型可以更容易地基于多个特征值的组合进行区分。
- en: In [Table 4-5](#different_data_representations), you can see how each of the
    representations we described would look for a few example data points.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [表 4-5](#different_data_representations) 中，您可以看到我们描述的每种表示方法对几个示例数据点的效果。
- en: Table 4-5\. Representing your data in a clearer way will make it much easier
    for your algorithms to perform well
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-5\. 以更清晰的方式表示您的数据将使您的算法更容易执行良好
- en: '| Human representation | Raw data (Unix datetime) | Day of week (DoW) | Day
    of month (DoM) | Cross (DoW / DoM) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 人类代表 | 原始数据（Unix日期时间） | 周几 | 日期 | 交叉（周几 / 日期） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Saturday, December 29, 2018, 00:00:00 | 1,546,041,600 | 7 | 29 | 174 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 2018年12月29日星期六，00:00:00 | 1,546,041,600 | 7 | 29 | 174 |'
- en: '| Saturday, December 29, 2018, 01:00:00 | 1,546,045,200 | 7 | 29 | 174 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 2018年12月29日星期六，01:00:00 | 1,546,045,200 | 7 | 29 | 174 |'
- en: '| … | … | … | … | … |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … | … | … |'
- en: '| Sunday, December 30, 2018, 23:59:59 | 1,546,214,399 | 1 | 30 | 210 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 2018年12月30日星期日，23:59:59 | 1,546,214,399 | 1 | 30 | 210 |'
- en: In [Figure 4-13](#regions_of_interest_repr), you can see how these feature values
    change with time and which ones make it simpler for a model to separate specific
    data points from others.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 4-13](#regions_of_interest_repr) 中，您可以看到这些特征值随时间变化的方式，以及哪些特征使得模型更容易将特定数据点与其他数据点分离开来。
- en: '![Last weekends of the month are easiest to separate using feature crosses
    and extracted features](assets/bmla_0418.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![使用特征交叉和提取特征最容易区分月末的最后周末](assets/bmla_0418.png)'
- en: Figure 4-13\. The last weekends of the month are easier to separate using feature
    crosses and extracted features
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-13\. 使用特征交叉和提取特征最容易区分月末的最后周末
- en: There is one last way to represent our data that will make it even easier for
    our model to learn the predictive value of the last two weekends of the month.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种表示我们的数据的方法，可以让我们的模型更轻松地学习月末的最后两个周末的预测价值。
- en: Giving your model the answer
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为您的模型提供答案
- en: It may seem like cheating, but if you know for a fact that a certain combination
    of feature values is particularly predictive, you can create a new binary feature
    that takes a nonzero value only when these features take the relevant combination
    of values. In our case, this would mean adding a feature called “is_last_two_weekends”,
    for example, that will be set to one only during the last two weekends of the
    month.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来像作弊，但如果您确实知道某种特征值组合特别具有预测性，您可以创建一个新的二进制特征，只有当这些特征采用相关组合值时才会取非零值。在我们的情况下，这意味着添加一个名为“is_last_two_weekends”的特征，例如，在月末的最后两个周末期间该特征将设置为1。
- en: If the last two weekends are as predictive as we had supposed they were, the
    model will simply learn to leverage this feature and will be much more accurate.
    When building ML products, never hesitate to make the task easier for your model.
    Better to have a model that works on a simpler task than one that struggles on
    a complex one.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果最近两个周末像我们预期的那样具有预测性，模型将简单地利用这一特征，并且将更加准确。在构建ML产品时，永远不要犹豫让任务对模型来说更容易。最好让模型在简单的任务上运行，而不是在复杂的任务上挣扎。
- en: Feature generation is a wide field, and methods exist for most types of data.
    Discussing every feature that is useful to generate for different types of data
    is outside the scope of this book. If you’d like to see more practical examples
    and methods, I recommend taking a look at [*Feature Engineering for Machine Learning*](http://shop.oreilly.com/product/0636920049081.do)
    (O’Reilly), by Alice Zheng and Amanda Casari.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 特征生成是一个广泛的领域，存在适用于大多数数据类型的方法。讨论为不同类型的数据生成有用特征的每个特征是本书范围之外的内容。如果您想看到更多实际例子和方法，我建议您查看[*机器学习特征工程*](http://shop.oreilly.com/product/0636920049081.do)（O’Reilly），作者是Alice
    Zheng和Amanda Casari。
- en: In general, the best way to generate useful features is by looking at your data
    using the methods we described and asking yourself what the easiest way is to
    represent it in a way that will make your model learn its patterns. In the following
    section, I’ll describe a few examples of features I generated using this process
    for the ML Editor.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，生成有用特征的最佳方法是使用我们描述的方法查看您的数据，并问自己以使模型学习其模式的方式表示它的最简单方法是什么。在接下来的部分中，我将描述使用这一过程为ML编辑器生成的几个特征示例。
- en: ML Editor Features
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML编辑器特性
- en: 'For our ML Editor, using the techniques described earlier to inspect our dataset
    (see details of the exploration in the exploring data to generate features notebook,
    in [this book’s GitHub repository](https://oreil.ly/ml-powered-applications)),
    we generated the following features:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的ML编辑器，使用之前描述的技术来检查我们的数据集（请参阅在[本书的GitHub存储库中的探索数据以生成特征笔记本中的详细信息](https://oreil.ly/ml-powered-applications)），我们生成了以下特征：
- en: Action verbs such as *can* and *should* are predictive of a question being answered,
    so we added a binary value that checks whether they are present in each question.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如*can*和*should*之类的动作动词预示着问题会得到回答，因此我们添加了一个检查每个问题中是否存在这些动作动词的二进制值。
- en: Question marks are good predictors as well, so we have generated a `has_question`
    feature.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问号也是一个很好的预测因子，因此我们生成了一个`has_question`特征。
- en: Questions about correct use of the English language tended not to get answers,
    so we added a `is_language_question` feature.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于英语语言正确使用的问题，往往得不到答案，因此我们添加了一个`is_language_question`特征。
- en: The length of the text of the question is another factor, with very short questions
    tending to go unanswered. This led to the addition of a normalized question length
    feature.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题的文本长度是另一个因素，非常短的问题往往得不到答复。这导致添加了一个归一化的问题长度特征。
- en: In our dataset, the title of the question contains crucial information as well,
    and looking at titles when labeling made the task much easier. This led to include
    the title text in all the earlier feature calculations.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的数据集中，问题的标题也包含关键信息，并且在标注时查看标题使任务变得更加容易。这导致在所有较早的特征计算中包括标题文本。
- en: Once we have an initial set of features, we can start building a model. Building
    this first model is the topic of the next chapter, [Chapter 5](ch05.html#first_model).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了一组初始特征，我们就可以开始构建模型。构建这个第一个模型是下一章[第5章](ch05.html#first_model)的主题。
- en: Before moving on to models, I wanted to dive deeper on the topic of how to gather
    and update a dataset. To do that, I sat down with Robert Munro, an expert in the
    field. I hope you enjoy the summary of our discussion here, and that it leaves
    you excited to move on to our next part, building our first model!
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入模型之前，我想更深入地探讨如何收集和更新数据集的主题。为此，我与罗伯特·门罗进行了交流，他是这个领域的专家。我希望你喜欢我们讨论的摘要，并期待你对我们下一个部分，构建我们的第一个模型，感到兴奋！
- en: 'Robert Munro: How Do You Find, Label, and Leverage Data?'
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 罗伯特·门罗：你如何找到、标记和利用数据？
- en: Robert Munro has founded several AI companies, building some of the top teams
    in artificial intelligence. He was chief technology officer at Figure Eight, a
    leading data labeling company during their biggest growth period. Before that,
    Robert ran product for AWS’s first native natural language processing and machine
    translation services. In our conversation, Robert shares some lessons he learned
    building datasets for ML.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 罗伯特·门罗成立了几家人工智能公司，在人工智能领域建立了一些顶尖团队。他曾是 Figure Eight 的首席技术官，在该公司最大的增长期间领导数据标注业务。在此之前，罗伯特负责了
    AWS 的首个本地自然语言处理和机器翻译服务的产品。在我们的对话中，罗伯特分享了他在构建机器学习数据集时学到的一些经验。
- en: 'Q: *How do you get started on an ML project?*'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: *如何开始一个机器学习项目？*'
- en: 'A: The best way is to start with the business problem, as it will give you
    boundaries to work with. In your ML editor case study example, are you editing
    text that someone else has written after they submit it, or are you suggesting
    edits live as somebody writes? The first would let you batch process requests
    with a slower model, while the second one would require something quicker.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 最好的方法是从业务问题开始，因为它会为你的工作提供边界。在你的机器学习编辑器案例研究中，例如，你是在提交后编辑别人写的文本，还是在别人写作时实时提出建议？第一种方式可以让你使用较慢的模型批量处理请求，而第二种方式则需要更快的处理。'
- en: In terms of models, the second approach would invalidate sequence-to-sequence
    models as they would be too slow. In addition, sequence-to-sequence models today
    do not work beyond sentence-level recommendations and require a lot of parallel
    text to be trained. A faster solution would be to leverage a classifier and use
    the important features it extracts as suggestions. What you want out of this initial
    model is an easy implementation and results you can have confidence in, starting
    with naive Bayes on bag of words features, for example.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型方面，第二种方法会使序列到序列模型失效，因为它们会变得过慢。此外，今天的序列到序列模型在句子级推荐之外并不起作用，需要大量平行文本来训练。更快的解决方案是利用分类器，并使用它提取的重要特征作为建议。你想要从这个初始模型中得到的是一个简单的实现和你可以信任的结果，例如从词袋特征的朴素贝叶斯开始。
- en: Finally, you need to spend some time looking at some data and labeling it yourself.
    This will give you an intuition for how hard the problem is and which solutions
    might be a good fit.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要花一些时间查看一些数据并自己标记它。这将让你对问题的难度有直观感觉，并找出可能合适的解决方案。
- en: 'Q: *How much data do you need to get started?*'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: *你需要多少数据才能开始？*'
- en: 'A: When gathering data, you are looking to guarantee that you have a representative
    and diverse dataset. Start by looking at the data you have and seeing if any types
    are unrepresented so that you can gather more. Clustering your dataset and looking
    for outliers can be helpful to speed up this process.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 在收集数据时，你要确保你拥有一个代表性和多样化的数据集。首先查看你拥有的数据，并看看是否有任何未被代表的类型，以便你可以收集更多数据。对数据集进行聚类并寻找异常值可以帮助加快这个过程。'
- en: For labeling data, in the common case of classification, we’ve seen that labeling
    on the order of 1,000 examples of your rarer category works well in practice.
    You’ll at least get enough signal to tell you whether to keep going with your
    current modeling approach. At around 10,000 examples, you can start to trust in
    the confidence of the models you are building.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据标记，通常在分类的常见情况下，我们发现，对于你更稀少的类别，标记约1000个示例在实践中效果良好。至少你会得到足够的信号告诉你是否继续使用当前的建模方法。大约在10000个示例时，你可以开始信任你正在构建的模型的置信度。
- en: As you get more data, your model’s accuracy will slowly build up, giving you
    a curve of how your performance scales with data. At any point you only care about
    the last part of the curve, which should give you an estimate of the current value
    more data will give you. In the vast majority of cases, the improvement you will
    get from labeling more data will be more significant than if you iterated on the
    model.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你获取更多数据，你的模型准确性会慢慢提高，为你提供一个随数据增加而改进性能的曲线。在任何时候，你只关心曲线的最后部分，这应该给你一个当前价值的估计，更多的数据将会给你更大的改进。在绝大多数情况下，从标记更多数据中获得的改进将比迭代模型更为显著。
- en: 'Q: *What process do you use to gather and label data?*'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: *你使用什么过程来收集和标记数据？*'
- en: 'A: You can look at your current best model and see what is tripping it up.
    Uncertainty sampling is a common approach: identify examples that your model is
    the most uncertain about (the ones closest to its decision boundary), and find
    similar examples to add to the training set.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 你可以查看你当前的最佳模型，看看它遇到了什么问题。不确定性采样是一种常见方法：识别你的模型最不确定的例子（即最接近其决策边界的例子），并找到类似的例子添加到训练集中。'
- en: You can also train an “error model” to find more data your current model struggles
    on. Use the mistakes your model makes as labels (labeling each data point as “predicted
    correctly” or “predicted incorrectly”). Once you train an “error model” on these
    examples, you can use it on your unlabeled data and label the examples that it
    predicts your model will fail on.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以训练一个“错误模型”来找出当前模型难以处理的更多数据。使用你的模型犯错作为标签（将每个数据点标记为“预测正确”或“预测错误”）。一旦你在这些例子上训练了一个“错误模型”，你可以将其用于你的未标记数据，并标记它预测你的模型会失败的例子。
- en: Alternatively, you can train a “labeling model” to find the best examples to
    label next. Let’s say you have a million examples, of which you’ve labeled only
    1,000\. You can create a training set of 1,000 randomly sampled labeled images,
    and 1,000 unlabeled, and train a binary classifier to predict which images you
    have labeled. You can then use this labeling model to identify data points that
    are most different from what you’ve already labeled and label those.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以训练一个“标注模型”来找到下一个最佳标注的例子。假设你有一百万个例子，你只标注了一千个。你可以创建一个包含一千个随机抽样标记图像和一千个未标记图像的训练集，并训练一个二元分类器来预测哪些图像你已经标记过。然后，你可以使用这个标注模型来识别与你已经标记过的内容最不同的数据点，并进行标注。
- en: 'Q: *How do you validate that your models are learning something useful?*'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: *你如何验证你的模型学到了有用的东西？*'
- en: 'A: A common pitfall is to end up focusing labeling efforts on a small part
    of the relevant dataset. It may be that your model struggles with articles that
    are about basketball. If you keep annotating more basketball articles, your model
    may become great at basketball but bad at everything else. This is why while you
    should use strategies to gather data, you should always randomly sample from your
    test set to validate your model.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 一个常见的陷阱是将标注工作集中在相关数据集的一小部分上。也许你的模型在篮球相关的文章上表现不佳。如果你继续标注更多的篮球文章，你的模型可能会在篮球方面表现出色，但其他方面表现不佳。这就是为什么在使用数据收集策略的同时，你应该始终从测试集中随机抽样以验证你的模型。'
- en: 'Finally, the best way to do it is to track when the performance of your deployed
    model drifts. You could track the uncertainty of the model or ideally bring it
    back to the business metrics: are your usage metrics gradually going down? This
    could be caused by other factors, but is a good trigger to investigate and potentially
    update your training set.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最好的方法是跟踪你部署模型性能何时出现漂移。你可以追踪模型的不确定性，或者最理想的是将其转化为业务指标：你的使用指标是否逐渐下降？这可能是由其他因素引起的，但是这是一个很好的触发点，可以调查并可能更新你的训练集。
- en: Conclusion
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we covered important tips to efficiently and effectively examine
    a dataset.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了有效和高效地检查数据集的重要提示。
- en: 'We started by looking at the quality of data and how to decide whether it is
    sufficient for our needs. Next, we covered the best way to get familiar with the
    type of data you have: starting with summary statistics and moving on to clusters
    of similar points to identify broad trends.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先看数据的质量以及如何决定它是否足够满足我们的需求。接下来，我们介绍了熟悉你所拥有的数据类型的最佳方式：从汇总统计开始，然后转向相似点的聚类，以识别广泛的趋势。
- en: We then covered why it is valuable to spend some significant time labeling data
    to identify trends that we can then leverage to engineer valuable features. Finally,
    we got to learn from Robert Munro’s experience helping multiple teams build state-of-the-art
    datasets for ML.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后讨论了为什么花费大量时间标记数据以识别趋势是有价值的，这些趋势我们可以利用来构建有价值的特征。最后，我们从罗伯特·蒙罗（Robert Munro）帮助多个团队构建先进机器学习数据集的经验中学到了一些东西。
- en: Now that we’ve examined a dataset and generated features we hope to be predictive,
    we are ready to build our first model, which we will do in [Chapter 5](ch05.html#first_model).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经检查了一个数据集并生成了我们希望能够预测的特征，我们准备构建我们的第一个模型，这将在[第5章](ch05.html#first_model)中完成。
