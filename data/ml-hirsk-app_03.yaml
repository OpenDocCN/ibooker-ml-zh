- en: Chapter 1\. Contemporary Machine Learning Risk Management
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章. 当代机器学习风险管理
- en: Building the best machine learning system starts with cultural competencies
    and business processes. This chapter presents numerous cultural and procedural
    approaches we can use to improve ML performance and safeguard our organizations’
    ML against real-world safety and performance problems. It also includes a case
    study that illustrates what happens when an ML system is used without proper human
    oversight. The primary goal of the approaches discussed in this chapter is to
    create better ML systems. This might mean improved in silico test data performance.
    But it really means building models that perform as expected once deployed in
    vivo, so we don’t lose money, hurt people, or cause other harms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 构建最佳的机器学习系统始于文化能力和业务流程。本章介绍了许多文化和程序方法，可以用来改进ML性能并保护我们组织的ML免受现实世界的安全性和性能问题。它还包括一个案例研究，说明了当ML系统在没有适当人类监督的情况下使用时会发生什么。本章讨论的方法的主要目标是创建更好的ML系统。这可能意味着改进体外测试数据的性能。但实际上意味着构建一旦部署在体内就能按预期运行的模型，以便我们不会损失金钱、伤害人员或造成其他伤害。
- en: Note
  id: totrans-2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '*In vivo* is Latin for “within the living.” We’ll sometimes use this term to
    mean something closer to “interacting with the living,” as in how ML models perform
    in the real world when interacting with human users. *In silico* means “by means
    of computer modeling or computer simulation,” and we’ll use this term to describe
    the testing data scientists often perform in their development environments before
    deploying ML models.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*In vivo*是拉丁文，意为“在生活中”。我们有时会使用这个术语来表示与人类用户交互时ML模型在真实世界中的表现方式。*In silico*意思是“通过计算建模或计算机模拟”，我们将使用这个术语来描述数据科学家在开发环境中部署ML模型之前经常进行的测试数据。'
- en: The chapter begins with a discussion of the current legal and regulatory landscape
    for ML and some nascent best-practice guidance, to inform system developers of
    their fundamental obligations when it comes to safety and performance. We’ll also
    introduce how the book aligns to the National Institute of Standards and Technology
    (NIST) [AI Risk Management Framework](https://oreil.ly/Or940) (RMF) in this part
    of the chapter. Because those who do not study history are bound to repeat it,
    the chapter then highlights AI incidents, and discusses why understanding AI incidents
    is important for proper safety and performance in ML systems. Since many ML safety
    concerns require thinking beyond technical specifications, the chapter then blends
    model risk management (MRM), information technology (IT) security guidance, and
    practices from other fields to put forward numerous ideas for improving ML safety
    culture and processes within organizations. The chapter will close with a case
    study focusing on safety culture, legal ramifications, and AI incidents.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以讨论当前机器学习法律和监管环境以及一些新兴的最佳实践指导开始，以帮助系统开发人员了解在安全性和性能方面的基本义务。我们还会介绍本书如何与国家标准与技术研究所（NIST）[AI风险管理框架](https://oreil.ly/Or940)（RMF）保持一致。因为不学习历史的人注定会重蹈覆辙，该章节还突出了AI事故，并讨论了为何了解AI事故对于ML系统的安全性和性能至关重要。由于许多ML安全问题需要超越技术规范的思考，该章节还融合了模型风险管理（MRM）、信息技术（IT）安全指导以及其他领域的实践，提出了许多改进组织内ML安全文化和流程的想法。该章节将以一个关于安全文化、法律后果和AI事故的案例研究作为结尾。
- en: None of the risk management approaches discussed in this chapter are a silver
    bullet. If we want to manage risk successfully, we’ll need to pick from the wide
    variety of available controls those that work best for our organization. Larger
    organizations will typically be able to do more risk management than smaller organizations.
    Readers at large organizations may be able to implement many controls across various
    departments, divisions, or internal functions. Readers at smaller organizations
    will have to choose their risk management tactics judiciously. In the end, a great
    deal of technology risk management comes down to human behavior. Whichever risk
    controls an organization implements, they’ll need to be paired with strong governance
    and policies for the *people* that build and maintain ML systems.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的风险管理方法没有一种是万全之策。如果我们希望成功地管理风险，我们需要从众多可用的控制措施中选择最适合我们组织的措施。较大的组织通常能够进行比较全面的风险管理。大型组织的读者可能能够在各个部门、分部或内部职能中实施许多控制措施。较小的组织的读者将不得不审慎选择其风险管理策略。最终，技术风险管理很大程度上取决于人的行为。无论组织实施哪些风险控制措施，都需要与建立和维护ML系统的*人员*的强有力治理和政策相结合。
- en: A Snapshot of the Legal and Regulatory Landscape
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 法律和监管环境的快照
- en: It’s a myth that ML is unregulated. ML systems can and do break the law. Forgetting
    or ignoring the legal context is one of the riskiest things an organization can
    do with respect to ML systems. That said, the legal and regulatory landscape for
    ML is complicated and changing quickly. This section aims to provide a snapshot
    of important laws and regulations for overview and awareness purposes. We’ll start
    by highlighting the pending EU AI Act. We’ll then discuss the many US federal
    laws and regulations that touch on ML, US state and municipal laws for data privacy
    and AI, and the basics of product liability, then end the section with a rundown
    of recent Federal Trade Commission (FTC) enforcement actions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ML 没有监管是个谬论。ML 系统可能会违法。忘记或忽视法律背景是组织对ML系统做的最危险的事情之一。尽管如此，ML 的法律和监管环境又复杂且快速变化。本节旨在提供重要的法律和监管法规的概览和认识。我们将首先介绍即将推出的欧盟AI法案。然后我们将讨论许多美国联邦法律和法规与ML相关，美国州和市政法律与AI相关，以及产品责任的基本情况，最后以对最近联邦贸易委员会（FTC）的执法行动的概述结束本节。
- en: Warning
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The authors are not lawyers and nothing in this book is legal advice. The intersection
    of law and AI is an incredibly complex topic that data scientists and ML engineers
    are not equipped to handle alone. You may have legal concerns about ML systems
    that you work on. If so, seek real legal advice.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作者不是律师，本书中没有任何法律建议。法律与AI的交集是一个极其复杂的主题，数据科学家和ML工程师无法单独处理。您可能对您所工作的ML系统有法律上的顾虑。如果是这样，请寻求真正的法律建议。
- en: The Proposed EU AI Act
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 欧盟AI法案建议
- en: The EU has proposed sweeping regulations for AI that are expected to be passed
    in 2023\. Known as the [EU AI Act](https://oreil.ly/x5dLT) (AIA), they would prohibit
    certain uses of AI like distorting human behavior, social credit scoring, and
    real-time biometric surveillance. The AIA deems other uses to be high risk, including
    applications in criminal justice, biometric identification, employment screening,
    critical infrastructure management, law enforcement, essential services, immigration,
    and others—placing a high documentation, governance, and risk management burden
    on these. Other applications would be considered limited or low risk, with fewer
    compliance obligations for their makers and operators. Much like the EU General
    Data Protection Regulation (GDPR) has changed the way companies handle data in
    the US and around the world, EU AI regulations are designed to have an outsized
    impact on US and other international AI deployments. Whether we’re working in
    the EU or not, we may need to start familiarizing ourselves with the AIA. One
    of the best ways is to read the [Annexes](https://oreil.ly/0k_TQ), especially
    Annexes 1 and 3–8, that define terms and layout documentation and conformity requirements.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟已提议针对人工智能的全面法规，预计将于2023年通过。被称为[欧盟AI法案](https://oreil.ly/x5dLT)（AIA），它们将禁止某些使用AI的方式，如扭曲人类行为、社会信用评分和实时生物识别监视。AIA将其他使用情形视为高风险，包括在刑事司法、生物识别识别、就业筛选、关键基础设施管理、执法、基本服务、移民等领域的应用，对这些领域施加了高度的文档化、治理和风险管理责任。其他应用将被视为有限或低风险，对其制造商和运营商的合规义务较少。就像欧盟的《通用数据保护条例》（GDPR）已经改变了美国及全球公司处理数据的方式一样，欧盟的AI法规旨在对美国及其他国际AI部署产生巨大影响。无论我们是在欧盟工作与否，可能都需要开始熟悉AIA。阅读[附件](https://oreil.ly/0k_TQ)，特别是定义术语和布局文件和符合性要求的附件1和3-8，是了解AIA的最佳方式之一。
- en: US Federal Laws and Regulations
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 美国联邦法律和法规
- en: Because we’ve been using algorithms in one form or another for decades in our
    government and economy, many US federal laws and regulations already touch on
    AI and ML. These regulations tend to focus on social discrimination by algorithms,
    but also treat transparency, privacy, and other topics. The Civil Rights Acts
    of 1964 and 1991, the Americans with Disabilities Act (ADA), the Equal Credit
    Opportunity Act (ECOA), the Fair Credit Reporting Act (FCRA), and the Fair Housing
    Act (FHA) are some of the federal laws that attempt to prevent discrimination
    by algorithms in areas like employment, credit lending, and housing. ECOA and
    FCRA, along with their more detailed implementation in Regulation B, attempt to
    increase transparency in ML-based credit lending and guarantee recourse rights
    for credit consumers. For a rejected credit application, lenders are expected
    to indicate the reasons for the rejection, i.e., an *adverse action*, and describe
    the features in the ML model that drove the decision. If the provided reasoning
    or data is wrong, consumers should be able to appeal the decision.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于几十年来我们在政府和经济中以各种形式使用算法，许多美国联邦法律和法规已经涉及人工智能（AI）和机器学习（ML）。这些法规通常关注算法引发的社会歧视问题，同时也涉及透明度、隐私等主题。1964年和1991年的《民权法案》、《美国残疾人法案》（ADA）、《平等信贷机会法》（ECOA）、《公平信用报告法》（FCRA）和《公平住房法》（FHA）等是一些旨在防止就业、信贷借贷和住房等领域的算法歧视的联邦法律。ECOA和FCRA以及它们在《B条例》中更为详细的实施尝试增加ML基础信贷借贷的透明度，并保障信贷消费者的申诉权利。对于被拒绝的信贷申请，贷方应当说明拒绝的原因，即*不利行动*，并描述驱动决策的ML模型的特征。如果提供的推理或数据有误，消费者应能够申诉决策。
- en: The practice of MRM, defined in part in the Federal Reserve’s [SR 11-7 guidance](https://oreil.ly/xpr5P),
    forms a part of regulatory examinations for large US banks, and sets up organizational,
    cultural, and technical processes for good and reliable performance of ML used
    in mission-critical financial applications. Much of this chapter is inspired by
    MRM guidance, as it’s the most battle-tested ML risk management framework out
    there. Laws like the Health Insurance Portability and Accountability Act of 1996
    (HIPAA) and the Family Educational Rights and Privacy Act (FERPA) set up serious
    data privacy expectations in healthcare and for students. Like the GDPR, HIPAA’s
    and FERPA’s interactions with ML are material, complex, and still debated. These
    are not even all the US laws that might affect our use of ML, but hopefully this
    brief listing provides an idea of what the US federal government has decided is
    important enough to regulate.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: MRM实践的定义部分包含在联邦储备委员会的[SR 11-7指南](https://oreil.ly/xpr5P)中，这形成了对大型美国银行进行监管审查的一部分，并为在关键金融应用中使用的机器学习的良好和可靠性表现设置了组织、文化和技术流程。本章的很大部分受到MRM指导的启发，因为它是经过最多次检验的机器学习风险管理框架。类似于1996年的《健康保险移植和责任法案》（HIPAA）和《家庭教育权利与隐私法案》（FERPA）等法律在医疗保健和学生数据隐私方面设定了严格的期望。与GDPR类似，HIPAA和FERPA与机器学习的互动具有实质性、复杂且仍在辩论中的影响。这些甚至不是所有可能影响我们机器学习使用的美国法律，但希望这个简要列举提供了美国联邦政府认为重要而需要监管的概念。
- en: State and Municipal Laws
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 州和市政法律
- en: US states and cities have also taken up laws and regulations for AI and ML.
    New York City (NYC) Local Law 144, which mandates bias audits for automated employment
    decision tools, was initially expected to go into effect in January 2023\. Under
    this law, every major employer in NYC will have to conduct bias testing of automated
    employment software and post the results on their website. Washington DC’s proposed
    Stop Discrimination by Algorithms Act attempts to replicate federal expectations
    for nondiscrimination and transparency, but for a much broader set of applications,
    for companies that operate in DC or use the data of many DC citizens.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 美国各州和城市也已经开始制定AI和ML的法律和法规。纽约市（NYC）的地方法规144号要求对自动化就业决策工具进行偏见审计，最初预计将于2023年1月生效。根据这项法律，纽约市的每家主要雇主都必须对自动化就业软件进行偏见测试，并在其网站上发布结果。华盛顿特区提议的《通过算法停止歧视法案》试图复制联邦对非歧视和透明度的期望，但适用范围更广，适用于在DC运营或使用许多DC市民数据的公司。
- en: Numerous states passed their own data privacy laws as well. Unlike the older
    HIPAA and FERPA federal laws, these state data privacy laws are often intentionally
    designed to partially regulate the use of AI and ML. States like California, Colorado,
    Virginia, and others have passed data privacy laws that mention increased transparency,
    decreased bias, or both, for automated decision-making systems. Some states have
    put biometric data or social media in their regulatory crosshairs too. For example,
    Illinois’ Biometric Information Privacy Act (BIPA) outlaws many uses of biometric
    data, and IL regulators have already started enforcement actions. The lack of
    a federal data privacy or AI law combined with this new crop of state and local
    laws makes the AI and ML compliance landscape very complicated. Our uses of ML
    may or may not be regulated, or may be regulated to varying degrees, based on
    the specific application, industry, and geography of the system.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 许多州还通过了自己的数据隐私法律。与老式的HIPAA和FERPA联邦法不同，这些州数据隐私法律通常是有意设计为部分监管AI和ML的使用。像加利福尼亚州、科罗拉多州、弗吉尼亚州等州通过的数据隐私法律提到增加透明度、减少偏见或两者兼顾，适用于自动决策系统。一些州还将生物特征数据或社交媒体纳入其监管范围。例如，伊利诺伊州的《生物信息隐私法》（BIPA）禁止许多生物特征数据的使用，并且伊州监管机构已经开始了执行行动。联邦数据隐私或AI法律的缺失，加上这些新的州和地方法律，使得AI和ML的合规性格局非常复杂。我们对ML的使用可能或可能不受到监管，或者根据系统的具体应用、行业和地理位置在不同程度上受到监管。
- en: Basic Product Liability
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本产品责任
- en: 'As makers of consumer products, data scientists and ML engineers have a fundamental
    obligation to create safe systems. To quote a recent Brookings Institute report,
    [“Products Liability Law as a Way to Address AI Harms”](https://oreil.ly/2K_R6),
    “Manufacturers have an obligation to make products that will be safe when used
    in reasonably foreseeable ways. If an AI system is used in a foreseeable way and
    yet becomes a source of harm, a plaintiff could assert that the manufacturer was
    negligent in not recognizing the possibility of that outcome.” Just like car or
    power tool manufacturers, makers of ML systems are subject to broad legal standards
    for negligence and safety. Product safety has been the subject of large amounts
    of legal and economic analysis, but this subsection will focus on one of the first
    and simplest standards for negligence: the Hand rule. Named after Judge Learned
    Hand, and coined in 1947, it provides a viable framework for ML product makers
    to think about negligence and due diligence. The Hand rule says that a product
    maker takes on a burden of care, and that the resources expended on that care
    should always be greater than the cost of a likely incident involving the product.
    Stated algebraically:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作为消费品制造商，数据科学家和机器学习工程师有责任创建安全系统。引用最近的布鲁金斯研究所报告，[“产品责任法作为解决人工智能伤害的途径”](https://oreil.ly/2K_R6)，“制造商有责任制造在合理预见的使用方式下安全的产品。如果人工智能系统在可预见的使用方式下造成伤害，原告可以主张制造商因未能认识到可能性而存在过失。”
    就像汽车或电动工具制造商一样，机器学习系统的制造商也要遵守广泛的过失和安全法律标准。产品安全已经成为大量法律和经济分析的主题，但本小节将专注于最早和最简单的过失标准之一：汉德法则。以法官勒纳德·汉德命名，并在1947年首次提出，它为机器学习产品制造商提供了一个可行的过失和尽职调查框架。汉德法则指出，产品制造商承担了一种关怀的负担，并且在这种关怀上投入的资源应始终大于可能涉及产品的事故的成本。用代数方式表述：
- en: <math alttext="dollar-sign upper B u r d e n greater-than-or-equal-to upper
    R i s k equals left-parenthesis upper P r o b a b i l i t y o f l o s s right-parenthesis
    left-parenthesis upper L o s s s i z e right-parenthesis dollar-sign"><mrow><mi>B</mi>
    <mi>u</mi> <mi>r</mi> <mi>d</mi> <mi>e</mi> <mi>n</mi> <mo>≥</mo> <mi>R</mi> <mi>i</mi>
    <mi>s</mi> <mi>k</mi> <mo>=</mo> <mo>(</mo> <mi>P</mi> <mi>r</mi> <mi>o</mi> <mi>b</mi>
    <mi>a</mi> <mi>b</mi> <mi>i</mi> <mi>l</mi> <mi>i</mi> <mi>t</mi> <mi>y</mi> <mi>o</mi>
    <mi>f</mi> <mi>l</mi> <mi>o</mi> <mi>s</mi> <mi>s</mi> <mo>)</mo> <mo>(</mo> <mi>L</mi>
    <mi>o</mi> <mi>s</mi> <mi>s</mi> <mi>s</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <mo>)</mo></mrow></math>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper B u r d e n greater-than-or-equal-to upper
    R i s k equals left-parenthesis upper P r o b a b i l i t y o f l o s s right-parenthesis
    left-parenthesis upper L o s s s i z e right-parenthesis dollar-sign"><mrow><mi>B</mi>
    <mi>u</mi> <mi>r</mi> <mi>d</mi> <mi>e</mi> <mi>n</mi> <mo>≥</mo> <mi>R</mi> <mi>i</mi>
    <mi>s</mi> <mi>k</mi> <mo>=</mo> <mo>(</mo> <mi>P</mi> <mi>r</mi> <mi>o</mi> <mi>b</mi>
    <mi>a</mi> <mi>b</mi> <mi>i</mi> <mi>l</mi> <mi>i</mi> <mi>t</mi> <mi>y</mi> <mi>o</mi>
    <mi>f</mi> <mi>l</mi> <mi>o</mi> <mi>s</mi> <mi>s</mi> <mo>)</mo> <mo>(</mo> <mi>L</mi>
    <mi>o</mi> <mi>s</mi> <mi>s</mi> <mi>s</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <mo>)</mo></mrow></math>
- en: In more plain terms, organizations are expected to apply care, i.e., time, resources,
    or money, to a level commensurate to the cost associated with a foreseeable risk.
    Otherwise liability can ensue. In [Figure 1-1](#hand), Burden is the parabolically
    increasing line, and risk, or Probability multiplied by Loss, is the parabolically
    decreasing line. While these lines are not related to a specific measurement,
    their parabolic shape is meant to reflect the last-mile problem in removing all
    ML system risk, and shows that the application of additional care beyond a reasonable
    threshold leads to diminishing returns for decreasing risk as well.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 更通俗地说，组织被期望按照与可预见风险相关的成本水平来应用关怀，即时间、资源或金钱。否则将可能导致法律责任。在[图1-1](#hand)中，负担是抛物线增长线，而风险或概率乘以损失是抛物线下降线。尽管这些线条与具体测量无关，但它们的抛物线形状旨在反映消除所有机器学习系统风险的最后一公里问题，并显示超过合理阈值的额外关怀会导致递减的风险回报率。
- en: '![mlha 0101](assets/mlha_0101.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0101](assets/mlha_0101.png)'
- en: Figure 1-1\. The Hand rule (adapted from [“Economic Analysis of Alternative
    Standards of Liability in Accident Law”](https://oreil.ly/9_u8H))
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1\. 汉德法则（改编自[“事故法中责任替代标准的经济分析”](https://oreil.ly/9_u8H)）
- en: Note
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注：
- en: A fairly standard definition for the *risk* of a technology incident is the
    estimated likelihood of the incident multiplied by its estimated cost. More broadly,
    the International Organization for Standardization (ISO) defines risk in the context
    of enterprise risk management as the “effect of uncertainty on objectives.”
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对技术事故的风险的相当标准定义是估计的事故发生的可能性乘以其估计的成本。更广泛地说，国际标准化组织（ISO）在企业风险管理背景下定义风险为“不确定性对目标的影响”。
- en: While it’s probably too resource intensive to calculate the quantities in the
    Hand rule exactly, it is important to think about these concepts of negligence
    and liability when designing an ML system. For a given ML system, if the probability
    of an incident is high, if the monetary or other loss associated with a system
    incident is large, or if both quantities are large, organizations need to spend
    extra resources on ensuring safety for that system. Moreover, organizations should
    document to the best of their ability that due diligence exceeds the estimated
    failure probabilities multiplied by the estimated losses.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管按照Hand法则准确计算数量可能太耗资源，但在设计机器学习系统时考虑到过失和责任的概念非常重要。对于给定的机器学习系统，如果事故发生的概率高，如果与系统事故相关的金钱或其他损失很大，或者两者都很大，组织需要额外投入资源来确保该系统的安全性。此外，组织应尽最大努力记录尽职调查超过估计的故障概率乘以估计的损失。
- en: Federal Trade Commission Enforcement
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联邦贸易委员会执法
- en: How might we actually get in trouble? If you’re working in a regulated industry,
    you probably know your regulators. But if we don’t know if our work is regulated
    or who might be enforcing consequences if we cross a legal or regulatory red line,
    it’s probably the US Federal Trade Commission we need to be most concerned with.
    The FTC is broadly focused on unfair, deceptive, or predatory trade practices,
    and they have found reason to take down at least three prominent ML algorithms
    in three years. With their new enforcement tool, *algorithmic disgorgement*, the
    FTC has the ability to delete algorithms and data, and, typically, prohibit future
    revenue generation from an offending algorithm. [Cambridge Analytica](https://oreil.ly/cM3V8)
    was the first firm to face this punishment, after their deceptive data collection
    practices surrounding the 2016 election. [Everalbum](https://oreil.ly/05SO5) and
    [WW](https://oreil.ly/PMOq0), known as Weight Watchers, have also faced disgorgement.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会陷入麻烦的情况是怎样的？如果你在受监管的行业工作，你可能知道你的监管者。但如果我们不知道我们的工作是否受到监管，或者如果我们越过了法律或监管的红线会有什么后果，那么我们可能需要最关注的是美国联邦贸易委员会（FTC）。FTC广泛关注不公平、欺骗或掠夺性贸易行为，并且他们已经找到理由在三年内摧毁了至少三个知名的机器学习算法。凭借他们的新执法工具*算法返还*，FTC有能力删除算法和数据，并且通常会禁止未来通过有问题的算法进行收入的生成。[剑桥分析公司](https://oreil.ly/cM3V8)是第一家面临这种惩罚的公司，因为他们在2016年选举期间围绕欺骗性数据收集实践。[Everalbum](https://oreil.ly/05SO5)和[WW](https://oreil.ly/PMOq0)，即重量观察者，也曾面临返还。
- en: The FTC has been anything but quiet about its intention to enforce federal laws
    around AI and ML. FTC commissioners have penned lengthy treatises on [algorithms
    and economic justice](https://oreil.ly/v8Z4y). They have also posted at least
    two blogs providing high-level guidance for companies who would like to avoid
    the unpleasantness of enforcement actions. These blogs highlight a number of concrete
    steps organizations should take. For example, in [“Using Artificial Intelligence
    and Algorithms”](https://oreil.ly/066Y-), the FTC makes it clear that consumers
    should not be misled into interacting with an ML system posing as a human. Accountability
    is another prominent theme in “Using Artificial Intelligence and Algorithms,”
    [“Aiming for Truth, Fairness, and Equity in Your Company’s Use of AI”](https://oreil.ly/XMqKo),
    and other related publications. In “Aiming for Truth, Fairness, and Equity in
    Your Company’s Use of AI,” the FTC states, “**Hold yourself accountable—or be
    ready for the FTC to do it for you**” (emphasis added by the original author).
    This extremely direct language is unusual from a regulator. In “Using Artificial
    Intelligence and Algorithms,” the FTC puts forward, “Consider how you hold yourself
    accountable, and whether it would make sense to use independent standards or independent
    expertise to step back and take stock of your AI.” The next section introduces
    some of the emerging independent standards we can use to increase accountability,
    make better products, and decrease any potential legal liability.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: FTC对于执行AI和ML的联邦法律一直持续关注，毫不掩饰其意图。 FTC委员会在[算法和经济正义](https://oreil.ly/v8Z4y)方面撰写了漫长的论文。
    他们还发布了至少两篇博客，为希望避免执行行动的公司提供了高层次的指导。 这些博客突出了组织应该采取的若干具体步骤。 例如，在[“使用人工智能和算法”](https://oreil.ly/066Y-)中，FTC明确表示，不应误导消费者与伪装成人类的ML系统互动。
    "责任"是“在公司使用AI中追求真相，公平和公平性”的另一个主题，“对自己负责，否则FTC将代您负责”（原作者强调）。 这种非常直接的语言对于监管者来说是不寻常的。
    在“使用人工智能和算法”中，FTC提出：“考虑如何对自己负责，以及使用独立标准或独立专业知识是否有意义，以退后一步并审视您的AI。” 下一节介绍了我们可以使用的一些新兴独立标准，以增加责任感，制造更好的产品并减少任何潜在的法律责任。
- en: Authoritative Best Practices
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权威最佳实践
- en: Data science mostly lacks professional standards and licensing today, but some
    authoritative guidance is starting to appear on the horizon. ISO is beginning
    to outline [technical standards for AI](https://oreil.ly/8ZeJQ). Making sure our
    models are in line with ISO standards would be one way to apply an independent
    standard to our ML work. Particularly for US-based data scientists, the NIST AI
    RMF is a very important project to watch.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当今数据科学大多数缺乏专业标准和许可，但一些权威指导正在逐渐出现。 ISO正在开始概述[AI的技术标准](https://oreil.ly/8ZeJQ)。
    确保我们的模型符合ISO标准将是将独立标准应用于我们的ML工作的一种方法。 特别是对于美国的数据科学家来说，NIST AI RMF是一个非常重要的项目要关注。
- en: 'Version 1 of the AI RMF was released in January 2023\. The framework puts forward
    characteristics of trustworthiness in AI systems: validity, reliability, safety,
    security, resiliency, transparency, accountability, explainability, interpretability,
    bias management, and enhanced privacy. Then it presents actionable guidance across
    four organizational functions—map, measure, manage, and govern—for achieving trustworthiness.
    The guidance in the map, measure, manage, and govern functions is subdivided into
    more detailed categories and subcategories. To see these categories of guidance,
    check out the [RMF](https://oreil.ly/kxq-G) or the [AI RMF playbook](https://oreil.ly/dn4xs),
    which provides even more detailed suggestions.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: AI RMF的第1版于2023年1月发布。 框架提出了AI系统信任度的特征：有效性，可靠性，安全性，韧性，透明度，责任，可解释性，解释性，偏见管理和增强隐私。
    然后，它在四个组织功能（映射，测量，管理和治理）中提供了可操作的指导。 映射，测量，管理和治理功能中的指导被细分为更详细的类别和子类别。 要查看这些指导类别，请查看[RMF](https://oreil.ly/kxq-G)或[AI
    RMF playbook](https://oreil.ly/dn4xs)，该书提供了更详细的建议。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The NIST AI Risk Management Framework is a *voluntary* tool for improving the
    trustworthiness of AI and ML systems. The AI RMF is not regulation and NIST is
    not a regulator.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: NIST AI风险管理框架是改进AI和ML系统信任度的*自愿*工具。 AI RMF不是法规，NIST也不是监管机构。
- en: To follow our own advice, and that of regulators and publishers of authoritative
    guidance, and to make this book more useful, we’ll be calling out how *we believe*
    the content of each chapter in [Part I](part01.html#part_1) aligns to the AI RMF.
    Following this paragraph, readers will find a callout box that matches the chapter
    subheadings with AI RMF subcategories. The idea is that readers can use the table
    to understand how employing the approaches discussed in each chapter may help
    them adhere to the AI RMF. Because the subcategory advice may, in some cases,
    sound abstract to ML practitioners, we provide more practice-oriented language
    that matches the RMF categories; this will be helpful in translating the RMF into
    in vivo ML deployments. Check out the callout box to see how we think [Chapter 1](#unique_chapter_id_1)
    aligns with the AI RMF, and look for similar tables at the start of each chapter
    in [Part I](part01.html#part_1).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了遵循我们自己的建议，以及监管机构和权威指南的建议，并使本书更加实用，我们将指出每个章节在 [第一部分](part01.html#part_1) 中与AI
    RMF的内容如何对应。在这段文字之后，读者将找到一个专门匹配AI RMF子类别的信息框。这样做的目的是让读者能够使用表格了解每个章节讨论的方法如何帮助他们遵循AI
    RMF。由于在某些情况下，子类别的建议可能对机器学习从业者而言显得抽象，我们提供更加实践导向的语言，与RMF类别相匹配；这将有助于将RMF转化为实际的机器学习部署。查看信息框，看看我们如何认为
    [第1章](#unique_chapter_id_1) 与AI RMF相符，并在 [第一部分](part01.html#part_1) 的每个章节开头寻找类似的表格。
- en: AI Incidents
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 事件
- en: In many ways, the fundamental goal of ML safety processes and related model
    debugging, also discussed in [Chapter 3](ch03.html#unique_chapter_id_3), is to
    prevent and mitigate AI incidents. Here, we’ll loosely define AI incidents as
    any outcome of the system that could cause harm. As becomes apparent when using
    the Hand rule as a guide, the severity of an AI incident is increased by the loss
    the incident causes, and decreased by the care taken by the operators to mitigate
    those losses.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在很多方面，机器学习安全流程和相关模型调试的基本目标，也在 [第3章](ch03.html#unique_chapter_id_3) 中讨论，是预防和减轻AI事件。在这里，我们宽泛地定义AI事件为系统可能造成危害的任何结果。当使用Hand法则作为指导时，一个AI事件造成的损失会增加，而操作人员采取的措施来减少这些损失会减少。
- en: 'Because complex systems drift toward failure, there is no shortage of AI incidents
    to discuss as examples. AI incidents can range from annoying to deadly—from [mall
    security robots falling down stairs](https://oreil.ly/fLHU1), to [self-driving
    cars killing pedestrians](https://oreil.ly/vFW_-), to [mass-scale diversion of
    healthcare resources](https://oreil.ly/2e8WQ) away from those who need them most.
    As pictured in [Figure 1-2](#ai_incident_taxonomy), AI incidents can be roughly
    divided into three buckets:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因为复杂系统趋向于失败，所以没有缺乏可以作为例子讨论的AI事件。AI事件可以从令人讨厌的事情到致命的事情不等——从 [购物中心安保机器人摔倒楼梯](https://oreil.ly/fLHU1)，到
    [自动驾驶汽车撞到行人](https://oreil.ly/vFW_-)，再到 [大规模转移医疗资源](https://oreil.ly/2e8WQ) 远离最需要它们的人群。如
    [图1-2](#ai_incident_taxonomy) 所示，AI事件大致可以分为三大类：
- en: Abuses
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 滥用
- en: AI can be used for nefarious purposes, apart from specific hacks and attacks
    on other AI systems. The day may already have come when hackers use AI to increase
    the efficiency and potency of their more general attacks. What the future could
    hold is even more frightening. Specters like autonomous drone attacks and ethnic
    profiling by authoritarian regimes are already on the horizon.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: AI可以被用于恶意目的，与其他AI系统的特定攻击和攻击无关。也许现在已经有黑客利用AI来增加其更一般攻击的效率和强度。未来可能会更加可怕，像自主无人机攻击和威权政权的族群分类已经在地平线上了。
- en: Attacks
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击
- en: Examples of all major types of attacks—confidentiality, integrity, and availability
    attacks (see [Chapter 5](ch05.html#unique_chapter_id_5) for more information)—have
    been published by researchers. Confidentiality attacks involve the exfiltration
    of training data or model logic from AI system endpoints. Integrity attacks include
    adversarial manipulation of training data or model outcomes, either through adversarial
    examples, evasion, impersonation, or poisoning. Availability attacks can be conducted
    through more standard denial-of-service approaches, through sponge examples that
    overuse system resources, or via algorithmic discrimination induced by some adversary
    to deny system services to certain groups of users.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经发布了各种主要类型的攻击示例 —— 保密性、完整性和可用性攻击（有关更多信息，请参见[第 5 章](ch05.html#unique_chapter_id_5)）。保密性攻击涉及从
    AI 系统端点窃取训练数据或模型逻辑。完整性攻击包括通过敌对示例、逃避、冒充或毒化来对训练数据或模型结果进行敌对操作。可用性攻击可以通过更标准的拒绝服务方法、过度使用系统资源的海绵示例，或通过某些对手引起的算法歧视来拒绝向某些用户群体提供系统服务。
- en: Failures
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 失败
- en: AI system failures tend to involve algorithmic discrimination, safety and performance
    lapses, data privacy violations, inadequate transparency, or problems in third-party
    system components.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: AI 系统的故障往往涉及算法歧视、安全和性能失误、数据隐私侵犯、透明度不足或第三方系统组件问题。
- en: 'AI incidents are a reality. And like the systems from which they arise, AI
    incidents can be complex. AI incidents have multiple causes: failures, attacks,
    and abuses. They also tend to blend traditional notions of computer security with
    concerns like data privacy and algorithmic discrimination.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: AI 事件是现实存在的。就像它们产生的系统一样，AI 事件可能非常复杂。AI 事件有多种原因：故障、攻击和滥用。它们还倾向于将传统的计算机安全概念与数据隐私和算法歧视等问题融合在一起。
- en: '![mlha 0102](assets/mlha_0102.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0102](assets/mlha_0102.png)'
- en: Figure 1-2\. A basic taxonomy of AI incidents (adapted from [“What to Do When
    AI Fails"](https://oreil.ly/AHfmK))
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2\. AI 事件的基本分类（改编自[“当 AI 失败时该怎么办”](https://oreil.ly/AHfmK)）
- en: The 2016 [Tay chatbot incident](https://oreil.ly/a-DhB) is an informative example.
    Tay was a state-of-the-art chatbot trained by some of the world’s leading experts
    at Microsoft Research for the purpose of interacting with people on Twitter to
    increase awareness about AI. Sixteen hours after its release—and 96,000 tweets
    later—Tay had spiraled into writing as a neo-Nazi pornographer and had to be shut
    down. What happened? Twitter users quickly learned that Tay’s adaptive learning
    system could easily be poisoned. Racist and sexual content tweeted at the bot
    was incorporated into its training data, and just as quickly resulted in offensive
    output. Data poisoning is an integrity attack, but due to the context in which
    it was carried out, this attack resulted in algorithmic discrimination. It’s also
    important to note that Tay’s designers, being world-class experts at an extremely
    well-funded research center, seemed to have put some guardrails in place. Tay
    would respond to certain hot-button issues with precanned responses. But that
    was not enough, and Tay devolved into a public security and algorithmic discrimination
    incident for Microsoft Research.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 2016 年的[Tay 聊天机器人事件](https://oreil.ly/a-DhB)是一个富有启发性的例子。 Tay 是由微软研究部门的一些全球领先专家为增强人们对
    AI 的认识而设计的最新型聊天机器人。发布仅 16 小时后——在发布 96,000 条推文后——Tay 已经演变成了一个新纳粹色情作家，并被迫关闭。 发生了什么？
    Twitter 用户很快就发现，Tay 的自适应学习系统很容易被毒化。 被发表在机器人上的种族主义和性内容被整合到了其训练数据中，并迅速导致了令人反感的输出。
    数据毒化是一种完整性攻击，但由于其进行的背景，此攻击导致了算法歧视。 还值得注意的是，Tay 的设计者们，作为一家拥有极其充足研究资金的顶尖研究中心的世界级专家，似乎已经设置了一些防范措施。
    Tay 会对某些热门问题做出预先制定的回应。 但这还远远不够，Tay 最终演变成了微软研究部门的公共安全和算法歧视事件。
- en: Think this was a one-off incident? Wrong. Just recently, again due to hype and
    failure to think through performance, safety, privacy, and security risks systematically,
    many of Tay’s most obvious failures were [repeated in Scatter Lab’s release of
    its Lee Luda chatbot](https://oreil.ly/5OLXV). When designing ML systems, plans
    should be compared to past known incidents in hope of preventing future similar
    incidents. This is precisely the point of recent [AI incident database efforts](https://oreil.ly/vvLbp)
    and associated [publications](https://oreil.ly/59yaY).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以为这是个别事件？错了。最近，再次由于炒作和未能系统地考虑性能、安全性、隐私和安全风险，Tay的许多明显失败在Scatter Lab发布的其李露达聊天机器人中[再次重演](https://oreil.ly/5OLXV)。设计ML系统时，计划应与过去已知的事件进行比较，以期预防未来类似事件。这正是最近[AI事件数据库工作](https://oreil.ly/vvLbp)及相关[出版物](https://oreil.ly/59yaY)的目的所在。
- en: AI incidents can also be an apolitical motivator for responsible technology
    development. For better or worse, cultural and political viewpoints on topics
    like algorithmic discrimination and data privacy can vary widely. Getting a team
    to agree on ethical considerations can be very difficult. It might be easier to
    get them working to prevent embarrassing and potentially costly or dangerous incidents,
    which should be a baseline goal of any serious data science team. The notion of
    AI incidents is central to understanding ML safety; a central theme of this chapter’s
    content is cultural competencies and business processes that can be used to prevent
    and mitigate AI incidents. We’ll dig into those mitigants in the next sections
    and take a deep dive into a real incident to close the chapter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: AI事件也可以成为负责任技术开发的非政治激励因素。无论好坏，有关算法歧视和数据隐私等话题的文化和政治观点可能存在广泛差异。让一个团队就伦理考虑达成一致可能非常困难。让他们致力于防止尴尬且潜在成本高昂或危险的事件可能会更容易，这应该是任何认真的数据科学团队的基本目标。AI事件的概念是理解ML安全的核心；本章内容的中心主题是可以用来预防和减轻AI事件的文化能力和业务流程。我们将在接下来的章节深入探讨这些减轻措施，并深入研究一个真实事件以结束本章。
- en: Cultural Competencies for Machine Learning Risk Management
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习风险管理的文化能力
- en: An organization’s culture is an essential aspect of responsible AI. This section
    will discuss cultural competencies like accountability, drinking our own champagne,
    domain expertise, and the stale adage “move fast and break things.”
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 组织文化是负责任AI的重要方面。本节将讨论文化能力，如责任制，自己的香槟，领域专业知识以及陈旧的“快速前进，打破障碍”的格言。
- en: Organizational Accountability
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组织责任制
- en: A key to the successful mitigation of ML risks is real accountability within
    organizations for AI incidents. If no one’s job is at stake when an ML system
    fails, gets attacked, or is abused for nefarious purposes, then it’s entirely
    possible that no one in that organization really cares about ML safety and performance.
    In addition to developers who think through risks, apply software quality assurance
    (QA) techniques, and model debugging methods, organizations need individuals or
    teams who validate ML system technology and audit associated processes. Organizations
    also need someone to be responsible for AI incident response plans. This is why
    leading financial institutions, whose use of predictive modeling has been regulated
    for decades, employ a practice known as model risk management. MRM is patterned
    off the Federal Reserve’s [SR 11-7 model risk management guidance](https://oreil.ly/xpr5P),
    which arose out the of the financial crisis of 2008\. Notably, implementation
    of MRM often involves accountable executives and several teams that are responsible
    for the safety and performance of models and ML systems.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 成功缓解ML风险的关键是组织内对AI事件的真正责任制。如果当ML系统失败、遭到攻击或被滥用以不正当目的时，没有人的工作岗位受到影响，那么很可能这个组织中没有人真正关心ML的安全性和性能。除了考虑风险、应用软件质量保证（QA）技术和模型调试方法的开发人员外，组织还需要有人员或团队验证ML系统技术并审计相关流程。组织还需要有人负责AI事件响应计划。这就是为什么领先的金融机构，在其长期受监管的预测建模使用中，采用一种被称为模型风险管理的实践。MRM模仿了联邦储备的[SR
    11-7模型风险管理指南](https://oreil.ly/xpr5P)，这是在2008年金融危机中出现的。值得注意的是，MRM的实施通常涉及负责任的高级管理人员和数个团队，这些团队负责模型和ML系统的安全性和性能。
- en: 'Implementation of MRM standards usually requires several different teams and
    executive leadership. These are some of the key tenets that form the cultural
    backbone for MRM:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: MRM标准的实施通常需要几个不同的团队和高层领导。以下是构成MRM文化支柱的关键原则之一：
- en: Written policies and procedures
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 书面政策和流程
- en: The organizational rules for making and using ML should be written and available
    for all organizational stakeholders. Those close to ML systems should have trainings
    on the policies and procedures. These rules should also be audited to understand
    when they need to be updated. No one should be able to claim ignorance of the
    rules, the rules should be transparent, and the rules should not change without
    approval. Policies and procedures should include clear mechanisms for escalating
    serious risks or problems to senior management, and likely should put forward
    whistleblower processes and protections.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 制定和使用机器学习的组织规则应该明确并对所有组织利益相关者可获得。接近机器学习系统的人员应接受有关政策和流程的培训。这些规则也应进行审计，以了解何时需要更新。没有人应该声称对规则一无所知，规则应该是透明的，并且不得未经批准更改。政策和流程应包括明确的机制，用于将严重风险或问题上报给高级管理层，并可能应提出举报者流程和保护措施。
- en: Effective challenge
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有效挑战
- en: Effective challenge dictates that experts with the capability to change a system,
    who did not build the ML system being challenged, perform validation and auditing.
    MRM practices typically distribute effective challenge across three “lines of
    defense,” where conscientious system developers make up the first line of defense
    and independent, skilled, and empowered technical validators and process auditors
    make up the second and third lines, respectively.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有效挑战要求具有改变系统能力的专家，这些专家并未构建被挑战的机器学习系统，进行验证和审计。MRM实践通常将有效挑战分布在三个“防线”上，有良心的系统开发者构成第一防线，独立、技术娴熟且授权的技术验证人员和流程审计员分别构成第二和第三防线。
- en: Accountable leadership
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的领导
- en: A specific executive within an organization should be accountable for ensuring
    AI incidents do not happen. This position is often referred to as *chief model
    risk officer* (CMRO). It’s also not uncommon for CMRO terms of employment and
    compensation to be linked to ML system performance. The role of CMRO offers a
    very straightforward cultural check on ML safety and performance. If our boss
    really cares about ML system safety and performance, then we start to care too.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 组织内特定的高管应负责确保AI事件不会发生。这个职位通常被称为*首席模型风险官*（CMRO）。CMRO的任职条件和薪酬往往与机器学习系统的表现挂钩。CMRO角色为机器学习的安全性和性能提供了非常直接的文化检查。如果我们的老板真的关心机器学习系统的安全性和表现，那么我们也会开始关注。
- en: Incentives
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 激励措施
- en: Data science staff and management must be incentivized to implement ML responsibly.
    Often, compressed product timelines can incentivize the creation of a minimum
    viable product first, with rigorous testing and remediation relegated to the end
    of the model lifecycle immediately before deployment to production. Moreover,
    ML testing and validation teams are often evaluated by the same criteria as ML
    development teams, leading to a fundamental misalignment where testers and validators
    are encouraged to move quickly rather than assure quality. Aligning timeline,
    performance evaluation, and pay incentives to team function helps solidify a culture
    of responsible ML and risk mitigation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学人员和管理人员必须被激励以负责任的方式实施机器学习。通常，压缩的产品时间表可能会激励先创建一个最小可行产品，而将严格的测试和修复工作放在模型生命周期的最后阶段，即部署到生产前。此外，机器学习的测试和验证团队通常会根据与机器学习开发团队相同的标准进行评估，导致了测试人员和验证人员被鼓励快速行动而非确保质量的基本不匹配。通过将时间表、绩效评估和支付激励与团队功能对齐，有助于巩固负责任的机器学习文化和风险缓解。
- en: Of course, small or young organizations may not be able to spare an entire full-time
    employee to monitor ML system risk. But it’s important to have an individual or
    group held accountable if ML systems cause incidents and rewarded if the systems
    work well. If an organization assumes that everyone is accountable for ML risk
    and AI incidents, the reality is that no one is accountable.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，小型或年轻组织可能无法分配一个全职员工来监控机器学习系统风险。但是，如果机器学习系统导致事件发生，有一个个人或团队应该负责，并且如果系统运行良好，则应该获得奖励。如果一个组织假设每个人都对机器学习风险和AI事件负责，那么实际上没有人会负责。
- en: Culture of Effective Challenge
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有效挑战文化
- en: Whether our organization is ready to adopt full-blown MRM practices, or not,
    we can still benefit from certain aspects of MRM. In particular, the cultural
    competency of effective challenge can be applied outside of the MRM context. At
    its core, effective challenge means actively challenging and questioning steps
    taken throughout the development of ML systems. An organizational culture that
    encourages serious questioning of ML system designs will be more likely to develop
    effective ML systems or products, and to catch problems before they explode into
    harmful incidents. Note that effective challenge cannot be abusive, and it must
    apply equally to all personnel developing an ML system, especially so-called “rockstar”
    engineers and data scientists. Effective challenge should also be structured,
    such as weekly meetings where current design thinking is questioned and alternative
    design choices are seriously considered.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们的组织是否准备采用成熟的MRM实践，我们仍然可以从MRM的某些方面中受益。特别是，有效挑战的文化能力可以应用于MRM背景之外的环境。在其核心，有效挑战意味着在整个ML系统开发过程中积极质疑和追问所采取的步骤。一个鼓励严肃质疑ML系统设计的组织文化，将更有可能开发出有效的ML系统或产品，并在问题爆发成有害事件之前抓住问题。需要注意的是，有效挑战不能是恶意的，必须对所有开发ML系统的人员平等适用，特别是所谓的“摇滚明星”工程师和数据科学家。有效挑战还应该是有结构的，例如每周会议，在这些会议中当前的设计思路会受到质疑，并且会认真考虑替代的设计选择。
- en: Diverse and Experienced Teams
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多样化和经验丰富的团队
- en: Diverse teams can bring wider and previously uncorrelated perspectives to bear
    on the design, development, and testing of ML systems. Nondiverse teams often
    do not. Many have documented the unfortunate outcomes that can arise as a result
    of data scientists not considering demographic diversity in the training or results
    of ML systems. A potential solution to these kinds of oversights is increasing
    demographic diversity on ML teams from its [current woeful levels](https://oreil.ly/7M9uB).
    Business or other domain experience is also important when building teams. Domain
    experts are instrumental in feature selection and engineering, and in the testing
    of system outputs. In the mad rush to develop ML systems, domain-expert participation
    can also serve as a safety check. Generalist data scientists often lack the experience
    necessary to deal with domain-specific data and results. Misunderstanding the
    meaning of input data or output results is a recipe for disaster that can lead
    to AI incidents when a system is deployed. Unfortunately, when it comes to data
    scientists forgetting or ignoring the importance of domain expertise, the social
    sciences deserve a special emphasis. In a trend referred to as [“tech’s quiet
    colonization of the social sciences”](https://oreil.ly/IcIBi), several organizations
    have pursued regrettable ML projects that seek to [replace decisions that should
    be made by trained social scientists](https://oreil.ly/xI9Jv) or that simply [ignore
    the collective wisdom of social science domain expertise altogether](https://oreil.ly/KvVSv).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 多样化的团队可以为ML系统的设计、开发和测试带来更广泛和以前未关联的视角。而非多样化的团队则经常不能做到这一点。许多人已经记录了数据科学家在培训或ML系统的结果中未考虑人口统计多样性可能导致的不幸后果。解决这类疏忽的一个潜在方案是增加ML团队中的人口统计多样性，从其[当前令人遗憾的水平](https://oreil.ly/7M9uB)。在构建团队时，业务或其他领域的经验也很重要。领域专家在特征选择和工程以及系统输出的测试中起着关键作用。在开发ML系统的疯狂竞争中，领域专家的参与也可以作为安全检查。广义的数据科学家通常缺乏处理特定领域数据和结果所需的经验。误解输入数据或输出结果的含义是一个灾难的配方，可能导致在系统部署时发生AI事故。不幸的是，当涉及到数据科学家忽视或忽略领域专业知识重要性时，社会科学就应该特别强调。在被称为[“科技对社会科学的悄然殖民”](https://oreil.ly/IcIBi)的趋势中，一些组织已经进行了令人遗憾的ML项目，试图[取代应由训练有素的社会科学家做出的决策](https://oreil.ly/xI9Jv)，或者简单地[完全忽视社会科学领域专业知识的集体智慧](https://oreil.ly/KvVSv)。
- en: Drinking Our Own Champagne
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亲自体验
- en: Also known as “eating our own dog food,” the practice of drinking our own champagne
    refers to using our own software or products inside of our own organization. Often
    a form of prealpha or prebeta testing, drinking our own champagne can identify
    problems that emerge from the complexity of in vivo deployment environments before
    bugs and failures affect customers, users, or the general public. Because serious
    issues like concept drift, algorithmic discrimination, shortcut learning, and
    underspecification are notoriously difficult to identify using standard ML development
    processes, drinking our own champagne provides a limited and controlled, but also
    realistic, test bed for ML systems. Of course, when organizations employ demographically
    and professionally diverse teams and include domain experts in the field where
    the ML system will be deployed, drinking our own champagne is more likely to catch
    a wide variety of problems. Drinking our own champagne also brings the classical
    Golden Rule into AI. If we’re not comfortable using a system on ourselves or in
    our own organization, then we probably shouldn’t deploy that system.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 也被称为“吃自己的狗食”，喝我们自己的香槟的做法指的是在我们自己的组织内使用我们自己的软件或产品。通常是一种预阿尔法或预贝塔测试的形式，喝我们自己的香槟可以在实际部署环境的复杂性中发现问题，这些问题在影响客户、用户或公众之前可以识别出来。由于像概念漂移、算法歧视、快捷学习和未充分规范化等严重问题在标准机器学习开发过程中很难识别，喝我们自己的香槟为机器学习系统提供了一个有限而受控的，但也是现实的测试平台。当组织雇佣了在机器学习系统部署领域具有不同人口统计和专业背景的团队，并且包括领域专家时，喝我们自己的香槟更有可能发现各种问题。喝我们自己的香槟还将经典的黄金法则引入到人工智能中。如果我们不愿意在自己身上或我们自己的组织中使用某个系统，那么我们可能不应该部署该系统。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'One important aspect to consider about deployment environments is the impact
    of our ML systems on ecosystems and the planet—for example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑部署环境的一个重要方面是我们的机器学习系统对生态系统和地球的影响，例如：
- en: The carbon footprint of ML models
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型的碳足迹
- en: The possibility that an ML system could damage the environment by causing an
    AI incident
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习系统可能通过引发 AI 事件对环境造成损害的可能性
- en: If we’re worried about the environmental impacts of our model, we should loop
    in ML governance with broader environmental, social, and governance efforts at
    our organization.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们担心我们模型的环境影响，我们应该将机器学习治理与我们组织的更广泛的环境、社会和治理工作联系起来。
- en: Moving Fast and Breaking Things
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速前进与破坏事物
- en: The mantra “move fast and break things” is almost a religious belief for many
    “rock-star” engineers and data scientists. Sadly, these top practitioners also
    seem to forget that when they go fast and break things, things get broken. As
    ML systems make more high-impact decisions that implicate autonomous vehicles,
    credit, employment, university grades and attendance, medical diagnoses and resource
    allocation, mortgages, pretrial bail, parole, and more, breaking things means
    more than buggy apps. It can mean that a small group of data scientists and engineers
    causes real harm at scale to many people. Participating in the design and implementation
    of high-impact ML systems requires a mindset change to prevent egregious performance
    and safety problems. Practitioners must change from prioritizing the number of
    software features they can push, or the test data accuracy of an ML model, to
    recognizing the implications and downstream risks of their work.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: “快速前进与破坏事物”这句口号对许多“摇滚明星”工程师和数据科学家几乎是一种宗教信仰。不幸的是，这些顶尖从业者似乎也忘记了，当他们快速前进并破坏事物时，事物就真的会被破坏。随着机器学习系统做出更多涉及自动驾驶车辆、信用、就业、大学成绩和考勤、医疗诊断和资源分配、抵押贷款、预审保释、等等高影响决策，破坏事物意味着不仅仅是有
    bug 的应用程序。它可能意味着一个小组数据科学家和工程师在规模上对许多人造成了实际的伤害。参与高影响机器学习系统的设计和实施需要改变思维方式，以防止严重的性能和安全问题。从优先考虑能够推动的软件功能数量或
    ML 模型的测试数据准确性，转向认识到他们工作的影响和下游风险，这是从业者必须进行的转变。
- en: Organizational Processes for Machine Learning Risk Management
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习风险管理的组织流程
- en: Organizational processes play a key role in ensuring that ML systems are safe
    and performant. Like the cultural competencies discussed in the previous section,
    organizational processes are a key nontechnical determinant of reliability in
    ML systems. This section on processes starts out by urging practitioners to consider,
    document, and attempt to mitigate any known or foreseeable failure modes for their
    ML systems. We then discuss more about MRM. While [“Cultural Competencies for
    Machine Learning Risk Management”](#cultural_competencies_ch01_1680867483647)
    focused on the people and mindsets necessary to make MRM a success, this section
    will outline the different processes MRM uses to mitigate risks in advanced predictive
    modeling and ML systems. While MRM is a worthy process standard to which we can
    all aspire, there are additional important process controls that are not typically
    part of MRM. We’ll look beyond traditional MRM in this section and highlight crucial
    risk control processes like pair or double programming and security permission
    requirements for code deployment. This section will close with a discussion of
    AI incident response. No matter how hard we work to minimize harms while designing
    and implementing an ML system, we still have to prepare for failures and attacks.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 组织流程在确保机器学习系统安全和性能方面起着关键作用。与前一节讨论的文化能力一样，组织流程是机器学习系统可靠性的关键非技术决定因素。本节关于流程的内容首先督促从业者考虑、记录并尝试减轻其机器学习系统中已知或可预见的故障模式。然后我们进一步讨论关于MRM的更多内容。虽然[“机器学习风险管理的文化能力”](#cultural_competencies_ch01_1680867483647)侧重于使MRM取得成功所必需的人和思维方式，本节将概述MRM在高级预测建模和机器学习系统中用于减轻风险的不同流程。虽然MRM是我们可以共同努力追求的一个值得的流程标准，但还有一些重要的流程控制不典型地包含在MRM中。我们将在本节超越传统的MRM，并突出像对编程或双人编程以及代码部署安全权限要求等关键风险控制流程。本节将以讨论AI事故响应结束。无论我们在设计和实施机器学习系统时多么努力地减少伤害，我们仍然必须准备应对失败和攻击。
- en: Forecasting Failure Modes
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测故障模式
- en: ML safety and ethics experts roughly agree on the importance of thinking through,
    documenting, and attempting to mitigate foreseeable failure modes for ML systems.
    Unfortunately, they also mostly agree that this is a nontrivial task. Happily,
    new resources and scholarship on this topic have emerged in recent years that
    can help ML system designers forecast incidents in more systematic ways. If holistic
    categories of potential failures can be identified, it makes hardening ML systems
    for better real-world performance and safety a more proactive and efficient task.
    In this subsection, we’ll discuss one such strategy, along with a few additional
    processes for brainstorming future incidents in ML systems.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习安全和伦理专家大致同意需要思考、记录并尝试减轻机器学习系统可预见故障模式的重要性。不幸的是，他们也大多同意这是一项非平凡的任务。值得庆幸的是，近年来在这一主题上出现了新的资源和学术研究，可以帮助机器学习系统设计者以更系统化的方式预测事件。如果可以确定潜在故障的整体类别，那么加强机器学习系统以获得更好的现实世界表现和安全性将成为一项更为积极和高效的任务。在本小节中，我们将讨论一种这样的策略，以及几种用于思考未来机器学习系统中事件的额外流程。
- en: Known past failures
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 已知的过去失败
- en: 'As discussed in [“Preventing Repeated Real World AI Failures by Cataloging
    Incidents: The AI Incident Database”](https://oreil.ly/BfMJC), one the most efficient
    ways to mitigate potential AI incidents in our ML systems is to compare our system
    design to past failed designs. Much like transportation professionals investigating
    and cataloging incidents, then using the findings to prevent related incidents
    and test new technologies, several ML researchers, commentators, and trade organizations
    have begun to collect and analyze AI incidents in hopes of preventing repeated
    and related failures. Likely the most high-profile and mature AI incident repository
    is the [AI Incident Database](https://oreil.ly/H8nmd). This searchable and interactive
    resource allows registered users to search a visual database with keywords and
    locate different types of information about publicly recorded incidents.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[“通过分类事件预防重复的现实世界AI故障：AI事故数据库”](https://oreil.ly/BfMJC)中所讨论的，我们在机器学习系统中减轻潜在AI事故的最有效方法之一是将我们的系统设计与过去的失败设计进行比较。就像交通专业人员调查和分类事件一样，然后利用所得结论预防相关事件并测试新技术，几位机器学习研究人员、评论员和行业组织已开始收集和分析AI事故，以期防止重复和相关的失败。最显著和成熟的AI事故库可能是[AI事故数据库](https://oreil.ly/H8nmd)。这一可搜索和互动资源允许注册用户使用关键字搜索视觉数据库，并查找有关公开记录事件的不同类型信息。
- en: Consult this resource while developing ML systems. If a system similar to the
    one we’re currently designing, implementing, or deploying has caused an incident
    in the past, this is one of strongest indicators that our new system could cause
    an incident. If we see something that looks familiar in the database, we should
    stop and think about what we’re doing a lot more carefully.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发ML系统时请参考此资源。如果类似于我们当前设计、实施或部署的系统在过去曾经引发过事件，这是我们新系统可能会引发事件的最强指标之一。如果在数据库中看到熟悉的东西，我们应该停下来，仔细思考我们正在做的事情。
- en: Failures of imagination
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想象力的失败
- en: 'Imagining the future with context and detail is never easy. And it’s often
    the context in which ML systems operate, accompanied by unforeseen or unknowable
    details, that lead to AI incidents. In a recent workshop paper, the authors of
    [“Overcoming Failures of Imagination in AI Infused System Development and Deployment”](https://oreil.ly/veB5T)
    put forward some structured approaches to hypothesize about those hard-to-imagine
    future risks. In addition to deliberating on the *who* (e.g., investors, customers,
    vulnerable nonusers), *what* (e.g., well-being, opportunities, dignity), *when*
    (e.g., immediately, frequently, over long periods of time), and *how* (e.g., taking
    an action, altering beliefs) of AI incidents, they also urge system designers
    to consider the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有详细背景和细节的情况下想象未来从来不容易。通常是机器学习系统运行的背景，加上意想不到或无法预知的细节，导致AI事件的发生。在最近的一篇研讨会论文中，《“克服AI注入系统开发和部署中的想象失败”》的作者提出了一些结构化方法来假设这些难以想象的未来风险。除了深思熟虑AI事件的“谁”（例如投资者、客户、脆弱的非用户）、“什么”（例如福祉、机会、尊严）、“何时”（例如立即、频繁地、长时间内）、“如何”（例如采取行动、改变信念）之外，他们还敦促系统设计者考虑以下内容：
- en: Assumptions that the impact of the system will be only beneficial (and to admit
    when uncertainty in system impacts exists)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设系统的影响只会是有益的（并承认系统影响存在不确定性的时候）
- en: The problem domain and applied use cases of the system, as opposed to just the
    math and technology
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统的问题域和应用用例，而不仅仅是数学和技术
- en: Any unexpected or surprising results, user interactions, and responses to the
    system
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何意外或令人惊讶的结果，用户互动以及对系统的响应
- en: Causing AI incidents is embarrassing, if not costly or illegal, for organizations.
    AI incidents can also hurt consumers and the general public. Yet, with some foresight,
    many of the currently known AI incidents could have been mitigated, if not wholly
    avoided. It’s also possible that in performing the due diligence of researching
    and conceptualizing ML failures, we find that our design or system must be completely
    reworked. If this is the case, take comfort that a delay in system implementation
    or deployment is likely less costly than the harms our organization or the public
    could experience if the flawed system was released.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 引起AI事件对组织来说是令人尴尬的，如果不是成本高昂或违法的话。AI事件还可能伤害消费者和公众利益。然而，通过一些远见，目前已知的许多AI事件本可以得到缓解，甚至完全避免。在进行研究和概念化ML失败的尽职调查时，我们还可能发现我们的设计或系统必须完全重做。如果情况如此，可以安慰自己，系统实施或部署的延迟可能比我们的组织或公众由于发布有缺陷的系统可能遭受的损害要少。
- en: Model Risk Management Processes
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型风险管理流程
- en: The process aspects of MRM mandate thorough documentation of modeling systems,
    human review of systems, and ongoing monitoring of systems. These processes represent
    the bulk of the governance burden for the Federal Reserve’s SR 11-7 MRM guidance,
    which is overseen by the Federal Reserve and the Office of the Comptroller of
    the Currency for predictive models deployed in material consumer finance applications.
    While only large organizations will be able to fully embrace all that MRM has
    to offer, any serious ML practitioner can learn something from the discipline.
    The following section breaks MRM processes down into smaller components so that
    readers can start thinking through using aspects of MRM in their organization.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: MRM流程的过程方面要求对建模系统进行彻底的文档记录，对系统进行人工审核，并对系统进行持续监控。这些流程代表了美联储SR 11-7 MRM指导方针的大部分治理负担，由美联储和美国货币监理署监督，用于在重要消费金融应用中部署的预测模型。虽然只有大型组织才能完全接受MRM所提供的所有内容，但任何认真的机器学习从业者都可以从这一学科中学到一些东西。以下部分将MRM流程细分为更小的组成部分，以便读者可以开始考虑在其组织中使用MRM的方面。
- en: Risk tiering
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 风险分级
- en: As outlined in the opening of this chapter, the product of the probability of
    a harm occurring and the likely loss resulting from that harm is an accepted way
    to rate the risk of a given ML system deployment. The product of risk and loss
    has a more formal name in the context of MRM, *materiality*. Materiality is a
    powerful concept that enables organizations to assign realistic risk levels to
    ML systems. More importantly, this risk-tiering allows for the efficient allocation
    of limited development, validation, and audit resources. Of course, the highest
    materiality applications should receive the greatest human attention and review,
    while the lowest materiality applications could potentially be handled by automatic
    machine learning (AutoML) systems and undergo minimal validation. Because risk
    mitigation for ML systems is an ongoing, expensive task, proper resource allocation
    between high-, medium-, and low-risk systems is a must for effective governance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章开头所述，某个ML系统部署的风险是可能发生的危害的概率和由此导致的可能损失的乘积的结果。在MRM的背景下，风险和损失的乘积有一个更正式的名字，*物质性*。物质性是一个强大的概念，使组织能够为ML系统分配现实的风险级别。更重要的是，这种风险分层允许有效分配有限的开发、验证和审计资源。当然，最高物质性应用程序应该得到最大的人工关注和审查，而最低物质性应用程序则有可能由自动机器学习（AutoML）系统处理，并进行最少的验证。因为ML系统的风险缓解是一个持续且昂贵的任务，所以在高、中、低风险系统之间进行适当的资源分配是有效治理的必需。
- en: Model documentation
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型文档
- en: 'MRM standards also require that systems be thoroughly documented. First, documentation
    should enable accountability for system stakeholders, ongoing system maintenance,
    and a degree of incident response. Second, documentation must be standardized
    across systems for the most efficient audit and review processes. Documentation
    is where the rubber hits the road for compliance. Documentation templates, illustrated
    at a very high level by the following section list, are documents that data scientists
    and engineers fill in as they move through a standardized workflow or in the later
    stages of model development. Documentation templates should include all the steps
    that a responsible practitioner should conduct to build a sound model. If parts
    of the document aren’t filled out, that points to sloppiness in the training process.
    Since most documentation templates and frameworks also call for adding one’s name
    and contact information to the finished model document, there should be no mystery
    about who is not pulling their weight. For reference, the following section list
    is a rough combination of typical sections in MRM documentation and the sections
    recommended by the [Annexes to the EU Artificial Intelligence Act](https://oreil.ly/p_Cqt):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: MRM标准还要求系统进行彻底的文档化。首先，文档化应该能够为系统利益相关者、持续的系统维护以及一定程度的事件响应负责。其次，文档化必须在各系统之间标准化，以实现最高效的审计和评审流程。文档化是确保合规性的关键所在。文档模板，如下一节列表所示，是数据科学家和工程师在标准化工作流程的后期阶段填写的文档，也是大多数文档模板和框架要求在最终模型文档中添加姓名和联系信息的地方，这样就清楚知道谁没有尽力。作为参考，以下章节列表粗略结合了MRM文档和欧盟《人工智能法案附件》推荐的典型部分：
- en: Basic Information
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本信息
- en: Names of Developers and Stakeholders
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发人员和利益相关者的名称
- en: Current Date and Revision Table
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前日期和修订表
- en: Summary of Model System
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型系统概要
- en: Business or Value Justification
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务或价值论证
- en: Intended Uses and Users
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预期用途及用户
- en: Potential Harms and Ethical Considerations
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在危害和伦理考虑
- en: Development Data Information
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发数据信息
- en: Source for Development Data
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发数据来源
- en: Data Dictionary
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据字典
- en: Privacy Impact Assessment
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私影响评估
- en: Assumptions and Limitations
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设与限制
- en: Software Implementation for Data Preprocessing
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理软件实施
- en: Model Information
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型信息
- en: Description of Training Algorithm with Peer-Reviewed References
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述经过同行评审的训练算法
- en: Specification of Model
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型规范
- en: Performance Quality
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能质量
- en: Assumptions and Limitations
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设与限制
- en: Software Implementation for Training Algorithm
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练算法的软件实施
- en: Testing Information
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试信息
- en: Quality Testing and Remediation
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质量测试与整改
- en: Discrimination Testing and Remediation
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 歧视测试与整改
- en: Security Testing and Remediation
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全测试与整改
- en: Assumptions and Limitations
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设与限制
- en: Software Implementation for Testing
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于测试的软件实施
- en: Deployment Information
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署信息
- en: Monitoring Plans and Mechanisms
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控计划和机制
- en: Up- and Downstream Dependencies
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上游和下游依赖关系
- en: Appeal and Override Plans and Mechanisms
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 申诉和覆盖计划和机制
- en: Audit Plans and Mechanisms
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审计计划和机制
- en: Change Management Plans
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变更管理计划
- en: Incident Response Plans
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件响应计划
- en: References (if we’re doing science, then we’re building on the shoulders of
    giants and we’ll have several peer-reviewed references in a formatted bibliography!)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考文献（如果我们正在进行科学研究，那么我们将在格式化的参考文献中建立在巨人的肩膀上！）
- en: Of course, these documents can be hundreds of pages long, especially for high-materiality
    systems. The proposed [datasheet](https://oreil.ly/mjKjy) and [model card](https://oreil.ly/DmMp4)
    standards may also be helpful for smaller or younger organizations to meet these
    goals. If readers are feeling like lengthy model documentation sounds impossible
    for their organization today, then maybe these two simpler frameworks might work
    instead.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些文件可能会有数百页之长，特别是对于重要性高的系统。对于较小或较年轻的组织，提议的[数据表](https://oreil.ly/mjKjy)和[模型卡](https://oreil.ly/DmMp4)标准也可能有所帮助，以实现这些目标。如果读者觉得为其组织撰写冗长的模型文档目前看起来不可行，那么也许这两种更简单的框架可能会起到作用。
- en: Model monitoring
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型监控
- en: A primary tenant of ML safety is that ML system performance in the real world
    is hard to predict and, accordingly, performance must be monitored. Hence, deployed-system
    performance should be monitored frequently and until a system is decommissioned.
    Systems can be monitored for any number of problematic conditions, the most common
    being input drift. While ML system training data encodes information about a system’s
    operating environment in a static snapshot, the world is anything but static.
    Competitors can enter markets, new regulations can be promulgated, consumer tastes
    can change, and pandemics or other disasters can happen. Any of these can change
    the live data that’s entering our ML system away from the characteristics of its
    training data, resulting in decreased, or even dangerous, system performance.
    To avoid such unpleasant surprises, the best ML systems are monitored both for
    drifting input and output distributions and for decaying quality, often known
    as *model decay*. While performance quality is the most common quantity to monitor,
    ML systems can also be monitored for anomalous inputs or predictions, specific
    attacks and hacks, and for drifting fairness characteristics.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习安全的一个主要租户是，机器学习系统在现实世界中的表现很难预测，因此必须进行监控。因此，部署系统的性能应经常进行监控，直到系统被废弃为止。系统可以监控任何数量的问题条件，最常见的是输入漂移。虽然机器学习系统的训练数据在静态快照中编码了有关系统操作环境的信息，但世界绝非静态。竞争对手可以进入市场，可以颁布新的法规，消费者口味可能会改变，以及可能会发生流行病或其他灾难。这些因素中的任何一个都可能使输入到我们的机器学习系统的实时数据偏离其训练数据的特征，从而导致性能下降甚至危险。为了避免这种不愉快的惊喜，最好的机器学习系统都会监控输入和输出分布的漂移以及质量下降，通常被称为*模型衰减*。虽然性能质量是最常见的监控指标，但机器学习系统也可以监控异常输入或预测、特定攻击和黑客，以及公平性特征的漂移。
- en: Model inventories
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型清单
- en: 'Any organization that is deploying ML should be able to answer straightforward
    questions like:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 任何部署机器学习的组织都应该能够回答简单的问题，比如：
- en: How many ML systems are currently deployed?
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前部署了多少个机器学习系统？
- en: How many customers or users do these systems affect?
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些系统影响了多少客户或用户？
- en: Who are the accountable stakeholders for each system?
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个系统的负责利益相关者是谁？
- en: MRM achieves this goal through the use of model inventories. A model inventory
    is a curated and up-to-date database of all an organization’s ML systems. Model
    inventories can serve as a repository for crucial information for documentation,
    but should also link to monitoring plans and results, auditing plans and results,
    important past and upcoming system maintenance and changes, and plans for incident
    response.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: MRM通过使用模型清单实现了这一目标。模型清单是组织所有机器学习系统的精心策划和最新数据库。模型清单可以作为关键信息文档的存储库，但也应链接到监控计划和结果、审计计划和结果、重要的过去和即将进行的系统维护和变更，以及事件响应计划。
- en: System validation and process auditing
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 系统验证和流程审核
- en: Under traditional MRM practices, an ML system undergoes two primary reviews
    before its release. The first review is a technical validation of the system,
    where skilled validators, not uncommonly PhD-level data scientists, attempt to
    poke holes in system design and implementation, and work with system developers
    to fix any discovered problems. The second review investigates processes. Audit
    and compliance personnel carefully analyze the system design, development, and
    deployment, along with documentation and future plans, to ensure all regulatory
    and internal process requirements are met. Moreover, because ML systems change
    and drift over time, review must take place whenever a system undergoes a major
    update or at an agreed upon future cadence.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的MRM实践下，机器学习系统在发布之前会经历两次主要审查。第一次审查是对系统进行技术验证，由熟练的验证人员（通常是博士级别的数据科学家）尝试揭示系统设计和实施中的问题，并与系统开发人员合作解决发现的问题。第二次审查涉及流程。审计和合规人员仔细分析系统设计、开发和部署情况，以及文档和未来计划，确保满足所有法规和内部流程要求。此外，由于机器学习系统会随时间变化和漂移，审查必须在系统经历重大更新或在约定的未来周期时进行。
- en: Readers may be thinking (again) that their organization doesn’t have the resources
    for such extensive reviews. Of course that is a reality for many small or younger
    organizations. The keys for validation and auditing, that should work at nearly
    any organization, are having technicians who did not develop the system test it,
    having a function to review nontechnical internal and external obligations, and
    having sign-off oversight for important ML system deployments.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可能又在想，他们的组织没有足够的资源进行如此广泛的审查。当然，对许多小型或年轻的组织来说，这是现实。对于几乎任何组织来说，验证和审计的关键在于拥有未开发系统的技术人员进行测试，有一个审查非技术内部和外部义务的功能，以及对重要的机器学习系统部署进行签字监督。
- en: Change management
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变更管理
- en: Like all complex software applications, ML systems tend to have a large number
    of different components. From backend ML code, to application programming interfaces
    (APIs), to graphical user interfaces (GUIs), changes in any component of the system
    can cause side effects in other components. Add in issues like data drift, emergent
    data privacy and anti-discrimination regulations, and complex dependencies on
    third-party software, and change management in ML systems becomes a serious concern.
    If we’re in the planning or design phase of a mission-critical ML system, we’ll
    likely need to make change management a first-class process control. Without explicit
    planning and resources for change management, process or technical mistakes that
    arise through the evolution of the system, like using data without consent or
    API mismatches, are very difficult to prevent. Furthermore, without change management,
    such problems might not even be detected until they cause an incident.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 像所有复杂的软件应用程序一样，机器学习系统往往包含大量不同的组件。从后端机器学习代码，到应用程序编程接口（API），再到图形用户界面（GUI），系统中任何组件的更改都可能引起其他组件的副作用。加入数据漂移、新兴数据隐私和反歧视法规等问题，以及对第三方软件的复杂依赖，使得机器学习系统的变更管理成为一个严重问题。如果我们处于任务关键的机器学习系统的规划或设计阶段，我们可能需要将变更管理作为一流的流程控制。如果没有明确的变更管理计划和资源，系统演变过程中可能出现的流程或技术错误，例如未经同意使用数据或API不匹配，将非常难以防止。此外，没有变更管理，这样的问题甚至可能在引发事件之前都不被察觉。
- en: We’ll circle back to MRM throughout the book. It’s one the most battle-tested
    frameworks for governance and risk management of ML systems. Of course, MRM is
    not the only place to draw inspiration for improved ML safety and performance
    processes, and the next subsection will draw out lessons from other practice areas.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中我们将围绕MRM进行讨论。它是治理和管理机器学习系统中最经受考验的框架之一。当然，MRM并非唯一可以从中获取启发来改进机器学习安全性和性能流程的地方，接下来的小节将从其他实践领域中提取经验教训。
- en: Note
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Reading the 21-page [SR 11-7 model risk management guidance](https://oreil.ly/0By87)
    is a quick way to up-skill yourself in ML risk management. When reading it, pay
    special attention to the focus on cultural and organizational structures. Managing
    technology risks is often more about people than anything else.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读21页的[SR 11-7模型风险管理指南](https://oreil.ly/0By87)是提升自己在机器学习风险管理方面技能的快速途径。阅读时特别注意文化和组织结构的重点。管理技术风险往往更多关乎人而非其他任何事物。
- en: Beyond Model Risk Management
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越模型风险管理
- en: 'There are many ML risk management lessons to be learned from financial audit,
    data privacy, and software development best practices and from IT security. This
    subsection will shine a light on ideas that exist outside the purview of traditional
    MRM: model audits, impact assessments, appeals, overrides, opt outs, pair or double
    programming, least privilege, bug bounties, and incident response, all from an
    ML safety and performance perspective.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 从财务审计、数据隐私和软件开发的最佳实践以及IT安全中，可以从许多ML风险管理教训中汲取经验。本小节将重点介绍传统MRM范围之外的想法：模型审计、影响评估、申诉、覆盖、选择退出、配对或双人编程、最小权限、漏洞赏金以及事件响应，所有这些都从ML安全性和性能角度来看。
- en: Model audits and assessments
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型审计和评估
- en: 'Audit is a common term in MRM, but it also has meanings beyond what it is typically
    known as—the third line of defense in a more traditional MRM scenario. The phrase
    *model audit* has come to prominence in recent years. A model audit is an official
    testing and transparency exercise focusing on an ML system that tracks adherence
    to some policy, regulation, or law. Model audits tend to be conducted by independent
    third parties with limited interaction between auditor and auditee organizations.
    For a good breakdown of model audits, check out the recent paper [“Algorithmic
    Bias and Risk Assessments: Lessons from Practice”](https://oreil.ly/eHxBb). The
    paper [“Closing the AI Accountability Gap: Defining an End-to-End Framework for
    Internal Algorithmic Auditing”](https://oreil.ly/vO9cH) puts forward a solid framework
    for audits and assessments, even including worked documentation examples. The
    related term, *model assessment*, seems to mean a more informal and cooperative
    testing and transparency exercise that may be undertaken by external or internal
    groups.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 审计是MRM中常见的术语，但它的含义远不止它通常被认为的在更传统的MRM场景中的第三道防线。短语*模型审计*近年来变得日益突出。模型审计是一种官方的测试和透明度行使，重点关注跟踪某些政策、法规或法律的ML系统的遵从情况。模型审计通常由独立的第三方进行，审计人员与被审计组织的互动有限。关于模型审计的详细介绍，请查阅最近的论文[“算法偏见和风险评估：实践经验教训”](https://oreil.ly/eHxBb)。论文[“弥合AI责任空白：定义内部算法审计的端到端框架”](https://oreil.ly/vO9cH)提出了一个坚实的审计和评估框架，甚至包括了工作文档示例。相关术语*模型评估*似乎指的是一种更为非正式和合作的测试和透明度行使，可以由外部或内部团体进行。
- en: ML audits and assessments may focus on bias issues or other serious risks including
    safety, data privacy harms, and security vulnerabilities. Whatever their focus,
    audits and auditors have be fair and transparent. Those conducting audits should
    be held to clear ethical or professional standards, which barely exist as of 2023\.
    Without these kinds of accountability mechanisms or binding guidelines, audits
    can be an ineffective risk management practice, and worse, tech-washing exercises
    that certify harmful ML systems. Despite flaws, audits are an en vogue favorite
    risk control tactic of policy-makers and researchers, and are being written into
    laws—for example, the aforementioned New York City Local Law 144.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ML 审计和评估可能侧重于偏见问题或其他严重风险，包括安全性、数据隐私危害和安全漏洞。无论它们的关注点在哪里，审计和审计人员都必须公正和透明。进行审计的人应当遵守明确的伦理或专业标准，截至2023年几乎不存在这样的标准。如果没有这些问责机制或约束性指南，审计可能会成为一种无效的风险管理实践，甚至是技术洗白的行为，用以认证有害的ML系统。尽管存在缺陷，审计是政策制定者和研究人员最受欢迎的风险控制策略之一，并被写入法律中——例如前述的纽约市地方法144号。
- en: Impact assessments
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 影响评估
- en: Impact assessments are a formal documentation approach used in many fields to
    forecast and record the potential issues a system could cause once implemented.
    Likely due to their use in [data privacy](https://oreil.ly/1OdKa), impact assessments
    are starting to show up in organizational ML policies and [proposed laws](https://oreil.ly/waTek).
    Impact assessments are an effective way to think through and document the harms
    that an ML system could cause, increasing accountability for designers and operators
    of AI systems. But impact assessments are not enough on their own. Remembering
    the definition of risk and materiality previously put forward, impact is just
    one factor in risk. Impacts must be combined with likelihoods to form a risk measure,
    then risks must be actively mitigated, where the highest-risk applications are
    accorded the most oversight. Impact assessments are just the beginning of a broader
    risk management process. Like other risk management processes, they must be performed
    at a cadence that aligns to the system being assessed. If a system changes quickly,
    it will need more frequent impact assessments. Another potential issue with impact
    assessments is caused when they are designed and implemented by the ML teams that
    are also being assessed. In this case, there will be a temptation to diminish
    the scope of the assessment and downplay any potential negative impacts. Impact
    assessments are an important part of broader risk management and governance strategies,
    but they must be conducted as often as required by a specific system, and likely
    conducted by independent oversight professionals.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 影响评估是许多领域中用于预测和记录系统一旦实施可能引起的潜在问题的正式文档方法。由于它们在数据隐私方面的使用，影响评估开始出现在组织的机器学习政策和[提议的法律](https://oreil.ly/waTek)中。影响评估是一种有效的方式来思考和记录机器学习系统可能造成的危害，增加了AI系统设计者和运营者的责任感。但影响评估单靠自身是不够的。在记住之前提出的风险和重要性定义的基础上，影响只是风险的一个因素。必须将影响与可能性结合起来形成风险度量，然后必须积极地减轻风险，其中风险最高的应用程序需要最多的监督。影响评估只是更广泛的风险管理过程的开始。像其他风险管理过程一样，它们必须以适应系统评估的节奏进行。如果一个系统变化迅速，那么它将需要更频繁的影响评估。影响评估的另一个潜在问题是当它们由也正在被评估的机器学习团队设计和实施时。在这种情况下，会有诱惑力减少评估的范围并淡化任何潜在的负面影响。影响评估是更广泛的风险管理和治理战略的重要组成部分，但必须根据特定系统的需求进行频繁进行，并可能由独立的监督专业人员进行。
- en: Appeal, override, and opt out
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 申诉、覆盖和选择退出
- en: 'Ways for users or operators to appeal and override inevitable wrong decisions
    should be built into most ML systems. It’s known by many names across disciplines:
    actionable recourse, intervenability, redress, or adverse action notices. This
    can be as simple as the “Report inappropriate predictions” function in the Google
    search bar, or it can be as sophisticated as presenting data and explanations
    to users and enabling appeal processes for demonstrably wrong data points or decision
    mechanisms. Another similar approach, known as opt out, is to let users do business
    with an organization the old-fashioned way without going through any automated
    processing. Many data privacy laws and major US consumer finance laws address
    recourse, opt out, or both. Automatically forcing wrong decisions on many users
    is one of the clearest ethical wrongs in ML. We shouldn’t fall into an ethical,
    legal, and reputational trap that’s so clear and so well-known, but many systems
    do. That’s likely because it takes planning and resources for both processes and
    technology, laid out from the beginning of designing an ML system, to get appeal,
    override, and opt out right.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 用户或运营商申诉和覆盖大多数机器学习系统中不可避免的错误决策的方式应该内建进去。在不同学科中它有许多名称：可行的追索权、干预性、救济或不良行为通知。这可以简单到谷歌搜索栏的“举报不当预测”功能，或者复杂到向用户呈现数据和解释，并允许对明显错误的数据点或决策机制提出申诉。另一种类似的方法是选择退出，让用户与组织以传统方式进行业务，无需经过任何自动化处理。许多数据隐私法律和主要的美国消费者金融法律涉及追索权、选择退出或两者兼而有之。自动强加错误决策于许多用户是机器学习中最明显的伦理错误之一。我们不应该陷入一个如此明显且众所周知的伦理、法律和声誉陷阱中，但许多系统确实如此。这很可能是因为要使申诉、覆盖和选择退出得当，需要从设计机器学习系统的一开始就进行规划和资源投入。
- en: Pair and double programming
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '-   配对和双重编程'
- en: 'Because they tend to be complex and stochastic, it’s hard to know if any given
    ML algorithm implementation is correct! This is why some leading ML organizations
    implement ML algorithms twice as a QA mechanism. Such double implementation is
    usually achieved by one of two methods: pair programming or double programming.
    In the pair programming approach, two technical experts code an algorithm without
    collaborating. Then they join forces and work out any discrepancies between their
    implementations. In double programming, the same practitioner implements the same
    algorithm twice, but in very different programming languages, such as Python (object-oriented)
    and SAS (procedural). They must then reconcile any differences between their two
    implementations. Either approach tends to catch numerous bugs that would otherwise
    go unnoticed until the system was deployed. Pair and double programming can also
    align with the more standard workflow of data scientists prototyping algorithms,
    while dedicated engineers harden them for deployment. However, for this to work,
    engineers must be free to challenge and test data science prototypes and should
    not be relegated to simply recoding prototypes.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们倾向于复杂且随机，很难确定任何给定的机器学习算法实现是否正确！这就是为什么一些领先的机器学习组织将机器学习算法实现两次作为质量保证机制的原因。这种双重实现通常通过以下两种方法之一来实现：配对编程或双重编程。在配对编程方法中，两位技术专家独立编写算法。然后他们联合起来解决他们实现之间的任何差异。在双重编程中，同一实践者在非常不同的编程语言中两次实现相同的算法，例如Python（面向对象）和SAS（过程化）。然后他们必须调和他们两个实现之间的任何差异。任何一种方法都倾向于捕捉到许多在系统部署之前否则会被忽视的错误。配对和双重编程也可以与数据科学家原型算法的更标准的工作流程保持一致，同时专业工程师为其进行部署。但是，为了使这一切发挥作用，工程师必须自由地挑战和测试数据科学原型，而不应仅仅重新编码原型。
- en: Security permissions for model deployment
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型部署的安全权限
- en: The concept of [*least privilege*](https://oreil.ly/0qP9-) from IT security
    states that no system user should ever have more permissions than they need. Least
    privilege is a fundamental process control that, likely because ML systems touch
    so many other IT systems, tends to be thrown out the window for ML build-outs
    and for so-called “rock star” data scientists. Unfortunately, this is an ML safety
    and performance antipattern. Outside the world of overhyped ML and rock star data
    science, it’s long been understood that engineers cannot adequately test their
    own code and that others in a product organization—product managers, attorneys,
    or executives—should make the final call as to when software is released.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: IT安全中[*最小权限*](https://oreil.ly/0qP9-)的概念指出，没有系统用户应该比他们需要的权限更多。最小权限是一个基本的过程控制，因为机器学习系统涉及到许多其他IT系统，所以往往会被机器学习的构建和所谓的“摇滚明星”数据科学家所抛弃。不幸的是，这是机器学习安全性和性能的反模式。在过度宣传的机器学习和摇滚明星数据科学的世界之外，人们早就意识到工程师无法充分测试自己的代码，并且产品组织中的其他人员——产品经理、律师或高管——应该决定何时发布软件的最终决定。
- en: For these reasons, the IT permissions necessary to deploy an ML system should
    be distributed across several teams within IT organizations. During development
    sprints, data scientists and engineers certainly must retain full control over
    their development environments. But, as important releases or reviews approach,
    the IT permissions to push fixes, enhancements, or new features to user-facing
    products are transferred away from data scientists and engineers to product managers,
    testers, attorneys, executives, or others. Such process controls provide a gate
    that prevents unapproved code from being deployed.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，部署机器学习系统所需的IT权限应该在IT组织的多个团队之间分配。在开发阶段，数据科学家和工程师确实必须保持对其开发环境的完全控制。但是，在重要的发布或审查即将到来时，将推动修复、增强或新功能推送到面向用户的产品的IT权限从数据科学家和工程师转移到产品经理、测试人员、律师、高管或其他人员。这样的过程控制提供了一个门槛，防止未经批准的代码被部署。
- en: Bug bounties
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 漏洞赏金
- en: Bug bounties are another concept we can borrow from computer security. Traditionally,
    a bug bounty is when an organization offers rewards for finding problems in its
    software, particularly security vulnerabilities. Since ML is mostly just software,
    we can do bug bounties for ML systems. While we can use bug bounties to find security
    problems in ML systems, we can also use them to find other types of problems related
    to reliability, safety, transparency, explainability, interpretability, or privacy.
    Through bug bounties, we use monetary rewards to incentivize community feedback
    in a standardized process. As we’ve highlighted elsewhere in the chapter, incentives
    are crucial in risk management. Generally, risk management work is tedious and
    resource consuming. If we want our users to find major problems in our ML systems
    for us, we need to pay them or reward them in some other meaningful way. Bug bounties
    are typically public endeavors. If that makes some organizations nervous, internal
    hackathons in which different teams look for bugs in ML systems may have some
    of the same positive effects. Of course, the more participants are incentivized
    to participate, the better the results are likely to be.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 漏洞赏金是我们可以从计算机安全中借鉴的另一个概念。传统上，漏洞赏金是指组织为发现其软件问题（特别是安全漏洞）而提供奖励。由于机器学习主要是软件，我们可以为机器学习系统设立漏洞赏金。虽然我们可以利用漏洞赏金在机器学习系统中发现安全问题，但我们也可以利用它们来发现与可靠性、安全性、透明度、可解释性、可解释性或隐私相关的其他类型问题。通过漏洞赏金，我们使用货币奖励来激励社区在标准化流程中提供反馈。正如我们在本章的其他地方所强调的那样，激励在风险管理中至关重要。一般而言，风险管理工作是繁琐且资源消耗大的。如果我们希望用户为我们找出机器学习系统中的重大问题，我们需要支付他们或以其他有意义的方式奖励他们。漏洞赏金通常是公开的努力。如果这让一些组织感到不安，内部黑客马拉松活动——不同团队在机器学习系统中寻找漏洞——可能会产生一些相同的积极影响。当然，参与者越多，激励参与的效果可能越好。
- en: AI incident response
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AI事故响应
- en: According to the vaunted [SR 11-7 guidance](https://oreil.ly/E7G2R), “even with
    skilled modeling and robust validation, model risk cannot be eliminated.” If risks
    from ML systems and ML models cannot be eliminated, then such risks will eventually
    lead to incidents. Incident response is already a mature practice in the field
    of computer security. Venerable institutions like [NIST](https://oreil.ly/glkOX)
    and [SANS](https://oreil.ly/gU-Vo) have published computer security incident response
    guidelines for years. Given that ML is a less mature and higher-risk technology
    than general-purpose enterprise computing, formal AI incident response plans and
    practices are a must for high-impact or mission-critical AI systems.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 根据备受尊敬的[SR 11-7指南](https://oreil.ly/E7G2R)，“即使具备熟练的建模和健壮的验证，模型风险也无法消除。”如果来自机器学习系统和模型的风险无法消除，那么这些风险最终将导致事故。事故响应在计算机安全领域已经是一个成熟的实践。类似于[NIST](https://oreil.ly/glkOX)和[SANS](https://oreil.ly/gU-Vo)这样的古老机构多年来一直发布计算机安全事故响应指南。考虑到机器学习技术相比于通用企业计算技术来说是一种较不成熟且风险较高的技术，高影响或任务关键的人工智能系统必须有正式的AI事故响应计划和实践。
- en: 'Formal AI incident response plans enable organizations to respond more quickly
    and effectively to inevitable incidents. Incident response also plays into the
    Hand rule discussed at the beginning of the chapter. With rehearsed incident response
    plans in place, organizations may be able to identify, contain, and eradicate
    AI incidents before they spiral into costly or dangerous public spectacles. AI
    incident response plans are one of the most basic and universal ways to mitigate
    AI-related risks. Before a system is deployed, incident response plans should
    be drafted and tested. For young or small organizations that cannot fully implement
    model risk management, AI incident response is a cost-effective and potent AI
    risk control to consider. Borrowing from computer incident response, AI incident
    response can be thought of in six phases:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 正式的人工智能事故响应计划使组织能够更快速、更有效地响应不可避免的事故。事故响应也涉及到本章开头讨论的“手”规则。通过事先准备的事故响应计划，组织可能能够在AI事故演变为昂贵或危险的公开场面之前识别、控制和消除AI事故。AI事故响应计划是减轻与AI相关风险的最基本和普遍的方式之一。在系统部署之前，应起草并测试事故响应计划。对于年轻或小型组织而言，无法完全实施模型风险管理，人工智能事故响应是一种经济高效且强大的AI风险控制方法。借鉴计算机事故响应，AI事故响应可以分为六个阶段：
- en: 'Phase 1: Preparation'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段：准备
- en: 'In addition to clearly defining an AI incident for our organization, preparation
    for AI incidents includes personnel, logistical, and technology plans for when
    an incident occurs. Budget must be set aside for response, communication strategies
    must be put in place, and technical safeguards for standardizing and preserving
    model documentation, out-of-band communications, and shutting down AI systems
    must be implemented. One of the best ways to prepare and rehearse for AI incidents
    are tabletop discussion exercises, where key organizational personnel work through
    a realistic incident. Good starter questions for an AI incident tabletop include
    the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为我们的组织明确定义AI事故外，为AI事故做准备还包括人员、后勤和技术计划，以应对事故发生时的情况。必须预留预算进行响应，必须制定沟通策略，并必须实施用于标准化和保留模型文档、越带频外沟通和关闭AI系统的技术保障。为AI事故做准备并排练的最佳方法之一是通过桌面讨论练习，组织关键人员通过真实的事件来进行工作。用于AI事故桌面讨论的好的起始问题包括以下内容：
- en: Who has the organizational budget and authority to respond to an AI incident?
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁有组织预算和权力来应对AI事故？
- en: Can the AI system in question be taken offline? By whom? At what cost? What
    upstream processes will be affected?
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所涉及的AI系统是否可以被下线？由谁操作？成本如何？会影响到哪些上游流程？
- en: Which regulators or law enforcement agencies need to be contacted? Who will
    contact them?
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要联系哪些监管机构或执法机构？谁来联系他们？
- en: Which external law firms, insurance agencies, or public relation firms need
    to be contacted? Who will contact them?
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要联系哪些外部律师事务所、保险机构或公共关系公司？谁来联系他们？
- en: Who will manage communications? Internally, between responders? Externally,
    with customers or users?
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁将负责管理沟通？在响应者之间内部沟通？在与客户或用户之间外部沟通？
- en: 'Phase 2: Identification'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段：识别
- en: Identification is when organizations spot AI failures, attacks, or abuses. Identification
    also means staying vigilant for AI-related abuses. In practice, this tends to
    involve more general attack identification approaches, like network intrusion
    monitoring, and more specialized monitoring for AI system failures, like concept
    drift or algorithmic discrimination. Often the last step of the identification
    phase is to notify management, incident responders, and others specified in incident
    response plans.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 识别是指组织发现AI失败、攻击或滥用的情况。识别还意味着保持对与AI相关的滥用保持警惕。在实践中，这往往涉及更一般的攻击识别方法，如网络入侵监控，以及更专门用于AI系统故障的监控，如概念漂移或算法歧视。识别阶段的最后一步通常是通知管理层、事件响应人员和其他在事件响应计划中指定的人员。
- en: 'Phase 3: Containment'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 第三阶段：封锁
- en: Containment refers to mitigating the incident’s immediate harms. Keep in mind
    that harms are rarely limited to the system where the incident began. Like more
    general computer incidents, AI incidents can have network effects that spread
    throughout an organizations’ and its customers’ technologies. Actual containment
    strategies will vary depending on whether the incident stemmed from an external
    adversary, an internal failure, or an off-label use or abuse of an AI system.
    If necessary, containment is also a good place to start communicating with the
    public.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 封锁指的是减轻事件的直接伤害。请记住，伤害很少限于事件开始的系统。与更一般的计算机事件类似，AI事件可能会产生影响整个组织及其客户技术的网络效应。实际的封锁策略将取决于事件是由外部对手、内部失误还是AI系统的非标签使用或滥用引起的。必要时，封锁也是与公众进行沟通的好的起点。
- en: 'Phase 4: Eradication'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 第四阶段：消灭
- en: Eradication involves remediating any affected systems. For example, sealing
    off any attacked systems from vectors of in- or ex-filtration, or shutting down
    a discriminatory AI system and temporarily replacing it with a trusted rule-based
    system. After eradication, there should be no new harms caused by the incident.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 消灭涉及纠正任何受影响的系统。例如，封锁任何受攻击系统的入侵或外泄渠道，或关闭歧视性AI系统，并临时替换为可信赖的基于规则的系统。消灭后，不应该再由此事件引起新的伤害。
- en: 'Phase 5: Recovery'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 第五阶段：恢复
- en: Recovery means ensuring all affected systems are back to normal and that controls
    are in place to prevent similar incidents in the future. Recovery often means
    retraining or reimplementing AI systems, and testing that they are performing
    at documented preincident levels. Recovery can also require careful analysis of
    technical or security protocols for personnel, especially in the case of an accidental
    failure or insider attack.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复意味着确保所有受影响的系统恢复正常，并采取控制措施以防止将来类似事件的发生。恢复通常意味着重新培训或重新实施AI系统，并测试它们是否在事前文档化的水平上运行。恢复也可能需要对人员的技术或安全协议进行仔细分析，尤其是在意外故障或内部攻击的情况下。
- en: 'Phase 6: Lessons learned'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 第6阶段：吸取教训
- en: '*Lessons learned* refers to corrections or improvements of AI incident response
    plans based on the successes and challenges encountered while responding to the
    current incident. Response plan improvements can be process- or technology-oriented.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*吸取的教训*指的是根据当前事件响应中遇到的成功和挑战，对AI事件响应计划进行的修正或改进。响应计划的改进可以是过程或技术导向的。'
- en: When reading the following case, think about the phases of incident response,
    and whether an AI incident response plan would have been an effective risk control
    for Zillow.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读以下案例时，考虑事件响应的各个阶段，以及AI事件响应计划是否对Zillow有效地控制风险。
- en: 'Case Study: The Rise and Fall of Zillow’s iBuying'
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：Zillow iBuying的兴起与衰落
- en: In 2018, the real estate tech company Zillow entered the business of buying
    homes and flipping them for a profit, known as *iBuying*. The company believed
    that its proprietary, ML-powered *Zestimate* algorithm could do more than draw
    eyeballs to its extremely popular web products. As reported by Bloomberg, Zillow
    employed domain experts to validate the numbers generated by their algorithms
    when they first started to buy homes. First, local real estate agents would price
    the property. The numbers were combined with the Zestimate, and a final team of
    experts vetted each offer before it was made.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，房地产科技公司Zillow进入购买房屋并翻新以获取利润的业务，即*iBuying*。公司相信其专有的、由机器学习驱动的*Zestimate*算法不仅能够吸引大量用户到其极其受欢迎的网络产品上。据彭博报道，Zillow雇用领域专家验证他们算法生成的数字，当他们开始购买房屋时。首先，当地的房地产经纪人会为房产定价。这些数字与Zestimate相结合，然后由最终的专家团队审核每一份报价之前。
- en: According to [Bloomberg](https://oreil.ly/LQQg3), Zillow soon phased out these
    teams of domain experts in order to “get offers out faster,” preferring the speed
    and scale of a more purely algorithmic approach. When the Zestimate did not adapt
    to a rapidly inflating real estate market in early 2021, Zillow reportedly intervened
    to increase the attractiveness of its offers. As a result of these changes, the
    company began acquiring properties at a rate of nearly 10,000 homes per quarter.
    More flips means more staff and more renovation contractors, but as Bloomberg
    puts it, “Zillow’s humans couldn’t keep up.” Despite increasing staffing levels
    by 45% and bringing on “armies” of contractors, the iBuying system was not achieving
    profitability. The combination of pandemic staffing and supply challenges, the
    overheated housing market, and complexities around handling large numbers of mortgages
    were just too much for the iBuying project to manage.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[Bloomberg](https://oreil.ly/LQQg3)，Zillow很快淘汰了这些领域专家团队，以便“更快地出价”，更倾向于更纯粹的算法方法的速度和规模。当Zestimate在2021年初未能适应迅速升温的房地产市场时，据报道，Zillow介入增加其报价的吸引力。由于这些变化，公司开始以每季度近1万套房屋的速度收购房产。翻新越多意味着需要更多的员工和更多的翻新承包商，但正如彭博所述，“Zillow的人员跟不上”。尽管人员增加了45%并引进了“大批”承包商，但iBuying系统未能实现盈利。由于疫情期间的人员和供应挑战、过热的住房市场以及处理大量抵押贷款的复杂性，iBuying项目难以管理。
- en: In October of 2021, Zillow announced that it would stop making offers through
    the end of the year. As a result of Zillow’s appetite for rapid growth, as well
    as labor and supply shortages, the company had a huge inventory of homes to clear.
    To solve its inventory problem, Zillow was posting most homes for resale at a
    loss. Finally, on November 2, Zillow announced that it was writing down its inventory
    by over $500 million. Zillow’s foray into the automated house-flipping business
    was over.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年10月，Zillow宣布将在年底前停止出价。由于Zillow对快速增长的渴望以及劳动力和供应短缺，公司拥有大量待清理的房屋库存。为解决库存问题，Zillow大多数房屋都是以亏损的价格重新上市。最终，11月2日，Zillow宣布将其库存减记超过5亿美元。Zillow进入自动翻新房屋业务的尝试告终。
- en: Fallout
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 副作用
- en: In addition to the huge monetary loss of its failed venture, Zillow announced
    that it would lay off about 2,000 employees—a full quarter of the company. In
    June of 2021, Zillow was trading at around $120 per share. At the time of this
    writing, nearly one year later, shares are approximately $40, erasing over $30
    billion in stock value. (Of course, the entire price drop can’t be attributed
    to the iBuying incident, but it certainly factored into the loss.) The downfall
    of Zillow’s iBuying is rooted in many interwoven causes, and cannot be decoupled
    from the pandemic that struck in 2020 and upended the housing market. In the next
    section, we’ll examine how to apply what we’ve learned in this chapter about governance
    and risk management to Zillow’s misadventures.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 除了其失败的冒险带来的巨额经济损失外，Zillow宣布将裁员约2,000名员工，占公司总数的四分之一。2021年6月，Zillow的股价约为每股120美元。截至本文撰写时，将近一年后，股价约为40美元，抹去了超过300亿美元的股票价值。（当然，整个价格下跌不能完全归咎于iBuying事件，但它确实影响了损失的程度。）Zillow的iBuying失败根源于许多交织在一起的原因，不能脱离2020年爆发的疫情和颠覆房地产市场的影响。在接下来的章节中，我们将探讨如何将本章关于治理和风险管理的学习应用于Zillow的不幸经历。
- en: Lessons Learned
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lessons Learned
- en: 'What does this chapter teach us about the Zillow iBuying saga? Based on the
    public reporting, it appears Zillow’s decision to sideline human review of high-materiality
    algorithms was probably a factor in the overall incident. We also question whether
    Zillow had adequately thought through the financial risk it was taking on, whether
    appropriate governance structures were in place, and whether the iBuying losses
    might have been handled better as an AI incident. We don’t know the answers to
    many of these questions with respect to Zillow, so instead we’ll focus on insights
    readers can apply at their own organizations:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章给我们关于Zillow iBuying传奇故事教会了什么？根据公开报道，Zillow决定排除高重要性算法的人工审查可能是整个事件中的一个因素。我们还质疑Zillow是否充分考虑了其承担的财务风险，是否建立了适当的治理结构，以及iBuying的损失是否可能作为AI事件更好地处理。关于Zillow的这些问题我们并不了解答案，所以我们将集中讨论读者可以应用于他们自己组织的见解。
- en: 'Lesson 1: Validate with domain experts.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 'Lesson 1: 与领域专家进行验证。'
- en: In this chapter, we stressed the importance of diverse and experienced teams
    as a core organizational competency for responsible ML development. Without a
    doubt, Zillow has internal and external access to world-class expertise in real
    estate markets. However, in the interest of speed and automation—sometimes referred
    to as “moving fast and breaking things” or “product velocity”—Zillow phased the
    experts out of the process of acquiring homes, choosing instead to rely on its
    Zestimate algorithm. According to follow-up reporting by [Bloomberg](https://oreil.ly/boQye)
    in May 2022, “Zillow told its pricing experts to stop questioning the algorithms,
    according to people familiar with the process.” This choice may have proven fatal
    for the venture, especially in a rapidly changing, pandemic-driven real estate
    market. No matter the hype, AI is not smarter than humans yet. If we’re making
    high-risk decisions with ML, keep humans in the loop.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们强调了多样化和经验丰富的团队作为负责任的ML开发的核心组织能力的重要性。毫无疑问，Zillow在房地产市场拥有全球一流的内外部专业知识。然而，出于速度和自动化的考虑——有时被称为“快速推进并打破事物”或“产品速度”——Zillow将专家排除在收购房屋过程之外，选择依赖其Zestimate算法。根据彭博社2022年5月的跟进报道，“据熟悉这一过程的人士称，Zillow告诉其定价专家停止质疑算法。”这种选择可能对该冒险业务产生致命影响，特别是在快速变化、受疫情驱动的房地产市场中。无论炒作多大，AI目前还不如人类聪明。如果我们在ML中做出高风险决策，请保持人类的参与。
- en: 'Lesson 2: Forecast failure modes.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 'Lesson 2: 预测故障模式。'
- en: The coronavirus pandemic of 2020 created a paradigm shift in many domains and
    markets. ML models, which usually assume that the future will resemble the past,
    likely suffered across the board in many verticals. We shouldn’t expect a company
    like Zillow to see a pandemic on the horizon. But as we’ve discussed, rigorously
    interrogating the failure modes of our ML system constitutes a crucial competency
    for ML in high-risk settings. We do not know the details of Zillow’s model governance
    frameworks, but the downfall of Zillow’s iBuying stresses the importance of effective
    challenge and asking hard questions, like “What happens if the cost of performing
    renovations doubles over the next two years?” and “What will be the business cost
    of overpaying for homes by two percent over the course of six months?” For such
    a high-risk system, probable failure modes should be enumerated and documented,
    likely with board of directors oversight, and the actual financial risk should
    have been made clear to all senior decision-makers. At our organization, we need
    to know the cost of being wrong with ML and that senior leadership is willing
    to tolerate those costs. Maybe senior leaders at Zillow were accurately informed
    of iBuying’s financial risks, maybe they weren’t. What we know now is that Zillow
    took a huge risk, and it did not pay off.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年的新冠疫情在许多领域和市场上引起了范式转变。通常假设未来会像过去的机器学习模型在许多行业可能普遍受到影响。我们不应该期望像Zillow这样的公司能预见到一场大流行。但正如我们所讨论的那样，严格审视我们的机器学习系统失败模式是高风险环境中机器学习的关键能力。我们不清楚Zillow的模型治理框架的具体细节，但Zillow
    iBuying的失败强调了有效挑战和提出难题的重要性，比如“如果未来两年进行装修的成本翻倍会怎样？”和“在六个月内为房屋支付超过两个百分点的成本会带来怎样的商业风险？”对于这样一个高风险系统，可能的失败模式应该被详细列举和记录，可能需要董事会的监督，实际的财务风险应该清楚地告知所有高级决策者。在我们的组织中，我们需要知道ML错误的成本，并且高级领导层愿意承担这些成本。也许Zillow的高级领导确实被告知了iBuying的财务风险，也许没有。现在我们知道的是，Zillow冒了很大的风险，但并没有得到回报。
- en: 'Lesson 3: Governance counts.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 'Lesson 3: 治理至关重要。'
- en: 'Zillow’s CEO is famous for risk-taking, and has a proven track record of winning
    big bets. But, we simply can’t win every bet we make. This is why we manage and
    govern risks when conducting automated decision making, especially in high-risk
    scenarios. SR 11-7 states, “the rigor and sophistication of validation should
    be commensurate with the bank’s overall use of models.” Zillow is not a bank,
    but Bloomberg’s May 2022 postmortem puts it this way: Zillow was “attempting to
    pivot from selling online advertising to operating what amounted to a hedge fund
    and a sprawling construction business.” Zillow drastically increased the materiality
    of its algorithms, but appears not to have drastically increased governance over
    those algorithms. As noted, most of the public reporting points to Zillow decreasing
    human oversight of its algorithms during its iBuying program, not increasing oversight.
    A separate risk function, empowered with the organizational stature to stop models
    from moving into production, and with the appropriate budget and staff levels,
    that reports directly to the board of directors and operates independently from
    business and technology functions headed by the CEO and CTO, is common in major
    consumer finance organization. This organizational structure, when it works as
    intended, allows for more objective and risk-based decisions about ML model performance,
    and avoids the conflicts of interest and confirmation bias that tend to occur
    when business and technology leaders evaluate their own systems for risk. We don’t
    know if Zillow had an independent model governance function—it is quite rare these
    days outside of consumer finance. But we do know that no risk or oversight function
    was able to stop the iBuying program before losses became staggering. While it’s
    a tough battle to fight as a single technician, helping our organization apply
    independent audits to its ML systems is a workable risk mitigation practice.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Zillow的首席执行官以冒险精神而闻名，并拥有赢得重大赌注的经验。但我们并非每一次都能赢得这些赌注。因此，在进行自动决策时，尤其是在高风险场景下，我们需要管理和控制风险。SR
    11-7规定：“验证的严谨性和复杂性应与银行整体模型使用相符。” Zillow并非银行，但彭博社2022年5月的事后分析这样表述：Zillow试图从在线广告销售转型为经营一家规模庞大的对冲基金和建筑业务。Zillow极大地增加了其算法的重要性，但似乎没有大幅增加对这些算法的治理。正如指出的那样，在其iBuying项目期间，大多数公开报道都指出Zillow减少了对其算法的人工监督，而非增加了监督。独立的风险功能应有权力，具有组织地位以阻止模型投入生产，并拥有适当的预算和员工水平，直接向董事会汇报，并独立于由首席执行官和首席技术官领导的业务和技术功能运作。这种组织结构在运行如期时，能够更客观地基于风险做出关于机器学习模型性能的决策，并避免业务和技术领导人评估自身系统风险时常见的利益冲突和确认偏差。我们不知道Zillow是否有独立的模型治理功能——在如今的消费金融之外，这种情况相当罕见。但我们知道，在损失变得惊人之前，没有任何风险或监督功能能够阻止iBuying项目。作为单一技术人员，协助我们的组织对其机器学习系统进行独立审计是一种可行的风险缓解实践。
- en: 'Lesson 4: AI incidents occur at scale.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 第四课：AI事件大规模发生。
- en: Zillow’s iBuying hijinks aren’t funny. Money was lost. Careers were lost—thousands
    of employees were laid off or resigned. This looks like a $30 billion AI incident.
    From the incident response lens, we need to be prepared for systems to fail, we
    need to be monitoring for systems to fail, and we need to have documented and
    rehearsed plans in place for containment, eradication, and recovery. From public
    reporting, it does appear that Zillow was aware of its iBuying problems, but its
    culture was more focused on winning big than preparing for failure. Given the
    size of the financial loss, Zillow’s containment efforts could have been more
    effective. Zillow was able to eradicate its most acute problems with the declaration
    of the roughly half-billion dollar write-off in November of 2021\. As for recovery,
    Zillow’s leadership has plans for a new real estate super app, but given the stock
    price at the time of this writing, recovery is a long way off and investors are
    weary. Complex systems drift toward failure. Perhaps a more disciplined incident-handling
    approach could save our organization when it bets big with ML.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Zillow的iBuying行为并不好笑。金钱损失了。职业生涯也受到了影响——成千上万的员工被裁员或辞职。这看起来像是一个价值300亿美元的人工智能事故。从事故响应的角度来看，我们需要做好系统可能失败的准备，需要监控系统可能失败的情况，并且需要有记录和演练的计划，以进行遏制、清除和恢复。根据公开报道，Zillow似乎意识到了它的iBuying问题，但其文化更注重大获成功，而不是为失败做好准备。考虑到财务损失的规模，Zillow的遏制努力可能本可以更加有效。Zillow通过在2021年11月宣布近50亿美元的减记来根除其最严重的问题。至于恢复，Zillow的领导层已经制定了一个新的房地产超级应用计划，但考虑到本文写作时的股价，恢复还有很长的路要走，而投资者们则感到疲惫。复杂系统向失败漂移。也许在我们与机器学习大赌一把时，采用更加严谨的事故处理方法能够拯救我们的组织。
- en: The final and most important lesson we can take away from the Zillow Offers
    saga is at the heart of this book. Emerging technologies always come with risks.
    Early automobiles were dangerous. Planes used to crash into mountainsides much
    more frequently. ML systems can perpetuate discriminatory practices, pose security
    and privacy risks, and behave unexpectedly. A fundamental difference between ML
    and other emerging technologies is that these systems can make decisions quickly
    and at huge scales. When Zillow leaned into its Zestimate algorithm, it could
    scale up its purchasing to hundreds of homes per day. In this case, the result
    was a write-down of half of a billion dollars, even larger stock losses, and the
    loss of thousands of jobs. This phenomenon of rapid failure at scale can be even
    more directly devastating when the target of interest is access to capital, social
    welfare programs, or the decision of who gets a new kidney.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从Zillow Offers事件中我们可以得到的最后而且最重要的教训是这本书的核心。新兴技术总是伴随着风险出现的。早期的汽车是危险的。飞机过去经常撞山。机器学习系统可能会持续性地推动歧视性实践，存在安全和隐私风险，并且表现出乎意料。机器学习和其他新兴技术的根本区别在于，这些系统可以快速做出决策并且在巨大的规模上操作。当Zillow依赖其Zestimate算法时，它可以将购买规模扩展到每天数百套房屋。在这种情况下，结果是损失了数亿美元，股票损失更大，成千上万的工作岗位消失。当目标是资本获取、社会福利计划或者决定谁可以得到新肾时，这种大规模快速失败的现象可能会更加直接地产生破坏性影响。
- en: Resources
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: Further Reading
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[ISO standards for AI](https://oreil.ly/N3WTp)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ISO人工智能标准](https://oreil.ly/N3WTp)'
- en: '[NIST AI Risk Management Framework](https://oreil.ly/mQ8aW)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NIST人工智能风险管理框架](https://oreil.ly/mQ8aW)'
- en: '[SR 11-7 model risk management guidance](https://oreil.ly/AANIg)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SR 11-7模型风险管理指南](https://oreil.ly/AANIg)'
