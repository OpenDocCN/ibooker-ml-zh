- en: Chapter 10\. Testing and Remediating Bias with XGBoost
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第十章。使用XGBoost测试和修复偏差
- en: This chapter presents bias testing and remediation techniques for structured
    data. While [Chapter 4](ch04.html#unique_chapter_id_4) addressed issues around
    bias from various perspectives, this chapter focuses on technical implementations
    of bias testing and remediation approaches. We’ll start off by training XGBoost
    on a variant of the credit card data. We’ll then test for bias by checking for
    differences in performance and outcomes across demographic groups. We’ll also
    try to identify any bias concerns at the individual observation level. Once we
    confirm the existence of measurable levels of bias in our model predictions, we’ll
    start trying to fix, or remediate, that bias. We employ pre-, in- and postprocessing
    remediation methods that attempt to fix the training data, model, and outcomes,
    respectively. We’ll finish off the chapter by conducting bias-aware model selection
    that leaves us with a model that is both performant and more fair than the original
    model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了针对结构化数据的偏差测试和修复技术。虽然[第四章](ch04.html#unique_chapter_id_4)从多个角度讨论了偏差问题，但本章集中于偏差测试和修复方法的技术实施。我们将从训练XGBoost模型开始，使用信用卡数据的变体。然后，我们将通过检查不同人群在性能和结果上的差异来测试偏差。我们还将尝试在个体观察级别识别任何偏差问题。一旦确认我们的模型预测中存在可测量的偏差水平，我们将开始尝试修复这些偏差。我们采用前处理、中处理和后处理的修复方法，试图修复训练数据、模型和结果。最后，我们将进行偏差感知的模型选择，得到一个性能优越且比原始模型更公平的模型。
- en: While we’ve been clear that technical tests and fixes for bias do not solve
    the problem of machine learning bias, they still play an important role in an
    effective overall bias mitigation or ML governance program. While fair scores
    from a model do not translate directly to fair outcomes in a deployed ML system—for
    any number of reasons—it’s still better to have fair scores than not. We’d also
    argue it’s one of the fundamental and obvious ethical obligations of practicing
    data scientists to test models that operate on people for bias. Another theme
    we’ve brought up before is that unknown risks are much harder to manage than known
    risks. When we know a system may present bias risks and harms, we can attempt
    to remediate that bias, monitor the system for bias, and apply many different
    sociotechnical risk controls—like bug bounties or user interviews—to mitigate
    any potential bias.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们明确表示，偏差的技术测试和修复并不能解决机器学习偏差问题，但它们在有效的整体偏差缓解或机器学习治理程序中仍然扮演重要角色。虽然模型的公平分数不会直接转化为部署的机器学习系统中的公平结果——出于任何原因——但拥有公平的分数总比没有好。我们还会认为，测试那些作用于人的模型是否存在偏差是数据科学家的基本和明显的伦理义务之一。我们之前提到的另一个主题是，未知风险比已知风险难以管理得多。当我们知道系统可能存在偏差风险和危害时，我们可以尝试修复这种偏差，监控系统的偏差，并应用许多不同的社会技术风险控制措施——如漏洞赏金或用户访谈——来减轻潜在的偏差。
- en: Note
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This chapter focuses on bias testing and remediation for a fairly traditional
    classifier, because this is where these topics are best understood, and because
    many complex artificial intelligence outcomes often boil down to a final binary
    decision that can be treated in the same way as a binary classifier. We highlight
    techniques for regression models throughout the chapter as well. See [Chapter 4](ch04.html#unique_chapter_id_4)
    for ideas on how to manage bias in multinomial, unsupervised, or generative systems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍了偏差测试和修复在一个相当传统的分类器上的应用，因为这是这些主题最好理解的地方，而且因为许多复杂的人工智能结果最终都可以归结为可以像二元分类器一样处理的最终二元决策。我们在整章中也突出了回归模型的技术。有关如何管理多项式、无监督或生成系统中的偏差问题的想法，请参阅[第四章](ch04.html#unique_chapter_id_4)。
- en: By the end of the chapter, readers should understand how to test a model for
    bias and then select a less biased model that also performs well. While we acknowledge
    there’s no silver bullet technical fix for ML bias, a model that is more fair
    and more performant is a better option for high-risk applications than a model
    that hasn’t been tested or remediated for bias. The chapter’s code examples are
    available [online](https://oreil.ly/machine-learning-high-risk-apps-code).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章末尾，读者应了解如何测试模型的偏差，并选择一个既不那么偏倚又表现良好的模型。虽然我们承认在机器学习偏差问题上没有银弹技术修复方案，但一个更公平、更高效的模型比一个未经偏差测试或修复的模型更适合高风险应用。本章的代码示例可在线获取[（链接）](https://oreil.ly/machine-learning-high-risk-apps-code)。
- en: 'Concept Refresher: Managing ML Bias'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念复习：管理机器学习偏差
- en: Before we dive into this chapter’s case study, let’s do a quick refresher of
    the applicable topics from [Chapter 4](ch04.html#unique_chapter_id_4). The most
    important thing to emphasize from [Chapter 4](ch04.html#unique_chapter_id_4) is
    that all ML systems are sociotechnical, and the kind of purely technical testing
    we’re focusing on in this chapter is not going to catch all the different bias
    issues that might arise from an ML system. The simple truth is that “fair” scores
    from a model, as measured on one or two datasets, give an entirely incomplete
    picture of the bias of the system. Other issues could arise from unrepresented
    users, accessibility problems, physical design mistakes, downstream misuse of
    the system, misinterpretation of results, and more.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨本章的案例研究之前，让我们快速回顾一下[第四章](ch04.html#unique_chapter_id_4)中适用的主题。从[第四章](ch04.html#unique_chapter_id_4)中最重要的事情是，所有的机器学习系统都是社会技术系统，而我们在本章中关注的纯技术测试并不能捕捉到可能由机器学习系统引起的所有不同的偏差问题。简单的事实是，“公平”的模型评分，如在一个或两个数据集上测量的，完全不能完整地反映系统的偏差情况。其他问题可能源于未被代表的用户、可访问性问题、物理设计错误、系统的下游误用、结果的误解等等。
- en: Warning
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Technical approaches to bias testing and mitigation must be combined with sociotechnical
    approaches to adequately address potential bias harms. We can’t ignore the demographic
    background of our own teams, the demographics of users or those represented in
    training and testing data, data science cultural issues (like entitled “rock stars”),
    and highly developed legal standards, and also expect to address bias in ML models.
    This chapter focuses mostly on technical approaches. [Chapter 4](ch04.html#unique_chapter_id_4)
    attempts to describe a broader sociotechnical approach to managing bias in ML.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 技术方法与偏差测试和缓解必须与社会技术方法结合起来，以充分解决潜在的偏差危害。我们不能忽视我们团队自身的人口背景、用户的人口统计信息或在训练和测试数据中被代表的人群、数据科学文化问题（比如被称为“摇滚明星”的问题）、以及高度发展的法律标准，并期望能够解决机器学习模型中的偏差。本章主要集中在技术方法上。[第四章](ch04.html#unique_chapter_id_4)试图描述一种更广泛的社会技术方法来管理机器学习中的偏差。
- en: We must augment technical bias testing and remediation efforts with an overall
    commitment to having a diverse set of stakeholders involved in ML projects and
    adherence to a systematic approach to model development. We also need to talk
    to our users and abide by model governance that holds humans accountable for the
    decisions of the computer systems we implement and deploy. To be blunt, these
    kinds of sociotechnical risk controls are likely more important and more effective
    than the technical controls we discuss in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须通过将多样化的利益相关者参与到机器学习项目中，并遵守系统化的模型开发方法，来增强技术偏差测试和补救工作的努力。我们还需要与用户沟通，并遵守将人类对计算机系统决策负责的模型治理。坦率地说，这些类型的社会技术风险控制可能比我们在本章讨论的技术控制更重要、更有效。
- en: 'Nonetheless, we don’t want to deploy blatantly biased systems, and if we can
    make the technology better, we should. Less biased ML systems are an important
    part of an effective bias mitigation strategy, and to get that right, we’ll need
    a lot of tools from our data science tool belt, like adversarial models, tests
    for practical and statistical differences in group outcomes, tests for differential
    performance across demographic groups, and various bias-remediation approaches.
    First, let’s go over some terms that we’ll be using throughout this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们不希望部署明显存在偏见的系统，如果我们能让技术变得更好，我们应该去做。较少偏见的机器学习系统是有效的偏见缓解策略的重要组成部分，为了做到这一点，我们将需要从我们的数据科学工具中获得很多工具，如对抗模型、对群体结果的实际和统计差异测试、对不同人群之间性能差异的测试以及各种偏见补救方法。首先，让我们来了解一下在本章中将会使用的一些术语：
- en: Bias
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见
- en: For this chapter we mean *systemic biases*—historical, societal, and institutional,
    as defined by the National Institute of Standards and Technology (NIST) [SP 1270](https://oreil.ly/R1FNW)
    AI bias guidance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们指的是*系统性偏见*——根据国家标准技术研究所（NIST）[SP 1270](https://oreil.ly/R1FNW) AI偏见指南中定义的历史性、社会性和制度性偏见。
- en: Adversarial model
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗模型
- en: In bias testing, we often train adversarial models on the predictions of the
    model we’re testing to predict demographic information. If an ML model (the adversary)
    can predict demographic information from another model’s predictions, then those
    predictions probably encode some amount of systemic bias. Crucially, the predictions
    of adversarial models also give us a row-by-row measure of bias. The rows where
    the adversary model is most accurate likely encode more demographic information,
    or proxies thereof, than other rows.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在偏见测试中，我们经常对我们正在测试的模型的预测进行对抗训练，以预测人口统计信息。如果一个机器学习模型（对手）能够从另一个模型的预测中预测出人口统计信息，那么这些预测很可能包含某种系统偏见。至关重要的是，对抗模型的预测还为我们提供了一种逐行测量偏见的方法。对手模型最准确的行往往可能编码了更多的人口统计信息或其代理物。
- en: Practical and statistical significance testing
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实用和统计显著性测试
- en: One of the oldest types of bias testing focuses on mean *outcome* differences
    across groups. We might use practical tests or effect size measurements, like
    adverse impact ratio (AIR) or standardized mean difference (SMD), to understand
    whether differences between mean outcomes are practically meaningful. We might
    use statistical significance testing to understand whether mean differences across
    demographic groups are more associated with our current sample of data or are
    likely to be seen again in the future.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最古老的偏见测试之一专注于不同群体之间的平均*结果*差异。我们可能使用实用测试或效果大小测量，如不利影响比率（AIR）或标准化平均差异（SMD），来理解平均结果差异是否在实际上具有意义。我们可能使用统计显著性测试来理解跨人口统计群体的平均差异是否更多地与我们当前的数据样本相关，或者在未来可能再次出现。
- en: Differential performance testing
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 差异性能测试
- en: Another common type of testing is to investigate performance differences across
    groups. We might investigate whether true positive rates (TPR), true negative
    rates (TNR), or R² (or root mean squared error) are roughly equal, or not, across
    demographic groups.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的测试类型是调查跨群体之间的性能差异。我们可能调查真正阳性率（TPR）、真正阴性率（TNR）或R²（或均方根误差）在人口统计群体之间是否大致相等或不相等。
- en: Four-fifths rule
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 四分之五法则
- en: The four-fifths rule is a guideline released in the 1978 [Uniform Guidelines
    on Employee Selection Procedures (UGESP)](https://oreil.ly/EBtZl) by the Equal
    Employment Opportunity Commission (EEOC). Part 1607.4 of the UGESP states that
    “a selection rate for any race, sex, or ethnic group which is less than four-fifths
    (4/5) (or eighty percent) of the rate for the group with the highest rate will
    generally be regarded by the Federal enforcement agencies as evidence of adverse
    impact.” For better or worse, the value of 0.8 for AIR—which compares event rates,
    like job selection or credit approval—has become a widespread benchmark for bias
    in ML systems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 1978年由平等就业机会委员会（EEOC）发布的《雇员选择程序统一指南（UGESP）》中的四分之五法则是一个指导方针。UGESP的第1607.4部分规定，“对于任何种族、性别或族裔群体的选择率，如果低于具有最高率的群体的率的四分之五（4/5）（或百分之八十），通常将被联邦执法机构视为对不利影响的证据。”不管是好是坏，0.8的AIR值——用于比较事件率，如职位选择或信贷批准——已成为机器学习系统中偏见的广泛基准。
- en: Remediation approaches
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 修复方法
- en: 'When testing identifies problems, we’ll want to fix them. Technical bias mitigation
    approaches are often referred to as *remediation*. One thing we can say about
    ML models and bias is that ML models seem to present more ways to fix themselves
    than traditional linear models. Due to the *Rashomon effect*—the fact that there
    are often many accurate ML models for any given training dataset—we simply have
    more levers to pull and switches to flip to find better options for decreased
    bias and sustained predictive performance in ML models versus simpler models.
    Since there are so many options for models in ML, there are many potential ways
    to remediate bias. Some of the most common include pre-, in-, and postprocessing,
    and model selection:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当测试发现问题时，我们希望修复它们。技术上的偏见缓解方法通常称为*修复*。关于机器学习模型和偏见，我们可以说的一件事是，与传统的线性模型相比，机器学习模型似乎提供了更多修复自身的方法。由于*拉尚蒙效应*——即任何给定训练数据集通常存在许多准确的机器学习模型——我们有更多的杠杆和开关可以调整，以找到减少偏见并保持机器学习模型持续预测性能的更好选择。由于在机器学习中存在如此多的模型选项，因此有许多潜在的修复偏见的方法。其中一些最常见的包括前处理、中处理和后处理，以及模型选择：
- en: Preprocessing
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理
- en: Rebalancing, reweighing, or resampling training data so that demographic groups
    are better represented or positive outcomes are distributed more equitably.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 重新平衡、重新加权或重新采样训练数据，以使人口统计群体更好地代表或积极结果更平等地分布。
- en: In-processing
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 内部处理
- en: Any number of alterations to ML training algorithms, including constraints,
    regularization and dual loss functions, or incorporation of adversarial modeling
    information, that attempt to generate more balanced outputs or performance across
    demographic groups.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ML训练算法的任何修改，包括约束、正则化和双重损失函数，或者整合对抗建模信息，旨在生成跨人口群体更平衡的输出或性能。
- en: Postprocessing
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理
- en: Changing model predictions directly to create less biased outcomes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 直接更改模型预测以创建更少偏见的结果。
- en: Model selection
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择
- en: Considering bias along with performance when selecting models. Typically, it’s
    possible to find a model with good performance and fairness characteristics if
    we measure bias and performance across a large set of hyperparameter settings
    and input features.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择模型时考虑偏见和性能。通常，如果我们在大量超参数设置和输入特征上测量偏见和性能，可以找到性能良好且公平特性的模型。
- en: Finally, we’ll need to remember that legal liability can come into play with
    ML bias issues. There are many legal liabilities associated with bias in ML systems,
    and since we’re not lawyers (and likely neither are you), we need to be humble
    about the complexity of law, not let the Dunning-Kruger effect take over, and
    defer to actual experts on nondiscrimination law. If we have any concerns about
    legal problems in our ML systems, now is the time to reach out to our managers
    or our legal department. With all this serious information in mind, let’s now
    jump into training an XGBoost model, and testing it for bias.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要记住，在ML偏见问题中，法律责任可能会产生影响。与ML系统中的偏见相关的法律责任有很多，由于我们不是律师（你可能也不是），我们需要对法律的复杂性保持谦逊，不要让达宁-克鲁格效应占上风，并请教真正的非歧视法律专家。如果我们对ML系统中的法律问题有任何疑虑，现在就是联系经理或法律部门的时候了。考虑到所有这些严肃的信息，现在让我们跳入训练一个XGBoost模型，并测试其是否存在偏见。
- en: Model Training
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'The first step in this chapter’s use case is to train an XGBoost model on the
    credit card example data. To avoid disparate treatment concerns, we will not be
    using demographic features as inputs to this model:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本章用例的第一步是在信用卡示例数据上训练一个XGBoost模型。为避免不同待遇问题，我们将不使用人口统计特征作为此模型的输入：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Generally speaking, for most business applications, it’s safest not to use demographic
    information as model inputs. Not only is this legally risky in spaces like consumer
    credit, housing, and employment, it also implies that business decisions should
    be based on race or gender—and that’s dangerous territory. It’s also true, however,
    that using demographic data in model training can decrease bias, and we’ll see
    a version of that when we try out in-processing bias remediation. There also may
    be certain kinds of decisions that should be based on demographic information,
    such as those about medical treatments. Since this is an example credit decision,
    and since we’re not sociologists or nondiscrimination law experts, we’re going
    to play it safe and not use demographic features in our model. We will be using
    demographic features to test for bias and to remediate bias later in the chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，对于大多数业务应用程序而言，最安全的做法是不使用人口统计信息作为模型输入。这不仅在消费信贷、住房和就业等领域存在法律风险，还意味着业务决策应该基于种族或性别——这是危险的领域。然而，使用人口统计数据来训练模型可以减少偏见，我们将在尝试内部处理偏见修复时看到一个版本。也许还有某些决策应该基于人口统计信息，比如关于医疗治疗的决策。由于这是一个示例信贷决策，并且我们不是社会学家或非歧视法律专家，我们将保守起见，不在我们的模型中使用人口统计特征。我们将使用人口统计特征来测试偏见并在本章后期修复偏见。
- en: Warning
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: One place where we as data scientists tend to go wrong is by using demographic
    information in models or technical bias-remediation approaches in a way that could
    amount to *disparate treatment*. Adherents to the *fairness through awareness*
    doctrine may rightly disagree in some cases, but as of today, the most conservative
    approach to bias management in ML related to housing, credit, employment, and
    other traditional high-risk applications is to use no demographic information
    directly in models or bias remediation. Using demographic information only for
    bias testing is generally acceptable. See [Chapter 4](ch04.html#unique_chapter_id_4)
    for more information.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家在模型中使用人口统计信息或技术性偏差修复方法时，常犯的一个错误是可能导致*不平等待遇*。尽管坚持*意识到公平*的信条在某些情况下可能会有不同看法，但截至目前，在与住房、信贷、就业和其他传统高风险应用相关的机器学习中，对偏差管理最保守的方法是在模型或偏差修复中直接不使用人口统计信息。通常可以接受的做法是仅将人口统计信息用于偏差测试。详细信息请参阅[第4章](ch04.html#unique_chapter_id_4)。
- en: Despite its risks, demographic information is important for bias management,
    and one way organizations go wrong in managing ML bias risks is by not having
    the necessary information on hand to test and then remediate bias. At minimum,
    this means having people’s names and zip codes, so that we could use [Bayesian
    improved surname geocoding](https://oreil.ly/1KpQT), and related techniques, to
    infer their demographic information. If data privacy controls allow, and the right
    security is in place, it’s most useful for bias testing to collect people’s demographic
    characteristics directly. It’s important to note that all the techniques used
    in this chapter do require demographic information, but for the most part, we
    can use demographic information that is inferred or collected directly. With these
    important caveats addressed, let’s look at training our constrained XGBoost model
    and selecting a score cutoff.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在风险，人口统计信息对于偏差管理至关重要。组织在管理机器学习偏差风险时犯的一个错误是没有必要的信息来进行测试和后续的偏差修正。至少，这意味着需要人们的姓名和邮政编码，以便我们可以使用[贝叶斯改进的姓氏地理编码](https://oreil.ly/1KpQT)和相关技术来推断其人口统计信息。如果数据隐私控制允许，并且有适当的安全措施，收集人们的人口统计特征用于偏差测试是最有用的。需要注意的是，本章使用的所有技术都需要人口统计信息，但大多数情况下，我们可以使用推断或直接收集的人口统计信息。在解决了这些重要的注意事项后，让我们来看看如何训练我们受限制的XGBoost模型并选择一个得分截断值。
- en: Warning
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Before training a model in a context where bias risks must be managed, we should
    always make sure that we have the right data on hand to test for bias. At minimum,
    this means name, zip code, and a BISG implementation. At maximum, it means collecting
    demographic labels and all the data privacy and security care that goes along
    with collecting and storing sensitive data. Either way, ignorance is not bliss
    when it comes to ML bias.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在必须管理偏差风险的情况下训练模型之前，我们应始终确保我们手头有足够的数据来进行偏差测试。至少需要姓名、邮政编码和BISG实现。在最大范围内，这意味着收集人口统计标签以及所有与收集和存储敏感数据相关的数据隐私和安全注意事项。无论哪种方式，当涉及机器学习偏差时，无知都不是幸福的状态。
- en: We will be taking advantage of monotonic constraints again. A major reason transparency
    is important with respect to managing bias in ML is that if bias testing highlights
    issues—and it often does—we have a better chance of understanding what’s broken
    about the model, and if we can fix it. If we’re working with an unexplainable
    ML model, and bias problems emerge, we often end up scrapping the whole model
    and hoping for better luck in the next unexplainable model. That doesn’t feel
    very scientific to us.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次利用单调约束的优势。在管理机器学习偏差时，透明度的重要性主要体现在，如果偏差测试指出问题（通常是这样），我们更有可能理解模型存在何种问题，并尝试修复它。如果我们使用的是一个无法解释的机器学习模型，而出现了偏差问题，我们通常会放弃整个模型，希望在下一个无法解释的模型中能够更好地运气。这对我们来说并不是很科学。
- en: We like to test, debug, and understand, to the extent possible, how and why
    ML models work. In addition to being more stable and more generalizable, our constrained
    XGBoost model should also be more transparent and debuggable. We also have to
    highlight that when we take advantage of monotonic constraints to enhance explainability
    *and* XGBoost’s custom objective functionality to consider performance and bias
    simultaneously (see [“In-processing”](#in-processing)), we’re modifying our model
    to be both more transparent *and* more fair. Those seem like the exact right kinds
    of changes to make if we’re worried about stable performance, maximum transparency,
    and minimal bias in a high-risk application. It’s great that XGBoost is mature
    enough to offer this level of deep customizability. (Unfortunately for readers
    working in credit, mortgage, housing, employment, and other traditionally regulated
    sectors, you likely need to check with legal departments before employing a custom
    objective function that processes demographic data due to risks of disparate treatment.)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们喜欢尽可能地测试、调试和理解机器学习模型的工作原理和原因。除了更稳定和更具普适性之外，我们受限制的XGBoost模型还应更具透明性和可调试性。我们还必须强调，当我们利用单调约束来增强可解释性，并利用XGBoost的自定义目标功能来同时考虑性能和偏差（参见[“In-processing”](#in-processing)），我们修改我们的模型使其更透明和更公平。如果我们担心稳定性能、最大透明度和最小偏差在高风险应用中进行修改，这些似乎是确实正确的改变。XGBoost已经发展成熟到可以提供这种深度定制的水平，这非常棒。
- en: Note
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We can combine monotonic constraints (enhanced explainability) and customized
    objective functions (bias management) in XGBoost to directly train more transparent
    and less biased ML models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在XGBoost中结合单调约束（增强可解释性）和自定义目标函数（偏差管理），直接训练更透明、偏差更小的机器学习模型。
- en: 'In terms of defining the constraints for this chapter, we use a basic approach
    based on Spearman correlation. Spearman correlation is nice because it considers
    monotonicity rather than linearity (as is the case with Pearson correlation coefficient).
    We also implement a `corr_threshold` argument to our constraint selection process
    so that small correlations don’t cause spurious constraints:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 就这一章节定义约束而言，我们采用基于Spearman相关系数的基本方法。Spearman相关系数很好，因为它考虑单调性而不是线性（与Pearson相关系数不同）。我们还实现了一个`corr_threshold`参数来处理约束选择过程，以防止小的相关性导致虚假约束：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To train the model, our code is very straightforward. We’ll start with hyperparameters
    we’ve used before to good result and not go crazy with hyperparameter tuning.
    We’re just trying to start off with a decent baseline because we’ll be doing a
    lot of model tuning and applying careful selection techniques when we get into
    bias remediation. Here’s what our first attempt at training looks like:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，我们的代码非常简单。我们将从我们以前使用过并取得良好结果的超参数开始，并且不会过于疯狂地调整超参数。我们只是试图从一个合理的基准开始，因为当我们进行偏差修正时，我们将进行大量的模型调优并应用谨慎的选择技术。这是我们第一次尝试训练的样子：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To calculate test values like AIR and other performance quality ratios across
    demographic groups in subsequent sections, we’ll need to establish a probability
    cutoff so that we can measure our model’s outcomes and not just its predicted
    probabilities. Much like when we train the model, we’re looking for a starting
    point to get some baseline readings right now. We’ll do that using common performance
    metrics like F1, precision, and recall. In [Figure 10-1](#cutoff) you can see
    that by picking a probability cutoff that maximizes F1, we make a solid trade-off
    between precision, which is the model’s proportion of *positive decisions* that
    are correct (positive predicted value), and recall, which is the model’s proportion
    of *positive outcomes* that are correct (true positive rate). For our model, that
    number is 0.26\. To start off, all predictions above 0.26 are not going to get
    the credit line increase on offer. All predictions that are 0.26 or below will
    be accepted.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要在后续章节中计算像AIR和其他绩效质量比率这样的测试值，我们需要设定一个概率截断点，这样我们可以衡量我们模型的结果，而不仅仅是其预测的概率。就像我们训练模型时一样，我们现在正在寻找一个起点来获取一些基准读数。我们将使用像F1、精确度和召回率这样的常见绩效指标来做到这一点。在[图 10-1](#cutoff)中，您可以看到，通过选择最大化F1的概率截断点，我们在精确度（模型正确的*正向决策*比例（正向预测值））和召回率（模型正确的*正向结果*比例（真正例率））之间取得了良好的折衷。对于我们的模型，该数字是0.26。要开始，所有高于0.26的预测都不会得到信用额度的增加。所有0.26或以下的预测都将被接受。
- en: '![mlha 1001](assets/mlha_1001.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 1001](assets/mlha_1001.png)'
- en: Figure 10-1\. A preliminary cutoff, necessary for initial bias testing, is selected
    by maximizing F1 statistic ([digital, color version](https://oreil.ly/EaaUe))
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 通过最大化F1统计量选择的初步截断点，用于初始偏倚测试（[数字，彩色版本](https://oreil.ly/EaaUe)）
- en: We know that we’ll likely end up tuning the cutoff due to bias concerns as well.
    In our data and example setup, increasing the cutoff means lending to more people.
    When we increase the cutoff, we hope that we are also lending to more different
    kinds of people. When we decrease the cutoff, we make our credit application process
    more selective, lending to fewer people, and likely fewer different kinds of people
    too. Another important note about cutoffs—if we’re are monitoring or auditing
    an already-deployed ML model, we should use the exact cutoff that is used for
    in vivo decision making, not an idealized cutoff based on performance statistics
    like we’ve selected here.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，由于偏倚问题，我们很可能最终会调整截断点。在我们的数据和示例设置中，增加截断点意味着向更多的人放贷。当我们增加截断点时，我们希望也能向更多不同类型的人放贷。当我们降低截断点时，我们使我们的信用申请流程更为选择性，向较少的人放贷，可能也是较少不同类型的人。关于截断点的另一个重要说明——如果我们正在监控或审计一个已部署的ML模型，我们应该使用实际用于体内决策的确切截断点，而不是基于我们在这里选择的性能统计的理想化截断点。
- en: Note
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In training and monitoring credit models, we have to remember that we typically
    only have good data for applicants that were selected in the past for a credit
    product. Most agree that this phenomenon introduces bias into any decision based
    only on previously selected individuals. What to do about it, widely discussed
    as *reject inference* techniques, is less clear. Keep in mind, similar bias issues
    apply to other types of applications where long-term data about unselected individuals
    is not available.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和监控信用模型时，我们必须记住，我们通常只有过去被选中申请信用产品的申请人的良好数据。大多数人都认为，这种现象会为仅基于先前选择的个体的决策引入偏倚。如何处理这个问题，被广泛讨论为*拒绝推断*技术，目前不太清楚。请记住，类似的偏倚问题也适用于其他类型的申请，其中长期关于未选中个体的数据是不可用的。
- en: Evaluating Models for Bias
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估偏倚模型
- en: 'Now that we have a model and a cutoff, let’s dig in and start to test it for
    bias. In this section, we’ll test for different types of bias: bias in performance,
    bias in outcome decisions, bias against individuals, and proxy bias. First we
    construct confusion matrices and many different performance and error metrics
    for each demographic group. We’ll apply established bias thresholds from employment
    as a rule of thumb to ratios of those metrics to identify any problematic bias
    in performance. We’ll then apply traditional bias tests and effect size measures,
    aligned with those used in US fair lending and employment compliance programs,
    to test model outcomes for bias. From there, we’ll look at residuals to identify
    any outlying individuals or any strange outcomes around our cutoff. We’ll also
    use an adversarial model to identify any rows of data that seem to be encoding
    more bias than others. We’ll close out our bias testing discussion by highlighting
    ways to find proxies, i.e., seemingly neutral input features that act like demographic
    information in models, that can lead to different types of bias problems.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个模型和一个截止日期，让我们深入挖掘并开始测试其偏见。在本节中，我们将测试不同类型的偏见：表现上的偏见，结果决策中的偏见，对个体的偏见以及代理偏见。首先，我们为每个人群构建混淆矩阵和许多不同的表现和错误指标。我们将应用从就业中建立的偏见阈值作为一种经验法则，用于识别性能中的任何问题性偏见比率。然后，我们将应用传统的偏见测试和效果大小测量，与美国公平信贷和就业合规程序中使用的测量对齐，以测试模型结果中的偏见。然后，我们将查看残差，以识别任何超出截止日期的个体或任何奇怪的结果。我们还将使用对抗模型来识别似乎编码了更多偏见的数据行。最后，我们将通过强调查找代理的方式来结束我们的偏见测试讨论，即在模型中起到类似人口统计信息的看似中立输入特征，这可能导致不同类型的偏见问题。
- en: Testing Approaches for Groups
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对群体的测试方法
- en: We’ll start off our bias-testing exercise by looking for problems in how our
    model treats groups of people on average. In our experience, it’s best to start
    with traditional testing guided by legal standards. For most organizations legal
    risks are the most serious for their AI systems, and assessing legal risks is
    the easiest path toward buy-in for bias testing. For that reason, and for brevity’s
    sake, we won’t consider intersectional groups in this chapter. We’ll stay focused
    on traditional protected classes and associated traditional race groups. Depending
    on the application, jurisdiction and applicable laws, stakeholder needs, or other
    factors, it may be most most appropriate to conduct bias testing across traditional
    demographic groups, intersectional demographic groups, or even across [skin tone
    scales](https://oreil.ly/GuN9L). For example, in fair lending contexts—due to
    established legal bias testing precedent—testing across traditional demographic
    groups first likely makes the most sense, and if time or organizational dynamics
    allow, we should circle back to intersectional testing. For general AI systems
    or ML models operating in the broader US economy, and not under specific nondiscrimination
    requirements, testing across intersectional groups should likely be the default
    when possible. For facial recognition systems, it might make the most sense to
    test across skin tone groups.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从检查我们的模型在平均情况下如何处理人群问题开始我们的偏见测试练习。根据我们的经验，最好从法律标准指导的传统测试开始。对于大多数组织而言，AI系统的法律风险是最严重的，评估法律风险是进行偏见测试的最简单途径。因此，为了简洁起见，我们在本章不考虑交叉组群。我们将专注于传统的受保护类别和相关的传统种族群体。根据应用、管辖权和适用法律、利益相关者需求或其他因素，跨传统人口统计群体、交叉人口统计群体甚至[肤色调查表](https://oreil.ly/GuN9L)进行偏见测试可能是最合适的。例如，在公平信贷背景下——由于已建立的法律偏见测试先例——首先跨传统人口统计群体进行测试可能是最合理的选择，如果时间或组织动态允许，我们应该回到交叉测试。对于在广泛美国经济中运行的一般AI系统或ML模型，并不受特定非歧视要求的约束，尽可能在交叉群体中进行测试可能是默认选择。对于面部识别系统，跨肤色组进行测试可能是最合理的选择。
- en: 'First, we’ll be looking at model performance and whether it’s roughly equivalent
    across traditional demographic groups or not. We’ll also be testing for the absence
    of [*group fairness*](https://oreil.ly/QJGP6), sometimes also called [*statistical*
    or *demographic parity*](https://oreil.ly/MBCCq), in model outcomes. These notions
    of group fairness are flawed, because defining and measuring groups of people
    is difficult, averages hide a lot of information about individuals, and the thresholds
    used for these tests are somewhat arbitrary. Despite these shortcomings, these
    tests are some of the most commonly used today. They can tell us useful information
    about how our model behaves at a high level and point out serious areas of concern.
    Like many of the tests we’ll discuss in this section, the key to interpreting
    them is as follows: *passing these tests doesn’t mean much—our model or system
    could still have serious in vivo bias issues—but failing them is a big red flag
    for bias.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将关注模型的性能，以及它在传统人口统计组群之间是否大致相等。 我们还将测试模型结果中[*群体公平性*](https://oreil.ly/QJGP6)，有时也称为[*统计*或*人口统计的平等*](https://oreil.ly/MBCCq)。
    这些群体公平性的概念存在缺陷，因为定义和测量人群是困难的，平均值隐藏了许多关于个体的信息，而这些测试所使用的阈值也有些随意。 尽管存在这些缺陷，这些测试如今仍是最常用的。
    它们可以向我们提供关于我们的模型在高层次行为如何以及指出严重关注领域的有用信息。 像本节中讨论的许多测试一样，解释它们的关键在于：*通过这些测试并不意味着什么——我们的模型或系统仍然可能存在严重的现场偏差问题——但未通过这些测试则是偏差的一个重要红旗。*
- en: Before jumping into the tests themselves, it’s important to think about where
    to test. Should we test in training, validation, or test data? The most standard
    partitions in which to test are validation and test data, just like when we test
    our model’s performance. Testing for bias in validation data can also be used
    for model selection purposes, as we’ll discuss in [“Remediating Bias”](#remediating_bias_ch10_1680869366352).
    Using test data should give us some idea of how our model will perpetuate bias
    once deployed. (There are no guarantees an ML model will perform similarly to
    what we observe in test data, so monitoring for bias after deployment is crucially
    important.) Bias testing in training data is mostly useful for observing differences
    in bias measurements from validation and test partitions. This is especially helpful
    if one partition stands out from the others, and can potentially be used for understanding
    drivers of bias in our model. If training, validation, and test sets are constructed
    so that training comes first in time and testing comes last—as they likely should
    be—comparing bias measurements across data partitions is also helpful for understanding
    trends in bias. It is a concerning sign to see bias measurements increase from
    training to validation to testing. One other option is to estimate variance in
    bias measurements using cross-validation or bootstrapping, as is done with standard
    performance metrics too. Cross-validation, bootstrapping, standard deviations
    or errors, confidence intervals, and other measure of variance for bias metrics
    can help us understand if our bias-testing results are more precise or more noisy—an
    important part of any data analysis.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始测试之前，重要的是考虑在哪里进行测试。我们应该在训练数据、验证数据还是测试数据中进行测试？最标准的测试分区是验证数据和测试数据，就像我们测试模型性能时一样。在验证数据中测试偏差也可以用于模型选择的目的，正如我们将在[“修正偏差”](#remediating_bias_ch10_1680869366352)中讨论的那样。使用测试数据应该能够让我们大致了解模型在部署后如何持续偏差。
    （没有保证ML模型在测试数据中表现类似于我们观察到的情况，因此部署后监控偏差至关重要。） 在训练数据中测试偏差主要用于观察与验证和测试分区中的偏差测量差异。
    如果一个分区与其他分区有显著差异，这尤其有助于了解我们模型中偏差的驱动因素。 如果训练、验证和测试集的构建使得训练时间最早，测试时间最晚——它们很可能应该如此——比较数据分区间的偏差测量也有助于了解偏差趋势。
    看到从训练到验证再到测试中偏差测量增加是一个令人担忧的迹象。 另一个选项是使用交叉验证或自举法来估计偏差测量的方差，这与标准性能指标一样。 交叉验证、自举法、标准差或误差、置信区间以及其他偏差指标的测量可以帮助我们理解我们的偏差测试结果是否更精确或更嘈杂——这是任何数据分析的重要组成部分。
- en: In the bias testing conducted in the following sections, we’ll be sticking to
    basic practices, and looking for biases in model performance and outcomes in validation
    and test data. If you’ve never tried bias testing, this is a good way to get started.
    And inside large organizations, where logistics and politics make it even more
    difficult, this might be the only bias testing that can be accomplished. Bias
    testing is never finished. As long as a model is deployed, it needs to be monitored
    and tested for bias. All of these practical concerns make bias testing a big effort,
    and for these reasons we’d urge you to begin with these standard practices that
    look for bias in performance and outcomes across large demographic groups, and
    then use any remaining time, resources, and will to investigate bias against individuals
    and to identify proxies or other drivers of bias in your model. That’s what we’ll
    get into now.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下部分进行的偏差测试中，我们将坚持基本做法，并寻找验证和测试数据中模型性能和结果的偏差。如果你从未尝试过偏差测试，这是一个好的开始。在大型组织内部，物流和政治使得这更加困难，这可能是唯一可以完成的偏差测试。偏差测试永远不会结束。只要模型部署，就需要监控和测试偏差。所有这些实际问题使得偏差测试成为一项艰巨的工作，因此我们建议您从寻找跨大型人群中性能和结果偏差的标准做法开始，然后利用剩余的时间、资源和意愿来调查个体的偏差并识别模型中的代理或其他偏差驱动因素。现在我们将深入探讨这些内容。
- en: Note
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Before we can begin bias testing, we must be absolutely clear about how a positive
    decision is represented in the data, what positive means in the real world, how
    our model’s predicted probabilities align to these two notions, and which cutoffs
    generate positive decisions. In our example, the decision that is desirable to
    the preponderance of model subjects is an outcome of zero, associated with probabilities
    below the cutoff value of 0.26\. Applicants who receive a classification of zero
    will be extended a line of credit.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始偏差测试之前，我们必须非常清楚地了解数据中积极决策的表示方式，积极在现实世界中的含义，我们模型预测的概率如何与这两个概念对齐，以及哪些截断生成积极决策。在我们的示例中，大多数模型对象希望的决策是零的结果，与截断值0.26以下的概率相关联。接收到零分类的申请人将获得信贷额度。
- en: Testing performance
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试性能
- en: A model should have roughly similar performance across demographic groups, and
    if it doesn’t, this is an important type of bias. If all groups are being held
    to the same standard by an ML model for receiving a credit product, but that standard
    is not an accurate predictor of future repayment behavior for some groups, that’s
    not fair. (This is somewhat similar to the employment notion of *differential
    validity*, discussed in [Chapter 4](ch04.html#unique_chapter_id_4).) To start
    testing for bias in performance across groups for a binary classifier like our
    XGBoost model, we’ll look at confusion matrices for each group and form different
    measures of performance and error across groups. We’ll consider common measures
    like true positive and false positive rates, as well as some that are less common
    in data science, like false discovery rate.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型在人口统计群体中应该有大致相似的表现，如果没有，这是一种重要的偏差类型。如果所有群体都受到机器学习模型相同的标准的约束以获得信贷产品，但该标准对于某些群体未来的还款行为不是准确的预测者，那就不公平了。（这与就业概念中的*differential
    validity*有些相似，详见[第4章](ch04.html#unique_chapter_id_4)。）为了开始测试我们的XGBoost模型在各组中的性能偏差，我们将查看每个组的混淆矩阵，并形成不同组间性能和误差的不同度量。我们将考虑常见的度量如真正率和假正率，以及一些在数据科学中不那么常见的度量，如虚假发现率。
- en: 'The following code block is far from the best implementation, because of its
    reliance on dynamic code generation and an `eval()` statement, but it is written
    to be maximally illustrative. In it, readers can see how the four cells in a confusion
    matrix can be used to calculate many different performance and error metrics:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块远非最佳实现，因为它依赖于动态代码生成和一个`eval()`语句，但它的编写旨在尽可能地说明。在其中，读者可以看到混淆矩阵中的四个单元格如何用于计算许多不同的性能和误差指标：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When we apply the `confusion_matrix_parser` function to confusion matrices for
    each demographic group, along with other code that loops through groups and the
    measures in `metric_dict`, we can make a table like [Table 10-1](#performance_table_race_raw).
    For brevity, we’ve focused on the race measurements in this subsection. If this
    were a real credit or mortgage model, we’d be looking at different genders, different
    age groups, those with disabilities, different geographies, and maybe even other
    subpopulations.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对每个人种群的混淆矩阵应用`confusion_matrix_parser`函数以及循环遍历`metric_dict`中的组和度量时，我们可以制作像[表 10-1](#performance_table_race_raw)的表格。为简洁起见，本小节我们专注于人种测量。如果这是一个真实的信用或抵押贷款模型，我们将关注不同性别、不同年龄组、残疾人士、不同地理区域甚至其他亚群体。
- en: Table 10-1\. Common performance and error measures derived from a confusion
    matrix across different race groups for test data
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-1\. 不同人种群体测试数据中从混淆矩阵中导出的常见性能和错误度量
- en: '| Group | Prevalence | Accuracy | True positive rate | Precision | …​ | False
    positive rate | False discovery rate | False negative rate | False omissions rate
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 组 | 流行率 | 准确率 | 真阳性率 | 精确率 | …​ | 假阳性率 | 错误发现率 | 假阴性率 | 假遗漏率 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Hispanic | 0.399 | 0.726 | 0.638 | 0.663 | …​ | 0.215 | 0.337 | 0.362 | 0.235
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 西班牙裔 | 0.399 | 0.726 | 0.638 | 0.663 | …​ | 0.215 | 0.337 | 0.362 | 0.235
    |'
- en: '| Black | 0.387 | 0.720 | 0.635 | 0.639 | …​ | 0.227 | 0.361 | 0.365 | 0.229
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 黑人 | 0.387 | 0.720 | 0.635 | 0.639 | …​ | 0.227 | 0.361 | 0.365 | 0.229 |'
- en: '| White | 0.107 | 0.830 | 0.470 | 0.307 | …​ | 0.127 | 0.693 | 0.530 | 0.068
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 白人 | 0.107 | 0.830 | 0.470 | 0.307 | …​ | 0.127 | 0.693 | 0.530 | 0.068 |'
- en: '| Asian | 0.101 | 0.853 | 0.533 | 0.351 | …​ | 0.111 | 0.649 | 0.467 | 0.055
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 亚裔 | 0.101 | 0.853 | 0.533 | 0.351 | …​ | 0.111 | 0.649 | 0.467 | 0.055 |'
- en: '[Table 10-1](#performance_table_race_raw) starts to show us some hints of bias
    in our model’s performance, but it’s not really measuring bias yet. It’s simply
    showing the value for different measurements across groups. We should start to
    pay attention when these values are obviously different for different groups.
    For example, precision looks quite different between demographic groups (white
    and Asian people on one hand, and Black and Hispanic people on the other). The
    same can be said about other measures like the false positive rate, false discovery
    rate, and false omissions rate. (Disparities in prevalence tell us that default
    occurs more *in the data* for Black and Hispanic people. Sadly this not uncommon
    in many US credit markets.) In [Table 10-1](#performance_table_race_raw), we are
    starting to get a hint that our model is predicting more defaults for Black and
    Hispanic people, but it’s still hard to tell if it’s doing a good or equitable
    job. (Just because a dataset records these kinds of values, does not make them
    objective or fair!) To help understand if the patterns we’re seeing are actually
    problematic, we need to take one more step. We’ll follow methods from traditional
    bias testing and divide the value for each group by the corresponding value for
    the control group and apply the four-fifths rule as a guide. In this case, we
    *assume* the control group is white people.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-1](#performance_table_race_raw)开始显示我们模型性能中的一些偏见迹象，但尚未真正衡量偏见。它仅仅展示了不同组之间不同测量值的价值。当这些值在不同群体间明显不同时，我们应该开始关注。例如，精确率在不同人种群体间显著不同（白人和亚裔在一方，黑人和西班牙裔在另一方）。同样的情况也适用于其他测量，如假阳性率、错误发现率和假遗漏率。（流行率的差异告诉我们，在美国许多信贷市场中，黑人和西班牙裔的数据中默认更为常见。）在[表 10-1](#performance_table_race_raw)中，我们开始察觉到我们的模型更多地预测黑人和西班牙裔的违约，但现在还很难判断它是否做得好或是否公平。（仅仅因为数据集记录这些数值，并不意味着它们是客观或公平的！）为了帮助理解我们看到的模式是否确实有问题，我们需要再进一步。我们将遵循传统偏见测试的方法，将每个组的值除以对照组的相应值，并应用五分之四法则作为指导。在本例中，我们*假设*对照组是白人。'
- en: Note
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Strictly speaking, in the employment context, the control group is the most
    favored group in an analysis, not necessarily white people or males. There may
    also be other reasons to use control groups that are not white people or males.
    Choosing the control or reference group for a bias-testing analysis is a difficult
    task, best done in concert with legal, compliance, social science experts, or
    stakeholders.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，在就业背景下，控制组是分析中最受青睐的群体，不一定是白人或男性。还可能有其他原因使用并非白人或男性的控制组。选择用于偏见测试分析的控制或参考组是一项困难的任务，最好与法律、合规、社会科学专家或利益相关者共同完成。
- en: 'Once we do this division, we see the values in [Table 10-2](#performance_table_race_ratios).
    (We divide each column in the table by the value in the white row. That’s why
    the white values are all 1.0.) Now we can look for values outside of a certain
    range. We’ll use the four-fifths rule, which has no legal or regulatory standing
    when used this way, to help us identify one such range: 0.8–1.25, or a 20% difference
    between groups. (Some prefer a tighter range of acceptable values, especially
    in high-risk scenarios, say 0.9–1.11, indicating a 10% difference between groups.)
    When we see values above 1 for these disparity measures, it means the protected
    or minority group has a higher value of the original measure, and vice versa for
    values below 1.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们进行这种划分，我们就可以看到[表格 10-2](#performance_table_race_ratios)中的数值。（我们将表中每一列除以白人行中的值。这就是为什么白人的数值都是
    1.0。）现在我们可以寻找一定范围之外的数值。我们将使用四分之五法则，尽管在这种使用方式下，它没有法律或监管地位，但它可以帮助我们识别一个这样的范围：0.8–1.25，即群体之间的
    20% 差异。（在高风险场景中，有些人更喜欢使用更严格的可接受值范围，例如 0.9–1.11，表示群体之间的 10% 差异。）当我们看到这些不平等度量的值超过
    1 时，意味着受保护或少数群体具有原始度量的较高值，反之亦然，对于低于 1 的值也是如此。
- en: Looking at [Table 10-2](#performance_table_race_ratios), we see no out-of-range
    values for Asian people. This means that the model performs fairly equitably across
    white and Asian people. However, we do see glaring out-of-range values for Hispanic
    and Black people for precision, false positive rate, false discovery rate, and
    false omissions rate disparities. While applying the four-fifths rule can help
    us flag these values, it really can’t help us interpret them. For this, we’ll
    have to rely on our human brains to think through these results. We also need
    to remember that a decision of 1 from our model is a predicted default, and that
    higher probabilities mean default is more likely in the eyes of the model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[表格 10-2](#performance_table_race_ratios)，我们发现亚裔群体没有超出范围的数值。这意味着模型在白人和亚裔之间的表现相对公平。然而，我们确实看到了西班牙裔和黑人在精确度、假阳率、假发现率和漏检率上存在明显的超出范围的数值差异。虽然应用四分之五法则可以帮助我们标记这些数值，但它确实不能帮助我们解释它们。为此，我们将不得不依靠我们的人类大脑来思考这些结果。我们还需要记住，模型对于
    1 的决策是预测的默认值，而更高的概率意味着在模型眼中默认值更有可能发生。
- en: Table 10-2\. Performance-based bias measures across race groups for test data
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 10-2\. 测试数据中基于表现的偏见度量跨种族群体的结果
- en: '| Group | Prevalence disparity | Accuracy disparity | True positive rate disparity
    | Precision disparity | …​ | False positive rate disparity | False discovery rate
    disparity | False negative rate disparity | False omissions rate disparity |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 群体 | 流行率不均 | 准确度不均 | 真正率不均 | 精确度不均 | …​ | 假阳率不均 | 假发现率不均 | 假阴率不均 | 漏检率不均
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Hispanic | 3.730 | 0.875 | 1.357 | 2.157 | …​ | 1.696 | 0.486 | 0.683 | 3.461
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 西班牙裔 | 3.730 | 0.875 | 1.357 | 2.157 | …​ | 1.696 | 0.486 | 0.683 | 3.461
    |'
- en: '| Black | 3.612 | 0.868 | 1.351 | 2.078 | …​ | 1.784 | 0.522 | 0.688 | 3.378
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 黑人 | 3.612 | 0.868 | 1.351 | 2.078 | …​ | 1.784 | 0.522 | 0.688 | 3.378 |'
- en: '| White | 1.000 | 1.000 | 1.000 | 1.000 | …​ | 1.000 | 1.000 | 1.000 | 1.000
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 白人 | 1.000 | 1.000 | 1.000 | 1.000 | …​ | 1.000 | 1.000 | 1.000 | 1.000 |'
- en: '| Asian | 0.943 | 1.028 | 1.134 | 1.141 | …​ | 0.873 | 0.937 | 0.881 | 0.821
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 亚裔 | 0.943 | 1.028 | 1.134 | 1.141 | …​ | 0.873 | 0.937 | 0.881 | 0.821 |'
- en: 'Given that prevalence of defaults in the data is so much higher for Black and
    Hispanic people, one thing these results suggest is that our model learned more
    about defaults in these groups, and predicts defaults at a higher rate in these
    groups. Traditional testing in the next section will try to get at the underlying
    question of whether it’s fair to predict more defaults in these groups. For now,
    we’re trying to figure out if the performance of the model is fair. Looking at
    which measures are out-of-range for protected groups and what they mean, we can
    say the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于黑人和西班牙裔的数据中违约的普遍性要高得多，这些结果表明我们的模型更多地了解了这些群体的违约情况，并在这些群体中以更高的比率预测违约。下一节的传统测试将试图回答是否公平地预测这些群体中的更多违约的问题。目前，我们正在尝试确定模型的表现是否公平。通过查看哪些措施在受保护群体中超出范围以及它们的含义，我们可以说以下内容：
- en: 'Precision disparity: ~2× (more) correct default predictions, out of those *predicted
    to* default.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度差异：~2×（更多）正确的违约预测，在*预测会*违约的人群中。
- en: 'False positive rate disparity: ~1.5× (more) incorrect default predictions,
    out of those *that did not* default.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误阳性率差异：~1.5×（更多）不正确的违约预测，在*没有*违约的人群中。
- en: 'False discovery rate disparity: ~0.5× (fewer) incorrect default predictions,
    out of those *predicted to* default'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误发现率差异：~0.5×（更少）不正确的违约预测，在*预测会*违约的人群中。
- en: 'False omissions rate disparity: ~3.5× (more) incorrect acceptance predictions,
    out of those *predicted not to* default.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误遗漏率差异：~3.5×（更多）不正确的接受预测，在*预测不会*违约的人群中。
- en: Precision and false discovery rate have the same denominator—the smaller group
    of those predicted to default—and can be interpreted together. They show that
    this model has a higher rate of true positives for Black and Hispanic people relative
    to white people—meaning a higher rate of correct default predictions for this
    group. The false discovery rate echoes this result, pointing to a lower rate of
    false positives, or incorrect default decisions, for the minority groups in question.
    Relatedly, the false omissions rate shows our model makes incorrect acceptance
    decisions at a higher rate, out of the larger group comprised of those predicted
    not to default, for Black and Hispanic people. Precision, false discovery rate,
    and false omissions rate disparities show serious bias issues, but a bias that
    favors Black and Hispanic people in terms of model performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度和误发现率具有相同的分母——预测违约的较小群体——可以一起解释。它们表明，相对于白人，该模型对黑人和西班牙裔的真正阳性率较高——意味着对于这些群体，正确的违约预测率较高。误发现率也反映了这一结果，指出了对于所讨论的少数群体，错误阳性率或不正确的违约决策率较低的情况。相关地，误遗漏率显示，我们的模型在预测不会违约的较大群体中，对于黑人和西班牙裔的错误接受决策率较高。精确度、误发现率和误遗漏率的差异显示了严重的偏见问题，但这是一种有利于黑人和西班牙裔的模型表现的偏见。
- en: False positive rate disparity shows something a little different. The false
    positive rate is measured out of the larger group of those who did not default,
    in reality. In that group, we do see higher rates of incorrect default decisions,
    or false positives, for Black and Hispanic people. Taken together, all these results
    point to a model with bias problems, some of which genuinely appear to favor minority
    groups. Of these, the false positive disparity is most concerning. It shows us
    that out of the relatively large group of people who did not default, Black and
    Hispanic people are predicted to default incorrectly at 1.5× the rate of white
    people. This means that a lot of historically disenfranchised people are being
    wrongly denied credit-line increases by this model, which can lead to real-world
    harm. Of course, we also see evidence of correct and incorrect acceptance decisions
    favoring minorities. None of this is a great sign, but we need to dig into outcomes
    testing in the next section to get a clearer picture of group fairness in this
    model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 误阳性率差异显示了略有不同的内容。误阳性率是在实际中未违约的较大人群中测量的。在该群体中，我们看到黑人和西班牙裔的错误违约决策率或误阳性率较高。所有这些结果综合起来指向一个存在偏见问题的模型，其中一些确实似乎有利于少数群体。其中，误阳性差异最令人担忧。它告诉我们，在相对较大的未违约人群中，黑人和西班牙裔被预测会错误地违约的率是白人的1.5倍。这意味着历史上被剥夺权利的人们正在被这个模型错误地拒绝信用额度提升，这可能导致现实世界中的伤害。当然，我们也看到了对少数群体有利的正确和错误接受决策的证据。这一切都不是一个好兆头，但我们需要在下一节的结果测试中深入挖掘，以获得该模型中群体公平性的更清晰图景。
- en: Note
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: For regression models, we can skip the confusion matrices and proceed directly
    to comparing measures like R² or root mean squared error across groups. Where
    appropriate, and especially for bounded measures like R² or mean average percentage
    error (MAPE), we can also apply the four-fifths rule (as a rule of thumb) to a
    ratio of these measures to help spot problematic performance bias.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归模型，我们可以跳过混淆矩阵，直接比较诸如R²或均方根误差之类的测量指标在各组之间的差异。在适当的情况下，特别是对于像R²或平均百分比误差（MAPE）这样的有界测量，我们还可以应用四分之五法则（作为经验法则）来比较这些测量的比率，以帮助发现问题性能偏见。
- en: In general, performance testing is a helpful tool for learning about wrong and
    negative decisions, like false positives. More traditional bias testing that focuses
    on outcomes rather than performance has a more difficult time highlighting bias
    problems in wrong or negative decisions. Unfortunately, as we’re about to see,
    performance and outcomes testing can show different results. While some of these
    performance tests show a model that favors minorities, we’ll see in the next section
    that that’s not true. Rates standardize out the raw numbers of people and raw
    scores of the model in theoretically useful ways. A lot of the positive results
    we saw here are for fairly small groups of people. When we consider real-world
    outcomes, the picture of bias in our model is going to be different and more clear.
    These kinds of conflicts between performance testing and outcomes testing are
    common and well documented, and we’d argue that outcomes testing—aligned with
    legal standards and what happens in the real world—is more important.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，性能测试是了解错误和负面决策（如假阳性）的有益工具。更传统的偏见测试侧重于结果而不是性能，对错误或负面决策中的偏见问题更难以凸显。不幸的是，正如我们将要看到的，性能和结果测试可能会展示出不同的结果。虽然一些性能测试显示出支持少数族裔的模型，但在下一节中我们将看到这并不正确。率会以理论上有用的方式标准化人数和模型的原始分数。我们在这里看到的许多正面结果都是针对相当小的人群。当我们考虑现实世界的结果时，我们模型中的偏见图像将会不同且更为清晰。性能测试与结果测试之间的这些冲突是常见且有充分的文献记载，我们认为与法律标准和实际情况一致的结果测试更为重要。
- en: Note
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There is a well-known tension between improved performance in data that encodes
    historical biases—like most of the data we work with—and balancing outcomes across
    demographic groups. Data is always affected by systemic, human, and statistical
    biases. If we make outcomes more balanced, this tends to decrease performance
    metrics in a biased dataset.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码历史偏见的数据中改进性能与在各种人口统计群体中平衡结果之间存在着众所周知的紧张关系。数据总是受系统性、人为和统计偏见的影响。如果我们使结果更加平衡，这往往会降低偏见数据集中的性能指标。
- en: Because it’s difficult to interpret all these different performance measures,
    some may have more meaning in certain scenarios than others, and they are likely
    to be in conflict with each other or outcomes testing results, prominent researchers
    put together a [decision tree](https://oreil.ly/-y827) (slide 40) to help focus
    on a smaller subset of performance disparity measures. According to this tree,
    where our model is punitive (higher probability means default/reject decision),
    and the clearest harm is incorrectly denying credit line increases to minorities
    (intervention not warranted), the false positive rate disparity should probably
    carry the highest weight in our prediction performance analysis. The false positive
    rate disparity doesn’t tell a nice story. Let’s see what outcomes testing shows.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于解释所有这些不同的性能测量很困难，某些情况下可能比其他情况更具意义，并且它们可能会彼此或与结果测试结果相冲突，著名研究人员编制了一个[决策树](https://oreil.ly/-y827)（第40页），以帮助专注于性能差异测量的较小子集。根据此树，如果我们的模型是惩罚性的（较高的概率意味着默认/拒绝决策），并且最明显的伤害是错误地拒绝向少数族裔提供信用额度增加（干预不合理），则假阳性率差异可能应在我们的预测性能分析中占据最高权重。假阳性率差异并不能讲述一个好故事。让我们看看结果测试显示了什么。
- en: Traditional testing of outcomes rates
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传统的结果率测试
- en: 'The way we set up our analysis, based on a binary classification model, it
    was easiest to look at *performance* across groups first using confusion matrices.
    What’s likely more important, and likely more aligned to legal standards in the
    US, is to analyze differences in *outcomes* across groups, using traditional measures
    of statistical and practical significance. We’ll pair two well-known practical
    bias-testing measures, AIR and SMD, with chi-squared and *t*-tests, respectively.
    Understanding whether a discovered difference in group outcomes is statistically
    significant is usually a good idea, but in this case, it might also be a legal
    requirement. Statistically significant differences in outcomes or mean scores
    is one of the most common legally recognized measures of discrimination, especially
    in areas like credit lending, where algorithmic decision making has been regulated
    for decades. By using practical tests and effect size measures, like AIR and SMD,
    with statistical significance tests, we get two pieces of information: the magnitude
    of the observed difference, and whether it’s statistically significant, i.e.,
    likely to be seen again in other samples of data.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们基于二元分类模型设置分析的方式，首先通过混淆矩阵查看跨组性能最为简单。在美国，更重要的可能是分析*结果*在组间的差异，使用传统的统计和实际显著性度量。我们将两个众所周知的实用偏差测试指标AIR和SMD与卡方检验和*t*检验配对使用。理解发现的组别结果差异是否具有统计显著性通常是个不错的主意，但在这种情况下，这可能也是法律要求。结果或均分的统计显著差异是歧视的最常见法律承认度量之一，特别是在信用借贷等领域，算法决策已受到几十年的监管。通过使用实用测试和效果量测量，如AIR和SMD，与统计显著性测试，我们得到两个信息：观察到的差异的大小以及其是否具有统计显著性，即是否可能在其他数据样本中再次看到。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re working in a regulated vertical or in a high-risk application, it’s
    a good idea to apply traditional bias tests with legal precedent first before
    applying newer bias-testing approaches. Legal risks are often the most serious
    organizational risks for many types of ML-based products, and laws are designed
    to protect users and stakeholders.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在受监管的行业或高风险应用中工作，建议首先应用传统偏差测试和法律先例，然后再应用较新的偏差测试方法。法律风险通常是许多基于ML的产品中最严重的组织风险，法律旨在保护用户和利益相关者。
- en: AIR is often applied to categorical outcomes, like credit lending or hiring
    outcomes, where someone either receives a positive outcome or not. AIR is defined
    as the rate of positive outcomes for a protected group, like minorities or women,
    divided by the same rate of positive outcomes for a control group, like white
    people or men. According to the four-fifths rule, we look for the AIR to be above
    0.8\. An AIR below 0.8 points to a serious problem. We then test whether this
    difference will probably be seen again or if it’s due to chance using a chi-squared
    test.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: AIR通常应用于分类结果，如信用借贷或招聘结果，即某人是否获得积极结果。AIR定义为受保护群体（如少数民族或女性）的积极结果率，除以控制组（如白人或男性）的相同积极结果率。根据四分之五法则，我们希望AIR在0.8以上。AIR低于0.8指向严重问题。然后，我们使用卡方检验测试这种差异是否可能再次出现，或者是否仅仅是由于偶然性。
- en: Note
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Impact ratios can also be used for regression models by dividing average scores
    or percentage of scores over the median score for a protected group by the same
    quantity for a control group, and applying the four-fifths rule as a guideline
    for identifying problematic results. Other traditional bias measurement approaches
    for regression models are *t*-tests and SMD.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 再者，影响比例也可用于回归模型，方法是通过受保护组的平均分数或百分比除以控制组的相同数量，并应用四分之五法则作为识别问题结果的指导方针。对于回归模型的其他传统偏差测量方法包括*t*检验和SMD。
- en: While AIR and chi-squared are most often used with binary classification, SMD
    and *t*-tests are often used on predictions from regression models, or on numeric
    quantities like wages, salaries, or credit limits. We’ll apply SMD and *t*-tests
    to our model’s predicted probabilities for demonstration purposes and to get some
    extra information about bias in our model. SMD is defined as the mean score for
    a protected group minus the mean score for a control group, with that quantity
    divided by a measure of the standard deviation of the score. SMD has well-known
    cutoffs at magnitudes of 0.2, 0.5, and 0.8 for small, medium, and large differences,
    respectively. We’ll use a *t*-test to decide whether the effect size measured
    by SMD is statistically significant.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然AIR和卡方检验最常用于二元分类，SMD和*t*检验常用于回归模型的预测或像工资、薪水或信用额这样的数值数量。我们将应用SMD和*t*检验到我们模型的预测概率上，以进行演示并获取关于我们模型偏见的额外信息。SMD定义为受保护群体的平均分数减去对照组的平均分数，然后除以分数的标准偏差的测量值。对于小、中和大差异，SMD有着广为人知的0.2、0.5和0.8的截止点。我们将使用*t*检验来决定SMD测量的效果大小是否具有统计学意义。
- en: Note
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This application of SMD—applied to the probabilities output by the model—would
    also be appropriate if the model scores would be fed into some downstream decision-making
    process, and it is impossible to generate model outcomes at the time of bias testing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: SMD的应用——应用于模型输出的概率——如果模型分数将被馈送到某些下游决策过程中，且在偏见测试时无法生成模型结果，那么也是合适的。
- en: In addition to significance tests, AIR, and SMD, we’ll also be analyzing basic
    descriptive statistics like counts, means, and standard deviations, as can be
    seen in [Table 10-3](#outcomes_table_race). When looking over [Table 10-3](#outcomes_table_race),
    it’s clear that there is a big difference in scores for Black and Hispanic people
    versus scores for white and Asian people. While our data is simulated, very sadly,
    this is not atypical in US consumer finance. Systemic bias is real, and fair lending
    data tends to prove it.^([1](ch10.html#idm45989998395696))
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 除了显著性测试、AIR和SMD之外，我们还将分析基本的描述性统计数据，如计数、平均值和标准偏差，正如在[表10-3](#outcomes_table_race)中所见。当审视[表10-3](#outcomes_table_race)时，很明显，黑人和西班牙裔与白人和亚裔的分数有很大差异。尽管我们的数据是模拟的，但非常遗憾，这在美国消费金融中并不罕见。系统性偏见是真实存在的，并且公平借贷数据往往证明了这一点。^([1](ch10.html#idm45989998395696))
- en: Table 10-3\. Traditional outcomes-based bias metrics across race groups for
    test data
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-3。对测试数据中种族群体的传统基于结果的偏见指标
- en: '| Group | Count | Favorable outcomes | Favorable rate | Mean score | Std. dev.
    score | AIR | AIR *p*-value | SMD | SMD *p*-value |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 组别 | 计数 | 有利的结果 | 有利率 | 平均分数 | 标准偏差分数 | AIR | AIR *p*-值 | SMD | SMD *p*-值
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Hispanic | 989 | 609 | 0.615 | 0.291 | 0.205 | 0.736 | 6.803e–36 | 0.528
    | 4.311e–35 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 西班牙裔 | 989 | 609 | 0.615 | 0.291 | 0.205 | 0.736 | 6.803e–36 | 0.528 | 4.311e–35
    |'
- en: '| Black | 993 | 611 | 0.615 | 0.279 | 0.199 | 0.735 | 4.343e–36 | 0.482 | 4.564e–30
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 黑人 | 993 | 611 | 0.615 | 0.279 | 0.199 | 0.735 | 4.343e–36 | 0.482 | 4.564e–30
    |'
- en: '| Asian | 1485 | 1257 | 0.846 | 0.177 | 0.169 | 1.012 | 4.677e–01 | –0.032
    | 8.162e–01 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 亚裔 | 1485 | 1257 | 0.846 | 0.177 | 0.169 | 1.012 | 4.677e–01 | –0.032 | 8.162e–01
    |'
- en: '| White | 1569 | 1312 | 0.836 | 0.183 | 0.172 | 1.000 | - | 0.000 | - |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 白人 | 1569 | 1312 | 0.836 | 0.183 | 0.172 | 1.000 | - | 0.000 | - |'
- en: In [Table 10-3](#outcomes_table_race), it’s immediately obvious that Black and
    Hispanic people have higher mean scores and lower favorable rates than white and
    Asian people, while all four groups have similar standard deviations for scores.
    Are these differences big enough to be a bias problem? That’s where our practical
    significance tests come in. AIR and SMD are both calculated in reference to white
    people. That’s why white people have scores of 1.0 and 0.0 for these, respectively.
    Looking at AIR, both Black and Hispanic AIRs are below 0.8\. Big red flag! SMDs
    for those two groups are around 0.5, meaning a medium difference in scores between
    groups. That’s not a great sign either. We’d like for those SMD values to be below
    or around 0.2, signifying a small difference.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表10-3](#outcomes_table_race)中，很明显，黑人和西班牙裔人的平均分较高，有利率较低，而白人和亚裔人的情况相反，所有四个群体的分数标准差类似。这些差异是否足够大到构成偏见问题？这就是我们的实际意义测试的用武之地。AIR和SMD都是相对于白人计算的。这就是为什么白人的得分分别为1.0和0.0。观察AIR，黑人和西班牙裔的AIR都低于0.8。大红灯！这两组的SMD约为0.5，意味着群体之间分数的中等差异。这也不是个好兆头。我们希望这些SMD值能够低于或接近0.2，表示小差异。
- en: Warning
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'AIR is often misinterpreted by data scientists. Here’s a simple way to think
    of it: An AIR value above 0.8 doesn’t mean much, and it certainly doesn’t mean
    a model is fair. However, AIR values below 0.8 point to a serious problem.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家经常误解AIR。这里有一个简单的思考方式：AIR 值超过 0.8 并不意味着太多，而且这当然也不意味着一个模型是公平的。然而，AIR 值低于
    0.8 指向了一个严重的问题。
- en: The next question we might ask in a traditional bias analysis is whether these
    practical differences for Black and Hispanic people are statistically significant.
    Bad news—they are very significant, with *p*-values approaching 0 in both cases.
    While datasets have exploded in size since the 1970s, a lot of legal precedent
    points to statistical significance at the 5% level (*p* = 0.05) for a two-sided
    hypothesis test as a marker of legally impermissible bias. Since this threshold
    is completely impractical for today’s large datasets, we recommend adjusting *p*-value
    cutoffs lower for larger datasets. However, we should also be prepared to be judged
    at *p* = 0.05 in regulated verticals of the US economy. Of course, fair lending
    and employment discrimination cases are anything but straightforward, and facts,
    context, and expert witnesses have as much to do with a final legal determination
    as any bias-testing number. An important takeaway here is that the law in this
    area is already established, and not as easily swayed by AI hype as internet and
    media discussions. If we’re operating in a high-risk space, we should probably
    conduct traditional bias tests in addition to newer tests, as we’ve done here.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统偏见分析中，我们接下来可能会问的一个问题是，对于黑人和西班牙裔来说，这些实际差异在统计上是否显著。不幸的消息是，它们在这两种情况下都非常显著，*p*
    值接近于 0。尽管自上世纪70年代以来数据集规模大幅扩展，但许多法律先例指出，对于双侧假设检验的统计显著性水平为 5%（*p* = 0.05），这是法律上不可容忍的偏见的标志。由于这个阈值对于当今大型数据集完全不切实际，我们建议为较大的数据集调整*p*值截断值。然而，在美国经济的受监管行业中，我们也应该准备好在*p*
    = 0.05的水平上进行评判。当然，在公平借贷和就业歧视案件中，并不是一切都那么简单明了，事实、背景和专家证人与最终法律裁决有着同等重要的关系，正如任何偏见测试数据一样。这里的一个重要经验教训是，这一领域的法律已经确立，不像互联网和媒体讨论那样容易被人工智能炒作所左右。如果我们在高风险领域操作，除了进行新的测试外，我们可能还应该进行传统的偏见测试，就像我们在这里做的一样。
- en: Warning
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: In consumer finance, housing, employment, and other traditionally regulated
    verticals of the US economy, nondiscrimination law is highly mature and not swayed
    by AI hype. Just because AIR and two-sided statistical tests feel outdated or
    simplistic to data scientists, does not mean our organizations won’t be judged
    by these standards if legal issues arise.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费金融、住房、就业和美国经济的其他传统受监管行业，非歧视法律已经非常成熟，并不会受人工智能炒作的影响。仅仅因为AIR和双侧统计测试对于数据科学家而言感觉过时或简单化，并不意味着我们的组织在法律问题出现时不会按照这些标准来评判。
- en: These race results point to a fairly serious discrimination problem in our model.
    If we were to deploy it, we’d be setting ourselves up for potential regulatory
    and legal problems. Worse than that, we’d be deploying a model we know perpetuates
    systemic biases and harms people. Getting an extension on a credit card can be
    a serious thing at different junctures in our lives. If someone is asking for
    credit, we should assume it’s genuinely needed. What we see here is that an example
    credit-lending decision is tinged with historical biases. These results also send
    a clear message. This model needs to be fixed before it’s deployed.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些种族结果指出了我们模型中相当严重的歧视问题。如果我们部署它，我们将为自己设定潜在的监管和法律问题。比这更糟糕的是，我们将部署一个我们知道会持续制度性偏见并伤害人们的模型。在我们生活的不同阶段，申请信用卡的延期可能是一件严肃的事情。如果有人要求信用，我们应该假设这是真正需要的。我们在这里看到的是，一个信用借贷决策的例子带有历史偏见的色彩。这些结果也传递了一个明确的信息。在部署之前，这个模型需要修复。
- en: Individual Fairness
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 个体公平性
- en: 'We’ve been focused on group fairness thus far, but we should also probe our
    model for individual fairness concerns. Unlike bias against groups, individual
    bias is a local issue that affects only a small and specific group of people,
    down to a single individual. There are two main techniques we’ll use to test this:
    residual analysis and adversarial modeling. In the first technique—residual analysis—we
    look at individuals very close to the decision cutoff and who incorrectly received
    unfavorable outcomes as a result. We want to make sure their demographic information
    isn’t pushing them into being denied for a credit product. (We can check very
    wrong individual outcomes far away from the decision cutoff too.) In the second
    approach—adversarial models—we’ll use separate models that try to predict protected
    group information using the input features and the scores from our original model,
    and we’ll look at those model’s Shapley additive explanations. When we find rows
    where adversarial predictions are very accurate, this is a hint that something
    in that row is encoding information that leads to bias in our original model.
    If we can identify what that something is across more than a few rows of data,
    we’re on the path to identifying potential drivers of proxy bias in our model.
    We’ll look into individual bias and then proxy bias before transitioning to the
    bias-remediation section of the chapter.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于群体公平性，但我们也应该对我们的模型进行个体公平性方面的探索。与对群体的偏见不同，个体偏见是一个局部问题，仅影响一个小而具体的人群，甚至可以是单个个体。我们将使用两种主要技术来测试这一点：残差分析和对抗建模。在第一种技术——残差分析中，我们查看非常接近决策界限且由于此而错误地获得不利结果的个体。我们希望确保他们的人口统计信息不会导致他们被拒绝信用产品。（我们也可以检查决策界限远处的非常错误的个体结果。）在第二种方法——对抗模型中，我们将使用单独的模型，尝试使用输入特征和原始模型的分数来预测受保护群体信息，并查看这些模型的Shapley加法解释。当我们发现对抗预测非常准确的行时，这表明该行中的某些信息编码导致我们原始模型中的偏见。如果我们能够在多行数据中确定这些信息，我们就可以找出模型中潜在的代理偏见的驱动因素。在转向章节中的偏见修复部分之前，我们将探讨个体偏见和代理偏见。
- en: 'Let’s dive into individual fairness. First, we wrote some code to pull out
    a few narrowly misclassified people from a protected group. These are observations
    that our model predicted would go delinquent, but they did not:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨个体公平性。首先，我们编写了一些代码，以从受保护群体中取出一些狭义上被错误分类的人员。这些是我们的模型预测会逾期的观察结果，但实际上没有逾期：
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The results are shown in [Table 10-4](#misclassified_observations), and they
    don’t suggest any egregious bias, but they do raise some questions. The first
    and third applicants appear to spending moderately and making payments on time
    for the most part. These individuals may have been placed on the wrong side of
    a decision boundary in an arbitrary manner. However, the individual in the second
    row of [Table 10-4](#misclassified_observations) appears not to be making progress
    on repaying their credit card debt. Perhaps they really should not have been approved
    for an increased line of credit.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[表 10-4](#misclassified_observations)中，并且它们并未暗示任何严重的偏见，但确实提出了一些问题。第一和第三个申请人似乎适度消费，并且大部分时间按时还款。这些个体可能以一种任意的方式被置于决策边界的错误一侧。然而，[表 10-4](#misclassified_observations)中第二行的个体似乎未能在偿还信用卡债务方面取得进展。也许他们确实不应该被批准增加信用额度。
- en: Table 10-4\. A subset of features for narrowly misclassified protected observations
    in validation data
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-4. 在验证数据中被狭义错误分类的受保护观察特征子集
- en: '| `LIMIT​_BAL` | `PAY_0` | `PAY_2` | `PAY_3` | …​ | `BILL​_AMT1` | `BILL​_AMT2`
    | `BILL​_AMT3` | …​ | `PAY​_AMT1` | `PAY​_AMT2` | `PAY​_AMT3` |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| `LIMIT​_BAL` | `PAY_0` | `PAY_2` | `PAY_3` | …​ | `BILL​_AMT1` | `BILL​_AMT2`
    | `BILL​_AMT3` | …​ | `PAY​_AMT1` | `PAY​_AMT2` | `PAY​_AMT3` |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| $58,000 | –1 | –1 | –2 | …​ | $600 | $700 | $0 | …​ | $200 | $700 | $0 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| $58,000 | –1 | –1 | –2 | …​ | $600 | $700 | $0 | …​ | $200 | $700 | $0 |'
- en: '| $58,000 | 0 | 0 | 0 | …​ | $8,500 | $5,000 | $0 | …​ | $750 | $150 | $30
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| $58,000 | 0 | 0 | 0 | …​ | $8,500 | $5,000 | $0 | …​ | $750 | $150 | $30
    |'
- en: '| $160,000 | –1 | –1 | –1 | …​ | $0 | $0 | $600 | …​ | $0 | $0 | $0 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| $160,000 | –1 | –1 | –1 | …​ | $0 | $0 | $600 | …​ | $0 | $0 | $0 |'
- en: 'Next steps to uncovering whether we’ve found a real individual bias problem
    might include the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 揭示我们是否发现了真正的个体偏见问题的下一步可能包括以下几点：
- en: Small perturbations of input features
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输入特征的微小扰动
- en: If some arbitrary change to an input feature, say decreasing `BILL_AMT1` by
    $5, changes the outcome for this person, then the model’s decision may be more
    related to a steep place in its response function intersecting with the decision
    cutoff than any tangible real-world reason.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对一个输入特征进行任意更改，例如将`BILL_AMT1`减少5美元，会改变这个人的结果，那么模型的决策可能更与其响应函数的陡峭处与决策截止点相交有关，而不是任何切实的现实原因。
- en: Searching for similar individuals
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索相似的个体
- en: If there are a handful—or more—individuals like the current individual, the
    model maybe segmenting some specific or intersectional subpopulation in an unfair
    or harmful way.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果像当前这个个体一样，有一小部分——或更多——个体，模型可能会以不公平或有害的方式对某些特定或交叉的亚群体进行分割。
- en: If either of these are the case, the right thing to do may be to extend this
    and similar individual’s line(s) of credit.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出现这两种情况之一，正确的做法可能是扩展这个以及类似个体的信用额度。
- en: We conducted a similar analysis for Hispanic and Asian observations and found
    similar results. We weren’t too surprised by these results, for at least two reasons.
    First, individual fairness questions are difficult and bring up issues of causality
    which ML systems tend not to address in general. Second, individual fairness and
    proxy discrimination are probably much larger risks for datasets with many rows—where
    entire subpopulations may end up on an arbitrary side of a decision boundary—and
    when a model contains many features, and especially *alternative data,* or features
    not directly linked to one’s ability to repay credit, that may otherwise enhance
    the predictiveness of the model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对西班牙裔和亚裔的观察进行了类似的分析，并得出类似的结果。对于这些结果，我们并不感到太惊讶，至少有两个原因。首先，个体公平性问题是困难的，并引发因果性问题，而机器学习系统通常不会在一般情况下解决这些问题。其次，个体公平性和代理歧视可能对数据集中的许多行——其中整个亚群体可能会落入决策边界的任意一侧——以及模型包含许多特征，特别是*替代数据*或与个体偿还能力无直接联系的特征，构成更大风险。
- en: Note
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Answering questions about individual fairness with 100% certainty is difficult,
    because they’re fundamentally *causal* questions. For complex, nonlinear ML models,
    it’s impossible to know whether a model made a decision on the basis of some piece
    of data (i.e., protected group information) that isn’t included in the model in
    the first place.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对个体公平性问题作出100%确定的答案是困难的，因为它们基本上是*因果*性问题。对于复杂、非线性的机器学习模型，要知道模型是否基于一些数据（即受保护的群体信息），而这些数据在模型中本来就不存在，是不可能的。
- en: That said, residual analysis, adversarial modeling, SHAP values, and the careful
    application of subject matter expertise can go a long way. For more reading on
    this subject, check out [“Explaining Quantitative Measures of Fairness”](https://oreil.ly/Tg66Z)
    from the creator of SHAP values, and [“On Testing for Discrimination Using Causal
    Models”](https://oreil.ly/IiP9W).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，残差分析、对抗建模、SHAP值以及主题专业知识的精心应用可以有很大帮助。想要进一步阅读这方面的内容，请查阅由SHAP值创始人撰写的《解释公平性的定量指标》[“Explaining
    Quantitative Measures of Fairness”](https://oreil.ly/Tg66Z)，以及《使用因果模型测试歧视问题》[“On
    Testing for Discrimination Using Causal Models”](https://oreil.ly/IiP9W)。
- en: 'Let’s move on to the second technique for testing individual fairness: adversarial
    modeling. We chose to train two adversarial models. The first model takes in the
    same input features as the original model, but attempts to predict protected groups’
    statuses rather than delinquencies. For simplicity, we trained a binary classifier
    on a target for protected class membership—a new marker for `Black` or `Hispanic`
    people. By analyzing this first adversarial model, we can get a good idea of which
    features have the strongest relationships with protected demographic group membership.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们来介绍测试个体公平性的第二种技术：对抗建模。我们选择训练两个对抗模型。第一个模型使用与原始模型相同的输入特征，但试图预测受保护群体的状态，而不是逾期情况。为简单起见，我们对受保护类成员身份——新的`Black`或`Hispanic`人的标记——进行了二元分类器的训练。通过分析这个第一个对抗模型，我们可以大致了解哪些特征与受保护人群的成员身份有最强的关系。
- en: The second adversarial model we train is exactly like the first, except it gets
    one additional input feature—the output probabilities of our original lending
    model. By comparing the two adversarial models, we will get an idea of how much
    *additional* information was encoded in the original model scores. And we’ll get
    this information at the observation level.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练的第二个对抗模型与第一个完全相同，只是多了一个输入特征——原始放贷模型的输出概率。通过比较这两个对抗模型，我们可以了解到原始模型分数中编码的额外信息有多少，并且我们将在观察级别上得到这些信息。
- en: Note
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Many ML tools that generate *row-by-row* debugging information—like residuals,
    adversarial model predictions, or SHAP values—can be used for examining individual
    bias issues.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 许多生成*逐行*调试信息（如残差、对抗模型预测或SHAP值）的机器学习工具，可用于检查个体偏见问题。
- en: We trained these adversarial models as binary XGBoost classifiers with similar
    hyperparameters to the original model. First, we took a look at the protected
    observations whose adversarial model scores increased the most when the original
    model probabilities were added as a feature. The results are shown in [Table 10-5](#adversarial_results).
    This table is telling us that for some observations, the original model scores
    are encoding enough information about protected group status that the second adversarial
    model is able to improve on the first by around 30 percentage points. These results
    tells us that we should take a deeper look into these observations, in order to
    identify any individual fairness problems by asking questions like we did for
    individual bias issues spotted with residuals. [Table 10-5](#adversarial_results)
    also helps us show again that removing *demographic markers* from a model does
    not remove *demographic information* from a model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用与原始模型类似的超参数作为二进制XGBoost分类器训练了这些对抗模型。首先，我们查看了在将原始模型概率添加为特征时，其对抗模型分数增加最多的受保护观察。结果显示在[表
    10-5](#adversarial_results)中。该表告诉我们，对于某些观察结果，原始模型分数编码了足够关于受保护组状态的信息，以至于第二个对抗模型能够在第一个模型基础上提高约30个百分点。这些结果告诉我们，我们应该深入研究这些观察结果，以识别任何个体公平性问题，例如我们在残差中发现的个体偏见问题时提出的问题。[表
    10-5](#adversarial_results)还帮助我们再次表明，从模型中移除*人口统计标记*并不会去除*人口统计信息*。
- en: Table 10-5\. The three protected observations that saw their scores increase
    the most between the two adversarial models in validation data
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-5\. 在验证数据中两个对抗模型之间看到其分数增加最多的三个受保护观察
- en: '| Observation | Protected | Adversary 1 score | Adversary 2 score | Difference
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 观察 | 受保护 | 对抗者 1 分数 | 对抗者 2 分数 | 差异 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 9022 | 1 | 0.288 | 0.591 | 0.303 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 9022 | 1 | 0.288 | 0.591 | 0.303 |'
- en: '| 7319 | 1 | 0.383 | 0.658 | 0.275 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 7319 | 1 | 0.383 | 0.658 | 0.275 |'
- en: '| 528 | 1 | 0.502 | 0.772 | 0.270 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 528 | 1 | 0.502 | 0.772 | 0.270 |'
- en: Recall from [Chapter 2](ch02.html#unique_chapter_id_2) that SHAP values are
    a row-by-row additive feature attribution scheme. That is, they tell us how much
    each feature in a model contributed to the overall model prediction. We computed
    the SHAP values on validation data for the second adversarial model (the one that
    includes our original model scores). In [Figure 10-2](#shaps), we took a look
    at the distribution of SHAP values for the top four most important features. Each
    of the features in [Figure 10-2](#shaps) is important to predicting protected
    class membership. Coming in as the most important feature for predicting protected
    group information is the original model scores, `p_DELINQ_NEXT`. This is interesting
    in and of itself, and the observations that have the highest SHAP values for this
    feature are good targets to investigate further for individual fairness violations.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第二章](ch02.html#unique_chapter_id_2)回想起，SHAP值是一种逐行加法特征归因方案。也就是说，它们告诉我们模型中每个特征对整体模型预测的贡献有多少。我们在第二个对抗模型（包括我们的原始模型分数）的验证数据上计算了SHAP值。在[图
    10-2](#shaps)中，我们查看了排名前四的最重要特征的SHAP值分布。[图 10-2](#shaps)中的每个特征对预测受保护类成员信息都很重要。作为预测受保护组信息最重要的特征，是原始模型分数，`p_DELINQ_NEXT`。这本身就很有趣，而且具有最高SHAP值的观察结果是进一步研究个体公平性违规的良好目标。
- en: '![mlha 1002](assets/mlha_1002.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 1002](assets/mlha_1002.png)'
- en: Figure 10-2\. The distribution of SHAP values for the four most important features
    in our adversarial model in validation data ([digital, color version](https://oreil.ly/n4z9i))
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 我们对验证数据中我们对抗模型中四个最重要特征的SHAP值分布（[数字，彩色版本](https://oreil.ly/n4z9i)）
- en: Maybe most interesting is the color gradient (change from light to dark) within
    the `p_DELINQ_NEXT` violin. Each violin is colored by the value of the feature
    itself for each observation in the density. That means that if our model was linear
    with no interactions, the color gradient across each violin would be smooth from
    light to dark. But that’s not what we observed. Within the `p_DELINQ_NEXT` violin,
    there is significant color variation within vertical slices of the plot. This
    can only arise when `p_DELINQ_NEXT` is being used by the model in conjunction
    with other features in order to drive the predictions. For example, the model
    might be learning something like *if `LIMIT_BAL` is below $20,000 and if credit
    utilization is above 50% and if the delinquency probability from the credit extension
    model is above 20% then the observation is likely to be a Black or Hispanic person.*
    While residuals and adversarial models can help us identify individual bias issues,
    SHAP can take us a step further by helping us understand what is driving that
    bias.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 或许最有趣的是`p_DELINQ_NEXT`小提琴图中的颜色渐变（从浅到深）。每个小提琴根据密度中每个观察值的特征值来着色。这意味着，如果我们的模型是线性的且没有交互作用，那么每个小提琴上的颜色渐变将会从浅到深是平滑的。但这不是我们观察到的情况。在`p_DELINQ_NEXT`小提琴内部，图的垂直切片内存在显著的颜色变化。这只能在`p_DELINQ_NEXT`与其他特征结合使用以驱动预测时发生。例如，模型可能会学习到类似于*如果`LIMIT_BAL`低于$20,000，信用利用率高于50%，并且来自信用扩展模型的违约概率超过20%，那么观察结果很可能是黑人或西班牙裔*的情况。尽管残差和对抗模型可以帮助我们识别个体偏差问题，但SHAP可以通过帮助我们理解驱动偏差的因素来进一步推进。
- en: Proxy Bias
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理偏差
- en: If the patterns like those we’ve identified only affect a few people, they can
    still be harmful. But when we see them affecting larger groups of people, we likely
    have a more global proxy bias issue on our hands. Remember that proxy bias happens
    when a single feature or a group of interacting features act like demographic
    information in our model. Given that ML models can often mix and match features
    to create latent concepts—and can do so in different ways on local, row-by-row
    bases—proxy bias is a fairly common driver of biased model outputs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们识别出的这些模式只影响少数人，它们仍然可能是有害的。但是当我们看到它们影响到更大群体时，我们可能面临一个更为全球性的代理偏差问题。请记住，代理偏差发生在当一个单一特征或一组相互作用的特征在我们的模型中起到类似人口统计信息的作用时。鉴于机器学习模型通常可以混合和匹配特征以创建潜在概念，并且可以在本地逐行基础上以不同方式进行此操作，代理偏差是导致模型输出偏差的一个相当常见的因素。
- en: Many of the tools we’ve discussed, like adversarial models and SHAP, can be
    used to hunt down proxies. We could begin to get at them by looking at, for example,
    SHAP feature interaction values. (Recall advanced SHAP techniques from Chapters
    [2](ch02.html#unique_chapter_id_2) and [6](ch06.html#unique_chapter_id_6).) The
    bottom-line test for proxies may be adversarial models. If another model can accurately
    predict demographic information from our model’s predictions, then our model encodes
    demographic information. If we include model input features in our adversarial
    models, we can use feature attribution measures to understand which single input
    features might be proxies, and apply other techniques and elbow grease to find
    proxies created by interactions. Good, old-fashioned decision trees can be some
    of the best adversarial models for finding proxies. Since ML models tend to combine
    and recombine features, plotting a trained adversarial decision tree may help
    us uncover more complex proxies.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论过的许多工具，比如对抗模型和SHAP，可以用来查找代理。例如，我们可以通过查看SHAP特征交互值来开始探索它们。（回忆一下第[2](ch02.html#unique_chapter_id_2)章和第[6](ch06.html#unique_chapter_id_6)章的高级SHAP技术。）对于代理来说，最核心的测试可能是对抗模型。如果另一个模型能够准确地从我们模型的预测中预测出人口统计信息，那么我们的模型就对人口统计信息进行了编码。如果我们在对抗模型中包括模型输入特征，我们可以使用特征归因措施来理解哪些单一输入特征可能是代理，并应用其他技术和辛勤工作来找出由交互创建的代理。好的老式决策树可以是发现代理的最佳对抗模型之一。由于机器学习模型倾向于结合和重新组合特征，绘制训练有素的对抗决策树可能有助于我们发现更复杂的代理。
- en: As readers can see, adversarial modeling can be a rabbit hole. But we hope we’ve
    convinced readers that it is a powerful tool for identifying individual rows that
    might be subject to discrimination under our models, and for understanding how
    our input features relate to protected group information and proxies. Now we’re
    going to move on to the important job of remediating the bias we found in our
    example lending model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如读者所见，对抗建模可能是一个兔子洞。但我们希望我们已经说服读者，它是一个强大的工具，用于识别可能在我们的模型下受到歧视的个体行，并且用于理解我们的输入特征如何与受保护群体信息和代理有关。现在，我们将继续进行重要的工作，即解决我们在示例信贷模型中发现的偏差问题。
- en: Remediating Bias
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修复偏差
- en: Now that we’ve identified several types of bias in our model, it’s time to roll
    up our sleeves and try to remediate it. Luckily, there are many tools to choose
    from and, due to the Rashomon effect, many different models to choose from too.
    We’ll try preprocessing remediation first. We’ll generate observation-level weights
    for our training data so that positive outcomes appear equally likely across demographic
    groups. We’ll then try an in-processing technique, sometimes known as *fair XGBoost*,
    in which demographic information is included in XGBoost’s gradient calculation
    so that it can be regularized during model training. For postprocessing, we’ll
    update our predictions around the decision boundary of the model. Since pre-,
    in-, and postprocessing may give rise to concerns about disparate treatment in
    several industry verticals and applications, we’ll close out the remediation section
    by outlining a simple and effective technique for model selection that searches
    over various input feature sets and hyperparameter settings to find a model with
    good performance and minimal bias. For each approach, we’ll also address any observed
    performance quality and bias-remediation trade-offs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经确定了模型中几种偏差类型，是时候动手解决了。幸运的是，有很多工具可供选择，并且由于拉什蒙效应，还有许多不同的模型可供选择。我们将首先尝试预处理修复。我们将为训练数据生成观测级别的权重，以便积极结果在人口统计组之间表现一致。然后，我们将尝试一种称为*公平
    XGBoost*的处理技术，其中包括人口统计信息在 XGBoost 的梯度计算中，以便在模型训练期间进行正规化。对于后处理，我们将在模型的决策边界周围更新我们的预测。由于预处理、处理和后处理可能在多个行业垂直和应用程序中引起不平等待遇的担忧，我们将通过概述一种简单有效的模型选择技术来关闭修复部分，该技术搜索各种输入特征集和超参数设置，以找到性能良好且偏差最小的模型。对于每种方法，我们还将讨论观察到的性能质量和偏差修复的权衡。
- en: Preprocessing
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理
- en: The first bias-remediation technique we’ll try is a preprocessing technique
    known as *reweighing*. It was published first by Faisal Kamiran and Toon Calders
    in their 2012 paper, [“Data Preprocessing Techniques for Classification Without
    Discrimination”](https://oreil.ly/lAj08). The idea of reweighing is to make the
    average outcome across groups equal using observation weights and then retrain
    the model. As we’ll see, before we preprocessed the training data, the average
    outcome, or average `y` variable value, is quite different across demographic
    groups. The biggest difference was for Asian and Black people, with average outcomes
    of 0.107 and 0.400, respectively. This means that on average, and looking only
    at the training data, Asian people’s probability of default was well within the
    range of being accepted for a credit-line increase, while the opposite was true
    for Black people. Their average score was solidly in the decline range. (Again,
    these values are not always objective or fair simply because they are recorded
    in digital data.) After we preprocess, we’ll see we can balance out both outcomes
    and bias-testing values to a notable degree.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试的第一种偏差修复技术是一种称为*重新加权*的预处理技术。这种技术最早由 Faisal Kamiran 和 Toon Calders 在他们2012年的论文["无歧视分类的数据预处理技术"](https://oreil.ly/lAj08)中首次发表。重新加权的思想是使用观察权重使各组的平均结果相等，然后重新训练模型。正如我们将看到的，在我们对训练数据进行预处理之前，跨人口统计组的平均结果或平均`y`变量值有很大的差异。亚裔和黑人的差异最大，分别为0.107和0.400的平均结果。这意味着从平均角度来看，仅仅通过训练数据来看，亚裔人的违约概率在可以接受信用额度增加的范围内，而黑人则相反。他们的平均分数显然处于拒绝范围之内。
    （再次强调，这些值并不总是客观或公平，仅仅因为它们记录在数字数据中。）经过我们的预处理之后，我们将看到我们可以在相当大的程度上平衡这两种结果和偏差测试值。
- en: Since reweighing is a very straightforward approach, we decided to implement
    it ourselves with the function in the following code snippet.^([2](ch10.html#idm45989998102272))
    To reweigh our data, we first need to measure average outcome rates—overall and
    for each demographic group. Then we determine observation-level, or row-level,
    weights that balance out the outcome rate across demographic groups. Observation
    weights are numeric values that tell XGBoost, and most other ML models, how much
    to weigh each row during training. If a row has a weight of 2, it’s like that
    row appears twice in the objective function used to train XGBoost. If we tell
    XGBoost that a row has a weight of 0.2, it’s like that row appears one-fifth of
    the times it actually does in the training data. Given the average outcome for
    each group and their frequency in the training data, it’s a basic algebra problem
    to determine the row weights that give all groups the same average outcome in
    the model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于重新加权是一个非常直接的方法，我们决定使用以下代码片段中的函数自己实现它。^([2](ch10.html#idm45989998102272)) 要重新加权我们的数据，我们首先需要测量整体和每个人口群体的平均结果率。然后我们确定观测级别或行级别的权重，以平衡跨人口群体的结果率。观测权重是数值，告诉XGBoost和大多数其他ML模型在训练期间如何加权每一行。如果一行的权重为2，则像这一行在用于训练XGBoost的目标函数中出现了两次。如果我们告诉XGBoost一行的权重为0.2，则像这一行在训练数据中出现了五分之一的次数。鉴于每个群体的平均结果和它们在训练数据中的频率，确定给出所有群体在模型中具有相同平均结果的行权重是一个基本的代数问题。
- en: '[PRE5]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are multiple kinds of sample weights. In XGBoost, and in most other ML
    models, observation-level weights are interpreted as frequency weights, where
    the weight on an observation is equivalent to “the number of times” it appears
    in the training data. This weighting scheme has its origins in survey sampling
    theory.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种样本权重。在XGBoost和大多数其他ML模型中，观测级别的权重被解释为频率权重，其中对观测的权重等同于它在训练数据中出现的“次数”。这种加权方案起源于调查抽样理论。
- en: The other main type of sample weights come from the theory of weighted least
    squares. Sometimes called precision weights, they quantify our uncertainty about
    the observation’s feature values, under the assumption that each observation is
    really an average of multiple underlying samples. These two notions of sample
    weights are not equivalent, so it’s important to know which one you’re specifying
    when you set a `sample_weights` parameter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种主要的样本权重类型来自加权最小二乘理论。有时称为精度权重，它们量化了我们对观测特征值的不确定性，假设每个观测实际上是多个潜在样本的平均值。这两种样本权重的概念并不相等，因此在设置`sample_weights`参数时知道你正在指定哪一种是很重要的。
- en: 'Applying the `reweight_dataset` function provides us with a vector of observation
    weights of the same length as the training data, such that a weighted average
    of the outcomes in the data within each demographic group is equal. Reweighing
    helps to undo manifestations of systemic biases in training data, teaching XGBoost
    that different kinds of people should have the same average outcome rates. In
    code, this is as simple as retraining XGBoost with the row weights from `reweight_dataset`.
    In our code, we call this vector of training weights `train_weights`. When we
    call the `DMatrix` function, we use the `weight=` argument to specify these bias-decreasing
    weights. After this, we simply retrain XGBoost:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 应用`reweight_dataset`函数为我们提供了一个与训练数据长度相同的观测权重向量，使得数据中每个人口群体内的结果加权平均值相等。重新加权有助于消除训练数据中系统偏差的表现，教会XGBoost不同种类的人应该具有相同的平均结果率。在代码中，这就像使用`reweight_dataset`的行权重重新训练XGBoost一样简单。在我们的代码中，我们将这个训练权重向量称为`train_weights`。当我们调用`DMatrix`函数时，我们使用`weight=`参数来指定这些减少偏差的权重。之后，我们简单地重新训练XGBoost：
- en: '[PRE6]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Table 10-6](#pre_air) shows both the original mean outcomes and original AIR
    values, along with the preprocessed mean outcomes and AIR. When we trained XGBoost
    on the unweighted data, we saw some problematic AIR values. Originally, the AIR
    was around 0.73 for Black and Hispanic people. These values are not great—signifying
    that for every 1,000 credit products the model extends to white people, this model
    only accepts applications from about 730 Hispanic or Black people. This level
    of bias is ethically troubling, but it could also give rise to legal troubles
    in consumer finance, hiring, or other areas that rely on traditional legal standards
    for bias testing. The four-fifths rule—while flawed and imperfect—tells us we
    should not see values below 0.8 for AIR. Luckily, in our case, reweighing provides
    good remediation results.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 10-6](#pre_air) 显示了原始平均结果和原始AIR值，以及预处理后的平均结果和AIR。当我们在未加权的数据上训练 XGBoost
    时，我们观察到了一些问题性AIR值。最初，黑人和西班牙裔的AIR约为0.73。这些值并不理想——表明对于每1000个信用产品，模型只接受约730个西班牙裔或黑人的申请。这种偏见水平在伦理上令人不安，但也可能在消费金融、招聘或其他依赖传统法律标准进行偏见测试的领域引发法律问题。五分之四法则——虽然存在缺陷和不完善——告诉我们，我们不应看到低于0.8的AIR值。幸运的是，在我们的案例中，重新加权提供了良好的补救效果。'
- en: In [Table 10-6](#pre_air), we can see that we increased the problematic AIR
    values for Hispanic and Black people to less borderline values, and importantly,
    without changing the AIR very much for Asian people. In short, reweighing decreased
    potential bias risks for Black and Hispanic people, without increasing those risks
    for other groups. Did this have any effect on the performance quality of our model?
    To investigate this, we introduced a hyperparameter, `lambda`, in [Figure 10-3](#pre_processing_accuracy)
    that dictates the strength of the reweighing scheme. When `lambda` is equal to
    zero, all observations get a sample weight of one. When the hyperparameter is
    equal to one, the mean outcomes are all equal, and we get the results in [Table 10-6](#pre_air).
    As shown in [Figure 10-3](#pre_processing_accuracy), we did observe some trade-off
    between increasing the strength of reweighing and performance as measured by F1
    in validation data. Next, let’s look at the result on Black and Hispanic AIRs
    as we sweep `lambda` across a range of values to understand more about that trade-off.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [表格 10-6](#pre_air) 中，我们可以看到，我们将西班牙裔和黑人的问题AIR值提高到了较边缘的值，而且重要的是，并没有对亚裔的AIR值造成太大改变。简言之，重新加权减少了黑人和西班牙裔的潜在偏见风险，而不会增加其他群体的风险。这是否对我们模型的性能质量产生了任何影响？为了调查这一点，我们在
    [图 10-3](#pre_processing_accuracy) 中引入了一个超参数 `lambda`，它决定了重新加权方案的强度。当 `lambda`
    等于零时，所有观测值的样本权重都为一。当超参数等于一时，平均结果都相等，并且我们得到了 [表格 10-6](#pre_air) 中的结果。正如 [图 10-3](#pre_processing_accuracy)
    所示，我们确实观察到了在验证数据的 F1 值中，增加重新加权强度与性能之间的某种权衡。接下来，让我们看看在扫描 `lambda` 跨越一系列值时，对黑人和西班牙裔AIR的影响，以更深入地了解这种权衡。
- en: Table 10-6\. Original and preprocessed mean outcomes for demographic groups
    in test data
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 10-6\. 测试数据中人口群体的原始和预处理后的平均结果
- en: '| Demographic group | Original mean outcome | Preprocessed mean outcome | Original
    AIR | Preprocessed AIR |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 人口群体 | 原始平均结果 | 预处理后的平均结果 | 原始AIR | 预处理后的AIR |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Hispanic | 0.398 | 0.22 | 0.736 | 0.861 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 西班牙裔 | 0.398 | 0.22 | 0.736 | 0.861 |'
- en: '| Black | 0.400 | 0.22 | 0.736 | 0.877 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 黑人 | 0.400 | 0.22 | 0.736 | 0.877 |'
- en: '| White | 0.112 | 0.22 | 1.000 | 1.000 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 白人 | 0.112 | 0.22 | 1.000 | 1.000 |'
- en: '| Asian | 0.107 | 0.22 | 1.012 | 1.010 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 亚裔 | 0.107 | 0.22 | 1.012 | 1.010 |'
- en: '![mlha 1003](assets/mlha_1003.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 1003](assets/mlha_1003.png)'
- en: Figure 10-3\. F1 scores of the model as the strength of the reweighing scheme
    is increased ([digital, color version](https://oreil.ly/wJ396))
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 随着重新加权方案强度的增加，模型的 F1 分数（[数字，彩色版本](https://oreil.ly/wJ396)）
- en: 'The results in [Figure 10-4](#pre_processing_airs) show that increasing `lambda`
    past 0.8 does not yield meaningful improvements in Black and Hispanic AIRs. Looking
    back at [Figure 10-3](#pre_processing_accuracy), this means we would experience
    a roughly 3% drop in silico. If we were thinking about deploying this model, we’d
    choose that hyperparameter value for retraining. The compelling story told between
    Figures [10-3](#pre_processing_accuracy) and [10-4](#pre_processing_airs) is this:
    simply by applying sampling weights to our dataset so as to emphasize favorable
    Black and Hispanic borrowers, we can increase AIRs for these two groups, while
    realizing only a nominal performance drop.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，在增加`lambda`至0.8以上时，并没有显著改善黑人和西班牙裔的AIR（[图 10-4](#pre_processing_airs)）。回顾[图 10-3](#pre_processing_accuracy)，这意味着我们将经历大约3%的模拟性能下降。如果我们考虑部署这个模型，我们会选择重新训练的超参数值。在[图 10-3](#pre_processing_accuracy)和[图 10-4](#pre_processing_airs)之间讲述的引人入胜的故事是：仅通过对数据集应用采样权重，以强调有利的黑人和西班牙裔借款人，我们可以提高这两组的AIRs，同时只有名义性能力的下降。
- en: '![mlha 1004](assets/mlha_1004.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 1004](assets/mlha_1004.png)'
- en: Figure 10-4\. Adverse impact ratios of the model as the strength of the reweighing
    scheme is increased ([digital, color version](https://oreil.ly/LKxEH))
  id: totrans-191
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 在加强重新加权方案的强度后，该模型的不利影响比率（[数字，彩色版](https://oreil.ly/LKxEH))
- en: Like nearly everything else in ML, bias remediation and our chosen approaches
    are an experiment, not rote engineering. They’re not guaranteed to work, and we
    always need to check if they actually work, first in validation and test data,
    then in the real world. It’s really important to remember that we don’t know how
    this model is going to perform in terms of accuracy or bias once it’s deployed.
    We always hope our in silico validation and test assessments are correlated to
    real-world performance, but there are simply no guarantees. We’d have high hopes
    that what looks like a ~5% decrease in in silico performance washes out once the
    model is deployed due to drifting data, changes in real-world operating contexts,
    and other in vivo surprises. All of this points to the need for monitoring both
    performance and bias once a model is deployed.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 像机器学习中的几乎所有其他内容一样，偏差修正和我们选择的方法都是一个实验，而不是刻板的工程。它们不能保证有效，并且我们始终需要检查它们是否真正有效，首先是在验证和测试数据中，然后是在实际世界中。重要的是要记住，我们不知道一旦部署模型，它在精度或偏差方面的表现如何。我们始终希望我们的模拟验证和测试评估与实际性能相关联，但是这里根本没有任何保证。我们希望，在模型部署后看起来像是模拟性能下降约5%的情况，由于数据漂移、实际运行环境的变化和其他在体内的意外，这种情况会被抹平。所有这些都表明，一旦模型部署，有必要监控性能和偏差。
- en: Reweighing is just one example of a preprocessing technique, and there are several
    other popular approaches. Preprocessing is simple, direct, and intuitive. As we’ve
    just seen, it can result in meaningful improvements in model bias with acceptable
    accuracy trade-offs. Check out [AIF360](https://oreil.ly/rDdhC) for examples of
    other credible preprocessing techniques.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 重新加权只是预处理技术的一个示例，还有其他几种流行的方法。预处理简单、直接且直观。正如我们刚才看到的，它可以在接受的精度牺牲的情况下，对模型偏差产生显著改善。查看[AIF360](https://oreil.ly/rDdhC)，了解其他可靠的预处理技术示例。
- en: In-processing
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: In-processing
- en: Next we’ll try an in-processing bias-remediation technique. Many interesting
    techniques have been proposed in recent years, including some that use adversarial
    models, as in [“Mitigating Unwanted Biases with Adversarial Learning”](https://oreil.ly/rFdZA)
    or [“Fair Adversarial Gradient Tree Boosting”](https://oreil.ly/kZ0xB). The idea
    behind these adversarial in-processing approaches is straightforward. When an
    adversarial model cannot predict demographic group membership from our main model’s
    predictions, then we feel good that our predictions do not encode too much bias.
    As highlighted earlier in the chapter, adversarial models also help to capture
    local information about bias. The rows where the adversary model is most accurate
    are likely the rows that encode the most demographic information. These rows can
    help us uncover individuals who may be experiencing the most bias, complex proxies
    involving several input features, and other local bias patterns.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将尝试一种内部处理的偏见修复技术。近年来提出了许多有趣的技术，包括使用对抗模型的一些技术，例如[“用对抗学习减轻不必要的偏见”](https://oreil.ly/rFdZA)或[“公平对抗梯度树提升”](https://oreil.ly/kZ0xB)。这些对抗内部处理方法的想法很简单。当对抗模型无法从我们主模型的预测中预测出人口统计组成员身份时，我们感到我们的预测不会编码太多的偏见。正如本章前面强调的那样，对抗模型还有助于捕捉关于偏见的局部信息。对抗模型在最准确的行上可能是编码了最多人口统计信息的行。这些行可以帮助我们揭示可能经历最多偏见的个人，涉及几个输入特征的复杂代理和其他局部偏见模式。
- en: There are also in-processing debiasing techniques that use only one model, and
    since they are usually a little easier to implement, we’ll focus on one of those
    for our use case. As opposed to using a second model, these in-processing methods
    use a dual objective function with a regularization approach. For example, [“A
    Convex Framework for Fair Regression”](https://oreil.ly/7dcHL) puts forward various
    regularizers that can be paired with linear and logistic regression models to
    decrease bias against groups and individuals. [“Learning Fair Representations”](https://oreil.ly/tgCE9)
    also includes a bias measurement in model objective functions, but then tries
    to create a new representation of training data that encodes less bias.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一些只使用一个模型的内部处理去偏见的技术，由于通常实施起来稍微容易些，我们将专注于我们的使用案例中的其中一种。与使用第二个模型相反，这些内部处理方法使用双目标函数和正则化方法。例如，[“公平回归的凸框架”](https://oreil.ly/7dcHL)提出了各种可以与线性和逻辑回归模型配对以减少对群体和个人的偏见的正则化器。[“学习公平的表示”](https://oreil.ly/tgCE9)也包括在模型目标函数中包含偏见度量，但随后尝试创建一个新的训练数据表示，编码较少的偏见。
- en: 'While these two approaches focus mostly on simple models, i.e., linear regression,
    logistic regression, and naive Bayes, we want to work with trees, and in particular,
    XGBoost. Turns out, we’re not the only ones. A research group at American Express
    recently released [“FairXGBoost: Fairness-Aware Classification in XGBoost”](https://oreil.ly/2gNo9),
    which includes instructions and experimental results on introducing a bias regularization
    term into XGBoost models, using XGBoost’s preexisting capability to train with
    custom-coded objective functions. This is how we’ll do in-processing, and as you’ll
    see soon, it’s remarkably direct to implement and gives good results on our example
    data.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两种方法主要集中在简单模型上，即线性回归、逻辑回归和朴素贝叶斯，但我们想要与树一起工作，尤其是XGBoost。事实证明，我们并不是唯一的。美国运通的一个研究小组最近发布了[“FairXGBoost：XGBoost中的公平意识分类”](https://oreil.ly/2gNo9)，其中包括关于在XGBoost模型中引入偏见正则化项的说明和实验结果，使用XGBoost的现有能力来训练具有自定义编码目标函数的模型。这就是我们将进行内部处理的方式，正如您很快将看到的，它实施起来非常直接，并在我们的示例数据上取得了良好的结果。
- en: Note
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Before we jump into the more technical descriptions, code, and results, we should
    mention that a great deal of the fairness regularization work we’ve discussed
    is based on, or otherwise related to, the seminal paper by Kamishima et al., [“Fairness-Aware
    Classifier with Prejudice Remover Regularizer”](https://oreil.ly/E_arn).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们跳入更多技术描述、代码和结果之前，我们应该提到我们讨论的许多公平性正则化工作基于或与Kamishima等人的开创性论文[“具有偏见消除正则化器的公平意识分类器”](https://oreil.ly/E_arn)相关。
- en: How does our chosen approach work? Objective functions are used to measure error
    during model training, where an optimization procedure tries to minimize that
    error and find the best model parameters. The basic idea of in-processing regularization
    techniques is to include a measure of bias in the model’s overall objective function.
    When the optimization function is used to calculate error and the ML optimization
    process tries to minimize that error, this also tends to result in decreasing
    measured bias. Another twist on this idea is to use a factor on the bias measurement
    term within the objective function, or a *regularization hyperparameter*, so that
    the effect of bias remediation can be tuned. In case readers didn’t know already,
    XGBoost supports a wide variety of objective functions so that we can ensure that
    the way error is measured actually maps to the real-world problem at hand. It
    also supports fully [customized objective functions](https://oreil.ly/pczVg) coded
    by users.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择的方法是如何工作的？客观函数用于在模型训练期间测量误差，在优化过程中试图最小化该误差并找到最佳模型参数。在处理中正则化技术的基本思想是在模型整体客观函数中包含偏差的测量。当优化函数用于计算误差时，机器学习优化过程试图最小化该误差，这也倾向于减少测量到的偏差。这个想法的另一个转变是在客观函数内使用偏差测量项的因子，或者*正则化超参数*，这样可以调节偏差修正的效果。如果读者还不知道，XGBoost支持各种客观函数，以确保误差测量方式实际映射到手头的实际问题。它还支持用户编写的完全[自定义客观函数](https://oreil.ly/pczVg)。
- en: 'The first step in implementing our in-processing approach will be to code a
    sample objective function. In the code snippet that follows, we define a simple
    objective function that tells XGBoost how to generate scores:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 实施我们的处理过程的第一步将是编写一个样本客观函数。在接下来的代码片段中，我们定义了一个简单的客观函数，告诉XGBoost如何生成评分：
- en: Calculate the first derivative of the objective function with respect to model
    output (gradient, `grad`).
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算客观函数关于模型输出的一阶导数（梯度，`grad`）。
- en: Calculate the second derivative of the objective function with respect to model
    output (Hessian, `hess`).
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算客观函数关于模型输出的二阶导数（海森矩阵，`hess`）。
- en: Incorporate demographic information (`protected`) into the objective function.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将人口统计信息（`protected`）合并到客观函数中。
- en: Control the strength of the regularization with a new parameter (lambda, `lambda`).
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用新参数（lambda，`lambda`）控制正则化的强度。
- en: We also create a simple wrapper for the objective that allows us to specify
    which groups we want to consider to be the protected class—those who we want to
    experience less bias due to regularization—and the strength of the regularization.
    While simplistic, the wrapper buys us quite a lot of functionality. It enables
    us to include multiple demographic groups into the protected group. This is important
    because models often exhibit bias against more than one group, and simply trying
    to remediate bias for one group may make things worse for other groups. The ability
    to supply custom `lambda` values is great because it allows for us to tune the
    strength of our regularization. As shown in [“Preprocessing”](#preprocessing_ch10_1680549482138),
    the ability to tune the regularization hyperparameter is crucial for finding an
    ideal trade-off with model accuracy.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了一个简单的包装器，用于指定我们希望将其视为受保护类别的群体——那些我们希望由于正则化而经历较少偏差——以及正则化的强度。尽管简单，这个包装器为我们提供了相当多的功能。它使我们能够将多个人口统计群体包括在受保护群体中。这一点很重要，因为模型经常对多个群体存在偏见，而仅试图为一个群体修正偏见可能会使其他群体的情况变得更糟。能够提供自定义`lambda`值的能力非常棒，因为它允许我们调整正则化的强度。正如[“预处理”](#preprocessing_ch10_1680549482138)所示，调节正则化超参数的能力对于找到与模型准确性的理想平衡至关重要。
- en: 'That’s a lot to pack into roughly 15 lines of Python code, but that’s why we
    picked this approach. It takes advantage of niceties in the XGBoost framework,
    it’s pretty simple, and it appears to increase AIR for historically marginalized
    minority groups in our example data:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在大约15行Python代码中，我们要做的事情很多，但这就是我们选择这种方法的原因。它利用了XGBoost框架中的便利性，非常简单，并且似乎增加了我们示例数据中历史上被边缘化的少数群体的AIR：
- en: '[PRE7]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once that custom objective is defined, we just need to use the `obj=` argument
    to pass it to XGBoost’s `train()` function. If we’ve written the code correctly,
    XGBoost’s robust training and optimization mechanisms should take care of the
    rest. Note how little code it takes to train with our custom objective:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了自定义目标函数，我们只需使用`obj=`参数将其传递给XGBoost的`train()`函数。如果我们编写的代码正确，XGBoost的强大训练和优化机制将处理其余的事务。注意，使用我们的自定义目标进行训练所需的代码量是多么少：
- en: '[PRE8]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Validation and test results for in-processing remediation are available in Figures
    [10-5](#in_processing_airs_f1) and [10-6](#in_processing_airs). To validate our
    hypothesis, we took advantage of our wrapper function and trained many different
    models with many different settings of `lambda`. In [Figure 10-6](#in_processing_airs),
    we can see that increasing `lambda` does decrease bias, as measured by an increasing
    Black and Hispanic AIR, whereas Asian AIR remains roughly constant around the
    good value of 1\. We can increase the AIR for the groups we tend to be most concerned
    about in consumer finance, without engaging in potential discrimination of other
    demographic groups. That is the result we want to see!
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 通过图表 [10-5](#in_processing_airs_f1) 和 [10-6](#in_processing_airs) 可以查看在处理过程中修复验证和测试结果。为了验证我们的假设，我们利用包装函数，并用多种不同的`lambda`设置训练了许多不同的模型。在
    [图 10-6](#in_processing_airs) 中，我们可以看到增加`lambda`确实减少了偏差，这由黑人和西班牙裔的增加AIR所衡量，而亚裔AIR则大致保持在值为1左右。我们可以增加那些在消费金融中我们最关注的群体的AIR，而不涉及对其他人口统计群体的潜在歧视。这正是我们想要看到的结果！
- en: What about the trade-off between performance and decreased bias? What we saw
    here is pretty typical in our experience. There’s a range of `lambda` values above
    which Black and Hispanic AIRs do not meaningfully increase, but the F1 score of
    the model continues to decrease to below 90% of the performance of the original
    model. We probably wouldn’t use the model wherein `lambda` is cranked up to the
    maximum level, so we’re probably looking at a small decrease in in silico test
    data performance and an as yet unknown change in in vivo performance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 那么关于性能和减少偏差之间的权衡呢？我们在这里看到的情况在我们的经验中相当典型。在某些`lambda`值上，黑人和西班牙裔的AIR并没有显著增加，但模型的F1分数继续下降，低于原模型性能的90%。我们可能不会使用`lambda`被调整到最大水平的模型，因此我们可能会看到在硅测试数据性能上的小幅下降，以及迄今为止在体内性能上的未知变化。
- en: '![mlha 1005](assets/mlha_1005.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 1005](assets/mlha_1005.png)'
- en: Figure 10-5\. The F1 scores of the model as `lambda` is increased ([digital,
    color version](https://oreil.ly/D5Hz_))
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 随着`lambda`增加，模型的F1分数（[数字，彩色版本](https://oreil.ly/D5Hz_)）
- en: '![mlha 1006](assets/mlha_1006.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 1006](assets/mlha_1006.png)'
- en: Figure 10-6\. AIR values across demographic groups as the regularization factor,
    `lambda`, is increased ([digital, color version](https://oreil.ly/tRfBx))
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 随着正则化因子`lambda`的增加，各人口统计群体的AIR值（[数字，彩色版本](https://oreil.ly/tRfBx))
- en: Postprocessing
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后处理
- en: Next we’ll move on to postprocessing techniques. Remember that postprocessing
    techniques are applied after a model has already been trained, so in this section
    we’ll modify the output probabilities of the original model that we trained at
    the beginning of the chapter.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将转向后处理技术。请记住，后处理技术是在模型已经训练好之后应用的，因此在本节中我们将修改我们在本章开始时训练的原始模型的输出概率。
- en: 'The technique that we’ll apply is called *reject option* postprocessing, and
    it dates back to [a 2012 paper](https://oreil.ly/2rh4r) by Kamiran et al. Remember
    that our model has a cutoff value, where scores above this value are given a binary
    outcome of 1 (an undesirable result for our credit applicants), and scores below
    the cutoff are given a predicted outcome of 0 (a favorable outcome). Reject option
    postprocessing works on the idea that for model scores *near* the cutoff value,
    the model is uncertain about the correct outcome. What we do is group together
    all observations that receive a score within a narrow interval around the cutoff,
    and then we reassign outcomes for these observations in order to increase the
    equity of model outcomes. Reject option postprocessing is easy to interpret and
    implement—we were able to do so with another relatively straightforward function:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用的技术称为*拒绝选项后处理*，最早可以追溯到[Kamiran et al.的2012篇论文](https://oreil.ly/2rh4r)。请记住，我们的模型设定了一个截止值，高于此值的分数被给予二进制结果1（对我们的信用申请者来说是一个不良结果），而低于截止值的分数则预测为0（一个有利的结果）。拒绝选项后处理的工作原理是，对于分数*接近*截止值的模型，模型对正确结果存在不确定性。我们所做的是将所有在截止值附近窄区间内接收到分数的观测结果分组在一起，然后重新分配这些观测结果以增加模型结果的公平性。拒绝选项后处理易于解释和实施——我们能够用另一个相对简单的函数来做到这一点：
- en: '[PRE9]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In [Figure 10-7](#reject_option) we can see the technique in action. The histograms
    show the distribution of model scores for each racial group, both before and after
    the postprocessing. We can see that in a small neighborhood of scores around 0.26
    (the original model cutoff), we have postprocessed all Black and Hispanic people
    into a favorable outcome by assigning them a score at the bottom of the range.
    Meanwhile, we have assigned white people in this *uncertainty zone* an unfavorable
    model outcome and left Asian scores unchanged. With these new scores in hand,
    let’s investigate how this technique affects model accuracy and AIRs.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-7](#reject_option)中，我们可以看到这种技术的应用。直方图显示了每个种族群体模型分数在后处理前后的分布。我们可以看到，在接近0.26分数（原始模型截止值）的一个小邻域内，我们已经通过将他们分数设定为范围底部，将所有黑人和西班牙裔人群后处理成有利的结果。同时，我们将在这个*不确定性区域*内的白人分配为不利的模型结果，并保持亚裔分数不变。有了这些新分数，让我们来调查这种技术如何影响模型的准确性和AIRs。
- en: '![mlha 1007](assets/mlha_1007.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 1007](assets/mlha_1007.png)'
- en: Figure 10-7\. Model scores for each demographic group before and after the application
    of reject option postprocessing ([digital, color version](https://oreil.ly/KJtVX))
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. 各人种群体模型分数在拒绝选项后处理前后的变化（[数字，彩色版本](https://oreil.ly/KJtVX)）
- en: The results of this experiment are exactly what we would have hoped—we were
    able to improve Black and Hispanic AIRs to above 0.9, while leaving Asian AIR
    around 1.00 ([Table 10-7](#post_processing_results)). The price we had to pay
    in terms of F1 score was a 6% decrease. We don’t find this to be a meaningful
    drop, but if we were concerned, we could decrease the size of the uncertainty
    zone to find a more favorable trade-off.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验的结果正是我们所期望的——我们能够将黑人和西班牙裔的AIRs提高到超过0.9，同时将亚裔AIR维持在约1.00（见[表 10-7](#post_processing_results)）。在F1分数方面，我们需要付出6%的降低。我们认为这不是一个有意义的下降，但如果我们担心的话，可以减小不确定性区域的大小，以找到更有利的权衡点。
- en: Table 10-7\. Original and postprocessed F1 scores and adverse impact ratios
    on validation data
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-7\. 在验证数据上的原始和后处理后的F1分数和不利影响比率
- en: '| Model | F1 score | Black AIR | Hispanic AIR | Asian AIR |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | F1分数 | 黑人AIR | 西班牙裔AIR | 亚裔AIR |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Original | 0.574 | 0.736 | 0.736 | 1.012 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | 0.574 | 0.736 | 0.736 | 1.012 |'
- en: '| Postprocessed | 0.541 | 0.923 | 0.902 | 1.06 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 后处理后 | 0.541 | 0.923 | 0.902 | 1.06 |'
- en: Model Selection
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型选择
- en: The final technique we’ll discuss is fairness-aware model selection. To be exact,
    we’ll conduct simple feature selection and random hyperparameter tuning while
    keeping track of model performance and AIRs. Readers are almost certainly already
    performing these steps when it comes to performance assessment, so this technique
    has fairly low overhead costs. Another advantage of model selection as a remediation
    technique is that it raises the fewest disparate treatment concerns. (On the other
    end of the spectrum is the reject option postprocessing, described in the previous
    section, wherein we literally changed model outcomes depending on the protected
    group status of each observation.)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的最后一种技术是关注公平性的模型选择。确切地说，我们将进行简单的特征选择和随机的超参数调整，同时跟踪模型性能和AIRs。读者在进行性能评估时几乎肯定已经执行了这些步骤，因此这种技术的开销相对较低。作为修复技术的模型选择的另一个优势是它引起的不公平对待关注最少。
    （在谱系的另一端是拒绝选项后处理，在此我们根据每个观察的受保护组状态实际更改了模型结果。）
- en: Note
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Random searches across feature sets and hyperparameter settings often reveal
    models with improved fairness characteristics and similar performance to a baseline
    model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索不同特征集和超参数设置通常会揭示具有改进公平特性和类似基准模型性能的模型。
- en: In this section, we’ll track F1 and AUC scores as our notion of model performance
    quality. In our experience, evaluating models on multiple measures of quality
    increases the likelihood of good in vivo performance. Another advantage of computing
    both F1 and AUC scores is that the first is measured on model outcomes and the
    second uses only output probabilities. If in the future we want to change the
    decision cutoff of our model, or pass the model scores as inputs into another
    process, we will be glad that we tracked AUC.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将跟踪F1和AUC分数作为我们对模型性能质量的概念。根据我们的经验，评估多个质量指标的模型可以增加在 vivo 性能良好的可能性。计算F1和AUC分数的另一个优势是，前者是在模型结果上测量的，而后者仅使用输出概率。如果将来我们想要更改模型的决策截断，或者将模型分数作为另一个过程的输入传递，我们将会很高兴我们跟踪了AUC。
- en: One more note before we dive into model selection—model selection is much more
    than just feature selection and hyperparameter tuning. It can also mean choosing
    between competing model architectures, or choosing between different bias-remediation
    techniques. In the conclusion of this chapter, we’ll round up all of our results
    to prepare for a final model selection, but in this section we’ll just focus on
    features and hyperparameters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论模型选择之前再说一句——模型选择远不止于特征选择和超参数调整。它还可以意味着在竞争性模型架构或不同偏差修复技术之间进行选择。在本章的结尾，我们将总结所有结果以准备最终的模型选择，但在本节中，我们将专注于特征和超参数。
- en: In our experience, feature selection can be a powerful remediation technique,
    but it works best when guided by subject matter experts and when alternative sources
    of data are available. For example, a compliance expert at a bank may know that
    a feature in a lending model can be swapped out with an alternative feature that
    encodes less historical bias. We don’t have the luxury of accessing these alternative
    features, so for our example data we’ll only have the option of *dropping* features
    from our model, and we’ll test the effect of dropping each feature individually
    while maintaining the original hyperparameters. Between feature selection and
    hyperparameter tuning, we’re about to train a lot of different models, so we’ll
    employ five-fold cross-validation using our original training data. If we choose
    the variant with the best performance on validation data, we run an increased
    risk of selecting a model that performs best only due to random chance.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，特征选择可以是一种有效的修复技术，但在学科专家的指导下，并且当可用替代数据源时效果最佳。例如，银行的合规专家可能知道，贷款模型中的某个特征可以用编码历史偏见较少的替代特征替换。我们无法接触这些替代特征，因此在我们的示例数据中，我们只能选择*丢弃*模型中的特征，并在保持原始超参数的同时测试每个特征被丢弃的效果。在特征选择和超参数调整之间，我们将要训练大量不同的模型，因此我们将使用我们的原始训练数据进行五折交叉验证。如果我们选择在验证数据上性能最佳的变体，我们将增加选择仅由于随机机会而表现最佳的模型的风险。
- en: Warning
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: While the Rashomon effect may mean we have many good models to chose from, we
    should not forget that this phenomenon may also be a sign of instability in our
    original model. If there are many models with settings similar to our original
    model, that also perform differently from our original model, this points to underspecification
    and misspecification issues. Remediated models must also be tested for stability,
    safety, and performance issues. See Chapters [3](ch03.html#unique_chapter_id_3),
    [8](ch08.html#unique_chapter_id_8), and [9](ch09.html#unique_chapter_id_9) for
    more information.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然拉肖蒙效应可能意味着我们有许多好的模型可供选择，但我们不应忘记这种现象可能也是我们原始模型不稳定的标志。如果有许多与我们原始模型设置相似但性能与原始模型不同的模型，这表明了欠规格化和误规格化问题。修复后的模型还必须进行稳定性、安全性和性能问题的测试。更多信息请参阅第
    [3](ch03.html#unique_chapter_id_3)、[8](ch08.html#unique_chapter_id_8) 和 [9](ch09.html#unique_chapter_id_9)
    章。
- en: After training these new models using cross-validation, we were able to realize
    an increase in Black and Hispanic cross-validation AIRs, alongside a small decrease
    in model cross-validation AUC. The most offending feature was `PAY_AMT5`, so we’ll
    proceed with random hyperparameter tuning without this feature.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用交叉验证训练这些新模型后，我们能够在黑人和西班牙裔交叉验证 AIR 中实现增加，同时模型交叉验证 AUC 稍微下降。最令人担忧的特征是 `PAY_AMT5`，因此我们将继续进行随机超参数调优，不包括此特征。
- en: Note
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It’s possible to be more sophisticated about feature selection by using adversarial
    models and explainable AI techniques. For inspiration, consider the article [“Explaining
    Measures of Fairness”](https://oreil.ly/SLn_8) and the associated notebook from
    the creator of SHAP, and [“Automating Procedurally Fair Feature Selection in Machine
    Learning”](https://oreil.ly/YSKnM) from Belitz et al.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用对抗模型和可解释 AI 技术，可以更加复杂地进行特征选择。作为灵感，可以考虑文章 [“解释公平度量”](https://oreil.ly/SLn_8)
    及 SHAP 之父的相关笔记本，以及 Belitz 等人的 [“在机器学习中自动执行程序公平特征选择”](https://oreil.ly/YSKnM)。
- en: 'To choose new model hyperparameters, we’ll use a random grid search using the
    scikit-learn API. Since we want to cross-validate AIRs throughout this process,
    we have to put together a scoring function to pass into scikit-learn. To simplify
    the code, we only track the Black AIR here—since it has been correlated to the
    Hispanic AIR throughout our analysis—but an average measure of AIR across protected
    groups is likely preferable. This code snippet shows how we used global variables
    and the `make_scorer()` interface to get this done:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择新的模型超参数，我们将使用 scikit-learn API 进行随机网格搜索。由于我们希望在整个过程中进行交叉验证 AIRs，因此我们必须构建一个评分函数传递给
    scikit-learn。为简化代码，我们只在此跟踪黑人 AIR —— 因为在我们的分析中它与西班牙裔 AIR 相关联 —— 但跨受保护群体的平均 AIR
    应更具优先选择性。此代码片段展示了我们如何使用全局变量和 `make_scorer()` 接口来完成此操作：
- en: '[PRE10]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we defined a reasonable grid of hyperparameters and built 50 new models:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了一个合理的超参数网格，并构建了 50 个新模型：
- en: '[PRE11]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The results of our random model selection procedure are shown in [Figure 10-8](#pareto).
    Each model is a point on the plot, with Black cross-validation AIR values on the
    x-axis and cross-validation AUC on the y-axis. As we’ve done here, it is useful
    to normalize model accuracy to the baseline value in order to easily make statements
    like “this alternative model shows a 2% drop in AUC from the original model.”
    Given this distribution of models, how do we go about choosing one for deployment?
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随机模型选择程序的结果显示在 [图 10-8](#pareto) 中。每个模型在图中表示为一个点，其中黑人交叉验证 AIR 值位于 x 轴上，交叉验证
    AUC 位于 y 轴上。如同我们在此处所做的，将模型准确性归一化到基准值中，以便轻松地做出像“这个替代模型显示了从原始模型中下降了 2% 的 AUC”这样的声明。考虑到这些模型的分布，我们如何选择一个用于部署的模型？
- en: '![mlha 1008](assets/mlha_1008.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 1008](assets/mlha_1008.png)'
- en: Figure 10-8\. The normalized accuracy and Black AIRs of each model after feature
    selection and hyperparameter tuning ([digital, color version](https://oreil.ly/7ru28))
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. 在特征选择和超参数调优后，每个模型的归一化准确率和黑人 AIR（[数字、彩色版本](https://oreil.ly/7ru28)）
- en: A common problem with bias-remediation approaches is that they often just move
    bias around from one demographic group to another. For example, women are now
    favored sometimes in credit and employment decisions in the US. It wouldn’t be
    surprising to see a bias-remediation technique dramatically decrease favorable
    outcomes for women in the process of increasing them for other groups impacted
    by systemic bias, but that’s not the outcome anyone really wants. If one group
    is disproportionally favored, and bias remediation equals that out—great. If,
    on the other hand, one group is favored a bit, and bias remediation ends up harming
    them to increase AIRs or other statistics for other groups, that’s obviously not
    great. In the next section, we’ll see how these two alternative models stack up
    against the other remediation techniques applied in this chapter.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见修复方法的一个常见问题是，它们经常只是把偏见从一个人群转移到另一个人群。例如，现在在美国有时候会偏向女性在信用和就业决策中。看到偏见修复技术在增加某些系统偏见影响的其他群体的有利结果的过程中大幅减少了女性的有利结果并不奇怪，但这不是任何人真正希望看到的结果。如果一个群体被不成比例地偏爱，而偏见修复使其平衡——那太好了。另一方面，如果一个群体稍微被偏爱，而偏见修复结果却伤害了他们以提高其他群体的AIRs或其他统计数据，那显然不是好事。在下一节中，我们将看到这两种替代模型在本章应用的其他偏见修复技术中的表现如何。
- en: Warning
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Any time we evaluate multiple models on the same dataset, we must be careful
    about overfitting and multiple comparisons. We should employ best practices such
    as reusable holdout, cross-validation, bootstrapping, out-of-time holdout data,
    and post­de⁠ployment monitoring to ensure that our results generalize.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们在同一数据集上评估多个模型时，必须小心过拟合和多重比较。我们应该采用最佳实践，如可重用的留置数据、交叉验证、自助法、超时留置数据和部署后监控，以确保我们的结果具有普遍适用性。
- en: Conclusion
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In [Table 10-8](#final_results), we’ve aggregated the results for all of the
    models trained in this chapter. We chose to focus on two measures of model accuracy,
    F1 score and AUC, and two measures of model bias, AIRs and false-positive rate
    (FPR) disparity.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表10-8](#final_results)中，我们汇总了本章训练的所有模型的结果。我们选择关注模型准确性的两个指标，F1分数和AUC，以及模型偏见的两个指标，AIRs和误报率（FPR）差异。
- en: Table 10-8\. Comparison of test data between bias-remediation techniques
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-8. 偏见修复技术在测试数据中的比较
- en: '| Measurement | Original model | Preprocessing (reweighting) | In-processing
    (regularized, `lambda` = 0.2) | Postprocessing (reject option, window = 0.1) |
    Model selection |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 测量 | 原始模型 | 预处理（重新加权） | 内部处理（正则化，`lambda` = 0.2） | 后处理（拒绝选项，窗口 = 0.1） | 模型选择
    |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| AUC | 0.798021 | 0.774183 | 0.764005 | 0.794894 | 0.789016 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| AUC | 0.798021 | 0.774183 | 0.764005 | 0.794894 | 0.789016 |'
- en: '| F1 | 0.558874 | 0.543758 | 0.515971 | 0.533964 | 0.543147 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| F1 | 0.558874 | 0.543758 | 0.515971 | 0.533964 | 0.543147 |'
- en: '| Asian AIR | 1.012274 | 1.010014 | 1.001185 | 1.107676 | 1.007365 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 亚裔AIR | 1.012274 | 1.010014 | 1.001185 | 1.107676 | 1.007365 |'
- en: '| Black AIR | 0.735836 | 0.877673 | 0.851499 | 0.901386 | 0.811854 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 黑人AIR | 0.735836 | 0.877673 | 0.851499 | 0.901386 | 0.811854 |'
- en: '| Hispanic AIR | 0.736394 | 0.861252 | 0.851045 | 0.882538 | 0.805121 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 西班牙裔AIR | 0.736394 | 0.861252 | 0.851045 | 0.882538 | 0.805121 |'
- en: '| Asian FPR disparity | 0.872567 | 0.929948 | 0.986472 | 0.575248 | 0.942973
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 亚裔FPR差异 | 0.872567 | 0.929948 | 0.986472 | 0.575248 | 0.942973 |'
- en: '| Black FPR disparity | 1.783528 | 0.956640 | 1.141044 | 0.852034 | 1.355846
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 黑人FPR差异 | 1.783528 | 0.956640 | 1.141044 | 0.852034 | 1.355846 |'
- en: '| Hispanic FPR disparity | 1.696062 | 0.899065 | 1.000040 | 0.786195 | 1.253355
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 西班牙裔FPR差异 | 1.696062 | 0.899065 | 1.000040 | 0.786195 | 1.253355 |'
- en: 'The results are exciting: many remediation techniques tested are able to realize
    meaningful improvements in AIRs and FPR disparities for Black and Hispanic borrowers
    with no serious negative impact on the Asian AIR. This is possible with only marginal
    changes in model performance.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人兴奋：许多测试的偏见修复技术能够显著改善黑人和西班牙裔借款人的AIRs和FPR差异，而对亚裔AIR没有严重负面影响。这仅仅需要在模型性能上进行边际改进。
- en: How should we choose which remediation technique to apply to our high-risk model?
    Hopefully, this chapter has convinced readers to try many things. Ultimately,
    the final decision rests with the law, business leadership, and the diverse team
    of stakeholders that we assemble. Within traditionally regulated vertical organizations,
    where disparate treatment is strictly prohibited, there are strict constraints
    on our choices. We can really only choose from model selection options available
    today. If we’re outside of these verticals, we have a much wider selection of
    remediation strategies to choose from.^([3](ch10.html#idm45989996778176)) We’d
    likely pick the preprocessing option for remediation given the minimal decrease
    in model performance versus in-processing and because postprocessing knocks some
    performance disparities out of acceptable ranges.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何选择适用于我们高风险模型的修复技术？希望本章能说服读者尝试许多方法。最终的决定取决于法律、业务领导和我们组建的多样化利益相关者团队。在传统受监管的垂直组织中，严格禁止不公平待遇，我们的选择受到严格限制。今天我们只能从现有的模型选择选项中进行选择。如果我们超出这些垂直领域，我们可以从更广泛的修复策略中进行选择。^([3](ch10.html#idm45989996778176))
    鉴于其对模型性能的最小影响和后处理可以将某些性能差异降至可接受范围之外，我们可能会选择预处理选项进行修复。
- en: 'Whether or not we are using model selection as a bias mitigation technique,
    and whether or not we have different pre-, in-, and postprocessed models to choose
    from, a rule of thumb for picking a remediated model is to do the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们是否使用模型选择作为偏见缓解技术，以及我们是否有不同的预处理、处理中和后处理模型可供选择，挑选修复模型的经验法则是执行以下步骤：
- en: Reduce the set of models to those that perform sufficiently well to meet business
    needs, e.g., performance within 5% of the original model.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型集合减少到能够满足业务需求的模型，例如，性能在原始模型的5%内。
- en: 'Among those models, pick the one that most closely:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这些模型中，选择那些最接近以下条件的模型：
- en: Remediates bias across all originally disfavored groups, e.g., all disfavored
    groups’ AIR is increased to ≥ 0.8.
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修复所有最初不利的群体的偏见，例如，所有不利群体的AIR增加到≥0.8。
- en: Does not discriminate against any initially favored group, e.g., no originally
    favored groups’ AIR is decreased to < 0.8.
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不歧视任何最初受欢迎的群体，例如，没有最初受欢迎群体的AIR降低到<0.8。
- en: Consult business partners, legal and compliance experts, and diverse stakeholders
    as part of the selection process.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为选择过程的一部分，咨询业务合作伙伴、法律和合规专家以及多样化的利益相关者。
- en: If we’re training a model that has the potential to impact people—and most models
    do—we have an ethical obligation to test it for bias. And when we find bias, we
    need to mitigate or remediate it. What we’ve gone over in this chapter is the
    technical part of bias management processes. To get bias remediation right also
    involves extending our release timelines, lots of careful communication between
    different stakeholders, and lots of retraining and retesting of ML models and
    pipelines. We’re confident that if we slow down, ask for help and input from stakeholders,
    and apply the scientific method, we will be able to tackle real-world bias challenges
    and deploy performant and minimally biased models.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在训练一个有可能影响人们的模型——而大多数模型确实会——我们有道德义务测试其是否存在偏见。当我们发现偏见时，我们需要采取措施进行缓解或修复。本章讨论的是偏见管理流程的技术部分。要正确进行偏见修复，还需要延长发布时间表，进行不同利益相关者之间的仔细沟通，并对机器学习模型和流程进行大量的重新训练和重新测试。我们相信，如果我们放慢速度，寻求利益相关者的帮助和意见，并应用科学方法，我们将能够解决现实世界中的偏见挑战，并部署性能优良且偏见最小化的模型。
- en: Resources
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: Code Examples
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 代码示例
- en: '[Machine-Learning-for-High-Risk-Applications-Book](https://oreil.ly/machine-learning-high-risk-apps-code)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习高风险应用书](https://oreil.ly/machine-learning-high-risk-apps-code)'
- en: Tools for Managing Bias
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 管理偏见的工具
- en: '[aequitas](https://oreil.ly/JzQFh)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[aequitas](https://oreil.ly/JzQFh)'
- en: 'AI Fairness 360:'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI公平性360：
- en: '[Python](https://oreil.ly/sYmc-)'
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python](https://oreil.ly/sYmc-)'
- en: '[R](https://oreil.ly/J53bZ)'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[R](https://oreil.ly/J53bZ)'
- en: '[Algorithmic Fairness](https://oreil.ly/JNzqk)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[算法公平性](https://oreil.ly/JNzqk)'
- en: '[fairlearn](https://oreil.ly/jYjCi)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[fairlearn](https://oreil.ly/jYjCi)'
- en: '[fairml](https://oreil.ly/DCkZ5)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[fairml](https://oreil.ly/DCkZ5)'
- en: '[fairmodels](https://oreil.ly/nSv8B)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[公平模型](https://oreil.ly/nSv8B)'
- en: '[fairness](https://oreil.ly/Dequ9)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[公平性](https://oreil.ly/Dequ9)'
- en: '[solas-ai-disparity](https://oreil.ly/X9fd6)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[solas-ai-disparity](https://oreil.ly/X9fd6)'
- en: '[tensorflow/fairness-indicators](https://oreil.ly/dHBSL)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[tensorflow/公平性指标](https://oreil.ly/dHBSL)'
- en: '[Themis](https://oreil.ly/zgrvV)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Themis](https://oreil.ly/zgrvV)'
- en: ^([1](ch10.html#idm45989998395696-marker)) If you’d like to satisfy your own
    curiosity on this matter, we urge you to analyze some freely available [Home Mortgage
    Disclosure Act data](https://oreil.ly/xYXdt).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#idm45989998395696-marker)) 如果你对此事感兴趣，我们建议你分析一些免费提供的[住房抵押披露法数据](https://oreil.ly/xYXdt)来满足自己的好奇心。
- en: ^([2](ch10.html#idm45989998102272-marker)) For an additional implementation
    and example usage of reweighing, check out AIF360’s [“Detecting and Mitigating
    Age Bias on Credit Decisions”](https://oreil.ly/ypEQc).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#idm45989998102272-marker)) 想要了解更多关于重新加权的实施和示例用法，请查看AIF360的["检测和减轻信贷决策中的年龄偏见"](https://oreil.ly/ypEQc)。
- en: ^([3](ch10.html#idm45989996778176-marker)) And don’t forget the bias remediation
    [decision tree (slide 40)](https://oreil.ly/vDv4T).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.html#idm45989996778176-marker)) 不要忘记偏倚修正[决策树（幻灯片 40）](https://oreil.ly/vDv4T)。
