- en: Chapter 5\. Introduction to Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。自然语言处理简介
- en: Natural language processing (NLP) is a technique in artificial intelligence
    that deals with the understanding of human-based language. It involves programming
    techniques to create a model that can understand language, classify content, and
    even generate and create new compositions in human-based language. We’ll be exploring
    these techniques over the next few chapters. There are also lots of services that
    use NLP to create applications such as chatbots, but that’s not in the scope of
    this book—instead, we’ll be looking at the foundations of NLP and how to model
    language so that you can train neural networks to understand and classify text.
    For a little fun, you’ll also see how to use the predictive elements of a machine
    learning model to write some poetry!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是人工智能中处理理解人类语言的技术。它涉及编程技术，用于创建能够理解语言、分类内容，甚至生成和创作新人类语言组合的模型。在接下来的几章中，我们将探讨这些技术。还有很多服务使用NLP创建应用程序，如聊天机器人，但这不在本书的范围内——相反，我们将研究NLP的基础以及如何建模语言，以便你能训练神经网络理解和分类文本。稍作调剂，你还将看到如何利用机器学习模型的预测元素来写一些诗歌！
- en: We’ll start this chapter by looking at how to decompose language into numbers,
    and how those numbers can then be used in neural networks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从如何将语言分解成数字开始这一章节，并探讨这些数字如何在神经网络中使用。
- en: Encoding Language into Numbers
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将语言编码成数字
- en: You can encode language into numbers in many ways. The most common is to encode
    by letters, as is done naturally when strings are stored in your program. In memory,
    however, you don’t store the letter *a* but an encoding of it—perhaps an ASCII
    or Unicode value, or something else. For example, consider the word *listen*.
    This can be encoded with ASCII into the numbers 76, 73, 83, 84, 69, and 78\. This
    is good, in that you can now use numerics to represent the word. But then consider
    the word *silent*, which is an antigram of *listen*. The same numbers represent
    that word, albeit in a different order, which might make building a model to understand
    the text a little difficult.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用多种方式将语言编码成数字。最常见的方法是按字母编码，就像在程序中存储字符串时自然而然地做的那样。然而，在内存中，你不是存储字母*a*，而是它的编码——也许是ASCII或Unicode值，或者其他什么。例如，考虑单词*listen*。可以用ASCII将其编码为数字76、73、83、84、69和78。这样做很好，因为现在你可以用数字来代表这个词。但是再考虑一下单词*silent*，它是*listen*的反字。同样的数字表示那个单词，尽管顺序不同，这可能会使建立理解文本的模型变得有些困难。
- en: Note
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: An *antigram* is a word that’s an anagram of another, but has the opposite meaning.
    For example, *united* and *untied* are antigrams, as are *restful* and *fluster*,
    *Santa* and *Satan*, *forty-five* and *over fifty*. My job title used to be Developer
    Evangelist but has since changed to Developer Advocate—which is a good thing because
    *Evangelist* is an antigram for *Evil’s Agent*!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*反字*是一个单词，它是另一个单词的字谜，但意思相反。例如，*united*和*untied*是反字，*restful*和*fluster*也是，*Santa*和*Satan*，*forty-five*和*over
    fifty*。我的职称过去是开发者福音使，但现在改为开发者倡导者——这是一件好事，因为*福音使*是*邪恶的代理人*的反字！'
- en: A better alternative might be to use numbers to encode entire words instead
    of the letters within them. In that case, *silent* could be number *x* and *listen*
    number *y*, and they wouldn’t overlap with each other.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的选择可能是使用数字来编码整个单词而不是其中的字母。在这种情况下，*silent*可以是数字*x*，*listen*可以是数字*y*，它们不会彼此重叠。
- en: Using this technique, consider a sentence like “I love my dog.” You could encode
    that with the numbers [1, 2, 3, 4]. If you then wanted to encode “I love my cat.”
    it could be [1, 2, 3, 5]. You’ve already gotten to the point where you can tell
    that the sentences have a similar meaning because they’re similar numerically—[1,
    2, 3, 4] looks a lot like [1, 2, 3, 5].
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术，考虑一句话像“I love my dog.”。你可以用数字[1, 2, 3, 4]来编码它。如果你想编码“I love my cat.”，它可能是[1,
    2, 3, 5]。你已经到了能够告诉这些句子有相似含义的地步，因为它们在数值上相似——[1, 2, 3, 4]看起来很像[1, 2, 3, 5]。
- en: This process is called *tokenization*, and you’ll explore how to do that in
    code next.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程称为*标记化*，接下来你将学习如何在代码中实现它。
- en: Getting Started with Tokenization
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始标记化
- en: 'TensorFlow Keras contains a library called `preprocessing` that provides a
    number of extremely useful tools to prepare data for machine learning. One of
    these is a `Tokenizer` that will allow you to take words and turn them into tokens.
    Let’s see it in action with a simple example:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Keras包含一个名为`preprocessing`的库，提供了许多非常有用的工具来准备机器学习数据。其中之一是一个`Tokenizer`，它允许您将单词转换为标记。让我们用一个简单的例子来看它的工作原理：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this case, we create a `Tokenizer` object and specify the number of words
    that it can tokenize. This will be the maximum number of tokens to generate from
    the corpus of words. We have a very small corpus here containing only six unique
    words, so we’ll be well under the one hundred specified.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们创建了一个`Tokenizer`对象，并指定它可以标记化的单词数。这将是从单词语料库生成的最大标记数。这里我们的语料库非常小，只包含六个唯一的单词，因此我们将远远低于指定的一百个单词。
- en: 'Once we have a tokenizer, calling `fit_on_texts` will create the tokenized
    word index. Printing this out will show a set of key/value pairs for the words
    in the corpus, like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了一个分词器，调用`fit_on_texts`将创建分词的单词索引。打印出来将显示语料库中单词的一组键/值对，如下所示：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The tokenizer is quite flexible. For example, if we were to expand the corpus
    with another sentence containing the word “today” but with a question mark after
    it, the results show that it would be smart enough to filter out “today?” as just
    “today”:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器非常灵活。例如，如果我们用另一个包含单词“今天”的句子扩展语料库，但后面加上了问号，结果显示它会智能地过滤掉“今天？”只保留“今天”：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This behavior is controlled by the `filters` parameter to the tokenizer, which
    defaults to removing all punctuation except the apostrophe character. So for example,
    “Today is a sunny day” would become a sequence containing [1, 2, 3, 4, 5] with
    the preceding encodings, and “Is it sunny today?” would become [2, 7, 4, 1]. Once
    you have the words in your sentences tokenized, the next step is to convert your
    sentences into lists of numbers, with the number being the value where the word
    is the key.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为由分词器的`filters`参数控制，默认情况下除了撇号字符之外会删除所有标点符号。因此，例如，“今天是个晴天”将变成一个包含[1, 2, 3,
    4, 5]的序列，而“今天是晴天吗？”将变成[2, 7, 4, 1]。一旦您的句子中的单词被分词，下一步就是将您的句子转换为数字列表，其中数字是单词作为键的值。
- en: Turning Sentences into Sequences
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将句子转换为序列
- en: 'Now that you’ve seen how to take words and tokenize them into numbers, the
    next step is to encode the sentences into sequences of numbers. The tokenizer
    has a method for this called `text_to_sequences`—all you have to do is pass it
    your list of sentences, and it will give you back a list of sequences. So, for
    example, if you modify the preceding code like this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到如何将单词分词成数字，下一步是将句子编码成数字序列。这个分词器有一个叫做`text_to_sequences`的方法——您只需将您的句子列表传递给它，它将返回一个序列列表。因此，例如，如果您像这样修改前面的代码：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'you’ll be given the sequences representing the three sentences. Remembering
    that the word index is this:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您将获得表示这三个句子的序列。记住单词索引如下：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'the output will look like this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE5]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can then substitute in the words for the numbers and you’ll see that the
    sentences make sense.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您可以用单词替换数字，您将看到句子是有意义的。
- en: Now consider what happens if you are training a neural network on a set of data.
    The typical pattern is that you have a set of data used for training that you
    know won’t cover 100% of your needs, but you hope covers as much as possible.
    In the case of NLP, you might have many thousands of words in your training data,
    used in many different contexts, but you can’t have every possible word in every
    possible context. So when you show your neural network some new, previously unseen
    text, containing previously unseen words, what might happen? You guessed it—it
    will get confused because it simply has no context for those words, and, as a
    result, any prediction it gives will be negatively affected.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑一下，如果您在一组数据上训练神经网络会发生什么。典型模式是，您有一组用于训练的数据，您知道它不会覆盖您所有的需求，但您希望它尽可能地覆盖。在自然语言处理的情况下，您可能有成千上万个单词在您的训练数据中，用在许多不同的上下文中，但您不可能在每种可能的上下文中都有每个可能的单词。因此，当您向神经网络展示一些新的、以前未见过的文本，包含以前未见过的单词时，可能会发生什么？您猜对了——它会感到困惑，因为它根本没有这些单词的上下文，结果它给出的任何预测都会受到负面影响。
- en: Using out-of-vocabulary tokens
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用了超出词汇表的标记
- en: 'One tool to use to handle these situations is an *out-of-vocabulary* (OOV)
    token. This can help your neural network to understand the context of the data
    containing previously unseen text. For example, given the previous small example
    corpus, suppose you want to process sentences like these:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这些情况的一种工具是*超出词汇表*（OOV）标记。这可以帮助你的神经网络理解包含以前未见过文本的数据的上下文。例如，考虑前面的小例子语料库，假设你想处理这样的句子：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Remember that you’re not adding this input to the corpus of existing text (which
    you can think of as your training data), but considering how a pretrained network
    might view this text. If you tokenize it with the words that you’ve already used
    and your existing tokenizer, like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你并不是将此输入添加到现有文本语料库中（可以将其视为你的训练数据），而是考虑预训练网络可能如何查看此文本。如果使用你已经使用过的单词和现有的分词器对其进行分词，就像这样：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Your results will look like this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果将如下所示：
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So the new sentences, swapping back tokens for words, would be “today is a day”
    and “it rainy.”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所以新的句子，将单词替换回标记后，会变成“today is a day”和“it rainy.”
- en: 'As you can see, you’ve pretty much lost all context and meaning. An out-of-vocabulary
    token might help here, and you can specify it in the tokenizer. You do this by
    adding a parameter called `oov_token`, as shown here—you can assign it any string
    you like, but make sure it’s not one that appears elsewhere in your corpus:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，你几乎失去了所有的上下文和含义。在这里可能会有帮助的是一个超出词汇表的标记，你可以在分词器中指定它。你可以通过添加一个称为`oov_token`的参数来实现这一点，就像这样——你可以分配任何你喜欢的字符串，但确保它不是语料库中其他地方出现过的字符串：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You’ll see the output has improved a bit:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到输出稍有改善：
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Your tokens list has a new item, “<OOV>,” and your test sentences maintain their
    length. Reverse-encoding them will now give “today is a <OOV> day” and “<OOV>
    it <OOV> rainy <OOV>.”
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你的标记列表有了一个新项目，“<OOV>”，而你的测试句子保持了它们的长度。反向编码它们现在会得到“today is a <OOV> day”和“<OOV>
    it <OOV> rainy <OOV>.”
- en: The former is much closer to the original meaning. The latter, because most
    of its words aren’t in the corpus, still lacks a lot of context, but it’s a step
    in the right direction.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 前者更接近原始含义。后者因为大部分词汇不在语料库中，仍然缺乏很多上下文，但这是朝着正确方向迈出的一步。
- en: Understanding padding
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解填充
- en: When training neural networks you typically need all your data to be in the
    same shape. Recall from earlier chapters that when training with images, you reformatted
    the images to be the same width and height. With text you face the same issue—once
    you’ve tokenized your words and converted your sentences into sequences, they
    can all be different lengths. To get them to be the same size and shape, you can
    use *padding*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，通常需要使所有数据具有相同的形状。回想一下前几章，当处理图像训练时，你将图像重新格式化为相同的宽度和高度。处理文本时，你面临相同的问题——一旦对单词进行了分词并将句子转换为序列，它们可能具有不同的长度。为了使它们具有相同的大小和形状，你可以使用*填充*。
- en: 'To explore padding, let’s add another, much longer, sentence to the corpus:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索填充的功能，让我们在语料库中添加另一个更长的句子：
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When you sequence that, you’ll see that your lists of numbers have different
    lengths:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当你对它进行序列化时，你会看到你的数字列表长度不同：
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: (When you print the sequences they’ll all be on a single line, but I’ve broken
    them into separate lines here for clarity.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: （当你打印这些序列时，它们会全部在一行上，但我在这里为了清晰起见将它们分成了不同的行。）
- en: 'If you want to make these the same length, you can use the `pad_sequences`
    API. First, you’ll need to import it:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使它们具有相同的长度，你可以使用`pad_sequences` API。首先，你需要导入它：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using the API is very straightforward. To convert your (unpadded) sequences
    into a padded set, you simply call `pad_sequences` like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个 API 非常简单。要将（未填充的）序列转换为填充后的集合，你只需调用`pad_sequences`，像这样：
- en: '[PRE14]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You’ll get a nicely formatted set of sequences. They’ll also be on separate
    lines, like this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到一组格式良好的序列。它们也会像这样分开显示在不同的行上：
- en: '[PRE15]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The sequences get padded with `0`, which isn’t a token in our word list. If
    you had wondered why the token list began at 1 when typically programmers count
    from 0, now you know!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 序列将被填充为`0`，这不是我们单词列表中的标记。如果你曾想知道为什么标记列表从 1 开始，而程序员通常从 0 开始计数，现在你知道了！
- en: You now have something that’s regularly shaped that you can use for training.
    But before going there, let’s explore this API a little, because it gives you
    many options that you can use to improve your data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了一个经过规则化处理的东西，可以用于训练。但在深入讨论之前，让我们稍微探讨一下这个 API，因为它提供了许多可以用来改进数据的选项。
- en: 'First, you might have noticed that in the case of the shorter sentences, to
    get them to be the same shape as the longest one, the requisite number of zeros
    were added at the beginning. This is called *prepadding*, and it’s the default
    behavior. You can change this using the `padding` parameter. For example, if you
    want your sequences to be padded with zeros at the end, you can use:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您可能已经注意到，在较短的句子的情况下，为了使它们与最长句子的形状相同，必须在开头添加相应数量的零。这称为*预填充*，这是默认行为。您可以使用`padding`参数来更改此行为。例如，如果您希望您的序列在末尾填充零，可以使用：
- en: '[PRE16]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output from this will be:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由此输出的结果将是：
- en: '[PRE17]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can see that now the words are at the beginning of the padded sequences,
    and the `0` characters are at the end.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到现在单词在填充序列的开头，而`0`字符在末尾。
- en: 'The next default behavior you may have observed is that the sentences were
    all made to be the same length as the *longest* one. It’s a sensible default because
    it means you don’t lose any data. The trade-off is you get a lot of padding. But
    what if you don’t want this, perhaps because you have one crazy long sentence
    that means you’d have too much padding in the padded sequences? To fix that you
    can use the `maxlen` parameter, specifying the desired maximum length, when calling
    `pad_sequences`, like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能注意到的下一个默认行为是，所有的句子都被制作成与*最长*句子相同的长度。这是一个合理的默认行为，因为它意味着您不会丢失任何数据。这样做的代价是会有很多填充。但是如果您不希望这样，也许因为您有一个非常长的句子，这意味着在填充的序列中会有太多的填充。为了解决这个问题，您可以在调用`pad_sequences`时使用`maxlen`参数，指定所需的最大长度，例如：
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output from this will be:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由此输出的结果将是：
- en: '[PRE19]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now your padded sequences are all the same length, and there isn’t too much
    padding. You have lost some words from your longest sentence, though, and they’ve
    been truncated from the beginning. What if you don’t want to lose the words from
    the beginning and instead want them truncated from the *end* of the sentence?
    You can override the default behavior with the `truncating` parameter, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您的填充序列的长度都相同，并且填充不过多。不过，您最长的句子确实丢失了一些单词，并且它们被从句子的开头截断了。如果您不想丢失开头的单词，而是希望它们从句子的*末尾*截断，您可以使用`truncating`参数覆盖默认行为，如下所示：
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The result of this will show that the longest sentence is now truncated at
    the end instead of the beginning:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的结果将显示最长的句子现在是在结尾而不是开头截断：
- en: '[PRE21]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: TensorFlow supports training using “ragged” (different-shaped) tensors, which
    is perfect for the needs of NLP. Using them is a bit more advanced than what we’re
    covering in this book, but once you’ve completed the introduction to NLP that’s
    provided in the next few chapters, you can explore the [documentation](https://oreil.ly/I1IJW)
    to learn more.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow支持使用“ragged”（形状不同的）张量进行训练，这非常适合NLP的需求。使用它们比我们在本书中覆盖的内容更为高级，但是一旦您完成了接下来几章提供的NLP介绍，您可以探索[文档](https://oreil.ly/I1IJW)以获取更多信息。
- en: Removing Stopwords and Cleaning Text
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去除停用词和清理文本
- en: In the next section you’ll look at some real-world datasets, and you’ll find
    that there’s often text that you *don’t* want in your dataset. You may want to
    filter out so-called *stopwords* that are too common and don’t add any meaning,
    like “the,” “and,” and “but.” You may also encounter a lot of HTML tags in your
    text, and it would be good to have a clean way to remove them. Other things you
    might want to filter out include rude words, punctuation, or names. Later we’ll
    explore a dataset of tweets, which often have somebody’s user ID in them, and
    we’ll want to filter those out.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将看到一些真实世界的数据集，您会发现通常有一些您不希望在数据集中的文本。您可能希望过滤掉所谓的*停用词*，它们太常见且没有任何意义，例如“the”，“and”和“but”。您在文本中可能还会遇到许多HTML标签，最好有一种清理方法将它们移除。您可能还希望过滤掉其他诸如粗鲁的词、标点符号或姓名之类的内容。稍后我们将探索一个推特数据集，这些推特通常会包含某人的用户ID，我们希望将其过滤掉。
- en: While every task is different based on your corpus of text, there are three
    main things that you can do to clean up your text programmatically.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个任务基于您的文本语料库都是不同的，但有三件主要的事情可以通过编程方式清理文本。
- en: 'The first is to strip out HTML tags. Fortunately, there’s a library called
    `BeautifulSoup` that makes this straightforward. For example, if your sentences
    contain HTML tags such as `<br>`, they’ll be removed by this code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是去除HTML标签。幸运的是，有一个叫做`BeautifulSoup`的库可以轻松实现这一点。例如，如果您的句子包含HTML标签如`<br>`，则可以通过以下代码将其删除：
- en: '[PRE22]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A common way to remove stopwords is to have a stopwords list and to preprocess
    your sentences, removing instances of stopwords. Here’s an abbreviated example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 删除停用词的常见方法是使用停用词列表预处理您的句子，删除停用词的实例。这里有一个简化的例子：
- en: '[PRE23]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: A full stopwords list can be found in some of the online [examples](https://oreil.ly/ObsjT)
    for this chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的停用词列表可以在本章的一些在线[示例](https://oreil.ly/ObsjT)中找到。
- en: 'Then, as you are iterating through your sentences, you can use code like this
    to remove the stopwords from your sentences:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在迭代句子时，您可以使用如下代码从句子中删除停用词：
- en: '[PRE24]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Another thing you might consider is stripping out punctuation, which can fool
    a stopword remover. The one just shown looks for words surrounded by spaces, so
    a stopword immediately followed by a period or a comma won’t be spotted.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还考虑删除标点符号，这可能会误导停用词移除器。刚刚展示的方法寻找被空格包围的单词，因此紧跟在句号或逗号后面的停用词将不会被发现。
- en: 'Fixing this problem is easy with the translation functions provided by the
    Python `string` library. It also comes with a constant, `string.punctuation`,
    that contains a list of common punctuation marks, so to remove them from a word
    you can do the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python `string`库提供的翻译函数轻松解决这个问题。它还提供了一个常量，`string.punctuation`，其中包含一组常见的标点符号，因此，要从单词中删除它们，您可以执行以下操作：
- en: '[PRE25]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, before filtering for stopwords, each word in the sentence has punctuation
    removed. So, if splitting a sentence gives you the word “it;” it will be converted
    to “it” and then stripped out as a stopword. Note, however, that when doing this
    you might have to update your stopwords list. It’s common for these lists to have
    abbreviated words and contractions like “you’ll” in them. The translator will
    change “you’ll” to “youll,” and if you want to have that filtered out, you’ll
    need to update your stopwords list to include it.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，在过滤停用词之前，句子中的每个单词都删除了标点符号。因此，如果分割句子后得到单词“it;”，它将被转换为“it”，然后作为停用词删除。然而，请注意，当进行此操作时，您可能需要更新停用词列表。这些列表通常包含缩写词和像“you’ll”这样的缩略词。翻译程序将“you’ll”更改为“youll”，如果您希望将其过滤掉，则需要更新您的停用词列表以包含它。
- en: Following these three steps will give you a much cleaner set of text to use.
    But of course, every dataset will have its idiosyncrasies that you’ll need to
    work with.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循这三个步骤将会为您提供一个更加干净的文本集合。当然，每个数据集都会有其特殊性，您需要与之配合工作。
- en: Working with Real Data Sources
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理真实数据源
- en: 'Now that you’ve seen the basics of getting sentences, encoding them with a
    word index, and sequencing the results, you can take that to the next level by
    taking some well-known public datasets and using the tools Python provides to
    get them into a format where they can be easily sequenced. We’ll start with one
    where a lot of the work has already been done for you in TensorFlow Datasets:
    the IMDb dataset. After that we’ll get a bit more hands-on, processing a JSON-based
    dataset and a couple of comma-separated values (CSV) datasets with emotion data
    in them!'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经了解了获取句子、使用单词索引对其进行编码并排序结果的基本知识，可以通过将一些知名的公共数据集与Python提供的工具结合使用，将其转换为可以轻松排序的格式。我们将从TensorFlow数据集中已经为您完成了大部分工作的IMDb数据集开始。之后，我们将更加亲手操作，处理基于JSON的数据集和包含情绪数据的几个逗号分隔值（CSV）数据集！
- en: Getting Text from TensorFlow Datasets
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从TensorFlow数据集获取文本
- en: We explored TFDS in [Chapter 4](ch04.xhtml#using_public_datasets_with_tensorflow_d),
    so if you’re stuck on any of the concepts in this section, you can take a quick
    look there. The goal behind TFDS is to make it as easy as possible to get access
    to data in a standardized way. It provides access to several text-based datasets;
    we’ll explore `imdb_reviews`, a dataset of 50,000 labeled movie reviews from the
    Internet Movie Database (IMDb), each of which is determined to be positive or
    negative in sentiment.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第四章](ch04.xhtml#using_public_datasets_with_tensorflow_d)中探讨了TFDS，所以如果您在本节的某些概念上遇到困难，可以快速查看那里。TFDS的目标是尽可能地简化以标准化方式获取数据的过程。它提供了对多个基于文本的数据集的访问；我们将探索`imdb_reviews`，这是来自互联网电影数据库（IMDb）的50,000条带有正面或负面情感标签的影评数据集。
- en: 'This code will load the training split from the IMDb dataset and iterate through
    it, adding the text field containing the review to a list called `imdb_sentences`.
    Reviews are a tuple of the text and a label containing the sentiment of the review.
    Note that by wrapping the `tfds.load` call in `tfds.as_numpy` you ensure that
    the data will be loaded as strings, not tensors:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将从 IMDb 数据集加载训练拆分，并遍历它，将包含评论的文本字段添加到名为`imdb_sentences`的列表中。评论是包含评论情感的文本和标签的元组。请注意，通过将`tfds.load`调用包装在`tfds.as_numpy`中，您确保数据将被加载为字符串，而不是张量：
- en: '[PRE26]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once you have the sentences, you can then create a tokenizer and fit it to
    them as before, as well as creating a set of sequences:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了句子，你就可以创建一个分词器并像以前那样将它们拟合到其中，同时创建一个序列集：
- en: '[PRE27]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You can also print out your word index to inspect it:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以打印出你的单词索引来检查它：
- en: '[PRE28]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'It’s too large to show the entire index, but here are the top 20 words. Note
    that the tokenizer lists them in order of frequency in the dataset, so common
    words like “the,” “and,” and “a” are indexed:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 它太大了，不能显示整个索引，但这里是前20个单词。请注意，分词器按照数据集中单词的频率顺序列出它们，因此像“the”、“and”和“a”这样的常见单词被索引了：
- en: '[PRE29]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: These are stopwords, as described in the previous section. Having these present
    can impact your training accuracy because they’re the most common words and they’re
    nondistinct.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是停用词，如前一节所述。由于它们是最常见的单词且不明显，它们可能会影响您的训练准确性。
- en: Also note that “br” is included in this list, because it’s commonly used in
    this corpus as the `<br>` HTML tag.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，“br”包含在此列表中，因为它在语料库中常用作`<br>`HTML标签。
- en: 'You can update the code to use `BeautifulSoup` to remove the HTML tags, add
    string translation to remove the punctuation, and remove stopwords from the given
    list as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以更新代码，使用`BeautifulSoup`去除 HTML 标签，添加字符串翻译以删除标点符号，并从给定列表中删除停用词如下：
- en: '[PRE30]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Note that the sentences are converted to lowercase before processing because
    all the stopwords are stored in lowercase. When you print out your word index
    now, you’ll see this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的句子在处理之前都会转换为小写，因为所有的停用词都存储在小写中。当你打印出你的单词索引时，你会看到这样：
- en: '[PRE31]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You can see that this is much cleaner than before. There’s always room to improve,
    however, and one thing I noted when looking at the full index was that some of
    the less common words toward the end were nonsensical. Often reviewers would combine
    words, for example with a dash (“annoying-conclusion”) or a slash (“him/her”),
    and the stripping of punctuation would incorrectly turn these into a single word.
    You can avoid this with a bit of code that adds spaces around these characters,
    so I added the following immediately after the sentence was created:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，这比以前要干净得多。然而，总有改进的余地，当我查看完整的索引时，我注意到末尾的一些不太常见的单词是荒谬的。通常评论者会组合词语，例如用破折号（“annoying-conclusion”）或斜杠（“him/her”），而去除标点符号会错误地将它们转换为单个单词。您可以通过在创建句子后立即添加一些代码来避免这种情况，在这里我添加了以下内容：
- en: '[PRE32]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This turned combined words like “him/her” into “him / her,” which then had the
    “/” stripped out and got tokenized into two words. This might lead to better training
    results later.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这将组合词如“him/her”转换为“him / her”，然后将“/”去除并分词为两个单词。这可能会在后续的训练结果中带来更好的效果。
- en: 'Now that you have a tokenizer for the corpus, you can encode your sentences.
    For example, the simple sentences we were looking at earlier in the chapter will
    come out like this:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了语料库的分词器，您可以对句子进行编码。例如，我们之前在本章中看到的简单句子将会像这样输出：
- en: '[PRE33]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: If you decode these, you’ll see that the stopwords are dropped and you get the
    sentences encoded as “today sunny day,” “today rainy day,” and “sunny today.”
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你解码它们，你会发现停用词被删除了，你得到的句子编码为“今天晴天”，“今天雨天”，和“晴天今天”。
- en: 'If you want to do this in code, you can create a new `dict` with the reversed
    keys and values (i.e., for a key/value pair in the word index, make the value
    the key and the key the value) and do the lookup from that. Here’s the code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在代码中实现这一点，你可以创建一个新的`dict`，将键和值颠倒（即，对于单词索引中的键/值对，将值作为键，将键作为值），然后从中进行查找。以下是代码：
- en: '[PRE34]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This will give the following result:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下结果：
- en: '[PRE35]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Using the IMDb subwords datasets
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 IMDb 子词数据集
- en: TFDS also contains a couple of preprocessed IMDb datasets using subwords. Here,
    you don’t have to break up the sentences by word; they have already been split
    up into subwords for you. Using subwords is a happy medium between splitting the
    corpus into individual letters (relatively few tokens with low semantic meaning)
    and individual words (many tokens with high semantic meaning), and this approach
    can often be used very effectively to train a classifier for language. These datasets
    also include the encoders and decoders used to split and encode the corpus.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: TFDS 还包含几个使用子词预处理的 IMDb 数据集。在这里，您无需按单词分割句子；它们已经被分割成子词。使用子词是在将语料库分割成单个字母（具有较低语义意义的相对较少标记）和单个单词（具有高语义意义的许多标记）之间的一种折衷方法，这种方法通常非常有效地用于语言分类器的训练。这些数据集还包括用于分割和编码语料库的编码器和解码器。
- en: 'To access them, you can call `tfds.load` and pass it `imdb_reviews/subwords8k`
    or `imdb_reviews/subwords32k` like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问它们，可以调用`tfds.load`并像这样传递`imdb_reviews/subwords8k`或`imdb_reviews/subwords32k`：
- en: '[PRE36]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You can access the encoder on the `info` object like this. This will help you
    see the `vocab_size`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像这样访问`info`对象上的编码器。这将帮助您查看`vocab_size`：
- en: '[PRE37]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This will output `8185` because the vocabulary in this instance is made up
    of 8,185 tokens. If you want to see the list of subwords, you can get it with
    the `encoder.subwords` property:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出`8185`，因为在此实例中，词汇表由 8,185 个标记组成。如果要查看子词列表，可以使用`encoder.subwords`属性获取它：
- en: '[PRE38]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Some things you might notice here are that stopwords, punctuation, and grammar
    are all in the corpus, as are HTML tags like `<br>`. Spaces are represented by
    underscores, so the first token is the word “the.”
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你可能注意到的一些事情是停用词、标点和语法都在语料库中，HTML 标签如`<br>`也在其中。空格用下划线表示，所以第一个标记是单词“the”。
- en: 'Should you want to encode a string, you can do so with the encoder like this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要编码一个字符串，可以像这样使用编码器：
- en: '[PRE39]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output of this will be a list of tokens:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出一个标记列表：
- en: '[PRE40]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'So, your five words are encoded into seven tokens. To see the tokens you can
    use the `subwords` property on the encoder, which returns an array. It’s zero-based,
    so whereas “Tod” in “Today” was encoded as `6427`, it’s the 6,426th item in the
    array:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您的五个单词被编码为七个标记。要查看标记，可以使用编码器的`subwords`属性，它返回一个数组。它是从零开始的，因此“Tod”在“Today”中被编码为`6427`，它是数组中的第
    6,426 项：
- en: '[PRE41]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'If you want to decode, you can use the `decode` method of the encoder:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要解码，可以使用编码器的`decode`方法：
- en: '[PRE42]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The latter lines will have an identical result because `encoded_string`, despite
    its name, is a list of tokens just like the one that is hardcoded on the next
    line.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其名称，后几行将具有相同的结果，尽管`encoded_string`是一个标记列表，就像在下一行上硬编码的那样。
- en: Getting Text from CSV Files
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 CSV 文件获取文本
- en: While TFDS has lots of great datasets, it doesn’t have everything, and often
    you’ll need to manage loading the data yourself. One of the most common formats
    in which NLP data is available is CSV files. Over the next couple of chapters
    you’ll use a CSV of Twitter data that I adapted from the open source [Sentiment
    Analysis in Text dataset](https://oreil.ly/QMMwV). You will use two different
    datasets, one where the emotions have been reduced to “positive” or “negative”
    for binary classification and one where the full range of emotion labels is used.
    The structure of each is identical, so I’ll just show the binary version here.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 TFDS 拥有大量优秀的数据集，但并非涵盖一切，通常您需要自行加载数据。自然语言处理数据最常见的格式之一是 CSV 文件。在接下来的几章中，您将使用我从开源[文本情感分析数据集](https://oreil.ly/QMMwV)中调整的
    Twitter 数据的 CSV 文件。您将使用两个不同的数据集，一个将情感减少为“positive”或“negative”以进行二元分类，另一个使用完整的情感标签范围。每个的结构都是相同的，因此我只会在此处显示二元版本。
- en: The Python `csv` library makes handling CSV files straightforward. In this case,
    the data is stored with two values per line. This first is a number (0 or 1) denoting
    if the sentiment is negative or positive. The second is a string containing the
    text.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的`csv`库使处理 CSV 文件变得简单。在这种情况下，数据存储为每行两个值。第一个是数字（0 或 1），表示情感是否为负面或正面。第二个是包含文本的字符串。
- en: 'The following code will read the CSV and do similar preprocessing to what we
    saw in the previous section. It adds spaces around the punctuation in compound
    words, uses `BeautifulSoup` to strip HTML content, and then removes all punctuation
    characters:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码将读取 CSV 文件，并对我们在前一节中看到的类似预处理进行处理。它在复合词中的标点周围添加空格，使用`BeautifulSoup`去除 HTML
    内容，然后移除所有标点符号：
- en: '[PRE43]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This will give you a list of 35,327 sentences.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为您提供一个包含 35,327 句子的列表。
- en: Creating training and test subsets
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建训练和测试子集
- en: 'Now that the text corpus has been read into a list of sentences, you’ll need
    to split it into training and test subsets for training a model. For example,
    if you want to use 28,000 sentences for training with the rest held back for testing,
    you can use code like this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在文本语料库已经被读入句子列表中，您需要将其拆分为训练和测试子集以训练模型。例如，如果您想使用28,000个句子进行训练，并将其余部分保留用于测试，您可以使用如下代码：
- en: '[PRE44]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now that you have a training set, you need to create the word index from it.
    Here is the code to use the tokenizer to create a vocabulary with up to 20,000
    words. We’ll set the maximum length of a sentence at 10 words, truncate longer
    ones by cutting off the end, pad shorter ones at the end, and use “<OOV>”:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了一个训练集，您需要从中创建单词索引。以下是使用标记器创建最多20,000个单词的词汇表的代码。我们将句子的最大长度设置为10个单词，通过截断更长的句子来结束，通过在末尾填充较短的句子，并使用“<OOV>”：
- en: '[PRE45]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You can inspect the results by looking at `training_sequences` and `training_padded`.
    For example, here we print the first item in the training sequence, and you can
    see how it’s padded to a max length of 10:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看`training_sequences`和`training_padded`来检查结果。例如，在这里我们打印训练序列的第一项，您可以看到它是如何被填充到最大长度10的：
- en: '[PRE46]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You can also inspect the word index by printing it:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以通过打印来检查单词索引：
- en: '[PRE47]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: There are many words here you might want to consider getting rid of as stopwords,
    such as “like” and “dont.” It’s always useful to inspect the word index.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多词语你可能想要考虑作为停用词去掉，比如“like”和“dont”。检查词索引总是很有用的。
- en: Getting Text from JSON Files
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从JSON文件获取文本
- en: Another very common format for text files is JavaScript Object Notation (JSON).
    This is an open standard file format used often for data interchange, particularly
    with web applications. It’s human-readable and designed to use name/value pairs.
    As such, it’s particularly well suited for labeled text. A quick search of Kaggle
    datasets for JSON yields over 2,500 results. Popular datasets such as the Stanford
    Question Answering Dataset (SQuAD), for example, are stored in JSON.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种非常常见的文本文件格式是JavaScript对象表示法（JSON）。这是一种开放标准的文件格式，通常用于数据交换，特别是与Web应用程序的交互。它易于人类阅读，并设计为使用名称/值对。因此，它特别适合用于标记文本。在Kaggle数据集的快速搜索中，JSON的结果超过2,500个。像斯坦福问答数据集（SQuAD）这样的流行数据集，例如，存储在JSON中。
- en: 'JSON has a very simple syntax, where objects are contained within braces as
    name/value pairs separated by a comma. For example, a JSON object representing
    my name would be:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: JSON有一个非常简单的语法，对象被包含在大括号中，作为以逗号分隔的名称/值对。例如，代表我的名字的JSON对象将是：
- en: '[PRE48]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'JSON also supports arrays, which are a lot like Python lists, and are denoted
    by the square bracket syntax. Here’s an example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: JSON还支持数组，这些数组非常类似于Python列表，并且由方括号语法表示。这里是一个例子：
- en: '[PRE49]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Objects can also contain arrays, so this is perfectly valid JSON:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对象也可以包含数组，因此这是完全有效的JSON：
- en: '[PRE50]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'A smaller dataset that’s stored in JSON and a lot of fun to work with is the
    News Headlines Dataset for Sarcasm Detection by [Rishabh Misra](https://oreil.ly/wZ3oD),
    available on [Kaggle](https://oreil.ly/_AScB). This dataset collects news headlines
    from two sources: *The Onion* for funny or sarcastic ones, and the *HuffPost*
    for normal headlines.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一个存储在JSON中并且非常有趣的小数据集是由[Rishabh Misra](https://oreil.ly/wZ3oD)创建的用于讽刺检测的新闻标题数据集，可以在[Kaggle](https://oreil.ly/_AScB)上获取。这个数据集收集了来自两个来源的新闻标题：*The
    Onion* 提供有趣或讽刺的标题，*HuffPost* 提供正常的标题。
- en: 'The file structure in the Sarcasm dataset is very simple:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 讽刺数据集中的文件结构非常简单：
- en: '[PRE51]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The dataset consists of about 26,000 items, one per line. To make it more readable
    in Python I’ve created a version that encloses these in an array so it can be
    read as a single list, which is used in the source code for this chapter.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含大约26,000个项目，每行一个。为了在Python中使其更易读，我创建了一个将这些项目封装在数组中的版本，这样它可以作为单个列表进行读取，这在本章的源代码中使用。
- en: Reading JSON files
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读取JSON文件
- en: Python’s `json` library makes reading JSON files simple. Given that JSON uses
    name/value pairs, you can index the content based on the name. So, for example,
    for the Sarcasm dataset you can create a file handle to the JSON file, open it
    with the `json` library, have an iterable go through, read each field line by
    line, and get the data item using the name of the field.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Python的`json`库使得读取JSON文件变得简单。鉴于JSON使用名称/值对，您可以根据字段的名称索引内容。因此，例如，对于讽刺数据集，您可以创建一个文件句柄到JSON文件，使用`json`库打开它，通过迭代逐行读取每个字段，通过字段的名称获取数据项。
- en: 'Here’s the code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE52]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This makes it simple to create lists of sentences and labels as you’ve done
    throughout this chapter, and then tokenize the sentences. You can also do preprocessing
    on the fly as you read a sentence, removing stopwords, HTML tags, punctuation,
    and more. Here’s the complete code to create lists of sentences, labels, and URLs,
    while having the sentences cleaned of unwanted words and characters:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得创建句子和标签列表变得简单，就像您在整个本章中所做的那样，并对句子进行分词。您还可以在阅读句子时动态进行预处理，删除停用词，HTML标签，标点符号等。以下是创建句子、标签和URL列表的完整代码，同时清理了不需要的词语和字符：
- en: '[PRE53]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'As before, these can be split into training and test sets. If you want to use
    23,000 of the 26,000 items in the dataset for training, you can do the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，这些可以分为训练集和测试集。如果您想要使用数据集中的26,000项中的23,000项进行训练，可以执行以下操作：
- en: '[PRE54]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'To tokenize the data and get it ready for training, you can follow the same
    approach as earlier. Here, we again specify a vocab size of 20,000 words, a maximum
    sequence length of 10 with truncation and padding at the end, and an OOV token
    of “<OOV>”:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对数据进行分词并准备好进行训练，您可以采用与之前相同的方法。在这里，我们再次指定词汇量为20,000个词，最大序列长度为10，末尾截断和填充，并使用“<OOV>”作为OOV标记：
- en: '[PRE55]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output will be the whole index, in order of word frequency:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将按单词频率顺序排列整个索引：
- en: '[PRE56]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Hopefully the similar-looking code will help you see the pattern that you can
    follow when preparing text for neural networks to classify or generate. In the
    next chapter you’ll see how to build a classifier for text using embeddings, and
    in [Chapter 7](ch07.xhtml#recurrent_neural_networks_for_natural_l) you’ll take
    that a step further, exploring recurrent neural networks. Then, in [Chapter 8](ch08.xhtml#using_tensorflow_to_create_text),
    you’ll see how to further enhance the sequence data to create a neural network
    that can generate new text!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 希望类似的代码能帮助您看到在准备文本供神经网络分类或生成时可以遵循的模式。在下一章中，您将看到如何使用嵌入来构建文本分类器，在[第7章](ch07.xhtml#recurrent_neural_networks_for_natural_l)中，您将进一步探索，探讨循环神经网络。然后，在[第8章](ch08.xhtml#using_tensorflow_to_create_text)中，您将看到如何进一步增强序列数据以创建能够生成新文本的神经网络！
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In earlier chapters you used images to build a classifier. Images, by definition,
    are highly structured. You know their dimension. You know the format. Text, on
    the other hand, can be far more difficult to work with. It’s often unstructured,
    can contain undesirable content such as formatting instructions, doesn’t always
    contain what you want, and often has to be filtered to remove nonsensical or irrelevant
    content. In this chapter you saw how to take text and convert it to numbers using
    word tokenization, and then explored how to read and filter text in a variety
    of formats. Given these skills, you’re now ready to take the next step and learn
    how *meaning* can be inferred from words—the first step in understanding natural
    language.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，您使用图像构建了一个分类器。图像本质上是高度结构化的。您知道它们的尺寸。您知道格式。另一方面，文本可能要复杂得多。它经常是非结构化的，可能包含不想要的内容，比如格式化指令，不总是包含您想要的内容，通常必须进行过滤以去除荒谬或无关的内容。在本章中，您学习了如何使用单词分词将文本转换为数字，并探讨了如何阅读和过滤各种格式的文本。有了这些技能，您现在已经准备好迈出下一步，学习如何从单词中推断*含义*——这是理解自然语言的第一步。
