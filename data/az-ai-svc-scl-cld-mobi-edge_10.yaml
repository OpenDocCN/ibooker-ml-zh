- en: Chapter 7\. Responsible AI Development and Use
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章 负责任的人工智能开发和使用
- en: In the previous chapters, we’ve looked at how to use the key Microsoft cloud
    AI services. But it’s also important to think about the bigger picture of how
    you build and use AI, so that you can take advantage of these cloud AI services
    without running into problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经看到如何使用关键的微软云AI服务。但同样重要的是考虑如何构建和使用AI的整体情况，这样你就可以充分利用这些云AI服务而不会遇到问题。
- en: AI and machine learning are powerful techniques that can make software more
    useful, systems more efficient, and people more productive. But they can also
    violate privacy, create security problems, replicate and amplify bias, automate
    decision making that has negative consequences for individuals or entire groups—or
    just be plain wrong on occasion.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: AI 和机器学习是强大的技术，可以使软件更有用，系统更高效，人们更加高效。但它们也可能侵犯隐私，带来安全问题，复制和放大偏见，自动化决策可能对个人或整个群体产生负面影响，或者偶尔出错。
- en: Tip
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'This is a big and complicated topic, and you don’t have to master every nuance
    to use cloud AI services. Don’t get overwhelmed by all the issues: you don’t need
    to do everything—but equally, don’t assume that you don’t need to do anything
    about responsible AI.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个庞大而复杂的话题，你不必掌握每一个细微差别来使用云AI服务。不要被所有问题压垮：你不需要做到所有事情，但同样，也不要认为你什么都不需要做来应对负责任的人工智能。
- en: The greater the potential of AI—like diagnosing cancer, detecting earthquakes,
    predicting failures in critical infrastructure, or guiding the visually impaired
    through an unfamiliar location—the greater the responsibility to get it right
    as AI expands into areas like healthcare, education, and criminal justice, where
    the social implications and consequences are significant. But even everyday uses
    of AI could inadvertently exclude or harm users if the systems aren’t fair, accountable,
    transparent, and secure.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: AI 的潜力越大——如诊断癌症、检测地震、预测关键基础设施的故障或引导视觉障碍者穿越陌生地点——就越有责任确保它的正确性，因为AI正在扩展到医疗、教育和刑事司法等领域，其中社会影响和后果非常重要。但即使是AI的日常使用，如果系统不公平、不负责、不透明和不安全，也可能无意中排除或伤害用户。
- en: Large language models like the GPT-3 model behind the Azure OpenAI Service are
    extremely powerful. In fact, they’re so good they often sound like a human is
    writing the responses. But they’ve been trained on content both from books and
    many web pages, and that means that some viewpoints are heavily represented in
    the training data, and more marginalized perspectives are much less common or
    missing altogether. That means the results might be inaccurate, unfair, or downright
    offensive, but the language may be extremely convincing—so you need to think carefully
    about where to use the service and what precautions to put in place, so that you
    can benefit from the power it offers without exposing your users to potentially
    problematic responses.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 像Azure OpenAI服务背后的GPT-3模型这样的大型语言模型非常强大。事实上，它们非常优秀，通常会让人觉得是人类在写回答。但它们经过了书籍和许多网页内容的训练，这意味着某些观点在训练数据中被大量呈现，而更边缘化的观点则少见甚至完全缺失。这意味着结果可能不准确、不公平或极具攻击性，但语言可能非常令人信服——因此，你需要仔细考虑在哪里使用服务以及采取哪些预防措施，这样你就可以从其提供的强大功能中受益，同时不会让用户暴露于潜在的问题回应中。
- en: As well as the social and ethical implications, there are legal issues to consider.
    The European Commission has proposed what’s often termed the first legal framework
    for artificial intelligence, but the first principle of the EU General Data Protection
    Regulation (GDPR) already requires that personal data be processed in a fair,
    lawful, and transparent manner.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 除了社会和伦理影响外，还有法律问题需要考虑。欧洲委员会已提出了被称为首个人工智能法律框架的提案，但欧盟《通用数据保护条例》的第一原则已经要求个人数据以公平、合法和透明的方式处理。
- en: Too many businesses adopt AI tools without taking the time to understand that
    using AI without making sure that it’s well managed and done ethically and fairly
    can have risks to the business and its reputation. One report on AI adoption in
    financial services,^([1](ch07.xhtml#ch01fn6)) where biased AI tools could wrongly
    exclude people from life-changing economic opportunities, showed a worrying lack
    of urgency around responsible AI. Two-thirds of the executives in the study couldn’t
    explain how specific AI model decisions or predictions are made; only a fifth
    of the organizations had an AI ethics board or monitor models in production for
    fairness.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 很多企业在没有花时间了解的情况下采用人工智能工具，而不确保其良好管理和道德公正，可能对企业及其声誉造成风险。一份关于金融服务中人工智能采用的报告^([1](ch07.xhtml#ch01fn6))显示，存在对负责任人工智能缺乏紧迫性的令人担忧现象，因为存在偏见的人工智能工具可能会错误地排除人们参与改变生活的经济机会。研究中有三分之二的高管无法解释具体的人工智能模型决策或预测如何做出；只有五分之一的组织设有人工智能伦理委员会或监控生产中模型的公平性。
- en: 'There’s clearly a spectrum here: some scenarios and technologies are more straightforward,
    and some are more sensitive and will require you to do more work assessing outcomes
    and impact.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这里存在一个光谱：有些情景和技术比较直接，而有些则更为敏感，需要您进行更多工作来评估结果和影响。
- en: As AI tools become more common, businesses that use AI to make decisions will
    need to work on keeping the trust of their customers and employees by taking on
    the responsibility of making sure their AI models, systems, and platforms are
    trustworthy, fair, and can be explained to the people they affect. Start by understanding
    the potential problems and unintended consequences, then take the risks into account
    as you build and test systems so they work the way you intend them to, for all
    your users, without discriminating against or harming them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能工具变得越来越普遍，利用人工智能做出决策的企业需要努力保持客户和员工的信任，承担确保其人工智能模型、系统和平台可信、公平且能向受影响的人解释的责任。首先要了解潜在问题和意外后果，然后在构建和测试系统时考虑风险，确保系统按照预期的方式运行，适用于所有用户，而不会歧视或伤害他们。
- en: 'Remember, there are two sides to responsible AI: you need to develop your AI
    system responsibly, but you also need to ensure responsible use of the system,
    whether that’s by your own employees or by your customers. And when you’re consuming
    prebuilt cloud AI services, you need to do that responsibly as well.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，负责任的人工智能有两个方面：您需要负责任地开发您的人工智能系统，但也需要确保系统的负责任使用，无论是由您自己的员工还是客户使用。当您使用预构建的云人工智能服务时，您也需要负责任地处理。
- en: For some very sensitive AI services like custom neural voice, where you’re effectively
    duplicating a real person’s voice so you can put words into their mouth, potentially
    in languages they don’t even speak, there are specific procedures you have to
    follow to use the service to make sure it’s not abused.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些非常敏感的人工智能服务，比如定制神经语音，在这种服务中，您实际上复制了真实人声，可以用其发表言论，甚至可能是在其不会讲的语言中，需要遵循特定的程序，以确保不被滥用。
- en: You’ll have to apply to Microsoft and be approved to use the custom neural voice
    service. You then have to provide Microsoft with signed releases from the person
    whose voice you’re recording to create the neural voice to show you have informed
    consent, and you have to follow a [code of conduct](https://go.microsoft.com/fwlink/?linkid=2190282)
    for how you use the voice. That includes always making it clear that it’s a synthetic
    voice and even limiting what you can do with it. It might seem like a great idea
    to let your customers create custom messages using your celebrity voice to send
    a birthday greeting or make their voicemail more fun, but what if someone creates
    a malicious or offensive message and claims it’s real?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要申请并获得Microsoft的批准才能使用定制神经语音服务。然后，您必须向Microsoft提供使用者签署的释放文件，以证明您已获得知情同意，并且您必须遵守使用语音的[行为准则](https://go.microsoft.com/fwlink/?linkid=2190282)。这包括始终明确表明这是合成语音，甚至限制您可以使用它做什么。让您的客户使用您的名人语音创建自定义消息，比如发送生日祝福或使他们的语音信箱更有趣，可能看起来是个好主意，但如果有人创造了恶意或冒犯性消息，并声称其是真实的呢？
- en: Most cloud AI services don’t have those kind of restrictions, but there may
    be settings where they will fail to deliver the outcome you want in ways it will
    be hard to avoid. That means it’s helpful to go through the same kind of thought
    process about what your users need to know as well as what could go wrong and
    what policies you want to have in place to avoid or mitigate those problems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数云端 AI 服务不会有这种限制，但可能在某些设置下无法以你希望的方式提供结果。这意味着进行与用户需要知道的内容相同的思考过程是有帮助的，同时还要考虑可能出现的问题以及你希望制定的政策来避免或减轻这些问题。
- en: Tip
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Being able to achieve responsible AI is more than a technical issue; there
    are strong cultural and organizational aspects. Understanding the impact of AI
    and embracing diverse perspectives so AI features work well for everyone may require
    some changes in mindset. It helps to get senior figures on board to make sure
    people and projects are accountable for results. You need to be clear about both
    AI and project metrics: what counts as succeeding? And if you’re tackling an area
    where responsible AI and transparency matter, it’s likely that there will be mistakes
    and missteps, so how the organization handles problems when they arise is important,
    to make sure projects get a chance to correct issues and improve.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实现负责任的 AI 不仅仅是技术问题；还涉及到强烈的文化和组织方面的因素。理解 AI 的影响并接纳多元化的视角，以确保 AI 功能能够对每个人都有效，可能需要在思维方式上做一些改变。让高级别人物参与进来有助于确保人们和项目对结果负责。你需要清楚地了解
    AI 和项目指标：什么算是成功？如果你正在处理负责任 AI 和透明度很重要的领域，那么可能会出现错误和失误，因此组织在问题出现时如何处理就显得尤为重要，以确保项目有机会纠正问题并改进。
- en: Understanding Responsible AI
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解负责任的 AI
- en: The first AI-specific regulations are still emerging, and we cover those in
    the next chapter, but as well as legal questions, you need to consider both ethics
    and performance—which aren’t as separate as you might think.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个具体针对 AI 的法规仍在起步阶段，我们将在下一章中进行讨论，但除了法律问题，你还需要考虑伦理和性能 —— 这两者并不像你想象的那样完全分离。
- en: 'There are many ways that AI can have unintended effects, and responsible AI
    covers a range of issues: bias, discrimination, fairness, inclusiveness, accountability,
    transparency, explainability, reliability and safety, privacy and security. Some
    of these ideas are straightforward—AI systems should perform reliably and safely,
    and be secure and respect privacy. Others are more complex and interconnected,
    so while we’ve broken up the tools into separate topics, you’ll find a lot of
    overlap.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: AI 可能会产生意外影响的方式有很多，负责任的 AI 涵盖了一系列问题：偏见、歧视、公平性、包容性、责任、透明度、可解释性、可靠性与安全性、隐私与安全性。其中一些理念比较直接
    —— AI 系统应该能够可靠且安全地运行，并且保护隐私。其他一些则更为复杂且相互关联，因此尽管我们将工具分解成不同的主题，你会发现它们之间有很多交叉点。
- en: AI systems should treat all people fairly (and not knowing that an AI system
    will make things harder for someone doesn’t stop you from being responsible if
    that happens). That’s related to inclusivity—AI systems should empower everyone—but
    also to transparency and accountability. AI systems should be understandable,
    and people (or organizations) should be accountable for AI systems, where that
    means having to explain and justify decisions and actions, answer questions, and
    face potential consequences.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AI 系统应该公平地对待所有人（即使不知道 AI 系统会让某人生活更加困难，如果发生了这种情况，你也要对此负责）。这与包容性有关 —— AI 系统应该赋能每个人
    —— 但也与透明度和责任相关。AI 系统应该是可理解的，并且人们（或组织）应对 AI 系统负责，这意味着必须解释和证明决策和行动，回答问题，并承担可能的后果。
- en: Prioritizing fairness in an AI system can mean making trade-offs with other
    priorities and examining assumptions, so as well as being transparent about the
    fact that AI is being used and what data you’re collecting from users, it’s important
    to be as transparent as possible about those priorities and assumptions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AI 系统中优先考虑公平性可能意味着在其他优先事项之间做出权衡，并审视假设，因此除了透明地说明正在使用 AI 和从用户那里收集的数据之外，还有必要尽可能透明地表明这些优先事项和假设。
- en: When we talk about “explaining” AI systems, that can mean different things to
    different people. Machine learning experts may want to see the weights in the
    model that resulted in a specific prediction, as well as details of the algorithm
    used and dataset it was trained on, the performance scores for the model, and
    other technical information. That won’t make things any clearer for the bank customer,
    who wants an explanation of why their loan was denied and what they can do about
    it, or for the business user who needs to understand why the system is predicting
    that a particular order won’t be ready in time and how reliable the prediction
    is likely to be.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论“解释”AI系统时，这对不同的人可能意味着不同的事情。机器学习专家可能希望看到导致特定预测结果的模型权重，以及所使用的算法和训练集的详细信息，模型的性能分数以及其他技术信息。但对于银行客户来说，这些都不会解释为什么他们的贷款被拒绝以及他们可以做些什么，对于业务用户来说，这些信息也不能解释为什么系统预测某个订单不能及时准备好，以及预测的可靠性如何。
- en: Responsible AI Improves Performance and Outcomes
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负责任的AI提升了性能和结果
- en: Don’t think about responsible AI as an ethical approach you care about only
    in the abstract. Putting these principles into practice is how you build and deploy
    a better product and achieve accurate, effective, reliable machine learning that
    gives you the answers you need. The benefits go far beyond compliance, and in
    the next chapter we’ll go into more detail about how delivering responsible AI
    is a key part of achieving machine learning best practices.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将负责任的AI仅仅视为你只在抽象层面关心的伦理方法。将这些原则付诸实践是你如何构建和部署更好产品、实现精确、有效、可靠的机器学习，为你提供所需的答案。这些好处远不止遵循合规性，接下来的章节我们将更详细地讨论如何提供负责任的AI是实现机器学习最佳实践的关键部分。
- en: Take transparency. It may be a legal requirement for you to be able to explain
    how decisions are made, including any that are automated or based on recommendations
    from machine learning systems. From a business perspective, you want customers
    to be able to trust your organization, so you need to provide transparency and
    you want to be clear about how decisions are being made and what impact they’re
    having on people. Data scientists need to be able to explain models to the business
    team that will be using those predictions, recommendations, or other machine learning
    features so they’re comfortable relying on them—but models that have more interpretability
    are also easier to debug and improve.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 关于透明度。你可能需要能够解释决策的方法，包括任何自动化或基于机器学习系统推荐的决策，这可能是法律要求的。从商业角度来看，你希望客户能够信任你的组织，因此你需要提供透明度，并清楚地说明决策的制定过程以及其对人们的影响。数据科学家需要能够向业务团队解释模型，这些模型将用于预测、推荐或其他机器学习特性，以便他们能够放心地依赖于它们——但是具有更高解释性的模型也更容易调试和改进。
- en: Experiment and Iterate
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验和迭代
- en: 'The more visibility you have into the machine learning you’re using, the better
    results you are likely to get from it, because you understand the inputs, the
    context, and any limitations of the model. Machine learning is a process of experimentation,
    where you start with the hypothesis of how you can use data to guide a decision
    and look for a machine learning model that best delivers that: interpretability
    will help you validate that the model matches your objectives as well as being
    fair.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你对使用的机器学习有越多的可见性，你可能会得到更好的结果，因为你了解输入数据、背景和模型的任何限制。机器学习是一个实验的过程，你从如何使用数据指导决策的假设开始，并寻找最能实现这一目标的机器学习模型：解释性将帮助你验证模型是否符合你的目标以及是否公平。
- en: Similarly, achieving reliability means understanding the blind spots in your
    model so you know where it’s failing and why. Thinking about the camera angles
    for spatial analysis as you consider privacy and sensitive locations will also
    help you get better accuracy. If errors in speech or image recognition are more
    common or more significant for specific groups of people, you may have a fairness
    issue—but that also suggests that your model needs more work generally.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，实现可靠性意味着了解模型中的盲点，这样你就知道它在哪些地方出现问题以及原因。在考虑隐私和敏感位置时，思考空间分析的摄像机角度也将有助于提高准确性。如果语音或图像识别中的错误对特定群体的人更常见或更显著，你可能会面临公平性问题——但这也表明你的模型需要更多的工作。
- en: Knowing where your data comes from and how it was collected makes it easier
    to know what further data you might need to improve results.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 知道数据的来源和收集方式可以更容易知道你可能需要哪些进一步的数据来改善结果。
- en: As you work to reduce errors, you need to continue to monitor error rates and
    where they occur to make sure you’re not addressing one problem and introducing
    another—a debugging principle developers will be very familiar with.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在努力减少错误时，你需要继续监控错误率及其发生位置，以确保你不是解决一个问题而引入另一个问题——这是开发人员非常熟悉的调试原则。
- en: 'Thinking about how to build an effective system involves looking at the humans
    involved in the system as well as the technical implementation details: that applies
    to AI services just as much as it does to the interface and user experience.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 思考如何构建有效的系统涉及到查看系统中涉及的人员以及技术实现细节：这对AI服务同样重要，正如它对界面和用户体验一样。
- en: You need to secure your training data and your machine learning models so a
    competitor or malicious user can’t subvert them or find ways of gaming the system
    (see [Chapter 8](ch08.xhtml#best_practices_for_machine_learning_pro) for more
    details on this). The specifics may be a little different, but again, starting
    with the same security and privacy concerns that apply to any database will help
    you deliver a more robust AI system.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要保护你的训练数据和机器学习模型，以防竞争对手或恶意用户篡改它们或找到操纵系统的方法（详见[第8章](ch08.xhtml#best_practices_for_machine_learning_pro)了解更多细节）。具体细节可能会有所不同，但同样，从适用于任何数据库的安全性和隐私问题出发，将有助于你交付更强大的AI系统。
- en: But there are also issues you can see with AI that won’t be familiar from other
    areas of development—and that’s where responsible AI tools can help.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但也有一些AI领域的问题在其他开发领域可能不太熟悉，这就是负责任AI工具可以帮助的地方。
- en: Tools for Delivering Responsible AI
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交付负责任AI的工具
- en: It’s important to be transparent about how you use data and apply machine learning,
    but you need to move beyond just transparency to make sure you operationalize
    the principles of responsible AI in your machine learning lifecycle. Different
    tools can help you to identify, diagnose, and mitigate errors, harms, and other
    failures of responsible AI, and help you explore data and make responsible decisions
    with it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用数据和应用机器学习的透明性方面非常重要，但你需要超越仅仅透明，确保在机器学习生命周期中实施负责任AI原则。不同的工具可以帮助你识别、诊断和减轻负责任AI的错误、伤害和其他失败，帮助你探索数据并在其中做出负责任的决策。
- en: Warning
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When you use cloud AI services like Cognitive Services and Azure Applied AI
    Services that have pretrained models or simple customization steps, many of the
    more operational tools won’t be applicable to your workflow (though they all will
    have been used as part of developing the models). But the planning tools will
    be relevant because you still have to think about responsible AI, whether that’s
    being transparent about the fact you’re using AI services to make decisions or
    offer predictions, looking at the accuracy and performance results for any custom
    models you train, or doing thorough testing with your own data to make sure that
    the tools work well for your entire audience and don’t exclude particular groups.
    If your users are likely to be taking photos outdoors, at night, or in the rain,
    use those kinds of images to train and test object and face recognition.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用云AI服务，如认知服务和Azure应用AI服务，这些服务具有预训练模型或简单的定制步骤时，许多更具操作性的工具将不适用于你的工作流程（尽管它们都已作为开发模型的一部分被使用过）。但规划工具将是相关的，因为你仍然需要考虑负责任的AI，无论是透明地告知你正在使用AI服务做出决策或提供预测，查看你训练的任何自定义模型的准确性和性能结果，还是使用自己的数据进行彻底测试，以确保这些工具能够很好地为你的整个受众群体工作，而不会排除特定群体。如果你的用户可能在户外、夜间或雨天拍照，则使用这些类型的图像来训练和测试对象和人脸识别。
- en: 'It’s up to Microsoft to build the service responsibly, but it’s also up to
    you to use it responsibly. You can’t simply rely on the service always getting
    it right: you need to include human oversight and review of predictions, classifications,
    and other results—refer to [“Human in the Loop Oversight”](#human_in_the_loop_oversight)
    for more details. And make sure to consult the transparency notes in the next
    section so you have a clear understanding of what the service can do well and
    any situations where it may not produce good results, and how to use it responsibly
    in light of that.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任地构建服务是微软的责任，但也是你的责任。你不能简单地依赖服务总是正确的：你需要包括人类监督和对预测、分类和其他结果的审查——请参阅[“人在环路监督”](#human_in_the_loop_oversight)了解更多细节。并确保查阅下一节的透明度说明，以便清楚地了解服务的优点以及可能产生不良结果的情况，以及如何在这种情况下负责任地使用它。
- en: Some tools that help you deliver responsible AI are relevant when you’re planning
    what you’re going to build and as you evaluate ongoing projects. Others will be
    part of your MLOps process (turn back to [Chapter 3](ch03.xhtml#traincomma_tunecomma_and_deploy_models)
    if you need a refresher on what that looks like in practice), so that you can
    train and assess models quickly and responsibly. Experimentation is a key part
    of successful machine learning, and if you’re training your own models you’ll
    want to train multiple models and compare them on transparency and fairness as
    well as on error rates and accuracy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在规划构建内容和评估进行中的项目时，一些有助于提供负责任AI的工具是相关的。其他工具将成为您的MLOps流程的一部分（如果您需要关于实际操作的刷新，请返回[第3章](ch03.xhtml#traincomma_tunecomma_and_deploy_models)），以便您可以快速而负责任地训练和评估模型。实验是成功机器学习的关键部分，如果您正在训练自己的模型，您将希望训练多个模型，并在透明度和公平性以及错误率和准确性上进行比较。
- en: There are many different tools available; in this chapter we’re concentrating
    on the various responsible AI tools available from Microsoft, which cover transparency,
    fairness, inclusiveness, reliability and safety, privacy, security, and accountability.
    Many of them are open source, but they’re also integrated into services like Azure
    Machine Learning. You can use Microsoft’s [Responsible AI Toolbox](https://aka.ms/rai-toolbox)
    to create a workflow for debugging machine learning models using the tools shown
    in [Figure 7-1](#use_the_tools_in_the_responsible_ai_too) for fairness, error
    analysis, and interpretability, or to do causal analysis to help you make data-driven
    decisions based on machine learning predictions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的工具可用；在本章中，我们专注于来自微软的各种负责任AI工具，涵盖透明度、公平性、包容性、可靠性和安全性、隐私、安全性以及问责制。其中许多是开源的，但它们也集成到像Azure
    Machine Learning这样的服务中。您可以使用微软的[负责任AI工具箱](https://aka.ms/rai-toolbox)根据图[7-1](#use_the_tools_in_the_responsible_ai_too)中显示的工具，为调试机器学习模型创建工作流程，用于公平性、错误分析和可解释性，或进行因果分析，帮助您基于机器学习预测做出数据驱动的决策。
- en: '![Use the tools in the Responsible AI Toolbox to create your own machine learning
    debugging workflow](Images/aasc_0701.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![使用负责任AI工具箱中的工具创建您自己的机器学习调试工作流程](Images/aasc_0701.png)'
- en: Figure 7-1\. Use the tools in the Responsible AI Toolbox to create your own
    machine learning debugging workflow
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 使用负责任AI工具箱中的工具创建您自己的机器学习调试工作流程
- en: All these tools are available separately, but the toolkit takes care of all
    the dependencies and wraps them in a helpful dashboard.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些工具都可以单独使用，但工具包会处理所有依赖关系，并将它们包装在一个有用的仪表板中。
- en: Tools for Transparency
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 透明度工具
- en: Transparency isn’t just about telling users that you’re collecting data and
    using AI, or even explaining what predictions are based on, although that’s important
    (you’ll see that called explainability or interpretability). It also covers making
    sure developers and data scientists are clear about where data comes from so they
    can use it responsibly; the same applies to any prebuilt models or AI services
    you use.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度不仅仅是告诉用户你正在收集数据和使用AI，或者解释预测基于什么，尽管这很重要（你会看到这被称为可解释性或解释性）。它还涵盖确保开发人员和数据科学家清楚数据来源，以便他们能够负责任地使用它；对于使用的任何预构建模型或AI服务也适用这一点。
- en: Whether you’re training your own model or customizing a pretrained one, you
    need to know where your training data comes from and what’s in it. We’ll look
    at how to create a responsible data culture in your organization in more detail
    in [Chapter 8](ch08.xhtml#best_practices_for_machine_learning_pro), but tracking
    the lifecycle of a training dataset so you know who built it and why is critical
    to understanding what the demographic skews might be in that set. Every dataset
    should be accompanied by a datasheet that documents why it was collected and how
    (including any processing done on it), exactly what data is contained in it (raw
    data or features, any confidential, sensitive, privileged, or potentially offensive
    data) and what’s missing, its recommended uses (including any known restrictions),
    what it’s already been used for, and so on. The [Microsoft Datasheets for Datasets
    template](https://go.microsoft.com/fwlink/?linkid=2190166) includes a set of questions
    to help you gather the information for the datasheet.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是训练自己的模型还是定制预训练模型，您都需要知道训练数据的来源和其中包含的内容。我们将在[第8章](ch08.xhtml#best_practices_for_machine_learning_pro)更详细地讨论如何在您的组织中创建负责任的数据文化，但追踪训练数据集的生命周期以了解谁构建了它以及原因对于理解该集合中可能存在的人口偏差至关重要。每个数据集应附有数据表，记录收集原因及方式（包括对其进行的任何处理），确切包含的数据（原始数据或特征，任何机密、敏感、特权或潜在冒犯性数据）以及缺失的内容，建议的用途（包括任何已知限制），它已经被用于什么等等。[Microsoft数据集数据表模板](https://go.microsoft.com/fwlink/?linkid=2190166)包含一系列问题，帮助您收集数据表信息。
- en: If possible, consider the information you will need to include in the datasheet
    before collecting data and document the motivation, composition, collection, preprocessing,
    distribution, maintenance, and uses of the dataset as you go along.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能，在收集数据之前考虑您需要包含在数据表中的信息，并随着过程记录数据的动机、组成、收集、预处理、分发、维护和使用。
- en: Model cards and transparency notes
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型卡片和透明度说明文档
- en: Even when you’re using a prebuilt model or an AI service, knowing how it was
    built and how it works may help you use it responsibly—and effectively.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您使用预构建模型或AI服务，了解它是如何构建和运作的，可能有助于您负责任和有效地使用它。
- en: Because AI is probabilistic and statistical, it’s unlikely to ever be completely
    accurate; understanding the limitations and where it is more likely to fail will
    help you design an overall system that makes better use of AI models because it
    takes that into account.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因为AI是概率和统计性的，不太可能完全准确；理解其局限性和可能失败的地方将有助于您设计一个整体系统，更好地利用AI模型，因为这考虑到了这一点。
- en: Model cards and transparency notes document AI models and services to help you
    understand their capabilities and limitations, as well as what choices you can
    make that influence the behavior of the system to achieve the best performance
    from it, like setting confidence score thresholds to minimize either false negatives
    or false positives, or how to preprocess data to get the best outcomes. That kind
    of information helps you make trade-offs and understand whether the service will
    work well with the kind of data you have available.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模型卡片和透明度说明文档记录AI模型和服务，帮助您了解它们的能力和限制，以及您可以做出哪些影响系统行为的选择，以达到最佳性能，例如设置置信度阈值以最小化假阴性或假阳性，或如何预处理数据以获得最佳结果。这种信息帮助您进行权衡，了解该服务是否能很好地处理您可用的数据类型。
- en: Microsoft is committed to responsible AI, and part of that is providing transparency
    notes and integration guidance for an increasing number of Azure AI services (see
    [Responsible Use of AI with Cognitive Services](https://go.microsoft.com/fwlink/?linkid=2190168)).
    These tell you what kind of data will give you the best results from each service
    and what it will not do as well (like explaining the limitations of sentiment
    analysis in the Text Analytics service or warning that the OpenAI Service isn’t
    suitable for open-ended scenarios where your users can generate content on any
    topic because that might produce offensive text or other undesirable but unintended
    responses). The documentation will tell you how to use the various Azure AI services,
    but the transparency notes will be especially useful for understanding what they’re
    most suitable for.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft致力于负责任的人工智能，其中一部分内容是为越来越多的Azure AI服务提供透明度说明和集成指南（参见[使用认知服务的负责任AI](https://go.microsoft.com/fwlink/?linkid=2190168)）。这些说明告诉您每种服务中哪些数据能够为您带来最佳结果，以及哪些方面可能不足（比如在文本分析服务中解释情感分析的局限性，或者警告OpenAI服务不适合用户可以在任何主题上生成内容的开放式场景，因为这可能会产生冒犯性文本或其他不受欢迎但非故意的响应）。文档将告诉您如何使用各种Azure
    AI服务，但透明度说明将特别有助于理解它们最适合什么样的用途。
- en: For each of the Cognitive Services and Applied AI Services, Microsoft looks
    at the different fairness issues that could apply (usually it’s quality of service)
    and tests the models used for them. So for speech-to-text, they have a test set
    covering age, gender, language and regional accents, plus other social factors
    that could cause accuracy to vary, corresponding to the real-world distribution
    of those in the population. As new versions of models are trained, they’re tested
    to make sure that, in a statistically significant way and to an acceptable margin
    of error, they work equally well for, say, men and women. Over time, transparency
    notes for the different services will include the details of which factors and
    groups are tested for, any areas where the models are known to perform less well,
    and what they haven’t yet been tested for, so you can compare that to your own
    population of users.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个认知服务和应用AI服务，Microsoft会审视可能适用的不同公平性问题（通常是服务质量），并对其使用的模型进行测试。因此，对于语音转文字来说，它们有一个测试集，涵盖年龄、性别、语言和地区口音，以及可能导致准确性变化的其他社会因素，这些因素与真实世界中这些因素在人口中的分布相对应。随着模型的新版本训练，它们会进行测试，确保在统计上显著且在可接受误差范围内，它们对男性和女性同样有效。随着时间的推移，不同服务的透明度说明将包括测试哪些因素和群体的详细信息，模型已知表现较差的任何领域，以及尚未测试的内容，以便您可以将其与自己的用户群进行比较。
- en: If the context in which you will be using a service is very different from the
    way Microsoft will have tested it (for example, speech recognition is usually
    tested in a quiet environment, and you will be using it in a noisy location where
    several people might be talking in the background) or you know you will be customizing
    the model, you will want to perform fairness assessments on your training data
    and model performance (we look at the tools for that later in this chapter) and
    keep records of those. You may also prefer to use AI in a more narrow and targeted
    way, limiting it to situations where you know it will perform well, experimenting
    with ways to expand on that once you’re getting good results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将使用服务的背景与Microsoft测试时的环境非常不同（例如，语音识别通常在安静的环境中进行测试，而您将在嘈杂的环境中使用它，背景中可能有多人交谈），或者您知道将要定制模型，您将希望对培训数据和模型性能进行公平性评估（我们将在本章后面看到相关工具），并保留这些记录。您可能还希望将AI应用于更狭窄和有针对性的方式，限制其仅适用于您知道它会表现良好的情况，并尝试扩展这一点，一旦获得良好结果。
- en: Some transparency notes also include deployment and integration guidelines with
    helpful information about how to use the service responsibly. That’s particularly
    useful for emerging areas like using computer vision for [spatial analysis](https://go.microsoft.com/fwlink/?linkid=2190285)—where
    you may need to balance privacy concerns with the health and safety benefits of
    monitoring an office or retail location to help maintain social distancing, making
    it even more important to make people aware what data is being collected and explain
    why. The details from these notes covering how your data will be processed, used,
    and stored by the Azure AI services may be useful to include in your own compliance
    records. You can create transparency notes and integration recommendations for
    your own models as an extension of creating datasheets for your datasets.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一些透明度说明还包括部署和集成指南，提供了关于如何负责地使用服务的有用信息。这对于新兴领域尤其有用，比如使用计算机视觉进行[空间分析](https://go.microsoft.com/fwlink/?linkid=2190285)，在这些领域中，您可能需要在平衡隐私关切与监控办公室或零售场所以帮助维持社交距离的健康与安全效益之间进行权衡，因此让人们意识到正在收集的数据及其原因变得更加重要。这些说明中的细节涵盖了
    Azure AI 服务如何处理、使用和存储您的数据，可能对包含在您自己的合规记录中会有所帮助。您可以创建透明度说明和集成建议，作为创建数据集数据表的扩展，用于您自己的模型。
- en: Checklists and planning processes for AI projects
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AI 项目的检查表和计划流程
- en: When it comes to applying the ideas of responsible AI, it helps to have a formal
    process that covers areas you need to consider and decisions or trade-offs you
    need to make. A checklist of questions that you have to answer before processing
    with a project involving AI can help put that into practice. You might want to
    try and classify models as low, medium, or high risk. For some systems, you might
    want to conduct a [formal algorithmic impact assessment](https://go.microsoft.com/fwlink/?linkid=2190167)
    that describes what it’s designed to do and says who will be responsible for fixing
    any problems. When you create and work through a checklist, you’re working at
    a higher level than dataset datasheets and transparency notes, but you’re exploring
    the same kinds of decisions about how to design, implement, and deploy AI systems.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用负责 AI 的理念时，有一个涵盖您需要考虑的领域以及您需要做出的决策或权衡的正式流程是有帮助的。在涉及 AI 的项目中进行处理前必须回答的问题检查表可以帮助将其付诸实践。您可能希望尝试将模型分类为低、中或高风险。对于某些系统，您可能希望进行[正式算法影响评估](https://go.microsoft.com/fwlink/?linkid=2190167)，该评估描述了其设计目标并指明谁将负责修复任何问题。当您创建并完成一个检查表时，您正在比数据集数据表和透明度说明更高层次地工作，但您正在探讨如何设计、实施和部署
    AI 系统的相同类型的决策。
- en: 'Microsoft offers an [AI Fairness Checklist](https://go.microsoft.com/fwlink/?linkid=2190169)
    that is itself a process to work through to help you create a more specific checklist
    for your own situation, and despite the name, it covers a range of responsible
    AI topics like reliability. Use it to ask questions like:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft 提供了一个[AI 公平性检查表](https://go.microsoft.com/fwlink/?linkid=2190169)，它本身就是一个流程，帮助您制定更具体的检查表，以适应您自己的情况，尽管其名称如此，但它涵盖了一系列关于可靠
    AI 主题如可靠性的负责 AI 主题。使用它来提出诸如：
- en: Who will be affected by the deployment of this system? Are there people who
    could be negatively affected because of this system or application being used?
    How could we mitigate the impact?
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁将受到这个系统部署的影响？是否有人因为使用该系统或应用而受到负面影响？我们如何减轻这种影响？
- en: What are some of the potential limitations, issues, or risks that could arise
    from this system?
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个系统可能存在哪些潜在的限制、问题或风险？
- en: Will the product or feature we’re planning to use perform well in our scenario?
    Test the AI feature on real-world data and check the accuracy before deploying.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们计划使用的产品或功能在我们的场景中表现如何？在部署之前，使用真实世界数据测试 AI 功能并检查其准确性。
- en: 'How will we identify and respond to error? AI services and features will rarely
    be 100% accurate all the time in practical use: how do we prepare for and deal
    with this?'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将如何识别和响应错误？在实际使用中，AI 服务和功能很少能够百分之百准确：我们如何做好准备并处理这些问题？
- en: How will we measure performance and success? Specific outcomes are often better
    measures than usage and adoption.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将如何衡量表现和成功？具体结果通常比使用和采纳率更好地衡量标准。
- en: Responsible AI relies on people making responsible decisions about what systems
    to build and thinking about the impact on those who will use or be affected by
    those systems.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 负责 AI 依赖于人们对构建系统做出负责任决策，并考虑到将使用或受到这些系统影响的人们。
- en: The [Human-AI eXperience (HAX) Toolkit](https://go.microsoft.com/fwlink/?linkid=2190286)
    has a set of guidelines for making those responsible decisions when you build
    AI systems that interact with people.  For example, most people will assume that
    a machine learning system is always learning and will pick up on their corrections
    and not make the same mistake twice; but that won’t happen unless you’re collecting
    the data and retraining the model. Make it clear whether that’s happening and
    when they might see improvements.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[Human-AI体验（HAX）工具包](https://go.microsoft.com/fwlink/?linkid=2190286) 提供了一套指南，用于在构建与人交互的AI系统时做出负责任的决策。例如，大多数人会认为机器学习系统总是在学习，并会接受他们的更正而不会再犯同样的错误；但这只有在你收集数据并重新训练模型时才会发生。明确是否发生了这种情况以及他们何时可能看到改进是很重要的。'
- en: Use the HAX workbook to help your whole team (designers, managers, and developers)
    think about what the system will do and how users will interact with it before
    you start building. Then work through the HAX playbook to explore and plan for
    what can go wrong.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用HAX工作簿帮助整个团队（设计师、经理和开发人员）在开始构建之前考虑系统将要做什么以及用户如何与之交互。然后通过HAX playbook来探索和规划可能出现的问题。
- en: Tip
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Bring all of these ideas together with a structured brainstorming session. [Judgement
    Call](https://go.microsoft.com/fwlink/?linkid=2190170) is a card game where your
    team can role-play who you think will be affected by your technology, how they
    will use it, what can go wrong, and what you can do about it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结构化的头脑风暴会议将所有这些想法整合在一起。[判断性判断](https://go.microsoft.com/fwlink/?linkid=2190170)
    是一个卡牌游戏，你的团队可以扮演谁会受到你的技术影响、他们将如何使用它、可能出现什么问题以及你可以采取什么措施的角色。
- en: Interpretability
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释性
- en: Once you’ve built your machine learning model, you need to assess it. Performance
    matters, but it’s just as important to understand as much as possible about how
    it works and what features in the model affect predictions and classifications.
    In regulated industries, interpretable models and explanations may be a legal
    requirement; elsewhere, stakeholders, users, and customers may have more trust
    in machine learning results if they can see what contributes to them—but the information
    may need to be presented quite differently for each audience.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建了你的机器学习模型，你需要对其进行评估。性能很重要，但了解模型如何工作以及模型中哪些特征影响预测和分类同样重要。在受监管的行业中，可解释的模型和解释可能是法律要求；在其他地方，如果利益相关者、用户和客户能看到贡献结果的因素，他们对机器学习结果可能会更加信任，但是信息可能需要针对每个受众以不同的方式呈现。
- en: Interpretability tools can help you understand which features in the data most
    affect the results. If you’re predicting the right selling price for a house,
    the age of the property and ratings of local schools will usually be the most
    significant features in the dataset, but the size of the house and the plot it’s
    built on, the number of bedrooms and bathrooms, whether it has a garage or a porch,
    and other factors like having a garden will also contribute.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性工具可以帮助你理解数据中哪些特征最影响结果。如果你在预测房屋的正确销售价格，物业的年龄和当地学校的评级通常是数据集中最重要的特征，但房屋的大小和建筑用地、卧室和浴室的数量、是否有车库或门廊，以及其他因素如有无花园也会起作用。
- en: The Responsible AI Toolbox has a model interpretability step powered by the
    open source [InterpretML](https://go.microsoft.com/fwlink/?linkid=2190287) Python
    package, which can help you understand the behavior of your model generally, or
    the reasons for specific predictions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任AI工具箱具有一个由开源[InterpretML](https://go.microsoft.com/fwlink/?linkid=2190287)
    Python包支持的模型可解释性步骤，它可以帮助你理解你的模型的行为一般性或特定预测的原因。
- en: Tip
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The Azure Machine Learning SDK includes model interpretability classes for InterpretML
    in azureml.interpret that can explain model predictions by showing the distribution
    of predictions and how much influence specific features have on results for the
    whole model or for individual predictions. It has a visualization dashboard that
    lets you explore explanations in a Jupyter Notebook, or you can see a simplified
    version of the dashboard in Azure Machine Learning studio. You can try out the
    Azure Machine Learning Interpret APIs in these [sample notebooks](https://go.microsoft.com/fwlink/?linkid=2190171).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning SDK包括InterpretML在azureml.interpret中的模型解释类，可以通过展示预测分布和特定特征对整体模型或个别预测结果的影响来解释模型预测。它有一个可视化仪表板，让您在Jupyter
    Notebook中探索解释，或者您可以在Azure Machine Learning工作室中看到仪表板的简化版本。您可以在这些[sample notebooks](https://go.microsoft.com/fwlink/?linkid=2190171)中尝试Azure
    Machine Learning Interpret API。
- en: As well as understanding why they didn’t get a loan or why the house pricing
    tool suggested a particular price, a customer may also want to know what they
    could do to get a different result—using those machine learning predictions in
    the real world. If you know which features in the model were important for the
    prediction, you can use counterfactuals—things that aren’t true but could be—to
    explore how the predictions would change and give them the option of exploring
    those suggestions, again, making it clear what they’re based on.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了了解为什么他们没有获得贷款或者为什么房屋定价工具建议特定的价格，客户可能还想知道他们可以做什么来获得不同的结果——在现实世界中应用那些机器学习预测。如果你知道模型中哪些特征对预测很重要，你可以使用反事实（counterfactuals）——那些不是真实的但可能的事情——来探索预测如何变化，并为他们提供探索这些建议的选择，再次明确它们基于什么。
- en: Warning
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Be careful how you present explanations of an AI system; you need to be clear
    that you’re explaining what the model learned so that people can decide whether
    to trust a prediction, not describing the way the real world works. Using frequencies
    rather than specific numbers can also help to make it clear that predictions are
    probabilities rather than certainties; think about the cone of probability on
    a hurricane map.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意如何呈现AI系统解释；您需要清楚地表明，您正在解释模型学到的东西，以便人们可以决定是否信任预测，而不是描述真实世界的运作方式。使用频率而不是具体数字也有助于表明预测是概率而非确定性；考虑飓风地图上的概率锥体。
- en: The counterfactual example analysis step in the  Responsible AI Toolbox, powered
    by [InterpretML DiCE](https://github.com/interpretml/DiCE), lets a business user
    experiment with the data in a loan application that was denied to give a customer
    some suggestions (including saying what features can and can’t be changed). It’s
    pointless to say “be five years younger,” and “earn an extra $10,000 a year” might
    not be immediately helpful, but being able to tell them they’ll be approved if
    they close one of their credit cards or repay this much on an existing loan is
    the kind of transparency that stops AI being a black box.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在由[InterpretML DiCE](https://github.com/interpretml/DiCE)提供支持的负责任AI工具箱中的反事实示例分析步骤，允许业务用户在被拒贷款申请中实验数据，以给客户提供一些建议（包括说明哪些特征可以和不能改变）。说“变年轻五岁”是毫无意义的，“每年多赚一万美元”可能不会立即有帮助，但能告诉他们如果关闭一个信用卡或偿还一笔现有贷款就会批准他们的这类透明度，是避免AI成为黑匣子的途径。
- en: 'But the same tool will also help you find both fairness concerns and problematic
    features. Models are only an approximation of the real world: they may not be
    accurate, and there may be correlations between features in the model that don’t
    necessarily reflect causation in the real world. If you’re looking at a model
    that predicts someone’s income and increasing their capital loss from zero to
    thousands of dollars results in a higher predicted income, either there’s an error
    in the model or it’s capturing a correlation like tax strategies.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但是同样的工具也将帮助您找到公平性关切和问题特征。模型只是对真实世界的一种近似：它们可能不准确，并且模型中的特征之间可能存在相关性，这并不一定反映真实世界中的因果关系。如果你在看一个预测某人收入的模型，而将他们的资本损失从零增加到数千美元会导致更高的预测收入，要么模型有误，要么它捕捉到了像税务策略这样的相关性。
- en: The other approach to this is asking “what if” questions to make a data-driven
    decision, using a causal analysis step powered by the [EconML](https://github.com/microsoft/EconML)
    Python package. For our house pricing scenario, we could look at a specific house
    in the dataset and see how having a bigger garage or more fireplaces would change
    the suggested price, or get a table of houses sorted by which will see the biggest
    increase in price with those changes so you can focus resources on what will have
    the biggest impact.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是提出“如果”问题以进行基于数据的决策，利用由 [EconML](https://github.com/microsoft/EconML) Python
    包支持的因果分析步骤。对于我们的房屋定价场景，我们可以查看数据集中的特定房屋，看看扩大车库或增加壁炉如何改变建议的价格，或者获取一张根据这些变化将看到最大价格增幅的房屋表，以便您可以集中资源投入对影响最大的事物。
- en: You can also use this to create policies. Instead of telling everyone who wants
    to sell a house that adding a porch will increase the value, you can use “what
    if” questions to establish that an older house on a small lot will sell for a
    higher price if you remove an existing porch but a newer house with the main floor
    over a certain number of square feet will sell for more money if you add a porch.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用此功能创建策略。与其告诉所有想要出售房屋的人添加门廊会增加价值，不如使用“如果”问题来确定，如果您去除现有的门廊，老房子在小地块上的销售价格会更高，但是如果您在某个平方英尺数以上的新房子上添加门廊，销售价格会更高。
- en: If you want to do more advanced causal inferencing, the [DoWhy Python library](https://github.com/Microsoft/dowhy)
    is also part of the Responsible AI Toolbox; it can help you model the assumptions
    you’re making (specifying what you do and don’t know) and then test them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想进行更高级的因果推理，[DoWhy Python 库](https://github.com/Microsoft/dowhy) 也是负责任 AI
    工具箱的一部分；它可以帮助您建模您所做的假设（指定您知道和不知道的内容），然后测试它们。
- en: If you aren’t familiar with causal inference, [ShowWhy](https://github.com/microsoft/showwhy)
    will help you ask questions, expose assumptions, and explore causes and confounders—variables
    that contribute to both what you think is the cause and the final outcome—with
    diagrams that show the relationships in your data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对因果推断不熟悉，[ShowWhy](https://github.com/microsoft/showwhy) 将帮助您提出问题，揭示假设，并探索造成和混杂变量——这些变量既有助于您认为是原因的因素，又影响最终结果——并通过显示数据中的关系的图表来展示这些关系。
- en: Tools for AI Fairness
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI 公平工具
- en: AI systems can behave unfairly because of bias in society that’s reflected in
    the training data, because of decisions made during development and deployment,
    or because there are flaws in the data (like not being representative of all your
    users) or in the system itself.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: AI 系统可能因为社会中的偏见反映在训练数据中、开发和部署过程中做出的决策，或者数据本身存在的缺陷（比如不代表所有用户）或系统本身存在的缺陷而表现出不公平行为。
- en: AI can automate bias that’s sporadically present in the training set and amplify
    that by applying it routinely rather than selectively. It can also introduce bias,
    bringing a biased linkage like “men are doctors, women are nurses” when translating
    from a language that doesn’t have gendered pronouns, turning “this person is a
    doctor, this person is a nurse” into “he is a doctor, she is a nurse.”
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: AI 可以自动化在训练集中偶尔出现的偏见，并通过常规应用而非选择性应用来放大该偏见。它还可以引入偏见，例如在翻译没有性别代词的语言时带有偏见的链接，将“这个人是医生，这个人是护士”翻译成“他是医生，她是护士”。
- en: You also need to think about mismatches between your data and the real world.
    If you assume that people who spend longer looking at a web page are more interested
    in that topic, you’re not taking into account people who might be on a slow internet
    connection, or whether the page design makes it harder for some people to read.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要考虑数据与现实世界之间的不匹配。如果您假设长时间浏览网页的人对该主题更感兴趣，那么您未考虑到可能使用慢互联网连接的人，或者页面设计是否使某些人难以阅读。
- en: AI fairness means assessing the possible negative outcomes a model could have
    on individuals or groups, and mitigating those. There are a lot of different negative
    impacts (often called “harms”) an AI system can have, but there are three main
    types of fairness to think about when you look at how an AI system behaves—although
    a fairness problem might cause more than one kind of harm.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: AI 公平意味着评估模型可能对个体或群体造成的负面影响，并加以缓解。AI 系统可能产生许多不同的负面影响（通常称为“伤害”），但在查看 AI 系统行为方式时，有三种主要公平类型需要考虑——尽管公平问题可能会导致超过一种伤害。
- en: Quality of service fairness is about the accuracy of the system. If you train
    your system on data that covers one specific scenario but use it in a broader,
    more complex situation, it’s likely to perform poorly. Facial recognition is often
    less accurate for people with dark skin, and especially so for women with dark
    skin; that’s a quality of service failure because it’s less accurate for one group
    of people. That’s because the datasets facial recognition models are trained on
    don’t have enough examples for the model to learn the right patterns.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 服务质量公平性与系统的准确性有关。如果你在涵盖一个特定场景的数据上训练你的系统，但在更广泛、更复杂的情境中使用它，它可能表现得很差。面部识别对于肤色较深的人往往准确率较低，特别是对于肤色较深的女性；这是一个服务质量失败，因为它对某些群体的准确性较低。这是因为面部识别模型所训练的数据集中没有足够的例子让模型学习到正确的模式。
- en: 'If you have insufficient coverage in your data, you may be able to mitigate
    the problem by collecting more data: if not, you can look into options like synthetic
    data, or you can choose not to use the model in those cases. So if you use click
    data to train a recommendation system, there will be some groups of people who
    have never been shown a recommendation, so you can’t get click data to train the
    system for them: in those cases, you can default to choosing randomly between
    some preset links.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据覆盖不足，你可以通过收集更多数据来减轻问题：如果不能，你可以考虑使用合成数据之类的选项，或者你可以选择在这些情况下不使用该模型。因此，如果你使用点击数据来训练推荐系统，会有一些群体的人从未看到过推荐，所以你无法获得他们的点击数据来训练系统：在这些情况下，你可以默认在一些预设链接之间随机选择。
- en: Allocation fairness is about whether your AI system is offering everyone the
    same opportunities or resources, or if one group of people is more likely to be
    offered a loan or have their resume accepted by a job application system. If you
    have a quality of service problem with your model and you apply it to a group
    of people that the model is less accurate for, you could end up creating an allocation
    harm. Speech-to-text models that have been trained on speech in quiet settings
    will be less accurate in a noisy environment, but you may also find that background
    noise affects the performance of the model more for one group than another.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 分配公平性涉及你的AI系统是否为每个人提供同样的机会或资源，或者一个群体的人是否更有可能获得贷款或者其简历被职位申请系统接受。如果你的模型有服务质量问题，并且将其应用于模型准确性较低的群体，你可能会造成分配上的伤害。在安静环境中训练的语音转文本模型在嘈杂环境中会更不准确，但你可能也会发现背景噪音对某些群体的模型性能影响更大。
- en: The [Fairlearn tool](https://fairlearn.org) in the Responsible AI Toolbox will
    show you both the accuracy of model performance across different groups, for finding
    quality of service issues, and the decisions it makes so you can look for allocation
    harms.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在责任AI工具箱中的[公平学习工具](https://fairlearn.org)将展示模型在不同群体间性能准确性以及它做出的决策，以便你寻找服务质量问题和分配伤害。
- en: Translating gender-neutral language as “he is a doctor, she is a nurse” is stereotyping,
    but it’s also an example of what’s called representational harm. If a particular
    group is systematically under- or overrepresented, it can suggest that they don’t
    belong in that profession or situation, like an image search for CEOs that returns
    pictures only of men. Image tagging and captioning can produce an image and caption
    that would be acceptable separately, but the juxtaposition can be problematic.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 将性别中性语言翻译为“他是医生，她是护士”是一种刻板印象，但也是所谓的表象性伤害的一个例子。如果特定群体系统性地被过度或不足地代表，这可能暗示他们不适合从事该职业或处境，如一个搜索CEO的图像搜索只返回男性的图片。图像标记和标题可以分别产生可接受的图像和标题，但并列在一起可能会引发问题。
- en: Warning
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Not all failures are equal. If your system produces an image caption that looks
    like it’s reinforcing harmful or historical stereotypes, that can be more of a
    problem for your users than a caption that just doesn’t make sense, even though
    they may both look like errors when you measure the accuracy of your model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 不是所有的失败都是相同的。如果你的系统生成的图像标题看起来强化了有害或历史性的刻板印象，这对你的用户可能比一个毫无意义的标题更有问题，尽管在衡量模型准确性时它们可能都看起来是错误的。
- en: InterpretML can help you understand the features in your model that can lead
    to some of these harms, as can the tools for exploring model accuracy in the next
    section. If you want to dive deeper into understanding the different kinds of
    harms, you’ll find more information at the end of this chapter.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: InterpretML可以帮助你理解模型中可能导致这些伤害的特征，接下来的部分中探索模型准确性的工具也可以。如果你想深入了解不同类型的伤害，你可以在本章的末尾找到更多信息。
- en: Tools for Reliability and Understanding Error
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠性和理解错误的工具
- en: Reliability is about understanding the blind spots of your model. Where and
    why is it failing?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠性是关于理解模型的盲点。它在哪里失败了，为什么失败了？
- en: Consider situations in which your models are less accurate rather than looking
    at a single performance score. If you’re working with images that include pictures
    taken outdoors, is the visual recognition still accurate when it’s dark and raining?
    If you’re building a loan approval model, does it perform similarly for groups
    of applicants of different age, gender, education, or ethnicity? The Error Analysis
    step in the Responsible AI Toolbox can help you understand the distribution of
    errors by discovering which groups in your data (known as cohorts) have high error
    rates, investigating which input features affect those error rates, and exploring
    the model, features, and dataset to debug predictions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑模型准确性较低的情况，而不是仅看单一的性能评分。如果你正在处理包含户外拍摄图片的图像，那么在天黑和下雨时，视觉识别是否仍然准确？如果你正在建立一个贷款批准模型，它是否对不同年龄、性别、教育或种族的申请人群体表现相似？负责任AI工具箱中的错误分析步骤可以帮助你通过发现数据中错误率较高的群体（称为群体），调查哪些输入特征影响这些错误率，并探索模型、特征和数据集来调试预测。
- en: The graphical visualization is particularly helpful here, showing error rates
    for specific features. You can mark the ones that look particularly interesting—or
    problematic—and then explore the statistics for those cohorts. Perhaps there are
    a lot of false negatives for certain groups; that means you’ll want to look at
    the data distribution in the data explorer to see if it’s balanced. If you have
    too little data for those groups it will make the model less accurate, so this
    can point you to what data you need to collect to improve it.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图形可视化在这里特别有帮助，显示特定特征的错误率。你可以标记那些看起来特别有趣或有问题的特征，然后探索这些群体的统计数据。也许对于某些群体有很多误判负例；这意味着你需要查看数据探索器中的数据分布，看看它是否平衡。如果这些群体的数据太少，将会降低模型的准确性，因此这可以指引你收集什么数据来改进它。
- en: You can also look at which are the most important features the model uses to
    make predictions and compare those across your cohorts. Again, that gives you
    a better understanding of what your model has learned. You can also generate explanations—although
    those will make sense to a data scientist rather than an end user, so if you’re
    going to present accuracy levels and other information about your model to the
    people who are interacting with it, you’ll need different tools for generating
    those.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以查看模型用来进行预测的最重要的特征，并将其在不同群体中进行比较。这可以更好地帮助你理解你的模型学到了什么。你还可以生成解释——尽管这些解释对数据科学家而言是有意义的，而不是最终用户，所以如果你打算向与模型交互的人员展示准确性水平和其他信息，你将需要不同的工具来生成这些信息。
- en: Human in the Loop Oversight
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类在环路监督
- en: 'Concepts of fairness are complicated: they can be domain and context sensitive,
    and you also may need to consider long-term impacts as well as immediate harms. 
    No automated attempt to assess fairness is going to give the same nuanced judgment
    as humans assessing the underlying distributions in the data, how well models
    represent the real world, and what impact they might have. The proposed EU regulations
    on development and use of AI require human oversight of “high-risk A.I. systems,”
    and the 2020 Washington State facial recognition law mandates “meaningful human
    review.”'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性的概念是复杂的：它们可能与领域和背景相关，你可能需要考虑长期的影响以及即时的伤害。自动评估公平性的尝试不会像人类评估数据中的基础分布、模型在多大程度上代表真实世界以及可能产生的影响那样给出细腻的判断。提议中的欧盟关于人工智能开发和使用的法规要求对“高风险人工智能系统”进行人工监督，而2020年华盛顿州的面部识别法案则规定进行“有意义的人工审查”。
- en: But simply having a person review and rubber-stamp decisions proposed by machine
    learning models doesn’t avoid all possible harms either; the human may have their
    own biases and assumptions, they may place too much trust in the automated system,^([2](ch07.xhtml#ch01fn7))
    or they may not have enough seniority to make any meaningful changes to the outcome.
    Just seeing the AI prediction might affect their own decision; if you look at
    a transcript before hearing an audio recording, it can be hard to hear the words
    spoken as anything other than the words you’ve read—just as the caption on an
    optical illusion can affect how you first perceive it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 但仅仅让一个人审查和盖章机器学习模型提出的决策并不能完全避免所有可能的伤害；人类可能有他们自己的偏见和假设，他们可能过于信任自动化系统^([2](ch07.xhtml#ch01fn7))，或者他们可能没有足够的资历来对结果做出任何有意义的改变。看到AI的预测可能会影响他们自己的决策；如果在听录音之前看了一份文字记录，很难将话语听成不是你已经读过的文字——就像光学错觉上的标题可能影响你最初的感知一样。
- en: You need to consider what information will be available to the human exercising
    that oversight, what incentives they have to uphold or reject automated decisions,
    and how you can responsibly present the AI suggestions and allow the user to explore
    alternatives.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要考虑向行使监督权的人提供哪些信息，他们有什么样的激励来支持或拒绝自动化决策，以及如何负责地呈现AI建议并允许用户探索替代方案。
- en: If you’re using the Azure OpenAI Service to generate text that looks as if it’s
    written by a human being, consider whether you need the user to know it was autogenerated
    rather than written and reviewed by another person. Offering multiple suggestions
    for users to pick from, and explicitly calling them suggestions, may avoid them
    just clicking OK without taking the time to consider if it’s accurate and appropriate.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Azure OpenAI服务生成看起来像是人类写的文本，请考虑是否需要让用户知道它是自动生成的，而不是由另一个人编写和审核的。为用户提供多个建议供选择，并明确称其为建议，可能会避免他们仅仅点击“确定”而不花时间考虑其准确性和适当性。
- en: The OpenAI Service includes content filters that let you customize the tone
    and topic of the content produced. But if it’s generating something critical,
    whether that’s legal advice, contract boilerplate, or APIs to give low-code developers
    access to your enterprise’s own data and services, you want to design a thorough
    review process into the workflow.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI服务包括内容过滤器，允许您定制内容的语调和主题。但是，如果它生成了某些关键性内容，无论是法律建议、合同模板还是API，让低代码开发人员访问您企业自己的数据和服务，您需要在工作流程中设计一个彻底的审查流程。
- en: Automatic aggregations in Power BI are created by machine learning that analyzes
    user query patterns so the system can cache aggregate measures that will improve
    query performance without consuming too many resources. A slider allows administrators
    to fine-tune this behavior, so they can make their own decisions about the trade-off
    between performance and storage. If it’s appropriate for your app, giving users
    the option to explore and experiment can help them make the best decision.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Power BI中的自动聚合是通过分析用户查询模式的机器学习创建的，因此系统可以缓存会提高查询性能的聚合度量，而不会消耗太多资源。管理员可以使用滑块来微调此行为，以便他们可以自行决定性能与存储之间的权衡。如果适合您的应用程序，为用户提供探索和实验的选择可能有助于他们做出最佳决策。
- en: Make sure it’s clear what the confidence values are for each prediction or decision,
    and build apps that support people by making information available rather than
    replace them. When measuring the effectiveness of AI systems, look beyond simple
    precision and recall metrics and look at whether they help people get better results
    than they would without using the system.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 确保每个预测或决策的置信度值是清晰的，并构建支持人们的应用程序，通过提供信息而不是取代他们来支持他们。在衡量AI系统的效果时，不要仅仅看简单的精确度和召回率指标，要看它们是否比不使用系统时帮助人们取得更好的结果。
- en: If you have a chatbot or an automated system, think about how you can provide
    an “off-ramp” to allow users to escalate their question to a human—and include
    the information they’ve already provided so they don’t have to explain everything
    from the beginning. There may be topics that the AI could handle but that you
    will want to have a person deal with instead, because they’re complicated or nuanced.
    And the more sophisticated the chatbot is—especially if you have a realistic neural
    voice or text that reads as if it was written by a human from the services we
    looked at in [Chapter 4](ch04.xhtml#using_azure_cognitive_services_to_build)—the
    more important it is to be clear when someone is interacting with an AI system
    rather than a real person and to tell them how to reach a human instead.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有聊天机器人或自动系统，请考虑如何提供“退出通道”，让用户升级其问题到人工环节，并包括他们已经提供的信息，这样他们就不必从头解释一切。AI可能能够处理的主题，您可能希望由人来处理，因为它们复杂或微妙。尤其是如果您拥有一个逼真的神经语音或文本，看起来就像是人类编写的服务，清楚地告诉用户何时与AI系统而不是真人交互，以及如何联系真人至关重要。
- en: Having a fallback option is key; if your user isn’t happy with the automatic
    results from an AI tool, give them a way to fill in the details themselves or
    to contact a human to help them instead. Collecting examples (with permission)
    of where your models aren’t producing good results gives you useful data for further
    development and training, but remember to tell the user what you will do with
    that data and whether they should expect to see changes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有备用选项至关重要；如果您的用户对AI工具的自动结果不满意，请为他们提供一种方式自行填写详细信息或联系人工帮助他们。收集例子（经过许可）表明您的模型未能产生良好的结果，为进一步的开发和培训提供有用的数据，但请记住告知用户您将如何处理这些数据以及他们是否应该期待看到变化。
- en: Tip
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Your users will frequently expect the AI system to learn and get better based
    on how they interact with it: make it clear whether that happens or not. Asking
    for feedback and examples of where the system isn’t working can be a good way
    to find beta testers for your next version. That will also help you understand
    which kind of failures are really important, because not all errors are the same
    and there are some you need to work harder to deal with (if they’re offensive
    rather than just confusing, say).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您的用户通常期望AI系统根据他们的互动学习和改进：清楚地表明是否会发生这种情况。请求反馈和例子，表明系统不起作用的地方，可以成为找到下一个版本的Beta测试人员的好方法。这也将帮助您了解哪些类型的故障真正重要，因为并非所有错误都相同，有些需要更多的工作来处理（如果它们是冒犯性的而不仅仅是令人困惑的话）。
- en: Use the tools for transparency, interpretability, error analysis, and causal
    reasoning to explore what you should tell the human in the loop. For example,
    the HAX Toolkit has a set of guidelines for how AI systems should interact with
    humans, illustrated by the design library of helpful patterns shown in [Figure 7-2](#the_guidelines_in_the_hax_toolkit_will),
    with examples drawn from familiar services.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用透明性、可解释性、错误分析和因果推理工具来探索您应该告知参与人员的内容。例如，HAX Toolkit提供了一组指南，说明了AI系统与人类的交互方式，这些指南通过设计库中的实用模式来说明，这些模式从熟悉的服务中提取了例子。
- en: '![The guidelines in the HAX Toolkit will help you present AI features responsibly](Images/aasc_0702.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![HAX Toolkit指南中的准则将帮助您负责地展示AI功能](Images/aasc_0702.png)'
- en: Figure 7-2\. The HAX Toolkit guidelines will help you present AI features responsibly
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. HAX Toolkit指南将帮助您负责地展示AI功能
- en: “Human in the loop” also applies while developing machine learning models—for
    example, using machine learning to suggest the labels for training data that a
    person (ideally someone with expertise in the area) still has to accept and apply.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发机器学习模型时，“人在循环中”的概念也适用——例如，使用机器学习建议标签来训练数据，但实际上仍需某人（最好是该领域的专家）接受并应用这些标签。
- en: Wrapping It Up
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter we’ve looked at what you can think of as the Spiderman principle:
    AI is powerful—and with great power comes great responsibility. Even if you’re
    using rather than building machine learning models, it’s important to think about
    what could go wrong and use the principles of responsible AI to avoid or mitigate
    potential issues. The tools we’ve covered will help you move from thinking about
    how to use AI ethically to making that part of your machine learning workflow
    to get the practical benefits; doing well by doing good, as it were.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了您可以将其视为蜘蛛侠原则的内容：AI是强大的——伴随着强大的力量而来的是巨大的责任。即使您只是使用而不是构建机器学习模型，思考可能出现的问题并使用负责任AI的原则来避免或减轻潜在问题是很重要的。我们介绍的工具将帮助您从如何道德使用AI思考到将其作为您的机器学习工作流程的一部分，以获得实际的好处；做好事做好事，正如我们所说的那样。
- en: But while responsible AI is where you need to start, there are other issues
    to consider alongside it, like experimentation and collaboration. In the next
    chapter we’ll look at how to build a strong data culture that will help you deliver
    all the best practices that support responsible AI.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 但是虽然负责任的AI是你需要开始的地方，但还有其他问题需要与之并行考虑，比如实验和协作。在下一章中，我们将探讨如何建立一个强大的数据文化，这将帮助您提供支持负责任AI的所有最佳实践。
- en: Further Resources
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多资源
- en: 'Responsible AI is a broad topic with important research still ongoing; if you
    want to dive in more deeply, there’s a wide range of resources:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 负责任的AI是一个广泛的话题，重要的研究仍在进行中；如果您希望深入了解更多，有大量的资源可供选择：
- en: See all Microsoft’s [Responsible AI principles](https://go.microsoft.com/fwlink/?linkid=2190289)
    in one place.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看Microsoft的所有[负责任AI原则](https://go.microsoft.com/fwlink/?linkid=2190289)。
- en: '[Microsoft Learn: Identify principles and practices for responsible AI](https://go.microsoft.com/fwlink/?linkid=2190290)
    covers setting up a governance model and applying responsible AI practices in
    your organization.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Microsoft Learn：识别负责任AI的原则和实践](https://go.microsoft.com/fwlink/?linkid=2190290)
    包括建立治理模型和在您的组织中应用负责任AI实践。'
- en: If you’re a manager or business executive managing people who build AI systems
    for your organizations, take a look at the [Ten Guidelines for Product Leaders
    to Implement AI Responsibly](https://go.microsoft.com/fwlink/?linkid=2190291),
    a guide written by Microsoft and Boston Consulting Group.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您是管理人员或业务主管，负责为您的组织构建AI系统的人员，请查看由微软和波士顿咨询集团编写的[产品领导人实施负责任AI的十项指南](https://go.microsoft.com/fwlink/?linkid=2190291)。
- en: If you want to explore AI checklists further, the BBC’s [Machine Learning Engine
    Principles](https://go.microsoft.com/fwlink/?linkid=2190292) includes a useful
    example, although some of the questions are very specific to dealing with content
    and broadcasting. For healthcare, the [Transparent Reporting of a multivariable
    prediction model for Individual Prognosis Or Diagnosis (TRIPOD) Initiative](https://www.tripod-statement.org)
    has a detailed checklist for developing prediction models, which gives a good
    idea of the level of detail necessary for transparency in a regulated industry.
    For organizations affected by the GDPR, the Information Commissioner’s Office’s
    (ICO’s) [AI and Data Protection Risk Toolkit](https://go.microsoft.com/fwlink/?linkid=2190294)
    focuses on compliance but includes a comprehensive list of risks, including fairness,
    transparency, and meaningful human review as well as security and accountability,
    with practical steps for addressing each risk.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想进一步探索AI核对表，BBC的[机器学习引擎原则](https://go.microsoft.com/fwlink/?linkid=2190292)
    提供了一个有用的例子，尽管其中一些问题非常具体，涉及内容和广播处理。对于医疗保健领域，[透明报告个体预后或诊断多变量预测模型(TRIPOD)倡议](https://www.tripod-statement.org)
    提供了一个详细的核对表，为开发预测模型提供了透明度规定行业所需的详细级别。对于受GDPR影响的组织，信息专员办公室（ICO）的[AI和数据保护风险工具包](https://go.microsoft.com/fwlink/?linkid=2190294)
    侧重于合规性，但包括一份详尽的风险清单，包括公平性、透明度、有意义的人类审查以及安全性和问责制，以及应对每个风险的实际步骤。
- en: ICO Guidance on [ensuring lawfulness, fairness, and transparency in AI systems](https://go.microsoft.com/fwlink/?linkid=2190179)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ICO关于[确保AI系统的合法性、公平性和透明性的指导](https://go.microsoft.com/fwlink/?linkid=2190179)。
- en: 'Azure Cloud Advocates Machine Learning for Beginners: [Fairness in Machine
    Learning](https://go.microsoft.com/fwlink/?linkid=2190296).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure云倡导者机器学习入门：[机器学习中的公平性](https://go.microsoft.com/fwlink/?linkid=2190296)。
- en: Explore different kinds of AI harms and fairness issues and learn how to assess
    which might occur with your system with [Harms Modeling](https://go.microsoft.com/fwlink/?linkid=2190178).
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索不同类型的AI伤害和公平性问题，并学习如何评估您的系统可能发生的问题，使用[Harms Modeling](https://go.microsoft.com/fwlink/?linkid=2190178)。
- en: See the impact of a common representational harm with [Gender Shades](http://gendershades.org),
    a site that lets you explore a seminal paper on responsible AI in facial recognition,
    and see what happened after it was published in the [Coded Bias](https://www.codedbias.com)
    documentary.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看常见的表征性伤害的影响，使用[Gender Shades](http://gendershades.org)，一个让您探索关于面部识别中负责任AI的重要论文的网站，并查看其在[Coded
    Bias](https://www.codedbias.com)纪录片发布后发生了什么。
- en: '[“Unfairness by Algorithm: Distilling the Harms of Automated Decision-Making.”](https://go.microsoft.com/fwlink/?linkid=2190297)
    The Future of Privacy Forum has a comprehensive taxonomy of the problems that
    can be caused by “algorithmic discrimination,” along with suggested strategies.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“算法不公平：提炼自动决策的危害。”](https://go.microsoft.com/fwlink/?linkid=2190297) 未来隐私论坛提供了由“算法歧视”引起的问题的全面分类，以及建议的策略。'
- en: The [Algorithmic Impact Assessment report](https://go.microsoft.com/fwlink/?linkid=2190298)
    looks at how to evaluate approaches to responsible AI and accountability.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[算法影响评估报告](https://go.microsoft.com/fwlink/?linkid=2190298) 研究如何评估负责任AI和责任制的方法。'
- en: ^([1](ch07.xhtml#ch01fn6-marker)) “[The State of Responsible AI”](https://go.microsoft.com/fwlink/?linkid=2190284),
    FICO, May 2021.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.xhtml#ch01fn6-marker)) “[负责任AI的现状](https://go.microsoft.com/fwlink/?linkid=2190284),
    FICO, 2021年5月。
- en: '^([2](ch07.xhtml#ch01fn7-marker)) Research suggests that even though we know
    they’re not people, we instinctively treat computers as if they were (Reeves,
    B., and C. Nass. *The Media Equation: How People Treat Computers, Television,
    and New Media Like Real People and Places*. [New York: Cambridge University Press,
    1996]). Helping your users create a “[mental model](https://go.microsoft.com/fwlink/?linkid=2190175)”
    of how an AI system makes predictions and decisions can help them decide when
    they should trust those suggestions and when they should question them.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch07.xhtml#ch01fn7-marker)) 研究表明，尽管我们知道它们不是人类，我们本能地将计算机视为人类对待（Reeves,
    B., and C. Nass. *The Media Equation: How People Treat Computers, Television,
    and New Media Like Real People and Places*. [纽约：剑桥大学出版社，1996]）。帮助您的用户创建一个关于AI系统如何进行预测和决策的“[心理模型](https://go.microsoft.com/fwlink/?linkid=2190175)”可以帮助他们决定何时应该信任这些建议，何时应该质疑它们。'
