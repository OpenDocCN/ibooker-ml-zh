- en: Chapter 7\. Recurrent Neural Networks for Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章。用于自然语言处理的递归神经网络
- en: In [Chapter 5](ch05.xhtml#introduction_to_natural_language_proces) you saw how
    to tokenize and sequence text, turning sentences into tensors of numbers that
    could then be fed into a neural network. You then extended that in [Chapter 6](ch06.xhtml#making_sentiment_programmable_using_emb)
    by looking at embeddings, a way to have words with similar meanings cluster together
    to enable the calculation of sentiment. This worked really well, as you saw by
    building a sarcasm classifier. But there’s a limitation to that, namely in that
    sentences aren’t just bags of words—often the *order* in which the words appear
    will dictate their overall meaning. Adjectives can add to or change the meaning
    of the nouns they appear beside. For example, the word “blue” might be meaningless
    from a sentiment perspective, as might “sky,” but when you put them together to
    get “blue sky” there’s a clear sentiment there that’s usually positive. And some
    nouns may qualify others, such as “rain cloud,” “writing desk,” “coffee mug.”
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第五章](ch05.xhtml#introduction_to_natural_language_proces)中，您看到了如何对文本进行标记化和序列化，将句子转换为可以输入神经网络的数字张量。然后在[第六章](ch06.xhtml#making_sentiment_programmable_using_emb)中，您通过研究嵌入来扩展了这一概念，这是一种使具有相似含义的词汇聚集在一起的方法，以便计算情感。这种方法非常有效，正如您通过构建讽刺分类器所看到的那样。但是，这也有其局限性，即句子不仅仅是词汇的集合——词汇的顺序通常会决定其整体含义。形容词可以增加或改变它们旁边的名词的含义。例如，“蓝色”从情感角度来看可能毫无意义，同样“天空”也是如此，但是当您将它们组合成“蓝天”时，通常会产生一种明确的积极情感。有些名词可能会修饰其他名词，比如“雨云”、“写字桌”、“咖啡杯”。
- en: 'To take sequences like this into account, an additional approach is needed,
    and that is to factor *recurrence* into the model architecture. In this chapter
    you’ll look at different ways of doing this. We’ll explore how sequence information
    can be learned, and how this information can be used to create a type of model
    that is better able to understand text: the *recurrent neural network* (RNN).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要考虑到这样的序列，需要一种额外的方法，那就是在模型架构中考虑*递归*。在本章中，您将看到不同的实现方式。我们将探索如何学习序列信息，以及如何利用这些信息来创建一种更能理解文本的模型：*递归神经网络*（RNN）。
- en: The Basis of Recurrence
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环的基础
- en: To understand how recurrence might work, let’s first consider the limitations
    of the models used thus far in the book. Ultimately, creating a model looks a
    little bit like [Figure 7-1](#high_level_view_of_model_creation). You provide
    data and labels and define a model architecture, and the model learns the rules
    that fit the data to the labels. Those rules then become available to you as an
    API that will give you back predicted labels for future data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解递归如何工作，让我们首先考虑到目前为止在本书中使用的模型的局限性。最终，创建一个模型看起来有点像[图7-1](#high_level_view_of_model_creation)。您提供数据和标签，并定义一个模型架构，模型学习适合数据与标签的规则。然后这些规则作为API提供给您，以便将来预测数据的标签。
- en: '![High-level view of model creation](Images/aiml_0701.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![模型创建的高级视图](Images/aiml_0701.png)'
- en: Figure 7-1\. High-level view of model creation
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1。模型创建的高级视图
- en: But, as you can see, the data is lumped in wholesale. There’s no granularity
    involved, and no effort to understand the sequence in which that data occurs.
    This means the words “blue” and “sky” have no different meaning in sentences such
    as “today I am blue, because the sky is gray” and “today I am happy, and there’s
    a beautiful blue sky.” To us the difference in the use of these words is obvious,
    but to a model, with the architecture shown here, there really is no difference.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如您所看到的，数据是作为整体存在的。没有细化的过程，也没有努力去理解数据发生的顺序。这意味着，“蓝色”和“天空”在“今天我感到忧郁，因为天空是灰色”和“今天我很开心，天空很美丽”这样的句子中没有不同的含义。对于我们来说，这些词的使用差异是显而易见的，但是对于一个使用这里显示的架构的模型来说，实际上是没有差异的。
- en: So how do we fix this? Let’s first explore the nature of recurrence, and from
    there you’ll be able to see how a basic RNN can work.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们该如何解决这个问题呢？让我们首先探索递归的本质，然后您就能看到基本的RNN是如何工作的。
- en: Consider the famous Fibonacci sequence of numbers. In case you aren’t familiar
    with it, I’ve put some of them into [Figure 7-2](#the_first_few_numbers_in_the_fibonacci_).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下著名的斐波那契数列。如果您对它不熟悉，我已经将一些放在[图7-2](#the_first_few_numbers_in_the_fibonacci_)中。
- en: '![The first few numbers in the Fibonacci sequence](Images/aiml_0702.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![斐波那契数列的前几个数字](Images/aiml_0702.png)'
- en: Figure 7-2\. The first few numbers in the Fibonacci sequence
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2。斐波那契数列的前几个数字
- en: The idea behind this sequence is that every number is the sum of the two numbers
    preceding it. So if we start with 1 and 2, the next number is 1 + 2, which is
    3\. The one after that is 2 + 3, which is 5, then 3 + 5, which is 8, and so on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个序列背后的思想是，每个数字都是前两个数字的和。所以如果我们从1和2开始，下一个数字是1 + 2，即3。然后是2 + 3，即5，然后是3 + 5，即8，依此类推。
- en: We can place this in a computational graph to get [Figure 7-3](#a_computational_graph_representation_of).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这放在计算图中得到[Figure 7-3](#a_computational_graph_representation_of)。
- en: '![A computational graph representation of the Fibonacci sequence](Images/aiml_0703.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![Fibonacci序列的计算图表示](Images/aiml_0703.png)'
- en: Figure 7-3\. A computational graph representation of the Fibonacci sequence
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 7-3\. Fibonacci序列的计算图表示
- en: Here you can see that we feed 1 and 2 into the function and get 3 as the output.
    We carry the second parameter (2) over to the next step, and feed it into the
    function along with the output from the previous step (3). The output of this
    is 5, and it gets fed into the function with the second parameter from the previous
    step (3) to produce an output of 8\. This process continues indefinitely, with
    every operation depending on those before it. The 1 at the top left sort of “survives”
    through the process. It’s an element of the 3 that gets fed into the second operation,
    it’s an element of the 5 that gets fed into the third, and so on. Thus, some of
    the essence of the 1 is preserved throughout the sequence, though its impact on
    the overall value is diminished.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到我们将1和2输入函数，得到3作为输出。我们将第二个参数（2）传递到下一个步骤，并将它与前一个步骤的输出（3）一起输入函数。这样得到的输出是5，并将其与前一个步骤的第二个参数（3）一起输入函数得到8。这个过程无限进行下去，每个操作都依赖于前面的操作。左上角的1在整个过程中“存活”。它是传入第二次操作的3的一部分，是传入第三次操作的5的一部分，依此类推。因此，1的某些本质在整个序列中得到了保留，尽管它对整体值的影响逐渐减弱。
- en: This is analogous to how a recurrent neuron is architected. You can see the
    typical representation of a recurrent neuron in [Figure 7-4](#a_recurrent_neuron).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于递归神经元的结构。你可以在[Figure 7-4](#a_recurrent_neuron)中看到典型的递归神经元表示。
- en: '![A recurrent neuron](Images/aiml_0704.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![递归神经元](Images/aiml_0704.png)'
- en: Figure 7-4\. A recurrent neuron
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 7-4\. 一个递归神经元
- en: A value x is fed into the function F at a time step, so it’s typically labeled
    x[t]. This produces an output y at that time step, which is typically labeled
    y[t]. It also produces a value that is fed forward to the next step, which is
    indicated by the arrow from F to itself.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值x在时间步中被输入到函数F中，通常标记为x[t]。这产生该时间步的输出y[t]，通常标记为y[t]。它还产生一个传递到下一个步骤的值，由从F到自身的箭头表示。
- en: This is made a little clearer if you look at how recurrent neurons work beside
    each other across time steps, which you can see in [Figure 7-5](#recurrent_neurons_in_time_steps).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看看时间步中的递归神经元如何在一起工作，这一点就会更清楚，你可以在[Figure 7-5](#recurrent_neurons_in_time_steps)中看到。
- en: '![Recurrent neurons in time steps](Images/aiml_0705.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![时间步中的递归神经元](Images/aiml_0705.png)'
- en: Figure 7-5\. Recurrent neurons in time steps
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 7-5\. 时间步中的递归神经元
- en: Here, x[0] is operated on to get y[0] and a value that’s passed forward. The
    next step gets that value and x[1] and produces y[1] and a value that’s passed
    forward. The next one gets that value and x[2] and produces y[2] and a pass-forward
    value, and so on down the line. This is similar to what we saw with the Fibonacci
    sequence, and I always find that to be a handy mnemonic when trying to remember
    how an RNN works.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，对x[0]进行操作得到y[0]和传递的值。下一步得到该值和x[1]，产生y[1]和传递的值。接下来的步骤中，获取该值和x[2]，产生y[2]和传递的值，依此类推。这与我们在斐波那契序列中看到的情况类似，我总是觉得这是一个很好的助记符，帮助记住RNN的工作原理。
- en: Extending Recurrence for Language
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展语言的递归
- en: In the previous section you saw how a recurrent neural network operating over
    several time steps can help maintain context across a sequence. Indeed, RNNs will
    be used for sequence modeling later in this book. But there’s a nuance when it
    comes to language that can be missed when using a simple RNN like those in [Figure 7-4](#a_recurrent_neuron)
    and [Figure 7-5](#recurrent_neurons_in_time_steps). As in the Fibonacci sequence
    example mentioned earlier, the amount of context that’s carried over will diminish
    over time. The effect of the output of the neuron at step 1 is huge at step 2,
    smaller at step 3, smaller still at step 4, and so on. So if we have a sentence
    like “Today has a beautiful blue <something>,” the word “blue” will have a strong
    impact on the next word; we can guess that it’s likely to be “sky.” But what about
    context that comes from further back in a sentence? For example, consider the
    sentence “I lived in Ireland, so in high school I had to learn how to speak and
    write <something>.”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，您看到了一个递归神经网络如何在多个时间步长上操作，以帮助在序列中保持上下文。确实，在本书的后面，RNNs 将用于序列建模。但是，当涉及到语言时，使用简单的
    RNN（如图 7-4 中的[递归神经元](#a_recurrent_neuron) 和图 7-5 中的[时间步骤中的递归神经元](#recurrent_neurons_in_time_steps)）可能会忽略一个细微差别。就像之前提到的斐波那契数列示例一样，随着时间的推移，所携带的上下文量会逐渐减少。在步骤
    1 的神经元输出的影响在步骤 2 时很大，在步骤 3 时较小，在步骤 4 时更小，依此类推。因此，如果我们有一个句子如“今天天气很好，有美丽的蓝色<something>”，单词“蓝色”对下一个单词的影响很大；我们可以猜测下一个单词可能是“天空”。但是来自句子更早部分的上下文呢？例如，考虑句子“I
    lived in Ireland, so in high school I had to learn how to speak and write<something>。”。
- en: That <something> is Gaelic, but the word that really gives us that context is
    “Ireland,” which is much further back in the sentence. Thus, for us to be able
    to recognize what <something> should be, a way for context to be preserved across
    a longer distance is needed. The short-term memory of an RNN needs to get longer,
    and in recognition of this an enhancement to the architecture, called *long short-term
    memory* (LSTM), was invented.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 那个<something>是盖尔语，但真正给我们提供上下文的词是“Ireland”，它在句子中要远得多。因此，为了能够识别<something>应该是什么，需要一种能够在更长距离上保留上下文的方法。RNN
    的短期记忆需要变长，在承认这一点的基础上，对架构的改进被发明出来，称为*长短期记忆*（LSTM）。
- en: While I won’t go into detail on the underlying architecture of how LSTMs work,
    the high-level diagram shown in [Figure 7-6](#high_level_view_of_lstm_architecture)
    gets the main point across. To learn more about the internal operations, check
    out Christopher Olah’s excellent [blog post](https://oreil.ly/6KcFA) on the subject.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细讨论 LSTM 如何工作的底层架构，但[图 7-6](#high_level_view_of_lstm_architecture) 中显示的高级图表传达了主要观点。要了解更多关于内部操作的信息，请查看
    Christopher Olah 的优秀[博文](https://oreil.ly/6KcFA)。
- en: The LSTM architecture enhances the basic RNN by adding a “cell state” that enables
    context to be maintained not just from step to step, but across the entire sequence
    of steps. Remembering that these are neurons, learning in the way neurons do,
    you can see that this ensures that the context that is important will be learned
    over time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 架构通过添加“单元状态”来增强基本的 RNN，使得不仅可以在步骤与步骤之间，而且可以跨整个步骤序列中保持上下文。请记住这些是神经元，像神经元一样学习，这确保了重要的上下文会随着时间学习。
- en: '![High-level view of LSTM architecture](Images/aiml_0706.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 架构的高级视图](Images/aiml_0706.png)'
- en: Figure 7-6\. High-level view of LSTM architecture
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6 LSTM 架构的高级视图
- en: An important part of an LSTM is that it can be bidirectional—the time steps
    are iterated both forward and backward, so that context can be learned in both
    directions. See [Figure 7-7](#high_level_view_of_lstm_bidirectional_a) for a high-level
    view of this.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 的一个重要部分是它可以是双向的——时间步骤既可以向前迭代，也可以向后迭代，因此可以在两个方向上学习上下文。详见[图 7-7](#high_level_view_of_lstm_bidirectional_a)
    中的高级视图。
- en: '![High-level view of LSTM bidirectional architecture](Images/aiml_0707.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 双向架构的高级视图](Images/aiml_0707.png)'
- en: Figure 7-7\. High-level view of LSTM bidirectional architecture
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7 LSTM 双向架构的高级视图
- en: In this way, evaluation in the direction from 0 to *number_of_steps* is done,
    as is evaluation from *number_of_steps* to 0\. At each step, the *y* result is
    an aggregation of the “forward” pass and the “backward” pass. You can see this
    in [Figure 7-8](#bidirectional_lstm).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，从 0 到*number_of_steps* 的方向进行评估，以及从*number_of_steps* 到 0 进行评估。在每个步骤中，*y* 结果是“前向”传递和“后向”传递的汇总。您可以在[图 7-8](#bidirectional_lstm)
    中看到这一点。
- en: '![Bidirectional LSTM](Images/aiml_0708.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![双向 LSTM](Images/aiml_0708.png)'
- en: Figure 7-8\. Bidirectional LSTM
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-8 双向 LSTM
- en: 'Consider each neuron at each time step to be F0, F1, F2, etc. The direction
    of the time step is shown, so the calculation at F1 in the forward direction is
    F1(->), and in the reverse direction it’s (<-)F1\. The values of these are aggregated
    to give the y value for that time step. Additionally, the cell state is bidirectional.
    This can be really useful for managing context in sentences. Again considering
    the sentence “I lived in Ireland, so in high school I had to learn how to speak
    and write <something>,” you can see how the <something> was qualified to be “Gaelic”
    by the context word “Ireland.” But what if it were the other way around: “I lived
    in <this country>, so in high school I had to learn how to speak and write Gaelic”?
    You can see that by going *backward* through the sentence we can learn about what
    <this country> should be. Thus, using bidirectional LSTMs can be very powerful
    for understanding sentiment in text (and as you’ll see in [Chapter 8](ch08.xhtml#using_tensorflow_to_create_text),
    they’re really powerful for generating text too!).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步考虑每个神经元为F0、F1、F2等。时间步的方向显示在这里，所以前向方向的F1计算是F1(->)，反向方向是(<-)F1。这些值被聚合以给出该时间步的y值。此外，细胞状态是双向的。这对于管理句子中的上下文非常有用。再次考虑句子“I
    lived in Ireland, so in high school I had to learn how to speak and write <something>”，你可以看到<something>是通过上下文词“Ireland”来限定为“Gaelic”。但如果情况反过来：“I
    lived in <this country>，so in high school I had to learn how to speak and write
    Gaelic”？通过反向遍历句子，我们可以了解<this country>应该是什么。因此，使用双向LSTM对于理解文本中的情感非常强大（正如你将在[第8章](ch08.xhtml#using_tensorflow_to_create_text)中看到的，它们对于生成文本也非常强大！）。
- en: Of course, there’s a lot going on with LSTMs, in particular bidirectional ones,
    so expect training to be slow. Here’s where it’s worth investing in a GPU, or
    at the very least using a hosted one in Google Colab if you can.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，使用LSTM时有很多复杂的情况，特别是双向LSTM，所以期望训练速度较慢。这时候值得投资一块GPU，或者至少在Google Colab上使用托管的GPU。
- en: Creating a Text Classifier with RNNs
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RNN创建文本分类器
- en: In [Chapter 6](ch06.xhtml#making_sentiment_programmable_using_emb) you experimented
    with creating a classifier for the Sarcasm dataset using embeddings. In that case
    words were turned into vectors before being aggregated and then fed into dense
    layers for classification. When using an RNN layer such as an LSTM, you don’t
    do the aggregation and can feed the output of the embedding layer directly into
    the recurrent layer. When it comes to the dimensionality of the recurrent layer,
    a rule of thumb you’ll often see is that it’s the same size as the embedding dimension.
    This isn’t necessary, but can be a good starting point. Note that while in [Chapter 6](ch06.xhtml#making_sentiment_programmable_using_emb)
    I mentioned that the embedding dimension is often the fourth root of the size
    of the vocabulary, when using RNNs you’ll often see that that rule is ignored
    because it would make the size of the recurrent layer too small.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.xhtml#making_sentiment_programmable_using_emb)中，你尝试使用嵌入创建了一个Sarcasm数据集的分类器。在那种情况下，单词在汇总之前被转换为向量，然后被送入密集层进行分类。当使用RNN层如LSTM时，你不会进行汇总，可以直接将嵌入层的输出馈送到递归层。当涉及到递归层的维度时，你经常会看到的一个经验法则是它与嵌入维度相同。这并非必需，但可以作为一个很好的起点。请注意，虽然在[第6章](ch06.xhtml#making_sentiment_programmable_using_emb)中我提到嵌入维度通常是词汇量的四分之一根号，但当使用RNN时，你通常会看到这个规则被忽略，因为这会使递归层的大小太小。
- en: 'So, for example, the simple model architecture for the sarcasm classifier you
    developed in [Chapter 6](ch06.xhtml#making_sentiment_programmable_using_emb) could
    be updated to this to use a bidirectional LSTM:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，例如，在[第6章](ch06.xhtml#making_sentiment_programmable_using_emb)中你开发的Sarcasm分类器的简单模型架构可以更新为使用双向LSTM：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The loss function and classifier can be set to this (note that the learning
    rate is 0.00001, or 1e–5):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数和分类器可以设置为这个（请注意学习率为0.00001，或者1e–5）：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When you print out the model architecture summary, you’ll see something like
    this. Note that the vocab size is 20,000 and the embedding dimension is 64\. This
    gives 1,280,000 parameters in the embedding layer, and the bidirectional layer
    will have 128 neurons (64 out, 64 back) :'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当你打印出模型架构摘要时，你会看到像这样的内容。注意词汇表大小为20,000，嵌入维度为64。这使得嵌入层有1,280,000个参数，双向层将有128个神经元（64个前向，64个后向）：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Figure 7-9](#accuracy_for_lstm_over_threezero_epochs) shows the results of
    training with this over 30 epochs.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-9](#accuracy_for_lstm_over_threezero_epochs)显示了使用这个模型在30个epochs上训练的结果。'
- en: As you can see, the accuracy of the network on training data rapidly climbs
    above 90%, but the validation data plateaus at around 80%. This is similar to
    the figures we got earlier, but inspecting the loss chart in [Figure 7-10](#loss_with_lstm_over_threezero_epochs)
    shows that while the loss for the validation set diverged after 15 epochs, it
    also flattened out to a much lower value than the loss charts in [Chapter 6](ch06.xhtml#making_sentiment_programmable_using_emb),
    despite using 20,000 words instead of 2,000.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，网络在训练数据上的准确率迅速上升至90%以上，但验证数据在大约80%处趋于平稳。这与我们之前得到的数据类似，但检查[图 7-10](#loss_with_lstm_over_threezero_epochs)中的损失图表表明，尽管在15个epoch后验证集的损失出现了分歧，但与[第6章](ch06.xhtml#making_sentiment_programmable_using_emb)中的损失图表相比，使用了2万个词而非2千个词后，它也趋于平缓并降至了较低的值。
- en: '![Accuracy for LSTM over 30 epochs](Images/aiml_0709.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 30个epoch的准确率](Images/aiml_0709.png)'
- en: Figure 7-9\. Accuracy for LSTM over 30 epochs
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-9\. LSTM 30个epoch的准确率
- en: '![Loss with LSTM over 30 epochs](Images/aiml_0710.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![LSTM 30个epoch的损失](Images/aiml_0710.png)'
- en: Figure 7-10\. Loss with LSTM over 30 epochs
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-10\. LSTM 30个epoch的损失
- en: This was just using a single LSTM layer, however. In the next section you’ll
    see how to use stacked LSTMs and explore the impact on the accuracy of classifying
    this dataset.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，这仅使用了单个LSTM层。在下一节中，您将看到如何使用堆叠LSTMs，并探讨其对分类此数据集准确性的影响。
- en: Stacking LSTMs
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠LSTMs
- en: In the previous section you saw how to use an LSTM layer after the embedding
    layer to help with classifying the contents of the Sarcasm dataset. But LSTMs
    can be stacked on top of each other, and this approach is used in many state-of-the-art
    NLP models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您看到如何在嵌入层后使用LSTM层来帮助分类讽刺数据集的内容。但是LSTMs可以相互堆叠，许多最先进的自然语言处理模型都使用了这种方法。
- en: 'Stacking LSTMs with TensorFlow is pretty straightforward. You add them as extra
    layers just like you would with a `Dense` layer, but with the exception that all
    of the layers prior to the last one will need to have their `return_sequences`
    property set to `True`. Here’s an example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow堆叠LSTMs非常简单。您只需像添加`Dense`层一样添加它们作为额外的层，但所有层都需要将其`return_sequences`属性设置为`True`，直到最后一层为止。以下是一个例子：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The final layer can also have `return_sequences=True` set, in which case it
    will return sequences of values to the dense layers for classification instead
    of single ones. This can be handy when parsing the output of the model, as we’ll
    discuss later. The model architecture will look like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层也可以设置`return_sequences=True`，在这种情况下，它将返回模型输出的序列值而不是单个值给密集层进行分类。这在解析模型输出时非常方便，稍后我们将讨论。模型架构将如下所示：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Adding the extra layer will give us roughly 100,000 extra parameters that need
    to be learned, an increase of about 8%. So, it might slow the network down, but
    the cost is relatively low if there’s a reasonable benefit.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 添加额外的层将使我们大约有100,000个额外的参数需要学习，增加约8%。因此，这可能会减慢网络的速度，但如果有合理的好处，成本相对较低。
- en: After training for 30 epochs, the result looks like [Figure 7-11](#accuracy_for_stacked_lstm_architecture).
    While the accuracy on the validation set is flat, examining the loss ([Figure 7-12](#loss_for_stacked_lstm_architecture))
    tells a different story.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练了30个epoch后，结果如[图 7-11](#accuracy_for_stacked_lstm_architecture)所示。虽然验证集的准确率保持平稳，但检查损失（[图 7-12](#loss_for_stacked_lstm_architecture)）却讲述了不同的故事。
- en: '![Accuracy for stacked LSTM architecture](Images/aiml_0711.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![堆叠LSTM架构的准确率](Images/aiml_0711.png)'
- en: Figure 7-11\. Accuracy for stacked LSTM architecture
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-11\. 堆叠LSTM架构的准确率
- en: As you can see in [Figure 7-12](#loss_for_stacked_lstm_architecture), while
    the accuracy for both training and validation looked good, the validation loss
    quickly took off upwards, a clear sign of overfitting.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图 7-12](#loss_for_stacked_lstm_architecture)中所见，尽管训练和验证的准确率看起来都很好，但验证损失迅速上升，这是过拟合的明显迹象。
- en: '![Loss for stacked LSTM architecture](Images/aiml_0712.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![堆叠LSTM架构的损失](Images/aiml_0712.png)'
- en: Figure 7-12\. Loss for stacked LSTM architecture
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-12\. 堆叠LSTM架构的损失
- en: This overfitting (indicated by the training accuracy climbing toward 100% as
    the loss falls smoothly, while the validation accuracy is relatively steady and
    the loss increases drastically) is a result of the model getting overspecialized
    for the training set. As with the examples in [Chapter 6](ch06.xhtml#making_sentiment_programmable_using_emb),
    this shows that it’s easy to be lulled into a false sense of security if you just
    look at the accuracy metrics without examining the loss.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合（由于训练准确性向 100% 上升，同时损失平稳下降，而验证准确性相对稳定且损失急剧增加）是模型过于专注于训练集的结果。与 [第 6 章](ch06.xhtml#making_sentiment_programmable_using_emb)
    中的示例一样，这表明如果仅查看准确性指标而不检查损失，就很容易陷入错误的安全感中。
- en: Optimizing stacked LSTMs
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化堆叠 LSTM
- en: In [Chapter 6](ch06.xhtml#making_sentiment_programmable_using_emb) you saw that
    a very effective method to reduce overfitting was to reduce the learning rate.
    It’s worth exploring whether that will have a positive effect on a recurrent neural
    network too.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 6 章](ch06.xhtml#making_sentiment_programmable_using_emb) 中，您看到减少学习率是减少过拟合非常有效的方法。值得探索一下，这是否对循环神经网络也会产生积极的影响。
- en: 'For example, the following code reduces the learning rate by 20% from 0.00001
    to 0.000008:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下代码将学习率从 0.00001 减少了 20%，变为 0.000008：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Figure 7-13](#impact_of_reduced_learning_rate_on_accu) demonstrates the impact
    of this on training. There doesn’t seem to be much of a difference, although the
    curves (particularly for the validation set) are a little smoother.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-13](#impact_of_reduced_learning_rate_on_accu) 展示了这一点对训练的影响。看起来差别不大，尽管曲线（特别是对于验证集）稍微平滑一些。'
- en: '![Impact of reduced learning rate on accuracy with stacked LSTMs](Images/aiml_0713.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![使用 dropout 的堆叠 LSTM 的准确性](Images/aiml_0713.png)'
- en: Figure 7-13\. Impact of reduced learning rate on accuracy with stacked LSTMs
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-13\. 堆叠 LSTM 的准确性随降低学习率的影响
- en: 'While an initial look at [Figure 7-14](#impact_of_reduced_learning_rate_on_loss)
    similarly suggests minimal impact on loss due to the reduced learning rate, it’s
    worth looking a little closer. Despite the shape of the curve being roughly similar,
    the rate of loss increase is clearly lower: after 30 epochs it’s at around 0.6,
    whereas with the higher learning rate it was close to 0.8\. Adjusting the learning
    rate hyperparameter certainly seems worth investigation.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管初步观察 [图 7-14](#impact_of_reduced_learning_rate_on_loss) 同样显示由于降低学习率而对损失的影响较小，但值得更仔细地观察。尽管曲线形状大致相似，但损失增长速率显然更低：经过
    30 个 epoch 后，损失约为 0.6，而较高学习率时接近 0.8。调整学习率超参数确实值得研究。
- en: '![Impact of reduced learning rate on loss with stacked LSTMs ](Images/aiml_0714.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![堆叠 LSTM 的损失随降低学习率的影响](Images/aiml_0714.png)'
- en: Figure 7-14\. Impact of reduced learning rate on loss with stacked LSTMs
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-14\. 减少学习率对堆叠 LSTM 的损失的影响
- en: Using dropout
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 dropout
- en: In addition to changing the learning rate parameter, it’s also worth considering
    using dropout in the LSTM layers. It works exactly the same as for dense layers,
    as discussed in [Chapter 3](ch03.xhtml#going_beyond_the_basics_detecting_featu),
    where random neurons are dropped to prevent a proximity bias from impacting the
    learning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 除了改变学习率参数外，在 LSTM 层中使用 dropout 也是值得考虑的。它的工作原理与密集层完全相同，正如在 [第 3 章](ch03.xhtml#going_beyond_the_basics_detecting_featu)
    中讨论的那样，随机删除神经元以防止邻近偏差影响学习。
- en: 'Dropout can be implemented using a parameter on the LSTM layer. Here’s an example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 可以通过 LSTM 层上的参数实现。以下是一个示例：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Do note that implementing dropout will greatly slow down your training. In my
    case, using Colab, it went from ~10 seconds per epoch to ~180 seconds.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在实施 dropout 后会显著减慢训练速度。在我的情况下，使用 Colab，每个 epoch 的时间从约 10 秒增加到约 180 秒。
- en: The accuracy results can be seen in [Figure 7-15](#accuracy_of_stacked_lstms_using_dropout).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性结果可以在 [图 7-15](#accuracy_of_stacked_lstms_using_dropout) 中看到。
- en: '![Accuracy of stacked LSTMs using dropout](Images/aiml_0715.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![使用 dropout 的堆叠 LSTM 的准确性](Images/aiml_0715.png)'
- en: Figure 7-15\. Accuracy of stacked LSTMs using dropout
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-15\. 使用 dropout 的堆叠 LSTM 的准确性
- en: As you can see, using dropout doesn’t have much impact on the accuracy of the
    network, which is good! There’s always a worry that losing neurons will make your
    model perform worse, but as we can see here that’s not the case.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，使用 dropout 对网络的准确性影响不大，这是好事！总是担心失去神经元会使模型表现更差，但正如我们在这里看到的那样，情况并非如此。
- en: There’s also a positive impact on loss, as you can see in [Figure 7-16](#loss_curves_for_dropout_enabled_lstms).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[图 7-16](#loss_curves_for_dropout_enabled_lstms) 中所见，损失也有积极的影响。
- en: While the curves are clearly diverging, they are closer than they were previously,
    and the validation set is flattening out at a loss of about 0.5\. That’s significantly
    better than the 0.8 seen previously. As this example shows, dropout is another
    handy technique that you can use to improve the performance of LSTM-based RNNs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然曲线明显分开，但它们比之前更接近，验证集的损失在约 0.5 左右趋于平缓。这比之前看到的 0.8 显著更好。正如这个例子所示，dropout 是另一种可以用来提高基于
    LSTM 的 RNN 性能的方便技术。
- en: '![Loss curves for dropout-enabled LSTMs](Images/aiml_0716.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![使用启用了 dropout 的 LSTM 的损失曲线](Images/aiml_0716.png)'
- en: Figure 7-16\. Loss curves for dropout-enabled LSTMs
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-16\. 使用 dropout 的 LSTM 的损失曲线
- en: It’s worth exploring these techniques to avoid overfitting in your data, as
    well as the techniques to preprocess your data that we covered in [Chapter 6](ch06.xhtml#making_sentiment_programmable_using_emb).
    But there’s one thing that we haven’t yet tried—a form of transfer learning where
    you can use prelearned embeddings for words instead of trying to learn your own.
    We’ll explore that next.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 探索这些技术以避免过拟合您的数据，以及我们在[第 6 章](ch06.xhtml#making_sentiment_programmable_using_emb)
    中介绍的预处理数据的技术。但有一件事我们还没有尝试过——一种迁移学习的形式，您可以使用预先学习的词嵌入，而不是尝试学习您自己的。我们将在接下来探讨这个问题。
- en: Using Pretrained Embeddings with RNNs
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RNNs 的预训练嵌入
- en: In all the previous examples, you gathered the full set of words to be used
    in the training set, then trained embeddings with them. These were initially aggregated
    before being fed into a dense network, and in this chapter you explored how to
    improve the results using an RNN. While doing this, you were restricted to the
    words in your dataset and how their embeddings could be learned using the labels
    from that dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有先前的例子中，您收集了用于训练集的完整单词集，然后用它们训练嵌入。这些最初是聚合的，然后被输入到稠密网络中，在这一章中，您探索了如何使用 RNN
    来改进结果。在这样做的同时，您受限于数据集中的词汇及其嵌入如何使用从该数据集的标签中学习。
- en: Think back to [Chapter 4](ch04.xhtml#using_public_datasets_with_tensorflow_d),
    where we discussed transfer learning. What if, instead of learning the embeddings
    for yourself, you could instead use prelearned embeddings, where researchers have
    already done the hard work of turning words into vectors and those vectors are
    proven? One example of this is the [GloVe (Global Vectors for Word Representation)
    model](https://oreil.ly/4ENdQ) developed by Jeffrey Pennington, Richard Socher,
    and Christopher Manning at Stanford.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第 4 章](ch04.xhtml#using_public_datasets_with_tensorflow_d)，我们讨论了迁移学习。如果，你可以使用预先学习的嵌入，而不是自己学习嵌入，那会怎样呢？研究人员已经把词语转换为向量，并且这些向量是经过验证的，一个例子是由斯坦福大学的杰弗里·彭宁顿、理查德·索切尔和克里斯托弗·曼宁开发的[GloVe（全球词向量表示）模型](https://oreil.ly/4ENdQ)。
- en: 'In this case, the researchers have shared their pretrained word vectors for
    a variety of datasets:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，研究人员分享了各种数据集的预训练词向量：
- en: A 6-billion-token, 400,000-word vocab set in 50, 100, 200, and 300 dimensions
    with words taken from Wikipedia and Gigaword
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个从维基百科和 Gigaword 中取词的 60 亿令牌、40 万字词汇、50、100、200 和 300 维度的集合
- en: A 42-billion-token, 1.9-million-word vocab in 300 dimensions from a common crawl
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自常见爬网的 4200 亿令牌、190 万字词汇、300 维度
- en: An 840-billion-token, 2.2-million-word vocab in 300 dimensions from a common
    crawl
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个来自常见爬网的 8400 亿令牌、220 万字词汇、300 维度
- en: A 27-billion-token, 1.2-million-word vocab in 25, 50, 100 and 200 dimensions
    from a Twitter crawl of 2 billion tweets
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个来自 Twitter 爬 20 亿推文的 270 亿令牌、120 万字词汇、25、50、100 和 200 维度
- en: Given that the vectors are already pretrained, it’s simple to reuse them in
    your TensorFlow code, instead of learning from scratch. First, you’ll have to
    download the GloVe data. I’ve opted to use the Twitter data with 27 billion tokens
    and a 1.2-million-word vocab. The download is an archive with 25, 50, 100, and
    200 dimensions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些向量已经预先训练，您可以简单地在您的 TensorFlow 代码中重复使用它们，而不是从头开始学习。首先，您需要下载 GloVe 数据。我选择使用
    270 亿令牌和 120 万字词汇的 Twitter 数据。下载是一个包含 25、50、100 和 200 维度的存档。
- en: 'To make it a little easier for you I’ve hosted the 25-dimension version, and
    you can download it into a Colab notebook like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使您更容易，我已经托管了 25 维度版本，并且您可以像这样将其下载到 Colab 笔记本中：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'It’s a ZIP file, so you can extract it like this to get a file called *glove.twitter.27b.25d.txt*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 ZIP 文件，所以您可以像这样解压它，得到一个名为*glove.twitter.27b.25d.txt*的文件：
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each entry in the file is a word, followed by the dimensional coefficients
    that were learned for it. The easiest way to use this is to create a dictionary
    where the key is the word, and the values are the embeddings. You can set up this
    dictionary like this:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中的每个条目都是一个单词，后跟为其学习的维度系数。使用这种方法的最简单方式是创建一个字典，其中键是单词，值是嵌入。您可以像这样设置此字典：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'At this point you’ll be able to look up the set of coefficients for any word
    simply by using it as the key. So, for example, to see the embeddings for “frog,”
    you could use:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您可以简单地使用单词作为键来查找任何单词的系数集。例如，要查看“frog”的嵌入，您可以使用：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With this resource in hand, you can use the tokenizer to get the word index
    for your corpus as before—but now you can create a new matrix, which I’ll call
    the *embedding matrix*. This will use the GloVe set embeddings (taken from `glove_embeddings`)
    as its values. So, if you examine the words in the word index for your dataset,
    like this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个资源，您可以像以前一样使用标记器获取您的语料库的单词索引，但现在您可以创建一个新的矩阵，我将其称为*嵌入矩阵*。这将使用GloVe集合的嵌入（从`glove_embeddings`中获取）作为其值。因此，如果您检查数据集中的单词索引的单词，就像这样：
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Then the first row in the embedding matrix should be the coefficients from GloVe
    for “<OOV>,” the next row will be the coefficients for “new,” and so on.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，嵌入矩阵中的第一行应该是“<OOV>”的GloVe系数，下一行将是“new”的系数，依此类推。
- en: 'You can create that matrix with this code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下代码创建该矩阵：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This simply creates a matrix with the dimensions of your desired vocab size
    and the embedding dimension. Then, for every item in the tokenizer’s word index,
    you look up the coefficients from GloVe in `glove_embeddings`, and add those values
    to the matrix.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是创建一个具有所需词汇大小和嵌入维度的矩阵。然后，对于标记器的每个单词索引中的每个项，您从`glove_embeddings`中查找系数，并将这些值添加到矩阵中。
- en: 'You then amend the embedding layer to use the pretrained embeddings by setting
    the `weights` parameter, and specify that you don’t want the layer to be trained
    by setting `trainable=False`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过设置`weights`参数将嵌入层改为使用预训练的嵌入，并通过设置`trainable=False`来指定不训练该层：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You can now train as before. However, you’ll want to consider your vocab size.
    One of the optimizations you did in the previous chapter to avoid overfitting
    was intended to prevent the embeddings becoming overburdened with learning low-frequency
    words; you avoided overfitting by using a smaller vocabulary of frequently used
    words. In this case, as the word embeddings have already been learned for you
    with GloVe, you could expand the vocabulary—but by how much?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以像以前一样进行训练。但是，您需要考虑您的词汇量大小。在上一章中，为了避免过拟合，您进行了优化，目的是避免使学习低频词汇的嵌入变得过度负担，您通过使用较小的经常使用的单词词汇来避免过拟合。在这种情况下，由于单词嵌入已经在GloVe中学习过，您可以扩展词汇表，但要扩展多少？
- en: The first thing to explore is how many of the words in your corpus are actually
    in the GloVe set. It has 1.2 million words, but there’s no guarantee it has *all*
    of your words.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要探索的是您的语料库中有多少单词实际上在GloVe集合中。它有120万个单词，但不能保证它*所有*都有您的单词。
- en: So, here’s some code to perform a quick comparison so you can explore how large
    your vocab should be.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里有一些代码可以进行快速比较，以便您可以探索您的词汇量应该有多大。
- en: 'First let’s sort the data out. Create a list of Xs and Ys, where X is the word
    index, and Y=1 if the word is in the embeddings and 0 if it isn’t. Additionally,
    you can create a cumulative set, where you count the quotient of words at every
    time step. For example, the word “OOV” at index 0 isn’t in GloVe, so its cumulative
    Y would be 0\. The word “new,” at the next index, is in GloVe, so it’s cumulative
    Y would be 0.5 (i.e., half of the words seen so far are in GloVe), and you’d continue
    to count that way for the entire dataset:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们整理一下数据。创建一个X和Y的列表，其中X是单词索引，如果单词在嵌入中则Y=1，否则为0。此外，您可以创建一个累积集合，在每个时间步骤计算单词的商。例如，索引为0的单词“OOV”不在GloVe中，所以它的累积Y为0。下一个索引处的单词“new”在GloVe中，所以它的累积Y为0.5（即到目前为止一半的单词在GloVe中），您可以继续以此方式统计整个数据集：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You then plot the Xs against the Ys with this code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用以下代码将X与Y进行绘制：
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This will give you a word frequency chart, which will look something like [Figure 7-17](#word_frequency_chart).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为您提供一个词频图，看起来类似于[图 7-17](#word_frequency_chart)。
- en: '![Word frequency chart](Images/aiml_0717.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![词频图](Images/aiml_0717.png)'
- en: Figure 7-17\. Word frequency chart
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-17\. 词频图
- en: As you can see in the chart, the density changes somewhere between 10,000 and
    15,000\. This gives you an eyeball check that somewhere around token 13,000 the
    frequency of words that are *not* in the GloVe embeddings starts to outpace those
    that *are*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在图表中所见，密度在 10,000 和 15,000 之间发生变化。这使您可以粗略检查，大约在第 13,000 个 token 处，不在 GloVe
    嵌入中的单词频率开始超过在其中的单词。
- en: 'If you then plot the `cumulative_x` against the `cumulative_y`, you can get
    a better sense of this. Here’s the code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您然后绘制 `cumulative_x` 对 `cumulative_y`，您可以更好地理解这一点。这是代码：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can see the results in [Figure 7-18](#plotting_frequency_of_word_index_agains).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [图 7-18](#plotting_frequency_of_word_index_agains) 中查看结果。
- en: '![Plotting frequency of word index against GloVe](Images/aiml_0718.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![绘制单词索引频率对 GloVe 的影响](Images/aiml_0718.png)'
- en: Figure 7-18\. Plotting frequency of word index against GloVe
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-18\. 绘制单词索引频率对 GloVe 的影响
- en: You can now tweak the parameters in `plt.axis` to zoom in to find the inflection
    point where words not present in GloVe begin to outpace those that are in GloVe.
    This is a good starting point for you to set the size of your vocabulary.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以调整 `plt.axis` 中的参数来放大，以找到不在 GloVe 中的单词开始超过那些在 GloVe 中的单词的拐点。这是您设置词汇表大小的良好起点。
- en: 'Using this method, I chose to use a vocab size of 13,200 (instead of the 2,000
    that was previously used to avoid overfitting) and this model architecture, where
    the `embedding_dim` is `25` because of the GloVe set I’m using:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我选择了词汇量为 13,200（而不是之前的 2,000，以避免过拟合），以及这种模型架构，其中 `embedding_dim` 是 `25`，因为我使用的是
    GloVe 集：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Training this for 30 epochs yields some excellent results. The accuracy is shown
    in [Figure 7-19](#stacked_lstm_accuracy_using_glove_embed). The validation accuracy
    is very close to the training accuracy, indicating that we are no longer overfitting.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 将其训练 30 个 epoch 可以获得一些出色的结果。准确性显示在 [图 7-19](#stacked_lstm_accuracy_using_glove_embed)
    中。验证准确性非常接近训练准确性，表明我们不再过拟合。
- en: '![Stacked LSTM accuracy using GloVe embeddings](Images/aiml_0719.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![使用 GloVe 嵌入的堆叠 LSTM 准确性](Images/aiml_0719.png)'
- en: Figure 7-19\. Stacked LSTM accuracy using GloVe embeddings
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-19\. 使用 GloVe 嵌入的堆叠 LSTM 准确性
- en: This is reinforced by the loss curves, as shown in [Figure 7-20](#stacked_lstm_loss_using_glove_embedding).
    The validation loss no longer diverges, showing that although our accuracy is
    only ~73% we can be confident that the model is accurate to that degree.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点得到了损失曲线的支持，如 [图 7-20](#stacked_lstm_loss_using_glove_embedding) 所示。验证损失不再发散，表明尽管我们的准确性只有约
    73%，但我们可以相信模型在这个程度上是准确的。
- en: '![Stacked LSTM loss using GloVe embeddings](Images/aiml_0720.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![使用 GloVe 嵌入的堆叠 LSTM 损失](Images/aiml_0720.png)'
- en: Figure 7-20\. Stacked LSTM loss using GloVe embeddings
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-20\. 使用 GloVe 嵌入的堆叠 LSTM 损失
- en: Training the model for longer shows very similar results, and indicates that
    while overfitting begins to occur right around epoch 80, the model is still very
    stable.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型训练更长时间显示出非常相似的结果，并表明尽管过拟合开始在大约第 80 个 epoch 发生，但模型仍然非常稳定。
- en: The accuracy metrics ([Figure 7-21](#accuracy_on_stacked_lstm_with_glove_ove))
    show a well-trained model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性指标 ([图 7-21](#accuracy_on_stacked_lstm_with_glove_ove)) 显示出一个训练良好的模型。
- en: The loss metrics ([Figure 7-22](#loss_on_stacked_lstm_with_glove_over_on)) show
    the beginning of divergence at around epoch 80, but the model still fits well.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 损失指标 ([图 7-22](#loss_on_stacked_lstm_with_glove_over_on)) 显示大约在第 80 个 epoch
    开始发散，但模型仍然适合。
- en: '![Accuracy on stacked LSTM with GloVe over 150 epochs](Images/aiml_0721.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![使用 GloVe 嵌入的堆叠 LSTM 准确性](Images/aiml_0721.png)'
- en: Figure 7-21\. Accuracy on stacked LSTM with GloVe over 150 epochs
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-21\. 使用 GloVe 嵌入的堆叠 LSTM 准确性在 150 个 epoch 上
- en: '![Loss on stacked LSTM with GloVe over 150 epochs](Images/aiml_0722.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![使用 GloVe 的堆叠 LSTM 在 150 个 epoch 上的损失](Images/aiml_0722.png)'
- en: Figure 7-22\. Loss on stacked LSTM with GloVe over 150 epochs
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-22\. 使用 GloVe 嵌入的堆叠 LSTM 在 150 个 epoch 上的损失
- en: This tells us that this model is a good candidate for early stopping, where
    you can just train it for 75–80 epochs to get the optimal results.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，这个模型是早期停止的一个好候选，您只需训练 75–80 个 epoch 就可以得到最佳结果。
- en: 'I tested it with headlines from *The Onion*, source of the sarcastic headlines
    in the Sarcasm dataset, against other sentences, as shown here:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我用 *The Onion* 的标题测试了它，这是 Sarcasm 数据集中讽刺标题的来源，与其他句子进行了对比，如下所示：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The results for these headlines are as follows—remember that values close to
    50% (0.5) are considered neutral, close to 0 nonsarcastic, and close to 1 sarcastic:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标题的结果如下 —— 请记住，接近 50%（0.5）的值被视为中性，接近 0 的值为非讽刺，接近 1 的值为讽刺：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The first and fourth sentences, taken from *The Onion*, showed 80%+ likelihood
    of sarcasm. The statement about the weather was strongly nonsarcastic (9%), and
    the sentence about going to high school in Ireland was deemed to be potentially
    sarcastic, but not with high confidence (62%).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 第一句和第四句，摘自*The Onion*，显示出80%以上的讽刺可能性。关于天气的声明强烈地不是讽刺（9%），而关于在爱尔兰上高中的句子被认为可能是讽刺的，但置信度不高（62%）。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced you to recurrent neural networks, which use sequence-oriented
    logic in their design and can help you understand the sentiment in sentences based
    not only on the words they contain, but also the order in which they appear. You
    saw how a basic RNN works, as well as how an LSTM can build on this to enable
    context to be preserved over the long-term. You used these to improve the sentiment
    analysis model you’ve been working on. You then looked into overfitting issues
    with RNNs and techniques to improve them, including using transfer learning from
    pretrained embeddings. In [Chapter 8](ch08.xhtml#using_tensorflow_to_create_text)
    you’ll use what you’ve learned to explore how to predict words, and from there
    you’ll be able to create a model that creates text, writing poetry for you!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您介绍了递归神经网络，它们在设计中使用面向序列的逻辑，并且可以帮助您理解句子中的情感，这不仅基于它们包含的单词，还基于它们出现的顺序。您了解了基本的RNN工作原理，以及LSTM如何在此基础上进行改进，从而能够长期保留上下文。您利用这些内容改进了自己一直在研究的情感分析模型。然后，您深入探讨了RNN的过拟合问题以及改进技术，包括利用预训练嵌入进行迁移学习。在[第八章](ch08.xhtml#using_tensorflow_to_create_text)中，您将利用所学知识探索如何预测单词，并从中创建一个为您写诗的文本生成模型！
