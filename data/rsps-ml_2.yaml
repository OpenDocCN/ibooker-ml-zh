- en: 'Chapter 2\. People: Humans in the Loop'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：人：人类在环路中
- en: “People worry that computers will get too smart and take over the world, but
    the real problem is that they’re too stupid and they’ve already taken over the
    world.”
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “人们担心计算机会变得太聪明并接管世界，但真正的问题是它们太愚蠢，它们已经接管了世界。”
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pedro Domingos
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 佩德罗·多明戈斯
- en: Since its inception, there has been the temptation to give AI and ML increasingly
    more agency. However, this should *not* be the goal for organizations deploying
    ML today. Due to all the AI incidents we’re seeing, we firmly believe the technology
    isn’t mature enough. Instead, the goal should be to make sure humans are in the
    loop of ML-based decision making. Human involvement is imperative because an all
    too common mistake, as the quote above highlights, is for firms to assume their
    responsible ML duties lie solely in technological implementation. This chapter
    presents many of the human considerations that companies must address when building
    out their ML infrastructure. We’ll start with organizational culture then shift
    the discussion to how practitioners and consumers can get more involved with the
    inner workings of ML systems. The chapter closes by highlighting some recent examples
    of employee activism and data journalism related to the responsible practice of
    ML.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自其成立以来，人们越来越倾向于赋予人工智能和机器学习越来越多的代理权。然而，这不应该是当今部署机器学习的组织的目标。由于我们正在看到的所有人工智能事件，我们坚信这项技术还不够成熟。相反，目标应该是确保人类参与到基于机器学习的决策中去。人类的参与至关重要，因为正如上面的引用所强调的那样，一个非常普遍的错误是企业认为他们的负责任的机器学习职责仅仅在于技术实施。本章节介绍了企业在构建机器学习基础设施时必须考虑的许多人类因素。我们将从组织文化开始，然后转向讨论从业者和消费者如何更多地参与机器学习系统的内部运作。章节最后强调了一些与负责任地实践机器学习相关的员工活动和数据新闻的最新例子。
- en: Responsible Machine Learning Culture
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任的机器学习文化
- en: An organization’s ML culture is an essential aspect of responsible ML. This
    section will discuss the cultural notions of accountability, dogfooding, effective
    challenge, and demographic and professional diversity. We’ll also discuss the
    arguably stale adage, “go fast and break things.”
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一个组织的机器学习文化是负责任机器学习的一个重要方面。本节将讨论责任、自我使用、有效挑战以及人口和职业多样性的文化概念。我们还将讨论那句看似陈词滥调的话：“快速前进并破坏事物”。
- en: Accountability
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 责任
- en: 'A key to the successful mitigation of ML risks is real accountability. Ask
    yourself: “Who tracks the way ML is developed and used at my organization? Who
    is responsible for auditing our ML systems? Do we have AI incident response plans?”
    For many organizations today, the answers may be, “no one” and, “no.” If no one’s
    job is on the line when an ML system fails or gets attacked, then it’s possible
    that no one at that organization really cares about ML risks. This is a primary
    reason that many leading financial institutions now employ chief model risk officers.
    Smaller organizations may not be able to spare an entire full-time employee to
    monitor ML model risk. Still, it’s essential to have an individual or group responsible
    and held accountable if ML systems misbehave. In our experience, if an organization
    assumes everyone is accountable for ML risk and AI incidents, the reality is that
    no one is accountable.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 成功缓解机器学习风险的关键是真正的责任制。问问自己：“谁追踪我组织中机器学习的开发和使用方式？谁负责审计我们的机器学习系统？我们有人工智能事件响应计划吗？”对于今天的许多组织来说，答案可能是“没有人”，“没有”。如果在一个机器学习系统失败或遭到攻击时没有人的工作受到威胁，那么可能该组织中真正没有人关心机器学习的风险。这是许多领先金融机构现在雇佣首席模型风险官的主要原因。较小的组织可能无法抽出一个全职员工来监控机器学习模型风险。但是，有一个负责任和被追责的个人或团队仍然至关重要，如果机器学习系统表现不佳。根据我们的经验，如果一个组织假设每个人都对机器学习风险和人工智能事件负责，那么现实情况可能是没有人真正负责。
- en: Dogfooding
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自我使用
- en: '*Dogfooding* is a term from software engineering that refers to an organization
    using its own software, i.e., “eating your own dog food.” In the context of responsible
    ML, dogfooding brings an additional layer of alpha or prealpha testing that is
    often neglected in the mad dash to profit from a perceived ML gold rush. More
    importantly, dogfooding can bring legal and risk questions to the forefront. If
    an organization has developed an ML system that operates in a manner that, say,
    violates their own privacy policies, or is meant to be deceptive or manipulative,
    employees engaging in dogfooding might find this objectionable and raise concerns.
    Dogfooding can bring the Golden Rule into ML: if you wouldn’t use an ML system
    on yourself, you probably should not use it on others. We’ll discuss diversity
    in the next section, but it’s worth mentioning here that if your team is more
    diverse, dogfooding is more likely to detect a wider variety of objectionable
    (or problematic) features.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dogfooding* 是软件工程的一个术语，指的是一个组织使用自己的软件，即“吃自己的狗食”。在负责任的ML的背景下，dogfooding带来了额外的Alpha或Prealpha测试层次，通常在从认为是ML黄金热潮中获利的疯狂冲刺中被忽视。更重要的是，dogfooding可以将法律和风险问题置于前沿。如果一个组织开发了一个操作方式违反他们自己的隐私政策、或者旨在欺骗或操纵的ML系统，那么参与dogfooding的员工可能会觉得这是不可接受的，并提出关注。Dogfooding可以将黄金法则引入ML中：如果你不会在自己身上使用一个ML系统，那么你可能不应该在其他人身上使用它。我们将在下一节讨论多样性，但在这里值得一提的是，如果您的团队更加多样化，dogfooding更有可能检测到更广泛的可挑剔（或有问题的）特性。'
- en: Demographic and Professional Diversity
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人口统计和专业多样性
- en: Many have [documented the unfortunate outcomes](https://oreil.ly/SA0ow) that
    can arise as a result of ML engineers not considering demographic diversity in
    the training or results of ML systems. A potential solution to these kinds of
    oversights is increasing demographic diversity on ML teams from its [current woeful
    levels](https://oreil.ly/xT8nF). Another type of diversity that can also help
    to mitigate ML risk is a diversity of professional experience. According to Professor
    Karl Broman at the University of Wisconsin, “If you’re analyzing data, [you’re
    doing statistics](https://oreil.ly/-qoYp)”. ML is very much a data analysis field,
    and as such, it is a statistical discipline. Despite Kaggle leaderboards prizing
    single outcomes from single models, ML systems often benefit from the perspectives
    of other data analysis disciplines such as statistics, econometrics, or psychometrics.
    These fields have rich histories of learning that they can bring to bear on almost
    any ML project. Security personnel are another useful technical add-on to ML projects,
    as ML can present data privacy and security concerns.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人已经[记录了因ML工程师在ML系统的培训或结果中未考虑人口统计多样性而可能产生的不幸结果](https://oreil.ly/SA0ow)。解决这些疏忽的一个潜在解决方案是增加ML团队的人口统计多样性，而其[当前令人遗憾的水平](https://oreil.ly/xT8nF)。另一种也可以帮助缓解ML风险的多样性是专业经验的多样性。根据威斯康星大学的卡尔·布罗曼教授所说，“如果你在分析数据，[你正在做统计学](https://oreil.ly/-qoYp)”
    。ML在很大程度上是一个数据分析领域，因此它是一个统计学的学科。尽管Kaggle的排行榜青睐于单一模型的单一结果，但ML系统通常受益于其他数据分析学科（如统计学、计量经济学或心理测量学）的视角。这些领域有丰富的学习历史，可以在几乎任何ML项目上发挥作用。安全人员是ML项目的另一个有用的技术补充，因为ML可能涉及数据隐私和安全问题。
- en: Developing teams with deep cross-disciplinary professional experience can be
    invaluable as you look to deploy ML. Many of the most successful quantitative
    investment and advisory companies, such as McKinsey or Renaissance Technologies,
    pride themselves on how they have assembled an elite team with extremely diverse
    technical backgrounds from physics, biology, medicine, astronomy, and other fields.
    Legal, compliance, and audit personnel can also be necessary for ML projects.
    ML projects can run afoul of laws, regulations, or corporate privacy policies.
    Involving oversight professionals from the beginning is a great way to assess
    and mitigate these risks. ML can push ethical boundaries, too, and very few ML
    engineers have the education or experience necessary to guide a project through
    murky ethical waters. Bringing professional ethicists into the development of
    ML systems can help manage moral problems as they arise.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 发展具有深度跨学科专业经验团队，在部署机器学习时非常宝贵。许多最成功的量化投资和咨询公司，如麦肯锡或文艺复兴技术公司，以他们如何组建来自物理学、生物学、医学、天文学等领域极其多样化技术背景的精英团队为傲。法律、合规和审计人员在机器学习项目中也可能是必要的。机器学习项目可能会触犯法律、法规或企业隐私政策。从一开始就涉及监督专业人员是评估和减轻这些风险的好方法。机器学习还可能挑战伦理底线，而很少有机器学习工程师具备管理项目穿越不明确伦理道德领域所需的教育或经验。将专业伦理学家引入机器学习系统的开发中，有助于在问题出现时管理道德问题。
- en: Cultural Effective Challenge
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文化有效挑战
- en: The notion of effective challenge was born out of the practice of model governance.
    When building complex ML systems, effective challenge roughly says that one of
    the best ways to guarantee good results is to actively challenge and question
    steps in the ML development process. There are more technical aspects of effective
    challenge, which will be addressed in [Chapter 4](ch04.xhtml#technology_engineering_machine_learning_for_human_trust_and_understanding).
    Still, a culture that encourages serious questioning of ML design choices will
    be more likely to catch problems before they balloon into AI incidents. Of course,
    cultural effective challenge cannot be abusive, and it must apply to everyone
    developing an ML system, even so-called “rockstar” engineers and data scientists.
    In our experience, cultural effective challenge practices should be structured,
    such as weekly meetings where alternative design and implementation choices are
    questioned and discussed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有效挑战的概念源于模型治理的实践。在构建复杂的机器学习系统时，有效挑战大致表明，确保良好结果的最佳途径之一是积极挑战和质疑机器学习开发过程中的步骤。关于有效挑战还有更多技术方面的内容，将在[第四章](ch04.xhtml#technology_engineering_machine_learning_for_human_trust_and_understanding)中详细讨论。但是，鼓励严肃质疑机器学习设计选择的文化，更有可能在问题扩大成人工智能事件之前发现问题。当然，文化有效挑战不能滥用，并且必须适用于所有开发机器学习系统的人员，即使是所谓的“摇滚明星”工程师和数据科学家。根据我们的经验，文化有效挑战实践应该是有结构的，例如每周会议上会质疑和讨论替代的设计和实施选择。
- en: Going Fast and Breaking Things
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速前进与破坏
- en: The saying, “go fast and break things” is enshrined in the mindsets of many
    top engineers and data scientists. Unfortunately, when you go fast and break things,
    things tend to break. If you’re working in the space of entertainment apps and
    advertisements for those apps, this may not be such a big deal. But suppose you’re
    using ML in medicine, human resources, credit lending, criminal justice, the military,
    or other high-stakes applications. In these fields, going fast and breaking things
    can break the law or ruin people’s lives. Practitioners must recognize the implications
    and downstream risks of their work instead of racing towards results for an outdated
    maxim.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: “快速前进并破坏”这句话深深印在许多顶级工程师和数据科学家的心中。不幸的是，当你快速前进并破坏时，事情往往会出问题。如果你在娱乐应用和其广告的领域工作，这可能并不是什么大问题。但假设你在医学、人力资源、信贷借贷、刑事司法、军事或其他高风险应用领域使用机器学习，快速前进并破坏可能会违法或毁了人们的生活。从业者必须意识到其工作的后果和下游风险，而不是为了过时的格言而赛跑追求结果。
- en: 'Traditional model governance practices offer options to defend against these
    kinds of breakages, such as rigorous validation and monitoring. However, these
    practices require serious resources: lots of people, time, and technology. Standard
    model governance may not be feasible for young or smaller organizations under
    commercial pressure to move quickly. Common sense indicates that when going fast
    and breaking things, and without conventional model governance, AI incidents are
    even more likely. So, if you need to go fast and break things, AI incident response
    plans can be crucial for your organization. With AI incident response, as discussed
    in [Chapter 3](ch03.xhtml#processes_taming_the_wild_west_of_machine_learning_workflows),
    smaller organizations without the resources for strict supervision of ML projects
    can spend their limited resources in ways that allow them to move quickly, but
    also confront the inevitability of AI incidents. In the long run, it’s possible
    that being prepared for complex system failures may ultimately be the fastest
    development strategy.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的模型治理实践提供了防范这些故障的选项，如严格的验证和监控。然而，这些做法需要大量的人力、时间和技术资源。对于处于商业压力下需要快速行动的年轻或较小的组织来说，标准的模型治理可能不可行。常识表明，当迅速行动和打破事物，以及没有传统模型治理时，人工智能事件更有可能发生。因此，如果您需要快速行动和打破事物，AI
    事件响应计划对您的组织至关重要。通过AI 事件响应，正如第3章所讨论的那样，没有资源严格监督机器学习项目的较小组织可以将有限资源用于可以使其快速行动，但也可以面对人工智能事件的方式。从长远来看，为复杂系统故障做好准备可能最终是最快的发展策略。
- en: Get in the Loop
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进入循环
- en: Now that we’ve touched on some cultural aspects of responsible ML, this section
    will describe concrete steps practitioners or managers can take to get more control
    over ML systems. As of today, humans still have a big role to play in the successful
    implementation of ML systems. As the quote at the beginning of this chapter highlights,
    many decision makers and practitioners may be putting too much faith in today’s
    ML systems. Along with the serious questioning of ML design choices in effective
    challenge, a human’s detailed review of ML systems is another viable risk mitigation
    strategy. Inventories and documentation are a staple of model governance, and
    many recent AI and ML best practice guidelines highlight the need for human audits
    of ML systems. Of course, all of this requires people with a deep understanding
    of the data and problem domain, and that ML systems are constructed to enable
    interactions with those domain experts. Without domain expertise, ML systems can
    be trained on incorrect data, results can be misinterpreted, audits are less meaningful,
    and data or programming errors may explode into full-blown AI incidents. ML systems
    should also typically be designed to allow users to provide meaningful feedback,
    particularly to appeal and override ML-based decisions, and, if necessary, flipping
    the kill switch!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涉及了一些关于负责任的机器学习的文化方面，本节将描述从业者或管理者可以采取的具体步骤，以便更好地控制机器学习系统。截至今天，人类在成功实施机器学习系统中仍然扮演着重要角色。正如本章开头的引用所强调的那样，许多决策者和从业者可能对当今的机器学习系统过于信任。除了有效挑战机器学习设计选择之外，人类对机器学习系统的详细审查是另一种可行的风险缓解策略。模型治理的清单和文档是模型治理的基础，许多最近的人工智能和机器学习最佳实践指南强调了对机器学习系统进行人工审计的必要性。当然，所有这些都需要深入了解数据和问题领域的人员，并且需要构建机器学习系统以便与这些领域专家进行交互。没有领域专业知识，机器学习系统可能会被训练在不正确的数据上，结果可能会被误解，审计也会变得不那么有意义，数据或编程错误可能会演变成全面的人工智能事件。机器学习系统通常也应设计为允许用户提供有意义的反馈，特别是对基于机器学习的决策进行申诉和覆盖，并在必要时切断开关！
- en: Human Audit of Machine Learning Systems
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习系统的人工审计
- en: With the advent of technologies that enable more behind-the-scenes transparency
    and better discrimination and security testing of ML systems, it is now possible
    to foster enhanced human understanding and trust of ML. These technologies will
    be discussed in [Chapter 4](ch04.xhtml#technology_engineering_machine_learning_for_human_trust_and_understanding),
    but the technologies still must be deployed by people. One of the best uses of
    these newer technologies is the human audit of ML systems. In a [recent paper](https://oreil.ly/2znTg),
    researchers at Google put forward a framework for ML model audits. They’ve also
    put forward basic sample documentation for [models](https://oreil.ly/Fqgug) and
    [data](https://oreil.ly/pzVBW). These developments come on the coattails of years
    of model governance in the financial services vertical, where governance, effective
    challenge, model inventories, model documentation, model validation, and multiple
    technical and decision maker review levels have been the norm for high-stakes
    applications of predictive models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 随着技术的进步，使得更多幕后透明性、更好的区分和安全性测试机器学习系统成为可能，现在可以促进人类对机器学习的理解和信任。这些技术将在[第四章](ch04.xhtml#technology_engineering_machine_learning_for_human_trust_and_understanding)中进行讨论，但这些技术仍然需要人们来部署。这些新技术最好的用途之一是对机器学习系统进行人工审计。在一篇[最近的论文](https://oreil.ly/2znTg)中，谷歌的研究人员提出了机器学习模型审计的框架。他们还提出了关于[模型](https://oreil.ly/Fqgug)和[数据](https://oreil.ly/pzVBW)的基本样本文档。这些发展是在多年的金融服务垂直模型治理的基础上进行的，其中治理、有效挑战、模型清单、模型文档、模型验证以及多个技术和决策者审查级别已经成为高风险预测模型应用的规范。
- en: 'What can you and your organization do to promote human audits of ML systems?
    The basics are relatively straightforward:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您和您的组织可以做什么来促进机器学习系统的人工审计？基本的做法相对简单：
- en: Create an inventory of ML systems
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建机器学习系统清单
- en: Nominate accountable executive(s)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定负责的执行官
- en: Instate executive and technical review of documented ML systems
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行机器学习系统的行政和技术审查
- en: Require technical and executive sign off before deploying ML systems
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署机器学习系统之前要求技术和行政签字
- en: Carefully document, validate, and monitor all ML systems
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细记录、验证和监控所有机器学习系统
- en: When you’re ready to move beyond these basic steps, check out the referenced
    papers from Google Research and look into resources from public model risk management
    forums, e.g., [The North American Chief Risk Officer Council](https://oreil.ly/myISS).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当你准备好超越这些基本步骤时，请查看谷歌研究的参考论文，并查看公共模型风险管理论坛的资源，例如[北美首席风险官理事会](https://oreil.ly/myISS)。
- en: Domain Expertise
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领域专业知识
- en: Many were introduced to the human expertise in-the-loop concept by the Pandora
    recommendation algorithm or something similar, which has ultimately evolved into
    a multibillion-dollar industry of expert labeling and decision review of ML systems.
    More generally, real-world success in ML almost always requires some input from
    humans with a deep understanding of the problem domain. Of course, such experts
    can help with feature selection and engineering, and interpretation of ML results.
    But the experts can also serve as a sanity check mechanism. For instance, if you’re
    developing a medical ML system, you should consult physicians and other medical
    professionals. How will generalist data scientists understand the subtlety and
    complexity inherent in medical data and the results of systems trained on such
    data? They might not be able to, which can lead to AI incidents when the system
    is deployed. The social sciences deserve a special callout in this regard as well.
    Described as [“tech’s quiet colonization of the social sciences”](https://oreil.ly/ImONd),
    some organizations are pursuing ill-advised ML projects that either [replace decisions
    that would be made by trained social scientists](https://oreil.ly/8SEp5) or they
    are using practices, such as [facial recognition for criminal risk assessments](https://oreil.ly/KYOb5)
    that have been condemned by actual social scientists.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人通过**Pandora推荐算法**或类似的内容首次接触到人类专家参与的概念，这最终演变成了一个价值数十亿美元的专家标注和机器学习系统决策审查的产业。更一般地说，机器学习在现实世界中的成功几乎总是需要来自深刻理解问题领域的人类的参与。当然，这些专家可以帮助进行特征选择和工程，以及解释机器学习的结果。但专家们还可以作为一种理智检查的机制。例如，如果你正在开发医学机器学习系统，你应该咨询医生和其他医疗专业人员。普通的数据科学家们如何理解医学数据的微妙和复杂性，以及这些数据训练系统的结果？他们可能做不到，这会导致系统部署时的人工智能事件。在这方面，社会科学也值得特别关注。一些组织被描述为[“科技对社会科学的悄然殖民”](https://oreil.ly/ImONd)，它们正在进行一些不明智的机器学习项目，这些项目要么[取代了专业社会科学家会做出的决策](https://oreil.ly/8SEp5)，要么使用了被实际社会科学家谴责的做法，比如[用于犯罪风险评估的面部识别技术](https://oreil.ly/KYOb5)。
- en: User Interactions with Machine Learning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户与机器学习的互动
- en: Since ML is always conducted with software, people at your organization will
    likely be interacting with ML results and outcomes through software. For maximum
    impact, nontechnical and decision-maker users need to understand and act on ML
    system results. Unfortunately, many ML systems and software packages generate
    only numeric outputs or visuals designed by highly technical researchers and practitioners.
    At best, this limits the number of people inside an organization who can work
    with AI and ML technologies. At worst, people can misunderstand poorly designed
    outputs, leading to process failures, customer dissatisfaction, and even AI incidents.
    When constructing ML systems, it is wise to consider the different types of users
    and personas who will need to interact with the system. Your organization should
    probably also have qualified user interaction professionals to help build out
    comprehensible and useful interfaces for these different ML system users.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习始终是通过软件进行的，因此您组织中的人们可能会通过软件与机器学习结果和成果进行交互。为了最大限度地发挥影响，非技术人员和决策者用户需要理解并采取行动机器学习系统的结果。不幸的是，许多机器学习系统和软件包只生成由高度技术的研究人员和从业者设计的数值输出或视觉效果。在最好的情况下，这限制了组织内能够使用人工智能和机器学习技术的人数。在最坏的情况下，人们可能会误解设计不良的输出，导致流程失败、客户不满意，甚至人工智能事件的发生。在构建机器学习系统时，考虑到将需要与系统交互的不同用户和角色是明智的选择。您的组织可能还应该有合格的用户互动专业人员，以帮助为这些不同的机器学习系统用户构建清晰易懂和有用的界面。
- en: User Appeal and Operator Override
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户吸引力和操作者覆盖
- en: What if a computer [unjustly kept you in prison](https://oreil.ly/ZnvS9)? What
    if a computer [erroneously accused you of a crime](https://oreil.ly/7xhyz)? What
    if a computer [kept you or a loved one out of the college of your dreams](https://oreil.ly/lElj1)?
    You’d probably like to know why and you’d probably like the ability to appeal
    such decisions. From the ML system operator’s standpoint, the operator might even
    want to tell you how the decision was made. (In some cases, the operator may be
    legally obligated to provide this information.) Or maybe the operator would like
    to have the capability to override individual ML-system decisions. If the ML system
    in question is based on a black-box algorithm, its operators may not be able to
    tell you how that decision was made, and they might not be able to check or override
    the decision promptly. Given that “all models are wrong,” at least at some point,
    all this seems like a recipe for disaster.^([1](ch02.xhtml#idm46137004313672))
    In the worst-case scenario, a black-box ML system will (inevitably) issue a wrong
    prediction and maybe many of them at high speeds. These wrong decisions will hurt
    consumers or the general public, and the ML system operator will be subject to
    reputational, if not regulatory, damages.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一台计算机[错误地让你呆在监狱里](https://oreil.ly/ZnvS9)怎么办？如果一台计算机[错误地指控你犯罪](https://oreil.ly/7xhyz)怎么办？如果一台计算机[让你或亲人错失了梦想中的大学](https://oreil.ly/lElj1)怎么办？您可能希望知道为什么，您可能希望有权利申诉这些决策。从机器学习系统操作者的角度来看，操作者甚至可能希望告诉您决策的制定过程。（在某些情况下，操作者可能有法律义务提供这些信息。）或者操作者可能希望拥有覆盖个别机器学习系统决策的能力。如果所涉及的机器学习系统基于黑箱算法，其操作者可能无法告诉您该决策是如何做出的，他们可能无法及时检查或覆盖这些决策。考虑到“所有模型都是错误的”，至少在某个时刻，所有这些似乎都是灾难的根源。^([1](ch02.xhtml#idm46137004313672))在最坏的情况下，黑箱机器学习系统将（不可避免地）发布错误预测，也许速度很快会有很多错误决策。这些错误决策会伤害消费者或公众，而机器学习系统操作者将面临声誉上，如果不是法规上的损害。
- en: 'This topic, also known as [“intervenability”](https://oreil.ly/VjIS7) in data
    privacy circles, is already fairly well understood. So, there are steps you can
    take to prevent your organization’s ML systems from making unappealable, and potentially
    illegal, black-box decisions:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主题在数据隐私圈中也被称为[“干预性”](https://oreil.ly/VjIS7)，目前已经被相当了解。因此，您可以采取措施防止您组织的机器学习系统做出不可申诉，可能违法的黑箱决策：
- en: Use of interpretable ML models or reliable post-hoc explanation techniques (preferably
    both)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可解释的机器学习模型或可靠的事后解释技术（最好两者兼有）
- en: Proper documentation of the processes used in these systems
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确记录这些系统中使用的过程
- en: Meticulous testing of ML system interpretability features before deployment
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在部署前，对机器学习系统解释能力功能进行细致测试
- en: The underpinning issue for appeal, override, and intervenability is transparency.
    Hence, your organization should understand how ML decisions are made, enabling
    operators to override—and consumers to appeal—ML system decisions logically. Ideally,
    ML systems should enable overall transparency for consumers and the public, especially
    those impacted by the ML system. This can even involve users probing these systems,
    extracting the reasoning behind individual decisions, and negating the decisions
    when necessary. These types of appeal and override mechanisms can also stop unavoidable
    ML system errors from becoming full-blown AI incidents.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 申诉、覆盖和干预能力的基础问题是透明度。因此，您的组织应了解机器学习决策的制定过程，使操作者能够逻辑地覆盖——消费者能够申诉——机器学习系统的决策。理想情况下，机器学习系统应该为消费者和公众，特别是受到机器学习系统影响的人群，提供整体透明度。这甚至可以涉及用户探测这些系统，提取个体决策背后的推理，并在必要时否定决策。这些类型的申诉和覆盖机制还可以阻止不可避免的机器学习系统错误演变为严重的人工智能事件。
- en: Kill Switches
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 断开开关
- en: The title of a recent Forbes article asks, [“Will There Be a Kill Switch For
    AI?”](https://oreil.ly/858_v). If your organization wants to mitigate risks around
    ML and AI, we hope the answer for your ML systems will be, “yes.” ML systems can
    make decisions very quickly—orders of magnitudes faster than humans. So, if your
    ML system goes seriously wrong, you will want to be able to turn it off fast.
    But how will you even know if your ML system is misbehaving? ML systems should
    be monitored for multiple kinds of problems, including inaccuracy, instability,
    discrimination, leakage of private data, and security vulnerabilities.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇最近的《福布斯》文章标题问道，[“人工智能会有关机开关吗？”](https://oreil.ly/858_v) 如果你的组织希望减少机器学习和人工智能的风险，我们希望你的机器学习系统的答案是，“是的”。机器学习系统可以非常快速地做出决策——比人类快得多。因此，如果你的机器学习系统出现严重问题，你将希望能够快速关闭它。但是你又如何知道你的机器学习系统是否在出现问题？机器学习系统应该监控多种问题，包括不准确性、不稳定性、歧视、私人数据泄露和安全漏洞。
- en: Once you’ve detected a severe problem, the question then becomes, can you turn
    off the ML system? ML system outputs often feed into downstream business processes,
    sometimes including other ML systems. These systems and business processes can
    be mission critical, as in the case of an ML system used for credit underwriting
    or e-retail product recommendations. To turn off an ML system, you’ll not only
    need the right technical know-how and personnel available, but you also need an
    understanding of the model’s place inside of broader organizational processes.
    During an ongoing AI incident is not a great time to start thinking about turning
    off a fatally flawed ML system. So, kill processes and kill switches are a great
    addition to your ML system documentation and AI incident response plans (see [Chapter 3](ch03.xhtml#processes_taming_the_wild_west_of_machine_learning_workflows)).
    This way, when the time comes to kill an ML system, your organization can be ready
    to make an informed decision.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦发现了严重问题，问题就变成了，你能关闭机器学习系统吗？机器学习系统的输出通常会进入到下游的业务流程中，有时还包括其他机器学习系统。这些系统和业务流程可能具有使命关键性，例如用于信用核准或电子零售产品推荐的机器学习系统。要关闭一个机器学习系统，你不仅需要正确的技术知识和人员，还需要了解该模型在更广泛组织流程中的位置。在进行中的人工智能事故中，不是开始考虑关闭一个致命缺陷的机器学习系统的好时机。因此，杀死进程和杀死开关是你的机器学习系统文档和人工智能事故响应计划的重要补充（参见[第三章](ch03.xhtml#processes_taming_the_wild_west_of_machine_learning_workflows)）。这样，当关闭一个机器学习系统的时机到来时，你的组织可以准备好做出明智的决定。
- en: 'Going Nuclear: Public Protests, Data Journalism, and White-Hat Hacking'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 走向核心：公众抗议、数据新闻和白帽黑客攻击
- en: Sometimes working within the confines of organizational culture or getting yourself
    into the loop of an ML system isn’t enough. Sometimes organizations can be so
    irresponsible with technology that employees, journalists, researchers, or others
    feel the need to take drastic action. The remainder of this chapter discusses
    some recent and relevant examples of walkouts, protests, investigative data journalism,
    and even white-hat hacking.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，仅仅在组织文化的限制内工作或融入到机器学习系统的循环中是不够的。有时，组织在技术上的不负责任会导致员工、记者、研究人员或其他人感到有必要采取极端行动。本章剩余部分将讨论一些最近和相关的示例，包括罢工、抗议、调查数据新闻和白帽黑客攻击。
- en: In recent years, employees at technology giants have staged protests to voice
    their dissatisfaction with company policies regarding [misinformation](https://oreil.ly/3aCWH),
    [climate change](https://oreil.ly/PusPE), [sexual harassment](https://oreil.ly/aRPdB),
    and other critical issues. As highly-skilled employees are perhaps the most valuable
    assets to most technology companies, companies do seem to pay attention to these
    protest activities, both by responding to protester demands and with retribution
    against protesters. Another exciting type of ML oversight has surfaced in recent
    years; it can be described best as a mixture of extreme data journalism and white-hat
    hacking. The catalyst for these actions appears to be the 2016 ProPublica analysis
    of the criminal risk assessment instrument known as COMPAS. In what was essentially
    a model extraction attack, journalists at ProPublica made a rough copy of COMPAS’s
    proprietary training data and black-box logic and used this analysis to make serious
    claims about [discrimination in algorithmic criminal risk assessments](https://oreil.ly/C99fu).
    Although the analysis results are scientifically controversial, the work brought
    widespread attention to the problem of algorithmic discrimination, and the company
    that licensed COMPAS later changed its name.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，技术巨头的员工举行抗议活动，表达对公司关于[虚假信息](https://oreil.ly/3aCWH)，[气候变化](https://oreil.ly/PusPE)，[性骚扰](https://oreil.ly/aRPdB)及其他重大问题政策的不满。作为大多数科技公司最宝贵的资产，高技能员工的抗议活动似乎确实引起了公司的重视，无论是通过回应抗议者的要求还是对抗议者采取报复行动。近年来，出现了一种令人兴奋的新型机器学习监督方式；最能描述它的可能是极端数据新闻和白帽黑客的混合体。这些行动的催化剂似乎是2016年ProPublica对称为COMPAS的犯罪风险评估工具进行分析。在实质上是一场模型提取攻击中，ProPublica的记者制作了一个粗略副本的COMPAS专有训练数据和黑盒逻辑，并利用这一分析对[算法犯罪风险评估中的歧视问题](https://oreil.ly/C99fu)提出了严重指控。尽管分析结果在科学上存在争议，但这项工作引起了广泛关注，使算法歧视问题备受关注，而许可了COMPAS的公司后来更名。
- en: In another example of external oversight of commercial technology, researchers
    at MIT, operating under the project name [Gender Shades](https://oreil.ly/eEVkJ),
    tested several commercial facial recognition tools for racial and gender discrimination.
    The results of the study were made public in 2018, and they were damning. Some
    of the day’s leading facial recognition technologies performed well on white males
    and very poorly on female persons of color. Once the Gender Shades results were
    made public, companies were forced to either correct their apparently discriminatory
    systems or defend them. While most companies chose to address the issue quickly,
    [Amazon chose to defend its Rekognition system](https://oreil.ly/jWW8l) and continued
    to license it to law enforcement. In the face of growing public outcry, [Amazon](https://oreil.ly/OEsIu)
    and [IBM](https://oreil.ly/aUPXl)—both cited in Gender Shades research—canceled
    their surveillance facial recognition programs in the summer of 2020\. Such public
    attempts to expose troubling uses of ML continue and are perhaps happening even
    more frequently. In January of 2020, Walmart employees tipped off journalists
    about an [antitheft ML system](https://oreil.ly/h-emE) they believe is error prone
    and unnecessarily increasing contact between customers and associates during the
    COVID-19 pandemic. In February of 2020, The Markup, a nonprofit news organization
    devoted to oversight of Big Tech, alleged in an analysis somewhat akin to the
    original COMPAS exposé, that [Allstate Insurance](https://oreil.ly/IJzRT) uses
    algorithms to charge its highest paying customers higher rates, essentially creating
    a “suckers list.”
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业技术的外部监督的另一个例子中，麻省理工学院的研究人员在[性别偏差](https://oreil.ly/eEVkJ)项目下进行了多个商业面部识别工具的种族和性别歧视测试。该研究结果于2018年公开，并且非常严重。当天的一些领先面部识别技术在白人男性上表现良好，但在有色人种女性上表现非常糟糕。一旦性别偏差的结果公开，公司们不得不要么纠正他们明显的歧视系统，要么为其辩护。尽管大多数公司选择迅速解决问题，[亚马逊选择为其Rekognition系统辩护](https://oreil.ly/jWW8l)，并继续向执法部门授权使用。面对不断增长的公众抗议声，[亚马逊](https://oreil.ly/OEsIu)和[IBM](https://oreil.ly/aUPXl)——在性别偏差研究中均有提及——在2020年夏季取消了他们的监控面部识别计划。这种揭露机器学习问题的公开尝试继续进行，且可能越来越频繁。2020年1月，沃尔玛的员工向记者爆料称，他们认为[防盗机器学习系统](https://oreil.ly/h-emE)存在错误，并在COVID-19大流行期间不必要地增加了顾客和员工的接触频率。2020年2月，专注于监督大科技公司的非营利新闻组织The
    Markup在一篇类似于原始COMPAS揭露的分析中，声称[全球保险](https://oreil.ly/IJzRT)利用算法向其最高付费客户收取更高的费率，实质上创建了一个“受骗者清单”。
- en: Fortunately, as evidenced by the fallout from the original COMPAS journalism
    and Gender Shades project, the rising awareness of ML risks in governments and
    the public seem to have at least some effect on organizational uses of ML. Organizations
    are seeing that at least sometimes, these types of incidents can be damaging to
    brand reputation, if not to the bottom line. As we transition our discussion to
    processes in [Chapter 3](ch03.xhtml#processes_taming_the_wild_west_of_machine_learning_workflows)
    and technology in [Chapter 4](ch04.xhtml#technology_engineering_machine_learning_for_human_trust_and_understanding),
    remember that as of today, people are still the key ingredient in nearly any deployment
    of ML or AI technologies. Whether it’s through fostering a culture of responsibility,
    getting more involved in the inner workings of ML systems, or taking more drastic
    actions, you really can make a difference in how ML systems impact our world.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，正如从原始COMPAS新闻报道和性别阴影项目的影响可以看出的那样，公众和政府对机器学习风险的日益关注似乎至少在某种程度上影响了组织对ML的使用。组织们逐渐意识到，至少在某些情况下，这些类型的事件可能会损害品牌声誉，即使不会影响到财务状况。当我们将讨论重点转移到[第三章](ch03.xhtml#processes_taming_the_wild_west_of_machine_learning_workflows)中的流程和[第四章](ch04.xhtml#technology_engineering_machine_learning_for_human_trust_and_understanding)中的技术时，请记住，截至今日，人仍然是几乎任何ML或AI技术部署中的关键因素。无论是通过培养责任文化，更深入地参与ML系统的内部运作，还是采取更激进的行动，你确实可以改变ML系统对我们世界的影响。
- en: ^([1](ch02.xhtml#idm46137004313672-marker)) The famous statistician George Box
    is credited with saying, “[all models are wrong, but some are useful](https://oreil.ly/0cJgu)”.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.xhtml#idm46137004313672-marker)) 著名统计学家乔治·博克斯被认为说过：“[所有模型都是错的，但有些是有用的](https://oreil.ly/0cJgu)”。
