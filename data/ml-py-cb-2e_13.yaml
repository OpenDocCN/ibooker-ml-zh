- en: Chapter 13\. Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章 线性回归
- en: 13.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13.0 引言
- en: '*Linear regression* is one of the simplest supervised learning algorithms in
    our toolkit. If you have ever taken an introductory statistics course in college,
    likely the final topic you covered was linear regression. Linear regression and
    its extensions continue to be a common and useful method of making predictions
    when the target vector is a quantitative value (e.g., home price, age). In this
    chapter we will cover a variety of linear regression methods (and some extensions)
    for creating well-performing prediction models.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性回归*是我们工具箱中最简单的监督学习算法之一。如果您曾经在大学里修过入门统计课程，很可能您最后学到的主题就是线性回归。线性回归及其扩展在当目标向量是定量值（例如房价、年龄）时继续是一种常见且有用的预测方法。在本章中，我们将涵盖多种线性回归方法（及其扩展）来创建性能良好的预测模型。'
- en: 13.1 Fitting a Line
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13.1 拟合一条线
- en: Problem
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to train a model that represents a linear relationship between the
    feature and target vector.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望训练一个能够表示特征和目标向量之间线性关系的模型。
- en: Solution
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use a linear regression (in scikit-learn, `LinearRegression`):'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性回归（在scikit-learn中，`LinearRegression`）：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Discussion
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Linear regression assumes that the relationship between the features and the
    target vector is approximately linear. That is, the *effect* (also called *coefficient*,
    *weight*, or *parameter*) of the features on the target vector is constant. In
    our solution, for the sake of explanation, we have trained our model using only
    three features. This means our linear model will be:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归假设特征与目标向量之间的关系大致是线性的。也就是说，特征对目标向量的*效果*（也称为*系数*、*权重*或*参数*）是恒定的。为了解释起见，在我们的解决方案中，我们只使用了三个特征来训练我们的模型。这意味着我们的线性模型将是：
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo>
    <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>3</mn></msub> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo>
    <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>3</mn></msub> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
- en: 'where <math display="inline"><mover accent="true"><mi>y</mi> <mo>^</mo></mover></math>
    is our target, <math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    is the data for a single feature, <math display="inline"><msub><mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mn>1</mn></msub></math> , <math display="inline"><msub><mover
    accent="true"><mi>β</mi><mo>^</mo></mover> <mn>2</mn></msub></math> , and <math
    display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover> <mn>3</mn></msub></math>
    are the coefficients identified by fitting the model, and <math display="inline"><mi>ϵ</mi></math>
    is the error. After we have fit our model, we can view the value of each parameter.
    For example, <math display="inline"><msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover>
    <mn>0</mn></msub></math>, also called the *bias* or *intercept*, can be viewed
    using `intercept_`:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，<math display="inline"><mover accent="true"><mi>y</mi> <mo>^</mo></mover></math>
    是我们的目标，<math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> 是单个特征的数据，<math
    display="inline"><msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub></math>，<math
    display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover> <mn>2</mn></msub></math>和<math
    display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover> <mn>3</mn></msub></math>是通过拟合模型确定的系数，<math
    display="inline"><mi>ϵ</mi></math>是误差。在拟合模型后，我们可以查看每个参数的值。例如，<math display="inline"><msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub></math>，也称为*偏差*或*截距*，可以使用`intercept_`查看：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mn>1</mn></msub></math> and <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mn>2</mn></msub></math> are shown using `coef_`:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 而`coef_`显示了<math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mn>1</mn></msub></math>和<math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mn>2</mn></msub></math>：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In our dataset, the target value is a randomly generated continuous variable:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，目标值是一个随机生成的连续变量：
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Using the `predict` method, we can predict the output based on the input features:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`predict`方法，我们可以根据输入特征预测输出：
- en: '[PRE7]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Not bad! Our model was off only by about 0.01!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！我们的模型只偏离了约0.01！
- en: The major advantage of linear regression is its interpretability, in large part
    because the coefficients of the model are the effect of a one-unit change on the
    target vector. Our model’s coefficient of the first feature was ~–0.02, meaning
    that we have the change in target for each additional unit change in the first
    feature.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的主要优势在于其可解释性，这在很大程度上是因为模型的系数是目标向量一单位变化的影响。我们模型的第一个特征的系数约为~–0.02，这意味着我们在第一个特征每增加一个单位时目标的变化。
- en: 'Using the `score` function, we can also see how well our model performed on
    the data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`score`函数，我们还可以看到我们的模型在数据上的表现：
- en: '[PRE9]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The default score for linear regression in scikit learn is R², which ranges
    from 0.0 (worst) to 1.0 (best). As we can see in this example, we are very close
    to the perfect value of 1.0\. However it’s worth noting that we are evaluating
    this model on data it has already seen (the training data), where typically we’d
    evaluate on a held-out test set of data instead. Nonetheless, such a high score
    would bode well for our model in a real setting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: scikit learn 中线性回归的默认得分是 R²，范围从 0.0（最差）到 1.0（最好）。正如我们在这个例子中所看到的，我们非常接近完美值 1.0。然而值得注意的是，我们是在模型已经见过的数据（训练数据）上评估该模型，而通常我们会在一个独立的测试数据集上进行评估。尽管如此，在实际情况下，这样高的分数对我们的模型是个好兆头。
- en: 13.2 Handling Interactive Effects
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13.2 处理交互效应
- en: Problem
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a feature whose effect on the target variable depends on another feature.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个特征，其对目标变量的影响取决于另一个特征。
- en: Solution
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Create an interaction term to capture that dependence using scikit-learn’s
    `PolynomialFeatures`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个交互项来捕获这种依赖关系，使用 scikit-learn 的 `PolynomialFeatures`：
- en: '[PRE11]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Discussion
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Sometimes a feature’s effect on our target variable is at least partially dependent
    on another feature. For example, imagine a simple coffee-based example where we
    have two binary features—​the presence of sugar (`sugar`) and whether or not we
    have stirred (`stirred`)—and we want to predict if the coffee tastes sweet. Just
    putting sugar in the coffee (`sugar=1, stirred=0`) won’t make the coffee taste
    sweet (all the sugar is at the bottom!) and just stirring the coffee without adding
    sugar (`sugar=0, stirred=1`) won’t make it sweet either. Instead it is the interaction
    of putting sugar in the coffee *and* stirring the coffee (`sugar=1, stirred=1`)
    that will make a coffee taste sweet. The effects of `sugar` and `stirred` on sweetness
    are dependent on each other. In this case we say there is an *interaction effect*
    between the features `sugar` and `stirred`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，一个特征对目标变量的影响至少部分依赖于另一个特征。例如，想象一个简单的基于咖啡的例子，我们有两个二进制特征——是否加糖（`sugar`）和是否搅拌（`stirred`）——我们想预测咖啡是否甜。仅仅加糖（`sugar=1,
    stirred=0`）不会使咖啡变甜（所有的糖都在底部！），仅仅搅拌咖啡而不加糖（`sugar=0, stirred=1`）也不会使其变甜。实际上，是将糖放入咖啡并搅拌（`sugar=1,
    stirred=1`）才能使咖啡变甜。`sugar` 和 `stirred` 对甜味的影响是相互依赖的。在这种情况下，我们称` sugar` 和 `stirred`
    之间存在*交互效应*。
- en: 'We can account for interaction effects by including a new feature comprising
    the product of corresponding values from the interacting features:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过包含一个新特征来考虑交互效应，该特征由交互特征的相应值的乘积组成：
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo>
    <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>3</mn></msub> <msub><mi>x</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo>
    <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>3</mn></msub> <msub><mi>x</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
- en: where <math display="inline"><msub><mi>x</mi><mn>1</mn></msub></math> and <math
    display="inline"><msub><mi>x</mi><mn>2</mn></msub></math> are the values of the
    `sugar` and `stirred`, respectively, and <math display="inline"><msub><mi>x</mi><mn>1</mn></msub><msub><mi>x</mi><mn>2</mn></msub></math>
    represents the interaction between the two.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><msub><mi>x</mi><mn>1</mn></msub></math> 和 <math display="inline"><msub><mi>x</mi><mn>2</mn></msub></math>
    分别是 `sugar` 和 `stirred` 的值，<math display="inline"><msub><mi>x</mi><mn>1</mn></msub><msub><mi>x</mi><mn>2</mn></msub></math>
    表示两者之间的交互作用。
- en: 'In our solution, we used a dataset containing only two features. Here is the
    first observation’s values for each of those features:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，我们使用了一个只包含两个特征的数据集。以下是每个特征的第一个观察值：
- en: '[PRE12]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To create an interaction term, we simply multiply those two values together
    for every observation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个交互项，我们只需为每个观察值将这两个值相乘：
- en: '[PRE14]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then view the interaction term for the first observation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到第一次观察的交互项：
- en: '[PRE15]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: However, while often we will have a substantive reason for believing there is
    an interaction between two features, sometimes we will not. In those cases it
    can be useful to use scikit-learn’s `PolynomialFeatures` to create interaction
    terms for all combinations of features. We can then use model selection strategies
    to identify the combination of features and interaction terms that produces the
    best model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虽然我们经常有充分的理由相信两个特征之间存在交互作用，但有时我们也没有。在这些情况下，使用 scikit-learn 的 `PolynomialFeatures`
    为所有特征组合创建交互项会很有用。然后，我们可以使用模型选择策略来识别产生最佳模型的特征组合和交互项。
- en: 'To create interaction terms using `PolynomialFeatures`, there are three important
    parameters we must set. Most important, `interaction_only=True` tells `PolynomialFeatures`
    to return only interaction terms (and not polynomial features, which we will discuss
    in [Recipe 13.3](#fitting-a-non-linear-relationship)). By default, `PolynomialFeatures`
    will add a feature containing 1s called a *bias*. We can prevent that with `include_bias=False`.
    Finally, the `degree` parameter determines the maximum number of features to create
    interaction terms from (in case we wanted to create an interaction term that is
    the combination of three features). We can see the output of `PolynomialFeatures`
    from our solution by checking to see if the first observation’s feature values
    and interaction term value match our manually calculated version:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`PolynomialFeatures`创建交互项，我们需要设置三个重要的参数。最重要的是，`interaction_only=True`告诉`PolynomialFeatures`仅返回交互项（而不是多项式特征，我们将在[Recipe
    13.3](#fitting-a-non-linear-relationship)中讨论）。默认情况下，`PolynomialFeatures`会添加一个名为*bias*的包含1的特征。我们可以通过`include_bias=False`来防止这种情况发生。最后，`degree`参数确定从中创建交互项的特征的最大数量（以防我们想要创建的交互项是三个特征的组合）。我们可以通过检查我们的解决方案中`PolynomialFeatures`的输出，看看第一个观察值的特征值和交互项值是否与我们手动计算的版本匹配：
- en: '[PRE17]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 13.3 Fitting a Nonlinear Relationship
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13.3 拟合非线性关系
- en: Problem
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to model a nonlinear relationship.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望对非线性关系进行建模。
- en: Solution
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Create a polynomial regression by including polynomial features in a linear
    regression model:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在线性回归模型中包含多项式特征来创建多项式回归：
- en: '[PRE19]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Discussion
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: So far we have discussed modeling only linear relationships. An example of a
    linear relationship would be the number of stories a building has and the building’s
    height. In linear regression, we assume the effect of number of stories and building
    height is approximately constant, meaning a 20-story building will be roughly
    twice as high as a 10-story building, which will be roughly twice as high as a
    5-story building. Many relationships of interest, however, are not strictly linear.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了建模线性关系。线性关系的一个例子是建筑物的层数与建筑物的高度之间的关系。在线性回归中，我们假设层数和建筑物高度的影响大致是恒定的，这意味着一个20层的建筑物大致会比一个10层的建筑物高出两倍，而一个5层的建筑物大致会比一个10层的建筑物高出两倍。然而，许多感兴趣的关系并不严格是线性的。
- en: Often we want to model a nonlinear relationship—for example, the relationship
    between the number of hours a student studies and the score she gets on a test.
    Intuitively, we can imagine there is a big difference in test scores between students
    who study for one hour compared to students who did not study at all. However,
    there is a much smaller difference in test scores between a student who studied
    for 99 hours and a student who studied for 100 hours. The effect that one hour
    of studying has on a student’s test score decreases as the number of hours increases.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常希望建模非线性关系，例如学生学习时间与她在考试中得分之间的关系。直觉上，我们可以想象，对于一个小时的学习和没有学习的学生之间的考试成绩差异很大。然而，在学习时间增加到99小时和100小时之间时，学生的考试成绩差异就会变得很小。随着学习小时数的增加，一个小时的学习对学生考试成绩的影响逐渐减小。
- en: 'Polynomial regression is an extension of linear regression that allows us to
    model nonlinear relationships. To create a polynomial regression, convert the
    linear function we used in [Recipe 13.1](#fitting-a-line):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归是线性回归的扩展，允许我们建模非线性关系。要创建多项式回归，将我们在[Recipe 13.1](#fitting-a-line)中使用的线性函数转换为多项式函数：
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mi>ϵ</mi></mrow></math>
- en: 'into a polynomial function by adding polynomial features:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加多项式特征将线性回归模型扩展为多项式函数：
- en: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup> <mo>+</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>+</mo> <msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>d</mi></msub> <msup><msub><mi>x</mi>
    <mn>1</mn></msub> <mi>d</mi></msup> <mo>+</mo> <mi>ϵ</mi></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>β</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup> <mo>+</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>+</mo> <msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>d</mi></msub> <msup><msub><mi>x</mi>
    <mn>1</mn></msub> <mi>d</mi></msup> <mo>+</mo> <mi>ϵ</mi></mrow></math>
- en: where <math display="inline"><mi>d</mi></math> is the degree of the polynomial.
    How are we able to use a linear regression for a nonlinear function? The answer
    is that we do not change how the linear regression fits the model but rather only
    add polynomial features. That is, the linear regression does not “know” that the
    <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math> is a quadratic
    transformation of <math display="inline"><mi>x</mi></math>. It just considers
    it one more variable.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><mi>d</mi></math>是多项式的次数。我们如何能够对非线性函数使用线性回归？答案是我们不改变线性回归拟合模型的方式，而只是添加多项式特征。也就是说，线性回归并不“知道”<math
    display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>是<math display="inline"><mi>x</mi></math>的二次转换，它只是将其视为另一个变量。
- en: 'A more practical description might be in order. To model nonlinear relationships,
    we can create new features that raise an existing feature, <math display="inline"><mi>x</mi></math>,
    up to some power: <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>,
    <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>, and so on. The
    more of these new features we add, the more flexible the “line” created by our
    model. To make this more explicit, imagine we want to create a polynomial to the
    third degree. For the sake of simplicity, we will focus on only one observation
    (the first observation in the dataset), <math display="inline"><mi>x</mi></math>[0]:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要更实际的描述。为了建模非线性关系，我们可以创建将现有特征 <math display="inline"><mi>x</mi></math> 提升到某个幂次的新特征：
    <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>、<math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>
    等。我们添加的这些新特征越多，模型创建的“线”就越灵活。为了更加明确，想象我们想要创建一个三次多项式。为了简单起见，我们将专注于数据集中的第一个观察值：
- en: '[PRE20]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To create a polynomial feature, we would raise the first observation’s value
    to the second degree, <math display="inline"><msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup></math> :'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个多项式特征，我们将第一个观察值的值提升到二次方，<math display="inline"><msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>2</mn></msup></math>：
- en: '[PRE22]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This would be our new feature. We would then also raise the first observation’s
    value to the third degree, <math display="inline"><msup><msub><mi>x</mi> <mn>1</mn></msub>
    <mn>3</mn></msup></math> :'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是我们的新功能。然后，我们还将第一个观察值的值提升到三次方，<math display="inline"><msup><msub><mi>x</mi>
    <mn>1</mn></msub> <mn>3</mn></msup></math>：
- en: '[PRE24]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'By including all three features (<math display="inline"><mi>x</mi></math>,
    <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>, and <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>)
    in our feature matrix and then running a linear regression, we have conducted
    a polynomial regression:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在我们的特征矩阵中包含所有三个特征（<math display="inline"><mi>x</mi></math>、<math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>
    和 <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>）并运行线性回归，我们进行了多项式回归：
- en: '[PRE26]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`PolynomialFeatures` has two important parameters. First, `degree` determines
    the maximum number of degrees for the polynomial features. For example, `degree=3`
    will generate <math display="inline"><msup><mi>x</mi><mn>2</mn></msup></math>
    and <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>. Second, by
    default `PolynomialFeatures` includes a feature containing only 1s (called a bias).
    We can remove that by setting `include_bias=False`.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`PolynomialFeatures` 有两个重要参数。首先，`degree` 确定多项式特征的最大次数。例如，`degree=3` 会生成 <math
    display="inline"><msup><mi>x</mi><mn>2</mn></msup></math> 和 <math display="inline"><msup><mi>x</mi><mn>3</mn></msup></math>。其次，默认情况下
    `PolynomialFeatures` 包括一个只包含1的特征（称为偏差）。我们可以通过设置 `include_bias=False` 来删除它。'
- en: 13.4 Reducing Variance with Regularization
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13.4 通过正则化减少方差
- en: Problem
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to reduce the variance of your linear regression model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望减少线性回归模型的方差。
- en: Solution
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use a learning algorithm that includes a *shrinkage penalty* (also called *regularization*)
    like ridge regression and lasso regression:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用包含*收缩惩罚*（也称为*正则化*）的学习算法，例如岭回归和拉索回归：
- en: '[PRE28]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Discussion
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'In standard linear regression the model trains to minimize the sum of squared
    error between the true (<math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>)
    and prediction (<math display="inline"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover>
    <mi>i</mi></msub></math> ) target values, or residual sum of squares (RSS):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准线性回归中，模型训练以最小化真实值（<math display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>）与预测值（<math
    display="inline"><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover> <mi>i</mi></msub></math>）目标值或残差平方和（RSS）之间的平方误差：
- en: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi>R</mi>
    <mi>S</mi> <mi>S</mi> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></mstyle></math>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mstyle displaystyle="true" scriptlevel="0"><mrow><mi>R</mi>
    <mi>S</mi> <mi>S</mi> <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></mstyle></math>
- en: 'Regularized regression learners are similar, except they attempt to minimize
    RSS *and* some penalty for the total size of the coefficient values, called a
    *shrinkage penalty* because it attempts to “shrink” the model. There are two common
    types of regularized learners for linear regression: ridge regression and the
    lasso. The only formal difference is the type of shrinkage penalty used. In *ridge
    regression*, the shrinkage penalty is a tuning hyperparameter multiplied by the
    squared sum of all coefficients:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化回归学习者类似，除了它们试图最小化RSS *和* 系数值总大小的某种惩罚，称为*收缩惩罚*，因为它试图“收缩”模型。线性回归的两种常见类型的正则化学习者是岭回归和拉索。唯一的形式上的区别是使用的收缩惩罚类型。在*岭回归*中，收缩惩罚是一个调整超参数，乘以所有系数的平方和：
- en: <math display="block"><mrow><mtext>RSS</mtext> <mo>+</mo> <mi>α</mi> <munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi></munderover> <msup><msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi></msub> <mn>2</mn></msup></mrow></math>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>RSS</mtext> <mo>+</mo> <mi>α</mi> <munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>p</mi></munderover> <msup><msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi></msub> <mn>2</mn></msup></mrow></math>
- en: 'where <math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mi>j</mi></msub></math> is the coefficient of the <math display="inline"><mi>j</mi></math>th
    of <math display="inline"><mi>p</mi></math> features and <math display="inline"><mi>α</mi></math>
    is a hyperparameter (discussed next). The *lasso* is similar, except the shrinkage
    penalty is a tuning hyperparameter multiplied by the sum of the absolute value
    of all coefficients:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><msub><mover accent="true"><mi>β</mi><mo>^</mo></mover>
    <mi>j</mi></msub></math>是第<math display="inline"><mi>j</mi></math>个<math display="inline"><mi>p</mi></math>特征的系数，<math
    display="inline"><mi>α</mi></math>是一个超参数（接下来会讨论）。*Lasso*则类似，只是收缩惩罚是一个调整的超参数，乘以所有系数的绝对值的和：
- en: <math display="block"><mrow><mfrac><mn>1</mn> <mrow><mn>2</mn><mi>n</mi></mrow></mfrac>
    <mtext>RSS</mtext> <mo>+</mo> <mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi></msub></mfenced></mrow></math>
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfrac><mn>1</mn> <mrow><mn>2</mn><mi>n</mi></mrow></mfrac>
    <mtext>RSS</mtext> <mo>+</mo> <mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>p</mi></munderover> <mfenced close="|" open="|" separators=""><msub><mover
    accent="true"><mi>β</mi> <mo>^</mo></mover> <mi>j</mi></msub></mfenced></mrow></math>
- en: where <math display="inline"><mi>n</mi></math> is the number of observations.
    So which one should we use? As a very general rule of thumb, ridge regression
    often produces slightly better predictions than lasso, but lasso (for reasons
    we will discuss in [Recipe 13.5](#reducing-features-with-lasso-regression)) produces
    more interpretable models. If we want a balance between ridge and lasso’s penalty
    functions we can use *elastic net*, which is simply a regression model with both
    penalties included. Regardless of which one we use, both ridge and lasso regressions
    can penalize large or complex models by including coefficient values in the loss
    function we are trying to minimize.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><mi>n</mi></math>是观察数。那么我们应该使用哪一个？作为一个非常一般的经验法则，岭回归通常比lasso产生稍微更好的预测，但lasso（我们将在[Recipe
    13.5](#reducing-features-with-lasso-regression)中讨论原因）产生更可解释的模型。如果我们希望在岭回归和lasso的惩罚函数之间取得平衡，我们可以使用*弹性网*，它只是一个包含两种惩罚的回归模型。无论我们使用哪一个，岭回归和lasso回归都可以通过将系数值包括在我们试图最小化的损失函数中来对大或复杂的模型进行惩罚。
- en: The hyperparameter, <math display="inline"><mi>α</mi></math>, lets us control
    how much we penalize the coefficients, with higher values of <math display="inline"><mi>α</mi></math>
    creating simpler models. The ideal value of <math display="inline"><mi>α</mi></math>
    should be tuned like any other hyperparameter. In scikit-learn, <math display="inline"><mi>α</mi></math>
    is set using the `alpha` parameter.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数<math display="inline"><mi>α</mi></math>让我们控制对系数的惩罚程度，较高的<math display="inline"><mi>α</mi></math>值会创建更简单的模型。理想的<math
    display="inline"><mi>α</mi></math>值应像其他超参数一样进行调整。在scikit-learn中，可以使用`alpha`参数设置<math
    display="inline"><mi>α</mi></math>。
- en: 'scikit-learn includes a `RidgeCV` method that allows us to select the ideal
    value for <math display="inline"><mi>α</mi></math>:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn包含一个`RidgeCV`方法，允许我们选择理想的<math display="inline"><mi>α</mi></math>值：
- en: '[PRE29]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can then easily view the best model’s <math display="inline"><mi>α</mi></math>
    value:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松查看最佳模型的<math display="inline"><mi>α</mi></math>值：
- en: '[PRE31]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'One final note: because in linear regression the value of the coefficients
    is partially determined by the scale of the feature, and in regularized models
    all coefficients are summed together, we must make sure to standardize the feature
    prior to training.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点：因为在线性回归中系数的值部分由特征的尺度确定，在正则化模型中所有系数都被合并在一起，因此在训练之前必须确保对特征进行标准化。
- en: 13.5 Reducing Features with Lasso Regression
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13.5 使用Lasso回归减少特征
- en: Problem
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to simplify your linear regression model by reducing the number of
    features.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望通过减少特征来简化您的线性回归模型。
- en: Solution
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use a lasso regression:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用lasso回归：
- en: '[PRE33]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Discussion
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'One interesting characteristic of lasso regression’s penalty is that it can
    shrink the coefficients of a model to zero, effectively reducing the number of
    features in the model. For example, in our solution we set `alpha` to `0.5`, and
    we can see that many of the coefficients are 0, meaning their corresponding features
    are not used in the model:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: lasso回归惩罚的一个有趣特征是它可以将模型的系数收缩到零，有效减少模型中的特征数。例如，在我们的解决方案中，我们将`alpha`设置为`0.5`，我们可以看到许多系数为0，意味着它们对应的特征未在模型中使用：
- en: '[PRE34]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'However, if we increase α to a much higher value, we see that literally none
    of the features are being used:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们将<math display="inline"><mi>α</mi></math>增加到一个更高的值，我们会看到几乎没有特征被使用：
- en: '[PRE36]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The practical benefit of this effect is that it means we could include 100 features
    in our feature matrix and then, through adjusting lasso’s α hyperparameter, produce
    a model that uses only 10 (for instance) of the most important features. This
    lets us reduce variance while improving the interpretability of our model (since
    fewer features are easier to explain).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这种效果的实际好处在于，我们可以在特征矩阵中包含 100 个特征，然后通过调整 lasso 的 α 超参数，生成仅使用最重要的 10 个特征之一的模型（例如）。这使得我们能够在提升模型的可解释性的同时减少方差（因为更少的特征更容易解释）。
