- en: Chapter 6\. Handling Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 处理文本
- en: 6.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.0 引言
- en: Unstructured text data, like the contents of a book or a tweet, is both one
    of the most interesting sources of features and one of the most complex to handle.
    In this chapter, we will cover strategies for transforming text into information-rich
    features and use some out-of-the-box features (termed *embeddings*) that have
    become increasingly ubiquitous in tasks that involve natural language processing
    (NLP).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化的文本数据，如书籍内容或推文，既是最有趣的特征来源之一，也是最复杂的处理之一。在本章中，我们将介绍将文本转换为信息丰富特征的策略，并使用一些出色的特征（称为*嵌入*），这些特征在涉及自然语言处理（NLP）的任务中变得日益普遍。
- en: This is not to say that the recipes covered here are comprehensive. Entire academic
    disciplines focus on handling unstructured data such as text. In this chapter,
    we will cover some commonly used techniques; knowledge of these will add valuable
    tools to our preprocessing toolbox. In addition to many generic text processing
    recipes, we’ll also demonstrate how you can import and leverage some pretrained
    machine learning models to generate richer text features.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着这里涵盖的配方是全面的。整个学术学科都专注于处理文本等非结构化数据。在本章中，我们将涵盖一些常用的技术；掌握这些将为我们的预处理工具箱增添宝贵的工具。除了许多通用的文本处理配方外，我们还将演示如何导入和利用一些预训练的机器学习模型来生成更丰富的文本特征。
- en: 6.1 Cleaning Text
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.1 清理文本
- en: Problem
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have some unstructured text data and want to complete some basic cleaning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一些非结构化文本数据，想要完成一些基本的清理工作。
- en: Solution
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'In the following example, we look at the text for three books and clean it
    by using Python’s core string operations, in particular `strip`, `replace`, and
    `split`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的例子中，我们查看三本书的文本，并通过Python的核心字符串操作，特别是`strip`、`replace`和`split`，对其进行清理：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We also create and apply a custom transformation function:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建并应用了一个自定义转换函数：
- en: '[PRE4]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we can use regular expressions to make powerful string operations:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用正则表达式进行强大的字符串操作：
- en: '[PRE6]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Discussion
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Some text data will need to be cleaned before we can use it to build features,
    or be preprocessed in some way prior to being fed into an algorithm. Most basic
    text cleaning can be completed using Python’s standard string operations. In the
    real world, we will most likely define a custom cleaning function (e.g., `capitalizer`)
    combining some cleaning tasks and apply that to the text data. Although cleaning
    strings can remove some information, it makes the data much easier to work with.
    Strings have many inherent methods that are useful for cleaning and processing;
    some additional examples can be found here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些文本数据在用于构建特征或在输入算法之前需要进行基本的清理。大多数基本的文本清理可以使用Python的标准字符串操作完成。在实际应用中，我们很可能会定义一个自定义的清理函数（例如`capitalizer`），结合一些清理任务，并将其应用于文本数据。虽然清理字符串可能会删除一些信息，但它使数据更易于处理。字符串具有许多有用的固有方法用于清理和处理；一些额外的例子可以在这里找到：
- en: '[PRE8]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: See Also
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Beginners Tutorial for Regular Expressions in Python](https://oreil.ly/hSqsa)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python正则表达式入门教程](https://oreil.ly/hSqsa)'
- en: 6.2 Parsing and Cleaning HTML
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.2 解析和清理HTML
- en: Problem
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have text data with HTML elements and want to extract just the text.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你有包含HTML元素的文本数据，并希望仅提取文本部分。
- en: Solution
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use Beautiful Soup’s extensive set of options to parse and extract from HTML:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Beautiful Soup广泛的选项集来解析和从HTML中提取：
- en: '[PRE10]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Discussion
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Despite the strange name, Beautiful Soup is a powerful Python library designed
    for scraping HTML. Typically Beautiful Soup is used to process HTML during live
    web scraping, but we can just as easily use it to extract text data embedded in
    static HTML. The full range of Beautiful Soup operations is beyond the scope of
    this book, but even the method we use in our solution shows how easy it can be
    to parse HTML and extract information from specific tags using `find()`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名字奇怪，Beautiful Soup是一个功能强大的Python库，专门用于解析HTML。通常Beautiful Soup用于实时网页抓取过程中处理HTML，但我们同样可以使用它来提取静态HTML中嵌入的文本数据。Beautiful
    Soup的全部操作远超出本书的范围，但即使是我们在解决方案中使用的方法，也展示了使用`find()`方法可以轻松地解析HTML并从特定标签中提取信息。
- en: See Also
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Beautiful Soup](https://oreil.ly/vh8h3)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Beautiful Soup](https://oreil.ly/vh8h3)'
- en: 6.3 Removing Punctuation
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.3 删除标点符号
- en: Problem
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a feature of text data and want to remove punctuation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一项文本数据的特征，并希望去除标点符号。
- en: Solution
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Define a function that uses `translate` with a dictionary of punctuation characters:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个使用`translate`和标点字符字典的函数：
- en: '[PRE12]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Discussion
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The Python `translate` method is popular due to its speed. In our solution,
    first we created a dictionary, `punctuation`, with all punctuation characters
    according to Unicode as its keys and `None` as its values. Next we translated
    all characters in the string that are in `punctuation` into `None`, effectively
    removing them. There are more readable ways to remove punctuation, but this somewhat
    hacky solution has the advantage of being far faster than alternatives.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的 `translate` 方法因其速度而流行。在我们的解决方案中，首先我们创建了一个包含所有标点符号字符（按照 Unicode 标准）作为键和
    `None` 作为值的字典 `punctuation`。接下来，我们将字符串中所有在 `punctuation` 中的字符翻译为 `None`，从而有效地删除它们。还有更可读的方法来删除标点，但这种有些“hacky”的解决方案具有比替代方案更快的优势。
- en: It is important to be conscious of the fact that punctuation contains information
    (e.g., “Right?” versus “Right!”). Removing punctuation can be a necessary evil
    when we need to manually create features; however, if the punctuation is important
    we should make sure to take that into account. Depending on the downstream task
    we’re trying to accomplish, punctuation might contain important information we
    want to keep (e.g., using a “?” to classify if some text contains a question).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 需要意识到标点包含信息这一事实是很重要的（例如，“对吧？”与“对吧！”）。在需要手动创建特征时，删除标点可能是必要的恶；然而，如果标点很重要，我们应该确保考虑到这一点。根据我们试图完成的下游任务的不同，标点可能包含我们希望保留的重要信息（例如，使用“？”来分类文本是否包含问题）。
- en: 6.4 Tokenizing Text
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.4 文本分词
- en: Problem
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have text and want to break it up into individual words.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一段文本，希望将其分解成单独的单词。
- en: Solution
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Natural Language Toolkit for Python (NLTK) has a powerful set of text manipulation
    operations, including word tokenizing:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的自然语言工具包（NLTK）具有强大的文本操作集，包括词分词：
- en: '[PRE14]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can also tokenize into sentences:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将其分词成句子：
- en: '[PRE16]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Discussion
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Tokenization*, especially word tokenization, is a common task after cleaning
    text data because it is the first step in the process of turning the text into
    data we will use to construct useful features. Some pretrained NLP models (such
    as Google’s BERT) utilize model-specific tokenization techniques; however, word-level
    tokenization is still a fairly common tokenization approach before getting features
    from individual words.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*分词*，尤其是词分词，在清洗文本数据后是一项常见任务，因为它是将文本转换为我们将用来构建有用特征的数据的第一步。一些预训练的自然语言处理模型（如 Google
    的 BERT）使用特定于模型的分词技术；然而，在从单词级别获取特征之前，词级分词仍然是一种相当常见的分词方法。'
- en: 6.5 Removing Stop Words
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.5 移除停用词
- en: Problem
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Given tokenized text data, you want to remove extremely common words (e.g.,
    *a*, *is*, *of*, *on*) that contain little informational value.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 给定标记化的文本数据，你希望移除极其常见的单词（例如，*a*、*is*、*of*、*on*），它们的信息价值很小。
- en: Solution
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use NLTK’s `stopwords`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NLTK 的 `stopwords`：
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Discussion
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'While “stop words” can refer to any set of words we want to remove before processing,
    frequently the term refers to extremely common words that themselves contain little
    information value. Whether or not you choose to remove stop words will depend
    on your individual use case. NLTK has a list of common stop words that we can
    use to find and remove stop words in our tokenized words:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然“停用词”可以指任何我们希望在处理前移除的单词集，但通常这个术语指的是那些本身包含很少信息价值的极其常见的单词。是否选择移除停用词将取决于你的具体用例。NLTK
    有一个常见停用词列表，我们可以用来查找并移除我们标记化的单词中的停用词：
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that NLTK’s `stopwords` assumes the tokenized words are all lowercased.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，NLTK 的 `stopwords` 假定标记化的单词都是小写的。
- en: 6.6 Stemming Words
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.6 词干提取
- en: Problem
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have tokenized words and want to convert them into their root forms.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一些标记化的单词，并希望将它们转换为它们的根形式。
- en: Solution
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use NLTK’s `PorterStemmer`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NLTK 的 `PorterStemmer`：
- en: '[PRE22]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Discussion
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: '*Stemming* reduces a word to its stem by identifying and removing affixes (e.g.,
    gerunds) while keeping the root meaning of the word. For example, both “tradition”
    and “traditional” have “tradit” as their stem, indicating that while they are
    different words, they represent the same general concept. By stemming our text
    data, we transform it to something less readable but closer to its base meaning
    and thus more suitable for comparison across observations. NLTK’s `PorterStemmer`
    implements the widely used Porter stemming algorithm to remove or replace common
    suffixes to produce the word stem.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*词干提取* 通过识别和移除词缀（例如动名词），将单词减少到其词干，同时保持单词的根本含义。例如，“tradition” 和 “traditional”
    都有 “tradit” 作为它们的词干，表明虽然它们是不同的词，但它们代表同一个一般概念。通过词干提取我们的文本数据，我们将其转换为不太可读但更接近其基本含义的形式，因此更适合跨观察进行比较。NLTK
    的 `PorterStemmer` 实现了广泛使用的 Porter 词干提取算法，以移除或替换常见的后缀，生成词干。'
- en: See Also
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[The Porter Stemming Algorithm](https://oreil.ly/Z4NTp)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Porter 词干提取算法](https://oreil.ly/Z4NTp)'
- en: 6.7 Tagging Parts of Speech
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.7 标记词性
- en: Problem
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have text data and want to tag each word or character with its part of speech.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您拥有文本数据，并希望标记每个单词或字符的词性。
- en: Solution
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use NLTK’s pretrained parts-of-speech tagger:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NLTK 的预训练词性标注器：
- en: '[PRE24]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is a list of tuples with the word and the tag of the part of speech.
    NLTK uses the Penn Treebank parts for speech tags. Some examples of the Penn Treebank
    tags are:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个包含单词和词性标签的元组列表。NLTK 使用宾树库的词性标签。宾树库的一些示例标签包括：
- en: '| Tag | Part of speech |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Tag | 词性 |'
- en: '| --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| NNP | Proper noun, singular |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| NNP | 专有名词，单数 |'
- en: '| NN | Noun, singular or mass |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| NN | 名词，单数或集合名词 |'
- en: '| RB | Adverb |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| RB | 副词 |'
- en: '| VBD | Verb, past tense |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| VBD | 动词，过去式 |'
- en: '| VBG | Verb, gerund or present participle |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| VBG | 动词，动名词或现在分词 |'
- en: '| JJ | Adjective |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| JJ | 形容词 |'
- en: '| PRP | Personal pronoun |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| PRP | 人称代词 |'
- en: 'Once the text has been tagged, we can use the tags to find certain parts of
    speech. For example, here are all nouns:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文本被标记，我们可以使用标签找到特定的词性。例如，这里是所有的名词：
- en: '[PRE26]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'A more realistic situation would be to have data where every observation contains
    a tweet, and we want to convert those sentences into features for individual parts
    of speech (e.g., a feature with `1` if a proper noun is present, and `0` otherwise):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 更现实的情况可能是有数据，每个观察都包含一条推文，并且我们希望将这些句子转换为各个词性的特征（例如，如果存在专有名词，则为 `1`，否则为 `0`）：
- en: '[PRE28]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Using `classes_` we can see that each feature is a part-of-speech tag:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `classes_`，我们可以看到每个特征都是一个词性标签：
- en: '[PRE30]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Discussion
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: If our text is English and not on a specialized topic (e.g., medicine) the simplest
    solution is to use NLTK’s pretrained parts-of-speech tagger. However, if `pos_tag`
    is not very accurate, NLTK also gives us the ability to train our own tagger.
    The major downside of training a tagger is that we need a large corpus of text
    where the tag of each word is known. Constructing this tagged corpus is obviously
    labor intensive and is probably going to be a last resort.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的文本是英语且不涉及专业主题（例如医学），最简单的解决方案是使用 NLTK 的预训练词性标注器。但是，如果 `pos_tag` 不太准确，NLTK
    还为我们提供了训练自己标注器的能力。训练标注器的主要缺点是我们需要一个大型文本语料库，其中每个词的标签是已知的。构建这种标记语料库显然是劳动密集型的，可能是最后的选择。
- en: See Also
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Alphabetical list of part-of-speech tags used in the Penn Treebank Project](https://oreil.ly/31xKf)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[宾树库项目中使用的词性标签的字母顺序列表](https://oreil.ly/31xKf)'
- en: 6.8 Performing Named-Entity Recognition
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.8 执行命名实体识别
- en: Problem
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to perform named-entity recognition in freeform text (such as “Person,”
    “State,” etc.).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望在自由文本中执行命名实体识别（如“人物”，“州”等）。
- en: Solution
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use spaCy’s default named-entity recognition pipeline and models to extract
    entites from text:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 spaCy 的默认命名实体识别管道和模型从文本中提取实体：
- en: '[PRE32]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Discussion
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Named-entity recognition is the process of recognizing specific entities from
    text. Tools like spaCy offer preconfigured pipelines, and even pretrained or fine-tuned
    machine learning models that can easily identify these entities. In this case,
    we use spaCy to identify a person (“Elon Musk”), organization (“Twitter”), and
    money value (“21B”) from the raw text. Using this information, we can extract
    structured information from the unstructured textual data. This information can
    then be used in downstream machine learning models or data analysis.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别是从文本中识别特定实体的过程。像spaCy这样的工具提供预配置的管道，甚至是预训练或微调的机器学习模型，可以轻松识别这些实体。在本例中，我们使用spaCy识别文本中的人物（“Elon
    Musk”）、组织（“Twitter”）和金额（“21B”）。利用这些信息，我们可以从非结构化文本数据中提取结构化信息。这些信息随后可以用于下游机器学习模型或数据分析。
- en: Training a custom named-entity recognition model is outside the scope of this
    example; however, it is often done using deep learning and other NLP techniques.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 训练自定义命名实体识别模型超出了本示例的范围；但是，通常使用深度学习和其他自然语言处理技术来完成此任务。
- en: See Also
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[spaCy Named Entity Recognition documentation](https://oreil.ly/cN8KM)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[spaCy命名实体识别文档](https://oreil.ly/cN8KM)'
- en: '[Named-entity recognition, Wikipedia](https://oreil.ly/G8WDF)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[命名实体识别，维基百科](https://oreil.ly/G8WDF)'
- en: 6.9 Encoding Text as a Bag of Words
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.9 将文本编码为词袋模型
- en: Problem
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have text data and want to create a set of features indicating the number
    of times an observation’s text contains a particular word.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您有文本数据，并希望创建一组特征，指示观察文本中包含特定单词的次数。
- en: Solution
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use scikit-learn’s `CountVectorizer`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn的`CountVectorizer`：
- en: '[PRE34]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This output is a sparse array, which is often necessary when we have a large
    amount of text. However, in our toy example we can use `toarray` to view a matrix
    of word counts for each observation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出是一个稀疏数组，在我们有大量文本时通常是必要的。但是，在我们的玩具示例中，我们可以使用`toarray`查看每个观察结果的单词计数矩阵：
- en: '[PRE36]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can use the `get_feature_names` method to view the word associated with
    each feature:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`get_feature_names`方法查看与每个特征关联的单词：
- en: '[PRE38]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note that the `I` from `I love Brazil` is not considered a token because the
    default `token_pattern` only considers tokens of two or more alphanumeric characters.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，“I”从“I love Brazil”中不被视为一个标记，因为默认的“token_pattern”只考虑包含两个或更多字母数字字符的标记。
- en: 'Still, this might be confusing so, for the sake of clarity, here is what the
    feature matrix looks like with the words as column names (each row is one observation):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能会令人困惑，为了明确起见，这里是特征矩阵的外观，其中单词作为列名（每行是一个观察结果）：
- en: '| beats | best | both | brazil | germany | is | love | sweden |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| beats | best | both | brazil | germany | is | love | sweden |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 0 | 0 | 2 | 0 | 0 | 1 | 0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 2 | 0 | 0 | 1 | 0 |'
- en: '| 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 |'
- en: '| 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 |'
- en: Discussion
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: One of the most common methods of transforming text into features is using a
    bag-of-words model. Bag-of-words models output a feature for every unique word
    in text data, with each feature containing a count of occurrences in observations.
    For example, in our solution, the sentence “I love Brazil. Brazil!” has a value
    of `2` in the “brazil” feature because the word *brazil* appears two times.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本转换为特征的最常见方法之一是使用词袋模型。词袋模型为文本数据中的每个唯一单词输出一个特征，每个特征包含在观察中出现的次数计数。例如，在我们的解决方案中，句子“I
    love Brazil. Brazil!”中，“brazil”特征的值为`2`，因为单词*brazil*出现了两次。
- en: The text data in our solution was purposely small. In the real world, a single
    observation of text data could be the contents of an entire book! Since our bag-of-words
    model creates a feature for every unique word in the data, the resulting matrix
    can contain thousands of features. This means the size of the matrix can sometimes
    become very large in memory. Luckily, we can exploit a common characteristic of
    bag-of-words feature matrices to reduce the amount of data we need to store.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解决方案中的文本数据故意很小。在现实世界中，文本数据的单个观察结果可能是整本书的内容！由于我们的词袋模型为数据中的每个唯一单词创建一个特征，因此生成的矩阵可能包含数千个特征。这意味着矩阵的大小有时可能会在内存中变得非常大。幸运的是，我们可以利用词袋特征矩阵的常见特性来减少我们需要存储的数据量。
- en: Most words likely do not occur in most observations, and therefore bag-of-words
    feature matrices will contain mostly 0s as values. We call these types of matrices
    *sparse*. Instead of storing all values of the matrix, we can store only nonzero
    values and then assume all other values are 0\. This will save memory when we
    have large feature matrices. One of the nice features of `CountVectorizer` is
    that the output is a sparse matrix by default.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数单词可能不会出现在大多数观察中，因此单词袋特征矩阵将主要包含值为0的值。我们称这些类型的矩阵为 *稀疏*。我们可以只存储非零值，然后假定所有其他值为0，以节省内存，特别是在具有大型特征矩阵时。`CountVectorizer`
    的一个好处是默认输出是稀疏矩阵。
- en: '`CountVectorizer` comes with a number of useful parameters to make it easy
    to create bag-of-words feature matrices. First, while by default every feature
    is a word, that does not have to be the case. Instead we can set every feature
    to be the combination of two words (called a 2-gram) or even three words (3-gram).
    `ngram_range` sets the minimum and maximum size of our *n*-grams. For example,
    `(2,3)` will return all 2-grams and 3-grams. Second, we can easily remove low-information
    filler words by using `stop_words`, either with a built-in list or a custom list.
    Finally, we can restrict the words or phrases we want to consider to a certain
    list of words using `vocabulary`. For example, we could create a bag-of-words
    feature matrix only for occurrences of country names:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer` 配备了许多有用的参数，使得创建单词袋特征矩阵变得容易。首先，默认情况下，每个特征是一个单词，但这并不一定是情况。我们可以将每个特征设置为两个单词的组合（称为2-gram）甚至三个单词（3-gram）。`ngram_range`
    设置了我们的*n*-gram的最小和最大大小。例如，`(2,3)` 将返回所有的2-gram和3-gram。其次，我们可以使用 `stop_words` 轻松地去除低信息的填充词，可以使用内置列表或自定义列表。最后，我们可以使用
    `vocabulary` 限制我们希望考虑的单词或短语列表。例如，我们可以仅为国家名称的出现创建一个单词袋特征矩阵：'
- en: '[PRE40]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: See Also
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[*n*-gram, Wikipedia](https://oreil.ly/XWIrM)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*n*-gram, 维基百科](https://oreil.ly/XWIrM)'
- en: '[Bag of Words Meets Bags of Popcorn](https://oreil.ly/IiyRV)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[袋装爆米花遇到袋装爆米花](https://oreil.ly/IiyRV)'
- en: 6.10 Weighting Word Importance
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.10 加权词重要性
- en: Problem
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want a bag of words with words weighted by their importance to an observation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望一个单词袋，其中单词按其对观察的重要性加权。
- en: Solution
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Compare the frequency of the word in a document (a tweet, movie review, speech
    transcript, etc.) with the frequency of the word in all other documents using
    term frequency-inverse document frequency (<math display="inline"><mtext class="left_paren"
    fontstyle="italic">tf-idf</mtext></math>). scikit-learn makes this easy with `TfidfVectorizer`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用词频-逆文档频率（<math display="inline"><mtext class="left_paren" fontstyle="italic">tf-idf</mtext></math>）比较单词在文档（推文、电影评论、演讲文稿等）中的频率与单词在所有其他文档中的频率。scikit-learn通过
    `TfidfVectorizer` 轻松实现这一点：
- en: '[PRE44]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Just as in [Recipe 6.9](#encoding-text-as-a-bag-of-words), the output is a
    sparse matrix. However, if we want to view the output as a dense matrix, we can
    use `toarray`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在[食谱 6.9](#encoding-text-as-a-bag-of-words)中一样，输出是一个稀疏矩阵。然而，如果我们想将输出视为密集矩阵，我们可以使用
    `toarray`：
- en: '[PRE46]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`vocabulary_` shows us the word of each feature:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`vocabulary_` 展示了每个特征的词汇：'
- en: '[PRE48]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Discussion
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The more a word appears in a document, the more likely it is that the word is
    important to that document. For example, if the word *economy* appears frequently,
    it is evidence that the document might be about economics. We call this *term
    frequency* (<math display="inline"><mtext class="left_paren" fontstyle="italic">tf</mtext></math>).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 单词在文档中出现的次数越多，该单词对该文档的重要性就越高。例如，如果单词 *economy* 经常出现，这表明文档可能与经济有关。我们称之为 *词频*
    (<math display="inline"><mtext class="left_paren" fontstyle="italic">tf</mtext></math>)。
- en: In contrast, if a word appears in many documents, it is likely less important
    to any individual document. For example, if every document in some text data contains
    the word *after* then it is probably an unimportant word. We call this *document
    frequency* (<math display="inline"><mtext class="left_paren" fontstyle="italic">df</mtext></math>).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果一个词在许多文档中出现，它可能对任何单个文档的重要性较低。例如，如果某个文本数据中的每个文档都包含单词 *after*，那么它可能是一个不重要的词。我们称之为
    *文档频率* (<math display="inline"><mtext class="left_paren" fontstyle="italic">df</mtext></math>)。
- en: 'By combining these two statistics, we can assign a score to every word representing
    how important that word is in a document. Specifically, we multiply <math display="inline"><mtext
    fontstyle="italic">tf</mtext></math> to the inverse of document frequency (<math
    display="inline"><mtext class="left_paren" fontstyle="italic">idf</mtext></math>):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这两个统计量，我们可以为每个单词分配一个分数，代表该单词在文档中的重要性。具体来说，我们将 <math display="inline"><mtext
    fontstyle="italic">tf</mtext></math> 乘以文档频率的倒数 <math display="inline"><mtext class="left_paren"
    fontstyle="italic">idf</mtext></math>：
- en: <math display="block"><mrow><mtext class="left_paren" fontstyle="italic">tf-idf</mtext>
    <mo>(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>)</mo> <mo>=</mo> <mi>t</mi> <mi>f</mi>
    <mo class="left_paren">(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>)</mo> <mo>×</mo>
    <mtext class="left_paren" fontstyle="italic">idf</mtext> <mo>(</mo> <mi class="left_paren">t</mi>
    <mo>)</mo></mrow></math>
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext class="left_paren" fontstyle="italic">tf-idf</mtext>
    <mo>(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>)</mo> <mo>=</mo> <mi>t</mi> <mi>f</mi>
    <mo class="left_paren">(</mo> <mi>t</mi> <mo>,</mo> <mi>d</mi> <mo>)</mo> <mo>×</mo>
    <mtext class="left_paren" fontstyle="italic">idf</mtext> <mo>(</mo> <mi class="left_paren">t</mi>
    <mo>)</mo></mrow></math>
- en: 'where <math display="inline"><mi>t</mi></math> is a word (term) and <math display="inline"><mi>d</mi></math>
    is a document. There are a number of variations in how <math display="inline"><mtext
    fontstyle="italic">tf</mtext></math> and <math display="inline"><mtext fontstyle="italic">idf</mtext></math>
    are calculated. In scikit-learn, <math display="inline"><mtext fontstyle="italic">tf</mtext></math>
    is simply the number of times a word appears in the document, and <math display="inline"><mtext
    fontstyle="italic">idf</mtext></math> is calculated as:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><mi>t</mi></math> 是一个单词（术语），<math display="inline"><mi>d</mi></math>
    是一个文档。关于如何计算 <math display="inline"><mtext fontstyle="italic">tf</mtext></math>
    和 <math display="inline"><mtext fontstyle="italic">idf</mtext></math> 有许多不同的变体。在
    scikit-learn 中，<math display="inline"><mtext fontstyle="italic">tf</mtext></math>
    简单地是单词在文档中出现的次数，<math display="inline"><mtext fontstyle="italic">idf</mtext></math>
    计算如下：
- en: <math display="block"><mrow><mtext class="left_paren" fontstyle="italic">idf</mtext>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>l</mi> <mi>o</mi>
    <mi>g</mi> <mfrac><mrow><mn>1</mn><mo>+</mo><msub><mi>n</mi> <mi>d</mi></msub></mrow>
    <mrow><mn>1</mn><mo>+</mo><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow></mfrac>
    <mo>+</mo> <mn>1</mn></mrow></math>
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext class="left_paren" fontstyle="italic">idf</mtext>
    <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>l</mi> <mi>o</mi>
    <mi>g</mi> <mfrac><mrow><mn>1</mn><mo>+</mo><msub><mi>n</mi> <mi>d</mi></msub></mrow>
    <mrow><mn>1</mn><mo>+</mo><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></mrow></mfrac>
    <mo>+</mo> <mn>1</mn></mrow></math>
- en: where <math display="inline"><msub><mi>n</mi><mi>d</mi></msub></math> is the
    number of documents, and <math display="inline"><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></math>
    is term <math display="inline"><mi>t</mi></math>’s document frequency (i.e., the
    number of documents where the term appears).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><msub><mi>n</mi><mi>d</mi></msub></math> 是文档数量，<math
    display="inline"><mtext class="left_paren" fontstyle="italic">df</mtext><mo>(</mo><mi>d</mi><mo>,</mo><mi>t</mi><mo>)</mo></math>
    是术语 <math display="inline"><mi>t</mi></math> 的文档频率（即术语出现的文档数量）。
- en: By default, scikit-learn then normalizes the <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors using the Euclidean norm (L2 norm). The higher the resulting value, the
    more important the word is to a document.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，scikit-learn 使用欧几里得范数（L2 范数）对 <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    向量进行归一化。结果值越高，单词对文档的重要性越大。
- en: See Also
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '[scikit-learn documentation: *tf–idf* term weighting](https://oreil.ly/40WeT)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn 文档: *tf–idf* 术语加权](https://oreil.ly/40WeT)'
- en: 6.11 Using Text Vectors to Calculate Text Similarity in a Search Query
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.11 使用文本向量计算搜索查询中的文本相似度
- en: Problem
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to use <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors to implement a text search function in Python.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要使用 <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    向量来实现 Python 中的文本搜索功能。
- en: Solution
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Calculate the cosine similarity between <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors using scikit-learn:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 计算 <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    向量之间的余弦相似度：
- en: '[PRE50]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Discussion
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Text vectors are incredibly useful for NLP use cases such as search engines.
    After calculating the <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors of a set of sentences or documents, we can use the same `tfidf` object
    to vectorize future sets of text. Then, we can compute cosine similarity between
    our input vector and the matrix of other vectors and sort by the most relevant
    documents.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 文本向量对于诸如搜索引擎之类的 NLP 用例非常有用。计算了一组句子或文档的 <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    向量后，我们可以使用相同的 `tfidf` 对象来向量化未来的文本集。然后，我们可以计算输入向量与其他向量矩阵之间的余弦相似度，并按最相关的文档进行排序。
- en: Cosine similarities take on the range of [0, 1.0], with 0 being least similar
    and 1 being most similar. Since we’re using <math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>
    vectors to compute the similarity between vectors, the frequency of a word’s occurrence
    is also taken into account. However, with a small corpus (set of documents) even
    “frequent” words may not appear frequently. In this example, “Sweden is best”
    is the most relevant text to our search query “Brazil is the best”. Since the
    query mentions Brazil, we might expect “I love Brazil. Brazil!” to be the most
    relevant; however, “Sweden is best” is the most similar due to the words “is”
    and “best”. As the number of documents we add to our corpus increases, less important
    words will be weighted less and have less effect on our cosine similarity calculation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度的取值范围为[0, 1.0]，其中0表示最不相似，1表示最相似。由于我们使用<math display="inline"><mtext fontstyle="italic">tf-idf</mtext></math>向量来计算向量之间的相似度，单词出现的频率也被考虑在内。然而，在一个小的语料库（文档集合）中，即使是“频繁”出现的词语也可能不频繁出现。在这个例子中，“瑞典是最好的”是最相关的文本，与我们的搜索查询“巴西是最好的”最相似。由于查询提到了巴西，我们可能期望“我爱巴西。巴西！”是最相关的；然而，由于“是”和“最好”，“瑞典是最好的”是最相似的。随着我们向语料库中添加的文档数量的增加，不重要的词语将被加权较少，对余弦相似度计算的影响也将减小。
- en: See Also
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Cosine Similarity, Geeks for Geeks](https://oreil.ly/-5Odv)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[余弦相似度，Geeks for Geeks](https://oreil.ly/-5Odv)'
- en: '[Nvidia Gave Me a $15K Data Science Workstation—Here’s What I Did with It (building
    a Pubmed search engine in Python)](https://oreil.ly/pAxbR)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Nvidia给了我一台价值15000美元的数据科学工作站——这是我在其中做的事情（用Python构建Pubmed搜索引擎）](https://oreil.ly/pAxbR)'
- en: 6.12 Using a Sentiment Analysis Classifier
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.12 使用情感分析分类器
- en: Problem
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to classify the sentiment of some text to use as a feature or in downstream
    data analysis.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望对一些文本的情感进行分类，以便作为特征或在下游数据分析中使用。
- en: Solution
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `transformers` library’s sentiment classifier.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`transformers`库的情感分类器。
- en: '[PRE52]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Discussion
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The `transformers` library is an extremely popular library for NLP tasks and
    contains a number of easy-to-use APIs for training models or using pretrained
    ones. We’ll talk more about NLP and this library in [Chapter 22](ch22.xhtml#neural-networks-for-unstructured-data),
    but this example serves as a high-level introduction to the power of using pretrained
    classifiers in your machine learning pipelines to generate features, classify
    text, or analyze unstructured data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`库是一个极为流行的自然语言处理任务库，包含许多易于使用的API，用于训练模型或使用预训练模型。我们将在[第22章](ch22.xhtml#neural-networks-for-unstructured-data)更详细地讨论NLP和这个库，但这个例子作为使用预训练分类器在您的机器学习流水线中生成特征、分类文本或分析非结构化数据的强大工具的高级介绍。'
- en: See Also
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Hugging Face Transformers Quick Tour](https://oreil.ly/7hT6W)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hugging Face Transformers快速导览](https://oreil.ly/7hT6W)'
