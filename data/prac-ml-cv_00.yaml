- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Machine learning on images is revolutionizing healthcare, manufacturing, retail,
    and many other sectors. Many previously difficult problems can now be solved by
    training machine learning (ML) models to identify objects in images. Our aim in
    this book is to provide intuitive explanations of the ML architectures that underpin
    this fast-advancing field, and to provide practical code to employ these ML models
    to solve problems involving classification, measurement, detection, segmentation,
    representation, generation, counting, and more.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 图像上的机器学习正在改变医疗保健、制造业、零售业和许多其他行业。通过训练机器学习（ML）模型识别图像中的对象，许多以前难以解决的问题现在可以解决。本书的目标是提供对支撑这一快速发展领域的ML架构的直观解释，并提供实用代码来应用这些ML模型解决涉及分类、测量、检测、分割、表征、生成、计数等问题。
- en: Image classification is the “hello world” of deep learning. Therefore, this
    book also provides a practical end-to-end introduction to deep learning. It can
    serve as a stepping stone to other deep learning domains, such as natural language
    processing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是深度学习的“hello world”。因此，本书还提供了深度学习的实用端到端介绍。它可以作为进入其他深度学习领域（如自然语言处理）的基础。
- en: You will learn how to design ML architectures for computer vision tasks and
    carry out model training using popular, well-tested prebuilt models written in
    TensorFlow and Keras. You will also learn techniques to improve accuracy and explainability.
    Finally, this book will teach you how to design, implement, and tune end-to-end
    ML pipelines for image understanding tasks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 您将学习如何设计用于计算机视觉任务的ML架构，并使用TensorFlow和Keras中流行的、经过充分测试的预建模型进行模型训练。您还将学习提高准确性和可解释性的技术。最后，本书将教您如何设计、实施和调整端到端的ML管道来理解图像任务。
- en: Who Is This Book For?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适合谁？
- en: The primary audience for this book is software developers who want to do machine
    learning on images. It is meant for developers who will use TensorFlow and Keras
    to solve common computer vision use cases.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的主要读者是希望在图像上进行机器学习的软件开发人员。它适用于将使用TensorFlow和Keras解决常见计算机视觉用例的开发人员。
- en: The methods discussed in the book are accompanied by code samples available
    at [*https://github.com/GoogleCloudPlatform/practical-ml-vision-book*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book).
    Most of this book involves open source TensorFlow and Keras and will work regardless
    of whether you run the code on premises, in Google Cloud, or in some other cloud.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本书讨论的方法附带在[*https://github.com/GoogleCloudPlatform/practical-ml-vision-book*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book)的代码示例。本书大部分涉及开源TensorFlow和Keras，并且无论您在本地、Google
    Cloud还是其他云上运行代码，它都能正常工作。
- en: Developers who wish to use PyTorch will find the textual explanations useful,
    but will probably have to look elsewhere for practical code snippets. We do welcome
    contributions of PyTorch equivalents of our code samples; please make a pull request
    to our GitHub repository.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 希望使用PyTorch的开发人员将会发现文本解释有用，但可能需要在其他地方寻找实用代码片段。我们欢迎提供PyTorch等效代码示例的贡献，请向我们的GitHub存储库提交拉取请求。
- en: How to Use This Book
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用本书
- en: We recommend that you read this book in order. Make sure to read, understand,
    and run the accompanying notebooks in the book’s [GitHub repository](https://github.com/GoogleCloudPlatform/practical-ml-vision-book)—you
    can run them in either Google Colab or Google Cloud’s Vertex Notebooks. We suggest
    that after reading each section of the text you try out the code to be sure you
    fully understand the concepts and techniques that are introduced. We strongly
    recommend completing the notebooks in each chapter before moving on to the next
    chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您按顺序阅读本书。确保阅读、理解并运行书中的附带笔记本在[GitHub存储库](https://github.com/GoogleCloudPlatform/practical-ml-vision-book)中的章节——您可以在Google
    Colab或Google Cloud的Vertex笔记本中运行它们。我们建议在阅读每个文本部分后尝试运行代码，以确保您充分理解引入的概念和技术。我们强烈建议在转到下一章之前完成每章的笔记本。
- en: Google Colab is free and will suffice to run most of the notebooks in this book;
    Vertex Notebooks is more powerful and so will help you run through the notebooks
    faster. The more complex models and larger datasets of Chapters [3](ch03.xhtml#image_vision),
    [4](ch04.xhtml#object_detection_and_image_segmentation), [11](ch11.xhtml#advanced_vision_problems),
    and [12](ch12.xhtml#image_and_text_generation) will benefit from the use of Google
    Cloud TPUs. Because all the code in this book is written using open source APIs,
    the code *should* also work in any other Jupyter environment where you have the
    latest version of TensorFlow installed, whether it’s your laptop, or Amazon Web
    Services (AWS) Sagemaker, or Azure ML. However, we haven’t tested it in those
    environments. If you find that you have to make any changes to get the code to
    work in some other environment, please do submit a pull request in order to help
    other readers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab是免费的，可以运行本书中大多数的笔记本；Vertex Notebooks更为强大，因此可以帮助您更快地运行笔记本。在第[3章](ch03.xhtml#image_vision)、[4章](ch04.xhtml#object_detection_and_image_segmentation)、[11章](ch11.xhtml#advanced_vision_problems)和[12章](ch12.xhtml#image_and_text_generation)中更复杂的模型和更大的数据集将受益于使用Google
    Cloud TPUs。因为本书中所有代码都使用开源API编写，所以代码*应该*也能在任何其他安装了最新版本TensorFlow的Jupyter环境中运行，无论是您的笔记本电脑，还是Amazon
    Web Services（AWS）Sagemaker，或者Azure ML。然而，我们尚未在这些环境中进行过测试。如果您发现需要进行任何更改才能使代码在其他环境中工作，请提交一个拉取请求，以帮助其他读者。
- en: The code in this book is made available to you under an Apache open source license.
    It is meant primarily as a teaching tool, but can serve as a starting point for
    your production models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码在Apache开源许可下向您提供。它主要作为教学工具，但也可以作为您生产模型的起点。
- en: Organization of the Book
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 书籍组织
- en: 'The remainder of this book is organized as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的其余部分组织如下：
- en: In [Chapter 2](ch02.xhtml#ml_models_for_vision), we introduce machine learning,
    how to read in images, and how to train, evaluate, and predict with ML models.
    The models we cover in [Chapter 2](ch02.xhtml#ml_models_for_vision) are generic
    and thus don’t work particularly well on images, but the concepts introduced in
    this chapter are essential for the rest of the book.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.xhtml#ml_models_for_vision)中，我们介绍了机器学习、如何读取图像以及如何使用ML模型进行训练、评估和预测。我们在第[2章](ch02.xhtml#ml_models_for_vision)中介绍的模型是通用的，因此在图像上的表现不是特别好，但本章介绍的概念对本书的其余部分至关重要。
- en: In [Chapter 3](ch03.xhtml#image_vision), we introduce some machine learning
    models that do work well on images. We start with transfer learning and fine-tuning,
    and then introduce a variety of convolutional models that increase in sophistication
    as we get further and further into the chapter.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.xhtml#image_vision)中，我们介绍了一些在图像上表现良好的机器学习模型。我们从迁移学习和微调开始，然后介绍了各种卷积模型，这些模型随着我们深入章节变得越来越复杂。
- en: In [Chapter 4](ch04.xhtml#object_detection_and_image_segmentation), we explore
    the use of computer vision to address object detection and image segmentation
    problems. Any of the backbone architectures introduced in [Chapter 3](ch03.xhtml#image_vision)
    can be used in [Chapter 4](ch04.xhtml#object_detection_and_image_segmentation).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第四章](ch04.xhtml#object_detection_and_image_segmentation)中，我们探讨了利用计算机视觉解决目标检测和图像分割问题的方法。任何在[第三章](ch03.xhtml#image_vision)介绍的主干架构都可以在[第四章](ch04.xhtml#object_detection_and_image_segmentation)中使用。
- en: In Chapters [5](ch05.xhtml#creating_vision_datasets) through [9](ch09.xhtml#model_predictions),
    we delve into the details of creating production computer vision machine learning
    models. We go though the standard ML pipeline stage by stage, looking at dataset
    creation in [Chapter 5](ch05.xhtml#creating_vision_datasets), preprocessing in
    [Chapter 6](ch06.xhtml#preprocessing), training in [Chapter 7](ch07.xhtml#training_pipeline),
    monitoring and evaluation in [Chapter 8](ch08.xhtml#model_quality_and_continuous_evaluation),
    and deployment in [Chapter 9](ch09.xhtml#model_predictions). The methods discussed
    in these chapters are applicable to any of the model architectures and use cases
    discussed in Chapters [3](ch03.xhtml#image_vision) and [4](ch04.xhtml#object_detection_and_image_segmentation).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第[5](ch05.xhtml#creating_vision_datasets)到第[9](ch09.xhtml#model_predictions)章，我们深入探讨了创建生产级计算机视觉机器学习模型的细节。我们逐个阶段地进行标准机器学习流程，包括在第[5章](ch05.xhtml#creating_vision_datasets)中的数据集创建，第[6章](ch06.xhtml#preprocessing)中的预处理，第[7章](ch07.xhtml#training_pipeline)中的训练，第[8章](ch08.xhtml#model_quality_and_continuous_evaluation)中的监控和评估，以及第[9章](ch09.xhtml#model_predictions)中的部署。这些章节讨论的方法适用于第[3](ch03.xhtml#image_vision)章和第[4](ch04.xhtml#object_detection_and_image_segmentation)章中讨论的任何模型架构和用例。
- en: In [Chapter 10](ch10.xhtml#trends_in_ml), we address three up-and-coming trends.
    We connect all the steps covered in Chapters [5](ch05.xhtml#creating_vision_datasets)
    through [9](ch09.xhtml#model_predictions) into an end-to-end, containerized ML
    pipeline, then we try out a no-code image classification system that can serve
    for quick prototyping and as a benchmark for more custom models. Finally, we show
    how to build explainability into image model predictions.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.xhtml#trends_in_ml)中，我们讨论了三大新兴趋势。我们将第[5](ch05.xhtml#creating_vision_datasets)至第[9](ch09.xhtml#model_predictions)章涵盖的所有步骤连接成端到端的、容器化的机器学习管道，然后尝试了一个无代码图像分类系统，可用于快速原型设计，也可作为更定制模型的基准。最后，我们展示了如何在图像模型预测中加入可解释性。
- en: In Chapters [11](ch11.xhtml#advanced_vision_problems) and [12](ch12.xhtml#image_and_text_generation),
    we demonstrate how the basic building blocks of computer vision are used to solve
    a variety of problems, including image generation, counting, pose detection, and
    more. Implementations are provided for these advanced use cases as well.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第[11](ch11.xhtml#advanced_vision_problems)章和第[12](ch12.xhtml#image_and_text_generation)章中，我们演示了计算机视觉的基本构建模块如何用于解决各种问题，包括图像生成、计数、姿态检测等。这些高级用例都有相应的实现。
- en: Conventions Used in This Book
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用以下排版约定：
- en: '*Italic*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*Italic*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 指示新术语、URL、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`Constant width`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, data types, environment variables,
    statements, and keywords.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序清单，以及段落内用来指代程序元素如变量名、函数名、数据类型、环境变量、语句和关键字的内容。
- en: '`**Constant width bold**`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`**Constant width bold**`'
- en: Used for emphasis in code snippets, and to show command or other text that should
    be typed literally by the user.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 用于强调代码片段中的内容，以及显示用户应直接输入的命令或其他文本。
- en: '`*Constant width italic*`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`*Constant width italic*`'
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 显示应由用户提供的值或由上下文确定的值。
- en: Tip
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: This element signifies a tip or suggestion.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这一元素表示提示或建议。
- en: Note
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This element signifies a general note.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这一元素表示一般注释。
- en: Warning
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This element signifies a warning.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这一元素表示警告。
- en: Using Code Examples
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用代码示例
- en: Supplemental material (code examples, exercises, etc.) is available for download
    at [*https://github.com/GoogleCloudPlatform/practical-ml-vision-book*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 补充材料（代码示例、练习等）可在[*https://github.com/GoogleCloudPlatform/practical-ml-vision-book*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book)下载。
- en: If you have a technical question or a problem using the code examples, please
    send email to [bookquestions@oreilly.com](mailto:bookquestions@oreilly.com).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有技术问题或者在使用代码示例时遇到问题，请发送邮件至[bookquestions@oreilly.com](mailto:bookquestions@oreilly.com)。
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing a CD-ROM
    of examples from O’Reilly books does require permission. Answering a question
    by citing this book and quoting example code does not require permission. Incorporating
    a significant amount of example code from this book into your product’s documentation
    does require permission.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在帮助您完成工作。一般情况下，如果本书提供了示例代码，您可以在您的程序和文档中使用它。除非您重现了大量代码，否则无需联系我们以获得许可。例如，编写一个使用本书多个代码块的程序不需要许可。出售或分发包含O'Reilly书籍示例的CD-ROM需要许可。引用本书并引用示例代码来回答问题不需要许可。将本书中大量示例代码整合到产品文档中需要许可。
- en: 'We appreciate, but do not require, attribution. An attribution usually includes
    the title, author, publisher, and ISBN. For example: “*Practical Machine Learning
    for Computer Vision*, by Valliappa Lakshmanan, Martin Görner, and Ryan Gillard.
    Copyright 2021 Valliappa Lakshmanan, Martin Görner, and Ryan Gillard, 978-1-098-10236-4.”'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢您的支持，但不要求您署名。署名通常包括标题、作者、出版商和ISBN。例如：“*Practical Machine Learning for Computer
    Vision*，作者Valliappa Lakshmanan、Martin Görner和Ryan Gillard，版权所有2021年Valliappa Lakshmanan、Martin
    Görner和Ryan Gillard，978-1-098-10236-4。”
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [permissions@oreilly.com](mailto:permissions@oreilly.com).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您认为您使用的代码示例超出了合理使用范围或以上述许可授权之外，请随时通过[permissions@oreilly.com](mailto:permissions@oreilly.com)联系我们。
- en: O’Reilly Online Learning
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O’Reilly在线学习
- en: For more than 40 years, O’Reilly Media has provided technology and business
    training, knowledge, and insight to help companies succeed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 超过40年来，O’Reilly Media一直为企业提供技术和业务培训、知识和洞察，帮助它们取得成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*http://oreilly.com*](http://oreilly.com).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专业知识。O’Reilly的在线学习平台为您提供按需访问的实时培训课程、深入学习路径、交互式编码环境以及来自O’Reilly和200多个其他出版商的大量文本和视频。更多信息，请访问[*http://oreilly.com*](http://oreilly.com)。
- en: How to Contact Us
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关本书的评论和问题发送至出版商：
- en: O’Reilly Media, Inc.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly Media, Inc.
- en: 1005 Gravenstein Highway North
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sebastopol, CA 95472
- en: 800-998-9938 (in the United States or Canada)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-998-9938（美国或加拿大）
- en: 707-829-0515 (international or local)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0515（国际或本地）
- en: 707-829-0104 (fax)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104（传真）
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at *[https://oreil.ly/practical-ml-4-computer-vision](https://oreil.ly/practical-ml-4-computer-vision)*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为本书设有网页，列出勘误、示例和任何其他信息。您可以访问此页面[*https://oreil.ly/practical-ml-4-computer-vision*](https://oreil.ly/practical-ml-4-computer-vision)。
- en: Email [bookquestions@oreilly.com](mailto:bookquestions@oreilly.comT) to comment
    or ask technical questions about this book.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过[bookquestions@oreilly.com](mailto:bookquestions@oreilly.com)发送电子邮件以评论或提出关于本书的技术问题。
- en: For news and information about our books and courses, visit [*http://www.oreilly.com*](http://www.oreilly.com).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们的书籍和课程的新闻和信息，请访问[*http://www.oreilly.com*](http://www.oreilly.com)。
- en: 'Find us on Facebook: [*http://facebook.com/oreilly*](http://facebook.com/oreilly)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在Facebook上找到我们：[*http://facebook.com/oreilly*](http://facebook.com/oreilly)
- en: 'Follow us on Twitter: [*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在Twitter上关注我们：[*http://twitter.com/oreillymedia*](http://twitter.com/oreillymedia)
- en: 'Watch us on YouTube: [*http://www.youtube.com/oreillymedia*](http://www.youtube.com/oreillymedia)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上观看我们：[*http://www.youtube.com/oreillymedia*](http://www.youtube.com/oreillymedia)
- en: Acknowledgments
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: We are very thankful to Salem Haykal and Filipe Gracio, our superstar reviewers
    who reviewed every chapter in this book—their eye for detail can be felt throughout.
    Thanks also to the O’Reilly technical reviewers Vishwesh Ravi Shrimali and Sanyam
    Singhal for suggesting the reordering that improved the organization of the book.
    In addition, we would like to thank Rajesh Thallam, Mike Bernico, Elvin Zhu, Yuefeng
    Zhou, Sara Robinson, Jiri Simsa, Sandeep Gupta, and Michael Munn for reviewing
    chapters that aligned with their areas of expertise. Any remaining errors are
    ours, of course.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非常感谢我们的超级审阅员Salem Haykal和Filipe Gracio，他们审阅了本书的每一章节——他们对细节的把握无处不在。同时也感谢O’Reilly的技术审阅员Vishwesh
    Ravi Shrimali和Sanyam Singhal提出的重新排序建议，改进了书籍的组织。此外，我们还要感谢Rajesh Thallam、Mike Bernico、Elvin
    Zhu、Yuefeng Zhou、Sara Robinson、Jiri Simsa、Sandeep Gupta和Michael Munn，他们审阅了与他们专业领域相关的章节。当然，任何剩余的错误都属于我们自己。
- en: We would like to thank Google Cloud users, our teammates, and many of the cohorts
    of the Google Cloud Advanced Solutions Lab for pushing us to make our explanations
    crisper. Thanks also to the TensorFlow, Keras, and Google Cloud AI engineering
    teams for being thoughtful partners.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢Google Cloud用户、我们的团队成员以及Google Cloud高级解决方案实验室的许多同事，他们推动我们使解释更加简洁。同时也感谢TensorFlow、Keras和Google
    Cloud AI工程团队成为深思熟虑的合作伙伴。
- en: Our O’Reilly team provided critical feedback and suggestions. Rebecca Novack
    suggested updating an earlier O’Reilly book on this topic, and was open to our
    recommendation that a practical computer vision book would now involve machine
    learning and so the book would require a complete rewrite. Amelia Blevins, our
    editor at O’Reilly, kept us chugging along. Rachel Head, our copyeditor, and Katherine
    Tozer, our production editor, greatly improved the clarity of our writing.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 O'Reilly 团队提供了重要的反馈和建议。Rebecca Novack 建议更新早期的 O'Reilly 关于这一主题的书籍，并且接受了我们关于实际计算机视觉书籍现在涉及机器学习的建议，因此这本书需要完全重写。我们的编辑
    Amelia Blevins 在 O'Reilly 保持了我们的进展。我们的副本编辑 Rachel Head 和制作编辑 Katherine Tozer 大大提高了我们写作的清晰度。
- en: Finally, and most importantly, thanks also to our respective families for their
    support.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但也是最重要的是，还要感谢我们各自的家人给予的支持。
- en: Valliappa Lakshmanan, Bellevue, WA
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Valliappa Lakshmanan，华盛顿州贝尔维尤
- en: Martin Görner, Bellevue, WA
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Martin Görner，华盛顿州贝尔维尤
- en: Ryan Gillard, Pleasanton, CA
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Ryan Gillard，加利福尼亚州普莱森顿
