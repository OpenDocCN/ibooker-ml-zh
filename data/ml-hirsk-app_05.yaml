- en: Chapter 3\. Debugging Machine Learning Systems for Safety and Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 章。调试机器学习系统以实现安全性和性能
- en: For decades, error or accuracy on holdout test data has been the standard by
    which machine learning models are judged. Unfortunately, as ML models are embedded
    into AI systems that are deployed more broadly and for more sensitive applications,
    the standard approaches for ML model assessment have proven to be inadequate.
    For instance, the overall test data area under the curve (AUC) tells us almost
    nothing about bias and algorithmic discrimination, lack of transparency, privacy
    harms, or security vulnerabilities. Yet, these problems are often why AI systems
    fail once deployed. For acceptable in vivo performance, we simply must push beyond
    traditional in silico assessments designed primarily for research prototypes.
    Moreover, the best results for safety and performance occur when organizations
    are able to mix and match the appropriate cultural competencies and process controls
    described in [Chapter 1](ch01.html#unique_chapter_id_1) with ML technology that
    promotes trust. This chapter presents sections on training, debugging, and deploying
    ML systems that delve into the numerous technical approaches for testing and improving
    in vivo safety, performance, and trust in AI. Note that Chapters [8](ch08.html#unique_chapter_id_8)
    and [9](ch09.html#unique_chapter_id_9) present detailed code examples for model
    debugging.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数十年来，错误或者在保留测试数据上的准确性一直是评判机器学习模型的标准。不幸的是，随着机器学习模型被嵌入到部署更广泛和更敏感的应用程序的人工智能系统中，传统的模型评估方法已经被证明是不足够的。例如，整体测试数据的曲线下面积（AUC）几乎无法告诉我们有关偏见和算法歧视、透明度不足、隐私伤害或安全漏洞的任何信息。然而，这些问题往往是人工智能系统部署后失败的原因。为了获得可接受的实时性能，我们必须超越为研究原型设计的传统模拟评估方法。此外，安全性和性能的最佳结果出现在组织能够混合和匹配在[第
    1 章](ch01.html#unique_chapter_id_1)中描述的适当的文化能力和流程控制与促进信任的机器学习技术时。本章介绍了关于培训、调试和部署机器学习系统的部分，深入探讨了测试和提高实时安全性、性能和AI信任的众多技术方法。请注意，第
    [8](ch08.html#unique_chapter_id_8) 和 [9](ch09.html#unique_chapter_id_9) 章提供了用于模型调试的详细代码示例。
- en: Training
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 培训
- en: The discussion of training ML algorithms begins with reproducibility, because
    without that, it’s impossible to know if any one version of an ML system is really
    any better than another. Data and feature engineering will be addressed briefly,
    and the training section closes by outlining key points for model specification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 关于训练ML算法的讨论从可重复性开始，因为没有这一点，我们无法知道ML系统的任何一个版本是否真的比另一个好。数据和特征工程将简要讨论，并且培训部分结束时概述了模型规范的关键点。
- en: Reproducibility
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可重复性
- en: 'Without reproducibility, we’re building on sand. Reproducibility is fundamental
    to all scientific efforts, including AI. Without reproducible results, it’s very
    hard to know if day-to-day efforts are improving, or even changing, an ML system.
    Reproducibility helps ensure proper implementation and testing, and some customers
    may simply demand it. The following techniques are some of the most common that
    data scientists and ML engineers use to establish a solid, reproducible foundation
    for their ML systems:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 没有可重复性，我们就像在沙上建房子。可重复性是所有科学工作的基础，包括人工智能。没有可重复的结果，很难知道每天的努力是否改进了ML系统，甚至是否改变了它。可重复性有助于确保正确的实施和测试，一些客户可能只是要求它。以下技术是数据科学家和ML工程师用来为他们的ML系统建立坚实可重复基础的一些最常见的技术之一：
- en: Benchmark models
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 基准模型
- en: Benchmark models are important safety and performance tools for training, debugging,
    and deploying ML systems. They’ll be addressed several times in this chapter.
    In the context of model training and reproducibility, we should always build from
    a reproducible benchmark model. This allows for a checkpoint for rollbacks if
    reproducibility is lost, but it also enables real progress. If yesterday’s benchmark
    is reproducible, and today’s gains above and beyond that benchmark are also reproducible,
    that’s real and measurable progress. If system performance metrics bounce around
    before changes are made, and they’re still bouncing around after changes are made,
    we have no idea if our changes helped or hurt.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基准模型是培训、调试和部署机器学习系统的重要安全性和性能工具。本章将多次讨论它们。在模型培训和可重复性的背景下，我们应始终从可重复的基准模型构建。这允许在可重复性丢失时进行回滚的检查点，同时也促进了实际进展。如果昨天的基准模型是可重复的，并且今天的进展超出了那个基准模型，那就是真正可衡量的进展。如果在进行更改之前系统性能指标反弹，而在进行更改后它们仍然反弹，我们就无法知道我们的更改是有帮助还是有害的。
- en: Hardware
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件
- en: As ML systems often leverage hardware acceleration via graphical processing
    units (GPUs) and other specialized system components, hardware is still of special
    interest for preserving reproducibility. If possible, try to keep hardware as
    similar as possible across development, testing, and deployment systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习系统通常通过图形处理单元（GPU）和其他专用系统组件实现硬件加速，硬件仍然是保持可重现性的特别关注点。如果可能，尽量在开发、测试和部署系统中保持硬件尽可能相似。
- en: Environments
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 环境
- en: ML systems always operate in some computational environment, specified by the
    system hardware, system software, and our data and ML software stack. Changes
    in any of these can affect the reproducibility of ML outcomes. Thankfully, tools
    like Python virtual environments and Docker containers that preserve software
    environments have become commonplace in the practice of data science. Additional
    specialized environment management software from [Domino](https://oreil.ly/USwuG),
    [gigantum](https://oreil.ly/1cE7-), [TensorFlow TFX](https://oreil.ly/kHKvx),
    and [Kubeflow](https://oreil.ly/F9ZaL) can provide even more expansive control
    of computational environments.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统始终在某种计算环境中运行，由系统硬件、系统软件以及我们的数据和机器学习软件堆栈指定。这些任何方面的变化都可能影响机器学习结果的可重现性。幸运的是，像Python虚拟环境和Docker容器这样的工具已经成为数据科学实践中的常见工具，用于保存软件环境。来自
    [Domino](https://oreil.ly/USwuG)、[gigantum](https://oreil.ly/1cE7-)、[TensorFlow
    TFX](https://oreil.ly/kHKvx) 和 [Kubeflow](https://oreil.ly/F9ZaL) 的其他专业环境管理软件可以提供更广泛的计算环境控制。
- en: Metadata
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据
- en: Data about data is essential for reproducibility. Track all artifacts associated
    with the model, e.g., datasets, preprocessing steps, data and model validation
    results, human sign-offs, and deployment details. Not only does this allow for
    rolling back to a specific version of a dataset or model, but it also allows for
    detailed debugging and forensic investigations of AI incidents. For an open source
    example of a nice tool for tracking metadata, check out [TensorFlow ML Metadata](https://oreil.ly/gmHkg).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有关数据的数据对于可重现性至关重要。跟踪与模型相关的所有工件，例如数据集、预处理步骤、数据和模型验证结果、人工签名和部署详细信息。这不仅允许回滚到数据集或模型的特定版本，还允许详细调试和AI事件的法证调查。关于跟踪元数据的开源示例工具，可以查看
    [TensorFlow ML Metadata](https://oreil.ly/gmHkg)。
- en: Random seeds
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随机种子
- en: Set by data scientists and engineers in specific code blocks, random seeds are
    the plow horse of ML reproducibility. Unfortunately, they often come with language-
    or package-specific instructions. Seeds can take some time to learn in different
    software, but when combined with careful testing, random seeds enable the building
    blocks of intricate and complex ML systems to retain reproducibility. This is
    a prerequisite for overall reproducibility.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和工程师在特定代码块中设置的随机种子是机器学习可重现性的耕牛。不幸的是，它们通常伴随语言或包特定的说明。在不同的软件中学习种子可能需要一些时间，但结合仔细的测试，随机种子使得复杂的机器学习系统的构建模块能够保持可重现性。这是整体可重现性的先决条件。
- en: Version control
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制
- en: Minor code changes can lead to drastic changes in ML results. Changes to our
    own code plus its dependencies must be tracked in a professional version control
    tool for any hope of reproducibility. Git and GitHub are free and ubiquitous resources
    for software version control, but there are plenty of other options to explore.
    Crucially, data can also be version-controlled with tools like [Pachyderm](https://oreil.ly/DvMCo)
    and [DVC](https://oreil.ly/S59Qv), enabling traceability in changes to data resources.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 小的代码更改可能会导致机器学习结果的 drasti**c** 改变。我们自己的代码及其依赖项的更改必须在专业的版本控制工具中跟踪，以便希望能够重现结果。Git
    和 GitHub 是软件版本控制的免费且无处不在的资源，但还有很多其他探索的选择。关键是，数据也可以通过诸如 [Pachyderm](https://oreil.ly/DvMCo)
    和 [DVC](https://oreil.ly/S59Qv) 等工具进行版本控制，从而追踪数据资源的变化。
- en: Though it may take some experimentation, some combination of these approaches
    and technologies should work to assure a level of reproducibility in our ML systems.
    Once this fundamental safety and performance control is in place, it’s time to
    consider other baseline factors like data quality and feature engineering.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可能需要一些试验，但这些方法和技术的某种组合应该能够保证我们机器学习系统的可重现性水平。一旦确保了这种基本的安全性和性能控制，就可以考虑数据质量和特征工程等其他基线因素了。
- en: Note
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Several topics like benchmarks, anomaly detection, and monitoring are ubiquitous
    in model debugging and ML safety, and they appear in several different sections
    and contexts in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型调试和机器学习安全中，诸如基准测试、异常检测和监控等几个主题无处不在，并且它们出现在本章的几个不同部分和上下文中。
- en: Data Quality
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量
- en: Entire books have been written about data quality and feature engineering for
    ML and ML systems. This short subsection highlights some of the most critical
    aspects of this vast practice area from a safety and performance perspective.
    First and foremost, biases, confounding features, incompleteness, and noise in
    development data form important assumptions and define limitations of our models.
    Other basics, like the size and shape of a dataset, are important considerations.
    ML algorithms are hungry for data. Both small data and wide, sparse data can lead
    to catastrophic performance failures in the real world, because both give rise
    to scenarios in which system performance appears normal on test data, but is simply
    untethered to real-world phenomena. Small data can make it hard to detect underfitting,
    underspecification, overfitting, or other fundamental performance problems. Sparse
    data can lead to overconfident predictions for certain input values. If an ML
    algorithm did not see certain data ranges during training due to sparsity issues,
    most ML algorithms will issue predictions in those ranges with no warning that
    the prediction is based on almost nothing. Fast-forwarding to our chapter case
    discussion, there is simply not enough training video in the world to fill out
    the entire space of example situations that self-driving cars need to learn to
    safely navigate. For example, people crossing the road at night on a bicycle is
    a danger most humans will recognize, but without many frames of labeled video
    of this somewhat rare event, a deep learning system’s ability to handle this situation
    will likely be compromised due to sparsity in training data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关于机器学习和机器学习系统的数据质量和特征工程已经有许多书籍写成。这个简短的小节重点介绍了从安全性和性能角度来看这个广阔实践领域中一些最关键的方面。首先和最重要的是，偏见、混杂特征、不完整性和开发数据中的噪声形成了重要的假设并定义了我们模型的限制。像数据集的大小和形状这样的基础知识也是重要的考虑因素。机器学习算法对数据有很高的需求。小数据和宽松散的数据都可能导致现实世界中的灾难性性能故障，因为两者都会导致系统在测试数据上表现正常，但实际上与现实世界现象没有实质联系。小数据可能使得难以检测到拟合不足、规范不足、过拟合或其他基本性能问题。散乱的数据可能导致对某些输入值的过度自信的预测。如果由于稀疏问题在训练中未能看到某些数据范围，大多数机器学习算法将在这些范围内发布预测，而不会发出基于几乎没有信息的警告。快进到我们章节的案例讨论，世界上根本没有足够的培训视频来填补自动驾驶汽车需要学习以安全驾驶的所有示例情况的整个空间。例如，夜间骑自行车过马路的人是大多数人都会意识到的危险，但如果没有许多带标签的这种相对罕见事件的视频帧，深度学习系统处理这种情况的能力可能会因训练数据的稀缺而受到损害。
- en: A number of other data problems can cause safety worries, such as poor data
    quality leading to entanglement or misrepresentation of important information
    and overfitting, or ML data and model pipeline problems. In the context of this
    chapter, *entanglement* means features, entities, or phenomena in training data
    proxying for other information with more direct relationships to the target (e.g.,
    snow proxying for a Husky in object recognition). Overfitting refers to the memorization
    of noise in training data and the resulting optimistic error estimates, and pipeline
    issues are problems that arise from combining different stages of data preparation
    and modeling components into one prediction-generating executable. [Table 3-1](#data_quality_tables)
    can be applied to most standard ML data to help identify common data quality problems
    with safety and performance implications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他数据问题可能会引起安全担忧，例如数据质量不佳导致纠缠或重要信息的误代表和过拟合，或者机器学习数据和模型管道问题。在本章的背景下，*纠缠* 意味着训练数据中的特征、实体或现象代理其他与目标更直接关系的信息（例如，在物体识别中，雪代理了哈士奇）。过拟合指的是在训练数据中记忆噪声和由此产生的乐观错误估计，而管道问题是由将数据准备和建模组件的不同阶段结合成一个生成预测的可执行文件而引起的问题。[表3-1](#data_quality_tables)
    可应用于大多数标准的机器学习数据，以帮助识别具有安全性和性能影响的常见数据质量问题。
- en: Table 3-1\. Common data quality problems, with symptoms and proposed solutions.
    Adapted from the George Washington University DNSC 6314 (Machine Learning I) class
    notes with permission.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-1。常见的数据质量问题，带有症状和建议的解决方案。根据乔治·华盛顿大学DNSC 6314（机器学习I）课堂笔记进行适应并获得许可。
- en: '| Problem | Common symptoms | Possible solutions |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 常见症状 | 可能的解决方案 |'
- en: '| --- | --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *Biased data*: When a dataset contains information about the phenomenon of
    interest, but that information is consistently and systematically wrong. (See
    [Chapter 4](ch04.html#unique_chapter_id_4) for more information.) | Biased models
    and biased, dangerous, or inaccurate results. Perpetuation of past social biases
    and discrimination. | Consult with domain experts and stakeholders. Apply the
    scientific method and [design of experiment (DOE)](https://oreil.ly/0kDC9) approaches.
    (Get more data. Get better data.) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| *偏倚数据*：当数据集包含感兴趣现象的信息，但这些信息始终且系统性地错误时。（详见[第四章](ch04.html#unique_chapter_id_4)获取更多信息。）
    | 偏倚模型和偏倚、危险或不准确的结果。延续过去的社会偏见和歧视。 | 与领域专家和利益相关者协商。应用科学方法和[实验设计（DOE）](https://oreil.ly/0kDC9)方法。（获取更多数据。获取更好的数据。）
    |'
- en: '| *Character data*: When certain columns, features, or instances are represented
    with strings of characters instead of numeric values. | Information loss. Biased
    models and biased, dangerous, or inaccurate results. Long, intolerable training
    times. | Various numeric encoding approaches (e.g., label encoding, target or
    feature encoding). Appropriate algorithm selection, e.g., tree-based models, naive
    Bayes classification. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| *字符数据*：当某些列、特征或实例使用字符串代替数值表示时。 | 信息丢失。偏倚模型和偏倚、危险或不准确的结果。漫长且无法忍受的训练时间。 | 各种数值编码方法（例如标签编码、目标或特征编码）。适当的算法选择，例如基于树的模型、朴素贝叶斯分类。
    |'
- en: '| *Data leakage*: When information from validation or test partitions leaks
    into training data. | Unreliable or dangerous out-of-domain predictions. Overfit
    models and inaccurate results. Overly optimistic in silico performance estimates.
    | Data governance. Ensuring all dates in training are earlier than in validation
    and test. Ensuring identical identifiers do not occur across partitions. Careful
    application of feature engineering—engineer after partitioning, not before. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| *数据泄漏*：当验证或测试分区的信息泄漏到训练数据中。 | 不可靠或危险的域外预测。过拟合模型和不准确的结果。在硅中过于乐观的性能估计。 | 数据治理。确保所有训练中的日期早于验证和测试中的日期。确保跨分区不出现相同的标识符。在特征工程中谨慎应用——在分区后工程化，而不是之前。
    |'
- en: '| *Dirty data*: A combination of all the issues in this table, very common
    in real-world datasets. | Information loss. Biased models and biased, inaccurate
    results. Long, intolerable training times. Unstable and unreliable parameter estimates
    and rule generation. Unreliable or dangerous out-of-domain predictions. | Combination
    of solution strategies herein. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| *脏数据*：这个表中所有问题的结合，在现实世界的数据集中非常普遍。 | 信息丢失。偏倚模型和偏倚、不准确的结果。漫长且无法忍受的训练时间。不稳定和不可靠的参数估计和规则生成。不可靠或危险的域外预测。
    | 组合此处的解决策略。 |'
- en: '| *Disparate feature scales*: When features, such as age and income, are recorded
    on different scales. | Unreliable parameter estimates, biased models, and biased,
    inaccurate results. | Standardization. Appropriate algorithm selection, e.g.,
    tree-based models. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| *不同特征尺度*：当诸如年龄和收入等特征记录在不同的尺度上时。 | 不可靠的参数估计、偏倚模型和偏倚、不准确的结果。 | 标准化。适当的算法选择，例如基于树的模型。
    |'
- en: '| *Duplicate data*: Rows, instances, or entities that occur more than intended.
    | Biased results due to unintentional overweighting of identical entities during
    training. Biased models, and biased, inaccurate results. | Careful data cleaning
    in consultation with domain experts. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| *重复数据*：出现超过预期的行、实例或实体。 | 因意外权重过高相同实体在训练期间导致的偏倚结果。偏倚模型和偏倚、不准确的结果。 | 与领域专家谨慎进行数据清理。
    |'
- en: '| *Entanglement*: When features, entities, or phenomena in training data proxy
    for other information with more direct relationships to the target (e.g., snow
    proxying for a Husky in object recognition). | Unreliable or dangerous out-of-domain
    predictions. Shortcut learning. | Apply the scientific method and DOE approaches.
    Apply interpretable models and post hoc explanation. In-domain testing. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| *纠缠*：当训练数据中的特征、实体或现象代表与目标直接关系更密切的其他信息时（例如，在对象识别中雪代表哈士奇）。 | 不可靠或危险的域外预测。快捷学习。
    | 应用科学方法和DOE方法。应用可解释的模型和事后解释。在域内进行测试。 |'
- en: '| *Fake or poisoned data*: Data, features, attributes, phenomena, or entities
    that are injected into or manipulated in training data to elicit artificial model
    outcomes. | Unreliable or dangerous out-of-domain predictions. Biased models and
    biased, inaccurate results. | Data governance. Data security. Application of robust
    ML approaches. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| *伪造或毒害数据*：在训练数据中注入或操纵的数据、特征、属性、现象或实体，以引发人工模型结果。 | 不可靠或危险的域外预测。偏倚模型和偏倚、不准确的结果。
    | 数据治理。数据安全。应用稳健的机器学习方法。 |'
- en: '| *High cardinality categorical features*: Features such as postal codes or
    product identifiers that represent many categorical levels of the same attribute.
    | Overfit models and inaccurate results. Long, intolerable compute times. Unreliable
    or dangerous out-of-domain predictions. | Target or feature encoding variants,
    average-by-level (or similar, e.g., median, BLUP). Discretization. Embedding approaches,
    e.g., entity embedding neural networks, factorization machines. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| *高基数分类特征*：例如邮政编码或产品标识符等代表同一属性的许多分类级别的特征。 | 过拟合模型和不准确的结果。漫长且无法容忍的计算时间。不可靠或危险的域外预测。
    | 目标或特征编码变体，按级别平均（或类似，例如中位数，BLUP）。离散化。嵌入方法，例如实体嵌入神经网络，因子分解机。 |'
- en: '| *Imbalanced target*: When one target class or value is much more common than
    others. | Single class model predictions. Biased model predictions. | Proportional
    over- or undersampling. Inverse prior probability weighting. Mixture models, e.g.,
    zero-inflated regression methods. Post hoc adjustment of predictions or decision
    thresholds. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| *不平衡的目标*：当一个目标类别或值比其他类别或值更常见时。 | 单一类别模型预测。偏倚的模型预测。 | 比例过采样或欠采样。逆先验概率加权。混合模型，例如零膨胀回归方法。后验调整预测或决策阈值。
    |'
- en: '| *Incomplete data*: When a dataset does not encode information about the phenomenon
    of interest. When uncollected information confounds model results. | Useless models,
    meaningless or dangerous results. | Consult with domain experts and stakeholders.
    Apply the scientific method and DOE approaches. (Get more data. Get better data.)
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| *不完整数据*：当数据集不编码感兴趣现象的信息时。未收集到的信息干扰模型结果。 | 无用的模型，毫无意义或危险的结果。 | 与领域专家和利益相关者咨询。应用科学方法和DOE方法。（获取更多数据。获取更好的数据。）
    |'
- en: '| *Missing values*: When specific rows or instances are missing information.
    | Information loss. Biased models and biased, inaccurate results. | Imputation.
    Discretization (i.e., binning). Appropriate algorithm selection, e.g., tree-based
    models, naive Bayes classification. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| *缺失值*：特定行或实例缺少信息时。 | 信息丢失。偏倚模型和偏倚、不准确的结果。 | 插补。离散化（即分箱）。适当的算法选择，例如基于树的模型，朴素贝叶斯分类。
    |'
- en: '| *Noise*: Data that fails to encode clear signals for modeling. Data with
    the same input values and different target values. | Unreliable or dangerous out-of-domain
    predictions. Poor performance during training. | Consult with domain experts and
    stakeholders. Apply the scientific method and DOE approaches. (Get more data.
    Get better data.) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| *噪声*：未能为建模提供清晰信号的数据。具有相同输入值但不同目标值的数据。 | 不可靠或危险的域外预测。训练过程中性能不佳。 | 与领域专家和利益相关者咨询。应用科学方法和DOE方法。（获取更多数据。获取更好的数据。）
    |'
- en: '| *Nonnormalized data*: Data in which values for the same entity are represented
    in different ways. | Unreliable out-of-domain predictions. Long, intolerable training
    times. Unreliable parameter estimates and rule generation. | Careful data cleaning
    in consultation with domain experts. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| *非标准化数据*：同一实体的值以不同方式表示的数据。 | 不可靠的域外预测。漫长且无法容忍的训练时间。不可靠的参数估计和规则生成。 | 与领域专家仔细进行数据清洗。
    |'
- en: '| *Outliers*: Rows or instances of data that are strange or unlike others.
    | Biased models and biased, inaccurate results. Unreliable parameter estimates
    and rule generation. Unreliable out-of-domain predictions. | Discretization (i.e.,
    binning). Winsorizing. Robust loss functions, e.g., Huber loss functions. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| *异常值*：与其他数据行或实例不同或奇怪的数据。 | 偏倚模型和偏倚、不准确的结果。不可靠的参数估计和规则生成。不可靠的域外预测。 | 离散化（即分箱）。Winsorizing。稳健损失函数，如Huber损失函数。
    |'
- en: '| *Sparse data*: Data with many zeros or missing values; data that does not
    encode enough information about the phenomenon of interest. | Long, intolerable
    training times. Meaningless or dangerous results due to lack of information, curse
    of dimensionality, or model misspecification. | Feature extraction or matrix factorization
    approaches. Appropriate data representation (i.e., COO, CSR). Application of business
    rules, model assertions, and constraints to make up for illogical model behavior
    learned in sparse regions of training data. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| *稀疏数据*: 数据中有很多零值或缺失值；数据未能编码足够关于感兴趣现象的信息。 | 长时间的、无法忍受的训练时间。由于信息不足、维度诅咒或模型规范错误导致的无意义或危险的结果。
    | 特征提取或矩阵因子分解方法。适当的数据表示（如 COO、CSR）。应用业务规则、模型断言和约束，弥补训练数据稀疏区域中学习到的不合逻辑的模型行为。 |'
- en: '| *Strong multicollinearity (correlation)*: When features have strong linear
    dependencies on one another. | Unstable parameter estimates, unstable rule generation,
    and dangerous or unstable predictions. | Feature selection. Feature extraction.
    L2 regularization. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| *强多重共线性（相关性）*: 特征之间存在强线性依赖关系。 | 不稳定的参数估计、不稳定的规则生成和危险或不稳定的预测。 | 特征选择。特征提取。L2
    正则化。 |'
- en: '| *Unrecognized time and date formats*: Time and date formats, of which there
    are many, that are encoded improperly by data handling or modeling software. |
    Unreliable or dangerous out-of-domain predictions. Unreliable parameter estimates
    and rule generation. Overfit models and inaccurate results. Overly optimistic
    in silico performance estimates. | Careful data cleaning in consultation with
    domain experts. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| *未识别的时间和日期格式*: 时间和日期格式，由数据处理或建模软件不正确编码。 | 不可靠或危险的领域外预测。不可靠的参数估计和规则生成。过拟合模型和不准确的结果。在硅内性能估计过于乐观。
    | 与领域专家协商进行仔细的数据清洗。 |'
- en: '| *Wide data*: Data with many more columns, features, pixels, or tokens than
    rows, instances, images, or documents. *P* >> *N*. | Long, intolerable training
    times. Meaningless or dangerous results due to the curse of dimensionality or
    model misspecification. | Feature selection, feature extraction, L1 regularization,
    models that do not assume *N* >> *P*. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| *宽数据*: 列数、特征数、像素数或标记数远远多于行数、实例数、图像数或文档数。 *P* >> *N*。 | 长时间的、无法忍受的训练时间。由于维度诅咒或模型规范错误导致的无意义或危险的结果。
    | 特征选择、特征提取、L1 正则化、不假设 *N* >> *P* 的模型。 |'
- en: 'There is a lot that can go wrong with data that then leads to unreliable or
    dangerous model performance in high-risk applications. It might be tempting to
    think we can feature engineer our way out of data quality problems. But feature
    engineering is only as good as the thought and code used to perform it. If we’re
    not extremely careful with feature engineering, we’re likely just creating more
    bugs and complexity for ourselves. Common issues with feature engineering in ML
    pipelines include the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在高风险应用中，数据可能出现许多问题，导致模型性能不可靠或危险。或许我们会认为可以通过特征工程解决数据质量问题。但特征工程的好坏取决于执行它的思维和代码质量。如果我们在特征工程中不十分小心，很可能只会给自己带来更多的错误和复杂性。机器学习流水线中常见的特征工程问题包括以下几点：
- en: API or version mismatches between data cleaning, preprocessing, and inference
    packages
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 或版本不匹配数据清洗、预处理和推理包之间
- en: Failing to apply all data cleaning and transformation steps during inference
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理期间未应用所有数据清洗和转换步骤
- en: Failing to readjust for oversampling or undersampling during inference
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理期间未重新调整过采样或欠采样
- en: Inability to handle values unseen during training gracefully or safely during
    inference
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法在推理期间优雅或安全地处理训练时未见过的值
- en: Of course, many other problems can arise in data preparation, feature engineering,
    and associated pipelines, especially as the types of data that ML algorithms can
    accept for training becomes more varied. Tools that detect and address such problems
    are also an important part of the data science toolkit. For Python Pandas users,
    the [ydata-profiling tool](https://oreil.ly/EDNSC) (formerly pandas-profiler)
    is a visual aid that helps to detect many basic data quality problems. R users
    also have options, as discussed by Mateusz Staniak and Przemysław Biecek in [“The
    Landscape of R Packages for Automated Exploratory Data Analysis”](https://oreil.ly/1cBlv).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在数据准备、特征工程和相关流水线中可能会出现许多其他问题，尤其是机器学习算法能接受的数据类型变得更加多样化时。检测和解决这类问题的工具也是数据科学工具包中的重要组成部分。对于
    Python Pandas 用户来说，[ydata-profiling 工具](https://oreil.ly/EDNSC)（以前称为 pandas-profiler）是一种可视化辅助工具，帮助检测许多基本的数据质量问题。R
    用户也有选择的余地，正如 Mateusz Staniak 和 Przemysław Biecek 在 [“R 自动探索性数据分析包的景观”](https://oreil.ly/1cBlv)
    中讨论的那样。
- en: Model Specification for Real-World Outcomes
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现真实世界结果的模型规范
- en: Once our data preparation and feature engineering pipeline is hardened, it’s
    time to think about ML model specification. Considerations for real-world performance
    and safety are quite different from those about getting published or maximizing
    performance on ML contest leaderboards. While measurement of validation and test
    error remain important, bigger questions of accurately representing data and commonsense
    real-world phenomena have the highest priority. This subsection addresses model
    specification for safety and performance by highlighting the importance of benchmarks
    and alternative models, calibration, construct validity, assumptions and limitations,
    proper loss functions, and avoiding multiple comparisons, and by previewing the
    emergent disciplines of robust ML and ML safety and reliability.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的数据准备和特征工程流水线成熟，就该考虑机器学习模型的规范。对于真实世界性能和安全性的考虑与发表论文或在机器学习竞赛排行榜上最大化性能的考虑大不相同。尽管验证和测试错误的测量仍然重要，但更大的问题是准确表示数据和常识真实世界现象具有最高优先级。本小节通过突出基准和替代模型的重要性，校准、结构有效性、假设和限制、适当的损失函数、避免多重比较，并预览强大机器学习和机器学习安全性与可靠性的新兴学科来讨论安全性和性能的模型规范。
- en: Benchmarks and alternatives
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基准和替代方案
- en: When starting an ML modeling task, it’s best to begin with a peer-reviewed training
    algorithm, and ideally to replicate any benchmarks associated with that algorithm.
    While academic algorithms rarely meet all the needs of complex business problems,
    starting from a well-known algorithm and benchmarks provides a baseline assurance
    that the training algorithm is implemented correctly. Once this check is addressed,
    then think about tweaking a complex algorithm to address specific quirks of a
    given problem.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始机器学习建模任务时，最好从经过同行评审的训练算法开始，并在可能的情况下复制与该算法相关联的任何基准。虽然学术算法很少能满足复杂业务问题的所有需求，但从一个知名算法和基准开始可以确保训练算法的实施正确。一旦完成这个检查，再考虑调整复杂算法以解决特定问题的特殊怪癖。
- en: Along with comparison to benchmarks, evaluation of numerous alternative algorithmic
    approaches is another best practice that can improve safety and performance outcomes.
    The exercise of training many different algorithms and judiciously selecting the
    best of many options for final deployment typically results in higher-quality
    models because it increases the number of models evaluated and forces users to
    understand differences between them. Moreover, evaluation of alternative approaches
    is important in complying with a broad set of US nondiscrimination and negligence
    standards. In general, these standards require evidence that different technical
    options were evaluated and an appropriate trade-off between consumer protection
    and business needs was made before deployment.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 除了与基准的比较之外，评估多种替代算法方法是另一个可以提高安全性和性能结果的最佳实践。训练许多不同的算法并谨慎选择最终部署的最佳选项的实践通常会产生更高质量的模型，因为它增加了评估的模型数量，并迫使用户理解它们之间的差异。此外，评估替代方法在符合广泛的美国非歧视和疏忽标准方面非常重要。一般来说，这些标准要求证明评估了不同的技术选项，并在部署之前在消费者保护和业务需求之间做出适当的权衡。
- en: Calibration
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 校准
- en: Just because a number between 0 and 1 pops out the end of a complex ML pipeline
    does not make it a probability. The uncalibrated probabilities generated by most
    ML classifiers usually have to be postprocessed to have any real meaning as probabilities.
    We typically use a scaling process, or even another model, to ensure that when
    a pipeline outputs 0.5, the event in question actually happened to about 50% of
    similar entities in past recorded data. [scikit-learn](https://oreil.ly/LxJbX)
    provides some basic diagnostics and functions for ML classifier calibration. Calibration
    issues can affect regression models too, when the distribution of model outputs
    don’t match the distribution of known outcomes. For instance, many numeric quantities
    in insurance are not normally distributed. Using a default squared loss function,
    instead of loss functions from the gamma or Tweedie family, may result in predictions
    that are not distributed like values from the known underlying data-generating
    process. However we think of calibration, the fundamental issue is that affected
    ML model predictions don’t match to reality. We’ll never make good predictions
    and decisions like this. We need our probabilities to be aligned to past outcome
    rates and we need our regression models to generate predictions of the same distribution
    as the modeled data-generating process.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅因为复杂的机器学习流程的末端弹出一个介于0和1之间的数字，并不意味着它是一个概率。大多数机器学习分类器生成的未校准概率通常需要进行后处理，以赋予其真正的概率意义。我们通常使用一个缩放过程，甚至是另一个模型，来确保当流程输出0.5时，所讨论的事件实际上发生在过去记录数据中大约50%的类似实体中。[scikit-learn](https://oreil.ly/LxJbX)提供了一些用于机器学习分类器校准的基本诊断和函数。当模型输出的分布与已知结果的分布不匹配时，校准问题也会影响回归模型。例如，保险中的许多数值量并不服从正态分布。使用默认的平方损失函数，而不是来自伽马或Tweedie家族的损失函数，可能导致预测不像已知的数据生成过程中的值那样分布。无论我们如何考虑校准，根本问题是受影响的机器学习模型预测与现实不符。我们永远无法做出好的预测和决策。我们需要我们的概率与过去的结果率保持一致，我们需要我们的回归模型生成与建模数据生成过程相同分布的预测值。
- en: Construct validity
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建效度
- en: Construct validity is an idea from social science (from psychometrics and testing,
    in particular). Construct validity means that there is a reasonable scientific
    basis to believe that test performance is indicative of the intended construct.
    Put another way, is there any scientific evidence that the questions and scores
    from a standardized test can predict college or job performance? Why are we bringing
    this up in an ML book? Because ML models are often used for the same purposes
    as psychometric tests these days, and in our opinion, ML models often lack construct
    validity. Worse, ML algorithms that don’t align with fundamental structures in
    training data or in their real-world domains can cause serious incidents.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 构建效度是社会科学（尤其是心理测量学和测试）的一个概念。构建效度意味着有合理的科学依据认为测试表现反映了预期的构建。换句话说，是否有科学证据表明标准化测试的问题和分数能预测大学或工作表现？为什么我们在机器学习书中提到这个？因为这些天机器学习模型经常用于与心理测量测试相同的目的，并且在我们看来，机器学习模型通常缺乏构建效度。更糟糕的是，不与训练数据或其现实世界领域的基本结构相符的机器学习算法可能会引发严重事件。
- en: Consider the choice between an ML model and a linear model, wherein many of
    us simply default to using an ML model. Selecting an ML algorithm for a modeling
    problem comes with a lot of basic assumptions—essentially that high-degree interactions
    and nonlinearity in input features are important drivers of the predicted phenomenon.
    Conversely, choosing to use a linear model implicitly downplays interactions and
    nonlinearities. If those qualities are important for good predictions, they’ll
    have to be specified explicitly for the linear model. In either case, it’s important
    to take stock of how main effects, correlations and local dependencies, interactions,
    nonlinearities, clusters, outliers, and hierarchies in training data, or in reality,
    will be handled by a modeling algorithm, and to test those mechanisms. For optimal
    safety and performance once deployed, dependencies on time, geographical locations,
    or connections between entities in various types of networks must also be represented
    within ML models. Without these clear links to reality, ML models lack construct
    validity and are unlikely to exhibit good in vivo performance. Feature engineering,
    constraints, loss functions, model architectures, and other mechanisms can all
    be used to match a model to its task.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择机器学习模型和线性模型之间时，许多人往往默认使用机器学习模型。选择机器学习算法来解决建模问题基本上是建立在一些基本假设上——即输入特征的高阶交互和非线性是预测现象的重要驱动因素。相反，选择使用线性模型则隐含地减少了交互作用和非线性的影响。如果这些特质对于良好的预测很重要，它们就必须对线性模型进行明确指定。无论哪种情况，重要的是要考虑如何通过建模算法处理训练数据中的主效应、相关性和局部依赖性、交互作用、非线性、簇、异常值以及层次结构，并测试这些机制。一旦部署，为了实现最佳的安全性和性能，必须在机器学习模型中表示与时间、地理位置或不同类型网络实体之间的依赖关系。如果没有这些与现实的明确联系，机器学习模型就会缺乏结构有效性，很可能无法表现出良好的实际性能。特征工程、约束、损失函数、模型架构和其他机制都可以用来使模型适应其任务。
- en: Assumptions and limitations
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假设和限制
- en: Biases, entanglement, incompleteness, noise, ranges, sparsity, and other basic
    characteristics of training data begin to define the assumptions and limitations
    of our models. As discussed, modeling algorithms and architectures also carry
    assumptions and limitations. For example, tree-based models usually can’t extrapolate
    beyond ranges in training data. Hyperparameters for ML algorithms are yet another
    place where hidden assumptions can cause safety and performance problems. Hyperparameters
    can be selected based on domain knowledge or via technical approaches like grid
    search and Bayesian optimization. The key is not to settle for defaults, to choose
    settings systematically, and not to trick ourselves due to multiple comparison
    issues. Testing for independence of errors between rows and features in training
    data or plotting model residuals and looking for strong patterns are general and
    time-tested methods for ensuring some basic assumptions have been addressed. It’s
    unlikely we’ll ever circumvent all the assumptions and limitations of our data
    and model. So we need to document any unaddressed or suspected assumptions and
    limitations in model documentation, and ensure users understand what uses of the
    model could violate its assumptions and limitations. Those would be considered
    out-of-scope or *off-label* uses—just like using a prescription drug in improper
    ways. By the way, construct validity is linked to model documentation and risk
    management frameworks focused on model limitations and assumptions. Oversight
    professionals want practitioners to work through the hypothesis behind their model
    in writing, and make sure it’s underpinned by valid constructs and not assumptions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差、纠缠、不完整、噪音、范围、稀疏性和训练数据的其他基本特征开始定义我们模型的假设和限制。正如讨论的那样，建模算法和架构也具有假设和限制。例如，基于树的模型通常无法在训练数据范围之外进行外推。机器学习算法的超参数是另一个隐藏假设可能引起安全性和性能问题的地方。可以根据领域知识或通过像网格搜索和贝叶斯优化这样的技术方法选择超参数。关键在于不要满足于默认设置，系统地选择设置，并且不要因为多重比较问题而自欺欺人。在训练数据中测试错误之间的独立性或绘制模型残差并寻找强烈模式是确保一些基本假设得到处理的一般和经过时间考验的方法。我们不太可能绕过所有数据和模型的假设和限制。因此，我们需要在模型文档中记录任何未解决或怀疑的假设和限制，并确保用户了解使用模型可能违反其假设和限制的情况。这些将被视为超出范围或*非标签*使用，就像在不恰当的方式使用处方药一样。顺便说一句，建构有效性与模型文档和专注于模型限制和假设的风险管理框架密切相关。监管专业人员希望从业者通过书面方式详细讨论其模型背后的假设，并确保其建立在有效的结构下，而不是假设之上。
- en: Default loss functions
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 默认损失函数
- en: Another often unstated assumption that comes with many learning algorithms involves
    squared loss functions. Many ML algorithms use a squared loss function by default.
    In most instances, a squared loss function, being additive across observations
    and having a linear derivative, is more a matter of mathematical convenience than
    anything else. With modern tools such as [autograd](https://oreil.ly/8icjS), this
    convenience is increasingly unnecessary. We should match our choice of loss function
    with our problem domain.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 许多学习算法伴随的另一个常常未明示的假设涉及平方损失函数。许多机器学习算法默认使用平方损失函数。在大多数情况下，平方损失函数因其在观测值之间的可加性和线性导数而更多是数学上的便利性问题。随着现代工具如[autograd](https://oreil.ly/8icjS)的出现，这种便利性越来越不必要。我们应该根据问题域匹配我们选择的损失函数。
- en: Multiple comparisons
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多重比较
- en: Model selection in ML often means trying many different sets of input features,
    model hyperparameters, and other model settings such as probability cutoff thresholds.
    We often use stepwise feature selection, grid searches, or other methods that
    try many different settings on the same set of validation or holdout data. Statisticians
    might call this a *multiple comparisons* problem and would likely point out that
    the more comparisons we do, the likelier we are to stumble upon some settings
    that simply happen to look good in our validation or holdout set. This is a sneaky
    kind of overfitting where we reuse the same holdout data too many times, select
    features, hyperparameters, or other settings that work well there, and then experience
    poor in vivo performance later on. Hence, [reusable holdout approaches](https://oreil.ly/QJlUV),
    which alter or resample validation or holdout data to make our feature, hyperparameter,
    or other settings more generalizable, are useful.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，模型选择通常意味着尝试许多不同的输入特征集、模型超参数和其他模型设置，如概率截断阈值。我们经常使用逐步特征选择、网格搜索或其他方法，在同一组验证或留存数据上尝试许多不同的设置。统计学家可能会称之为*多重比较*问题，并可能指出，我们进行的比较越多，我们碰巧找到的那些看起来很好的设置就越有可能。这是一种隐蔽的过拟合类型，我们在后续的实际性能中重新使用同一留存数据太多次，选择在那里效果良好的特征、超参数或其他设置，然后经历后期性能不佳。因此，[可重复使用的留存方法](https://oreil.ly/QJlUV)，通过改变或重新采样验证或留存数据，使我们的特征、超参数或其他设置更具普适性，是非常有用的。
- en: The future of safe and robust machine learning
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习的安全和稳健的未来
- en: The new field of [robust ML](https://oreil.ly/1G1Wp) is churning out new algorithms
    with improved stability and security characteristics. Various researchers are
    creating new learning algorithms with guarantees for optimality, like [optimal
    sparse decision trees](https://oreil.ly/gOmtg). And researchers have put together
    excellent [tutorial materials](https://oreil.ly/wC5M1) on ML safety and reliability.
    Today, these approaches require custom implementations and extra work, but hopefully
    these safety and performance advances will be more widely available soon.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 新兴的[稳健机器学习](https://oreil.ly/1G1Wp)领域正在涌现出具有改进稳定性和安全特性的新算法。各种研究人员正在创建具有最优性保证的新学习算法，例如[最优稀疏决策树](https://oreil.ly/gOmtg)。研究人员还整理了关于机器学习安全性和可靠性的优秀[教程材料](https://oreil.ly/wC5M1)。今天，这些方法需要定制实现和额外工作，但希望这些安全性和性能进步很快将更普遍地可用。
- en: Model Debugging
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型调试
- en: Once a model has been properly specified and trained, the next step in the technical
    safety and performance assurance process is testing and debugging. In years past,
    such assessments focused on aggregate quality and error rates in holdout data.
    As ML models are incorporated in public-facing ML systems, and the number of publicly
    reported AI incidents is increasing dramatically, it’s clear that more rigorous
    validation is required. The new field of [model debugging](https://oreil.ly/IY0gU)
    is rising to meet this need. Model debugging treats ML models more like code and
    less like abstract mathematics. It applies a number of testing methods to find
    software flaws, logical errors, inaccuracies, and security vulnerabilities in
    ML models and ML system pipelines. Of course, these bugs must also be fixed when
    they are found. This section explores model debugging in some detail, starting
    with basic and traditional approaches, then outlines the common bugs we’re trying
    to find, moves on to specialized testing techniques, and closes with a discussion
    of bug remediation methods.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被正确规范和训练，技术安全和性能保障流程的下一步就是测试和调试。多年来，这类评估集中在留存数据的整体质量和错误率上。随着机器学习模型被纳入面向公众的机器学习系统，并且公开报道的AI事件数量急剧增加，更加严格的验证显然是必要的。新兴的[模型调试](https://oreil.ly/IY0gU)领域应运而生，以满足这一需求。模型调试将机器学习模型更多地视为代码，而不是抽象数学。它应用一系列测试方法来发现机器学习模型和机器学习系统管道中的软件缺陷、逻辑错误、不准确性和安全漏洞。当然，在发现这些缺陷时，也必须加以修复。本节详细探讨了模型调试，从基本和传统方法开始，然后概述了我们试图发现的常见缺陷，进而介绍了专业的测试技术，并最终讨论了缺陷修复方法。
- en: Note
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In addition to many explainable ML models, the open source package [PiML](https://oreil.ly/1O3hi)
    contains an exhaustive set of debugging tools for ML models trained on structured
    data. Even if it’s not an exact fit for a given use case, it’s a great place to
    learn more and gain inspiration for model debugging.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 除了许多可解释的 ML 模型外，开源软件包[PiML](https://oreil.ly/1O3hi)包含了一套详尽的面向结构化数据训练的 ML 模型调试工具。即使它不完全适用于特定用例，它也是一个学习更多和获得模型调试灵感的好去处。
- en: Software Testing
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件测试
- en: 'Basic software testing becomes much more important when we stop thinking of
    pretty figures and impressive tables of results as the end goal of an ML model
    training task. When ML systems are deployed, they need to work correctly under
    various circumstances. Almost more than anything else related to ML systems, making
    software work is an exact science. Best practices for software testing are well-known
    and can even be made automatic in many cases. At a minimum, mission-critical ML
    systems should undergo the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们停止把漂亮的图表和令人印象深刻的结果表视为 ML 模型训练任务的最终目标时，基本的软件测试变得更加重要。当 ML 系统部署时，它们需要在各种情况下正常工作。在与
    ML 系统相关的几乎所有其他方面中，使软件正常工作是一门精确的科学。软件测试的最佳实践是众所周知的，并且在许多情况下甚至可以自动化。至少，关键任务的 ML
    系统应该进行以下测试：
- en: Unit testing
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试
- en: All functions, methods, subroutines, or other code blocks should have tests
    associated with them to ensure they behave as expected, accurately, and are reproducible.
    This ensures the building blocks of an ML system are solid.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所有函数、方法、子程序或其他代码块都应该有关联的测试，以确保它们按预期行为，并且是可重复的。这确保了 ML 系统的构建模块是坚固的。
- en: Integration testing
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 集成测试
- en: All APIs and interfaces between modules, tiers, or other subsystems should be
    tested to ensure proper communication. API mismatches after backend code changes
    are a classic failure mode for ML systems. Use integration testing to catch this
    and other integration fails.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 应该对所有 API 和模块、层或其他子系统之间的接口进行测试，以确保正确的通信。后端代码更改后的 API 不匹配是 ML 系统的经典失败模式。使用集成测试来捕获此类及其他集成故障。
- en: Functional testing
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 功能测试
- en: Functional testing should be applied to ML system user interfaces and endpoints
    to ensure that they behave as expected once deployed.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 功能测试应该应用于 ML 系统用户界面和端点，以确保它们在部署后的行为符合预期。
- en: Chaos testing
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 混沌测试
- en: Testing under chaotic and adversarial conditions can lead to better outcomes
    when our ML systems face complex and surprising in vivo scenarios. Because it
    can be difficult to predict all the ways an ML system can fail, chaos testing
    can help probe a broader class of failure modes, and provide some cover against
    so-called “unknown unknowns.”
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在混乱和对抗条件下进行测试可以在我们的 ML 系统面对复杂和令人意外的实际场景时取得更好的结果。由于很难预测 ML 系统可能失败的所有方式，混沌测试可以帮助探索更广泛的故障模式，并在所谓的“未知未知”面前提供一些保障。
- en: 'Two additional ML-specific tests should be added into the mix to increase quality
    further:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 还应该将两种额外的 ML 特定测试加入到组合中，以进一步提高质量：
- en: Random attack
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 随机攻击
- en: Random attacks are one way to do chaos testing in ML. Random attacks expose
    ML models to vast amounts of random data to catch both software and math problems.
    The real world is a chaotic place. Our ML system will encounter data for which
    it’s not prepared. Random attacks can decrease those occurrences and any associated
    glitches or incidents.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随机攻击是在 ML 中进行混沌测试的一种方式。随机攻击将 ML 模型暴露于大量随机数据中，以捕捉软件和数学问题。现实世界是一个混乱的地方。我们的 ML
    系统将会遇到它未准备好的数据。随机攻击可以减少这些事件及相关的故障或事件。
- en: Benchmarking
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试
- en: Use benchmarks to track system improvements over time. ML systems can be incredibly
    complex. How can we know if the three lines of code an engineer changes today
    will make a difference in the performance of the system as a whole? If system
    performance is reproducible, and benchmarked before and after changes, it’s much
    easier to answer such questions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基准来跟踪系统随时间的改进。ML 系统可能非常复杂。我们怎么知道工程师今天改变的三行代码对整个系统性能会有影响呢？如果在变更前后对系统性能进行可重复性的基准测试，回答这类问题就容易多了。
- en: ML is software. So, all the testing that’s done on traditional enterprise software
    assets should be done on important ML systems as well. If we don’t know where
    to start with model debugging, we start with random attacks. Readers may be shocked
    at the math or software bugs random data can expose in ML systems. When we can
    add benchmarks to our organization’s continuous integration/continuous development
    (CI/CD) pipelines, that’s the another big step toward assuring the safety and
    performance of ML systems.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ML 是软件。因此，所有传统企业软件资产上进行的测试也应该在重要的 ML 系统上进行。如果我们不知道从哪里开始进行模型调试，我们就从随机攻击开始。读者可能会对随机数据在
    ML 系统中暴露出的数学或软件错误感到震惊。当我们能够将基准添加到组织的持续集成/持续开发（CI/CD）流水线中时，这是向保证 ML 系统的安全性和性能迈出的另一大步。
- en: Note
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Random attacks are probably the easiest and most effective way to get started
    with model debugging. If debugging feels overwhelming, or you don’t know where
    to start, start with random attacks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 随机攻击可能是开始模型调试最简单和最有效的方式。如果调试感觉压倒性，或者不知道从哪里开始，可以从随机攻击开始。
- en: Traditional Model Assessment
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统模型评估
- en: Once we feel confident that the code in our ML systems is functioning as expected,
    it’s easier to concentrate on testing the math of our ML algorithms. Looking at
    standard performance metrics is important. But it’s not the end of the validation
    and debugging process—it’s the beginning. While exact values and decimal points
    matter, from a safety and performance standpoint, they matter much less than they
    do on the leaderboard of an ML contest. When considering in-domain performance,
    it’s less about exact numeric values of assessment statistics, and more about
    mapping in silico performance to in vivo performance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对我们的 ML 系统中的代码有信心，集中精力测试我们的 ML 算法的数学就变得更容易。查看标准性能指标非常重要。但这并不是验证和调试过程的终点，而是开始。在考虑域内性能时，与评估统计数字的精确值相关性较小，更多的是将体外表现映射到体内表现。
- en: If possible, try to select assessment statistics that have a logical interpretation
    and practical or statistical thresholds. For instance, RMSE can be calculated
    for many types of prediction problems, and crucially, it can be interpreted in
    units of the target. Area under the curve, for classification tasks, is bounded
    between 0.5 at the low end and 1.0 at the high end. Such assessment measures allow
    for commonsense interpretation of ML model performance and for comparisons to
    widely accepted thresholds for determining quality. It’s also important to use
    more than one metric and to analyze performance metrics across important segments
    in our data as well as across training, validation, and testing data partitions.
    When comparing performance across segments within training data, it’s important
    that all those segments exhibit roughly equivalent and high-quality performance.
    Amazing performance on one large customer segment, and poor performance on everyone
    else, will look fine in average assessment statistic values like RMSE. However,
    it won’t look fine if it leads to public brand damage due to many unhappy customers.
    Varying performance across segments can also be a sign of underspecification,
    a serious ML bug we’ll dig into in this chapter. Performance across training,
    validation, and test datasets are usually analyzed for underfitting and overfitting
    too. Like model performance, we can look for overfitting and underfitting across
    entire data partitions or across segments.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，尽量选择具有逻辑解释和实际或统计阈值的评估统计数字。例如，对于许多类型的预测问题可以计算 RMSE，并且关键是它可以用目标的单位进行解释。对于分类任务的曲线下面积，其范围在低端为
    0.5，在高端为 1.0。这些评估措施允许对 ML 模型性能进行常识解释，并与确定质量的广泛接受的阈值进行比较。使用多个指标进行分析同样重要，并且要跨数据的重要部分以及跨训练、验证和测试数据分区分析性能指标。在比较训练数据内部段的性能时，重要的是所有这些段展示出大致相等和高质量的性能。在一个大客户段上表现出色，但在其他所有人身上表现不佳，在像
    RMSE 这样的平均评估统计值中看起来还可以。然而，如果由于许多不满意的客户导致公共品牌损害，这看起来并不好。在段间性能变化也可能是不充分规范的迹象，这是我们在本章中将深入探讨的严重
    ML 缺陷。对训练、验证和测试数据集的性能分析通常也是为了分析欠拟合和过拟合。像模型性能一样，我们可以查找整个数据分区或段的过拟合和欠拟合。
- en: 'Another practical consideration related to traditional model assessment is
    selecting a probability cutoff threshold. Most ML models for classification generate
    numeric probabilities, not discrete decisions. Selecting the numeric probability
    cutoff to associate with actual decisions can be done in various ways. While it’s
    always tempting to maximize some sophisticated assessment measure, it’s also a
    good idea to consider real-world impact. Let’s consider a classic lending example.
    Say a probability of default model threshold is originally set at 0.15, meaning
    that everyone who scores less than a 0.15 probability of default is approved for
    a loan, and those that score at the threshold or over are denied. Think through
    questions such as the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 传统模型评估中另一个实际考虑因素是选择概率截断阈值。大多数分类的ML模型生成数值概率，而不是离散决策。选择与实际决策相关联的数值概率截断可以通过多种方式完成。虽然总是诱人地最大化某些复杂的评估指标，但考虑现实世界的影响也是一个好主意。让我们考虑一个经典的贷款例子。假设违约概率模型的阈值最初设置为0.15，这意味着得分低于0.15的每个人都可以批准贷款，而得分达到或超过阈值的人则被拒绝。思考以下问题：
- en: What is the expected monetary return for this threshold? What is the financial
    risk?
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于这个阈值，预期的货币回报是什么？财务风险是多少？
- en: How many people will get the loan at this threshold?
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个阈值下，会有多少人能获得贷款？
- en: How many women? How many minority group members?
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多少女性？多少少数族裔成员？
- en: Outside of the probability cutoff thresholds, it’s always a good idea to estimate
    in-domain performance, because that’s what we really care about. Assessment measures
    are nice, but what matters is making money versus losing money, or even saving
    lives versus taking lives. We can take a first crack at understanding real-world
    value by assigning monetary, or other, values to each cell of a confusion matrix
    for classification problems or to each residual unit for regression problems.
    Do a back-of-the-napkin calculation. Does it look like our model will make money
    or lose money? Once we get the gist of this kind of valuation, we can even incorporate
    value levels for different model outcomes directly into ML loss functions, and
    optimize toward the best-suited model for real-world deployment.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率截断阈值之外，估计领域内表现总是个好主意，因为这是我们真正关心的。评估指标很好，但重要的是赚钱与亏钱，甚至是挽救生命与失去生命。我们可以首先尝试通过为分类问题的混淆矩阵的每个单元或回归问题的每个残差单元分配货币或其他价值来理解现实世界的价值。做一次草稿计算。我们的模型看起来会赚钱还是赔钱？一旦我们掌握了这种估值的要领，我们甚至可以直接将不同模型结果的价值水平纳入ML损失函数中，并优化以适应最适合的实际部署模型。
- en: Error and accuracy metrics will always be important for ML. But once ML algorithms
    are used in deployed ML systems, numeric values and comparisons matter less than
    they do for publishing papers and data science competitions. So, keep using traditional
    assessment measures, but try to map them to in-domain safety and performance.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ML来说，错误和准确度指标始终很重要。但是一旦ML算法在部署的ML系统中使用，数值值和比较的重要性就不如在发表论文和数据科学竞赛中那么重要了。因此，继续使用传统的评估措施，但尝试将它们映射到领域内的安全性和性能。
- en: Common Machine Learning Bugs
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的机器学习bug
- en: We’ve discussed a lack of reproducibility, data quality problems, proper model
    specification, software bugs, and traditional assessment. But there’s still a
    lot more that can go wrong with complex ML systems. When it comes to the math
    of ML, there are a few emergent gotchas and many well-known pitfalls. This subsection
    will discuss bugs, including distributional shifts, epistemic uncertainty, weak
    spots, instability, leakage, looped inputs, overfitting, shortcut learning, underfitting,
    and underspecification.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了无法再现性、数据质量问题、适当的模型规范、软件错误和传统评估。但是复杂ML系统可能出现更多问题。当涉及到ML的数学时，有一些新出现的陷阱和许多众所周知的陷阱。本小节将讨论包括分布偏移、认识不确定性、弱点、不稳定性、泄漏、循环输入、过拟合、快捷学习、欠拟合和规范不足在内的bug。
- en: Distribution shifts
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布偏移
- en: Shifts in the underlying data between different training data partitions and
    after model deployment are common failure modes for ML systems. Whether there’s
    a new competitor entering a market or a devastating worldwide pandemic, the world
    is a dynamic place. Unfortunately, most of today’s ML systems learn patterns from
    static snapshots of training data and try to apply those patterns in new data.
    Sometimes that data is holdout validation or testing partitions. Sometimes it’s
    live data in a production scoring queue. Regardless, drifting distributions of
    input features is a serious bug that must be caught and squashed.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同训练数据分区之间以及模型部署后，底层数据的变化是机器学习系统的常见故障模式。无论是新竞争对手进入市场还是全球性灾害爆发，世界都是一个动态变化的地方。不幸的是，今天大多数的机器学习系统从静态的训练数据快照中学习模式，并尝试在新数据中应用这些模式。有时候这些数据是保留的验证或测试分区数据，有时候是生产评分队列中的实时数据。然而，输入特征分布的漂移是一个严重的错误，必须被捕捉并解决。
- en: Warning
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Systems based on adaptive, online, or reinforcement learning, or that update
    themselves with minimal human intervention, are subject to serious adversarial
    manipulation, error propagation, feedback loop, reliability, and robustness risks.
    While these systems may represent the current state of the art, they need high
    levels of risk management.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 基于自适应、在线或强化学习的系统，或者只需最少人为干预即可更新自身的系统，面临严重的对抗性操纵、误差传播、反馈环路、可靠性和稳健性风险。虽然这些系统可能代表了当前技术水平，但它们需要高水平的风险管理。
- en: When training ML models, watch out for distributional shifts between training,
    cross-validation, validation, or test sets using population stability index (PSI),
    Kolmogorov-Smirnov (KS) tests, t-tests, or other appropriate measures. If a feature
    has a different distribution from one training partition to another, drop it or
    regularize it heavily. Another smart test for distributional shifts to conduct
    during debugging is to simulate distributional shifts for potential deployment
    conditions and remeasure model quality, with a special focus on poor-performing
    rows. If we’re worried about how our model will perform during a recession, we
    can simulate distributional shifts to simulate more late payments, lower cash
    flow, and higher credit balances and then see how our model performs. It’s also
    crucial to record information about distributions in training data so that drift
    after deployment can be detected easily.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练机器学习模型时，要注意训练、交叉验证、验证或测试集之间的分布变化，可以使用人口稳定性指数（PSI）、Kolmogorov-Smirnov（KS）检验、t检验或其他适当的措施。如果一个特征在一个训练分区与另一个训练分区有不同的分布，可以删除它或者进行严格的正则化。在调试期间进行的另一个智能分布变化测试是模拟可能部署条件下的分布变化，并重新测量模型质量，特别关注表现不佳的行。如果我们担心我们的模型在经济衰退期间的表现，我们可以模拟分布变化来模拟更多的延迟付款、较低的现金流和更高的信用余额，然后看看我们的模型的表现如何。在训练数据中记录分布信息非常关键，这样可以轻松检测部署后的漂移。
- en: Epistemic uncertainty and data sparsity
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 认识不确定性和数据稀疏性
- en: '*Epistemic uncertainty* is a fancy way of saying instability and errors that
    arise from a lack of knowledge. In ML, models traditionally gain knowledge from
    training data. If there are parts of our large multidimensional training data
    that are sparse, it’s likely our model will have a high degree of uncertainty
    in that region. Sound theoretical and far-fetched? It’s not. Consider a basic
    credit lending model. We tend to have lots of data about people who already have
    credit cards and pay their bills, and tend to lack data on people who don’t have
    credit cards (their past credit card data doesn’t exist) or don’t pay their bills
    (because the vast majority of customers pay). It’s easy to know to extend credit
    cards to people with high credit scores that pay their bills. The hard decisions
    are about people with shorter or bumpier credit histories. The lack of data for
    the people we really need to know about can lead to serious epistemic uncertainty
    issues. If only a handful of customers, out of millions, are four or five months
    late on their most recent payment, then an ML model simply doesn’t learn very
    much about the best way to handle these people.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*认识论不确定性* 是指由于缺乏知识而产生的不稳定性和错误的一种花哨的说法。在机器学习中，模型传统上通过训练数据获取知识。如果我们庞大的多维训练数据中有稀疏部分，那么在该区域内我们的模型很可能存在高度的不确定性。听起来理论上有道理，却不是空穴来风。考虑一个基本的信贷放款模型。我们倾向于有大量关于已持有信用卡并支付账单的人的数据，但往往缺乏关于没有信用卡的人（因为他们的过去信用卡数据不存在）或者不支付账单的人（因为绝大多数客户都会支付）的数据。轻松知道要向信用评分高且按时支付账单的人发放信用卡。艰难的决策是针对信用历史较短或波动较大的人。对于我们确实需要了解的人群缺乏数据可能会导致严重的认识论不确定性问题。如果数百万客户中只有少数几个客户最近账单拖欠了四五个月，那么机器学习模型对于如何最好地处理这些人几乎没有学习到什么。'
- en: This phenomenon is illustrated in the section [“Underspecification”](#underspecification_ch03_1678201227471)
    (which begins on [​](#underspecification_ch03_1678201227471)), where an example
    model is nonsensical for people who are more than two months late on their most
    recent payment. This region of poor, and likely unstable, performance is sometimes
    known as a *weak spot*. It’s hard to find these weak spots by looking at aggregate
    error or performance measures. This is just one reason of many to test models
    carefully over segments in training or holdout data. It’s also why we pair partial
    dependence and individual conditional expectation plots with histograms in [Chapter 2](ch02.html#unique_chapter_id_2).
    In these plots we can see if model behavior is supported by training data, or
    not. Once we’ve identified a sparse region of data leading to epistemic uncertainty
    and weak spots, we usually have to turn to human knowledge—by constraining the
    form of the model to behave logically based on domain experience, augmenting the
    model with business rules, or potentially handing the cases that fall into sparse
    regions over to human workers to make tough calls.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象在 [“规约性不足”](#underspecification_ch03_1678201227471) 部分有所体现（该部分始于 [​](#underspecification_ch03_1678201227471)），在这里举例说明了模型对于最近账单拖欠超过两个月的人是毫无意义的。这种表现较差且可能不稳定的区域有时被称为
    *弱点*。通过观察总体误差或性能指标很难找到这些弱点。这只是仔细在训练或留置数据段测试模型的众多原因之一。这也是为什么我们在 [第2章](ch02.html#unique_chapter_id_2)
    中将部分依赖和个别条件期望图与直方图配对使用。在这些图表中，我们可以看到模型行为是否受到训练数据的支持。一旦我们确定了导致认识论不确定性和弱点的数据稀疏区域，通常需要依靠人类知识——通过约束模型形式以根据领域经验逻辑行事，通过增加业务规则来扩展模型，或者将落入稀疏区域的案例交给人工工作者做出艰难决策。
- en: Instability
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不稳定性
- en: 'ML models can exhibit instability, or lack of robustness or reliability, in
    the training process or when making predictions on live data. Instability in training
    is often related to small training data, sparse regions of training data, highly
    correlated features within training data, or high-variance model forms, such as
    deep single decision trees. Cross-validation is a typical tool for detecting instability
    during training. If a model displays noticeably different error or accuracy properties
    across cross-validation folds, then we have an instability problem. Training instability
    can often be remediated with better data and lower-variance model forms such as
    decision tree ensembles. Plots of ALE or ICE also tend to reveal prediction instability
    in sparse regions of training data, and instability in predictions can be analyzed
    using sensitivity analysis: perturbations, simulations, stress testing, and adversarial
    example searches.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型在训练过程中或在对实时数据进行预测时可能表现出不稳定性或缺乏鲁棒性和可靠性。训练过程中的不稳定性通常与训练数据量小、训练数据的稀疏区域、训练数据中高度相关的特征或高方差的模型形式（如深度单一决策树）有关。交叉验证是检测训练过程中不稳定性的典型工具。如果模型在交叉验证折叠中显示出显著不同的错误或准确性特性，则存在不稳定性问题。通过更好的数据和低方差的模型形式（如决策树集成）通常可以修复训练不稳定性。ALE或ICE的绘图也倾向于揭示训练数据稀疏区域中的预测不稳定性，可以使用敏感性分析（扰动、模拟、压力测试和对抗性示例搜索）来分析预测的不稳定性。
- en: Note
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'There are two easy ways to think about instability in ML:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中有两种简单的方式来思考不稳定性：
- en: When a small change to input data results in a large change in output data
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当输入数据的微小变化导致输出数据的巨大变化
- en: When the addition of a small amount of training data results in a largely different
    model upon retraining
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当增加少量训练数据导致重新训练时得到一个截然不同的模型时
- en: If probing our response surface or decision boundary with these techniques uncovers
    wild swings in predictions, or our ALE or ICE curves are bouncing around, especially
    in the high or low ranges of feature values, we also have an instability problem.
    This type of instability can often be fixed with constraints and regularization.
    Check out the code examples in [Chapter 8](ch08.html#unique_chapter_id_8) to see
    this remediation in action.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果通过这些技术来探索我们的响应曲面或决策边界揭示出预测出现的剧烈波动，或者我们的ALE或ICE曲线在特征值的高低范围内反复跳动，尤其是在极端值附近，这说明我们也存在不稳定性问题。这种类型的不稳定性通常可以通过约束和正则化来修复。请查看[第
    8 章](ch08.html#unique_chapter_id_8)中的代码示例，看看这些修复方法是如何实施的。
- en: Leakage
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 泄漏
- en: 'Information leakage between training, validation, and test data partitions
    happens when information from validation and testing partitions leaks into a training
    partition, resulting in overly optimistic error and accuracy measurements. Leakage
    can happen for a variety of reasons, including the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练、验证和测试数据分区之间的信息泄漏发生在验证和测试分区的信息泄漏到训练分区，导致过度乐观的错误和准确性测量。泄漏可能由于多种原因而发生，包括以下几点：
- en: Feature engineering
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程
- en: If used incorrectly, certain feature engineering techniques such as imputation
    or principal components analysis may contaminate training data with information
    from validation and test data. To avoid this kind of leakage, perform feature
    engineering uniformly, but separately, across training data partitions. Or ensure
    that information, like means and modes used for imputation, are calculated in
    training data and applied to validation and testing data, and not vice versa.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用不正确，某些特征工程技术，如插补或主成分分析，可能会将验证和测试数据的信息污染到训练数据中。为了避免这种泄漏，需对训练数据分区统一进行特征工程，但是分开操作。或确保在训练数据中计算并应用于验证和测试数据的信息，如插补所需的均值和模式，而不是反过来。
- en: Mistreatment of temporal data
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对时间数据的错误处理
- en: 'Don’t use the future to predict the past. Most data has some association with
    time, whether explicit, as in time-series data, or some other implicit relationship.
    Mistreating or breaking this relationship with random sampling is a common cause
    of leakage. If we’re dealing with data where time plays a role, time needs to
    be used in constructing model validation schemes. The most basic rule is that
    the earliest data should be in training partitions while later data should be
    divided into validation and test partitions, also according to time. A solid (and
    free) resource for time-series forecasting best practices is the text [*Forecasting:
    Principles and Practice* (OTexts)](https://oreil.ly/R2y6N).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '不要用未来来预测过去。大多数数据都与时间有些关联，无论是显性的，如时间序列数据，还是其他隐含的关系。误用或打破这种关系会导致常见的信息泄漏问题。如果我们处理的是时间相关的数据，模型验证方案中必须使用时间。最基本的规则是，最早的数据应该用于训练分区，而较晚的数据应该根据时间分成验证和测试分区。关于时间序列预测最佳实践的一个可靠（并且免费）资源是[*Forecasting:
    Principles and Practice* (OTexts)](https://oreil.ly/R2y6N)。'
- en: Multiple identical entities
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 多个相同的实体
- en: Sometimes the same person, financial or computing transaction, or other modeled
    entity will be in multiple training data partitions. When this occurs, care should
    be taken to ensure that ML models do not memorize characteristics of these individuals
    then apply those individual-specific patterns to different entities in new data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有时相同的个人、财务或计算交易，或其他建模实体会出现在多个训练数据分区中。当这种情况发生时，必须小心确保机器学习模型不会记住这些个体的特征，然后将这些个体特定的模式应用于新数据中的不同实体。
- en: 'Keeping an untouched, time-aware holdout set for an honest estimate of real-world
    performance can help with many of these different leakage bugs. If error or accuracy
    on such a holdout set looks a lot less rosy than on partitions used in model development,
    we might have a leakage problem. More complex modeling schemes involving stacking,
    gates, or bandits can make leakage much harder to prevent and detect. However,
    a basic rule of thumb still applies: do not use data involved in learning or model
    selection to make realistic performance assessments. Using stacking, gates, or
    bandits means we need more holdout data for the different stages of these complex
    models to make an accurate guess at in vivo quality. More general controls such
    as careful documentation of data validation schemes and model monitoring in deployment
    are also necessary for any ML system.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 保留一个未触及的、具有时间意识的留出集，可以帮助诚实估计真实世界性能中的许多信息泄漏问题。如果这样的留出集上的误差或准确性看起来比在模型开发中使用的分区上要不那么乐观，我们可能会有信息泄漏问题。更复杂的建模方案，包括堆叠、门控或赌博，可以使信息泄漏变得更难以预防和检测。然而，一个基本的经验法则仍然适用：不要使用在学习或模型选择中涉及的数据来进行真实性能评估。使用堆叠、门控或赌博意味着我们需要更多的留出数据来为这些复杂模型的不同阶段做出准确的实时质量猜测。对数据验证方案的仔细记录和部署中的模型监控等更一般的控制也对任何机器学习系统都是必要的。
- en: Looped inputs
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环输入
- en: As ML systems are incorporated into broader digitalization efforts, or implemented
    as part of larger decision support efforts, multiple data-driven systems often
    interact. In these cases, error propagation and feedback loop bugs can occur.
    Error propagation occurs when small errors in one system cause or amplify errors
    in another system. Feedback loops are a way an ML system can fail by being right.
    Feedback loops occur when an ML system affects its environment and then those
    effects are reincorporated into system training data. Examples of feedback loops
    include when predictive policing leads to overpolicing of certain neighborhoods
    or when employment algorithms intensify diversity problems in hiring by continually
    recommending correct, but nondiverse, candidates. Dependencies between systems
    must be documented and deployed models must be monitored so that debugging efforts
    can detect error propagation or feedback loop bugs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习系统被纳入更广泛的数字化努力中，或者作为更大决策支持努力的一部分实施，多个数据驱动系统通常会互相交互。在这些情况下，误差传播和反馈循环错误可能会发生。误差传播发生在一个系统中的小错误导致或放大另一个系统中的错误时。反馈循环是一种机器学习系统可能因正确而失败的方式。反馈循环发生在机器学习系统影响其环境，然后这些影响重新纳入系统训练数据时。反馈循环的例子包括预测性警务导致对某些社区的过度警务，或者就业算法通过不断推荐正确但非多样化的候选人而加剧了招聘中的多样性问题。系统之间的依赖关系必须记录，部署的模型必须监控，以便调试工作可以检测到误差传播或反馈循环错误。
- en: Overfitting
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合
- en: 'Overfitting happens when a complex ML algorithm memorizes too much specific
    information from training data, but does not learn enough generalizable concepts
    to be useful once deployed. Overfitting is often caused by high-variance models,
    or models that are too complex for the data at hand. Overfitting usually manifests
    in much better performance on training data than on validation, cross-validation,
    and test data partitions. Since overfitting is a ubiquitous problem, there are
    many possible solutions, but most involve decreasing the variance in our chosen
    model. Examples of these solutions include the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当复杂的机器学习算法从训练数据中过多地记忆特定信息，却未学习到足够通用概念以便部署后发挥作用时，就会出现过拟合问题。过拟合通常由高方差模型或对于手头数据来说过于复杂的模型引起。过拟合通常表现为在训练数据上的性能明显优于验证、交叉验证和测试数据分区上的性能。由于过拟合是一个普遍存在的问题，有许多可能的解决方案，但大多数都涉及减少所选模型的方差。这些解决方案的示例包括：
- en: Ensemble models
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型
- en: Ensemble techniques, particularly bootstrap aggregation (i.e., bagging) and
    gradient boosting are known to reduce error from single high-variance models.
    So, we try one of these ensembling approaches if we encounter overfitting. Just
    keep in mind that when switching from one model to many, we can decrease overfitting
    and instability, but we’ll also likely lose interpretability.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 集成技术，特别是自举聚合（即装袋）和梯度提升已知可以减少单个高方差模型的错误。因此，如果遇到过拟合问题，我们可以尝试其中一种集成方法。请记住，当从单一模型切换到多模型时，虽然可以减少过拟合和不稳定性，但也可能会失去解释性。
- en: Reducing architectural complexity
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 减少架构复杂性
- en: Neural networks can have too many hidden layers or hidden units. Ensemble models
    can have too many base learners. Trees can be too deep. If we think we’re observing
    overfitting, we make our model architecture less complex.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可能有太多的隐藏层或隐藏单元。集成模型可能有太多的基学习器。树可能太深。如果我们认为观察到了过拟合现象，我们会使我们的模型架构更简单。
- en: Regularization
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化
- en: Regularization refers to many sophisticated mathematical approaches for reducing
    the strength, complexity, or number of learned rules or parameters in an ML model.
    In fact, many types of ML models now incorporate multiple options for regularization,
    so we make sure we employ these options to decrease the likelihood of overfitting.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化指的是许多复杂的数学方法，用于减少机器学习模型中学习规则或参数的强度、复杂性或数量。事实上，现在许多类型的机器学习模型都包含多种正则化选项，因此我们确保使用这些选项来减少过拟合的可能性。
- en: Simpler hypothesis model families
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单的假设模型家族
- en: Some ML models will be more complex than others out-of-the-box. If our neural
    network or GBM looks to be overfit, we can try a less complex decision tree or
    linear model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习模型在初始状态下可能比其他模型更复杂。如果我们的神经网络或梯度提升机（GBM）看起来出现了过拟合现象，我们可以尝试使用更简单的决策树或线性模型。
- en: Overfitting is traditionally seen as the Achilles’ heel of ML. While it is one
    of the most frequently encountered bugs, it’s also just one of many possible technical
    risks to consider from a safety and performance perspective. As with leakage,
    as ML systems become more complex, overfitting becomes harder to detect. Always
    keep an untouched holdout set with which to estimate real-world performance before
    deployment. More general controls like documentation of validation schemes, model
    monitoring, and A/B testing of models on live data also need to be applied to
    prevent overfitting.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合传统上被视为机器学习的致命弱点。虽然这是最常见的错误之一，但从安全性和性能角度考虑，它只是许多可能的技术风险之一。与信息泄漏一样，随着机器学习系统变得更加复杂，过拟合变得更难检测。在部署之前，始终保留一组未触及的留存数据集，以评估真实世界的性能。还需要应用更通用的控制措施，如验证方案的文档化、模型监控和在实时数据上进行模型的A/B测试，以防止过拟合。
- en: Shortcut learning
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 快速学习
- en: Shortcut learning occurs when a complex ML system is thought to be learning
    and making decisions about one subject, say anomalies in lung scans or job interview
    performance, but it’s actually learned about some simpler related concept, such
    as machine identification numbers or Zoom video call backgrounds. Shortcut learning
    tends to arise from entangled concepts in training data, a lack of construct validity,
    and failure to adequately consider and document assumptions and limitations. We
    use explainable models and explainable AI techniques to understand what learned
    mechanisms are driving model decisions, and we make sure we understand how our
    ML system makes scientifically valid decisions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 快捷学习发生在复杂的机器学习系统被认为是在学习和决策某一主题，比如肺部扫描异常或工作面试表现，但实际上它学习了一些更简单相关的概念，比如机器识别号或Zoom视频通话背景。快捷学习往往源于训练数据中纠缠的概念、缺乏构造效度以及未能充分考虑和记录假设和限制。我们使用可解释模型和可解释人工智能技术来理解驱动模型决策的学习机制，并确保我们理解我们的机器学习系统如何做出科学上有效的决策。
- en: Underfitting
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**欠拟合**'
- en: If someone tells us a statistic about a set of data, we might wonder how much
    data that statistic is based on, and whether that data was of high enough quality
    to be trustworthy. What if someone told us they had millions, billions, or even
    trillions of statistics for us to consider? They would need lots of data to make
    a case that all these statistics were meaningful. Just like averages and other
    statistics, each parameter or rule within an ML model is learned from data. Big
    ML models need lots of data to learn enough to make their millions, billions,
    or trillions of learned mechanisms meaningful. Underfitting happens when a complex
    ML algorithm doesn’t have enough training data, constraints, or other input information,
    and it learns just a few generalizable concepts from training data, but not enough
    specifics to be useful when deployed. Underfitting can be diagnosed by poor performance
    on both training and validation data. Another piece of evidence for underfitting
    is if our model residuals have significantly more structure than random noise.
    This suggests that there are meaningful patterns in the data going undetected
    by our model, and it’s another reason we examine our residuals for model debugging.
    We can mitigate underfitting by increasing the complexity of our models or, preferably,
    providing more training data. We can also provide more input information in other
    ways, such as new features, Bayesian priors applied to our model parameter distributions,
    or various types of architectural or optimization constraints.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人告诉我们关于一组数据的统计信息，我们可能会想知道这些统计数据基于多少数据，以及这些数据的质量是否足够高，可以信任。如果有人告诉我们，他们有数百万、数十亿甚至数万亿的统计数据供我们考虑，他们需要大量数据来证明所有这些统计数据是有意义的。就像平均数和其他统计数据一样，机器学习模型中的每个参数或规则都是从数据中学习的。大型机器学习模型需要大量数据才能学习到足够多的内容，使其数百万、数十亿或数万亿学习机制具有意义。当复杂的机器学习算法缺乏足够的训练数据、约束或其他输入信息时，就会发生欠拟合，它只从训练数据中学习到了一些可泛化的概念，但没有足够的具体内容在部署时发挥作用。欠拟合可以通过在训练和验证数据上表现不佳来诊断。另一个欠拟合的证据是如果我们的模型残差比随机噪声具有显著更多的结构。这表明数据中存在着我们的模型未能检测到的有意义模式，这也是我们检查模型调试的残差的另一个原因。我们可以通过增加模型复杂性或更好地提供训练数据来减轻欠拟合问题。我们还可以通过其他方式提供更多的输入信息，例如新特征、应用于模型参数分布的贝叶斯先验，或各种类型的架构或优化约束。
- en: Underspecification
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**不充分规范**'
- en: Forty researchers recently published [“Underspecification Presents Challenges
    for Credibility in Modern Machine Learning”](https://oreil.ly/Da9g0). This paper
    gives a name to a problem that has existed for decades, *underspecification*.
    Underspecification arises from the core ML concept of the multiplicity of good
    models, sometimes also called the Rashomon effect. For any given dataset, there
    are many accurate ML models. How many? Vastly more than human technicians have
    any chance of understanding in most cases. While we use validation data to select
    a good model from many models attempted during training, validation-based model
    selection is not a strong enough control to ensure we picked the best model—or
    even a serviceable model—for deployment. Say that for some dataset there are a
    million total good ML models based on training data and a large number of potential
    hypothesis models. Selecting by validation data may cut that number of models
    down to a pool of one hundred total models. Even in this simple scenario, we’d
    still only have a 1 in 100 chance of picking the right model for deployment. How
    can we increase those odds? By injecting domain knowledge into ML models. By combining
    validation-based model selection with domain-informed constraints, we have a much
    better chance at selecting a viable model for the job at hand.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 四十位研究人员最近发表了[《“欠规范性对现代机器学习的可信度构成挑战”》](https://oreil.ly/Da9g0)。这篇论文为几十年来存在的问题命名，即*欠规范性*。欠规范性源于核心的机器学习概念——多个优良模型的存在，有时也被称为拉肖蒙效应。对于任何给定的数据集，存在许多准确的机器学习模型。有多少呢？通常远远超过人类技术人员能够理解的范围。虽然我们使用验证数据从训练期间尝试的多个模型中选择一个好的模型，但基于验证数据的模型选择并不足以确保我们选择了最佳模型——甚至是一个可用的模型用于部署。假设某个数据集有一百万个基于训练数据的好的机器学习模型以及大量的潜在假设模型。通过验证数据选择可能会将这些模型减少到一百个模型的池子中。即使在这种简单的情况下，我们仍然只有百分之一的机会选择正确的模型用于部署。我们如何增加这些机会？通过将领域知识注入机器学习模型中。通过将基于验证的模型选择与领域信息约束相结合，我们有更大的机会选择一个适合当前任务的可行模型。
- en: Happily, testing for underspecification can be fairly straightforward. One major
    symptom of underspecification is model performance that’s dependent on computational
    hyperparameters that are not related to the structure of the domain, data, or
    model. If our model’s performance varies due to random seeds, number of threads
    or GPUs, or other computational settings, our model is probably underspecified.
    Another test for underspecification is illustrated in [Figure 3-1](#segmented_error).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，测试欠规范性可能相对简单。欠规范性的一个主要症状是模型性能依赖于与领域结构、数据或模型无关的计算超参数。如果我们的模型性能因随机种子、线程或GPU数量或其他计算设置而变化，那么我们的模型可能是欠规范的。另一个测试欠规范性的方法在[图 3-1](#segmented_error)中有所说明。
- en: '[Figure 3-1](#segmented_error) displays several error and accuracy measures
    across important segments in the example training data and model. Here, a noticeable
    shift in performance for segments defined by higher values of the important feature
    `PAY_0` points to a potential underspecification problem, likely due to data sparsity
    in that region of the training data. (Performance across segments defined by `SEX`
    is more equally balanced, which a good sign from a bias-testing perspective, but
    certainly not the only test to be considered for bias problems.) Fixing underspecification
    tends to involve applying real-world knowledge to ML algorithms. Such domain-informed
    mechanisms include graph connections, monotonic constraints, interaction constraints,
    beta constraints, or other architectural constraints.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-1](#segmented_error)展示了在示例训练数据和模型的重要部分中的多个错误和准确度测量。在这里，对于由重要特征`PAY_0`的高值定义的段落，性能的显著变化指向潜在的欠规范问题，这很可能是由于该区域训练数据的数据稀疏性所致。（在由`SEX`定义的段落中，性能更加平衡，这是从偏见测试的角度来看一个好迹象，但这当然不是检测偏见问题的唯一测试。）修复欠规范性通常涉及将现实世界的知识应用于机器学习算法中。这样的领域知识机制包括图连接、单调约束、交互约束、beta约束或其他架构约束。'
- en: '![](assets/mlha_0301.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/mlha_0301.png)'
- en: Figure 3-1\. Analyzing accuracy and errors across key segments is an important
    debugging method for detecting bias, underspecification, and other serious ML
    bugs ([digital, color version](https://oreil.ly/URzZG))
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. 分析关键部分的准确度和错误是检测偏见、欠规范性和其他严重机器学习错误的重要调试方法（[数字，彩色版本](https://oreil.ly/URzZG)）。
- en: Note
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Nearly all of the bugs discussed in this section, chapter, and book can affect
    certain segments of data more than others. For optimal performance, it’s important
    to test for weak spots (performance quality), overfitting and underfitting, instability,
    distribution shifts, and other issues across different kinds of segments in training,
    validation, and test or holdout data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本节、本章以及本书中讨论的几乎所有错误都可能会对某些数据段产生更大影响。为了获得最佳性能，重要的是在训练、验证和测试或留存数据的不同类型段落中测试弱点（性能质量）、过度拟合和欠拟合、不稳定性、分布变化以及其他问题。
- en: Each of the ML bugs discussed in this subsection has real-world safety and performance
    ramifications. A unifying theme across these bugs is that they cause systems to
    perform differently than expected when deployed in vivo and over time. Unpredictable
    performance leads to unexpected failures and AI incidents. Using the knowledge
    of potential bugs and bug detection methods discussed here to ensure estimates
    of validation and test performance are relevant to deployed performance will go
    a long way toward preventing real-world incidents. Now that we know what bugs
    we’re looking for, in terms of software, traditional assessment, and ML math,
    next we’ll address how to find these bugs with residual analysis, sensitivity
    analysis, benchmark models, and other testing and monitoring approaches.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节讨论的每个机器学习错误都具有现实世界的安全性和性能影响。这些错误的一个统一主题是，它们导致系统在实际部署和长期使用中表现与预期不同。不可预测的性能会导致意外故障和人工智能事件。利用本文讨论的潜在错误和错误检测方法的知识，确保验证和测试性能的估计与部署性能相关，将有助于预防现实世界中的事件。现在我们知道要找什么错误，从软件、传统评估和机器学习数学的角度出发，接下来我们将讨论如何通过残差分析、敏感性分析、基准模型以及其他测试和监控方法来发现这些错误。
- en: Residual Analysis
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 残差分析
- en: Residual analysis is another type of traditional model assessment that can be
    highly effective for ML models and ML systems. At its most basic level, residual
    analysis means learning from mistakes. That’s an important thing to do in life,
    as well as in organizational ML systems. Moreover, residual analysis is a tried-and-true
    model diagnostic technique. This subsection will use an example and three generally
    applicable residual analysis techniques to apply this established discipline to
    ML.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 残差分析是另一种传统模型评估的类型，对于机器学习模型和系统非常有效。在其最基本的层面上，残差分析意味着从错误中学习。这不仅在生活中非常重要，也在组织的机器学习系统中同样重要。此外，残差分析是一种经过时间考验的模型诊断技术。本小节将使用一个例子和三种通用的残差分析技术来将这一成熟的学科应用于机器学习。
- en: Note
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We use the term *residual* to mean an appropriate measurement of error, somewhat
    synonymous to the model’s loss measurement. We understand we’re not the using
    it in the strictly defined ŷ[i]–y[i] sense. We use this term to reinforce the
    importance and long history of residual analysis in regression diagnostics and
    to highlight its basic absence from common ML workflows.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用术语*残差*来表示适当的误差测量，与模型损失测量有些同义。我们明白这里并非严格按照定义的 ŷ[i]–y[i] 的意义来使用它。我们使用这个术语来强调残差分析在回归诊断中的重要性和悠久历史，并突显其在常见的机器学习工作流中的基本缺失。
- en: Note that in the following sections readers may see demographic features in
    the dataset, like `SEX`, that are used for bias testing. For the most part, this
    chapter treats the example credit lending problem as a general predictive modeling
    exercise, and does not consider applicable fair lending regulations. See Chapters
    [4](ch04.html#unique_chapter_id_4) and [10](ch10.html#unique_chapter_id_10) for
    in-depth discussions relating to bias management in ML that also address some
    legal and regulatory concerns.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在接下来的章节中，读者可能会看到数据集中的人口特征，比如`SEX`，用于偏见测试。在很大程度上，本章将把例子信贷放贷问题视为一般的预测建模练习，并不考虑适用的公平放贷法规。关于机器学习中偏见管理以及一些法律和监管问题的深入讨论，请参阅第[4](ch04.html#unique_chapter_id_4)章和第[10](ch10.html#unique_chapter_id_10)章。
- en: Analysis and visualizations of residuals
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 残差分析和可视化
- en: Plotting overall and segmented residuals and examining them for telltale patterns
    of different kinds of problems is a long-running model diagnostic technique. Residual
    analysis can be applied to ML algorithms to great benefit with a bit of creativity
    and elbow grease. Simply plotting residuals for an entire dataset can be helpful,
    especially to spot outlying rows causing very large numeric errors or to analyze
    overall trends in errors. However, breaking residual values and plots down by
    feature and level is likely to be more informative. Even if we have a lot features
    or features with many categorical levels, we’re not off the hook. Start with the
    most important features and their most common levels. Look for strong patterns
    in residuals that violate the assumptions of our model. Many types of residuals
    should be randomly distributed, indicating that the model has learned all the
    important information from the data, aside from irreducible noise. If we spot
    strong patterns or other anomalies in residuals that have been broken down by
    feature and level, we first determine whether these errors arise from data, and
    if not, we can use XAI techniques to track down issues in our model. Residual
    analysis is considered standard practice for important linear regression models.
    ML models are arguably higher risk and more failure prone, so they need even more
    residual analysis.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对整体和分段残差进行绘图，并检查它们是否显示出不同问题类型的典型模式，是长期以来的模型诊断技术。残差分析可应用于机器学习算法，通过一些创造性工作和努力，能带来很大的益处。简单地绘制整个数据集的残差可以很有帮助，特别是用于发现导致极大数值误差的异常行或分析误差的整体趋势。然而，按特征和级别分解残差值和图形可能更具信息性。即使我们有很多特征或特征具有多个分类水平，我们也不能掉以轻心。从最重要的特征及其最常见的级别开始。寻找违反模型假设的残差中的强烈模式。许多类型的残差应随机分布，表明模型已从数据中学习到所有重要信息，除了不可避免的噪音。如果我们在按特征和级别分解的残差中发现强烈模式或其他异常情况，我们首先确定这些错误是否来自数据，如果不是，则可以使用XAI技术追踪模型中的问题。残差分析被认为是重要的线性回归模型的标准做法。机器学习模型可能面临更高的风险和更多的故障，因此它们需要更多的残差分析。
- en: Modeling residuals
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 残差建模
- en: Modeling residuals with interpretable models is another great way to learn more
    about the mistakes our ML system could make. In [Figure 3-2](#resid_model), we’ve
    trained a single, shallow decision tree on the residuals from a more complex model
    associated with customers who missed a credit card payment.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用可解释模型对残差建模是了解我们的机器学习系统可能出错方式的另一种好方法。在[图3-2](#resid_model)中，我们对从与错过信用卡付款相关的更复杂模型的残差进行了单一且浅层的决策树训练。
- en: This decision tree encodes rules that describe how the more complex model is
    wrong. For example, we can see the model generates the largest numeric residuals
    when someone misses a credit card payment, but looks like a great customer. When
    someone’s most recent repayment status (`PAY_0`) is less than 0.5, their second
    most recent payment amount (`PAY_AMT2`) is greater than or equal to 2,802.50,
    their fourth most recent repayment status (`PAY_4`) is less than 1, and their
    credit limit is greater than or equal to 256,602, we see logloss residuals of
    2.71, on average. That’s a big error rate that drags down our overall performance,
    and that can have bias ramifications if we make too many false negative guesses
    about already favored demographic groups.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该决策树编码了描述更复杂模型错误的规则。例如，我们可以看到当某人错过信用卡付款时，但看起来是一个很好的客户时，模型生成的最大数值残差。当某人的最近的还款状态（`PAY_0`）小于0.5，他们第二最近的还款金额（`PAY_AMT2`）大于或等于2,802.50，他们第四最近的还款状态（`PAY_4`）小于1，并且他们的信用额度大于或等于256,602时，我们看到平均对数损失残差为2.71。这是一个大的错误率，会拉低我们的整体性能，并且如果我们对已受青睐的人口群体做出太多的误判，可能会产生偏见后果。
- en: Another intriguing use for the tree is to create model assertions, real-time
    business rules about model predictions, that could be used to flag when a wrong
    decision is occurring as it happens. In some cases, the assertions might simply
    alert model monitors that a wrong decision is likely being issued, or model assertions
    could involve corrective action, like routing this row of data to a more specialized
    model or to human case workers.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的用途是使用决策树创建模型断言，即关于模型预测的实时业务规则，可以在发生错误时进行标记。在某些情况下，这些断言可能仅仅是警示模型监控器可能正在发布错误决策，或者模型断言可能涉及纠正措施，例如将这行数据路由到更专业的模型或人工案件工作者。
- en: '![mlha 0302](assets/mlha_0302.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0302](assets/mlha_0302.png)'
- en: Figure 3-2\. An interpretable decision tree model for customers who missed credit
    card payments
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 一个可解释的决策树模型，用于错过信用卡支付的客户
- en: Local contribution to residuals
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对残差的本地贡献
- en: Plotting and modeling residuals are older techniques that are well-known to
    skilled practitioners. A more recent breakthrough has made it possible to calculate
    accurate Shapley value contributions to model errors. This means for any feature
    or row of any dataset, we can now know which features are driving model predictions,
    and which features are driving model errors. What this advance really means for
    ML is yet to be determined, but the possibilities are certainly intriguing. One
    obvious application for this new Shapley value technique is to compare feature
    importance for predictions to feature importance for residuals, as in [Figure 3-3](#resid_contribs).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制和建模残差是精通者熟悉的更古老的技术。最近的一个突破使得可以计算出对模型错误的 Shapley 值贡献。这意味着对于任何数据集的任何特征或行，我们现在可以知道哪些特征驱动了模型的预测，哪些特征驱动了模型的错误。这个进步对于
    ML 究竟意味着什么，尚有待确定，但可能性确实令人着迷。这种新的 Shapley 值技术的一个明显应用是比较预测的特征重要性和残差的特征重要性，如 [图 3-3](#resid_contribs)
    中所示。
- en: '![mlha 0303](assets/mlha_0303.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0303](assets/mlha_0303.png)'
- en: Figure 3-3\. A comparison of Shapley feature importance for predictions and
    for model errors ([digital, color version](https://oreil.ly/k6nDo))
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. 对预测和模型错误的 Shapley 特征重要性进行比较（[数字化、彩色版本](https://oreil.ly/k6nDo)）
- en: In [Figure 3-3](#resid_contribs), feature importance for predictions is shown
    on top, and feature importance for the errors of the model, as calculated by logloss,
    is shown on the bottom. We can see that `PAY_0` dominates both predictions and
    errors, confirming that this model is too reliant on `PAY_0` in general. We can
    also see that `PAY_2` and `PAY_3` are ranked higher for contributions to error
    than contributions to predictions. Given this, it might make sense to experiment
    with dropping, replacing, or corrupting these features. Note that [Figure 3-3](#resid_contribs)
    is made from aggregating Shapley contributions to logloss across an entire validation
    dataset. However, these quantities are calculated feature-by-feature and row-by-row.
    We could also apply this analysis across segments or demographic groups in our
    data, opening up interesting possibilities for detecting and remediating nonrobust
    features for different subpopulations under the model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 3-3](#resid_contribs) 中，预测的特征重要性显示在上方，以 logloss 计算的模型错误的特征重要性显示在下方。我们可以看到，`PAY_0`
    在预测和错误中都占主导地位，从而证实了该模型在一般情况下过于依赖`PAY_0`。我们还可以看到，相较于对预测的贡献，`PAY_2` 和 `PAY_3` 在对错误的贡献上排名更高。鉴于此，尝试删除、替换或损坏这些特征可能是有意义的。请注意，[图 3-3](#resid_contribs)
    是通过在整个验证数据集上汇总对 logloss 的 Shapley 贡献而制作的。然而，这些数量是逐个特征和逐行计算的。我们还可以将这种分析应用于数据中的段或人口群体，这为检测和纠正模型下不稳定特征的有趣可能性打开了新的可能性。
- en: Warning
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Feature importance plots that look like [Figure 3-3](#resid_contribs), with
    one feature drastically outweighing all the others, bode very poorly for in vivo
    reliability and security. If the distribution of that single important feature
    drifts, our model performance is going to suffer. If hackers find a way to modify
    values of that feature, they can easily manipulate our predictions. When one feature
    dominates a model, we likely need a business rule relating to that feature instead
    of an ML model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 类似 [图 3-3](#resid_contribs) 的特征重要性图，一个特征远远超过其他所有特征，这对 vivo 的可靠性和安全性非常不利。如果该单一重要特征的分布发生变化，我们的模型性能就会受到影响。如果黑客找到一种修改该特征值的方法，他们就可以轻易操纵我们的预测。当一个特征主导了模型，我们很可能需要一个与该特征相关的业务规则，而不是一个
    ML 模型。
- en: This ends our brief tour of residual analysis for ML. Of course there are other
    ways to study the errors of ML models. If readers prefer another way, then go
    for it! The important thing is to do some kind of residual analysis for all high-stakes
    ML systems. Along with sensitivity analysis, to be discussed in the next subsection,
    residual analysis is an essential tool in the ML model debugging kit.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对 ML 的残差分析的简要介绍。当然，还有其他研究 ML 模型错误的方法。如果读者喜欢其他方法，那就去试试吧！重要的是对所有高风险的 ML
    系统进行某种残差分析。除了接下来要讨论的敏感性分析，残差分析是 ML 模型调试工具箱中的一个必不可少的工具。
- en: Sensitivity Analysis
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 敏感性分析
- en: 'Unlike linear models, it’s very hard to understand how ML models extrapolate
    or perform on new data without testing them explicitly. That’s the simple and
    powerful idea behind sensitivity analysis. Find or simulate data for interesting
    scenarios, then see how our model performs on that data. We really won’t know
    how our ML system will perform in these scenarios unless we conduct basic sensitivity
    analysis. Of course, there are structured and more efficient variants of sensitivity
    analysis, such as in the [InterpretML](https://oreil.ly/zdzxX) library from Microsoft
    Research. Another great option for sensitivity analysis, and a good place to start
    with more advanced model debugging techniques, is random attacks, discussed in
    [“Software Testing”](#software_testing). Many other approaches, like stress testing,
    visualization, and adversarial example searches also provide standardized ways
    to conduct sensitivity analysis:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性模型不同，很难理解机器学习模型在新数据上的外推或性能，除非明确对其进行测试。这就是敏感性分析的简单而强大的理念。找到或模拟有趣的场景数据，然后查看我们的模型在该数据上的表现。除非进行基本的敏感性分析，否则我们真的不会知道我们的机器学习系统在这些情景中的表现如何。当然，还有结构化和更有效的敏感性分析变体，比如微软研究院的[InterpretML](https://oreil.ly/zdzxX)库。进行敏感性分析的另一个好选择，也是更高级模型调试技术的良好起点，是随机攻击，如[“软件测试”](#software_testing)中讨论的。像压力测试、可视化和对抗示例搜索等其他方法也提供了标准化的敏感性分析方式：
- en: Stress testing
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 压力测试
- en: Stress testing involves simulating data that represents realistic adverse scenarios,
    like recessions or pandemics, and making sure our ML models and any downstream
    business processes will hold up to the stress of the adverse situation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 压力测试涉及模拟代表现实逆境场景的数据，如经济衰退或大流行，并确保我们的机器学习模型和任何下游业务流程能够抵御逆境情况的压力。
- en: Visualizations
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化
- en: Visualizations, such as plots of accumulated local effects, individual conditional
    expectation, and partial dependence curves, are well-known, highly structured
    ways to observe the performance of ML algorithms across various real or simulated
    values of input features. These plots can also reveal areas of data sparsity that
    can lead to weak spots in model performance.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化，如累积局部效应的绘图、个体条件期望和部分依赖曲线，是观察机器学习算法在各种真实或模拟输入特征值下性能的已知高度结构化方法。这些图表还可以揭示可能导致模型性能弱点的数据稀疏区域。
- en: Adversarial example searches
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗示例搜索
- en: Adversarial examples are rows of data that evoke surprising responses from ML
    models. Deep learning approaches can be used to generate adversarial examples
    for unstructured data, and ICE and genetic algorithms can be used to generate
    adversarial examples for structured data. Adversarial examples (and the search
    for them) are a great way to find local areas of instability in our ML response
    functions or decision boundaries that can cause incidents once deployed. As readers
    can see in [Figure 3-4](#adv_search), an adversarial example search is a great
    way to put a model through its paces.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗示例是能够引起机器学习模型意外反应的数据行。深度学习方法可用于生成非结构化数据的对抗示例，而ICE和遗传算法可用于生成结构化数据的对抗示例。对抗示例（及其寻找过程）是发现我们机器学习响应函数或决策边界局部不稳定性的一种好方法，一旦部署可能引发事故。正如读者可以在[图 3-4](#adv_search)中看到的那样，对抗示例搜索是评估模型性能的一种极佳方式。
- en: Conformal approaches
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 符合性方法
- en: '[Conformal approaches](https://oreil.ly/f_Vrf) that attempt to calculate empirical
    bounds for model predictions can help us understand model reliability through
    establishing the upper and lower limits of what can be expected from model outputs.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[符合性方法](https://oreil.ly/f_Vrf)尝试计算模型预测的经验界限，可以通过建立模型输出的上限和下限来帮助我们理解模型的可靠性。'
- en: Perturbation tests
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 扰动测试
- en: Randomly perturbing validation, test, or holdout data to simulate different
    types of noise and drift, and then remeasuring ML model performance, can also
    help establish the general bounds of model robustness. With this kind of perturbation
    testing, we can understand and document the amount of noise or shift that we know
    will break our model. One thing to keep in mind is that poor-performing rows often
    decline in performance faster than average rows under perturbation testing. Watch
    poor-performing rows carefully to understand if and when they drag the performance
    of the entire model down.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 随机扰动验证、测试或保留数据以模拟不同类型的噪声和漂移，然后重新测量ML模型的性能，也可以帮助确定模型鲁棒性的一般界限。通过这种扰动测试，我们可以理解和记录会破坏我们模型的噪声或漂移量。需要记住的一件事是，表现不佳的行通常在扰动测试下的表现下降速度比平均行更快。要仔细观察表现不佳的行，以了解它们何时会拖累整个模型的性能。
- en: '![mlha 0304](assets/mlha_0304.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0304](assets/mlha_0304.png)'
- en: Figure 3-4\. Results from an adversarial example search that reveal interesting
    model behavior ([digital, color version](https://oreil.ly/_vTJW))
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4。展示有趣模型行为的对抗性示例搜索结果（[数字，彩色版本](https://oreil.ly/_vTJW)）
- en: '[Figure 3-4](#adv_search) was created by first finding an ICE curve that exhibited
    a large swing in predictions. Using the row of data responsible for that ICE curve
    as a seed, and perturbing the values of the four most important features in that
    row thousands of times and generating associated predictions, leads to the numerous
    plots in [Figure 3-4](#adv_search). The first finding of the adversarial example
    search is that this heuristic technique, based on ICE curves, enables us to generate
    adversarial examples that can evoke almost any response we want from the model.
    We found rows that reliably yield very low and very high predictions and everything
    in between. If this model was available via a prediction API, we could play it
    like a fiddle.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-4](#adv_search)首先通过找到展示预测大幅波动的ICE曲线来创建。使用负责该ICE曲线的数据行作为种子，对该行中的四个最重要特征的值进行数千次扰动并生成相关预测，从而导致[图 3-4](#adv_search)中的多个图表。对敌对示例搜索的第一个发现是，基于ICE曲线的这种启发式技术使我们能够生成几乎可以从模型中获得任何响应的对抗性示例。我们发现，某些行可可靠地产生非常低和非常高的预测以及两者之间的一切。如果此模型可通过预测API使用，我们可以轻松操纵它。'
- en: In the process of finding all those adversarial examples, we also learned things
    about our model. First, it is likely monotonic in general, and definitely monotonic
    across all the rows we simulated in the adversarial example search. Second, this
    model issues default predictions for people that make extremely high payments.
    Even if someone’s most recent payment was one million dollars, and above their
    credit limit, this model will issue default predictions once that person becomes
    two months late on their payments. This could pose problems for prepayment. Do
    we really want to issue a default or delinquency decision for someone who prepaid
    millions of dollars but is now two months late on their most recent payments?
    Maybe, but it’s likely not a decision that should be made quickly or automatically,
    as this model would do. Third, it appears we may have found a route for a true
    adversarial example attack. Low recent repayment amounts result in surprisingly
    sharp increases in probability of default. If a hacker wants to evoke high probability
    of default predictions from this model, setting `PAY_AMT1` and `PAY_AMT2` to low
    values could be how they do it.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻找所有这些对抗性示例的过程中，我们还了解了有关我们的模型的一些信息。首先，总体上它可能是单调的，并且在我们模拟的所有行中绝对是单调的。其次，该模型针对那些进行了极高支付的人会发出默认预测。即使某人最近的支付金额是一百万美元，并且超过了他们的信用额度，一旦此人的最近付款逾期两个月，该模型将会发出默认预测。这可能对提前还款构成问题。我们真的想为那些预付了数百万美元但现在最近付款逾期两个月的人发出默认或违约决定吗？也许是，但这不是应该迅速或自动做出的决定，就像这个模型会做的那样。第三，看起来我们可能已经找到了真正的对抗性示例攻击路径。最近低还款金额导致违约概率显著增加。如果黑客想从这个模型中引发高违约预测，设置`PAY_AMT1`和`PAY_AMT2`为低值可能是他们的方法。
- en: 'Like we mentioned for residual analysis, readers may have other sensitivity
    analysis techniques in mind, and that’s great. Just make sure you apply some form
    of realistic simulation testing to your ML models. This chapter’s case study is
    an example of the worst kind of outcome resulting from a failure to conduct realistic
    simulation testing prior to deploying an ML system. This ends our brief discussion
    of sensitivity analysis. For those who would like to dive in even deeper, we recommend
    Chapter 19 of Kevin Murphy’s free and open [*Probabilistic Machine Learning: Advanced
    Topics*](https://oreil.ly/mHWno) (MIT Press). Next, we’ll discuss benchmark models
    in different contexts, another time-tested and commonsense model debugging approach.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '就像我们在残差分析中提到的那样，读者可能会想到其他的敏感性分析技术，这是很好的。只是确保在你的机器学习模型上应用某种形式的现实模拟测试。本章的案例研究就是一个在部署机器学习系统前未进行现实模拟测试导致的最糟糕结果的例子。这就结束了我们对敏感性分析的简要讨论。对于那些希望更深入了解的人，我们推荐凯文·墨菲的免费开放书籍[*Probabilistic
    Machine Learning: Advanced Topics*](https://oreil.ly/mHWno)（MIT出版社）第19章。接下来，我们将讨论不同背景下的基准模型，这是一种经过时间考验和常识化的模型调试方法。'
- en: Benchmark Models
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准模型
- en: Benchmark models have been discussed at numerous points in this chapter. They
    are a very important safety and performance tool, with uses throughout the ML
    lifecycle. This subsection will discuss benchmark models in the context of model
    debugging and also summarize other critical uses.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的多个部分都讨论了基准模型。它们是非常重要的安全和性能工具，在整个机器学习生命周期中都有用。本小节将讨论在模型调试的背景下使用基准模型，并总结其他关键用途。
- en: Note
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When possible, compare ML model performance to a linear model or GLM performance
    benchmark. If the linear model beats the ML model, use the linear model.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，比较机器学习模型的性能与线性模型或广义线性模型的性能基准。如果线性模型胜过机器学习模型，则使用线性模型。
- en: 'The first way to use a benchmark model for debugging is to compare performance
    between a benchmark and the ML system in question. If the ML system does not outperform
    a simple benchmark—and many may not—it’s back to the drawing board. Assuming a
    system passes this initial baseline test, benchmark models can then be used as
    a comparison tool to interrogate mechanisms and find bugs within an ML system.
    For instance, data scientists can ask the question, “Which predictions does my
    benchmark get right and my ML system get wrong?” Given that the benchmark should
    be well understood, it should be clear why it is correct, and this understanding
    should also provide some clues as to what the ML system is getting wrong. Benchmarks
    can also be used for reproducibility and model monitoring purposes as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基准模型进行调试的第一种方法是比较基准和问题机器学习系统的性能。如果机器学习系统无法胜过简单的基准模型——很多情况下可能是如此——那么就得重新来过。假设一个系统通过了这个初始基线测试，基准模型可以用作比较工具，用于审查机制并找出机器学习系统中的bug。例如，数据科学家可以提出这样的问题：“我的基准模型预测正确而我的机器学习系统预测错误的是哪些？”鉴于基准模型应该是被充分理解的，应该清楚它为什么是正确的，这种理解也应该提供一些线索，表明机器学习系统出了什么问题。基准还可以用于可复现性和模型监控，如下所述：
- en: Reproducibility benchmarks
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 可复现性基准
- en: Before making changes to a complex ML system, it is imperative to have a reproducible
    benchmark from which to measure performance gains or losses. A reproducible benchmark
    model is an ideal tool for such measurement tasks. If this model can be built
    into CI/CD processes that enable automated testing for reproducibility and comparison
    of new system changes to established benchmarks, even better.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在对复杂的机器学习系统进行更改之前，有必要有一个可复现的基准模型，用于衡量性能的增减。一个可复现的基准模型是这种测量任务的理想工具。如果这个模型可以被集成到允许自动化测试可复现性和将新系统更改与已建立基准进行比较的CI/CD流程中，那就更好了。
- en: Debugging benchmarks
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 调试基准
- en: Comparing complex ML model mechanisms and predictions to a trusted, well-understood
    benchmark model’s mechanisms and predictions is an effective way to spot ML bugs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 将复杂的机器学习模型机制和预测与一个信任的、深入理解的基准模型的机制和预测进行比较，是发现机器学习bug的有效方法。
- en: Monitoring benchmarks
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 监控基准
- en: Comparing real-time predictions between a trusted benchmark model and a complex
    ML system is a way to catch serious ML bugs in real time. If a trusted benchmark
    model and a complex ML system give noticeably different predictions for the same
    instance of new data, this can be a sign of an ML hack, data drift, or even bias
    and algorithmic discrimination. In such cases, benchmark predictions can be issued
    in place of ML system predictions, or predictions can be withheld until human
    analysts determine if the ML system prediction is valid.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个受信任的基准模型和复杂的机器学习系统之间比较实时预测是捕捉严重机器学习错误的一种方式。如果一个受信任的基准模型和复杂的机器学习系统对相同的新数据实例给出明显不同的预测，这可能是机器学习黑客、数据漂移，甚至偏见和算法歧视的迹象。在这种情况下，可以使用基准预测代替机器学习系统的预测，或者在人类分析员确定机器学习系统预测有效之前暂停预测。
- en: Note
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that debugging techniques are often fallible statistical or ML approaches,
    and may need to be debugged themselves.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，调试技术通常是靠不住的统计或机器学习方法，有时甚至需要对它们进行调试。
- en: If we set benchmarks up efficiently, it may even be possible to use the same
    model for all three tasks. A benchmark can be run before starting work to establish
    a baseline from which to improve performance, and that same model can be used
    in comparisons for debugging and model monitoring. When a new version of the system
    outperforms an older version in a reproducible manner, the ML model at the core
    of the system can become the new benchmark. If our organization can establish
    this kind of workflow, we’ll be benchmarking and iterating our way to increased
    ML safety and performance.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有效地设置基准，甚至可能可以使用同一个模型执行所有三项任务。可以在开始工作之前运行基准来建立基线，然后将同一个模型用于调试和模型监控的比较。当一个新版本的系统以可重复的方式优于旧版本时，系统核心的机器学习模型可以成为新的基准。如果我们的组织能够建立这种工作流程，我们将通过基准和迭代的方式提高机器学习的安全性和性能。
- en: 'Remediation: Fixing Bugs'
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修复：修复错误
- en: 'The last step in debugging is fixing bugs. The previous subsections have outlined
    testing strategies, bugs to be on the lookout for, and a few specific fixes. This
    subsection outlines general ML bug-fixing approaches and discusses how they might
    be applied in the example debugging scenario. General strategies to consider during
    ML model debugging include the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 调试的最后一步是修复错误。前面的小节已经概述了测试策略、需要注意的错误以及一些具体的修复方法。本小节概述了一般的机器学习错误修复方法，并讨论了如何在示例调试场景中应用这些方法。在机器学习模型调试期间需要考虑的一般策略包括以下几点：
- en: Anomaly detection
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测
- en: Strange inputs and outputs are usually bad news for ML systems. These can be
    evidence of a real-time security, bias, or safety and performance problem. Monitor
    ML system data queues and predictions for anomalies, record the occurrence of
    anomalies, and alert stakeholders to their presence when necessary.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 奇怪的输入和输出通常对机器学习系统不利。这些可能是实时安全性、偏见或安全性与性能问题的证据。监视机器学习系统的数据队列和预测结果，记录异常事件的发生，并在必要时提醒利益相关者。
- en: Note
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A number of rule-based, statistical, and ML techniques can be used to detect
    anomalies in unseen data queues. These include data integrity constraints, confidence
    limits, control limits, autoencoders, and isolation forests.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用一些基于规则、统计和机器学习技术来检测未见数据队列中的异常。这些技术包括数据完整性约束、置信限、控制限、自编码器和孤立森林。
- en: Experimental design and data augmentation
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设计和数据增强
- en: Collecting better data is often a fix-all for ML bugs. What’s more, data collection
    doesn’t have to be done in a trial-and-error fashion, nor do data scientists have
    to rely on data exhaust byproducts of other organizational processes for selecting
    training data. The mature science of design of experiment has been used by data
    practitioners for decades to ensure they collect the right kind and amount of
    data for model training. Arrogance related to the perceived omnipotence of “big”
    data and overly compressed deployment timelines are the most common reasons data
    scientists don’t practice DOE. Unfortunately, these are not scientific reasons
    to ignore DOE.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 收集更好的数据通常可以解决机器学习错误。此外，数据收集不必通过反复试验，数据科学家也不必依赖其他组织流程的数据副产品来选择训练数据。设计实验的成熟科学已经被数据从业者使用了几十年，以确保他们收集适当类型和数量的数据进行模型训练。关于“大”数据的普遍权威和过度压缩的部署时间表是数据科学家不实践设计实验的最常见原因。不幸的是，这些不是忽视设计实验的科学理由。
- en: Model assertions
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 模型断言
- en: Model assertions are business rules applied to ML model predictions that correct
    for shortcomings in learned ML model logic. Using business rules to improve predictive
    models is a time-honored remediation technique that will likely be with us for
    decades to come. If there is a simple, logical rule that can be applied to correct
    a foreseeable ML model failure, don’t be shy about implementing it. The best practitioners
    and organizations in the predictive analytics space have used this trick for decades.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 模型断言是应用于ML模型预测的业务规则，用于修正学习的ML模型逻辑中的缺陷。使用业务规则来改进预测模型是一种历史悠久的补救技术，可能会伴随我们数十年。如果存在简单的逻辑规则可以应用于纠正可预见的ML模型失败，请勇敢地实施。在预测分析领域，最优秀的从业者和组织已经使用这个技巧几十年了。
- en: Model editing
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 模型编辑
- en: Given that ML models are software, those software artifacts can be edited to
    correct for any discovered bugs. Certain models, like GA2Ms or explainable boosting
    machines (EBMs) are designed to be edited for the purposes of model debugging.
    Other types of models may require more creativity to edit. Either way, editing
    must be justified by domain considerations, as it’s likely to make performance
    on training data appear worse. ML models optimize toward lower error. If we edit
    this highly optimized structure to make in-domain performance better, we’ll likely
    worsen traditional assessment statistics. That’s OK. We care more about in vivo
    safety, robustness, and reliability than in silico test error.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于ML模型是软件，这些软件构件可以被编辑以纠正发现的任何错误。某些模型，如GA2Ms或可解释增强机（EBMs），旨在被编辑用于模型调试。其他类型的模型可能需要更多的创造力来编辑。无论如何，编辑必须由领域考虑来证明，因为这可能使训练数据上的表现看起来更差。ML模型优化以降低误差。如果我们编辑这个高度优化的结构以使领域内性能更好，我们很可能会恶化传统的评估统计数据。没关系。我们更关心体内的安全性、鲁棒性和可靠性，而不是硅内测试错误。
- en: Model management and monitoring
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 模型管理和监控
- en: ML models and the ML systems that house them are dynamic entities that must
    be monitored to the extent that resources allow. All mission-critical ML systems
    should be well-documented, inventoried, and monitored for security, bias, and
    safety and performance problems in real time. When something starts to go wrong,
    stakeholders need to be alerted quickly. [“Deployment”](#deployment_ch03_1680544010503)
    gives a more detailed treatment of model monitoring.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ML模型及其容纳它们的ML系统是动态实体，必须根据资源允许的程度进行监视。所有关键任务的ML系统应该有良好的文档记录、清单和实时监控安全、偏倚、安全性和性能问题。当某些事情开始出错时，利益相关者需要快速得到警报。["部署"](deployment_ch03_1680544010503)给出了有关模型监控的更详细处理。
- en: Monotonic and interaction constraints
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 单调性和交互约束
- en: Many ML bugs occur because ML models have too much flexibility and become untethered
    from reality due to learning from biased and inaccurate training data. Constraining
    models with real-world knowledge is a general solution to several types of ML
    bugs. Monotonic and interaction constraints, in popular tools like XGBoost, can
    help ML practitioners enforce logical domain assumptions in complex ML models.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 许多ML错误发生是因为ML模型具有过多的灵活性，并且由于学习自偏倚和不准确的训练数据而脱离现实。用现实世界的知识约束模型是解决几种ML错误的一般方法。在像XGBoost这样的流行工具中，单调性和交互约束可以帮助ML从业者在复杂的ML模型中执行逻辑领域假设。
- en: Noise injection and strong regularization
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声注入和强正则化
- en: Many ML algorithms come with options for regularization. However, if an ML model
    is overemphasizing a certain feature, stronger or external regularization might
    need to be applied. L0 regularization can be used to limit the number of rules
    or parameters in a model directly, and when necessary, manual noise injection
    can be used to corrupt signals from certain features to deemphasize those with
    any undue importance in ML models.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 许多ML算法都有正则化的选项。然而，如果ML模型过分强调某个特征，可能需要应用更强或外部的正则化。L0正则化可以直接用于限制模型中的规则或参数数量，当必要时，可以使用手动噪声注入来破坏某些特征的信号，以减少那些在ML模型中不当重要的特征。
- en: The scientific method
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 科学方法
- en: Confirmation bias among data scientists, ML engineers, their managers, and business
    partners often conspires to push half-baked demos out the door as products, based
    on the assumptions and limitations of in silico test data assessments. If we’re
    able to follow the scientific method by recording a hypothesis about real-world
    results and objectively test that hypothesis with a designed experiment, we have
    much higher chances at in vivo success. See [Chapter 12](ch12.html#conclusion)
    for more thoughts on using the scientific method in ML.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家、ML工程师、他们的经理和业务合作伙伴之间的确认偏见经常共谋，将半成品演示推向市场产品，基于体外测试数据评估的假设和局限性。如果我们能够遵循科学方法，记录关于实际结果的假设，并通过设计的实验客观地测试该假设，我们在实际应用中取得成功的机会就会大大提高。有关在ML中使用科学方法的更多想法，请参阅[第12章](ch12.html#conclusion)。
- en: Warning
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Generally speaking, ML is still more of an empirical science than an engineering
    discipline. We don’t yet fully understand when ML works well and all the ways
    it can fail, especially when deployed in vivo. This means we have to apply the
    scientific method and avoid issues like confirmation bias to attain good real-world
    results. Simply using the right software and platforms, and following engineering
    best practices, does not mean our models will work well.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，机器学习（ML）仍然更像是经验科学而不是工程学科。我们尚未完全理解ML在何种情况下表现良好以及可能的所有失败方式，特别是在实际应用中。这意味着我们必须应用科学方法，避免问题，如确认偏见，以达到良好的实际应用效果。仅仅使用正确的软件和平台，并遵循工程最佳实践，并不意味着我们的模型会表现良好。
- en: There’s more detailed information regarding model debugging and the example
    data and model in [“Resources”](#Resources). For now, we’ve learned quite a bit
    about model debugging, and it’s time to turn our attention to safety and performance
    for deployed ML systems.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 有关模型调试和示例数据及模型的详细信息，请参见[“资源”](#Resources)。目前，我们已经学到了关于模型调试的很多知识，现在是时候转向部署的ML系统的安全性和性能。
- en: Deployment
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: 'Once bugs are found and fixed, it’s time to deploy our ML system to make real-world
    decisions. ML systems are much more dynamic than most traditional software systems.
    Even if system operators don’t change any code or setting of the system, the results
    can still change. Once deployed, ML systems must be checked for in-domain safety
    and performance, they must be monitored, and their operators must be able to shut
    them off quickly. This section will cover how to enhance safety and performance
    once an ML system is deployed: domain safety, model monitoring, and kill switches.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦发现并修复了bug，就是时候部署我们的ML系统以进行实际决策了。ML系统比大多数传统软件系统更加动态。即使系统操作员不更改任何代码或系统设置，结果仍可能发生变化。一旦部署，必须检查ML系统的领域安全性和性能，必须对其进行监控，并确保操作员能够迅速关闭它们。本节将介绍如何增强ML系统部署后的安全性和性能：领域安全性、模型监控和紧急关闭开关。
- en: Domain Safety
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领域安全性
- en: 'Domain safety means safety in the real world. This is very different from standard
    model assessment, or even enhanced model debugging. How can practitioners work
    toward real-world safety goals? A/B testing and champion challenger methodologies
    allow for some amount of testing in real-time operating environments. Process
    controls, like enumerating foreseeable incidents, implementing controls to address
    those potential incidents, and testing those controls under realistic or stressful
    conditions, are also important for solid in vivo performance. To make up for incidents
    that can’t be predicted, we apply chaos testing, random attacks, and manual prediction
    limits to our ML system outputs. Let’s divide incidents into those we can foresee,
    and those we can’t, and consider a few details for both cases:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 领域安全性意味着在实际世界中的安全性。这与标准的模型评估甚至改进后的模型调试非常不同。从业者如何朝着实现实际世界安全目标的方向努力？A/B测试和冠军挑战方法允许在实时操作环境中进行一定量的测试。流程控制，如列举可预见的事件、实施用于应对潜在事件的控制措施，并在真实或有压力的条件下测试这些控制措施，对于实现实际应用的稳定性非常重要。为了弥补无法预测的事件，我们对ML系统输出应用混沌测试、随机攻击和手动预测限制。让我们将事件分为可以预见的和无法预见的，并考虑这两种情况的一些细节：
- en: Foreseeable real-world incidents
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 可预见的实际世界事件
- en: 'A/B testing and champion-challenger approaches, in which models are tested
    against one another on live data streams or under other realistic conditions,
    are a first step toward robust in-domain testing. Beyond these somewhat standard
    practices, resources should be spent on domain experts and thinking through possible
    incidents. For example, common failure modes in credit lending include bias and
    algorithmic discrimination, lack of transparency, and poor performance during
    recessions. For other applications, say autonomous vehicles, there are numerous
    ways they could accidentally or intentionally cause harm. Once potential incidents
    are recorded, then safety controls can be adopted for the most likely or most
    serious potential incidents. In credit lending, models are tested for bias, explanations
    are provided to consumers via adverse action notices, and models are monitored
    to catch performance degradation quickly. In autonomous vehicles, we still have
    a lot to learn, as [“Case Study: Death by Autonomous Vehicle”](#case_autonomous_vehicle)
    will show. Regardless of the application, safety controls must be tested, and
    these tests should be realistic and performed in collaboration with domain experts.
    When it comes to human safety, simulations run by data scientists are not enough.
    Safety controls need to be tested and hardened in vivo and in coordination with
    people who have a deep understanding of safety in the application domain.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: A/B测试和冠军挑战者方法是通过在实时数据流或其他现实条件下相互测试模型的第一步，以实现领域内健壮测试。除了这些较为标准的做法之外，资源应该投入到领域专家和思考可能发生的事件上。例如，在信贷借款中常见的故障模式包括偏见和算法歧视、缺乏透明度以及在经济衰退期间表现不佳。对于其他应用，比如自动驾驶车辆，它们可能会以多种方式意外或有意造成伤害。一旦记录了潜在的事件，那么安全控制措施就可以被采用来针对最可能或最严重的潜在事件。在信贷借款中，模型被测试是否存在偏见，通过不利行动通知向消费者提供解释，并且对模型进行监控以便及时捕捉性能下降。在自动驾驶车辆领域，我们仍需学习很多，正如["案例研究：自动驾驶车辆致死"](#case_autonomous_vehicle)将展示的那样。无论是哪种应用，安全控制措施必须进行测试，并且这些测试应该是现实的，并与领域专家合作进行。当涉及到人类安全时，由数据科学家运行的模拟不足够。安全控制措施需要在活体中进行测试和加固，并与深刻理解应用领域安全的人员协调一致。
- en: Unforeseeable real-world incidents
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 不可预见的现实世界事件
- en: Interactions between ML systems and their environments can be complex and surprising.
    For high-stakes ML systems, it’s best to admit that unforeseeable incidents can
    occur. We can try to catch some of these potential surprises before they occur
    with chaos testing and random attacks. Important ML systems should be tested in
    strange and chaotic use cases and exposed to large amounts of random input data.
    While these are time- and resource-consuming tests, they are one of the few tools
    available to test for so-called “unknown unknowns.” Given that no testing regime
    can catch every problem, it’s also ideal to apply commonsense prediction limits
    to systems. For instance, large loans or interest rates should not be issued without
    some kind of human oversight. Nor should autonomous vehicles be allowed to travel
    at very high speeds without human intervention. Some actions simply should not
    be performed purely automatically as of today, and prediction limits are one way
    to implement that kind of control.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统与其环境之间的互动可能是复杂且令人惊讶的。对于高风险的机器学习系统，最好承认可能会发生不可预见的事件。我们可以尝试在这些潜在的惊喜发生之前，通过混乱测试和随机攻击来捕获一些问题。重要的机器学习系统应该在奇怪和混乱的使用案例中进行测试，并暴露于大量随机输入数据。尽管这些测试需要耗费时间和资源，但它们是少数几个可用来测试所谓的“未知未知”的工具之一。鉴于没有测试制度能够捕捉到每一个问题，将常识性的预测限制应用于系统也是理想的。例如，在没有人工监督的情况下，不应该发放大额贷款或高利率。也不应该让自动驾驶车辆以非常高的速度行驶，而缺乏人类干预。今天，有些行动仍然不应完全自动执行，预测限制是实施这种控制的一种方式。
- en: Another key aspect of domain safety is knowing if problems are occurring. Sometimes
    glitches can be caught before they grow into harmful incidents. To catch problems
    quickly, ML systems must be monitored. If incidents are detected, incident response
    plans or kill switches may need to be activated.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 领域安全的另一个关键方面是知晓是否发生问题。有时候，故障可能在变成有害事件之前被捕获到。为了迅速发现问题，机器学习系统必须进行监控。如果检测到事件，可能需要启动事件响应计划或者关闭开关。
- en: Model Monitoring
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型监控
- en: It’s been mentioned numerous times in this chapter, but important ML systems
    must be monitored once deployed. This subsection focuses on the technical aspects
    of model monitoring. It outlines the basics of model decay, robustness, and concept
    drift bugs, how to detect and address drift, and the importance of measuring multiple
    key performance indicators (KPIs) in monitoring, as well as briefly highlighting
    a few other notable model monitoring concepts.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中已经多次提到，重要的机器学习系统一旦部署，必须进行监控。本小节重点讨论模型监控的技术方面。它概述了模型衰减、稳健性和概念漂移漏洞的基础知识，如何检测和解决漂移问题，以及在监控中测量多个关键绩效指标（KPIs）的重要性，并简要突出了一些其他值得注意的模型监控概念。
- en: Model decay and concept drift
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型衰减和概念漂移
- en: No matter what we call it, the data coming into an ML system is likely to drift
    away from the data on which the system was trained. The change in the distribution
    of input values over time is sometimes labeled *data drift*. The statistical properties
    of what we’re trying to predict can also drift, and sometimes this is known specifically
    as *concept drift*. The COVID-19 crisis is likely one of history’s best examples
    of these phenomena. At the height of the pandemic, there was likely a very strong
    drift toward more cautious consumer behavior accompanied by an overall change
    in late payment and credit default distributions. These kinds of shifts are painful
    to live through, and they can wreak havoc on an ML system’s accuracy. It’s important
    to note that we sometimes make our own concept drift issues, by engaging in *off-label
    use* of ML models.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们如何称呼它，输入到机器学习系统中的数据很可能会偏离系统训练时的数据。随着时间推移，输入值分布的变化有时被称为*数据漂移*。我们试图预测的统计特性也可能会漂移，有时这特指为*概念漂移*。COVID-19危机很可能是历史上这些现象的最佳例证之一。在疫情高峰期间，消费者行为很可能向更加谨慎的方向漂移，伴随着迟付和信用违约分布的整体变化。这些变化是痛苦的，它们可能会严重影响机器学习系统的准确性。需要注意的是，我们有时会因为*离标使用*机器学习模型而导致自己的概念漂移问题。
- en: Note
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Both input data and predictions can drift. Both types of drift can be monitored,
    and the two types of drift may or may not be directly related. When performance
    degrades without significant input drift this may be due to real-world concept
    drift.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据和预测结果都可能会漂移。这两种漂移都可以被监测到，而且这两种漂移可能直接相关也可能不相关。当性能下降时，如果没有显著的输入漂移，这可能是由于现实世界中的概念漂移。
- en: Detecting and addressing drift
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测和解决漂移
- en: The best approach to detect drift is to monitor the statistical properties of
    live data—both input variables and predictions. Once a mechanism has been put
    in place to monitor statistical properties, we can set alerts or alarms to notify
    stakeholders when there is a significant drift. Testing inputs is usually the
    easiest way to start detecting drift. This is because sometimes true data labels,
    i.e., true outcome values associated with ML system predictions, cannot be known
    for long periods of time. In contrast, input data values are available immediately
    whenever an ML system must generate a prediction or output. So, if current input
    data properties have changed from the training data properties, we likely have
    a problem on our hands. Watching ML system outputs for drift can be more difficult
    due to information needed to compare current and training quality being unavailable
    immediately. (Think about mortgage default versus online advertising—default doesn’t
    happen at the same pace as clicking an online advertisement.) The basic idea for
    monitoring predictions is to watch predictions in real time and look for drift
    and anomalies, potentially using methodologies such as statistical tests, control
    limits, and rules or ML algorithms to catch outliers. And when known outcomes
    become available, test for degradation in model performance and engage in sustained
    bias management quickly and frequently.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 检测漂移的最佳方法是监控实时数据的统计属性——包括输入变量和预测。一旦建立了监控统计属性的机制，我们可以设置警报或告警，以在发生重大漂移时通知利益相关者。测试输入通常是开始检测漂移的最简单方法。这是因为有时候长时间内不可能知道真实的数据标签，即与
    ML 系统预测相关联的真实结果值。相反，输入数据值在 ML 系统必须生成预测或输出时立即可用。因此，如果当前输入数据属性与训练数据属性不同，我们可能面临问题。由于比较当前和训练质量所需的信息不能立即获得，监视
    ML 系统输出的漂移可能更加困难。 （考虑抵押贷款违约与在线广告点击不同时发生的情况。）监控预测的基本思想是实时观察预测并寻找漂移和异常，可能使用统计测试、控制限和规则或
    ML 算法来捕捉异常值。而当已知的结果变得可用时，测试模型性能的下降并快速和频繁地进行持续偏差管理。
- en: 'There are known strategies to address inevitable drift and model decay. These
    include the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 有已知的策略来解决不可避免的漂移和模型衰减。这些包括以下内容：
- en: Refreshing an ML system with extended training data containing some amount of
    new data
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用包含一定量新数据的扩展训练数据刷新 ML 系统
- en: Refreshing or retraining an ML system frequently
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频繁刷新或重新训练 ML 系统
- en: Refreshing or retraining an ML system when drift is detected
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当检测到漂移时，刷新或重新训练 ML 系统
- en: It should be noted that any type of retraining of ML models in production should
    be subject to the risk mitigation techniques discussed in this chapter and elsewhere
    in the book—just like they should be applied to the initial training of an ML
    system.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，任何生产中的 ML 模型重新训练都应遵循本章和书中其他地方讨论的风险缓解技术，就像它们应用于 ML 系统的初始训练一样。
- en: Monitoring multiple key performance indicators
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控多个关键绩效指标
- en: Most discussions of model monitoring focus on model accuracy as the primary
    key performance indicator (KPI). Yet, bias, security vulnerabilities, and privacy
    harms can, and likely should, be monitored as well. The same bias testing that
    was done at training time can be applied when new known outcomes become available.
    Numerous other strategies, discussed in Chapters [5](ch05.html#unique_chapter_id_5)
    and [11](ch11.html#unique_chapter_id_11), can be used to detect malicious activities
    that could compromise system security or privacy. Perhaps the most crucial KPI
    to measure, if at all possible, is the actual impact of the ML system. Whether
    it’s saving or generating money, or saving lives, measuring the intended outcome
    and actual value of the ML system can lead to critical organizational insights.
    Assign monetary or other values to confusion matrix cells in classification problems,
    and to residual units in regression problems, as a first step toward estimating
    actual business value. See [Chapter 8](ch08.html#unique_chapter_id_8) for a basic
    example of estimating business value.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数关于模型监控的讨论侧重于模型准确性作为主要的关键绩效指标（KPI）。然而，偏差、安全漏洞和隐私损害也应该被监控，可能也应该被监控。在新的已知结果可用时，可以应用与训练时相同的偏差测试。在第[5](ch05.html#unique_chapter_id_5)章和第[11](ch11.html#unique_chapter_id_11)章讨论的众多其他策略中，可以用来检测可能危及系统安全或隐私的恶意活动。如果可能的话，可能是最关键的KPI是ML系统的实际影响。无论是节省或创造金钱，还是挽救生命，衡量ML系统的预期结果和实际价值可以带来关键的组织见解。在分类问题中，为混淆矩阵单元分配货币或其他值，并为回归问题中的残余单元分配货币或其他值，是估计实际业务价值的第一步。请参见第[8](ch08.html#unique_chapter_id_8)章，以获取估计业务价值的基本示例。
- en: Out-of-range values
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超出范围的值
- en: Training data can never cover all of the data an ML system might encounter once
    deployed. Most ML algorithms and prediction functions do not handle out-of-range
    data well, and may simply issue an average prediction or crash, and do so without
    notifying application software or system operators. ML system operators should
    make specific arrangements to handle data, such as large-magnitude numeric values,
    rare categorical values, or missing values that were not encountered during training
    so that ML systems will operate normally and warn users when they encounter out-of-range
    data.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据永远无法覆盖ML系统在部署后可能遇到的所有数据。大多数ML算法和预测功能无法很好地处理超出范围的数据，可能会简单地发出平均预测或崩溃，而不会通知应用软件或系统操作员。ML系统操作员应该做出具体安排来处理数据，如大量数值、罕见分类值或在训练期间未遇到的缺失值，以确保ML系统在遇到超出范围的数据时能正常运行并警告用户。
- en: Anomaly detection and benchmark models
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异常检测和基准模型
- en: 'Anomaly detection and benchmark models round out the technical discussion of
    model monitoring in this subsection. These topics have been treated elsewhere
    in this chapter, and are touched on briefly here in the monitoring context:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测和基准模型在本小节中补充了关于模型监控的技术讨论。这些主题在本章的其他地方已经讨论过，并在此处简要涉及了监控的背景下：
- en: Anomaly detection
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测
- en: Strange input or output values in an ML system can be indicative of stability
    problems or security and privacy vulnerabilities. It’s possible to use statistics,
    ML, and business rules to monitor anomalous behavior in both inputs and outputs,
    and across an entire ML system. Record any such detected anomalies, report them
    to stakeholders, and be ready to take more drastic action when necessary.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ML系统中的奇怪输入或输出值可能表明稳定性问题或安全和隐私漏洞。可以使用统计数据、ML和业务规则来监控输入和输出的异常行为，以及整个ML系统。记录任何检测到的异常行为，向利益相关者报告，并在必要时准备采取更严厉的行动。
- en: Benchmark models
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 基准模型
- en: Comparing simpler benchmark models and ML system predictions as part of model
    monitoring can help to catch stability, fairness, or security anomalies in near
    real time. A benchmark model should be more stable, easier to confirm as minimally
    discriminatory, and should be harder to hack. We use a highly transparent benchmark
    model and our more complex ML system together when scoring new data, then compare
    our ML system predictions against the trusted benchmark prediction in real time.
    If the difference between the ML system and the benchmark is above some reasonable
    threshold, then fall back to issuing the benchmark model’s prediction or send
    the row of data for more review.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 将简单基准模型与ML系统预测进行比较作为模型监控的一部分，可以帮助及时发现稳定性、公平性或安全性异常。基准模型应更稳定，更易确认为最少歧视性，且更难以被攻击。我们在评分新数据时同时使用高度透明的基准模型和更复杂的ML系统，然后实时比较ML系统的预测与信任的基准预测。如果ML系统与基准模型之间的差异超过某个合理的阈值，那么退回到发出基准模型的预测或者将数据行发送进行更多审查。
- en: 'Whether it’s out-of-range values in new data, disappointing KPIs, drift, or
    anomalies—these real-time problems are where rubber meets road for AI incidents.
    If our monitoring detects these issues, a natural inclination will be to turn
    the system off. The next subsection addresses just this issue: kill switches for
    ML systems.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是新数据中的超出范围值、令人失望的关键绩效指标、漂移或异常—这些实时问题是AI事件发生的关键节点。如果我们的监控检测到这些问题，自然而然的反应可能是关闭系统。下一小节将专门讨论这个问题：ML系统的紧急停止开关。
- en: Kill switches
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 紧急停止开关
- en: Kill switches are rarely single switches or scripts, but a set of business and
    technical processes bundled together that serve to turn an ML system off—to the
    degree that’s possible. There’s a lot to consider before flipping a proverbial
    kill switch. ML system outputs often feed into downstream business processes,
    sometimes including other ML systems. These systems and business processes can
    be mission critical, such as an ML system used for credit underwriting or e-retail
    payment verification. To turn off an ML system, we not only need the right technical
    know-how and personnel available, but we also need an understanding of the system’s
    place inside of broader organizational processes. During an ongoing AI incident
    is a bad time to start thinking about turning off a fatally flawed ML system.
    So, kill processes and kill switches are a great addition to our ML system documentation
    and AI incident response plans (see [Chapter 1](ch01.html#unique_chapter_id_1)).
    This way, when the time comes to kill an ML system, our organization can be ready
    to make a quick and informed decision. Hopefully we’ll never be in a position
    where flipping an ML system kill switch is necessary, but unfortunately AI incidents
    have grown more common in recent years. When technical remediation methods are
    applied alongside cultural competencies and business processes for risk mitigation,
    the safety and performance of ML systems is enhanced. When these controls are
    not applied, bad things can happen.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 紧急停止开关很少是单独的开关或脚本，而是一套捆绑在一起的业务和技术流程，用于尽可能地关闭ML系统。在触发类比的紧急停止开关之前，需要考虑很多因素。ML系统的输出通常会进入下游的业务流程，有时包括其他ML系统。这些系统和业务流程可能是使命关键的，例如用于信贷核准或电子零售支付验证的ML系统。要关闭ML系统，不仅需要正确的技术知识和可用的人员，还需要理解该系统在更广泛组织流程中的位置。在进行中的AI事件中开始考虑关闭严重缺陷的ML系统是一个糟糕的时机。因此，紧急停止流程和紧急停止开关是我们ML系统文档和AI事件响应计划的重要补充（见[第1章](ch01.html#unique_chapter_id_1)）。这样，当需要关闭ML系统时，我们的组织可以做出迅速而明智的决定。希望我们永远不会处于必须触发ML系统紧急停止开关的境地，但不幸的是，近年来AI事件变得越来越常见。当在技术补救方法与文化能力和业务流程相结合时，可以增强ML系统的安全性和性能。如果不应用这些控制措施，可能会发生糟糕的事情。
- en: 'Case Study: Death by Autonomous Vehicle'
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：自动驾驶车辆致命事件
- en: On the night of March 18, 2018, Elaine Herzberg was walking a bicycle across
    a wide intersection in Tempe, Arizona. In what has become one of the most high-profile
    AI incidents, she was struck by an autonomous Uber test vehicle traveling at roughly
    40 mph. According to the National Transportation Safety Board (NTSB), the test
    vehicle driver, who was obligated to take control of the vehicle in emergency
    situations, was distracted by a smartphone. The self-driving ML system also failed
    to save Ms. Herzberg. The system did not identify her until 1.2 seconds before
    impact, too late to prevent a brutal crash.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年3月18日晚，伊莱恩·赫兹伯格在亚利桑那州凤凰城的一个宽广的十字路口推着自行车过马路。在成为最引人注目的AI事故之一的事件中，她被一辆自动驾驶的Uber测试车辆撞击，该车以大约40英里每小时的速度行驶。根据国家运输安全委员会（NTSB）的说法，测试车辆的驾驶员在紧急情况下有义务控制车辆，但却因手机分心。自动驾驶的机器学习系统也未能拯救赫兹伯格女士。直到撞击前1.2秒系统才识别出她，已经太晚避免了严重的碰撞。
- en: Fallout
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后果
- en: Autonomous vehicles are thought to offer safety benefits over today’s status
    quo of human-operated vehicles. While fatalities involving self-driving cars are
    rare, ML-automated driving has yet to deliver on the original promise of safer
    roads. The NTSB’s report [states](https://oreil.ly/2nEOv) that this Uber’s “system
    design did not include a consideration for jaywalking pedestrians.” The report
    also criticized lax risk assessments and an immature safety culture at the company.
    Furthermore, an Uber employee had raised serious concerns about 37 crashes in
    the previous 18 months and common problems with test vehicle drivers just days
    before the Tempe incident. As a result of the Tempe crash, Uber’s autonomous vehicle
    testing was stopped in four other cities and local governments all over the US
    and Canada began reexamining safety protocols for self-driving vehicle tests.
    The driver has been charged with negligent homicide. Uber has been excused from
    criminal liability, but came to a monetary settlement with the deceased’s family.
    The city of Tempe and the State of Arizona were also sued by Ms. Herzberg’s family
    for $10 million each.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶车辆被认为比今天由人操作的车辆提供更多安全保障。虽然涉及自动驾驶汽车的致命事故很少见，但机器学习自动驾驶尚未兑现更安全道路的最初承诺。NTSB
    的报告[指出](https://oreil.ly/2nEOv)Uber的“系统设计未考虑行人乱穿马路的情况”。报告还批评公司松散的风险评估和不成熟的安全文化。此外，Uber员工在凤凰城事故前几天就严重关注了前18个月内的37起事故以及测试车辆驾驶员的常见问题。由于凤凰城的事故，Uber在其他四个城市停止了自动驾驶车辆的测试，美国和加拿大各地的地方政府开始重新审视自动驾驶车辆测试的安全协议。驾驶员被控过失杀人罪。Uber被免除刑事责任，但与遇难者家属达成了经济赔偿协议。Tempe市和亚利桑那州还因赫兹伯格女士家属对他们分别索赔1000万美元而被起诉。
- en: An Unprepared Legal System
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个不做准备的法律体系
- en: It must be noted that the legal system in the US is somewhat unprepared for
    the reality of AI incidents, potentially leaving employees, consumers, and the
    general public largely unprotected from the unique dangers presented by ML systems
    operating in our midst. The EU Parliament has put forward a liability regime for
    ML systems that would mostly prevent large technology companies from escaping
    their share of the consequences in future incidents. In the US, any plans for
    federal AI product safety regulations are still in a preliminary phase. In the
    interim, individual cases of AI safety incidents will likely be decided by lower
    courts with little education and experience in handling AI incidents, enabling
    Big Tech and other ML system operators to bring vastly asymmetric legal resources
    to bear against individuals caught up in incidents related to complex ML systems.
    Even for the companies and ML system operators, this legal limbo is not ideal.
    While the lack of regulation seems to benefit those with the most resources and
    expertise, it makes risk management and predicting the outcomes of AI incidents
    more difficult. Regardless, future generations may judge us harshly for allowing
    the criminal liability of one of the first AI incidents, involving many data scientists
    and other highly paid professionals and executives, to be pinned solely on a safety
    driver of a supposedly automated vehicle.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 必须指出，美国的法律体系在处理人工智能事故的现实中显得有些措手不及，这可能会使员工、消费者和一般公众在面对我们中间运行的机器学习系统带来的独特危险时，基本上毫无保护。欧洲议会已提出了针对机器学习系统的责任制度，这将大部分防止大型技术公司逃避未来事件中的责任。在美国，任何关于联邦人工智能产品安全法规的计划仍处于初步阶段。在此期间，AI安全事故的个别案例可能会由缺乏处理AI事故教育和经验的下级法院决定，使大科技公司和其他机器学习系统运营商能够利用极不对称的法律资源来对付卷入复杂机器学习系统事故的个人。即使对于公司和机器学习系统运营商来说，这种法律空白也并非理想。虽然缺乏法规似乎有利于那些拥有最多资源和专业知识的人，但这也使得风险管理和预测AI事故结果变得更加困难。无论如何，未来的世代可能会因我们允许将第一起AI事故的刑事责任，牵涉到许多数据科学家和其他高薪专业人士及高管，全部归咎于一名据称是自动驾驶车辆的安全驾驶员而对我们进行严厉的评判。
- en: Lessons Learned
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学到的教训
- en: What lessons learned from this and previous chapters could be applied to this
    case?
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一章节和之前的章节中学到了什么教训可以应用到这个案例中？
- en: 'Lesson 1: Culture is important.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 第一课：文化至关重要。
- en: A mature safety culture is a broad risk control, bringing safety to the forefront
    of design and implementation work, and picking up the slack in corner cases that
    processes and technology miss. Learned from the last generation of life-changing
    commercial technologies, like aerospace and nuclear power, a more mature safety
    culture at Uber could have prevented this incident, especially since an employee
    raised serious concerns in the days before the crash.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 成熟的安全文化是一种广泛的风险控制措施，将安全置于设计和实施工作的前沿，并在流程和技术遗漏的特殊情况下弥补不足。从上一代改变生活的商业技术（如航空航天和核能）中汲取的经验表明，优步如果有更成熟的安全文化，可能已经可以预防这起事件，尤其是因为事故发生前几天有员工提出了严重的担忧。
- en: 'Lesson 2: Mitigate foreseeable failure modes.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 第二课：减轻可以预见的故障模式。
- en: The NTSB concluded that Uber’s software did not specifically consider jaywalking
    pedestrians as a failure mode. For anyone who’s driven a car with pedestrians
    around, this should have been an easily foreseeable problem for which any self-driving
    car should be prepared. ML systems generally are not prepared for incidents unless
    their human engineers make them prepared. This incident shows us what happens
    when those preparations are not made in advance.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 国家运输安全委员会（NTSB）得出结论称，优步的软件并未将乱穿马路的行人视为故障模式。对于任何开过有行人的车的人来说，这应该是一个很容易预见的问题，任何自动驾驶汽车都应该做好准备。通常情况下，除非人类工程师为此做好准备，机器学习系统并不会为这类事件做好准备。这一事件向我们展示了当这些准备工作未提前做好时会发生什么。
- en: 'Lesson 3: Test ML systems in their operating domain.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 第三课：在其运行领域测试机器学习系统。
- en: After the crash, Uber stopped and reset its self-driving car program. After
    improvements, it was able to show via simulation that its new software would have
    started breaking four seconds before impact. Why wasn’t the easily foreseeable
    reality of jaywalking pedestrians tested with these same in-domain simulations
    before the March 2018 crash? The public may never know. But enumerating failure
    modes and testing them in realistic scenarios could prevent our organization from
    having to answer these kinds of unpleasant questions.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 碰撞后，Uber 停止并重置其自动驾驶汽车程序。改进后，它能够通过模拟展示，新软件在撞击前四秒就开始失效。为什么在2018年3月的撞车事件之前，没有将步行的行人这种易预见的现实用相同的领域模拟测试呢？公众可能永远不会知道。但是列举失败模式并在现实场景中测试它们可以防止我们的组织不得不回答这类令人不快的问题。
- en: A potential bonus lesson here is to consider not only accidental failures, like
    the Uber crash, but also malicious hacks against ML systems and the abuse of ML
    systems to commit violence. Terrorists have turned motor vehicles into deadly
    weapons before, so this is a known failure mode. Precautions must be taken in
    autonomous vehicles, and in driving assistance features, to prevent hacking and
    violent outcomes. Regardless of whether it is an accident or a malicious attack,
    AI incidents will certainly kill more people. Our hope is that governments and
    other organizations will take ML safety seriously, and minimize the number of
    these somber incidents in the future.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 一个潜在的额外教训是考虑不仅是意外故障，比如Uber的事故，还有针对ML系统的恶意黑客攻击以及利用ML系统进行暴力行为。恐怖分子以前曾将机动车辆变成致命武器，所以这是一个已知的失败模式。必须在自动驾驶汽车和驾驶辅助功能中采取预防措施，以防止黑客攻击和暴力事件。无论是意外还是恶意攻击，AI事故肯定会造成更多人员伤亡。我们希望政府和其他组织能认真对待ML安全问题，并在未来尽量减少这类严峻事件的发生。
- en: Resources
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: Further Reading
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[“A Comprehensive Study on Deep Learning Bug Characteristics”](https://oreil.ly/89R6O)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“深度学习错误特征的综合研究”](https://oreil.ly/89R6O)'
- en: '[“Debugging Machine Learning Models”](https://oreil.ly/685C3)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“调试机器学习模型”](https://oreil.ly/685C3)'
- en: '[“Real-World Strategies for Model Debugging”](https://oreil.ly/LvrLk)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“模型调试的现实世界策略”](https://oreil.ly/LvrLk)'
- en: '[“Safe and Reliable Machine Learning”](https://oreil.ly/mLU8l)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“安全可靠的机器学习”](https://oreil.ly/mLU8l)'
- en: '[“Overview of Debugging ML Models”](https://oreil.ly/xZGoN)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“机器学习模型调试概述”](https://oreil.ly/xZGoN)'
- en: '[“DQI: Measuring Data Quality in NLP”](https://oreil.ly/aa7rv)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“在NLP中测量数据质量的DQI”](https://oreil.ly/aa7rv)'
- en: '[“Identifying and Overcoming Common Data Mining Mistakes”](https://oreil.ly/w19Qm)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“识别和克服常见数据挖掘错误”](https://oreil.ly/w19Qm)'
- en: Code Examples
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 代码示例
- en: '[Basic sensitivity and residual analysis example](https://oreil.ly/Tcu65)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基本敏感性和残差分析示例](https://oreil.ly/Tcu65)'
- en: '[Advanced sensitivity analysis example](https://oreil.ly/QPFFx)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高级敏感性分析示例](https://oreil.ly/QPFFx)'
- en: '[Advanced residual analysis example](https://oreil.ly/Poe20)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高级残差分析示例](https://oreil.ly/Poe20)'
