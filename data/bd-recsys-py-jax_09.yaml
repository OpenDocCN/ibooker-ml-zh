- en: Chapter 7\. Serving Models and Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章\. 服务模型和架构
- en: As we think about how recommendation systems utilize the available data to learn
    and eventually serve recommendations, it’s crucial to describe how the pieces
    fit together. The combination of the data flow and the jointly available data
    for learning is called the *architecture*. More formally, the architecture is
    the connections and interactions of the system or network of services; for data
    applications, the architecture also includes the available features and objective
    functions for each subsystem. Defining the architecture typically involves identifying
    components or individual services, defining the relationships and dependencies
    among those components, and specifying the protocols or interfaces through which
    they will communicate.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑推荐系统如何利用可用数据进行学习并最终提供推荐时，描述这些部件如何配合是至关重要的。数据流和共同可用的数据组合用于学习的组合被称为*架构*。更正式地说，架构是系统或服务网络的连接和互动；对于数据应用程序，架构还包括每个子系统的可用特征和目标函数。定义架构通常涉及识别组件或个别服务，定义这些组件之间的关系和依赖关系，并指定它们将通过的协议或接口。
- en: In this chapter, we’ll spell out some of the most popular and important architectures
    for recommendation systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细阐述一些最流行和最重要的推荐系统架构。
- en: Architectures by Recommendation Structure
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建议结构的架构
- en: 'We have returned several times to the concept of collector, ranker, and server,
    and we’ve seen that they may be regarded via two paradigms: the online and the
    offline modes. Further, we’ve seen how many of the components in [Chapter 6](ch06.html#data-processing)
    satisfy some of the core requirements of these functions.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次提到收集器、排名器和服务器的概念，并且我们看到它们可以通过在线和离线两种模式来看待。此外，我们还看到了[第6章](ch06.html#data-processing)中的许多组件如何满足这些功能的核心要求。
- en: Designing large systems like these requires several architectural considerations.
    In this section, we will demonstrate how these concepts are adapted based on the
    type of recommendation system you are building. We’ll compare a mostly standard
    item-to-user recommendation system, a query-based recommendation system, context-based
    recommendations, and sequence-based recommendations.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 设计这样的大型系统需要考虑几个架构因素。在本节中，我们将演示如何根据您正在构建的推荐系统类型来调整这些概念。我们将比较大部分标准的物品对用户推荐系统，基于查询的推荐系统，基于上下文的推荐和基于序列的推荐。
- en: Item-to-User Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 物品对用户的推荐
- en: We’ll start by describing the architecture of the system we’ve been building
    in the book thus far. As proposed in [Chapter 4](ch04.html#ch:system_design),
    we built the collector offline to ingest and process our recommendations. We utilize
    representations to encode relationships between items, users, or user-item pairs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从描述迄今为止在本书中建立的系统的架构开始。正如在[第4章](ch04.html#ch:system_design)中提出的那样，我们离线构建了收集器以摄取和处理我们的推荐。我们利用表示来编码物品、用户或用户-物品对之间的关系。
- en: The online collector takes the request, usually in the form of a user ID, and
    finds a neighborhood of items in this representation space to pass along to the
    ranker. Those items are filtered when appropriate and sent for scoring.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在线收集器接受请求，通常以用户ID的形式，并在表示空间中找到物品邻域以传递给排名器。适当时会过滤这些物品，并发送进行评分。
- en: The offline ranker learns the relevant features for scoring and ranking, training
    on the historical data. It then uses this model and, in some cases, item features
    as well for inference.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 离线排名器通过训练历史数据来学习得分和排名所需的相关特征。然后，它使用这个模型，并且在某些情况下还使用物品特征进行推断。
- en: In the case of recommendation systems, this inference computes the scores associated
    to each item in the set of potential recommendations. We usually sort by this
    score, which you’ll learn more about in [Part III](part03.html#ranking). Finally,
    we integrate a final round of ordering based on some business logic (described
    in [Chapter 14](ch14.html#HardRanking)). This last step is part of the serving,
    where we impose requirements like test criteria or recommendation diversity requirements.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐系统的情况下，此推断计算与潜在推荐集中每个物品相关联的分数。我们通常按此分数排序，关于这一点，您将在[第III部分](part03.html#ranking)中了解更多。最后，我们基于某些业务逻辑（在[第14章](ch14.html#HardRanking)中描述）整合最后一轮排序。这最后一步是服务的一部分，我们在此强加要求，如测试标准或推荐多样性要求。
- en: '[Figure 7-1](#fig:four-stage-recommender) is an excellent overview of the retrieval,
    ranking, and serving structure, although it depicts four stages and uses slightly
    different terminology. In this book, we combine the filtering stage shown here
    into retrieval.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-1](#fig:four-stage-recommender)是检索、排序和提供结构的极好概述，尽管它描绘了四个阶段并使用了稍微不同的术语。在本书中，我们将此处显示的过滤阶段与检索结合在一起。'
- en: Query-Based Recommendations
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于查询的推荐
- en: 'To start off our process, we want to make a query. The most obvious example
    of a query is a text query as in text-based search engines; however, queries may
    be more general! For example, you may wish to allow search-by-image or search-by-tag
    options. Note that an important type of query-based recommender uses an *implicit*
    query: the user is providing a search query via UI choices or by behaviors. While
    these systems are quite similar in overall structure to the item-to-user systems,
    let’s discover how to modify them to fit our use case.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始我们的过程，我们想提出一个查询。最明显的查询示例是文本查询，如基于文本的搜索引擎；但是，查询可能更加普遍！例如，您可能希望允许按图像搜索或按标签搜索的选项。请注意，一个重要类型的基于查询的推荐器使用*隐式*查询：用户通过
    UI 选择或行为提供搜索查询。虽然这些系统在整体结构上与项目到用户系统非常相似，但让我们发现如何修改它们以适应我们的用例。
- en: '![Four Stage Recommender](assets/brpj_0701.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![四阶段推荐系统](assets/brpj_0701.png)'
- en: Figure 7-1\. A four-stage recommendation system (adapted from an image by Karl
    Higley and Even Oldridge)
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1。一个四阶段推荐系统（根据Karl Higley和Even Oldridge的图片调整）
- en: We want to integrate more context about the query into the first step of the
    request. Note that we don’t want to throw out the user-item matching components
    of this system. Even though the user is performing a search, personalizing the
    recommendations based on their taste is useful. Instead, we need to utilize the
    query as well; later we will discuss various technical strategies, but a simple
    summary for now is to also generate an embedding for the query. Note that the
    query is like an item or a user but is sufficiently different.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将有关查询的更多上下文整合到请求的第一步中。请注意，我们不想丢弃此系统的用户-项目匹配组件。即使用户正在执行搜索，根据他们的口味个性化推荐也是有用的。相反，我们也需要利用查询；稍后我们将讨论各种技术策略，但现在简单总结一下也是生成查询嵌入的一种方法。请注意，查询类似于项目或用户，但有足够的不同。
- en: Some strategies might include similarity between the query and items, or co-occurrence
    of the query and items. Either way, we now have a query representation and user
    representation, and we want to utilize both for our recommendation. One simple
    approach is to use the query representation for retrieval, but during the scoring,
    score via both query-item and user-item, combining them via a multiobjective loss.
    Another approach is to use the user for retrieval and then the query for filtering.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一些策略可能包括查询与项目之间的相似性，或者查询与项目的共现。无论哪种方式，我们现在都有了一个查询表示和用户表示，我们希望利用两者进行推荐。一种简单的方法是使用查询表示进行检索，但在评分过程中，通过查询-项目和用户-项目的得分，通过多目标损失将它们结合起来。另一种方法是使用用户进行检索，然后使用查询进行过滤。
- en: Different Embeddings
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同的嵌入
- en: Unfortunately, while we’d love the same embedding space (for nearest-neighbors
    lookup) to work well for our queries and our documents (items, etc.), this is
    often not the case. The simplest example is something like asking questions and
    hoping to find relevant Wikipedia articles. This problem is often referred to
    as the queries being “out of distribution” from the documents.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，尽管我们很希望相同的嵌入空间（用于最近邻查找）对我们的查询和我们的文档（项目等）都能很好地工作，但情况通常并非如此。最简单的例子是类似于提问并希望找到相关维基百科文章的情况。这个问题通常被称为查询与文档“不在分布”中。
- en: Wikipedia articles are written in a declarative informative article style, whereas
    questions are often brief and casual. If you were to use an embedding model focused
    on capturing semantic meaning, you’d naively expect the queries to be located
    in significantly different subspaces than the articles. This means that your distance
    computations will be affected. This is often *not* a huge problem because you
    retrieve via relative distances, and you can hope that the shared subspaces are
    enough to provide a good retrieval. However, it can be hard to predict when these
    perform poorly.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科文章采用陈述性信息文章风格，而问题通常简短随意。如果您使用一个专注于捕捉语义意义的嵌入模型，您可能会天真地期望查询位于与文章显著不同的子空间中。这意味着您的距离计算将受到影响。这通常*不*是一个巨大问题，因为您通过相对距离检索，并且您可以希望共享的子空间足以提供良好的检索。然而，这种情况下的表现不佳很难预测。
- en: The best practice is to carefully examine the embeddings on common queries and
    on target results. These problems can be *especially* bad on implicit queries
    like a series of actions taken at a particular time of day to look up food recommendations.
    In this case, we expect the queries to be wildly different from the documents.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是仔细检查常见查询和目标结果上的嵌入。这些问题在像特定时间采取的一系列行动这样的隐式查询上可能特别糟糕。在这种情况下，我们预计查询与文档大相径庭。
- en: Context-Based Recommendations
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于上下文的推荐
- en: A context is quite similar to a query but tends to be more obviously feature
    based and frequently less similar to the items/users distributions. *Context*
    is usually the term used to represent exogenous features to the system that may
    have an effect on the system—i.e., auxiliary information such as time, weather,
    or location. Context-based recommendation is similar to query based in that context
    is an additional signal that the system needs to consider during recommendation,
    but more often than not, the query should dominate the signal for recommendation,
    whereas the context should not.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文（*Context*）与查询相似，但倾向于更明显地基于特征，并且通常与项目/用户分布不太相似。*上下文*通常用于表示系统外的特征，可能对系统产生影响——例如时间、天气或位置等辅助信息。基于上下文的推荐与基于查询的推荐类似，上下文是系统在推荐过程中需要考虑的额外信号，但往往查询应该主导推荐的信号，而上下文则不应该。
- en: Let’s take a simple example of ordering food. A query for a food-delivery recommendation
    system would look like *Mexican food*; this is an extremely important signal from
    the user looking for burritos or quesadillas of how the recommendations should
    look. A context for a food-delivery recommendation system would look like *it’s
    almost lunchtime*. This signal is useful but may not outweigh user personalization.
    Putting hard-and-fast rules on this weighting can be difficult, so usually we
    don’t, and instead we learn parameters via experimentation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个简单的订餐的例子。一个食品配送推荐系统的查询可能看起来像*墨西哥食品*；这是用户寻找墨西哥卷饼或奎萨迪亚的极为重要的信号，决定了推荐的外观。一个食品配送推荐系统的上下文可能看起来像*快到午餐时间了*。这个信号很有用，但可能不如用户个性化的重要。在这种权重上设定硬性规则可能很困难，通常我们不这样做，而是通过实验学习参数。
- en: Context features fit into the architecture similar to the way queries do, via
    learned weightings as part of the objective function. Your model will learn a
    representation between context features and items, and then add that affinity
    into the rest of the pipeline. Again, you can make use of this early in the retrieval,
    later in the ranking, or even during the serving step.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文特征与体系结构类似于查询的方式，通过学习的加权作为目标函数的一部分。您的模型将学习上下文特征与项目之间的关系，然后将该亲和力添加到其余流程中。同样，您可以在检索早期使用它，在排名后期使用它，甚至在服务步骤中使用它。
- en: Sequence-Based Recommendations
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于序列的推荐
- en: '*Sequence-based recommendations* build on context-based recommendations but
    with a specific type of context. Sequential recommendations are based on the idea
    that the recent items the user has been exposed to should have a significant influence
    on the recommendations. A common example here is a music-streaming service, as
    the last few songs that have been played can significantly inform what the user
    might want to hear next. To ensure that this *autoregressive*, or sequentially
    predictive, set of features has an influence on recommendations, we can treat
    each item in the sequence as a weighted context for the recommendation.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于序列的推荐* 是在基于上下文的推荐基础上构建的，但具有特定类型的上下文。序列推荐基于这样一个思想：用户最近接触的项目应显著影响推荐。一个常见的例子是音乐流媒体服务，因为最近播放的几首歌可以显著地影响用户可能想要听的下一首歌曲。为了确保这种*自回归*或序列预测的特征对推荐有影响，我们可以将序列中的每个项目视为推荐的加权上下文。'
- en: Usually, the item-item representation similarities are weighted to provide a
    collection of recommendations, and various strategies are used for combining these.
    In this case, we normally expect the user to be of high importance in the recommendations,
    but the sequence is also of high importance. One simple model is to think of the
    sequence of items as a sequence of tokens, and form a single embedding for that
    sequence—as in NLP applications. This embedding can be used as the context in
    a context-based recommendation architecture.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，项目-项目的表示相似性会加权以提供一系列推荐，并使用各种策略来组合这些推荐。在这种情况下，我们通常期望用户在推荐中非常重要，但序列也非常重要。一个简单的模型是将项目序列视为标记序列，并形成该序列的单一嵌入，就像在NLP应用中一样。这种嵌入可以作为上下文基础推荐架构中的上下文使用。
- en: Naive Sequence Embeddings
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 天真的序列嵌入
- en: The combinatorics of one-embedding-per-sequence explode in cardinality; the
    number of potential items in each sequential slot is very large, and each item
    in the sequence multiplies those possibilities together. Imagine, for example,
    five-word sequences, where the number of possibilities for each item is close
    to the size of the English lexicon, and thus it would be that size to the fifth
    power. We provide simple strategies for dealing with this in [Chapter 17](ch17.html#Attention).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一序列每一次嵌入的组合学，其基数会爆炸式增长；每个连续位置的潜在项数非常大，并且序列中的每一项都会将这些可能性相乘。例如，考虑五个单词的序列，每个项目的可能性接近英语词汇量的大小，因此总数将达到五次方。我们在[第17章](ch17.html#Attention)中提供了处理此类情况的简单策略。
- en: Why Bother with Extra Features?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么要关心额外的特征？
- en: Sometimes it is useful to step back and ask if a new technology is actually
    worth caring about. So far in this section, we’ve introduced four new paradigms
    for thinking about a recommender problem. That level of detail may seem surprising
    and potentially even unnecessary.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候退一步思考一个新技术是否真的值得关注是很有用的。到目前为止，在本节中，我们介绍了四种思考推荐问题的新范式。这种详细程度可能令人惊讶，甚至可能显得不必要。
- en: One of the core reasons that things like context- and query-based recommendations
    become relevant is to deal with some of the issues mentioned before around sparsity
    and cold starting. Sparsity makes things that aren’t cold seem cold via the learner’s
    underexposure to them, but true cold starting also exists because of new items
    being added to catalogs with high frequency in most applications. We will address
    cold starting in detail, but for now, suffice it to say that one strategy for
    warm starting is to use other features that *are* available even in this regime.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 像上下文和查询导向的推荐这样的事物之所以变得相关，是因为要处理之前提到的一些关于稀疏性和冷启动的问题。稀疏性使得那些本不算冷的事物因为学习者对其低频率的暴露而看起来冷，但真正的冷启动也因新项目在大多数应用中高频添加到目录中而存在。我们将详细讨论冷启动，但现在可以说，对于暖启动的一种策略是利用即使在这种情况下也可用的其他特征。
- en: In applications of ML that are explicitly feature based, we rarely battle the
    cold-start problem to such a degree, because at inference time we’re confident
    that the model parameters useful for prediction are well aligned with those features
    that are available. In this way, feature-included recommendation systems are bootstrapping
    from a potentially weaker learner that has more guaranteed performance via always-available
    features.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在显式基于特征的ML应用中，我们很少会如此严重地遇到冷启动问题，因为在推断时，我们相信用于预测的模型参数与可用的特征是良好对齐的。通过这种方式，包含特征的推荐系统可以从一个潜在较弱的学习者引导启动，通过始终可用的特征保证更可靠的性能。
- en: The second analogy that the previous architectures are reflecting is that of
    boosting. Boosted models operate via the observation that ensembles of weaker
    learners can reach better performance. Here we are asking for some additional
    features to help these networks ensemble with weak learners, to boost their performance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 前面架构反映的第二个类比是增强学习。增强学习模型通过观察到弱学习者的集合可以达到更好的性能。在这里，我们要求一些额外的特征来帮助这些网络与弱学习者合奏，以提高它们的性能。
- en: Encoder Architectures and Cold Starting
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器架构和冷启动
- en: The previous problem framings of various types of recommendation problems point
    out four model architectures, each fitting into our general framework of collector,
    ranker, and server. With this understanding, let’s discuss in a bit more detail
    how model architecture can become intertwined with serving architecture. In particular,
    we also need to discuss feature encoders.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 各种类型的推荐问题的前期问题框架指出了四种模型架构，每种都适合我们的收集器、排序器和服务器的一般框架。在了解了这一点之后，让我们稍微详细地讨论一下模型架构如何与服务架构交织在一起。特别是，我们还需要讨论特征编码器。
- en: The key opportunity from encoder-augmented systems is that for users, items,
    or contexts without much data, we can still form embeddings on the fly. Recall
    from before that our embeddings make the rest of our system possible, but cold-starting
    recommendations is a huge challenge.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器增强系统的关键机会是对于没有太多数据的用户、项目或上下文，我们仍然可以即时形成嵌入。回想一下，我们的嵌入使我们的系统的其余部分成为可能，但冷启动推荐是一个巨大的挑战。
- en: The *two-towers architecture*—or dual-encoder networks—introduced in [“Sampling-Bias-Corrected
    Neural Modeling for Large Corpus Item Recommendations”](https://oreil.ly/gQHfo)
    by Xinyang Yi et al. is shown in [Figure 7-2](#fig:two-towers-arch) explicit model
    architecture is aimed at prioritizing features of both the user and items when
    building a scoring model for a recommendation system. We’ll see a lot more discussion
    of matrix factorization (MF), which is a kind of latent collaborative filtering
    (CF) derived from the user-item matrix and some linear algebraic algorithms. In
    the preceding section, we explained why additional features matter. Adding these
    *side-car* features into an MF paradigm is possible and has shown to be successful—for
    example, applications [CF for implicit feedback](https://oreil.ly/cbePb), [factorization
    machines](http://libfm.org/), and [SVDFeature](https://oreil.ly/cN7fP). However,
    in this model we will take a more direct approach.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Xinyang Yi等人在[“大语料库项目推荐的采样偏差校正神经建模”](https://oreil.ly/gQHfo)中介绍的*双塔架构*——或双编码器网络——在[图7-2](#fig:two-towers-arch)中显示了显式的模型架构，旨在在构建推荐系统的评分模型时，优先考虑用户和项目的特征。我们将看到更多关于矩阵分解（MF）的讨论，这是一种从用户-项目矩阵派生出的潜在协同过滤（CF）的一种线性代数算法。在前一节中，我们解释了为什么额外的特征很重要。将这些*附加*特征添加到MF范式中是可能的，并且已经被证明是成功的——例如，应用于[隐式反馈的CF](https://oreil.ly/cbePb)，[分解机器](http://libfm.org/)和[SVDFeature](https://oreil.ly/cN7fP)等应用。然而，在这个模型中，我们将采取更直接的方法。
- en: '![Two Towers Architecture](assets/brpj_0702.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![两个塔的架构](assets/brpj_0702.png)'
- en: Figure 7-2\. The two towers responsible for the two embeddings
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. 负责两个嵌入的两个塔
- en: In this architecture, we take the left tower to be responsible for items and
    the right tower to be responsible for the user and, when appropriate, context.
    These two tower architectures are inspired by the NLP literature and, in particular,
    [“Learning Text Similarity with Siamese Recurrent Networks”](https://oreil.ly/m7IrK)
    by Paul Neculoiu et al.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，我们将左侧塔视为负责项目的部分，将右侧塔视为负责用户和（在适当时）上下文的部分。这两种塔式架构受自然语言处理文献的启发，特别是Paul Neculoiu等人的[“使用连体递归网络学习文本相似性”](https://oreil.ly/m7IrK)。
- en: Let’s detail how this model architecture is applied to recommending videos on
    YouTube. For a full overview of where this architecture was first introduced,
    see [“Deep Neural Networks for YouTube Recommendations”](https://oreil.ly/aXekc)
    by Paul Covington et al. Training labels will be given by clicks, but with an
    additional regression feature <math alttext="r Subscript i Baseline element-of
    StartSet 0 comma 1 EndSet"><mrow><msub><mi>r</mi> <mi>i</mi></msub> <mo>∈</mo>
    <mfenced close="}" open="{" separators=""><mn>0</mn> <mo>,</mo> <mn>1</mn></mfenced></mrow></math>
    , where the minimum value corresponds to a click but trivial watch time, and the
    maximum of the range corresponds to a full watch.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细说明这种模型架构如何应用于YouTube视频推荐。要了解此架构首次引入的完整概述，请参见[“YouTube推荐的深度神经网络”](https://oreil.ly/aXekc)由保罗·科温顿等人撰写。训练标签将通过点击来给出，但还带有额外的回归特征
    <math alttext="r Subscript i Baseline element-of StartSet 0 comma 1 EndSet"><mrow><msub><mi>r</mi>
    <mi>i</mi></msub> <mo>∈</mo> <mfenced close="}" open="{" separators=""><mn>0</mn>
    <mo>,</mo> <mn>1</mn></mfenced></mrow></math> ，其中最小值对应于点击但是没有明显的观看时间，而范围的最大值对应于完全观看。
- en: As we’ve mentioned, this model architecture will explicitly include features
    from both user and items. The video features will consist of categorical and continuous
    features, like `VideoId`, `ChannelId`, `VideoTopic`, and so on. An embedding layer
    is used for many of the categorical features to move to dense representations.
    The user features include watch histories via bag of words and standard user features.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，这种模型架构明确包括来自用户和项目的特征。视频特征包括分类和连续特征，如`VideoId`、`ChannelId`、`VideoTopic`等。嵌入层用于许多分类特征以转换为密集表示。用户特征包括通过词袋和标准用户特征的观看历史。
- en: 'This model structure combines many of the ideas you’ve seen before but has
    relevant takeaways for our system architecture. First is the idea of sequential
    training. Each *temporal batch* of samples should be trained in sequence to ensure
    that model drift is shown to the model; we will discuss prequential datasets in
    [“Prequential validation”](ch10.html#prequential). Next, we present an important
    idea for the productionizing of these kinds of models: encoders.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型结构结合了许多您之前见过的想法，但对我们的系统架构有相关的收获。首先是顺序训练的概念。每个*时间批次*的样本应按顺序训练，以确保模型漂移显示给模型；我们将在[“预测验证”](ch10.html#prequential)中讨论预测数据集。接下来，我们为这些模型的产品化提出了一个重要的想法：编码器。
- en: In these models, we have feature encoders as the early layers in both towers,
    and when we move to inference, we will still need these encoders. When performing
    the online recommendations, we will be given `UserId` and `VideoId` and will first
    need to collect their features. As discussed in [“Feature Stores”](ch06.html#feature-stores),
    the feature store will be useful in getting these raw features, but we need to
    also encode the features into the dense representations necessary for inference.
    This is something that can be stored in the feature store for known entities,
    but for unknown entities we will need to do the feature embedding at inference
    time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些模型中，我们将特征编码器作为两个塔的早期层，并且当我们转移到推断时，我们仍然需要这些编码器。在执行在线推荐时，我们将获得`UserId`和`VideoId`，并且首先需要收集它们的特征。如[“特征存储”](ch06.html#feature-stores)所述，特征存储将有助于获取这些原始特征，但我们还需要将特征编码为推断所需的密集表示。对于已知实体，可以将此信息存储在特征存储中，但对于未知实体，我们需要在推断时进行特征嵌入。
- en: Encoding layers serve as a simple model for mapping a collection of features
    to a dense representation. When fitting encoding layers as the first step in a
    neural network, the common strategy is to take the first <math alttext="k"><mi>k</mi></math>
    layers and reuse them as an encoder model. More specifically, if <math alttext="script
    upper L Superscript i Baseline comma 0 less-than-or-equal-to i less-than-or-equal-to
    k"><mrow><msup><mi>ℒ</mi> <mi>i</mi></msup> <mo>,</mo> <mn>0</mn> <mo>≤</mo> <mi>i</mi>
    <mo>≤</mo> <mi>k</mi></mrow></math> are the layers responsible for feature encoding,
    call <math alttext="upper E m b left-parenthesis ModifyingAbove upper V With caret
    right-parenthesis equals script upper L Superscript k Baseline left-parenthesis
    script upper L Superscript k minus 1 Baseline left-parenthesis ellipsis script
    upper L Superscript 0 Baseline left-parenthesis ModifyingAbove upper V With caret
    right-parenthesis right-parenthesis right-parenthesis"><mrow><mi>E</mi> <mi>m</mi>
    <mi>b</mi> <mrow><mo>(</mo> <mover accent="true"><mi>V</mi> <mo>^</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mi>ℒ</mi> <mi>k</mi></msup> <mrow><mo>(</mo>
    <msup><mi>ℒ</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup> <mrow><mo>(</mo>
    <mo>...</mo> <msup><mi>ℒ</mi> <mn>0</mn></msup> <mrow><mo>(</mo> <mover accent="true"><mi>V</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
    the function that maps a feature vector <math alttext="ModifyingAbove upper V
    With caret"><mover accent="true"><mi>V</mi> <mo>^</mo></mover></math> to its dense
    representation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 编码层用作将一组特征映射到稠密表示的简单模型。在神经网络中将编码层作为第一步时，常见的策略是取前<math alttext="k"><mi>k</mi></math>层并将其重用为编码器模型。更具体地说，如果<math
    alttext="script upper L Superscript i Baseline comma 0 less-than-or-equal-to i
    less-than-or-equal-to k"><mrow><msup><mi>ℒ</mi> <mi>i</mi></msup> <mo>,</mo> <mn>0</mn>
    <mo>≤</mo> <mi>i</mi> <mo>≤</mo> <mi>k</mi></mrow></math>是负责特征编码的层，则称<math alttext="upper
    E m b left-parenthesis ModifyingAbove upper V With caret right-parenthesis equals
    script upper L Superscript k Baseline left-parenthesis script upper L Superscript
    k minus 1 Baseline left-parenthesis ellipsis script upper L Superscript 0 Baseline
    left-parenthesis ModifyingAbove upper V With caret right-parenthesis right-parenthesis
    right-parenthesis"><mrow><mi>E</mi> <mi>m</mi> <mi>b</mi> <mrow><mo>(</mo> <mover
    accent="true"><mi>V</mi> <mo>^</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>ℒ</mi>
    <mi>k</mi></msup> <mrow><mo>(</mo> <msup><mi>ℒ</mi> <mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msup>
    <mrow><mo>(</mo> <mo>...</mo> <msup><mi>ℒ</mi> <mn>0</mn></msup> <mrow><mo>(</mo>
    <mover accent="true"><mi>V</mi> <mo>^</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>)</mo></mrow></mrow></math>为将特征向量<math alttext="ModifyingAbove upper V With
    caret"><mover accent="true"><mi>V</mi> <mo>^</mo></mover></math>映射到其稠密表示的函数。
- en: In our previous system architecture, we would include this encoder as part of
    the fast layer, after receiving features from the feature store. It’s also important
    to note that we would still want to utilize vector search; these feature embedding
    layers are used upstream of the vector search and nearest neighbor searches.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们以前的系统架构中，我们将这个编码器作为快速层的一部分，收到来自特征存储的特征后。还要注意的是，我们仍然希望利用向量搜索；这些特征嵌入层在向量搜索和最近邻搜索的上游使用。
- en: Encoder as a Service
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器作为服务
- en: Encoders and retrieval are a key part of the multistage recommendation pipeline.
    We’ve spoken briefly about the latent spaces in question (for more details, see
    [“Latent Spaces”](ch10.html#latent-spaces)), and we’ve alluded to an *encoder*.
    Briefly, an encoder is the model that converts users, items, queries, etc., into
    the latent space in which you’ll perform nearest-neighbors search. These models
    can be trained via a variety of processes, many of which will be discussed later,
    but it’s important to discuss where they live once trained.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和检索是多阶段推荐流水线的关键部分。我们曾简要谈到过涉及的潜在空间（更多细节请参见[“潜在空间”](ch10.html#latent-spaces)），我们也提到了一个*编码器*。简而言之，编码器是将用户、项目、查询等转换为您将执行最近邻搜索的潜在空间的模型。这些模型可以通过多种过程进行训练，其中许多将在后面讨论，但重要的是讨论一下它们在训练后的存储位置。
- en: Encoders are often simple API endpoints that take the content to be embedded
    and return a vector (a list of floats). Encoders often work at the batch layer
    to encode all the documents/items that will be retrieved, but they must *also*
    be connected to the real-time layer to encode the queries as they come in. A common
    pattern is to set up a batch endpoint and a single query endpoint to facilitate
    optimization for both modalities. These endpoints should be fast and highly available.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器通常是简单的 API 端点，接受要嵌入的内容并返回一个向量（一组浮点数）。编码器通常在批处理层工作，以对将要检索的所有文档/项目进行编码，但它们也必须*同时*连接到实时层，以在查询到来时对其进行编码。一个常见的模式是设置一个批处理端点和一个单一查询端点，以促进两种模式的优化。这些端点应该快速并且高度可用。
- en: If you’re working with text data, a good starting place is to use BERT or GPT-based
    embeddings. The easiest at this time are provided as a hosted service from OpenAI.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您处理文本数据，一个很好的起点是使用基于 BERT 或 GPT 的嵌入。此时最简单的方法是从 OpenAI 提供的托管服务获取。
- en: Deployment
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署
- en: Like many ML applications, the final output of a recommendation system is itself
    a small program that runs continuously and exposes an API to interact with it;
    batch recommendations are often a powerful place to start, performing all the
    necessary recommendations ahead of time. Throughout this chapter, we’ve seen the
    pieces embedded in our backend system, but now we will discuss the components
    closer to the user.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 像许多ML应用程序一样，推荐系统的最终输出本身是一个持续运行并公开API以与之交互的小程序；批量推荐通常是一个强大的起点，提前执行所有必要的推荐。在本章中，我们已经看到了嵌入在后端系统中的部件，但现在我们将讨论更靠近用户的组件。
- en: In our relatively general architecture, the server is responsible for handing
    over the recommendations, after all the work that comes before, and should adhere
    to a preset schema. But what does this deployment look like?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们相对通用的架构中，服务器负责在处理所有前面工作后交付推荐结果，并应遵循预设的模式。但是部署看起来是什么样子呢？
- en: Models as APIs
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型作为 API
- en: 'Let’s discuss two systems architectures that might be appropriate for serving
    your models in production: microservice and monolith.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论两种系统架构，这些架构可能适合在生产中为您的模型提供服务：微服务和单体。
- en: 'In web applications, this dichotomy is well covered from many perspectives
    and special use cases. As ML engineers, data scientists, and potentially data
    platform engineers, it’s not necessary to dig deep into this area, but it’s essential
    to know the basics:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在Web应用程序中，从许多角度和特殊用例涵盖了这种二分法。作为ML工程师、数据科学家，以及潜在的数据平台工程师，深入挖掘这个领域并不是必要的，但了解基础知识是至关重要的：
- en: Microservice architectures
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构
- en: Each component of the pipeline should be its own small program with a clear
    API and output schema. Composing these API calls allows for flexible and predictable
    pipelines.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的每个组件应该是自己的小程序，具有清晰的API和输出模式。组合这些API调用可以实现灵活和可预测的管道。
- en: Monolithic architectures
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 单体架构
- en: One application should contain all the necessary logic and components for model
    predictions. Keeping the application self-contained means fewer interfaces that
    need to be kept aligned and fewer rabbit holes to hunt around in when a location
    in your pipeline is being starved.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个应用程序应包含所有模型预测所需的逻辑和组件。保持应用程序自包含意味着需要保持一致的接口较少，当管道中的某个位置饥饿时，需要搜索的兔子洞也较少。
- en: 'Whatever you choose as your strategy, you’ll need to make a few decisions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您选择什么策略，您都需要做出几个决定：
- en: '*How large is the necessary application?*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*所需应用的规模有多大？*'
- en: If your application will need fast access to large datasets at inference time,
    you’ll need to think carefully about memory requirements.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的应用在推理时需要快速访问大型数据集，您需要仔细考虑内存需求。
- en: '*What access does your application need?*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*您的应用程序需要什么访问权限？*'
- en: We’ve previously discussed using technologies like bloom filters and feature
    stores. These resources may be tightly coupled to your application (by building
    them in memory in the application) or may be an API call away. Make sure your
    deployment accounts for these relationships.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过使用布隆过滤器和特征存储等技术。这些资源可能与您的应用程序紧密耦合（通过在应用程序中内存构建它们）或可能只是一个 API 调用。确保您的部署考虑了这些关系。
- en: '*Should your model be deployed to a single node or a cluster?*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*您的模型应部署到单节点还是集群？*'
- en: For some model types, even at the inference step we wish to utilize distributed
    computing. This will require additional configuration to allow for fast parallelization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些模型类型，即使在推理阶段，我们也希望利用分布式计算。这将需要额外的配置以允许快速并行化。
- en: '*How much replication do you need?*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*您需要多少复制？*'
- en: Horizontal scaling allows you to have multiple copies of the same service running
    simultaneously to reduce the demand on any particular instance. This is important
    for ensuring availability and performance. As we horizontally scale, each service
    can operate independently, and various strategies exist for coordinating these
    services and an API request. Each replica is usually its own containerized application,
    and these APIs like CoreOS and Kubernetes are used to manage these. The requests
    themselves must also be balanced to the different replicas via something like
    nginx.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 水平扩展允许您同时运行多个相同服务的副本，以减少对任何特定实例的需求。这对确保可用性和性能至关重要。随着我们的水平扩展，每个服务可以独立运行，有多种策略用于协调这些服务和
    API 请求。每个副本通常是其自身的容器化应用程序，这些 API 如 CoreOS 和 Kubernetes 用于管理它们。请求本身也必须通过诸如 nginx
    之类的方式平衡到不同的副本上。
- en: '*What are the relevant APIs that are exposed?*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*哪些相关的 API 是公开的？*'
- en: Each application in the stack should have a clear set of exposed schemas and
    an explicit communication about the types of other applications that may call
    to the APIs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 堆栈中的每个应用程序应该有一组明确公开的模式，并明确关于可能调用 API 的其他应用程序类型的通信。
- en: Spinning Up a Model Service
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型服务的启动
- en: So what can you use to get your model into an application? A variety of frameworks
    for application development are useful; some of the most popular in Python are
    Flask, FastAPI, and Django. Each has different advantages, but we’ll discuss FastAPI
    here.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，您可以用什么将您的模型放入应用程序中呢？有许多应用开发框架是有用的；在 Python 中最流行的一些包括 Flask、FastAPI 和 Django。每种都有不同的优点，但我们在这里将讨论
    FastAPI。
- en: FastAPI is a targeted framework for API applications, making it especially well
    fit for serving ML models. It calls itself an asynchronous server gateway interface
    (ASGI) framework, and its specificity grants a ton of simplicity.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 是一个针对 API 应用程序的目标框架，使其特别适合用于提供 ML 模型。它称自己为异步服务器网关接口（ASGI）框架，其特定性带来了大量的简便性。
- en: 'Let’s take a simple example of turning a fit torch model into a service with
    the FastAPI framework. First, let’s utilize an artifact store to pull down our
    fit model. Here we are using the Weights & Biases artifact store:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个简单的例子来将一个适合的 torch 模型转化为一个使用 FastAPI 框架的服务。首先，让我们利用一个工件存储库来下载我们的适合模型。这里我们使用的是
    Weights & Biases 的工件存储库：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This looks just like your notebook workflow, so let’s see how easy it is to
    integrate this with FastAPI:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来就像您的笔记本工作流程一样，所以让我们看看如何将其与 FastAPI 集成，这是多么容易：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I hope you share my enthusiasm that we now have a model as a service in five
    additional lines of code. While this scenario includes simple examples of logging,
    we’ll discuss logging in greater detail later in this chapter to help you improve
    observability in your applications.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望您能分享我的热情，我们现在已经用额外的五行代码将一个模型作为一个服务。虽然这个场景包括了简单的日志示例，但我们将在本章后面更详细地讨论日志记录，以帮助您提高应用程序的可观察性。
- en: Workflow Orchestration
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作流编排
- en: 'The other component necessary for your deployed system is workflow orchestration.
    The model service is responsible for receiving requests and serving results, but
    many system components need to be in place for this service to do anything of
    use. These workflows have several components, so we will discuss them in sequence:
    containerization, scheduling, and CI/CD.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 部署系统所需的另一个组件是工作流编排。模型服务负责接收请求并提供结果，但是为了使此服务有用，许多系统组件需要就位。这些工作流程具有多个组件，因此我们将按顺序讨论它们：容器化、调度和
    CI/CD。
- en: Containerization
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器化
- en: 'We’ve discussed how to put together a simple service that can return the results,
    and we suggested using FastAPI; however, the question of environments is now relevant.
    When executing Python code, it is important to keep the environment consistent
    if not identical. FastAPI is a library for designing the interfaces; Docker is
    the software that manages the environment that code runs in. It’s common to hear
    Docker described as a container or containerization tool: this is because you
    load a bunch of apps—or executable components of code—into one shared environment.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何组合一个简单的服务，可以返回结果，并建议使用 FastAPI；然而，环境的问题现在是相关的。在执行 Python 代码时，保持环境的一致性，如果不是相同的话，这是很重要的。FastAPI
    是设计接口的库；Docker 是管理代码运行环境的软件。常常听到 Docker 被描述为容器或容器化工具：这是因为您将一堆应用程序或可执行的代码组件加载到一个共享环境中。
- en: We have a few subtle things to note at this point. The meaning of *environment*
    encapsulates both the Python environment of package dependencies and the larger
    environment, including the operating system or GPU drivers. The environment is
    usually initialized from a predetermined *image* that installs the most basic
    aspects of what you’ll need access to and in many cases is less variable across
    services to promote consistency and standardization. Finally, the container is
    usually equipped with a list of infrastructure code necessary to work wherever
    it is to be deployed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们有一些微妙的事情需要注意。*环境*的含义包括Python环境中的软件包依赖项以及更大的环境，包括操作系统或GPU驱动程序。环境通常从预定的*镜像*中初始化，该镜像安装了您需要访问的基本内容，在许多情况下在服务之间变化较小，以促进一致性和标准化。最后，容器通常配备了一系列基础设施代码，这些代码在部署到任何地方时都是必需的。
- en: In practice, you specify details of the Python environment via your *requirements*
    file, which consists of a list of Python packages. Note that some library dependencies
    are outside Python and will require additional configuration mechanisms. The operating
    system and drivers are usually built as part of a base image; you can find these
    on DockerHub or similar. Finally, *infrastructure as code* is a paradigm wherein
    you write code to orchestrate the necessary steps in getting your container configured
    to run in the infrastructure it will be deployed into. Dockerfile and Docker Compose
    are specific to the Docker container interfacing with infrastructure, but you
    can further generalize these concepts to include other details of the infrastructure.
    This infrastructure as code begins to encapsulate provisioning of resources in
    your cloud, setting up open ports for network communication, access control via
    security roles, and more. A common way to write this code is in Terraform. This
    book doesn’t dive into infrastructure specification, but infrastructure as code
    is becoming a more important tool to the ML practitioner. Many companies are beginning
    to attempt to simplify these aspects of training and deploying systems including
    Weights & Biases or Modal.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，您可以通过*requirements*文件来指定Python环境的详细信息，该文件包含了一系列Python包。请注意，一些库依赖关系在Python之外，并且需要额外的配置机制。操作系统和驱动程序通常作为基础镜像的一部分构建；您可以在DockerHub或类似的地方找到它们。最后，*基础设施即代码*是一种范式，您可以编写代码来编排在部署到的基础设施中运行您的容器所需的步骤。Dockerfile和Docker
    Compose专门用于Docker容器与基础设施的接口，但您可以进一步将这些概念推广到包括基础设施的其他细节。这种基础设施即代码开始封装云中资源的供应、设置用于网络通信的开放端口、通过安全角色进行访问控制等。编写此代码的常见方式是使用Terraform。本书不深入讨论基础设施规范，但基础设施即代码正在成为ML从业者更重要的工具。许多公司正开始尝试简化培训和部署系统的这些方面，包括Weights
    & Biases或Modal。
- en: Scheduling
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调度
- en: 'Two paradigms exist for scheduling jobs: cron and triggers. Later we’ll talk
    more about the continuous training loop and active learning processes, but upstream
    of those is your ML workflow. ML workflows are a set of ordered steps necessary
    to prepare your model for inference. We’ve introduced our notion of collector,
    ranker, and server, which are organized into a sequence of stages for recommendation
    systems—but these are the three coarsest elements of the system topology.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种调度作业的范式：cron和触发器。稍后我们将更详细地讨论持续训练循环和主动学习过程，但在这些过程之上是您的ML工作流程。ML工作流是为了为推理准备您的模型而必需的一组有序步骤。我们介绍了我们的收集器、排名器和服务器的概念，它们被组织成推荐系统的一系列阶段，但这些是系统拓扑结构中最粗略的三个元素。
- en: In ML systems, we frequently assume that there’s an upstream stage of the workflow
    that corresponds to data transformations, as discussed in [Chapter 6](ch06.html#data-processing).
    Wherever that stage takes place, the output of those transformations results in
    our vector store—and potentially the additional feature stores. The handoff between
    those steps and the next steps in your workflow are the result of a job scheduler.
    As mentioned previously, tools like Dagster and Airflow can run sequences of jobs
    with dependent assets. These kinds of tools are needed to orchestrate the transitions
    and to ensure that they’re timely.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统中，我们经常假设工作流程的上游阶段对应于数据转换，如[第6章](ch06.html#data-processing)中所讨论的那样。无论这个阶段发生在哪里，这些转换的输出将导致我们的向量存储，可能还包括额外的特征存储。这些步骤之间的交接以及工作流程中的下一步骤是作业调度器的结果。正如之前提到的，像Dagster和Airflow这样的工具可以运行依赖资产的作业序列。这些工具需要编排过渡并确保它们及时完成。
- en: '*Cron* refers to a time schedule where a workflow should begin—for example,
    hourly at the top of the hour or four times a day. *Triggers* refers to the instigation
    of a job run when another event has taken place—for example, if an endpoint receives
    a request, or a set of data gets a new version, or a limit of responses is exceeded.
    These are meant to capture more ad hoc relationships between the next job stage
    and the trigger. Both paradigms are very important.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*Cron* 是指工作流应该开始的时间表，例如每小时的整点或一天四次。*触发器* 是指当发生另一个事件时启动作业运行的情况，例如如果一个端点接收请求，或者一组数据有了新版本，或者超过响应限制。这些旨在捕捉下一个作业阶段与触发器之间的更多临时关系。这两种范式都非常重要。'
- en: CI/CD
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CI/CD
- en: Your workflow execution system is the backbone of your ML systems, often the
    bridge between the data collection process, the training process, and the deployment
    process. Modern workflow execution systems also include automatic validation and
    tracking so that you can audit the steps on the way to production.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您的工作流执行系统是您的机器学习系统的支柱，通常是数据收集过程、训练过程和部署过程之间的桥梁。现代工作流执行系统还包括自动验证和跟踪，以便您可以审核通往生产的步骤。
- en: '*Continuous integration* (CI) is a term taken from software engineering to
    enforce a set of checks on new code in order to accelerate the development process.
    In traditional software engineering, this comprises automating unit and integration
    testing, usually run after checking the code into version control. For ML systems,
    CI may mean running test scripts against the model, checking the typed output
    of data transformations, or running validation sets through the model and benchmarking
    the performance against previous models.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*持续集成* (CI) 是从软件工程中借用的术语，用于对新代码强制执行一系列检查，以加速开发过程。在传统的软件工程中，这包括自动化单元测试和集成测试，通常是在将代码检入版本控制之后运行。对于机器学习系统，CI
    可能意味着对模型运行测试脚本，检查数据转换的输入输出类型，或者运行验证集通过模型，并将性能与先前模型进行基准测试。'
- en: '*Continuous deployment* (CD) is also a term popularized in software engineering
    to refer to automating the process of pushing new packaged code into an existing
    system. In software engineering, deploying code when it has passed the relevant
    checks speeds development and reduces the risk of stale systems. In ML, CD can
    involve strategies like automatically deploying your new model behind a service
    endpoint in shadow (which we’ll discuss in [“Shadowing”](#shadowing-sect)) to
    test that it works as expected under live traffic. It could also mean deploying
    a model behind a very small allocation of an A/B test or multiarm bandit treatment
    to begin to measure effects on target outcomes. CD usually requires effective
    triggering by the requirements it has to satisfy before being pushed. It’s common
    to hear CD utilizing a model registry, where you house and index variations on
    your model.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*持续部署* (CD) 也是一个在软件工程中流行的术语，用来指代将新封装的代码自动推送到现有系统的过程。在软件工程中，当代码通过相关检查后部署可以加快开发速度，降低系统过时的风险。在机器学习中，CD
    可能涉及诸如将新模型自动部署在服务端点的背后以进行影子测试（我们将在[“Shadowing”](#shadowing-sect)中讨论），以验证其在实时流量下的预期表现。这也可能意味着在A/B测试或多臂老虎机处理的非常小分配后部署模型，以开始测量目标结果的影响。CD
    通常需要根据推送前必须满足的要求进行有效的触发。通常可以听到CD使用模型注册表，您可以在其中存储和索引模型的各种变体。'
- en: Alerting and Monitoring
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 警报和监控
- en: 'Alerting and monitoring take a lot of their inspiration from the DevOps world
    for software engineering. Here are some high-level principles that will guide
    our thinking:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 警报和监控大部分灵感来自于软件工程的DevOps世界。以下是一些指导我们思考的高层原则：
- en: Clearly defined schemas and priors
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确定义的模式和先验
- en: Observability
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可观察性
- en: Schemas and Priors
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式和先验
- en: When designing software systems, you almost always have expectations about how
    the components fit together. Just as you anticipate the input and output to functions
    when writing code, in software systems you anticipate these at each interface.
    This is relevant not only for microservice architectures; even in a monolith architecture,
    components of the system need to work together and often have boundaries between
    their defining responsibilities.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计软件系统时，您几乎总是对组件如何相互配合有期望。就像在编写代码时预期函数的输入和输出一样，在软件系统中，您预期每个接口的输入和输出。这不仅适用于微服务架构；即使在单体架构中，系统的组件也需要协同工作，并且通常在其定义的责任之间存在边界。
- en: 'Let’s make this more concrete via an example. You’ve built a user-item latent
    space, a feature store for user features, a bloom filter for client avoids (things
    the client specifically tells you they don’t want), and an experiment index that
    defines which of two models should be used for scoring. First let’s examine the
    latent space; when provided a `user_id`, we need to look up its representation,
    and we already have some assumptions:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来具体化这一点。您已经建立了一个用户-项目的潜在空间，一个用于用户特征的特征存储，一个用于客户避免的布隆过滤器（客户明确告诉您他们不想要的东西），以及一个实验索引，定义了应该用于评分的两个模型中的哪一个。首先让我们检查潜在空间；当提供一个`user_id`时，我们需要查找其表示，并且我们已经有一些假设：
- en: The `user_id` provided will be of the correct type.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供的`user_id`将是正确类型。
- en: The `user_id` will have a representation in our space.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user_id`将在我们的空间中有一个表示。'
- en: The representation returned will be of the correct type and shape.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回的表示将是正确的类型和形状。
- en: The component values of the representation vector will be in the appropriate
    domain. (*The support of representations in your latent space may vary day to
    day*.)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示向量的组件值将在适当的域中。（*潜在空间中表示的支持可能会每天变化*。）
- en: 'From here, we need look up the <math alttext="k"><mi>k</mi></math> ANN, which
    incurs more assumptions:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们需要查找`k`近邻，这涉及更多的假设：
- en: There are <math alttext="greater-than-or-equal-to k"><mrow><mo>≥</mo> <mi>k</mi></mrow></math>
    vectors in our latent space.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的潜在空间中有`≥ k`个向量。
- en: Those vectors adhere to the expected distributional behavior of the latent space.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些向量遵循潜在空间的预期分布行为。
- en: 'While these seem like relatively straightforward applications of unit tests,
    canonizing these assumptions is important. Take the last assumption in both of
    the two services: how can you know the appropriate domain for the representation
    vectors? As part of your training procedure, you’ll need to calculate this and
    then store it for access during the inference pipeline.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些看起来像是相对直接的单元测试应用，但是规范化这些假设是很重要的。考虑这两种服务中的最后一个假设：你如何知道表示向量的适当域？作为训练过程的一部分，您需要计算这一点，然后存储以便在推理管道期间访问。
- en: In the second case, when finding nearest neighbors in high-dimensional spaces,
    well-discussed difficulties arise in distributional uniformity, but this can mean
    particularly poor performance for recommendations. In practice, we have observed
    a spiky nature to the behavior of <math alttext="k"><mi>k</mi></math> -nearest
    neighbors in latent spaces, leading to difficult challenges downstream in ensuring
    diversity of recommendations. These distributions can be estimated as priors,
    and simple checks like KL divergence can be used online; we can estimate the average
    behavior of the embeddings and the difference between local geometries.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况下，在高维空间中找到最近的邻居时，会出现分布均匀性的困难，但这可能导致推荐性能特别差。在实践中，我们观察到在潜在空间中`k`个最近邻的行为呈现出尖锐的特性，这在确保推荐多样性时会带来后续的挑战。这些分布可以被估计为先验，并且像KL散度这样的简单检查可以在线上使用；我们可以估计嵌入的平均行为以及局部几何之间的差异。
- en: In both cases, collecting and logging the output of this information can provide
    a rich history of what is going on with your system. This can shorten debugging
    loops later if model performance is low in production.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，收集和记录这些信息的输出可以为系统的运行情况提供丰富的历史记录。如果模型在生产中的性能低下，这可以缩短调试循环。
- en: 'Returning to the possibility of `user_id` lacking a representation in our space:
    this is precisely the cold-start problem! In that case, we need to transition
    over to a different prediction pipeline: perhaps user-feature-based, explore-exploit,
    or even hardcoded recommendations. In this setting, we need to understand next
    steps when a schema condition is not met and then gracefully move forward.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 回到`user_id`在我们空间中缺乏表示的可能性：这正是冷启动问题！在这种情况下，我们需要切换到不同的预测管道：也许是基于用户特征的，探索-利用的，甚至是硬编码的推荐。在这种设置下，我们需要理解当模式条件不满足时的下一步操作，然后优雅地前进。
- en: Integration Tests
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成测试
- en: Let’s consider one higher-level challenge that might emerge in a system like
    this at the level of integration. Some refer to these issues as *entanglement.*
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑在像这样的系统中可能出现的一个更高级别的挑战。有些人称这些问题为*纠缠*。
- en: 'You’ve learned through experimentation that you should find <math alttext="k
    equals 20"><mrow><mi>k</mi> <mo>=</mo> <mn>20</mn></mrow></math> ANNs in the item
    space for a user to get good recommendations. You make a call to your representation
    space, get your 20 items, and pass them onto the filtering step. However, this
    user is quite picky; they have previously made many restrictions on their account
    about the kind of recommendations they allow: no shoes, no dresses, no jeans,
    no hats, no handbags—what’s a struggling recommendation system to do?'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实验，您已经了解到为了给用户提供良好的推荐，应在项目空间中找到<math alttext="k equals 20"><mrow><mi>k</mi>
    <mo>=</mo> <mn>20</mn></mrow></math>个ANNs。您向表征空间发出调用，获取您的20个项目，并将它们传递给过滤步骤。然而，这个用户非常挑剔；他们以前在账户上设置了许多关于允许的推荐种类的限制：不要鞋子，不要裙子，不要牛仔裤，不要帽子，不要手提包——这对一个挣扎的推荐系统来说是个难题。
- en: 'Naively, if you take the 20 neighbors and pass them into the bloom, you’re
    likely to be left with nothing! You can approach this challenge in two ways:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你简单地将20个相邻项传递到布隆中，你可能什么也得不到！您可以通过两种方式解决这个挑战：
- en: Allow for a callback from the filter step to the retrieval (see [“Predicate
    Pushdown”](ch15.html#PredicatePushdown))
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许从过滤步骤到检索的回调（见[“谓词下推”](ch15.html#PredicatePushdown)）
- en: Build a user distribution and store that for access during retrieval
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立用户分布并在检索期间存储该分布
- en: In the first approach, you give access to your filter step to call the retrieval
    step with a larger <math alttext="k"><mi>k</mi></math> until the requirements
    are satisfied after the bloom. Of course, this incurs significant slowdown as
    it requires multiple passes and ever-growing queries with redundancy! While this
    approach is simple, it requires building defensively and knowing ahead of time
    what may go wrong.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种方法中，您可以让您的过滤步骤调用检索步骤，使用更大的<math alttext="k"><mi>k</mi></math>，直到布隆之后满足要求。当然，这会导致显著的减速，因为它需要多次通过和越来越多的冗余查询！虽然这种方法很简单，但它需要防御性地构建，并提前了解可能出现的问题。
- en: In the second approach, during training, you can sample from the user space
    to build estimates of the appropriate <math alttext="k"><mi>k</mi></math> for
    varying numbers of avoids by user. Then, giving access to a lookup of total avoids
    by user to the collector can help defend against this behavior.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种方法中，在训练过程中，您可以从用户空间中抽样以建立适合不同避开数量的<math alttext="k"><mi>k</mi></math>的估算值。然后，向收集器提供用户总避开数量的查找可以帮助防范这种行为。
- en: Over-Retrieval
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过度检索
- en: Sometimes people in information retrieval perform *over-retrieval* to mitigate
    issues of conflicting requirements from the search request, which can arise if
    the user makes a search and applies many filters simultaneously. This is applicable
    in recommendation systems as well.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有时信息检索中的人们会*过度检索*以缓解搜索请求中冲突要求的问题，这可能会发生在用户进行搜索并同时应用多个过滤器时。这在推荐系统中同样适用。
- en: If you retrieve only exactly the number of potential recommendations you need
    to serve to the user, downstream rules or poor personalization scores can sometimes
    cause a serious issue for serving up recommendations. This is why it is common
    to retrieve more items than you anticipate showing to the user.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只检索与您预计向用户展示的潜在推荐数量相等的项目，下游规则或低个性化评分有时会导致严重问题。这就是为什么常常检索比您预期展示给用户的项目数量更多的原因。
- en: Observability
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可观察性
- en: Many tools in software engineering can assist with observability—understanding
    the *whys* of what’s going on in the software stack. Because the systems we are
    building become quite distributed, the interfaces become critical monitoring points,
    but the paths also become complex.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程中的许多工具可以帮助理解*软件堆栈*中发生的事情。由于我们正在构建的系统变得非常分布，接口成为关键的监控点，但路径也变得复杂。
- en: Spans and traces
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨度和跟踪
- en: Common terms in this area are *spans* and *traces,* which refer to two dimensions
    of a call stack, illustrated in [Figure 7-3](#fig:trace-spans). Given a collection
    of connected services, as in our preceding examples, an individual inference request
    will pass through some or all of those services in a sequence. The sequence of
    service requests is the *trace*. The potentially parallel time delays of each
    of these services is the *span*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这一领域常见的术语是*跨度*和*跟踪*，它们指的是调用堆栈的两个维度，在[图7-3](#fig:trace-spans)中有所说明。在我们之前的示例中给定一组连接的服务，一个个体推理请求将按顺序通过其中一些或全部这些服务。服务请求的序列是*跟踪*。每个服务的潜在并行时间延迟是*跨度*。
- en: '![A Trace-Span diagram for a request](assets/brpj_0703.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![一个请求的追踪跨度图示](assets/brpj_0703.png)'
- en: Figure 7-3\. The spans of a trace
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 追踪的跨度
- en: The graphical representation of spans usually demonstrates how the time for
    one service to respond comprises several other delays from other calls.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 跨度的图形表示通常展示了一个服务响应时间如何包括其他调用的各种延迟。
- en: '*Observability* enables you to see traces, spans, and logs in conjunction to
    appropriately diagnose the behavior of your system. In our example of utilizing
    a callback from the filter step to get more neighbors from the collector, we might
    see a slow response and wonder, “What has happened?” By viewing the spans and
    traces, we’d be able to see that the first call to the collector was as expected,
    then the filter step made a call to the collector, then another call to the collector,
    and so on, which built up a huge span for the filter step. Combining that view
    with logging would help us rapidly diagnose what might be happening.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*可观察性*使您能够同时查看追踪、跨度和日志，以适当地诊断系统行为。在我们的例子中，利用从过滤步骤的回调获取更多邻居的回调时，我们可能会看到一个缓慢的响应，并想知道发生了什么事情。通过查看跨度和追踪，我们能够看到第一次调用收集器如预期般进行，然后过滤步骤调用了收集器，然后又调用了收集器，依此类推，这为过滤步骤建立了一个巨大的跨度。将此视图与日志结合起来将帮助我们快速诊断可能发生的情况。'
- en: Timeouts
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超时
- en: In the preceding example, we had a long process that could lead to a very bad
    user experience. In most cases, we impose hard restrictions on how bad we let
    things get; these are called *timeouts*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们有一个可能导致非常糟糕用户体验的漫长过程。在大多数情况下，我们对事情变得如何的严格限制；这些被称为*超时*。
- en: Usually, we have an upper bound on how long we’re willing to wait for our inference
    response, so implementing timeouts aligns our system with these restrictions.
    It’s important in these cases to have a *fallback.* In the setting of recommendation
    systems, a fallback usually comprises things like the MPIR prepared such that
    it incurs minimal additional delay.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们对推断响应等待的时间有一个上限，因此实现超时可以使我们的系统符合这些限制。在这些情况下，拥有一个*备用方案*非常重要。在推荐系统的环境中，备用方案通常包括如MPIR这样的准备，以使其附加的延迟最小化。
- en: Evaluation in Production
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生产环境中的评估
- en: If the previous section was about understanding what’s coming into your model
    in production, this one might be summarized as what’s coming out of your model
    in production. At a high level, evaluation in production can be thought of as
    extending all your model-validation techniques to the inference time. In particular,
    you are looking at *what the model actually is doing*!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前一节是关于理解模型在生产中接收到的内容，那么这一节可以概括为生产中模型输出的内容。从高层次来看，生产中的评估可以被认为是将所有您的模型验证技术扩展到推断时间。特别是，您正在查看*模型实际正在执行的操作*！
- en: On one hand, we already have tools to do this evaluation. You can use the same
    methods to evaluate performance as you do for training, but now on real observations
    streaming in. However, this process is not as obvious as we might first guess.
    Let’s discuss some of the challenges.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，我们已经有了工具来进行这种评估。您可以使用与训练相同的方法来评估性能，但现在是在实时流中观察到的真实数据上。然而，这个过程并不像我们最初想象的那么明显。让我们讨论一些挑战。
- en: Slow Feedback
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓慢的反馈
- en: 'Recommendation systems fundamentally are trying to lead to item selection,
    and in many cases, purchases. But if we step back and think more holistically
    about the purpose of integrating recommendation systems into businesses, it’s
    to drive revenue. If you’re an ecommerce shop, item selection and revenue may
    seem easily associated: a purchase leads to revenue, so good item recommendation
    leads to revenue. However, what about returns? Or even a harder question: is this
    revenue incremental? One challenge with recommendation systems is that it can
    be difficult to draw a causal arrow between any metric used to measure the performance
    of your models to the business-oriented KPIs.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统基本上是试图导致物品选择，并在许多情况下是购买。但是，如果我们退后一步，更全面地思考将推荐系统整合到业务中的目的，那就是推动收入。如果您是一个电子商店，物品选择和收入可能似乎很容易关联：购买导致收入，因此良好的物品推荐导致收入。但是，退货呢？甚至更难的问题：这种收入是否是增量的？推荐系统面临的一个挑战是，很难在任何用于衡量模型性能的指标与业务导向的关键绩效指标之间画出因果关系。
- en: We call this *slow feedback* because sometimes the loop from a recommendation
    to a meaningful metric and back to the recommender can take weeks or even longer.
    This is especially challenging when you want to run experiments to understand
    whether a new model should be rolled out. The length of the test may need to stretch
    quite a bit more to get meaningful results.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称之为*慢反馈*，因为有时从推荐到有意义的度量再回到推荐系统可能需要几周甚至更长时间。当您希望运行实验以理解是否应该推出新模型时，这尤其具有挑战性。测试的长度可能需要更长时间才能获得有意义的结果。
- en: Usually, the team aligns on a proxy metric that the data scientists believe
    is a good estimator for the KPI, and that proxy metric is measured live. This
    approach has a huge variety of challenges, but it often suffices and provides
    motivation for more testing. Well-correlated proxies are often a great start to
    get directional information indicating where to take further iterations.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，团队会对数据科学家认为是KPI良好估计器的代理指标达成一致。该代理指标实时测量。这种方法有各种挑战，但通常足够并促使进行更多测试。良好相关的代理通常是获取方向性信息的好起点，指示进一步迭代的方向。
- en: Model Metrics
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型指标
- en: 'So, what are the key metrics to track for your model in production? Given that
    we’re looking at recommendation systems at inference time, we should seek to understand
    the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在生产中跟踪模型的关键指标是什么？考虑到我们在推断时间内正在研究推荐系统，我们应该努力理解以下内容：
- en: Distribution of recommendation across categorical features
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类特征的推荐分布
- en: Distribution of affinity scores
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亲和分数的分布
- en: Number of candidates
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选人数
- en: Distribution of other ranking scores
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他排名分数的分布
- en: As we discussed before, during the training process, we should be calculating
    broadly the ranges of our similarity scores in our latent space. Whether we are
    looking at high-level estimations or finer ones, we can use these distributions
    to get warning signals that something might be strange. Simply comparing the output
    of our model during inference, or over a set of inference requests, to these precompute
    distributions can be extremely helpful.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论过的，在训练过程中，我们应该广泛计算潜在空间中相似性分数的范围。无论我们是在看高级估计还是精细估计，我们可以使用这些分布来获得警示信号，表明可能存在异常。简单地将我们模型在推断期间或一组推断请求中的输出与这些预先计算的分布进行比较，可以极大地帮助。
- en: Comparing distributions can be a long topic, but one standard approach is computing
    *KL-divergence* between the observed distribution and the expected distribution
    from training. By computing KL divergence between these, we can understand how
    *surprising* the model’s predictions are on a given day.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 比较分布可能是一个长话题，但一个标准方法是计算观察分布与训练预期分布之间的*KL散度*。通过计算这些之间的KL散度，我们可以理解模型在某一天的预测有多*意外*。
- en: What we’d really like is to understand the receiver operating characteristic
    curve (ROC) of our model predictions with respect to one of our conversion types.
    However, this involves yet another integration to tie back to logging. Since our
    model API produces only the recommendation, we’ll still need to tie into logging
    from the web application to understand outcomes! To tie back in outcomes, we must
    join the model predictions with the logging output to get the evaluation labels,
    which can be done via log-parsing technologies (like Grafana, ELK, or Prometheus).
    We’ll see more of this in [Chapter 8](ch08.html#ch:wikipedia-e2e).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正希望的是理解模型预测在一个转化类型上的接收者操作特征曲线（ROC）。然而，这涉及到另一个整合以回溯到日志记录。由于我们的模型API仅生成建议，我们仍然需要从Web应用程序中与日志记录相结合以理解结果！为了关联结果，我们必须将模型预测与日志输出进行关联，以获取评估标签，这可以通过日志解析技术（如Grafana、ELK或Prometheus）完成。我们将在[第8章](ch08.html#ch:wikipedia-e2e)中看到更多关于这个的内容。
- en: Receiver Operating Characteristic Curve
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接收者操作特征曲线
- en: If we assume that the relevance scores are estimating whether the item will
    be relevant to the user, this forms a binary classification problem. Utilizing
    these (normalized) scores, we can build an ROC to estimate over the distributions
    of queries when the relevance score begins to accurately predict a relevant item
    via retrieval history. This curve can thus be used to estimate parameters like
    necessary retrieval depth or even problematic queries.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设相关性分数正在估计项目是否与用户相关，这形成了一个二元分类问题。利用这些（归一化）分数，我们可以构建一个ROC曲线，以估计在查询分布中，当相关性分数开始准确预测一个相关项目时的必要检索深度。因此，这条曲线可以用来估计参数，如必要的检索深度或甚至有问题的查询。
- en: Continuous Training and Deployment
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续的训练和部署
- en: It may feel like we’re done with this story since we have models tracked and
    production monitoring in place, but rarely are we satisfied with set-it-and-forget-it
    model development. One important characteristic of ML products is that models
    frequently need to be updated to even be useful. Previously, we discussed model
    metrics and that sometimes performance in production might look different from
    our expectations based on the trained models’ performance. This can be further
    exacerbated by model drift.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 或许我们会觉得故事已经结束了，因为我们已经跟踪了模型并在生产中进行了监控，但我们很少满足于设定即忘的模型开发。机器学习产品的一个重要特征是，模型经常需要更新才能有用。之前，我们讨论了模型指标，有时在生产中的性能可能与我们基于训练模型性能的预期有所不同。这可能会被模型漂移进一步加剧。
- en: Model Drift
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型漂移
- en: '*Model drift* is the notion that the same model may exhibit different prediction
    behavior over time, merely due to changes in the data-generating process. A simple
    example is a time-series forecasting model. When you build a time-series forecasting
    model, the especially unique property that is essential for good performance is
    *autoregression*: the value of the function covaries with previous values of the
    function. We won’t go into detail on time-series forecasting, but suffice it to
    say: your best hope of making a good forecast is to use up-to-date data! If you
    want to forecast stock prices, you should always use the most recent prices as
    part of your predictions.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型漂移*是同一模型随时间可能表现出不同预测行为的概念，仅仅因为数据生成过程发生变化。一个简单的例子是时间序列预测模型。当你建立一个时间序列预测模型时，特别重要的性质是*自回归*：函数的值与函数先前的值协变。我们不会详细讨论时间序列预测，但可以说：你做出良好预测的最佳希望是使用最新的数据！如果你想预测股票价格，你应该始终使用最近的价格作为预测的一部分。'
- en: This simple example demonstrates how models may drift, and forecasting models
    are not so different from recommendation models—especially when considering the
    seasonal realities of many recommendation problems. A model that did well two
    weeks ago needs to be retrained with recent data to be expected to continue to
    perform well.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的例子展示了模型可能如何漂移，而预测模型与推荐模型在考虑许多推荐问题的季节性现实时并没有太大不同。两周前表现良好的模型需要重新训练，使用最新数据才能继续表现良好的预期。
- en: One criticism of a model that drifts is “that’s the smoking gun of an overfit
    model,” but in reality these models require a certain amount of over-parameterization
    to be useful. In the context of recommendation systems, we’ve already seen that
    quirks like the Matthew effect have disastrous effects on the expected performance
    of a recommender model. If we don’t consider things like new items in our recommender,
    we are doomed to fail. Models can drift for a variety of reasons, often coming
    down to exogenous factors in the generating process that may not be captured by
    the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一个漂移的模型的批评是“这是一个过拟合模型的明显表现”，但实际上，这些模型需要一定量的过参数化才能有用。在推荐系统的背景下，我们已经看到，像马修效应这样的怪现象对推荐模型的预期性能有灾难性影响。如果我们不考虑推荐系统中的新项目等因素，我们注定会失败。模型可能因多种原因而漂移，通常归结为生成过程中的外生因素，这些因素可能未被模型捕捉到。
- en: One approach to dealing with and predicting stale models is to simulate these
    scenarios during training. If you suspect that the model goes stale mostly because
    of the distribution changing over time, you can employ sequential cross-validation—training
    on a contiguous period and testing on a subsequent period—but with a specified
    block of time delay. For example, if you think your model performance is going
    to decrease after two weeks because it’s being trained on out-of-date observations,
    then during training you can purposely build your evaluation to incorporate a
    two-week delay before measuring performance. This is called *two-phase prediction
    comparison*, and by comparing the performances, you can estimate drift magnitudes
    to keep an eye out in production.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 处理和预测模型陈旧的一种方法是在训练期间模拟这些情景。如果你怀疑模型主要因为分布随时间变化而变得陈旧，你可以采用顺序交叉验证——在连续的时间段进行训练，然后在随后的时间段进行测试，但带有指定的时间延迟。例如，如果你认为你的模型在两周后的表现会下降，因为它是基于过时观察结果训练的，那么在训练过程中，你可以故意构建你的评估，在测量性能之前加入两周的延迟。这被称为*两阶段预测比较*，通过比较性能，你可以估计在生产中要注意的漂移量。
- en: A wealth of statistical approaches can be used to rein in these differences.
    In lieu of a deep dive into variational modeling for variability and reliability
    for your predictions, we’ll discuss continuous training and deployment and open
    this peanut with a sledge hammer.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 有丰富的统计方法可以用来控制这些差异。我们不深入讨论变分建模以及为你的预测的可变性和可靠性开展的工作，而是讨论连续训练和部署，并用一把锤子打开这个花生。
- en: Deployment Topologies
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署拓扑
- en: Let’s consider a few structures for deploying models that will not only keep
    your models well in tune but also accommodate iteration, experimentation, and
    optimization.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑几种用于部署模型的结构，这不仅可以使你的模型保持良好的调整，还能够容纳迭代、实验和优化。
- en: Ensembles
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成
- en: '*Ensembles* are a type of model structure in which multiple models are built,
    and the predictions from those models are pooled together in one of a variety
    of ways. While this notion of an ensemble is usually packaged into the model called
    for inference, you can generalize the idea to your deployment topology.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ensembles* 是一种模型结构类型，其中构建了多个模型，并且这些模型的预测以各种方式汇总在一起。尽管这种集成的概念通常打包到推理模型中，但你可以将这个想法推广到你的部署拓扑中。'
- en: Let’s take an example that builds on our previous discussion of prediction priors.
    If we have a collection of models with comparable performance on a task, we can
    deploy them in an ensemble, weighted by their deviation from the prior distributions
    of prediction that we’ve set before. This way, instead of having a simple yes/no
    filter on the output of your model’s range, you can more smoothly transition potentially
    problematic predictions into more expected ones.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子，继续我们之前关于预测先验讨论的内容。如果我们有一组在任务上表现相当的模型，我们可以按其偏离我们设定的预测先验分布进行加权，以集成它们。这样，不仅仅在你的模型输出范围上有一个简单的是/否过滤器，而且可以更平滑地将潜在问题预测过渡为更符合预期的结果。
- en: 'Another benefit of treating the ensemble as a deployment topology instead of
    only a model architecture is that you can *hot-swap* components of an ensemble
    as you make improvements in specific subdomains of your observation feature space.
    Take, for example, a life-time-value (LTV) model comprising three components:
    one that predicts well for new clients, another for activated clients, and a third
    for super-users. You may find that pooling via a voting mechanism performs the
    best on average, so you decide to implement a bagging approach. This works well,
    but later you find a better model for the new clients. By using the deployment
    topology for your ensemble, you can swap in the new model for the new clients
    and start comparing performance in your ensemble in production. This brings us
    to the next strategy, model comparison.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 将集成视为部署拓扑而不仅仅是模型架构的另一个好处是，你可以在观察特征空间的特定子域中进行改进时，通过热插拔集成的组件。例如，生命周期价值（LTV）模型由三个组件组成：一个用于预测新客户，另一个用于激活客户，第三个用于超级用户。你可能会发现通过投票机制进行汇总在平均上表现最佳，因此你决定实施装袋方法。这样做效果很好，但后来你发现了一个更好的新客户模型。通过使用集成的部署拓扑，你可以将新客户的新模型换入，并开始在生产环境中比较集成的性能。这带我们进入下一个策略，模型比较。
- en: Ensemble Modeling
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成建模
- en: '*Ensemble modeling* is popular in all kinds of ML, built upon the simple notion
    that the mixture of expert opinions is strictly more effective than a single estimator.
    In fact, assume for a moment that you have <math alttext="upper M"><mi>M</mi></math>
    classifiers with error rate <math alttext="epsilon"><mi>ϵ</mi></math> ; then for
    an <math alttext="upper N"><mi>N</mi></math> class classification problem, your
    error would be <math alttext="upper P left-parenthesis y greater-than-or-equal-to
    k right-parenthesis equals sigma-summation Underscript k Overscript n Endscripts
    asterisk StartBinomialOrMatrix n Choose k EndBinomialOrMatrix epsilon Superscript
    k Baseline asterisk left-parenthesis 1 minus epsilon right-parenthesis Superscript
    n minus k"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>≥</mo> <mi>k</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>k</mi></mrow> <mi>n</mi></msubsup>
    <mo>*</mo> <mfenced close=")" open="(" separators=""><mfrac linethickness="0pt"><mi>n</mi>
    <mi>k</mi></mfrac></mfenced> <msup><mi>ϵ</mi> <mi>k</mi></msup> <mo>*</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>ϵ</mi><mo>)</mo></mrow>
    <mrow><mi>n</mi><mo>-</mo><mi>k</mi></mrow></msup></mrow></math> , and the exciting
    part is that this is smaller than <math alttext="epsilon"><mi>ϵ</mi></math> for
    all values less than 0.5!'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*集成建模* 在各种ML中都很流行，建立在一个简单的观念上，即专家意见的混合比单一估计器更有效。实际上，假设你有 <math alttext="upper
    M"><mi>M</mi></math> 分类器，错误率为 <math alttext="epsilon"><mi>ϵ</mi></math>；那么对于一个
    <math alttext="upper N"><mi>N</mi></math> 类分类问题，你的错误将是 <math alttext="upper P
    left-parenthesis y greater-than-or-equal-to k right-parenthesis equals sigma-summation
    Underscript k Overscript n Endscripts asterisk StartBinomialOrMatrix n Choose
    k EndBinomialOrMatrix epsilon Superscript k Baseline asterisk left-parenthesis
    1 minus epsilon right-parenthesis Superscript n minus k"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mi>y</mi> <mo>≥</mo> <mi>k</mi> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo>
    <mrow><mi>k</mi></mrow> <mi>n</mi></msubsup> <mo>*</mo> <mfenced close=")" open="("
    separators=""><mfrac linethickness="0pt"><mi>n</mi> <mi>k</mi></mfrac></mfenced>
    <msup><mi>ϵ</mi> <mi>k</mi></msup> <mo>*</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>ϵ</mi><mo>)</mo></mrow>
    <mrow><mi>n</mi><mo>-</mo><mi>k</mi></mrow></msup></mrow></math>，令人兴奋的是，这对于所有小于0.5的值都比
    <math alttext="epsilon"><mi>ϵ</mi></math> 小！'
- en: Shadowing
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阴影
- en: Deploying two models, even for the same task, can be enormously informative.
    We call this *shadowing* when one model is “live” and the other is secretly also
    receiving all the requests and doing inference, and logging the results, of course.
    By shadowing traffic to the other model, you get the best expectations possible
    about how the model behaves before making your model live. This is especially
    useful when wanting to ensure that the prediction ranges align with expectation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 部署两个模型，即使是同一个任务，也可以提供极大的信息量。当一个模型是“实时”的，而另一个在暗中也接收所有请求并进行推理和记录结果时，我们称之为*阴影*，当然。通过将流量引导到另一个模型，您可以在使您的模型实时之前获得有关模型行为的最佳期望。这在希望确保预测范围与期望一致时特别有用。
- en: In software engineering and DevOps, there’s a notion of *staging* for software.
    It’s a hotly contested question of “how much of the real infrastructure should
    staging see,” but shadowing is the staging of ML models. You can basically build
    a parallel pipeline for your entire infrastructure to connect for shadow models,
    or you can just put them both in the line of fire and have the request sent to
    both but use only one response. Shadowing is also crucial for implementing experimentation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程和DevOps中，有一个关于软件的*暂存*的概念。关于“暂存应该看到多少真实基础设施”的问题争议不小，但是模型的*阴影*是暂存的机制。你基本上可以为整个基础设施建立一个并行管道来连接阴影模型，或者你可以将它们都置于火线上，并将请求发送到两者，但仅使用一个响应。阴影对于实施实验至关重要。
- en: Experimentation
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验
- en: As good data scientists, we know that without a proper experimental framework,
    it’s risky to advertise much about the performance of a feature or, in this case,
    model. Experimentation can be handled with shadowing by having a controller layer
    that is taking the incoming requests and orchestrating which of the deployed models
    to curry the response along. A simple A/B experimentation framework might ask
    for a randomization at every request, whereas something like a multiarmed bandit
    will require the controller layer to have notions of the reward function.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 作为优秀的数据科学家，我们知道如果没有一个合适的实验框架，宣传一个功能或者模型的性能是有风险的。通过阴影，可以通过一个控制器层处理传入请求，并协调部署的模型来返回响应。一个简单的A/B实验框架可能会在每个请求时进行随机化，而像多臂老虎机这样的东西则需要控制器层具有奖励函数的概念。
- en: Experimentation is a deep topic that we don’t have the knowledge or space to
    do adequate justice, but it’s useful to know that this is where experimentation
    can fit into the larger deployment pipeline.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 实验是一个深入的话题，我们没有足够的知识或空间来充分展示，但知道实验可以如何适应更大的部署流程是有用的。
- en: The Evaluation Flywheel
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估飞轮
- en: By now, it’s likely obvious that a production ML model is far from a static
    object. Production ML systems of any kind are subject to as many deployment concerns
    as a traditional software stack, in addition to the added challenge of dataset
    shift and new users/items. In this section, we’ll look closely at the feedback
    loops introduced and understand how the components fit together to continuously
    improve our system—even with little input from a data scientist or ML engineer.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，一个生产ML模型显然不是一个静态对象。任何类型的生产ML系统都会受到与传统软件堆栈一样多的部署问题的影响，除了面临数据集变化和新用户/物品的额外挑战。在本节中，我们将密切关注引入的反馈循环，并理解各组件如何配合以持续改进我们的系统——即使数据科学家或ML工程师几乎没有输入。
- en: Daily Warm Starts
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每日热启动
- en: As we’ve now discussed several times, we need a connection between the continuous
    output of our model and retraining. The first simplest example of this is daily
    warm starts, which essentially ask us to utilize the new data seen each day in
    our system.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们现在已经多次讨论过的那样，我们需要将我们模型的连续输出与重新训练联系起来。这里最简单的例子是每日热启动，本质上要求我们利用系统中每天看到的新数据。
- en: As might already be obvious, some of the recommendation models that show great
    success are quite large. Retraining some of them can be a massive undertaking,
    and simply *rerunning everything* each day is often infeasible. So, what can be
    done?
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 正如可能已经显而易见的那样，一些显示出巨大成功的推荐模型非常庞大。重新训练其中一些可能是一项巨大的任务，而仅仅每天重新运行所有内容通常是不可行的。那么，可以做些什么呢？
- en: 'Let’s ground this conversation in the user-user CF example that we’ve been
    sketching out; the first step was to build an embedding via our similarity definition.
    Let’s recall:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个对话基于我们一直在草拟的用户-用户CF示例进行具体化；第一步是通过我们的相似性定义构建一个嵌入。让我们回想一下：
- en: <math alttext="normal upper U normal upper S normal i normal m Subscript upper
    A comma upper B Baseline equals StartFraction sigma-summation Underscript x element-of
    script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis
    r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline
    right-parenthesis left-parenthesis r Subscript upper B comma x Baseline minus
    r overbar Subscript upper B Baseline right-parenthesis Over StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar
    Subscript upper A Baseline right-parenthesis squared EndRoot StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper B comma x Baseline minus r overbar
    Subscript upper B Baseline right-parenthesis squared EndRoot EndFraction" display="block"><mrow><msub><mi>USim</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo>
    <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi></msub>
    <mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow></mrow> <mrow><msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub> <msup><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt>
    <msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow></math>
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper U normal upper S normal i normal m Subscript upper
    A comma upper B Baseline equals StartFraction sigma-summation Underscript x element-of
    script upper R Subscript upper A comma upper B Baseline Endscripts left-parenthesis
    r Subscript upper A comma x Baseline minus r overbar Subscript upper A Baseline
    right-parenthesis left-parenthesis r Subscript upper B comma x Baseline minus
    r overbar Subscript upper B Baseline right-parenthesis Over StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper A comma x Baseline minus r overbar
    Subscript upper A Baseline right-parenthesis squared EndRoot StartRoot sigma-summation
    Underscript x element-of script upper R Subscript upper A comma upper B Baseline
    Endscripts left-parenthesis r Subscript upper B comma x Baseline minus r overbar
    Subscript upper B Baseline right-parenthesis squared EndRoot EndFraction" display="block"><mrow><msub><mi>USim</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><msub><mo>∑</mo>
    <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>A</mi></msub>
    <mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow></mrow> <mrow><msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub> <msup><mrow><mo>(</mo><msub><mi>r</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>x</mi></mrow></msub> <mo>-</mo><msub><mover accent="true"><mi>r</mi>
    <mo>¯</mo></mover> <mi>A</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt>
    <msqrt><mrow><msub><mo>∑</mo> <mrow><mi>x</mi><mo>∈</mo><msub><mi>ℛ</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></mrow></msub>
    <msup><mrow><mo>(</mo><msub><mi>r</mi> <mrow><mi>B</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow></math>
- en: Here we remember that the similarity between two users is dependent on the shared
    ratings and on each user’s average rating.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们记得两个用户之间的相似性取决于共享的评分以及每个用户的平均评分。
- en: On a given day, let’s say <math alttext="upper X overTilde equals StartSet x
    overTilde bar x was rated since yesterday by a user EndSet"><mrow><mover accent="true"><mi>X</mi>
    <mo>˜</mo></mover> <mo>=</mo> <mfenced close="}" open="{" separators=""><mover
    accent="true"><mi>x</mi> <mo>˜</mo></mover> <mo>∣</mo> <mi>x</mi> <mtext>was</mtext>
    <mtext>rated</mtext> <mtext>since</mtext> <mtext>yesterday</mtext> <mtext>by</mtext>
    <mtext>a</mtext> <mtext>user</mtext></mfenced></mrow></math> . Then we’d need
    to update our user similarities, but ideally we’d leave everything else the same.
    To update the user’s data, we see that all <math alttext="x overTilde"><mover
    accent="true"><mi>x</mi> <mo>˜</mo></mover></math> rated by two users, <math alttext="upper
    A"><mi>A</mi></math> and <math alttext="upper B dollar-sign comma w i l l c o
    n t r i b u t e c h a n g e s period upper O n e a l s o m i g h t n o t i c e
    t h a t t h e l a t e x m a t h colon left-bracket dollar-sign r overbar Subscript
    upper A Baseline"><mi>B</mi></math> <math><msub><mover accent="true"><mi>r</mi>
    <mo>¯</mo></mover> <mi>A</mi></msub></math> and <math alttext="r overbar Subscript
    upper B"><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub></math>
    , would need to change, but we could probably skip these updates in many cases
    where the number of ratings by those users was large. All in all, this means for
    each <math alttext="x overTilde"><mover accent="true"><mi>x</mi> <mo>˜</mo></mover></math>
    , we should look up which users previously rated <math alttext="x"><mi>x</mi></math>
    and update the user similarity between them and the new rater.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在某一天，假设 <math alttext="upper X overTilde equals StartSet x overTilde bar x was
    rated since yesterday by a user EndSet"><mrow><mover accent="true"><mi>X</mi>
    <mo>˜</mo></mover> <mo>=</mo> <mfenced close="}" open="{" separators=""><mover
    accent="true"><mi>x</mi> <mo>˜</mo></mover> <mo>∣</mo> <mi>x</mi> <mtext>was</mtext>
    <mtext>rated</mtext> <mtext>since</mtext> <mtext>yesterday</mtext> <mtext>by</mtext>
    <mtext>a</mtext> <mtext>user</mtext></mfenced></mrow></math> 。那么我们需要更新我们的用户相似性，但理想情况下，我们会保持其他一切不变。为了更新用户的数据，我们看到所有被两个用户
    <math alttext="x overTilde"><mover accent="true"><mi>x</mi> <mo>˜</mo></mover></math>
    评价的情况，<math alttext="upper A"><mi>A</mi></math> 和 <math alttext="upper B dollar-sign
    comma w i l l c o n t r i b u t e c h a n g e s period upper O n e a l s o m i
    g h t n o t i c e t h a t t h e l a t e x m a t h colon left-bracket dollar-sign
    r overbar Subscript upper A Baseline"><mi>B</mi></math> <math><msub><mover accent="true"><mi>r</mi>
    <mo>¯</mo></mover> <mi>A</mi></msub></math> 和 <math alttext="r overbar Subscript
    upper B"><msub><mover accent="true"><mi>r</mi> <mo>¯</mo></mover> <mi>B</mi></msub></math>
    ，需要进行更改，但在许多情况下，我们可能可以跳过这些更新，其中这些用户的评级数量较多。总之，这意味着对于每个 <math alttext="x overTilde"><mover
    accent="true"><mi>x</mi> <mo>˜</mo></mover></math> ，我们应该查找之前评价过 <math alttext="x"><mi>x</mi></math>
    的用户，并更新他们之间以及新评分者之间的用户相似性。
- en: This is a bit ad hoc, but for many methods you can utilize these tricks to reduce
    a full retraining. This would avoid a full batch retraining, via a fast layer.
    Other approaches exist, like building a separate model that can approximate recommendations
    for low-signal items. This can be done via feature models and can significantly
    reduce the complexity of these quick retrainings.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点临时抱佛脚，但对于许多方法，您可以利用这些技巧来减少完全的重新训练。这将通过一个快速层避免完整的批量重新训练。还有其他方法，例如构建一个能够近似低信号项目推荐的单独模型。这可以通过特征模型实现，并且可以显著减少这些快速重新训练的复杂性。
- en: Lambda Architecture and Orchestration
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lambda 架构与编排
- en: On the more extreme end of the spectrum of these strategies is the lambda architecture;
    as discussed in [Chapter 6](ch06.html#data-processing), the lambda architecture
    seeks to have a much more frequent pipeline for adding new data into the system.
    The *speed* layer is responsible for working on small batches to perform the data
    transformations, and on model fitting to combine with the core model. As a reminder,
    many other aspects of the pipeline should also be updated during these fast layers,
    like the nearest neighbors graph, the feature store, and the filters.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些策略的极端端点上是 Lambda 架构；正如在[第 6 章](ch06.html#data-processing)中讨论的那样，Lambda 架构旨在对系统中的新数据具有更频繁的流水线。*速度*
    层负责处理小批量进行数据转换，并在模型拟合时与核心模型结合。作为提醒，流水线的许多其他方面也应在这些快速层次中更新，如最近邻图、特征存储和过滤器。
- en: Different components of the pipeline can require different investments to keep
    updated, so their schedules are an important consideration. You might be starting
    to notice that keeping all of these aspects in sync can be a bit challenging.
    If you have model training, model updating, feature store updates, redeployment,
    and new items/users all coming in on potentially different schedules, a *lot*
    of coordination may be necessary. This is where an *orchestration tool* can become
    relevant. A variety of approaches exist, but a few useful technologies here are
    GoCD, MetaFlow, and KubeFlow; the latter is more oriented at Kubernetes infrastructures.
    Another pipeline orchestration tool that can handle both batch and streaming pipelines
    is Apache Beam.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线的不同组件可能需要不同的投资来保持更新，因此它们的时间表是一个重要的考虑因素。您可能已经开始注意到，保持所有这些方面的同步可能有些挑战。如果您的模型训练、模型更新、特征存储更新、重新部署以及新项目/用户可能在潜在不同的时间表上进行，则可能需要大量的协调工作。这就是*编排工具*变得相关的地方。存在多种方法，但在这里一些有用的技术包括
    GoCD、MetaFlow 和 KubeFlow；后者更多地面向 Kubernetes 基础设施。另一个可以处理批处理和流处理管道的管道编排工具是 Apache
    Beam。
- en: Generally, for ML deployment pipelines, we need to have a reliable core pipeline
    and the ability to keep the systems up to date as more data pours in. Orchestration
    systems usually define the topology of the systems, the relevant infrastructure
    configurations, and the mapping of the code artifacts needing to be run—not to
    mention the CRON schedules of when all these jobs need to run. Code as infrastructure
    is a popular paradigm that captures these goals as a mantra, so that even all
    this configuration itself is reproducible and automatable.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，对于 ML 部署管道，我们需要一个可靠的核心管道以及随着更多数据涌入的能力来保持系统更新。编排系统通常定义系统的拓扑结构、相关的基础设施配置以及需要运行的代码工件的映射——更不用说所有这些作业需要运行的
    CRON 时间表了。代码即基础设施是一个流行的范式，捕捉这些目标作为口头禅，以便甚至这些配置本身也是可重现和可自动化的。
- en: In all these orchestration considerations, there’s a heavy overlap with containerization
    and *how* these steps may be deployed. Unfortunately, most of this discussion
    is beyond the scope of this book, but a simple overview is that containerized
    deployment with something like Docker is extremely helpful for ML services, and
    managing those deployments with various container management systems, like Kubernetes,
    is also popular.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些编排考虑中，与容器化和*如何*部署这些步骤有很大的重叠。不幸的是，大部分这些讨论超出了本书的范围，但简单概述是，像 Docker 这样的容器化部署对
    ML 服务非常有帮助，并且使用各种容器管理系统（如 Kubernetes）来管理这些部署也很流行。
- en: Logging
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志记录
- en: Logging has come up several times already. Previously in this chapter, you saw
    that logging was important for ensuring that our system was behaving as expected.
    Let’s discuss some best practices for logging and how they fit into our plans.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录已经多次提及。在本章的前面，您看到了日志记录对确保我们的系统表现如预期的重要性。让我们讨论一些日志记录的最佳实践以及它们如何融入我们的计划中。
- en: 'When we discussed traces and spans earlier, we were able to get a snapshot
    of the entire call stack of the services involved in responding to a request.
    Linking the services together to see the larger picture is incredibly useful,
    and when it comes to logging, gives us a hint as to how we should be orienting
    our thinking. Returning to our favorite RecSys architecture, we have the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前讨论过轨迹和跨度时，我们能够对响应请求的服务调用堆栈的整体进行快照。将这些服务连接起来以看到更大的画面非常有用，特别是在日志记录方面，这给了我们一个关于如何定位我们的思维的提示。回到我们喜爱的推荐系统架构，我们有以下内容：
- en: Collector receiving the request and looking up the embedding relevant to the
    user
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集器接收请求并查找与用户相关的嵌入
- en: Computing ANN on items for that vector
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对该向量上的项目计算 ANN
- en: Applying filters via blooms to eliminate potential bad recommendations
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过布隆过滤器应用筛选以消除潜在的不良推荐
- en: Augmenting features of the candidate items and user via the feature stores
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征存储增强候选项和用户的特征
- en: Scoring of candidates via the ranking model and estimating potential confidence
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过排名模型对候选人进行评分并估算潜在置信度
- en: Ordering and application of business logic or experimentation
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务逻辑或实验的排序和应用
- en: Each of these elements has potential applications of logging, but let’s now
    think about how to link them together. The relevant concept from microservices
    is correlation IDs; a *correlation ID* is simply an identifier that’s passed along
    the call stack to ensure the ability to link everything later. As is likely obvious
    at this point, each of these services will be responsible for its own logging,
    but the services are almost always more useful in aggregate.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元素都有可能应用日志记录，但现在让我们考虑如何将它们链接在一起。来自微服务的相关ID概念；*关联ID*只是一个通过调用堆栈传递以确保稍后链接一切的标识符。显而易见的是，每个服务都将负责自己的日志记录，但这些服务在聚合时几乎总是更有用。
- en: These days, Kafka is often used as the log-stream processor to listen for logs
    from all the services in your pipeline and to manage their processing and storing.
    Kafka relies on a message-based architecture; each service is a producer, and
    Kafka helps manage those messages to consumer channels. In terms of log management,
    the Kafka cluster receives all the logs in the relevant formats, hopefully augmented
    with correlation IDs, and sends them off to an ELK stack. The *ELK stack*—Elasticsearch,
    Logstash, Kibana—consists of a Logstash component to handle incoming log streams
    and apply structured processing, Elasticsearch to build search indices to the
    log store, and Kibana to add a UI and high-level dashboarding to the logging.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这些天，Kafka经常被用作日志流处理器，用来监听管道中所有服务的日志，并管理它们的处理和存储。Kafka依赖于基于消息的架构；每个服务都是生产者，而Kafka则帮助管理这些消息到消费者通道。在日志管理方面，Kafka集群接收所有相关格式的日志，希望能够附加相关的关联ID，并将它们发送到ELK堆栈。*ELK堆栈*
    — Elasticsearch、Logstash、Kibana — 包括Logstash组件来处理传入的日志流并应用结构化处理，Elasticsearch用于构建日志存储的搜索索引，Kibana添加了UI和高级仪表板以进行日志记录。
- en: This stack of technologies is focused on ensuring that you have access and observability
    from your logs. Other technologies focus on other aspects, but what should you
    be logging?
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术堆栈专注于确保您能够从日志中访问和观测。其他技术专注于其他方面，但您应该记录什么呢？
- en: Collector logs
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集器日志
- en: 'Again, we wish to log during the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们希望在以下情况下记录：
- en: Collector receiving the request and looking up the embedding relevant to the
    user
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集器接收请求并查找与用户相关的嵌入
- en: Computing ANN on items for that vector
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算该向量上的ANN项目
- en: The collector receives a request, consisting in our simplest example of `user_id`,
    `requesting_timestamp`, and any augmenting keyword elements (kwargs) that might
    be required. A `correlation_id` should be passed along from the requester or generated
    at this step. A log with these basic keys should be fired, along with the timestamp
    of request received. A call is made to the embedding store, and the collector
    should log this request. Then the embedding store should log this request when
    received, along with the embedding store’s response. Finally, the collector should
    log the response as it returns. This may feel like a lot of redundant information,
    but the explicit parameters included in the API calls become extremely useful
    when troubleshooting.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 收集器接收到请求，最简单的示例包括`user_id`、`requesting_timestamp`和可能需要的增强关键字元素（kwargs）。一个`correlation_id`应该从请求者传递或在此步骤生成。应该发送带有这些基本键的日志，以及接收到请求的时间戳。调用嵌入式存储，并且收集器应记录此请求。然后，当接收到请求时，嵌入式存储应记录此请求，以及嵌入式存储的响应。最后，当收集器返回响应时，应记录响应。这可能感觉像是大量冗余信息，但在故障排除时，API调用中包含的显式参数变得极其有用。
- en: The collector now has the vector it will need to perform a vector search, so
    it will make a call to the ANN service. Logging this call, and any relevant logic
    in choosing the <math alttext="k"><mi>k</mi></math> for number of neighbors will
    be important, along with the ANN’s received API request, the relevant state for
    computing ANN, and ANN’s response. Back in the collector, logging that response
    and any potential data augmentation for downstream service requirements are the
    next steps.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在收集器已经具备执行向量搜索所需的向量，因此它将调用ANN服务。记录此调用，以及选择邻居数`<math alttext="k"><mi>k</mi></math>`的相关逻辑，以及ANN收到的API请求，用于计算ANN的相关状态和ANN的响应。在收集器中，记录该响应以及用于下游服务要求的任何潜在数据增强是下一步。
- en: At this point, at least six logs have been emitted—only reinforcing the need
    for a way to link these all together. In practice, you often have other relevant
    steps in your service that should be logged (e.g., checking that the distribution
    of distances in returned neighbors is appropriate for downstream ranking).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，至少已经发出了六个日志——这进一步强调了需要一种方式将它们全部链接在一起。实际上，您的服务中通常还有其他相关步骤应该记录（例如，检查返回邻居中距离分布是否适合下游排名）。
- en: Note that if the embedding lookup was a miss, logging that miss is obviously
    important, as well as logging the subsequent request to the cold-start recommendation
    pipeline. The cold-start pipeline will incur additional logs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果嵌入查找失败，显然记录该失败非常重要，以及记录后续请求到冷启动推荐流程的情况。冷启动流程会产生额外的日志。
- en: Filtering and scoring
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤和评分
- en: 'Now we need to monitor the following steps:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要监视以下步骤：
- en: Applying filters via blooms to eliminate potential bad recommendations
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过布隆过滤器应用筛选，以消除潜在的不良推荐
- en: Augmenting features to the candidate items and user via the feature stores
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过特征存储对候选项和用户进行特征增强
- en: Scoring candidates via the ranking model, and potential confidence estimation
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过排名模型对候选人进行评分，以及可能的置信度估计
- en: We should log the incoming request to the filtering service as well as the collection
    of filters we wish to apply. Additionally, as we search the blooms for each item
    and rule them in or out of the bloom, we should build up some structured logging
    of which items are caught in which filters and then log all this as a blob for
    later inspection. Responses and requests should be logged as part of feature augmentation—where
    we should log requests and responses to the feature store.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该记录进入过滤服务的传入请求，以及我们希望应用的所有过滤器的收集。此外，当我们为每个项目搜索布隆过滤器并根据布隆过滤器将其筛选进入或退出时，我们应该建立一些结构化的日志记录，记录哪些项目被哪些过滤器捕获，然后将所有这些记录为后续检查的
    blob。响应和请求应作为特征增强的一部分进行记录——我们应该记录特征存储的请求和响应。
- en: Also log the augmented features that end up attached to the item entities. This
    may seem redundant with the feature store itself, but understanding which features
    were added during a recommendation pipeline is *crucial* when looking back later
    to figure out why the pipeline might have behaved differently than anticipated.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 也要记录增强特征，这些特征最终附加到项目实体上。这可能与特征存储本身重复，但是在稍后回顾时理解推荐流水线中添加了哪些特征是*至关重要*的。
- en: At the time of scoring, the entire set of candidates should be logged with the
    features necessary for scoring and the output scores. It’s extremely powerful
    to log this entire dataset, because training later can use these to get a better
    sense for real ranking sets. Finally, the response is passed to the next step
    with the ranked candidates and all their features.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在评分时，应记录整个候选集及其评分所需的特征和输出分数。记录整个数据集非常有用，因为后续训练可以利用这些数据更好地理解真实排名集。最后，将带有排名候选人及其所有特征的响应传递到下一步。
- en: Ordering
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排序
- en: 'We have one more step to go, but it’s an essential one: *ordering and application
    of business logic or experimentation*. This step is probably the most important
    logging step, because of how complicated and ad hoc the logic in this step can
    get.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有最后一步要走，但这是一个至关重要的步骤：*业务逻辑或实验的排序和应用*。这一步可能是最重要的日志记录步骤，因为这一步骤中的逻辑可以变得非常复杂和特定。
- en: If you have multiple intersecting business requirements implemented via filters
    at this step, while also integrating with experimentation, you can find yourself
    seriously struggling to unpack how reasonable expectations coming out of the ranker
    have turned into a mess by response time. Techniques like logging the incoming
    candidates, keyed to why they’re eliminated, and the order of business rules applied
    will make reconstructing the behavior much more tractable.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在此步骤中通过多个交叉业务需求实施过滤器，同时还与实验集成，您可能会发现自己严重努力解析由于响应时间而导致的合理预期如何变成混乱的情况。像记录传入候选项，以及它们被淘汰的原因和应用的业务规则顺序这样的技术，将使重构行为变得更加可操作。
- en: Additionally, experimentation routing will likely be handled by another service,
    but the experiment ID seen in this step and the way that experiment assignment
    was utilized are the responsibility of the server. As we ship off the final recommendations,
    or decide to go another round, one last log of the state of the recommendation
    will ensure that app logs can be validated with responses.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，实验路由可能会由另一个服务处理，但是在这一步骤中看到的实验ID以及实验分配方式的使用责任属于服务器。在我们完成最终推荐或决定再次进行时，推荐状态的最后一次日志将确保应用程序日志可以通过响应进行验证。
- en: Active Learning
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主动学习
- en: So far, we have discussed using updating data to train on a much more frequent
    schedule, and we’ve discussed how to provide good recommendations, even when the
    model hasn’t seen enough data for those entities. An additional opportunity for
    the feedback loop of recommendation and rating is active learning.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论过使用更新的数据以更频繁的时间表进行训练，并且我们已经讨论过如何在模型尚未看到足够数据的情况下提供良好的推荐。推荐和评级的反馈循环的另一个机会是主动学习。
- en: We won’t be able to go deep into the topic, which is a large and active field
    of research, but we will discuss the core ideas in relation to recommendation
    systems. *Active learning* changes the learning paradigm a bit by suggesting that
    the learner should not only be passively collecting labeled (maybe implicit) observations
    but also attempting to mine relations and preferences from them. Active learning
    determines which data and observations would be most useful in improving model
    performance and then seeks out those labels. In the context of RecSys, we know
    that the Matthew effect is one of our biggest challenges, in that many potentially
    good matches for a user may be lacking enough or appropriate ratings to bubble
    to the top during the recommendations.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法深入讨论这个大而活跃的研究领域，但我们将讨论与推荐系统相关的核心思想。*主动学习* 通过建议学习者不仅仅是被动地收集标记（可能是隐式的）观察结果，还试图从中挖掘关系和偏好，从而改变了学习范式。主动学习确定哪些数据和观察结果在提高模型性能方面最有用，然后寻找这些标签。在推荐系统的背景下，我们知道马太效应是我们面临的最大挑战之一，因为对用户可能有很好匹配的许多项目，缺乏足够或适当的评级，无法在推荐过程中排在前列。
- en: 'What if we employed a simple policy: every new item to the store gets recommended
    as a second option to the first 100 customers. Two outcomes would result:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们采用一个简单的策略：每个新项目进入商店时，都推荐给前100位顾客作为第二选择。这将导致两个结果：
- en: We would quickly establish data for our new item to help cold-start it.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们会迅速为我们的新项目建立数据，以帮助冷启动它。
- en: We would likely decrease the performance of our recommender.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能会降低我们推荐系统的性能。
- en: In many cases, the second outcome is worth enduring to achieve the first, but
    when? And is this the right way to approach this problem? Active learning provides
    a methodical approach to these problems.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，第二个结果是值得忍受以实现第一个结果的，但是何时呢？这样做是否是解决这个问题的正确方法？主动学习为解决这些问题提供了一种系统的方法。
- en: 'Another more specific advantage of active learning schemes is that you can
    broaden the distribution of observed data. In addition, to just cold-start items,
    we can use active learning to target broadening users’ interests. This is usually
    framed as an uncertainty-reduction technique, as it can be used to improve the
    confidence in recommendations in a broader range of item categories. Here’s a
    simple example: a user shops for only sci-fi books, so one day you show them a
    few extremely well-liked Westerns to see whether that user might be open to occasionally
    getting recommendations for Westerns. See [“Propensity Weighting for Recommendation
    System Evaluation”](ch10.html#propensity) for more details.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习方案的另一个更具体的优势是，可以扩展观察数据的分布。除了仅冷启动项目外，我们还可以使用主动学习来扩展用户的兴趣。这通常被描述为一种减少不确定性的技术，因为它可以用来提高更广泛类别的项目推荐的信心。这里有一个简单的例子：一个用户只购买科幻书籍，因此有一天你向他们展示了一些非常受欢迎的西部片，以查看该用户是否可能偶尔接受西部片的推荐。更多详情请参阅[“推荐系统评估的倾向性加权”](ch10.html#propensity)。
- en: 'An active learning system is instrumented as a loss function inherited from
    the model it’s trying to enhance—usually tied to uncertainty in some capacity—and
    it’s attempting to minimize that loss. Given a model <math alttext="script upper
    M"><mi>ℳ</mi></math> trained on a set of observations and labels <math alttext="StartSet
    x Subscript i Baseline comma y Subscript i Baseline EndSet"><mfenced close="}"
    open="{" separators=""><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi>
    <mi>i</mi></msub></mfenced></math> , with loss <math alttext="script upper L"><mi>ℒ</mi></math>
    , an active learner seeks to find a new observation, <math alttext="x overbar"><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover></math> , such that if a label was
    obtained, <math alttext="y overbar"><mover accent="true"><mi>y</mi> <mo>¯</mo></mover></math>
    , the loss would decrease via the model’s training including this new pair. In
    particular, the goal is to approximate the marginal reduction in loss due to each
    possible new observation and find the observation that maximizes that reduction
    in the loss function:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主动学习系统作为从试图增强的模型继承的损失函数——通常与某种形式的不确定性相关联——并试图最小化该损失。给定一个训练在一组观察和标签 <math alttext="StartSet
    x Subscript i Baseline comma y Subscript i Baseline EndSet"><mfenced close="}"
    open="{" separators=""><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi>
    <mi>i</mi></msub></mfenced></math> 上的模型 <math alttext="script upper M"><mi>ℳ</mi></math>
    ，与损失 <math alttext="script upper L"><mi>ℒ</mi></math> ，一个主动学习者寻求找到一个新的观察， <math
    alttext="x overbar"><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></math>
    ，如果获得一个标签 <math alttext="y overbar"><mover accent="true"><mi>y</mi> <mo>¯</mo></mover></math>
    ，则通过模型的训练，包括这对新的观察，损失将减少。特别地，目标是近似由于每个可能的新观察而导致的损失的边际减少，并找到最大化损失函数减少的观察。
- en: <math alttext="Argmax Subscript x overbar Baseline left-parenthesis script upper
    L left-parenthesis script upper M Subscript StartSet x Sub Subscript i Subscript
    comma y Sub Subscript i Subscript EndSet Baseline right-parenthesis minus script
    upper L left-parenthesis script upper M Subscript StartSet x Sub Subscript i Subscript
    comma y Sub Subscript i Subscript EndSet union StartSet x overbar EndSet Baseline
    right-parenthesis right-parenthesis" display="block"><mrow><msub><mi>Argmax</mi>
    <mover accent="true"><mi>x</mi> <mo>¯</mo></mover></msub> <mfenced close=")" open="("
    separators=""><mi>ℒ</mi> <mfenced close=")" open="(" separators=""><msub><mi>ℳ</mi>
    <mfenced close="}" open="{" separators=""><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mfenced></msub></mfenced> <mo>-</mo> <mi>ℒ</mi> <mfenced close=")"
    open="(" separators=""><msub><mi>ℳ</mi> <mrow><mfenced close="}" open="{" separators=""><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>y</mi> <mi>i</mi></msub></mfenced> <mo>∪</mo><mfenced
    close="}" open="{" separators=""><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></mfenced></mrow></msub></mfenced></mfenced></mrow></math>
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Argmax Subscript x overbar Baseline left-parenthesis script upper
    L left-parenthesis script upper M Subscript StartSet x Sub Subscript i Subscript
    comma y Sub Subscript i Subscript EndSet Baseline right-parenthesis minus script
    upper L left-parenthesis script upper M Subscript StartSet x Sub Subscript i Subscript
    comma y Sub Subscript i Subscript EndSet union StartSet x overbar EndSet Baseline
    right-parenthesis right-parenthesis" display="block"><mrow><msub><mi>Argmax</mi>
    <mover accent="true"><mi>x</mi> <mo>¯</mo></mover></msub> <mfenced close=")" open="("
    separators=""><mi>ℒ</mi> <mfenced close=")" open="(" separators=""><msub><mi>ℳ</mi>
    <mfenced close="}" open="{" separators=""><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mfenced></msub></mfenced> <mo>-</mo> <mi>ℒ</mi> <mfenced close=")"
    open="(" separators=""><msub><mi>ℳ</mi> <mrow><mfenced close="}" open="{" separators=""><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>y</mi> <mi>i</mi></msub></mfenced> <mo>∪</mo><mfenced
    close="}" open="{" separators=""><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></mfenced></mrow></msub></mfenced></mfenced></mrow></math>
- en: 'The structure of an active learning system roughly follows these steps:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主动学习系统的结构大致遵循以下步骤：
- en: Estimate marginal decrease in loss due to obtaining one of a set of observations.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计由于获得一组观察而导致的损失边际减少。
- en: Select the observation with the largest effect.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择具有最大影响的观察。
- en: '*Query* the user; i.e., provide the recommendation to obtain a label.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*询问*用户；即提供推荐以获取标签。'
- en: Update the model.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新模型。
- en: It’s probably clear that this paradigm requires a much faster training loop
    than our previous fast retraining schemes. Active learning can be instrumented
    in the same infrastructure as our other setups, or it can have its own mechanisms
    for integration into the pipeline.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 很显然，这种范式要求比我们先前的快速重新训练方案更快速的训练循环。主动学习可以被仪器化为与我们其他设置相同的基础设施，或者它可以有其自己的机制集成到流水线中。
- en: Types of optimization
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化类型
- en: 'The optimization procedure carried out by an active learner in a recommendation
    system has two approaches: personalized and nonpersonalized. Because RecSys is
    all about personalization, it’s no surprise that we would, in time, want to push
    the utility of our active learning further by integrating the great details we
    already know about users.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习系统中由主动学习者执行的优化过程有两种方法：个性化和非个性化。由于推荐系统（RecSys）专注于个性化，我们很自然地希望通过整合我们已经了解的关于用户的详细信息，进一步推动我们的主动学习的效用。
- en: We can think of these two approaches as global loss minimization and local loss
    minimization. Active learning that isn’t personalized tends to be about minimizing
    the loss over the entire system, not for only one user. (This split doesn’t perfectly
    capture the ontology, but it’s a useful mnemonic). In practice, optimization methods
    are nuanced and sometimes utilize complicated algorithms and training procedures.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这两种方法视为全局损失最小化和局部损失最小化。非个性化的主动学习往往是关于在整个系统中最小化损失，而不仅仅是一个用户的损失。在实践中，优化方法是微妙的，有时使用复杂的算法和训练过程。
- en: 'Let’s talk through some factors to optimize for nonpersonalized active learning:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一些优化非个性化主动学习的因素：
- en: '*User rating variance*'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*用户评分方差*'
- en: Consider which items have the largest variance in user ratings to try to get
    more data on those we find the most complicated in our observations.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑哪些项目在用户评分中具有最大的方差，以尝试获取更多关于我们在观察中发现最复杂的项目的数据。
- en: '*Entropy*'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*熵*'
- en: Consider the dispersion of ratings of a particular item across an ordinal feature.
    This is useful for understanding whether our set of ratings for an item is distributed
    uniformly at random.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑特定项目在序数特征上评分的离散度。这对于了解一个项目的评分集是否是均匀随机分布很有用。
- en: '*Greedy extend*'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*贪婪扩展*'
- en: Measure which items seem to yield the worst performance in our current model;
    this attempts to improve our performance overall by collecting more data on the
    hardest items to recommend well.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 测量似乎在当前模型中表现最差的项目；这试图通过收集更多关于最难推荐的项目的数据来提高我们的整体表现。
- en: '*Representatives or exemplars*'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '*代表性或典型案例*'
- en: Pick out items that are extremely representative of large groups of items; we
    can think of this as “If we have good labels for this, we have good labels for
    everything like this.”
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 挑选出极具代表性的大量项目中的项目；我们可以将其视为“如果我们对这个有好的标签，那么对所有类似项目也有好的标签。”
- en: '*Popularity*'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*流行度*'
- en: Select items that the user is most likely to have experience with to maximize
    the likelihood that they’ll give an opinion or rating.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 选择用户最有可能有过经验的项目，以最大化他们提供意见或评分的可能性。
- en: '*Co-coverage*'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*协同覆盖*'
- en: Attempt to amplify the ratings for frequently occurring pairs in the dataset;
    this strikes directly at the CF structure to maximize the utility of observations.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试放大数据集中频繁出现的对的评分；这直接针对CF结构，以最大化观察的效用。
- en: 'On the personalized side:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在个性化方面：
- en: '*Binary prediction*'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '*二元预测*'
- en: To maximize the chances that the user can provide the requested rating, choose
    the items that the user is more likely to have experienced. This can be achieved
    via an MF on the binary ratings matrix.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化用户能够提供所请求评分的机会，选择用户更有可能体验过的项目。这可以通过二元评分矩阵上的MF实现。
- en: '*Influence based*'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于影响力*'
- en: Estimate the influence of item ratings on the rating prediction of other items,
    and select the items with the largest influence. This attempts to directly measure
    the impact of a new item rating on the system.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 评估项目评分对其他项目评分预测的影响，并选择具有最大影响力的项目。这试图直接衡量新项目评分对系统的影响。
- en: '*Rating optimized*'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '*评分优化*'
- en: Obviously, there’s an opportunity to simply use the best rating or best rating
    within a class to perform active learning queries, but this is precisely the standard
    strategy in recommendation systems to serve good recommendations.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，可以使用最佳评分或类内最佳评分执行主动学习查询，但这恰好是推荐系统中服务良好推荐的标准策略。
- en: '*User segmented*'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*用户分割*'
- en: When available, use user segmentation and feature clusters within users to anticipate
    when users have opinions and preferences on an item by virtue of the user-similarity
    structure.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，利用用户分割和用户内特征簇，预测用户是否会对项目有意见和偏好，基于用户相似性结构。
- en: In general, a soft trade-off exists between active learning that’s useful for
    maximally improving your model globally and active learning that’s useful for
    maximizing the likelihood that a user can and will rate a particular item. Let’s
    look at one particular example that uses both.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，存在一种软性权衡，即对于全局最大化模型改进有用的主动学习和对于最大化用户能够和愿意对特定项目评分有用的主动学习。让我们看一个同时使用两者的特定示例。
- en: 'Application: User sign-up'
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用：用户注册
- en: One common hurdle to overcome in building recommendation systems is on-boarding
    new users. By definition, new users will be cold-starting with no ratings of any
    kind and will likely not expect great recommendations from the start.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建推荐系统时，一个常见的障碍是引导新用户加入。根据定义，新用户将从零开始，没有任何评分，可能不会从一开始就期待到优秀的推荐。
- en: We may begin with the MPIR for all new users—simply show them *something* to
    get them started and then learn as you go. But is there something better?
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有新用户，我们可以从MPIR开始——简单地向他们展示*某些东西*，然后边学边用。但是否有更好的方法呢？
- en: 'One approach you’ve probably experienced is the user onboarding flow: a simple
    set of questions employed by many websites to quickly ascertain basic information
    about the user, to help guide early recommendation. If discussing our book recommender,
    this might be asking what genres the user likes, or in the case of a coffee recommender,
    how the user brews coffee in the morning. It’s probably clear that these questions
    are building up knowledge-based recommender systems and don’t directly feed into
    our previous pipelines but can still provide some help in early recommendations.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能经历过的一种方法是用户引导流程：许多网站采用的一组简单问题，快速获取用户的基本信息，以帮助早期推荐。如果讨论我们的图书推荐器，这可能是询问用户喜欢哪些类型的书籍，或者在咖啡推荐器的情况下，询问用户早上如何冲泡咖啡。显然，这些问题正在建立基于知识的推荐系统，并不直接与我们之前的流水线相结合，但仍然可以在早期推荐中提供一些帮助。
- en: If instead we looked at all our previous data and asked, “Which books in particular
    are most useful for determining a user’s taste?,” this would be an active learning
    approach. We could even have a decision tree of possibilities as the user answered
    each question, wherein the answer determines which next question is most useful
    to ask.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们反过来看所有以前的数据，并问：“特别是哪些书对确定用户口味最有用？”，这将是一种主动学习方法。当用户回答每个问题时，我们甚至可以有一个可能性的决策树，其中答案决定了下一个最有用的问题是什么。
- en: Summary
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Now we have the confidence that we can serve up our recommendations, and even
    better, we have instrumented our system to gather feedback. We’ve shown how you
    can gain confidence before you deploy and how you can experiment with new models
    or solutions. Ensembles and cascades allow you to combine testing with iteration,
    and the data flywheel provides a powerful mechanism for improving your product.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有信心能够提供我们的推荐，更好的是，我们已经为我们的系统装备了收集反馈的工具。我们已经展示了在部署之前如何获得信心，以及如何尝试新的模型或解决方案。集成和级联使您可以将测试与迭代相结合，而数据飞轮为改进您的产品提供了强大的机制。
- en: You may be wondering how to put all this new knowledge into practice, to which
    the next chapter will speak. Let’s understand how data processing and simple counting
    can lead to an effective—and useful!—recommendation system.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道如何将所有这些新知识付诸实践，下一章将讨论这个问题。让我们了解数据处理和简单计数如何可以导致一个有效的——而且有用的！——推荐系统。
