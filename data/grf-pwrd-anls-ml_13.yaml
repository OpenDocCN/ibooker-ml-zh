- en: Chapter 10\. Graph-Powered Machine Learning Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章\. 基于图的机器学习方法
- en: 'After completing this chapter, you should be able to:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，你将能够：
- en: List three basic ways that graph data and analytics can improve machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出图数据和分析如何改进机器学习的三种基本方式
- en: Point out which graph algorithms have proved valuable for unsupervised learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指出哪些图算法在无监督学习中已被证明是有价值的
- en: Extract graph features to enrich your training data for supervised machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取图特征以丰富你的训练数据，用于监督机器学习。
- en: Describe how neural networks have been extended to learn on graphs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述神经网络如何扩展到图上进行学习
- en: Provide use cases and examples to illustrate graph-powered machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供使用案例和示例，以说明基于图的机器学习。
- en: Choose which types of graph-powered machine learning are right for you
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择适合你的基于图的机器学习类型
- en: 'We now begin the third theme of our book: Learn. That is, we’re going to get
    serious about the core of machine learning: model training. [Figure 10-1](#machine_learning_pipeline)
    shows the stages of a simple machine learning pipeline. In Part 1 of this book,
    we explored the Connect theme, which fits the first two stages of the pipeline:
    data acquisition and data preparation. Graph databases make it easy to pull data
    from multiple sources into one connected database and to perform entity resolution.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始书中的第三个主题：学习。也就是说，我们要认真对待机器学习的核心：模型训练。[图10-1](#machine_learning_pipeline)展示了一个简单的机器学习流程的各个阶段。在本书的第一部分中，我们探讨了连接主题，这符合流程的前两个阶段：数据获取和数据准备。图数据库使得从多个来源提取数据到一个连接的数据库中，并执行实体解析变得更加容易。
- en: '![Machine learning pipeline](assets/gpam_1001.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习流程](assets/gpam_1001.png)'
- en: Figure 10-1\. Machine learning pipeline
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1\. 机器学习流程
- en: 'In this chapter, we’ll show how graphs enhance the central stages in the pipeline:
    feature extraction and all-important model training. Features are simply the characteristics
    or properties of your data entities, like the age of a person or the color of
    a sweater. Graphs offer a whole new realm of features that are based on how an
    entity connects to other entities. The addition of these unique graph-oriented
    features provides machine learning with better raw materials with which to build
    its models.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将展示图如何增强流程的核心阶段：特征提取和至关重要的模型训练。特征简单地说就是数据实体的特性或属性，比如一个人的年龄或一件衣服的颜色。图提供了一整套基于实体如何与其他实体连接的特征，这些独特的面向图的特征增强了机器学习的原始材料，使其能够建立更好的模型。
- en: 'This chapter has four sections. The first three sections each describe a different
    way that graphs enhance machine learning. First, we will start with unsupervised
    learning with graph algorithms, as it is similar to the techniques we discussed
    in Part 2: Analyze. Second, we will turn to graph feature extraction for supervised
    and unsupervised machine learning. Third, we culminate with model training directly
    on graphs, for both supervised and unsupervised approaches. This includes techniques
    for clustering, embedding, and neural networks. The fourth section reviews the
    various methods in order to compare them and to help you decide which approaches
    will meet your needs.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括四个部分。前三个部分分别描述了图如何增强机器学习的不同方式。首先，我们将从使用图算法进行无监督学习开始，因为这与我们在第二部分讨论的技术类似。其次，我们将转向用于监督和无监督机器学习的图特征提取。第三，我们以直接在图上进行模型训练结束，包括聚类、嵌入和神经网络的技术。第四部分回顾了各种方法，以便比较它们，并帮助您决定哪些方法能够满足您的需求。
- en: Unsupervised Learning with Graph Algorithms
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用图算法进行无监督学习
- en: Unsupervised learning is the sibling of supervised learning and reinforcement
    learning, who together form the three major branches of machine learning. If you
    want your AI system to learn how to do a task, to classify things according to
    your categories, or to make predictions, you want to use supervised learning and/or
    reinforcement learning. Unsupervised learning, however, has the great advantage
    of being self-sufficient and ready to go. Unlike supervised learning, you don’t
    need to already know the right answer for some cases. Unlike reinforcement learning,
    you don’t have to be patient and forgiving as you stumble through learning by
    trial and error. Unsupervised learning simply takes the data you have and reports
    what it learned.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是监督学习和强化学习的姊妹，它们共同构成机器学习的三大分支。如果你希望你的AI系统学会如何执行任务，按照你的类别分类事物，或者进行预测，你会选择使用监督学习和/或强化学习。然而，无监督学习有一个巨大的优势，即自给自足且准备就绪。不像监督学习，有些情况下你不需要已经知道正确答案。不像强化学习，你不必在通过试错学习时耐心和宽容。无监督学习只是获取你拥有的数据并报告其所学到的内容。
- en: An unsupervised learning algorithm can look at your network of customers and
    sales and identify your actual market segments, which may not fit simple ideas
    of age and income. An unsupervised learning algorithm can point out customer behavior
    that is an outlier, or far from normal, by determining “normal” from your data
    and not from your preconceptions. For example, an outlier can point out which
    customers are likely to churn (that is, discontinue using your product or service).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个无监督学习算法可以查看你的客户和销售网络，并识别你实际的市场细分，这可能不符合年龄和收入的简单观念。无监督学习算法可以通过从你的数据而不是你的先入之见中确定“正常”来指出异常或远离正常的客户行为。例如，异常值可以指出哪些客户可能会流失（即停止使用你的产品或服务）。
- en: 'The first way we will learn from graph data is by applying graph algorithms
    to discover patterns or characteristics of our data. In [Chapter 6](ch06.html#analyzing_connections_for_deeper_insigh),
    we gave a detailed overview of five categories of algorithms. In this section,
    we’ll discuss which of these algorithms are a good fit for unsupervised learning.
    We’ll also introduce another graph analytics task: frequent pattern mining.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从图数据中学习的第一种方式是应用图算法来发现数据的模式或特征。在[第6章](ch06.html#analyzing_connections_for_deeper_insigh)中，我们详细介绍了五种算法类别。在本节中，我们将讨论哪些算法适合无监督学习。我们还将介绍另一个图分析任务：频繁模式挖掘。
- en: Learning Through Similarity and Community Structure
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过相似性和社区结构学习
- en: Among the five algorithm categories presented in [Chapter 6](ch06.html#analyzing_connections_for_deeper_insigh),
    the last one—classification and prediction—is commonly considered by data scientists
    to be in the machine learning domain. Classification, in particular, is usually
    supervised learning. Prediction comes in all flavors. As we previously pointed
    out, both of those tasks hinge on some means of measuring similarity. Similarity
    algorithms, then, are key tools for machine learning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.html#analyzing_connections_for_deeper_insigh)中介绍的五种算法类别中，最后一种——分类和预测——通常被数据科学家认为属于机器学习领域。特别是分类通常是监督学习。预测有各种各样的形式。正如我们之前指出的那样，这两个任务都依赖于某种度量相似性的方法。因此，相似性算法是机器学习的关键工具之一。
- en: 'If you find all the vertices that have high Jaccard similarity, it might not
    feel like you’re doing machine learning. You could take it one step further: was
    the number of similar vertices that you found much higher or much lower than what
    you expected? You could base your expectation on the typical number of connections
    that a vertex has and the likelihood that two random vertices will have a neighbor
    in common. Characteristics such as these can tell you important things about your
    graph and the real-world things that it represents. For example, suppose a large
    corporation maps out its employees and various job-related data in a graph. A
    manager could search to see if there are other employees with job qualifications
    similar to those of the manager’s current team. What do the results say about
    cross-training, resiliency, and redundancy in the workforce?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你找到所有具有高Jaccard相似性的顶点，可能感觉不像在做机器学习。你可以再进一步：找到的相似顶点数量是比你预期的要高还是低？你可以根据顶点通常具有的连接数以及两个随机顶点有共同邻居的可能性来建立你的期望。这些特征可以告诉你关于图表及其所代表的现实世界事物的重要信息。例如，假设一个大公司在图表中映射出其员工和各种与工作相关的数据。经理可以搜索是否有其他具有与其当前团队相似工作资格的员工。结果对于员工交叉培训、弹性和工作力量中的冗余有何影响？
- en: 'When a graph’s structure is determined by lots of individual players rather
    than by central planning, its community structure is not known a priori; we have
    to analyze the graph to discover the structure. The structure is a reflection
    of the entities and their mutual relationships, so learning the structure tells
    us something about the entities and their dynamics. The modularity-based algorithms
    like Louvain and Leiden are fine examples of self-learning: determining community
    membership by looking at the graph’s own relative densities of connections. The
    recursively defined SimRank and RoleSim measures also fit the self-learning trait
    of unsupervised learning. Then isn’t PageRank also a form of unsupervised learning?'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个图的结构由许多个体玩家而不是中央规划决定时，它的社区结构不是事先已知的；我们必须分析图来发现结构。结构反映了实体及其相互关系，因此学习结构告诉我们一些关于实体及其动态的信息。基于模块性的算法如Louvain和Leiden是自学习的良好示例：通过查看图的自身连接相对密度来确定社区成员资格。递归定义的SimRank和RoleSim度量也符合无监督学习的自学习特征。那么PageRank难道不也是无监督学习的一种形式吗？
- en: These algorithms are also quite valuable. Many financial institutions have found
    that applying centrality and community algorithms to graphs of transactions has
    helped them to better identify financial crimes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法也非常有价值。许多金融机构发现，将中心性和社区算法应用于交易图表中有助于更好地识别金融犯罪。
- en: Finding Frequent Patterns
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查找频繁模式
- en: 'As we’ve said in this book, graphs are wonderful because they make it easy
    to discover and analyze multiconnection patterns based on connections. In Part
    2: Analyze, we talked about finding particular patterns, and we will return to
    that topic later in this chapter. In the context of unsupervised learning, this
    is the goal:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书所述，图表非常棒，因为它们可以轻松发现和分析基于连接的多重连接模式。在第二部分：分析中，我们讨论了查找特定模式，并且在本章稍后将回到这个话题。在无监督学习的背景下，这是目标：
- en: Find any and all patterns that occur frequently.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 发现所有频繁发生的模式。
- en: Computer scientists call this the *frequent subgraph mining* task, because a
    pattern of connections is just a subgraph. This task is particularly useful for
    understanding natural behavior and structure, such as consumer behavior, social
    structures, biological structures, and even software code structure. However,
    it also presents a much more difficult problem. “Any and all” patterns in a large
    graph means a vast number of possible occurrences to check. The saving grace is
    the threshold parameter T. To be considered frequent, a pattern must occur at
    least T times. Selecting a good value for T is important. We want it high enough
    to filter out small, insignificant patterns—the more we filter, the less overall
    work we need to do—but not so high as to exclude interesting patterns. Choosing
    a good threshold can be a machine learning task in itself.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学家称这为*频繁子图挖掘*任务，因为连接的模式只是一个子图。这个任务特别适用于理解自然行为和结构，比如消费者行为、社会结构、生物结构甚至软件代码结构。然而，它也提出了一个更为困难的问题。“任何和所有”大图中的模式意味着要检查大量可能的出现。阈值参数T是挽救的一线希望。要被视为频繁，模式必须至少出现T次。选择一个好的T值很重要。我们希望它足够高，以过滤掉小的、不重要的模式—我们过滤得越多，我们需要做的总体工作就越少—但不要过高以至于排除有趣的模式。选择一个好的阈值可以成为一个机器学习任务。
- en: 'There are many advanced approaches to attempt speeding up frequent subgraph
    mining, but the basic approach is to start with one-edge patterns, keep the patterns
    that occur at least T times, and then try to connect those patterns to make bigger
    patterns:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多高级方法尝试加快频繁子图挖掘的速度，但基本方法是从一边的模式开始，保留至少出现T次的模式，然后尝试连接这些模式以形成更大的模式：
- en: Group all the edges according to their type and the types of their endpoint
    vertices. For example, **`Shopper-(bought)-Product`** is a pattern.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据它们的类型和端点顶点的类型对所有边进行分组。例如，**`Shopper-(bought)-Product`**是一个模式。
- en: Count how many times each pattern occurs.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个模式出现的次数。
- en: Keep all the frequent patterns (having at least T members) and discard the rest.
    For example, we keep `Shopper-(lives_in)-Florida` but eliminate `Shopper-(lives_in)-Guam`
    because it is infrequent.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留所有频繁模式（至少有T个成员）并且丢弃其余模式。例如，我们保留`Shopper-(lives_in)-Florida`但是排除`Shopper-(lives_in)-Guam`因为它不频繁。
- en: Consider every pair of groups that have compatible vertex types (e.g., groups
    1 and 2 both have a `**Shopper**` vertex), and see how many individual vertices
    in group 1 are also in group 2\. Merge these individual small patterns to make
    a new group for the larger pattern. For example, we merge the cases where the
    same person in the frequent pattern `Shopper-(bought)-Blender` was also in the
    frequent pattern `Shopper-(lives_in)-Florida`.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑每对具有兼容顶点类型的组（例如，组1和2都有一个**`Shopper`**顶点），并查看组1中有多少个单独顶点也在组2中。将这些单独的小模式合并以形成更大模式的新组。例如，我们合并在频繁模式`Shopper-(bought)-Blender`中相同人物也在频繁模式`Shopper-(lives_in)-Florida`中的情况。
- en: Repeat steps 2 and 3 (filtering for frequency) for these newly formed larger
    patterns.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和3（过滤频率）针对这些新形成的大模式。
- en: Repeat step 4 using the expanded collection of patterns.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用扩展的模式集重复步骤4。
- en: Stop when no new frequent patterns have been built.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当没有新的频繁模式建立时停止。
- en: There is a complication with counting (step 2). The complication is *isomorphism*,
    that is, how the same set of vertices and edges can fit a template pattern in
    more than one way. Consider the pattern A-(friend_of)-B. If Jordan is a friend
    of Kim, which implies that Kim is a friend of Jordan, is that one instance of
    the pattern or two? Now suppose the pattern is “find pairs of friends A and B,
    who are both friends with a third person, C.” This forms a triangle. Let’s say
    Jordan, Kim, and Logan form a friendship triangle. There are six possible ways
    we could assign Jordan, Kim, and Logan to the variables A, B, and C. You need
    to decide up front whether these types of symmetrical patterns should be counted
    separately or merged into one instance, and then make sure your counting method
    is correct.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在计数中存在一个复杂性（步骤2）。这个复杂性是*同构性*，也就是说，相同的顶点和边集如何以多种方式适配模板模式。考虑模式A-(friend_of)-B。如果Jordan是Kim的朋友，这暗示Kim也是Jordan的朋友，这是一个实例还是两个实例？现在假设模式是“找到朋友A和B的成对，他们都和第三个人C是朋友”。这形成一个三角形。假设Jordan，Kim和Logan形成一个友谊三角形。我们可以有六种可能的方式将Jordan，Kim和Logan分配给变量A，B和C。您需要事先决定是否应将这些类型的对称模式分开计数，然后确保您的计数方法是正确的。
- en: 'Graph algorithms can perform unsupervised machine learning on graph data. Key
    takeaways from this section are as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图算法可以对图数据进行无监督机器学习。本节的关键收获如下：
- en: 'Graph algorithms in several categories fit the self-learning idea of unsupervised
    learning: similarity, community detection, centrality, prediction, and frequent
    pattern mining.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几种类别的图算法符合无监督学习的自学习理念：相似性、社区检测、中心性、预测以及频繁模式挖掘。
- en: Unsupervised learning has the benefit of providing insight without the requirement
    of prior classifications. Unsupervised learning can also make observations relative
    to the data’s own context.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习的好处在于提供见解，而无需事先分类。无监督学习还可以根据数据自身的上下文进行观察。
- en: Extracting Graph Features
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取图特征
- en: In the previous section, we showed how you can use graph algorithms to perform
    unsupervised machine learning. In most of those examples, we analyzed the graph
    as a whole to discover some characteristics, such as communities or frequent patterns.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们展示了如何使用图算法进行无监督机器学习。在大多数示例中，我们分析整个图以发现一些特征，例如社区或频繁模式。
- en: 'In this section, you’ll learn how graphs can provide additional and valuable
    features to describe and help you understand your data. A *graph feature* is a
    characteristic that is based on the pattern of connections in the graph. A feature
    can be either local—attributed to the neighborhood of an individual vertex or
    edge—or global—pertaining to the whole graph or a subgraph. In most cases, we
    are interested in vertex features: characteristics of the neighborhood around
    a vertex. That’s because vertices usually represent the real-world entities we
    want to model with machine learning.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解图如何提供额外和有价值的特征，以描述和帮助您理解您的数据。*图特征*是基于图中连接模式的特征。特征可以是局部的——归因于单个顶点或边的邻域——或全局的——涉及整个图或子图。在大多数情况下，我们对顶点特征感兴趣：顶点周围邻域的特征。这是因为顶点通常代表我们想要用机器学习建模的现实世界实体。
- en: When an entity (an instance of a real-world thing) has several features and
    we arrange those features in a standard order, we call the ordered list a *feature
    vector*. Some of the methods we’ll look at in this section provide individual
    features; others produce entire sets of features. You can concatenate a vertex’s
    entity properties (the ones that aren’t based on connections) with its graph features
    obtained from one or more of the methods discussed here to make a longer, richer
    feature vector. We’ll also look at a special feature vector called an *embedding*,
    which summarizes a vertex’s entire neighborhood.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个实体（现实世界物体的一个实例）有多个特征，并且我们按照标准顺序排列这些特征时，我们称之为*特征向量*。本节我们将讨论的一些方法提供单个特征，其他方法则生成整套特征。您可以将一个顶点的实体属性（那些不基于连接的属性）与从本节讨论的一个或多个方法中获得的图特征连接起来，以制作更长、更丰富的特征向量。我们还将看一种特殊的特征向量称为*嵌入*，它总结了顶点的整个邻域。
- en: These features can provide insight as is, but one of their most powerful uses
    is to enrich the training data for supervised machine learning. Feature extraction
    is one of the key phases in a machine learning pipeline (refer back to [Figure 10-1](#machine_learning_pipeline)).
    For graphs, this is particularly important because traditional machine learning
    techniques are designed for vectors, not for graphs. So in a machine learning
    pipeline, feature extraction is also where we transform the graph into a different
    representation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征可以直接提供见解，但它们最强大的用途之一是丰富监督机器学习的训练数据。特征提取是机器学习管道中的关键阶段之一（请参考[图 10-1](#machine_learning_pipeline)）。对于图形数据来说，这尤为重要，因为传统的机器学习技术设计用于向量，而不是图形。因此，在机器学习管道中，特征提取也是将图形转换为不同表示的地方。
- en: 'In the sections that follow, we’ll look at three key topics: domain-independent
    features, domain-dependent features, and the exciting developments in graph embedding.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论三个关键主题：领域无关特征、领域相关特征以及图嵌入的激动人心的发展。
- en: Domain-Independent Features
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领域无关特征
- en: If graph features are new to you, the best way to understand them is to look
    at simple examples that would work for any graph. Because these features can be
    used regardless of the type of data we are modeling, we say they are *domain independent*.
    Consider the graph in [Figure 10-2](#graph_with_directed_friendship_edges). We
    see a network of friendships, and we count occurrences of some simple domain-independent
    graph features.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图特征对您来说是新的，理解它们的最佳方法是查看适用于任何图的简单示例。因为这些特征可以用于我们建模的任何类型的数据，我们称它们为*领域无关*。考虑图中的[图 10-2](#graph_with_directed_friendship_edges)。我们看到一个友谊网络，并计算一些简单的领域无关图特征的出现次数。
- en: '![Graph with directed friendship edges](assets/gpam_1002.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![带有有向友谊边的图](assets/gpam_1002.png)'
- en: Figure 10-2\. Graph with directed friendship edges (see a larger version of
    this figure at [https://oreil.ly/gpam1002](https://oreil.ly/gpam1002))
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 带有有向友谊边的图（请在 [https://oreil.ly/gpam1002](https://oreil.ly/gpam1002)
    查看此图的更大版本）
- en: '[Table 10-1](#examples_of_domain_independent_features) shows the results for
    four selected vertices (Alex, Chase, Fiona, Justin) and four selected features.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-1](#examples_of_domain_independent_features) 展示了四个选定顶点（亚历克斯、查斯、菲奥娜、贾斯汀）和四个选定特征的结果。'
- en: Table 10-1\. Examples of domain-independent features from the graph of [Figure 10-2](#graph_with_directed_friendship_edges)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-1\. 来自图[图 10-2](#graph_with_directed_friendship_edges) 的领域无关特征示例
- en: '|   | Number of in-neighbors | Number of out-neighbors | Number of vertices
    within two forward hops | Number of triangles (ignoring direction) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|   | 入邻居数量 | 出邻居数量 | 两个前向跳跃内的顶点数量 | 三角形数量（忽略方向） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Alex** | 0 | 2 (Bob, Fiona) | 6 (B, C, F, G, I, J) | 0 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **亚历克斯** | 0 | 2 (鲍勃、菲奥娜) | 6 (B、C、F、G、I、J) | 0 |'
- en: '| **Chase** | 1 (Bob) | 2 (Damon, Eddie) | 2 (D, E) | 1 (Chase, Damon, Eddie)
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **查斯** | 1 (鲍勃) | 2 (达蒙、埃迪) | 2 (D、E) | 1 (查斯、达蒙、埃迪) |'
- en: '| **Fiona** | 2 (Alex, Ivy) | 3 (George, Ivy, Justin) | 4 (G, I, J, H) | 1
    (Fiona, George, Ivy) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **菲奥娜** | 2 (亚历克斯、艾维) | 3 (乔治、艾维、贾斯汀) | 4 (G、I、J、H) | 1 (菲奥娜、乔治、艾维) |'
- en: '| **Justin** | 1 (Fiona) | 0 | 0 | 0 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **贾斯汀** | 1 (菲奥娜) | 0 | 0 | 0 |'
- en: 'You could easily come up with more features, by looking farther than one or
    two hops, by considering generic weight properties of the vertices or edges, and
    by calculating in more sophisticated ways: computing average, maximum, or other
    functions. Because these are domain-independent features, we are not thinking
    about the meaning of “person” or “friend.” We could change the object types to
    “computers” and “sends data to.” Domain-independent features, however, may not
    be the right choice for you if there are many types of edges with very different
    meanings.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看超过一到两个跳点的示例，考虑顶点或边的通用权重属性，并通过计算更复杂的方式（计算平均值、最大值或其他函数）轻松生成更多特征。因为这些是领域无关的特征，我们不考虑“人”或“朋友”的含义。我们可以将对象类型更改为“计算机”和“发送数据至”。但是，如果存在许多具有非常不同含义的边类型，则领域无关特征可能不适合您。
- en: Graphlets
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图元
- en: 'Another option for extracting domain-independent features is to use graphlets.^([1](ch10.html#ch01fn36))
    *Graphlets* are small subgraph patterns that have been systematically defined
    so that they include every possible configuration up to a maximum number of vertices.
    [Figure 10-3](#graphlets_up_to_five_vertices_left_pare) shows all 72 graphlets
    for subgraphs up to five vertices (or nodes). Note that the figure shows two types
    of identifiers: shape IDs (G0, G1, G2, etc.) and graphlet IDs (1, 2, 3, etc.).
    Shape G1 encompasses two different graphlets: graphlet 1, when the reference vertex
    is on the end of the three-vertex chain, and graphlet 2, when the reference vertex
    is in the middle.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种提取与领域无关特征的选择是使用图元。^([1](ch10.html#ch01fn36)) *图元* 是小型子图模式，已经系统地定义，以包括每个可能的配置，直到最大顶点数。[图 10-3](#graphlets_up_to_five_vertices_left_pare)
    展示了所有最多五个顶点（或节点）的 72 种图元。请注意，图中显示了两种类型的标识符：形状 ID（G0、G1、G2 等）和图元 ID（1、2、3 等）。形状
    G1 包含两种不同的图元：当参考顶点位于三顶点链的末端时是图元 1，当参考顶点位于中间时是图元 2。
- en: Counting the occurrences of every graphlet pattern around a given vertex provides
    a standardized feature vector that can be compared to any other vertex in any
    graph. This universal signature lets you cluster and classify entities based on
    their neighborhood structure for applications such as predicting the world trade
    dynamics of a nation^([2](ch10.html#ch01fn37)) or link prediction in dynamic social
    networks like Facebook.^([3](ch10.html#ch01fn38))
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 统计围绕给定顶点的每个图案图形的出现次数，提供了一个标准化的特征向量，可以与任何图中的其他顶点进行比较。这种通用签名让您可以基于其邻域结构对实体进行聚类和分类，适用于诸如预测一个国家的世界贸易动态^([2](ch10.html#ch01fn37))或动态社交网络如
    Facebook 的链接预测^([3](ch10.html#ch01fn38))等应用。
- en: '![Graphlets up to five vertices (or nodes) in size](assets/gpam_1003.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图形图案，最多五个顶点（或节点）](assets/gpam_1003.png)'
- en: Figure 10-3\. Graphlets up to five vertices (or nodes) in size^([4](ch10.html#ch01fn39))
    (see a larger version of this figure at [https://oreil.ly/gpam1003](https://oreil.ly/gpam1003))
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 最多五个顶点（或节点）的图形图案^([4](ch10.html#ch01fn39))（请在 [https://oreil.ly/gpam1003](https://oreil.ly/gpam1003)
    查看此图的较大版本）
- en: A key rule for graphlets is that they are the *induced* subgraph for a set of
    vertices within a graph of interest. Induced means that they include *all* the
    edges that the base graph has among the selected set of vertices. This rule causes
    each particular set of vertices to match at most one graphlet pattern.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图形图案的一个关键规则是它们是感兴趣图中一组顶点的*诱导*子图。诱导意味着它们包括选定顶点集之间的*所有*边缘。该规则使得每个特定的顶点集最多匹配一个图形图案模式。
- en: For example, consider the four persons Fiona, George, Howard, and Ivy in [Figure 10-2](#graph_with_directed_friendship_edges).
    Which shape and graphlet do they match, if any? It’s shape G7, because those four
    persons form a rectangle with one cross-connection. They do not match shape G5,
    the square, because of that cross-connection between George and Ivy. While we’re
    talking about that cross-connection, look carefully at the two graphlets for shape
    G7, graphlets 12 and 13\. Graphlet 13’s source node is located at one end of the
    cross-connection, just as George and Ivy are. This means graphlet 13 is one of
    their graphlets. Fiona and Howard are at the other corners of the square, which
    don’t have the cross-connection. Therefore they have graphlet 12 in their graphlet
    portfolios.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 [图 10-2](#graph_with_directed_friendship_edges) 中考虑 Fiona、George、Howard
    和 Ivy 这四人。如果有的话，它们匹配哪种形状和图形图案？它是形状 G7，因为这四人形成一个带有一个交叉连接的矩形。它们不匹配形状 G5，即方形，因为 George
    和 Ivy 之间有交叉连接。当我们谈论那个交叉连接时，请仔细查看形状 G7 的两个图形图案，图形图案 12 和 13\. 图形图案 13 的源节点位于交叉连接的一端，正如
    George 和 Ivy 所在的位置。这意味着图形图案 13 是它们的图形图案之一。Fiona 和 Howard 位于方形的另外两个角落，它们没有交叉连接。因此，它们在其图形图案合集中具有图形图案
    12。
- en: There is obviously some overlap between the ad hoc features we first talked
    about (e.g., number of neighbors) and graphlets. Suppose a vertex A has three
    neighbors, B, C, and D, as shown in [Figure 10-4](#immediate_neighbors_and_graphlet_implic).
    However, we do not know about any other connections. What do we know about vertex
    A’s graphlets?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们最初讨论的特征之间存在一些重叠（例如邻居数量）和图形图案之间。假设顶点 A 有三个邻居 B、C 和 D，如 [图 10-4](#immediate_neighbors_and_graphlet_implic)
    所示。然而，我们不知道任何其他连接。关于顶点 A 的图形图案我们了解到什么？
- en: It exhibits the graphlet 0 pattern three times. Counting the occurrences is
    important.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它展示了图形图案 0 模式三次。计算其出现次数非常重要。
- en: 'Now consider subgraphs with three vertices. We can define three different subgraphs
    containing A: (A, B, C), (A, B, D), and (A, C, D). Each of those threesomes satisfies
    either graphlet 2 or 3\. Without knowing about the connections among B, C, and
    D (the dotted-line edges in the figure), we cannot be more specific.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在考虑包含三个顶点的子图。我们可以定义包含 A 的三种不同子图：(A, B, C), (A, B, D), 和 (A, C, D)。这些三元组中的每一个都满足图形图案
    2 或 3\. 如果不知道 B、C 和 D 之间的连接（图中虚线边），我们就无法更加具体地判断。
- en: Considering all four vertices, we might be tempted to say they match graphlet
    7\. Since there might be other connections between B, C, and D, it might actually
    be a different graphlet. Which one? Graphlet 11 if there’s one peripheral connection,
    graphlet 13 if it’s two connections, or graphlet 14 if it’s all three possible
    connections.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑所有四个顶点，我们可能会说它们匹配图形图案 7\. 由于 B、C 和 D 之间可能存在其他连接，实际上可能是不同的图形图案。是哪一个？如果有一个外围连接，则为图形图案
    11；如果有两个连接，则为图形图案 13；如果有所有三种可能的连接，则为图形图案 14。
- en: '![Image](assets/gpam_1004.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/gpam_1004.png)'
- en: Figure 10-4\. Immediate neighbors and graphlet implications
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 直接邻居和图形图案的影响
- en: The advantage of graphlets is that they are thorough and methodical. Checking
    for all graphlets up to five-node size is equal to considering all the details
    of the source vertex’s four-hop neighborhood. You could run an automated graphlet
    counter without spending time and money to design custom feature extraction. The
    disadvantage of graphlets is that they can require a lot of computational work,
    and it might be more productive to focus on a more selective set of domain-dependent
    features. We’ll cover these types of features shortly.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图图特征的优点在于它们是彻底和有条理的。检查所有大小为五节点的图图相当于考虑源顶点四跳邻域的所有细节。您可以运行自动化的图图计数器，而不必花费时间和金钱来设计定制特征提取。图图的缺点在于它们可能需要大量的计算工作，而且可能更有成效地专注于更有选择性的领域相关特征。我们很快将介绍这些特征类型。
- en: Graph algorithms
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图算法
- en: 'Here’s a third option for extracting domain-independent graph features: graph
    algorithms! In particular, the centrality and ranking algorithms that we discussed
    in Part 2: Analyze work well because they systematically look at everything around
    a vertex and produce a score for each vertex. [Figure 10-5](#pagerank_scores_for_the_friendship_grap)
    and [Figure 10-6](#closeness_centrality_scores_for_the_fri) show the PageRank
    and closeness centrality^([5](ch10.html#ch01fn40)) scores, respectively, for the
    graph presented earlier in [Figure 10-2](#graph_with_directed_friendship_edges).
    For example, Alex has a PageRank score of 0.15, while Eddie has a PageRank score
    of 1\. This tells us that Eddie is valued by his peers much more than Alex. Eddie’s
    ranking is due not only to the number of connections but also to the direction
    of edges. Howard, who like Eddie has two connections and is at the far end of
    the rough “C” shape of the graph, has a PageRank score of only 0.49983 because
    one edge comes in and the other goes out.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有第三种提取领域无关图特征的选项：图算法！特别是在第2部分讨论过的中心性和排名算法，因为它们系统地查看每个顶点周围的所有内容，并为每个顶点产生一个评分。[图 10-5](#pagerank_scores_for_the_friendship_grap)
    和 [图 10-6](#closeness_centrality_scores_for_the_fri) 分别展示了早期展示的图的PageRank和接近中心度^([5](ch10.html#ch01fn40))分数。例如，Alex的PageRank分数为0.15，而Eddie的PageRank分数为1\.
    这告诉我们，Eddie比Alex受到同行更多的重视。Eddie的排名不仅取决于连接的数量，还取决于边的方向。像Eddie一样有两个连接且位于图形粗略的“C”形末端的Howard，只有0.49983的PageRank分数，因为一条边进入，另一条边出去。
- en: '![Image](assets/gpam_1005.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图像](assets/gpam_1005.png)'
- en: Figure 10-5\. PageRank scores for the friendship graph (see a larger version
    of this figure at [https://oreil.ly/gpam1005](https://oreil.ly/gpam1005))
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 友谊图的PageRank分数（请在[https://oreil.ly/gpam1005](https://oreil.ly/gpam1005)上查看此图的更大版本）
- en: The closeness centrality scores in [Figure 10-6](#closeness_centrality_scores_for_the_fri)
    tell a completely different story. Alex has a top score of 0.47368 because she
    is at the middle of the C. Damon and Howard have scores at or near the bottom—0.11111
    and 0.22222, respectively—because they are at the ends of the C.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-6](#closeness_centrality_scores_for_the_fri) 中的接近中心度分数讲述了一个完全不同的故事。Alex因为位于“C”形中心而获得了0.47368的最高分。Damon和Howard的分数接近或接近底部——分别为0.11111和0.22222，因为他们位于“C”形的末端。'
- en: '![Image](assets/gpam_1006.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图像](assets/gpam_1006.png)'
- en: Figure 10-6\. Closeness centrality scores for the friendship graph (see a larger
    version of this figure at [https://oreil.ly/gpam1006](https://oreil.ly/gpam1006))
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 友谊图的接近中心度分数（请在[https://oreil.ly/gpam1006](https://oreil.ly/gpam1006)上查看此图的更大版本）
- en: 'The main advantage of domain-independent feature extraction is its universality:
    generic extraction tools can be designed and optimized in advance, and the tools
    can be applied immediately on any data. Its unguided approach, however, can make
    it a blunt instrument.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 领域无关特征提取的主要优势在于其普遍性：通用提取工具可以事先设计和优化，并可立即应用于任何数据。然而，其未导向的方法可能使其成为一种粗糙的工具。
- en: Domain-independent feature extraction has two main drawbacks. Because it doesn’t
    pay attention to what types of edges and vertices it considers, it can group together
    occurrences that have the same shape but have radically different meanings. The
    second drawback is that it can waste resources computing and cataloging features
    that have no real importance or no logical meaning. Depending on your use case,
    you may want to focus on a more selective set of domain-dependent features.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 领域无关特征提取有两个主要缺点。因为它不关注考虑的边和顶点类型，它可能会将形状相同但含义完全不同的出现物组合在一起。第二个缺点是它可能会浪费资源计算和分类没有实际重要性或逻辑意义的特征。根据您的用例，您可能希望专注于更有选择性的领域相关特征集。
- en: Domain-Dependent Features
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 领域相关特征
- en: A little bit of domain knowledge can go a long way toward making your feature
    extraction smarter and more efficient.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 少量领域知识可以大大提升您的特征提取智能和效率。
- en: When extracting domain-dependent features, the first thing you want to do is
    pay attention to the vertex types and edge types in your graph. It’s helpful to
    look at a display of your graph’s schema. Some schemas break down information
    hierarchically into graph paths, such as **`City-(IN)-State-(IN)-Country`** or
    **`Day-(IN)-Month-(IN)-Year`**. This is a graph-oriented way of indexing and pregrouping
    data according to location or date. This is the case in a graph model for South
    Korean COVID-19 contact-tracing data,^([6](ch10.html#ch01fn41)) shown in [Figure 10-7](#graph_schema_for_south_korean_covid_one).
    While `city-to-country` and `day-to-year` are each two-hop paths, those paths
    are simply baseline information and do not hold the significance of a two-hop
    path like **`Patient-(INFECTED_BY)-Patient-(INFECTED_BY)-Patient`**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取领域相关特征时，首先要注意图中的顶点类型和边类型。查看图的模式显示非常有帮助。一些模式将信息分层地分解为图路径，例如`City-(IN)-State-(IN)-Country`或`Day-(IN)-Month-(IN)-Year`。这是按位置或日期对数据进行图导向索引和预分组的方式。这在南韩COVID-19接触追踪数据的图模型中有所体现^([6](ch10.html#ch01fn41))，如[Figure 10-7](#graph_schema_for_south_korean_covid_one)所示。虽然`城市到国家`和`日到年`各自都是两跳路径，但这些路径只是基线信息，不具有像`Patient-(INFECTED_BY)-Patient-(INFECTED_BY)-Patient`这样的两跳路径的重要性。
- en: You can see how the graphlet approach and other domain-independent approaches
    can provide confusing results when you have mixed edge types. A simple solution
    is to take a domain-semi-independent approach by considering only certain vertex
    types and edge types when looking for features. For example, if looking for graphlet
    patterns, you might want to ignore the `**Month**` vertices and their connecting
    edges. You might still care about the year of birth of patients and the (exact)
    day on which they traveled, but you don’t need the graph to tell you that each
    year contains 12 months.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到当混合边类型时，图形途径方法和其他领域无关方法可能会提供令人困惑的结果。一个简单的解决方案是通过在查找特征时仅考虑某些顶点类型和边类型来采取领域半独立方法。例如，如果要查找图形模式，您可能希望忽略`**Month**`顶点及其连接边。您可能仍然关心患者的出生年份和他们旅行的确切日期，但不需要图告诉您每年包含12个月。
- en: '![Image](assets/gpam_1007.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1007.png)'
- en: Figure 10-7\. Graph schema for South Korean COVID-19 contact-tracing data (see
    a larger version of this figure at [https://oreil.ly/gpam1007](https://oreil.ly/gpam1007))
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7。南韩COVID-19接触追踪数据的图模式（查看此图的更大版本：[https://oreil.ly/gpam1007](https://oreil.ly/gpam1007)）
- en: 'With this vertex- and edge-type awareness, you can refine some of the domain-independent
    searches. For example, while you can run PageRank on any graph, the scores will
    only have significance if all the edges have the same or relatively similar meanings.
    It would not make sense to run PageRank on the entire COVID-19 contact-tracing
    graph because we can’t rank all the different vertex types and edge types on one
    scale. It would make sense, however, to consider only the `**Patient**` vertices
    and `**INFECTED_BY**` edges. PageRank would then tell you who was the most influential
    Patient in terms of causing infection: patient zero, so to speak.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种顶点和边类型的认知，您可以优化一些与领域无关的搜索。例如，尽管可以在任何图上运行PageRank，但只有当所有边具有相同或相似的含义时，得分才有意义。在整个COVID-19接触追踪图上运行PageRank是没有意义的，因为我们无法将所有不同的顶点类型和边类型排在同一尺度上。然而，仅考虑`**Patient**`顶点和`**INFECTED_BY**`边是有意义的。然后，PageRank将告诉您在引起感染方面谁是最有影响力的患者：可以说是零号患者。
- en: 'In this type of scenario, you also want to apply your understanding of the
    domain to think of small patterns with two or more edges of specific types that
    indicate something meaningful. For this COVID-19 contact-tracing schema, the most
    important facts are Infection Status (InfectionCase), Who (Patient), Where (City
    and TravelEvent), and When (Day_). Paths that connect these are important. A possible
    feature is “number of travel events made by Patient P in March 2020.” A more specific
    feature is “number of Infected Patients in the same city as Patient P in March
    2020.” That second feature is the type of question we posed in Part 2: Analyze.
    You’ll find examples of vertex- and edge-type-specific PageRank and domain-dependent
    pattern queries in the TigerGraph Cloud Starter Kit for COVID-19.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您还希望应用您对领域的理解来考虑具有两个或更多特定类型边缘的小模式，这些边缘表明某种意义。对于这种 COVID-19 接触追踪方案，最重要的事实是感染状态（InfectionCase）、谁（Patient）、在哪里（City
    和 TravelEvent）以及何时（Day_）。连接这些路径是重要的。一个可能的特征是“患者 P 在 2020 年 3 月的旅行事件数”。一个更具体的特征是“在
    2020 年 3 月，与患者 P 相同城市的感染患者数量”。这第二个特征是我们在第二部分提出的问题类型：分析。您将在 TigerGraph Cloud Starter
    Kit for COVID-19 中找到基于顶点和边缘类型特定 PageRank 和领域相关模式查询的示例。
- en: Let’s pause for a minute to reflect on your immediate goal for extracting these
    features. Do you expect these features to directly point out actionable situations,
    or are you building a collection of patterns to feed into a machine learning system?
    The machine learning system could figure out which features matter, to what degree,
    and in what combination. If you’re doing the latter, which is our focus for this
    chapter, then you don’t need to build overly complex features. Instead, focus
    on building-block features. Try to include some that provide a number (e.g., how
    many travel events) or a choice among several possibilities (e.g., city most visited).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们停顿一分钟，思考一下您提取这些特征的即时目标。您是否希望这些特征直接指出可操作的情况，还是正在构建一个供机器学习系统使用的模式集合？机器学习系统可以确定哪些特征重要，程度如何，以及以什么组合。如果是后者，这也是我们本章的重点，那么您无需构建过于复杂的特征。相反，专注于基础特征。尝试包含一些提供数字的特征（例如，多少个旅行事件）或在几种可能性之间进行选择的特征（例如，最常访问的城市）。
- en: 'To provide a little more inspiration for using graph-based features, here are
    some examples of domain-dependent features used in real-world systems to help
    detect financial fraud:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更多地激发使用基于图的特征的灵感，这里有一些现实世界系统中使用的领域相关特征的例子，以帮助检测金融欺诈：
- en: How many shortest paths are there between a loan applicant and a known fraudster,
    up to a maximum path length (because very long paths represent negligible risk)?
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贷款申请人与已知欺诈者之间有多少条最短路径，最大路径长度为上限（因为非常长的路径代表风险微乎其微）？
- en: How many times has the loan applicant’s mailing address, email address, or phone
    number been used by differently named applicants?
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贷款申请人的邮寄地址、电子邮件地址或电话号码已被不同名称的申请人使用了多少次？
- en: How many charges has a particular credit card made in the last 10 minutes?
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定信用卡在过去10分钟内产生了多少次消费？
- en: While it is easy to see that high values on any of these measures make it more
    likely that a situation involves financial misbehavior, our goal is to select
    the right features and the right threshold values. Having the right tests for
    fraud cuts down on both false negatives (missing cases of real fraud) and false
    positives (labeling a situation as fraud when it really isn’t). False positives
    are doubly damaging. They hurt the business because they are rejecting an honest
    business transaction, and they hurt the customer who has been unjustly labeled
    a crook.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然很容易看出任何这些指标的高值更可能涉及金融不端行为，但我们的目标是选择正确的特征和正确的阈值。对欺诈的正确测试可以减少假阴性（错过真实欺诈案例）和假阳性（将一个本不是欺诈的情况标记为欺诈）。假阳性有双重伤害。它们伤害企业，因为它们拒绝了一个诚实的商业交易，也伤害了被不公正地标记为骗子的客户。
- en: 'Graph Embeddings: A Whole New World'
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图嵌入：一个全新的世界
- en: Our last approach to feature extraction is graph embedding, a hot topic of recent
    research and discussion. Some authorities may find it unusual that we are classifying
    graph embedding as a type of feature extraction. Isn’t graph embedding a kind
    of dimensionality reduction? Isn’t it representation learning? Isn’t it a form
    of machine learning itself? All of those are true. Let’s first define a graph
    embedding.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后一种特征提取的方法是图嵌入，这是最近研究和讨论的热门话题。一些权威人士可能觉得我们将图嵌入分类为一种特征提取有些不寻常。图嵌入不是一种降维吗？它不是表示学习吗？它不是一种机器学习本身吗？这些说法都是正确的。让我们首先定义一下图嵌入。
- en: An *embedding* is a representation of a topological object in a particular system
    such that the properties we care about are preserved (or approximated well). The
    last part, preserving the properties we care about, gets to the heart of why we
    use embeddings. A well-chosen embedding makes it more convenient to see what we
    want to see.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌入* 是将一个拓扑对象表示为特定系统中的表示，使得我们关心的属性被保持（或者被很好地近似）。最后一部分，保持我们关心的属性，正是我们使用嵌入的核心原因。选择适当的嵌入使得我们更方便地看到我们想要看到的内容。'
- en: 'Here are several examples to help illustrate the meaning of embeddings:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个例子来帮助说明嵌入的含义：
- en: The Earth is a sphere, but we print world maps on flat paper. The representation
    of the Earth on paper is an embedding. There are several different standard representations
    or embeddings of the Earth as a map. [Figure 10-8](#three_embeddings_of_the_earthapostrophe)
    shows some examples.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地球是一个球体，但我们在平面纸上印制世界地图。地球在纸上的表示就是一种嵌入。地球有几种不同的标准表示或嵌入作为地图。[图 10-8](#three_embeddings_of_the_earthapostrophe)展示了一些例子。
- en: Prior to the late 2010s, when someone said “graph embedding,” they probably
    meant something like the Earth example. To represent all the connections in a
    graph without edges touching one another, you often need three or more dimensions.
    Whenever you see a graph on a flat surface, it’s an embedding, as in [Figure 10-9](#some_graphs_can_be_embedded_in_two_d_sp).
    Moreover, unless your data specifies the location of vertices, then even a 3D
    representation is an embedding because it’s a particular choice about the placement
    of the vertices. From a theoretical perspective, it actually takes up to *n −
    1* dimensions to represent a graph with *n* vertices.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 2010 年代末之前，当有人提到“图嵌入”时，他们可能指的是像地球这样的东西。要表示图中所有的连接而不让边相交，通常需要三个或更多维度。每当你在一个平面上看到一个图时，它都是一个嵌入，如[图 10-9](#some_graphs_can_be_embedded_in_two_d_sp)所示。此外，除非你的数据指定了顶点的位置，否则即使是三维表示也是一个嵌入，因为它是关于顶点放置的特定选择。从理论上讲，实际上需要
    *n − 1* 维度来表示具有 *n* 个顶点的图。
- en: In natural language processing (NLP), a word embedding is a sequence of scores
    (i.e., a feature vector) for a given word (see [Figure 10-10](#word_embedding)).
    There is no natural interpretation of the individual scores, but a machine learning
    program sets the scores so that words that tend to occur near one another in training
    documents have similar embeddings. For example, “machine” and “learning” might
    have similar embeddings. A word embedding is not convenient for human use, but
    it is very convenient for computer programs that need a computational way of understanding
    word similarities and groupings.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，词嵌入是给定单词的一系列分数（即特征向量）。对于个别分数没有自然解释，但是机器学习程序设置这些分数，以便在训练文档中经常在一起出现的单词具有相似的嵌入。例如，“机器”和“学习”可能具有相似的嵌入。词嵌入对人类使用并不方便，但对需要计算机方式理解单词相似性和分组的计算机程序非常方便。
- en: '![Image](assets/gpam_1008.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/gpam_1008.png)'
- en: Figure 10-8\. Three embeddings of the Earth’s surface onto 2D space^([7](ch10.html#ch01fn42))
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. 地球表面的三种嵌入到二维空间的表示^([7](ch10.html#ch01fn42))
- en: '![Image](assets/gpam_1009.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/gpam_1009.png)'
- en: Figure 10-9\. Some graphs can be embedded in 2D space without intersecting edges
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-9\. 一些图可以在二维空间中嵌入，而不会有交叉边。
- en: '![Image](assets/gpam_1010.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/gpam_1010.png)'
- en: Figure 10-10\. Word embedding
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-10\. 词嵌入
- en: 'In recent years, graph embedding has taken on a new meaning, analogous to word
    embedding. We compute one or more feature vectors to approximate the graph’s neighborhood
    structure. In fact, when people say “graph embedding,” they often mean *vertex
    embedding*: computing a feature vector for each vertex in the graph. A vertex’s
    embedding tells us something about how it connects to others. We can then use
    the collection of vertex embeddings to approximate the graph, no longer needing
    to consider the edges. There are also methods to summarize the whole graph as
    one embedding. This is useful for comparing one graph to another. In this book,
    we will focus on vertex embeddings.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，图嵌入（graph embedding）已经获得了新的含义，类似于词嵌入（word embedding）。我们计算一个或多个特征向量来近似图的邻域结构。实际上，当人们说“图嵌入”时，他们通常指的是*顶点嵌入*：计算图中每个顶点的特征向量。顶点的嵌入告诉我们有关它如何连接到其他顶点的信息。然后，我们可以使用顶点嵌入的集合来近似表示整个图，不再需要考虑边。还有方法将整个图汇总为一个嵌入，这对比较不同图很有用。在本书中，我们将重点讨论顶点嵌入。
- en: '[Figure 10-11](#left_parenthesisaright_parenthesis_kara) shows an example of
    a graph (a) and portion of its vertex embedding (b). The embedding for each vertex
    (a series of 32 numbers) describes the structure of its neighborhood without directly
    mentioning any of its neighbors.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 10-11](#left_parenthesisaright_parenthesis_kara) 展示了图例子（a）及其部分顶点嵌入（b）的示例。每个顶点的嵌入（32
    个数字的系列）描述了其邻域的结构，而不直接提到任何邻居。'
- en: '![Image](assets/gpam_1011.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1011.png)'
- en: Figure 10-11\. (a) Karate club graph^([8](ch10.html#ch01fn43)) and (b) 64-element
    embedding for two of its vertices
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-11\. (a) Karate 俱乐部图^([8](ch10.html#ch01fn43)) 和 (b) 其中两个顶点的 64 元素嵌入
- en: Let’s return to the question of classifying graph embeddings. What graph embeddings
    give us is a set of feature vectors. For a graph with a million vertices, a typical
    embedding vector would be a few hundred elements long, a lot less than the upper
    limit of one million dimensions. Therefore, graph embeddings represent a form
    of dimensionality reduction. If we’re using graph embeddings to get feature vectors,
    they’re also a form of feature extraction. As we will see, the methodology to
    generate an embedding qualifies as machine learning, so they are also representation
    learning.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到分类图嵌入的问题。图嵌入给我们的是一组特征向量。对于一个拥有百万个顶点的图，一个典型的嵌入向量可能只有几百个元素，远少于一百万维的上限。因此，图嵌入代表了一种降维的形式。如果我们使用图嵌入来获取特征向量，它们也是一种特征提取的形式。正如我们将看到的，生成嵌入的方法符合机器学习的定义，因此它们也是表示学习的一种形式。
- en: Does any feature vector qualify as an embedding? That depends on whether your
    selected features are telling you what you want to know. Graphlets come closest
    to the learned embeddings we are going to examine because of the methodical way
    they deconstruct neighborhood relationships.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 任何特征向量都能够作为嵌入吗？这取决于你选择的特征是否告诉了你想知道的信息。由于它们系统地分解了邻域关系，图小片段是我们即将研究的学习嵌入的最接近模型。
- en: Random walk-based embeddings
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于随机游走的嵌入
- en: One of the best-known approaches for graph embedding is to use random walks
    to get a statistical sample of the neighborhood surrounding each vertex *v*. A
    random walk is a sequence of connected hops in a graph *G*. The walk starts at
    some vertex *v*. It then picks a random neighbor of *v* and moves there. It repeats
    this selection of random neighbors until it is told to stop. In an unbiased walk,
    there is an equal probability of selecting any of the outgoing edges.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图嵌入中最著名的方法之一是使用随机游走来获取围绕每个顶点 *v* 的邻域的统计样本。随机游走是在图 *G* 中连接的跳跃序列。游走从某个顶点 *v* 开始。然后选择
    *v* 的一个随机邻居并移动到那里。它重复选择随机邻居直到被告知停止。在无偏的游走中，选择任何一条出边的概率都是相等的。
- en: Random walks are great because they are easy to do and they gather a lot of
    information efficiently. All feature extraction methods we have looked at before
    require following careful rules about how to traverse the graph; graphlets are
    particularly demanding due to their very precise definitions and distinctions
    from one another. Random walks are carefree. Just go.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 随机游走非常适合，因为它们易于操作且能够高效地收集大量信息。我们之前看到的所有特征提取方法都要求遵循如何遍历图的精确规则；图小片段特别严格，因为它们具有非常精确的定义和彼此的区别。随机游走则毫不在意，随便走。
- en: For the example graph in [Figure 10-12](#an_ordinary_graph_for_leisurely_walks),
    suppose we start a random walk at vertex A. There is an equal probability of one
    in three that we will next go to vertex B, C, or D. If you start the walk at vertex
    E, there is a 100% chance that the next step will be to vertex B. There are variations
    of random-walk rules with the possibility of staying in place, reversing your
    last step, jumping back to the start, or jumping to a random vertex.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图 10-12 中的示例图，在 [Figure 10-12](#an_ordinary_graph_for_leisurely_walks) 中假设我们从顶点
    A 开始随机行走。有三分之一的概率我们会下一步到达顶点 B、C 或 D 中的一个。如果你从顶点 E 开始行走，则下一步将百分百地到达顶点 B。有随机行走规则的变化，可能停留在原地，反转上一步，跳回起点或跳转到随机顶点。
- en: '![Image](assets/gpam_1012.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1012.png)'
- en: Figure 10-12\. An ordinary graph for leisurely walks
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-12 普通图，适合悠闲行走
- en: Each walk can be recorded as a list of vertices, in the order in which they
    were visited. A-D-H-G-C is a possible walk. You can think of each walk as a signature.
    What does the signature tell us? Suppose that walk W1 starts at vertex 5 and then
    goes to 2\. Walk W2 starts at vertex 9 and then goes to 2\. Now they are both
    at 2\. From here on, they have exactly the same probabilities for the remainder
    of their walks. Those individual walks are unlikely to be the same, but if there
    is a concept of a “typical walk” averaged over a sampling of several walks, then,
    yes, the signatures of 5 and 9 would be similar. All because 5 and 9 share a neighbor
    2\. Moreover, the “typical walk” of vertex 2 itself would be similar, except offset
    by one step.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 每次行走可以记录为顶点列表，按访问顺序排列。A-D-H-G-C 是可能的一次行走。你可以将每次行走看作是一个签名。这些签名告诉我们什么呢？假设行走 W1
    从顶点 5 开始，然后到达 2\. 行走 W2 从顶点 9 开始，然后也到达 2\. 现在它们都在 2\. 从这里开始，它们在剩余行走中的概率完全相同。这些单独的行走不太可能相同，但是如果有一个“典型行走”的概念，通过对多次行走进行采样平均，那么，是的，顶点
    5 和 9 的签名会相似。这一切都因为 5 和 9 共享了邻居 2\. 此外，顶点 2 本身的“典型行走”也会相似，只是偏移了一个步骤。
- en: It turns out that these random walks gather neighborhood information much in
    the same way that SimRank and RoleSim gather theirs. The difference is that those
    role similarity algorithms considered *all* paths (by considering all neighbors),
    which is computationally expensive. Let’s take a look at two random-walk-based
    graph embedding algorithms that use a completely different computational method,
    one borrowed from neural networks.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这些随机行走以与 SimRank 和 RoleSim 类似的方式收集邻域信息。不同之处在于，那些角色相似性算法考虑了 *所有* 路径（通过考虑所有邻居），这在计算上是昂贵的。让我们看看两种基于随机行走的图嵌入算法，它们使用了完全不同的计算方法，其中一种是从神经网络中借鉴过来的。
- en: DeepWalk
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepWalk
- en: The DeepWalk algorithm^([9](ch10.html#ch10newfn1)) collects *k* random walks
    of length *λ* for every vertex in the graph. If you happen to know the word2vec
    algorithm^([10](ch10.html#ch10newfn2)), the rest is easy. Treat each vertex like
    a word and each walk like a sentence. Pick a window width *w* for the *skip-grams*
    and a length *d* for your embedding. You will end up with an embedding (latent
    feature vector) of length *d* for each vertex. The DeepWalk authors found that
    having walk count *k* = 30, walk length *λ* = 40, window width *w* = 10, and embedding
    length *d* = 64 worked well for their test graphs. Your results may vary. [Figure 10-13](#left_parenthesisaright_parenthesis_rand)(a)
    shows an example of a random walk, starting at vertex C and with a length of 16,
    which is 15 steps or hops from the starting point. The shadings will be explained
    when we explain skip-grams.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: DeepWalk 算法^([9](ch10.html#ch10newfn1)) 对图中每个顶点收集长度为 *λ* 的 *k* 个随机行走。如果你了解 word2vec
    算法^([10](ch10.html#ch10newfn2))，其余的就简单了。将每个顶点看作一个词，每次行走看作一句话。选择窗口宽度 *w* 用于 *skip-grams*，以及长度
    *d* 用于嵌入。你将得到每个顶点长度为 *d* 的嵌入（潜在特征向量）。DeepWalk 的作者发现，在他们的测试图中，设置行走计数 *k* = 30，行走长度
    *λ* = 40，窗口宽度 *w* = 10，嵌入长度 *d* = 64 效果很好。你的结果可能会有所不同。图 10-13 (a) 展示了一个从顶点 C 开始长度为
    16 的随机行走示例，即距起点 15 步或跳跃。当我们解释 *skip-grams* 时，这些阴影将会被解释。
- en: '![Image](assets/gpam_1013.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1013.png)'
- en: Figure 10-13\. (a) Random walk vector and (b) a corresponding skip-gram
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-13 (a) 随机行走向量及其对应的 *skip-gram*
- en: We assume you don’t know word2vec, so we’ll give a high-level explanation, enough
    so you can appreciate what is happening. This is the conceptual model. The actual
    algorithm plays a lot of statistical tricks to speed up the work. First, we construct
    a simple neural network with one hidden layer, as shown in [Figure 10-14](#neural_network_for_deepwalk).
    The input layer accepts vectors of length *n*, where *n* = number of vertices.
    The hidden layer has length *d*, the embedding length, because it is going to
    be learning the embedding vectors. The output layer also has length *n*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您不了解word2vec，因此我们将提供一个高层次的解释，足以让您理解正在发生的事情。这是概念模型。实际的算法通过许多统计技巧加快了工作速度。首先，我们构建了一个简单的神经网络，有一个隐藏层，如[图 10-14](#neural_network_for_deepwalk)所示。输入层接受长度为*n*的向量，其中*n*
    = 顶点数。隐藏层的长度为*d*，嵌入长度，因为它将学习嵌入向量。输出层的长度也为*n*。
- en: '![Image](assets/gpam_1014.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1014.png)'
- en: Figure 10-14\. Neural network for DeepWalk
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-14\. DeepWalk的神经网络
- en: Each vertex needs to be assigned a position in the input and output layer. For
    example, vertex A is at position 1, vertex B is at position 2, and so on. Between
    the layers are two meshes of *n* × *d* connections, from each element in one layer
    to each element in the next layer. Each edge has a random weight initially, but
    we will gradually adjust the weights of the first mesh.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 每个顶点都需要在输入层和输出层中分配一个位置。例如，顶点A位于位置1，顶点B位于位置2，依此类推。在层与层之间有两个*n* × *d*连接的网格，从一个层的每个元素到下一个层的每个元素。每条边最初具有随机权重，但我们将逐渐调整第一个网格的权重。
- en: Start with one walk for one starting vertex. At the input, we represent the
    vertex using *one-hot encoding*. The vector element that corresponds to the starting
    vertex is set to 1; all other elements are set to 0\. We are going to train this
    neural network to predict the neighborhood of the vertex given at the input.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个起始顶点开始进行一次行走。在输入时，我们使用*one-hot编码*表示顶点。与起始顶点对应的向量元素设置为1；所有其他元素设置为0。我们将训练这个神经网络来预测给定输入顶点的邻域。
- en: Applying the weights in the first mesh to our one-hot input, we get a weighted
    vertex in the hidden layer. This is the current guess for the embedding of the
    input vertex. Take the values in the hidden layers and multiply by the weights
    of the second mesh to get the output layer values. You now have a length-*n* vector
    with random weights.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 将第一个网格中的权重应用于我们的one-hot输入，我们得到隐藏层中的加权顶点。这是输入顶点嵌入的当前猜测。将隐藏层中的值乘以第二个网格的权重以获得输出层的值。现在你有一个具有随机权重的长度为*n*的向量。
- en: We’re going to compare this output vector with a skip-gram representation of
    the walk. This is where we use the window parameter *w*. For each vertex in the
    graph, count how many times it appears within *w*-steps before or after the input
    vertex *v*. We’ll skip the normalization process, but your final skip-gram vector
    expresses the relative likelihood that each of the *n* vertices is near vertex
    *v* in this walk. Now we’ll explain the result of [Figure 10-13](#left_parenthesisaright_parenthesis_rand).
    Vertex C was the starting point for the random walk; we’ve used dark shading to
    highlight every time we stepped on vertex C. The light shading shows every step
    that is within *w* = 2 steps of vertex C. Then, we form the skip-gram in (b) by
    counting how many times we set foot on each vertex within the shaded zones. For
    example, vertex G was stepped on twice, so the skip-gram has 2 in the position
    for G. This is a long walk on a small graph, so most vertices were stepped on
    within the windows. For short walks on big graphs, most of the values will be
    0.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比较这个输出向量与步行的skip-gram表示。这是我们使用窗口参数*w*的地方。对于图中的每个顶点，在输入顶点*v*的前后*w*步内出现的次数。我们将跳过标准化过程，但您的最终skip-gram向量表达了在这次行走中每个*n*个顶点靠近顶点*v*的相对可能性。现在我们将解释[图 10-13](#left_parenthesisaright_parenthesis_rand)的结果。顶点C是随机行走的起点；我们用深色标记了每次踏上顶点C的情况。浅色显示了距离顶点C*w*
    = 2步的每一步。然后，我们在(b)中形成skip-gram，通过计算在阴影区域内每个顶点被踏足的次数。例如，顶点G被踏足了两次，因此skip-gram在G的位置有2。这是在一个小图上的长步行，因此大多数顶点都在窗口内被踏足。对于大图上的短步行，大多数值将为0。
- en: Our output vector was supposed to be a prediction of this skip-gram. Comparing
    each position in the two vectors, if the output vector’s value is higher than
    the skip-gram’s value, then reduce the corresponding weight in the input mesh
    will be lower. If the value is lower, then raise the corresponding weight.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输出向量应该是这个skip-gram的预测。比较两个向量中每个位置的值，如果输出向量的值高于skip-gram的值，则减少输入网格中相应的权重。如果值较低，则提高相应的权重。
- en: You’ve processed one walk. Repeat this for one walk of each vertex. Now repeat
    for a second walk of each vertex, until you’ve adjusted your weights for *k* ×
    *n* walks. You’re done! The weights of the first *n* × *d* mesh are the length-*d*
    embeddings for your *n* vectors. What about the second mesh? Strangely, we were
    never going to directly use the output vectors, so we didn’t bother to adjust
    its weight.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经处理了一次遍历。重复这个过程，对每个顶点进行第二次遍历，直到你调整了*k* × *n*次权重。完成了！第一个*n* × *d*网格的权重是您*n*向量的长度-*d*嵌入。第二个网格呢？奇怪的是，我们从来不打算直接使用输出向量，所以我们没有费心调整它的权重。
- en: 'Here is how to interpret and use a vertex embedding:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何解释和使用顶点嵌入的方法：
- en: For neural networks in general, you can’t point to a clear real-world meaning
    of the individual elements in the latent feature vector.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于神经网络来说，通常无法指出潜在特征向量中各个元素的明确现实世界含义。
- en: 'Based on how we trained the network, though, we can reason backward from the
    skip-grams, which represent the neighbors around a vertex: vertices that have
    similar neighborhoods should have similar embeddings.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于我们如何训练网络，我们可以从skip-gram逆推，这些skip-gram表示顶点周围的邻居：具有相似邻域的顶点应该具有相似的嵌入。
- en: If you remember the example earlier about two paths that were offset by one
    step, note that those two paths would have very similar skip-grams. So vertices
    that are close to one another should have similar embeddings.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您记得之前关于两条路径偏移一个步骤的例子，请注意，这两条路径的skip-gram应该非常相似。因此，彼此接近的顶点应该具有相似的嵌入。
- en: One critique of DeepWalk is that its uniformly random walk is too random. In
    particular, it may wander far from the source vertex before getting an adequate
    sample of the neighborhoods closer to the source. One way to address that is to
    include a probability of resetting the walk by magically teleporting back to the
    source vertex and then continuing with random steps again, as shown in [Zhou,
    Wu, and Tan](https://oreil.ly/3YiLc). This is known as “random walk with restart.”
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对DeepWalk的一个批评是它的均匀随机游走过于随机。特别是在从源顶点远离之前，可能会漫步到足够接近源的邻域的样本。解决这个问题的一种方法是通过包括重置漫步的概率，神奇地传送回源顶点，然后再次进行随机步骤，如[Zhou,
    Wu, and Tan](https://oreil.ly/3YiLc)所示。这被称为“带重启的随机游走”。
- en: Node2vec
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Node2vec
- en: 'An interesting extension of the random walk is [node2vec](https://oreil.ly/GoPWK).
    It uses the same skip-gram training process as DeepWalk, but it gives the user
    two adjustment parameters to control the direction of the walk: go farther (depth),
    go sideways (breadth), or go back a step. Farther and back seem obvious, but what
    exactly does sideways mean?'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 随机游走的一个有趣扩展是[node2vec](https://oreil.ly/GoPWK)。它使用与DeepWalk相同的skip-gram训练过程，但它给用户两个调整参数来控制游走的方向：走得更远（深度）、走到侧面（广度）、或者后退一步。远和后退看起来很明显，但侧面到底意味着什么呢？
- en: 'Suppose we start at vertex A of the graph in [Figure 10-15](#illustrating_the_biased_random_walk_wit).
    Its neighbors are vertices B, C, and D. Since we are just starting, any of the
    choices would be moving forward. Let’s go to vertex C. For the second step, we
    can choose from any of vertex C’s neighbors: A, B, F, G, or H.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从图中的顶点A开始，图示见[图 10-15](#illustrating_the_biased_random_walk_wit)。它的邻居是顶点B、C和D。由于我们刚开始，选择任何一个都会向前移动。让我们去顶点C。在第二步中，我们可以从顶点C的邻居中选择：A、B、F、G或H。
- en: '![Image](assets/gpam_1015.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1015.png)'
- en: Figure 10-15\. Illustrating the biased random walk with memory used in node2vec
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-15\. 说明带有内存的偏向随机游走在node2vec中使用
- en: 'If we remember what our choices were in our previous step, we can classify
    our current set of neighbors into three groups: back, sideways, and forward. We
    also assign a weight to each connecting edge, which represents the unnormalized
    probability of selecting that edge.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们记得上一步中的选择，我们可以将当前的邻居分成三组：后向、侧向和前向。我们还为每个连接的边分配一个权重，代表选择该边的未标准化概率。
- en: Back
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 后向
- en: 'In our example: vertex A. Edge weight = 1/*p*.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中：顶点A。边的权重 = 1/*p*。
- en: Sideways
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 侧面
- en: 'These are the vertices that were available in the last step and are also available
    in this step. They represent a second chance to visit a different neighbor of
    where you were previously. In our example: vertex B. Edge weight = 1.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在上一步中可用的顶点，并且在本步中也可用。它们代表了访问先前所在位置的不同邻居的第二次机会。在我们的示例中：顶点B。边权重=1。
- en: Forward
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正向
- en: 'There are all the vertices that don’t go back or sideways. In our example:
    vertices F, G, and H. Edge weight = 1/*q*.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 所有不返回或侧向的顶点。在我们的示例中：顶点F、G和H。边权重=1/*q*。
- en: If we set *p* = *q* = 1, then all choices have equal probability, so we’re back
    to an unbiased random walk. If *p* < 1, then returning is more likely than going
    sideways. If *q* < 1, then each of the forward (depth) options is more likely
    than sideways (breadth). Returning also keeps the walk closer to home (e.g., similar
    to breadth-first-search), because if you step back and then step forward randomly,
    you are trying out the different options in your previous neighborhood.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们设置*p* = *q* = 1，则所有选择的概率相等，因此我们回到了一个无偏的随机游走。如果*p* < 1，则返回比侧向更有可能。如果*q* <
    1，则每个前向（深度）选项比侧向（广度）更有可能。返回还使步行保持在家附近（例如，类似于广度优先搜索），因为如果您向后走，然后随机向前走，您正在尝试前一邻域中的不同选项。
- en: This ability to tune the walk makes node2vec more flexible than DeepWalk, which
    results in better models, in many cases, in exchange for higher computational
    cost.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 调整步行路径使得node2vec比DeepWalk更加灵活，这在许多情况下产生了更好的模型，但是以更高的计算成本为代价。
- en: 'Besides the random-walk approach, there are several other techniques for graph
    embedding, each with their advantages and disadvantages: matrix factorization,
    edge reconstruction, graph kernel, and generative models. [FastRP](https://oreil.ly/QhbWl)
    and [NodePiece](https://oreil.ly/r6FZ5) show a promising balance of real-world
    efficiency and accuracy. Though already a little dated, Hongyun Cai, Vincent W.
    Zheng, and Kevin Chen-Chuan Chang’s [*A Comprehensive Survey of Graph Embedding:
    Problems, Techniques, and Applications*](https://oreil.ly/bIKdV) provides a thorough
    overview with several accessible tables, which compare the features of different
    techniques. Ilya Makarov, Dmitrii Kiselev, Nikita Nikitinsky, and Lovro Subelj
    have a more recent [survey on graph embeddings](https://oreil.ly/3FxLA).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 除了随机游走方法之外，还有几种其他的图嵌入技术，各有优缺点：矩阵因子分解、边重建、图核函数和生成模型。[FastRP](https://oreil.ly/QhbWl)
    和 [NodePiece](https://oreil.ly/r6FZ5) 展示了在现实世界中效率和准确性之间有一个有前途的平衡。虽然已经有些过时，但是**蔡宏运**、**郑卫华**和**张晓川**的[*图嵌入综述：问题、技术和应用的全面调查*](https://oreil.ly/bIKdV)
    提供了一个详尽的概述，并附有几个易于理解的表格，比较了不同技术的特点。**伊利亚·马卡罗夫**、**德米特里·基谢列夫**、**尼基塔·尼基金斯基**和**洛夫罗·苏贝尔**最近进行了一项[关于图嵌入的调查](https://oreil.ly/3FxLA)。
- en: 'Graphs can provide additional and valuable features to describe and understand
    your data. Key takeaways from this section are as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图可以提供额外和有价值的特征来描述和理解您的数据。本节的主要收获如下：
- en: A graph feature is a characteristic that is based on the pattern of connections
    in the graph.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图特征是基于图中连接模式的特征。
- en: Graphlets and graph algorithms such as centrality provide domain-independent
    features for any graph.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图小片和诸如中心性等的图算法为任何图提供了领域独立的特征。
- en: Applying some domain knowledge to guide your feature extraction leads to more
    meaningful features.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用一些领域知识来指导您的特征提取将产生更有意义的特征。
- en: Machine learning can produce vertex embeddings, which encode vertex similarity
    and proximity in terms of compact feature vectors.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习可以生成顶点嵌入，这些嵌入使用紧凑的特征向量编码顶点的相似性和接近性。
- en: Random walks are a simple way to sample a vertex’s neighborhood.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机漫步是采样顶点邻域的简单方法。
- en: Graph Neural Networks
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图神经网络
- en: In the popular press, it’s not AI unless it uses a neural network, and it’s
    not machine learning unless it uses deep learning. Neural networks were originally
    designed to emulate how the human brain works, but they evolved to address the
    capabilities of computers and mathematics. The predominant models assume that
    your input data is in a matrix or tensor; it’s not clear how to present and train
    the neural network with interconnected vertices. But are there graph-based neural
    networks? Yes!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在流行媒体中，如果没有使用神经网络，那就不是AI；如果没有使用深度学习，那就不是机器学习。神经网络最初是为了模拟人类大脑的工作原理而设计的，但它们已经发展到可以处理计算机和数学的能力。主流模型假定您的输入数据是一个矩阵或张量；目前尚不清楚如何呈现和训练具有相互连接顶点的神经网络。但是有基于图的神经网络吗？是的！
- en: Graph neural networks (GNNs) are conventional neural networks with an added
    twist for graphs. Just as there are several variations of neural networks, there
    are several variations of GNNs. The simplest way to include the graph itself into
    the neural network is through convolution.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNN）是传统神经网络加入图形特性的变体。就像神经网络有几种变体一样，GNN也有几种变体。将图本身包含到神经网络中的最简单方式是通过卷积。
- en: Graph Convolutional Networks
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图卷积网络
- en: In mathematics, *convolution* is how two functions affect the result if one
    acts on the other in a particular way. It is often used to model situations in
    which one function describes a primary behavior and another function describes
    a secondary effect. For example, in image processing, convolution takes into account
    neighboring pixels to improve identification of boundaries and to add artificial
    blur. In audio processing, convolution is used to both analyze and synthesize
    room reverberation effects. A convolutional neural network (CNN) is a neural network
    that includes convolution in the training process. For example, you could use
    a CNN for facial recognition. The CNN would systematically take into account neighboring
    pixels, an essential duty when analyzing digital images.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，*卷积* 是指两个函数以特定方式相互作用时对结果的影响。它通常用于模拟一个函数描述主要行为，而另一个函数描述次要效应的情况。例如，在图像处理中，卷积考虑到相邻像素以改善边界识别并添加人工模糊。在音频处理中，卷积用于分析和合成房间混响效果。卷积神经网络（CNN）是在训练过程中包含卷积的神经网络。例如，你可以用CNN进行人脸识别。CNN会系统地考虑相邻像素，在分析数字图像时是至关重要的职责。
- en: A graph convolutional network (GCN) is a neural network that uses graph traversal
    as a convolution function during the learning process. While there were some earlier
    related works, the first model to distill the essence of graph convolution into
    a simple but powerful neural network was presented in 2017 in Thomas Kipf and
    Max Welling’s “[Semi-Supervised Classification with Graph Convolutional Networks”](https://oreil.ly/b14rL).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积网络（GCN）是在学习过程中使用图遍历作为卷积函数的神经网络。虽然早期有一些相关工作，但第一个将图卷积的本质提炼为简单而强大的神经网络模型是由Thomas
    Kipf和Max Welling于2017年提出的“[使用图卷积网络进行半监督分类](https://oreil.ly/b14rL)”。
- en: For graphs, we want the embedding for each vertex to include information about
    the relationships to other vertices. We can use the principle of convolution to
    accomplish this. [Figure 10-16](#convolution_using_neighbors_of_a_vertex) shows
    a simple convolution function, with a general model at top and a more specific
    example at bottom.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图形，我们希望每个顶点的嵌入包含关于与其他顶点关系的信息。我们可以使用卷积原理来实现这一点。[图 10-16](#convolution_using_neighbors_of_a_vertex)
    显示了一个简单的卷积函数，顶部是一个通用模型，底部是一个更具体的例子。
- en: '![Image](assets/gpam_1016.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1016.png)'
- en: Figure 10-16\. Convolution using neighbors of a vertex
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-16\. 使用顶点邻居的卷积
- en: 'In part (a) of the figure, the primary function is Features(*v*): given a vertex
    *v*, output its feature vector. The convolution combines the features of *v* with
    the features of all of the neighbors of *v*: u1, u2,...,u*N*. If the features
    are numeric, a simple convolution would be adding them. The result is the newly
    convolved features of *v*. In part (b), we set *v* = D from [Figure 10-15](#illustrating_the_biased_random_walk_wit).
    Vertex D has two neighbors, A and H. We insert one more step after summing the
    feature vectors: divide by the degree of the primary vertex. Vertex D has 2 neighbors,
    so we divide by 2\. This regularizes the output values so that they don’t keep
    getting bigger and bigger. (Yes, technically we should divide by deg(*v*) + 1,
    but the simpler version seems to be good enough.)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中的部分（a），主要函数是Features(*v*)：给定顶点 *v*，输出其特征向量。卷积将 *v* 的特征与所有邻居 *u1, u2,...,u*N*
    的特征结合起来。如果特征是数值的，简单的卷积操作可以是加法。结果是 *v* 的新卷积特征。在部分（b），我们设定 *v* = D，来自[图 10-15](#illustrating_the_biased_random_walk_wit)。顶点
    D 有两个邻居，A 和 H。在求和特征向量之后，我们再插入一步：除以主顶点的度。顶点 D 有 2 个邻居，所以我们除以 2。这样可以使输出值规范化，以防止它们不断增大。（是的，严格来说我们应该除以
    deg(*v*) + 1，但更简单的版本似乎已经足够好了。）
- en: 'Let’s do a quick example:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个快速的例子：
- en: '[PRE0]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'By having neighbors share their feature values, this simple convolution function
    performs selective information sharing: it determines what is shared (features)
    and by whom (neighbors). A neural network that uses this convolution function
    will tend to evolve according to these maxims:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通过让邻居分享其特征值，这个简单的卷积函数执行选择性信息共享：它决定了什么是被分享的（特征）以及由谁来分享（邻居）。使用这种卷积函数的神经网络往往会按照以下准则演变：
- en: Vertices that share many neighbors will tend to be similar.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享许多邻居的顶点往往会相似。
- en: Vertices that share the same initial feature values will tend to be similar.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享相同初始特征值的顶点往往会相似。
- en: These properties are reminiscent of random-walk graph embeddings.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性让人联想到随机游走图嵌入。
- en: How do we take this convolution and integrate it into a neural network? Take
    a look at [Figure 10-17](#two_layer_graph_convolutional_network).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将这种卷积操作集成到神经网络中？请看[图10-17](#two_layer_graph_convolutional_network)。
- en: '![Image](assets/gpam_1017.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![Image](assets/gpam_1017.png)'
- en: Figure 10-17\. Two-layer graph convolutional network
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-17. 两层图卷积网络
- en: This two-layer network flows from left to right. The input is the feature vectors
    for all of the graph’s vertices. If the feature vectors run horizontally and we
    stack the vertices vertically, then we get an *n* × *f* matrix, where *f* is the
    number of features. Next, we apply the adjacency-based convolution. We then apply
    a set of randomized weights (similar to what we did with random-walk graph embedding
    networks) to merge and reduce the features to an embedding of size h1\. Typically
    h1 < *f*. Before storing the values in the embedding matrix, we apply an activation
    function (indicated by the blocky “S” in a circle), which acts as a filter/amplifier.
    Low values are pushed lower, and high values are pushed higher. Activation functions
    are used in most neural networks.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个两层网络从左向右流动。输入是图中所有顶点的特征向量。如果特征向量水平排列，顶点垂直堆叠，则得到一个*n* × *f*矩阵，其中*f*是特征数。接下来，我们应用基于邻接矩阵的卷积。然后我们应用一组随机化权重（类似于随机游走图嵌入网络中所做的），将特征合并并减少到大小为*h1*的嵌入。通常*h1
    < *f*。在将值存储到嵌入矩阵之前，我们应用一个激活函数（圆圈中的方块“S”表示），它作为滤波器/放大器。低值被推低，高值被推高。激活函数在大多数神经网络中都有使用。
- en: Because this is a two-layer network, we repeat the same steps. The only differences
    are that this embedding may have a different size, where typically h2 ≤ h1, and
    this weight mesh has a different set of random weights. If this is the final layer,
    then it’s considered the output layer with the output results. By having two layers,
    the output embedding for each vertex takes into account the neighbors within two
    hops. You can add more layers to consider deeper neighbors. Two or three layers
    often provide the best results. With too many layers, the radius of each vertex’s
    neighborhood becomes so large that it overlaps significantly even with the neighborhoods
    of unrelated vertices.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个两层网络，我们重复相同的步骤。唯一的区别在于这个嵌入可能具有不同的大小，通常*h2 ≤ h1*，以及这个权重网格具有不同的随机权重集。如果这是最后一层，那么它被视为输出层，并输出结果。通过有两层，每个顶点的输出嵌入考虑了两个跳之内的邻居。您可以添加更多层来考虑更深的邻居。通常两到三层提供最佳结果。层数过多时，每个顶点邻域的半径变得如此大，以至于即使与不相关顶点的邻域也有显著的重叠。
- en: Our example here is demonstrating how a GCN can be used in unsupervised learning
    mode. No training data or target function was provided; we just merged features
    of vertices with their neighbors. Surprisingly, you can get useful results from
    an unsupervised, untrained GCN. The authors of GCN experimented with a three-layer
    untrained GCN, using the well-known Karate Club dataset. They set the output layer’s
    embedding length of two, so that they could interpret the two values as coordinate
    points. When plotted, the output data points showed community clustering that
    matched the known communities in Zachary’s Karate Club.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的例子展示了如何在无监督学习模式下使用GCN。我们没有提供训练数据或目标函数；我们只是将顶点的特征与其邻居的特征合并。令人惊讶的是，无监督、未经训练的GCN也能产生有用的结果。GCN的作者们尝试了一个三层未经训练的GCN，使用了著名的Karate
    Club数据集。他们将输出层的嵌入长度设置为二，这样他们可以将这两个值解释为坐标点。绘制时，输出数据点显示出与Zachary的Karate Club中已知社区匹配的社区聚类。
- en: The GCN architecture is general enough to be used for unsupervised, supervised,
    semisupervised, or even reinforcement learning. The only difference between a
    GCN and a vanilla feed-forward neural network is the addition of the step to aggregate
    the features of a vector with those of its neighbors. [Figure 10-18](#generic_model_for_responsive_learning_i)
    shows a generic model for how neural networks tune their weights. The graph convolution
    in GCN affects only the block labeled Forward Propagation Layers. All of the other
    parts (input values, target values, weight adjustment, etc.) are what determine
    what type of learning you are doing. That is, the type of learning is decided
    independently from your use of graph convolution.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: GCN 架构足够通用，可用于无监督、监督、半监督甚至强化学习。GCN 和普通前馈神经网络唯一的区别在于添加向量特征与邻居特征聚合的步骤。[图 10-18](#generic_model_for_responsive_learning_i)
    展示了神经网络如何调整权重的通用模型。GCN 中的图卷积仅影响标有前向传播层的块。所有其他部分（输入值、目标值、权重调整等）决定了你正在进行的学习类型。也就是说，学习类型独立于你使用图卷积决定。
- en: '![Image](assets/gpam_1018.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图像](assets/gpam_1018.png)'
- en: Figure 10-18\. Generic model for responsive learning in a neural network
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-18\. 神经网络中响应式学习的通用模型
- en: Attention neural networks use a more advanced form of feedback and adjustment.
    The details are beyond the scope of this book, but graph attention neural networks
    (GATs) can tune the weight (aka focus the attention) of each neighbor when adding
    them together for convolution. That is, a GAT performs a weighted sum instead
    of a simple sum of a neighbor’s features, and the GAT trains itself to learn the
    best weights. When applied to the same benchmark tests, GAT outperforms GCN slightly.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力神经网络使用更高级的反馈和调整形式。这本书的范围之外详细介绍，但图注意力神经网络（GATs）可以调整每个邻居的权重（即聚焦注意力）并将它们相加进行卷积。也就是说，GAT
    执行加权求和而不是邻居特征的简单求和，并且GAT会自我训练以学习最佳权重。当应用于相同的基准测试时，GAT 稍微优于GCN。
- en: GraphSAGE
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GraphSAGE
- en: One limitation of the basic GCN model is that it does a simple averaging of
    vertex plus neighbor features. It seems we would want some more control and tuning
    of this convolution. Also, large variations in the number of neighbors for different
    vertices may lead to training difficulties. To address this limitation, William
    Hamilton, Rex Ying, and Jure Leskovec presented GraphSAGE in 2017 in their paper
    “[Inductive Representation Learning on Large Graphs”](https://oreil.ly/YTlH9).
    Like GCN, this technique also combines information from neighbors, but it does
    it a little differently. To standardize the learning from neighbors, GraphSAGE
    samples a fixed number of neighbors from each vertex. [Figure 10-21](#block_diagram_of_graphsage)
    shows a block diagram of GraphSAGE, with sampled neighbors.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 基本GCN模型的一个限制是它仅对顶点加邻居特征做简单平均。我们似乎希望对这种卷积进行更多控制和调整。此外，不同顶点的邻居数量大幅变化可能导致训练困难。为了解决这一限制，William
    Hamilton、Rex Ying 和 Jure Leskovec 在他们的论文“[大规模图上的归纳表征学习](https://oreil.ly/YTlH9)”中于2017年提出了GraphSAGE。像GCN一样，这种技术也结合了邻居的信息，但做法有所不同。为了标准化邻居的学习，GraphSAGE
    从每个顶点中采样固定数量的邻居。[图 10-21](#block_diagram_of_graphsage) 展示了GraphSAGE的块图，包括采样的邻居。
- en: '![Image](assets/gpam_1021.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图像](assets/gpam_1021.png)'
- en: Figure 10-21\. Block diagram of GraphSAGE
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-21\. GraphSAGE 的块图
- en: With GraphSAGE, the features of the neighbors are combined according to a chosen
    aggregation function. The function could be addition, as in GCNs. Any order-independent
    aggregation function could be used; long short-term memory (LSTM) with random
    ordering and max-pooling work well. The source vertex is not included in the aggregation
    as it is in GCN; instead, the aggregated feature vectors and the source vertex’s
    feature vector are concatenated to make a double-length vector. We then apply
    a set of weights to mix the features together, apply an activation function, and
    store as the next layer’s representation of the vertices. This series of sets
    constitutes one layer in the neural network and the gathering of information within
    one hop of each vertex. A GraphSAGE network has k layers, each with its own set
    of weights. GraphSAGE proposes a loss function that rewards nearby vertices if
    they have similar embeddings and rewards distant vertices if they have dissimilar
    embeddings.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GraphSAGE 中，邻居的特征根据选择的聚合函数进行组合。这个函数可以是加法，就像 GCN 中一样。可以使用任何无序的聚合函数；长短期记忆（LSTM）在随机顺序和最大池化中工作良好。与
    GCN 不同，源顶点不包含在聚合中；相反，聚合特征向量和源顶点的特征向量被串联起来形成双倍长度的向量。然后我们应用一组权重来混合这些特征，应用激活函数，并存储为顶点下一层的表示。这一系列集合构成神经网络中的一层，以及每个顶点一跳内信息的收集。GraphSAGE
    网络有 k 层，每层都有自己的权重集合。GraphSAGE 提出了一个损失函数，如果附近的顶点具有相似的嵌入，就奖励它们，并且如果它们具有不相似的嵌入，则奖励远距离的顶点。
- en: Besides training on the full graph, as you would with GCN, you can train Graph
    SAGE with only a sample of the vertices and their neighborhoods. The fact that
    GraphSAGE’s aggregation functions use equal-sized samples of neighborhoods means
    that it doesn’t matter how you arrange the inputs. That freedom of arrangement
    is what allows you to train with one sample and then test or deploy with a different
    sample. Because it builds a model based on these generalized graph neighborhood
    properties, GraphSAGE performs *inductive learning*. That is, the model can be
    used to make predictions about new vertices that were not in the original dataset.
    In contrast, GCN directly uses the adjacency matrix, which forces it to use the
    full graph with the vertices arranged in a particular order. Training with the
    full data and learning a model for only that data is *transductive learning*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GCN 一样，在整个图上训练，你可以只用顶点及其邻域的一部分来训练 Graph SAGE。GraphSAGE 的聚合函数使用等大小的邻域样本，这意味着输入的排列方式并不重要。这种排列自由度使你可以使用一个样本进行训练，然后使用不同的样本进行测试或部署。因为它基于这些泛化的图邻域属性构建模型，GraphSAGE
    执行 *归纳学习*。也就是说，该模型可以用来预测原始数据集中不存在的新顶点。相比之下，GCN 直接使用邻接矩阵，这强制它使用特定顺序排列的完整图。使用完整数据进行训练，并仅针对该数据学习模型是
    *传导学习*。
- en: Whether or not learning from a sample will work on your particular graph depends
    on whether your graph’s structure and features follow global trends, such that
    a random subgraph looks similar to another subgraph of similar size. For example,
    one part of a forest may look a lot like another part of a forest. For a graph
    example, suppose you have a Customer 360 graph including all of a customer’s interactions
    with your sales team, website, and events, their purchases, and all other profile
    information you have been able to obtain. Last year’s customers are rated based
    on the total amount and frequency of their purchases. It is reasonable to expect
    that if you used GraphSAGE with last year’s graph to predict the customer rating,
    it should do a decent job of predicting the ratings of this year’s customers.
    [Table 10-2](#comparison_of_gcn_and_graphsage_traits) summarizes all the similarities
    and differences between GCN and GraphSAGE that we have presented.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的特定图中，从样本学习是否有效取决于你的图的结构和特征是否遵循全局趋势，这样一个随机子图看起来就像另一个大小相似的子图。例如，森林的一部分可能看起来很像另一部分森林。举个图的例子，假设你有一个包含客户与销售团队、网站和活动的所有互动、他们的购买以及你能获取的所有其他个人资料信息的客户360图。去年的客户根据其购买总金额和频率进行评级。合理地预期，如果你使用
    GraphSAGE 和去年的图来预测客户评级，它应该能够很好地预测今年客户的评级。[Table 10-2](#comparison_of_gcn_and_graphsage_traits)
    总结了我们提出的 GCN 和 GraphSAGE 之间所有的相似性和差异。
- en: Table 10-2\. Comparison of GCN and GraphSAGE traits
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Table 10-2\. GCN 和 GraphSAGE 特性比较
- en: '|   | GCN | GraphSAGE |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|   | GCN | GraphSAGE |'
- en: '| --- | --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Neighbors for aggregation** | All | Sample of *n* neighbors |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| **聚合邻居** | 所有 | *n* 个邻居样本'
- en: '| **Aggregation function** | Mean | Several options |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| **聚合函数** | 平均 | 几个选项 |'
- en: '| **Aggregating a vertex with neighbors?** | Aggregated with others | Concatenated
    to others |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| **聚合顶点与邻居？** | 与其他聚合 | 与其他连接 |'
- en: '| **Do weights need to be learned?** | Not for unsupervised transductive model
    | Yes, for inductive model |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| **需要学习权重吗？** | 对于无监督的转导模型不需要 | 对于归纳模型需要 |'
- en: '| **Supervised?** | Yes | Yes |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| **监督？** | 是 | 是 |'
- en: '| **Self-supervised?** | With modification | With modification |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| **自监督？** | 经过修改 | 经过修改 |'
- en: '| **Can be trained on a sample of vertices** | No | Yes |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| **可以对顶点样本进行训练吗？** | 否 | 是 |'
- en: 'Graph-based neural networks put graphs into the mainstream of machine learning.
    Key takeaways from this section are as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的神经网络使图在机器学习中变得主流。本节的关键收获如下：
- en: The graph convolutional neural network enhances the vanilla neural network by
    averaging together the feature vectors of each vertex’s neighbors with its own
    features during the learning process.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图卷积神经网络在学习过程中通过平均每个顶点的邻居的特征向量与自己的特征进行增强，从而提升了基本神经网络的能力。
- en: 'GraphSAGE makes two key improvements to the basic GCN: vertex and neighborhood
    sampling, and keeping features of vectors separate from those of its neighbors.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GraphSAGE对基本的GCN进行了两个关键改进：顶点和邻域采样，并在学习过程中保持向量特征与其邻居的特征分离。
- en: GCN learns transductively (uses the full data to learn only about that data),
    whereas GraphSAGE learns inductively (uses a data sample to learn a model that
    can apply to other data samples).
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCN以转导方式学习（仅使用全数据学习该数据的信息），而GraphSAGE以归纳方式学习（使用数据样本学习可以应用于其他数据样本的模型）。
- en: The modular nature of neural networks and graph enhancements means that the
    ideas of GCN and GraphSAGE can be transferred to many other flavors of neural
    networks.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络和图增强的模块化特性意味着GCN和GraphSAGE的思想可以转移到许多其他类型的神经网络中。
- en: Comparing Graph Machine Learning Approaches
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较图机器学习方法
- en: This chapter has covered many different ways to learn from graph data, but it
    only scratched the surface. Our goal was not to present an exhaustive survey but
    to provide a framework from which to continue to grow. We’ve outlined the major
    categories of and techniques for graph-powered machine learning, we’ve described
    what characterizes and distinguishes them, and we’ve provided simple examples
    to illustrate how they operate. It’s worthwhile to briefly review these techniques.
    Our goal here is not only to summarize but also to provide you with guidance for
    selecting the right techniques to help you learn from your connected data.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了许多从图数据中学习的不同方法，但只是皮毛。我们的目标不是提供详尽的调查，而是提供一个框架，供您继续成长。我们概述了图驱动机器学习的主要类别和技术，描述了它们的特征和区别，并提供了简单的示例来说明它们的运作方式。简要回顾这些技术是值得的。我们的目标不仅是总结，还包括为您选择正确的技术提供指导，帮助您从连接的数据中学习。
- en: Use Cases for Machine Learning Tasks
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习任务的用例
- en: '[Table 10-3](#use_cases_for_graph_data_learning_tasks) pulls together examples
    of use cases for each of the major learning tasks. These are the same basic data
    mining and machine learning tasks you might perform on any data, but the examples
    are particularly relevant for graph data.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10-3](#use_cases_for_graph_data_learning_tasks)汇集了每个主要学习任务的用例示例。这些是您可能在任何数据上执行的基本数据挖掘和机器学习任务，但这些示例对图数据特别相关。'
- en: Table 10-3\. Use cases for graph data learning tasks
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-3. 图数据学习任务的用例
- en: '| Task | Use case examples |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 用例示例 |'
- en: '| --- | --- |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Community detection** | Delineating social networks |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **社区检测** | 描绘社交网络 |'
- en: '| Finding a financial crime network |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 发现一个金融犯罪网络 |'
- en: '| Detecting a biological ecosystem or chemical reaction network |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 发现生物生态系统或化学反应网络 |'
- en: '| Discovering a network of unexpectedly interdependent components or processes,
    such as software procedures or legal regulations |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 发现意外依赖组件或过程的网络，例如软件流程或法律法规 |'
- en: '| **Similarity** | Abstraction of physical closeness, inverse of distance |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **相似性** | 物理接近的抽象，距离的倒数 |'
- en: '| Prerequisite for clustering, classification, and link prediction |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 聚类、分类和链接预测的先决条件 |'
- en: '| Entity resolution: finding two online identities that probably refer to the
    same real-world person |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 实体解析：找到两个在线身份，可能指的是同一个现实世界的人 |'
- en: '| Product recommendation or suggested action |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 产品推荐或建议行动 |'
- en: '| Identifying persons who perform the same role in different but analogous
    networks |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 识别在不同但类似网络中执行相同角色的人员 |'
- en: '| **Find unknown patterns** | Identifying the most common “customer journeys”
    on your website or in your app |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| **发现未知模式** | 识别你的网站或应用中最常见的“客户旅程” |'
- en: '| Once current patterns are identified, then noticing changes |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 一旦识别出当前的模式，就开始注意变化 |'
- en: '| **Link prediction** | Predicting someone’s future purchase or willingness
    to purchase |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| **链接预测** | 预测某人未来的购买或购买意愿 |'
- en: '| Predicting that a business or personal relationship exists, even though it
    is not recorded in the data |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 预测企业或个人关系的存在，即使这些关系未记录在数据中 |'
- en: '| **Feature extraction** | Enriching your customer data with graph features,
    so that your machine learning training to categorize and model your customers
    will be more successful |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| **特征提取** | 通过图特征丰富你的客户数据，从而使得你的机器学习训练在分类和建模客户方面更加成功 |'
- en: '| **Embedding** | Transforming a large set of features to a more compact set,
    for more efficient computation |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| **嵌入** | 将大量特征转换为更紧凑的集合，以进行更高效的计算 |'
- en: '| Holistically capturing a neighbor’s signature without designing specific
    feature extraction queries |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 在没有设计特定特征提取查询的情况下，全面捕获邻居的特征签名 |'
- en: '| **Classification (predicting a category)** | Given some past examples of
    fraud, creating a model for identifying new cases of fraud |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| **分类（预测类别）** | 根据过去欺诈案例创建用于识别新欺诈案例的模型 |'
- en: '| Predicting categorical outcomes for future vaccine patients, based on test
    results of past patients |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 预测未来疫苗接种者的分类结果，基于过去患者的测试结果 |'
- en: '| **Regression (predicting a numerical value)** | Predicting weight loss for
    diet program participants, based on results of past participants |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| **回归（预测数值）** | 基于过去参与者的结果预测减重效果 |'
- en: Once you have identified what type of task you want to perform, consider the
    available graph-based learning techniques, what they provide, and their key strengths
    and differences.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定您想要执行的任务类型，请考虑可用的基于图的学习技术，它们提供的内容以及它们的主要优势和差异。
- en: Pattern Discovery and Feature Extraction Methods
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式发现和特征提取方法
- en: '[Table 10-4](#pattern_discovery_and_feature_extractio) lists the graph algorithms
    and feature extraction methods we encountered both in this chapter and in [Chapter 6](ch06.html#analyzing_connections_for_deeper_insigh).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-4](#pattern_discovery_and_feature_extractio) 列出了我们在本章和[第六章](ch06.html#analyzing_connections_for_deeper_insigh)中遇到的图算法和特征提取方法。'
- en: Table 10-4\. Pattern discovery and feature extraction methods in graphs
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 10-4\. 图中的模式发现和特征提取方法
- en: '| Task | Graph-based learning methods | Comments |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 基于图的学习方法 | 评论 |'
- en: '| --- | --- | --- |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Community detection** | Connected components | One connection to the community
    is enough |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| **社区检测** | 连通分量 | 一个连接到社区就足够了 |'
- en: '| k-core | At least k connections to other community members |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| k-核 | 至少与其他社区成员有 k 个连接 |'
- en: '| Modularity optimization (e.g., Louvain) | Relatively higher density of connections
    inside than between communities |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 模块性优化（例如，Louvain） | 社区内连接的密度相对较高 |'
- en: '| **Similarity** | Jaccard neighborhood similarity | Counts how many relationships
    in common, for nonnumerical data |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| **相似性** | Jaccard邻域相似性 | 计算共同关系的数量，适用于非数值数据 |'
- en: '| Cosine neighborhood similarity | Compares numeric or weighted vectors of
    relationships |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 余弦邻域相似性 | 比较数值或加权关系向量 |'
- en: '| Role similarity | Defines similarity recursively as having similar neighbors
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 角色相似性 | 递归地定义为具有相似邻居 |'
- en: '| **Find unknown patterns** | Frequent pattern mining | Starts with small patterns
    and builds to larger patterns |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| **发现未知模式** | 频繁模式挖掘 | 从小模式开始构建到大模式 |'
- en: '| **Domain-independent feature extraction** | Graphlets | Systematic list of
    all possible neighborhood configurations |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| **领域无关特征提取** | 图子结构 | 所有可能的邻域配置的系统列表 |'
- en: '| PageRank | Rank is based on the number and rank of in-neighbors, for directed
    graphs among vertices of the same type |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 页面排名 | 排名基于同类型顶点之间的入度和排名 |'
- en: '| Closeness centrality | Closeness = average distance to any other vertex |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 接近中心性 | 接近度 = 到任何其他顶点的平均距离 |'
- en: '| Betweenness centrality | How often a vertex lies on the shortest path between
    any two vertices; slow to compute |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 中介中心性 | 顶点在任意两个顶点之间的最短路径上出现的频率；计算速度较慢 |'
- en: '| **Domain-dependent feature extraction** | Search for patterns relevant to
    your domain | Custom effort by someone with domain knowledge |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| **领域相关特征提取** | 搜索与您领域相关的模式 | 需要具有领域知识的人的定制努力 |'
- en: '| **Dimensionality reduction and embedding** | DeepWalk | Embeddings will be
    similar if the vectors have similar random walks, considering nearness and role;
    more efficient than SimRank |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| **降维和嵌入** | DeepWalk | 如果向量具有类似的随机游走，则嵌入将类似，考虑接近性和角色；比SimRank更高效 |'
- en: '| node2vec | DeepWalk with directional tuning of the random walks, for greater
    tuning |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| node2vec | 具有随机游走方向调整的DeepWalk，用于更高的调整 |'
- en: 'Graph Neural Networks: Summary and Uses'
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图神经网络：总结和用途
- en: The graph neural networks presented in this chapter not only are directly useful
    in many cases but are also templates to show more advanced data scientists how
    to transform any neural network technique to include graph connectivity in the
    training. The key is the convolution step, which takes the features of neighboring
    vertices into account. All of the GNN approaches presented can be used for either
    unsupervised or supervised learning. [Table 10-5](#summary_of_three_types_of_graph_neural)
    compares graph neural network methods.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的图神经网络不仅在许多情况下直接有用，而且还是向更高级数据科学家展示如何在训练中包含图连接的模板。关键在于卷积步骤，该步骤考虑了相邻顶点的特征。所展示的所有GNN方法都可以用于无监督或监督学习。[表 10-5](#summary_of_three_types_of_graph_neural)比较了图神经网络方法。
- en: Table 10-5\. Summary of three types of graph neural networks
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-5\. 三种图神经网络类型的总结
- en: '| Name | Description | Uses |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 描述 | 用途 |'
- en: '| --- | --- | --- |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Graph convolutional network (GCN) | Convolution: average of neighbor’s features
    | Clustering or classification on a particular graph |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 图卷积网络（GCN） | 卷积：邻居特征的平均 | 特定图上的聚类或分类 |'
- en: '| GraphSAGE | Convolution: average of a sample of neighbor’s features | Learning
    a representative model on a sample of a graph, in addition to clustering or classification
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| GraphSAGE | 卷积：邻居特征样本的平均 | 在样本图上学习代表性模型，除了聚类或分类 |'
- en: '| Graph attention neural network (GAT) | Convolution: weighted average of neighbor’s
    features | Clustering, classification, and model learning; added tuning and complexity
    by learning weights for the convolution |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 图注意力神经网络（GAT） | 卷积：邻居特征的加权平均 | 聚类、分类和模型学习；通过学习卷积权重增加调整和复杂性 |'
- en: Chapter Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节总结
- en: 'Graphs and graph-based algorithms contribute to several stages of the machine
    learning pipeline: data acquisition, data exploration and pattern discovery, data
    preparation, feature extraction, dimensionality reduction, and model training.
    As data scientists know, there is no golden ticket, no single technique that solves
    all their problems. Instead, you work to acquire tools for your toolkit, develop
    the skills to use your tools well, and gain an understanding about when to use
    them.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图及基于图的算法对机器学习流程的几个阶段有贡献：数据获取、数据探索和模式发现、数据准备、特征提取、降维和模型训练。正如数据科学家所知，没有金票，没有一种单一的技术可以解决所有问题。相反，你努力获取工具箱中的工具，发展使用这些工具的技能，并了解何时使用它们。
- en: '^([1](ch10.html#ch01fn36-marker)) Graphlets were first presented in Nataša
    Pržulj, Derek G. Corneil, and Igor Jurisi, “Modeling Interactome: Scale-Free or
    Geometric?” *Bioinformatics* 20, no. 18 (December 2004): 3508–3515, [*https://doi.org/10.1093/bioinformatics/bth436*](https://doi.org/10.1093/bioinformatics/bth436).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch10.html#ch01fn36-marker)) Graphlets最早由Nataša Pržulj, Derek G. Corneil,
    和Igor Jurisi在“建模相互作用组：无标度还是几何？”*生物信息学* 20, no. 18 (2004年12月): 3508–3515中提出, [*https://doi.org/10.1093/bioinformatics/bth436*](https://doi.org/10.1093/bioinformatics/bth436).'
- en: ^([2](ch10.html#ch01fn37-marker)) Anida Sarajlić, Noël Malod-Dognin, Ömer Nebil
    Yaveroğlu, and Nataša Pržulj, “Graphlet-Based Characterization of Directed Networks,”
    *Scientific Reports* 6 (2016), [*https://www.nature.com/articles/srep35098*](https://www.nature.com/articles/srep35098).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#ch01fn37-marker)) Anida Sarajlić, Noël Malod-Dognin, Ömer Nebil
    Yaveroğlu, and Nataša Pržulj，“基于图元的有向网络特征化”，*科学报告* 6 (2016), [*https://www.nature.com/articles/srep35098*](https://www.nature.com/articles/srep35098).
- en: '^([3](ch10.html#ch01fn38-marker)) Mahmudur Rahman and Mohammad Al Hasan, “Link
    Prediction in Dynamic Networks using Graphlet,” in *Machine Learning and Knowledge
    Discovery in Databases*, Proceedings, Part I, ed. Paolo Frasconi, Niels Landwehr,
    Giuseppe Manco, Jilles Vreeken (Riva del Garda, Italy: European Conference, ECML
    PKDD: 2016), 394–409.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.html#ch01fn38-marker)) Mahmudur Rahman 和 Mohammad Al Hasan，“使用图小结进行动态网络中的链接预测”，收录于《数据库中的机器学习与知识发现》第一部分，由
    Paolo Frasconi、Niels Landwehr、Giuseppe Manco 和 Jilles Vreeken 编辑（意大利瑞瓦德尔加达：欧洲会议
    ECML PKDD，2016 年），394–409 页。
- en: ^([4](ch10.html#ch01fn39-marker)) Tijana Milenković and Nataša Pržulj, “Uncovering
    Biological Network Function via Graphlet Degree Signatures,” *Cancer Informatics*
    6, no. 10 (April 2008), [*https://www.researchgate.net/publication/26510215_Przulj_N_Uncovering_Biological_Network_Function_via_Graphlet_Degree_Signatures*](https://www.researchgate.net/publication/26510215_Przulj_N_Uncovering_Biological_Network_Function_via_Graphlet_Degree_Signatures).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch10.html#ch01fn39-marker)) Tijana Milenković 和 Nataša Pržulj，“通过图小结度量揭示生物网络功能”，*癌症信息学*
    6 卷，第 10 期（2008 年 4 月），[*https://www.researchgate.net/publication/26510215_Przulj_N_Uncovering_Biological_Network_Function_via_Graphlet_Degree_Signatures*](https://www.researchgate.net/publication/26510215_Przulj_N_Uncovering_Biological_Network_Function_via_Graphlet_Degree_Signatures)。
- en: '^([5](ch10.html#ch01fn40-marker)) These algorithms were introduced in Part
    2: Analyze.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch10.html#ch01fn40-marker)) 这些算法首次出现在第二部分：“分析”。
- en: ^([6](ch10.html#ch01fn41-marker)) “[NeurIPS 2020] Data Science for COVID-19
    (DS4C),” Kaggle, accessed May 25, 2023, [*https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset*](https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch10.html#ch01fn41-marker)) “[NeurIPS 2020] COVID-19 的数据科学 (DS4C)，” Kaggle，访问于
    2023 年 5 月 25 日，[*https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset*](https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset)。
- en: ^([7](ch10.html#ch01fn42-marker)) From John P. Snyder and Philip M. Voxland,
    “An Album of Map Projections,” second printing (US Geological Survey Professional
    Paper 1453, 1994), [*https://pubs.usgs.gov/pp/1453/report.pdf*](https://pubs.usgs.gov/pp/1453/report.pdf).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch10.html#ch01fn42-marker)) 来自 John P. Snyder 和 Philip M. Voxland 的《地图投影集锦》，第二版（美国地质调查专业论文
    1453 号，1994 年），[*https://pubs.usgs.gov/pp/1453/report.pdf*](https://pubs.usgs.gov/pp/1453/report.pdf)。
- en: ^([8](ch10.html#ch01fn43-marker)) This visualized partitioning comes from “Zachary’s
    Karate Club,” Wikipedia, April 5, 2017, [*https://en.wikipedia.org/wiki/File:Zachary%27s_karate_club.png*](https://en.wikipedia.org/wiki/File:Zachary%27s_karate_club.png).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch10.html#ch01fn43-marker)) 这种可视化的分区来自于“Zachary's Karate Club”，维基百科，2017
    年 4 月 5 日，[*https://en.wikipedia.org/wiki/File:Zachary%27s_karate_club.png*](https://en.wikipedia.org/wiki/File:Zachary%27s_karate_club.png)。
- en: ^([9](ch10.html#ch10newfn1-marker)) See the article by Perozzi, Al-Rfou, and
    Skiena [*https://dl.acm.org/doi/abs/10.1145/2623330.2623732*](https://dl.acm.org/doi/abs/10.1145/2623330.2623732).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch10.html#ch10newfn1-marker)) 参见 Perozzi、Al-Rfou 和 Skiena 的文章 [*https://dl.acm.org/doi/abs/10.1145/2623330.2623732*](https://dl.acm.org/doi/abs/10.1145/2623330.2623732)。
- en: ^([10](ch10.html#ch10newfn2-marker)) See the article by Mikolov, Sutskever,
    Chen, Corrado, and Dean [*https://arxiv.org/abs/1310.4546*](https://arxiv.org/abs/1310.4546).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch10.html#ch10newfn2-marker)) 参见 Mikolov、Sutskever、Chen、Corrado 和 Dean
    的文章 [*https://arxiv.org/abs/1310.4546*](https://arxiv.org/abs/1310.4546)。
