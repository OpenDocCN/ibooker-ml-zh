- en: Chapter 1\. From Product Goal to ML Framing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章\. 从产品目标到 ML 框架
- en: ML allows machines to learn from data and behave in a probabilistic way to solve
    problems by optimizing for a given objective. This stands in opposition to traditional
    programming, in which a programmer writes step-by-step instructions describing
    *how* to solve a problem. This makes ML particularly useful to *build systems
    for which we are unable to define a heuristic solution*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习允许机器从数据中学习，并以概率方式行事，以解决优化特定目标的问题。这与传统编程相对立，传统编程中程序员会编写逐步说明如何解决问题。这使得机器学习特别适合于*构建我们无法定义启发式解决方案的系统*。
- en: '[Figure 1-1](#algorithm_vs_learning) describes two ways to write a system to
    detect cats. On the left, a program consists of a procedure that has been manually
    written out. On the right, an ML approach leverages a dataset of photos of cats
    and dogs labeled with the corresponding animal to allow a model to learn the mapping
    from image to category. In the ML approach, there is no specification of how the
    result should be achieved, only a set of example inputs and outputs.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-1](#algorithm_vs_learning) 描述了编写检测猫的系统的两种方法。在左侧，一个程序由手动编写的过程组成。在右侧，机器学习方法利用带有相应动物标签的猫和狗的照片数据集，允许模型学习从图像到类别的映射。在机器学习方法中，没有规定结果应该如何实现，只有一组示例输入和输出。'
- en: '![Comparing writing an algorithm to learning a model](assets/bmla_0101.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![比较编写算法和学习模型](assets/bmla_0101.png)'
- en: Figure 1-1\. From defining procedures to showing examples
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1\. 从定义过程到展示示例
- en: ML is powerful and can unlock entirely new products, but since it is based on
    pattern recognition, it introduces a level of uncertainty. It is important to
    identify which parts of a product would benefit from ML and how to frame a learning
    goal in a way that minimizes the risks of users having a poor experience.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）非常强大，可以解锁全新的产品，但由于它基于模式识别，所以引入了一定程度的不确定性。重要的是要确定产品的哪些部分会从 ML 中受益，并且如何框定学习目标，以最小化用户体验差的风险。
- en: For example, it is close to impossible (and extremely time-consuming to attempt)
    for humans to write step-by-step instructions to automatically detect which animal
    is in an image based on pixel values. By feeding thousands of images of different
    animals to a convolutional neural network (CNN), however, we can build a model
    that performs this classification more accurately than a human. This makes it
    an attractive task to tackle with ML.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于人类来说，根据像素值自动检测图像中的动物并写出逐步说明几乎是不可能的（并且极其耗时）。然而，通过向卷积神经网络（CNN）输入成千上万张不同动物的图像，我们可以构建一个比人类更准确地进行分类的模型。这使得用机器学习解决这种问题变得很有吸引力。
- en: On the other side, an application that calculates your taxes automatically should
    rely on guidelines provided by the government. As you may have heard, having errors
    on your tax return is generally frowned upon. This makes the use of ML for automatically
    generating tax returns a dubious proposition.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，自动计算您的税款的应用程序应依赖政府提供的指南。正如您可能已经听说的那样，税务申报错误通常是不受欢迎的。这使得使用机器学习自动生成税务申报存在一定的风险。
- en: You never want to use ML when you can solve your problem with a manageable set
    of deterministic rules. By manageable, I mean a set of rules that you could confidently
    write and that would not be too complex to maintain.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当您可以通过一组可管理的确定性规则解决问题时，您永远不希望使用机器学习。在这里，可管理意味着您可以自信地编写这些规则，并且不会因维护过于复杂而困扰。
- en: So while ML opens up a world of different applications, it is important to think
    about which tasks *can* and *should* be solved by ML. When building products,
    you should start from a concrete business problem, determine whether it requires
    ML, and then work on finding the ML approach that will allow you to iterate as
    rapidly as possible.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然机器学习为不同的应用领域开辟了新世界，但重要的是要考虑哪些任务*可以*和*应该*通过机器学习来解决。在构建产品时，您应该从一个具体的业务问题出发，确定是否需要机器学习，然后努力找到一种机器学习方法，使您能够尽快迭代。
- en: 'We will cover this process in this chapter, starting with methods to estimate
    what tasks are able to be solved by ML, which ML approaches are appropriate for
    which product goals, and how to approach data requirements. I will illustrate
    these methods with the ML Editor case study that we mentioned in [“Our Case Study:
    ML–Assisted Writing”](preface01.html#cs_ml_asstd_writing), and an interview with
    Monica Rogati.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中详细介绍这个过程，首先介绍估算哪些任务可以通过机器学习解决，哪些机器学习方法适合哪些产品目标，以及如何处理数据需求。我将通过我们在[“我们的案例研究：ML辅助写作”](preface01.html#cs_ml_asstd_writing)中提到的ML编辑器案例研究以及与Monica
    Rogati的一次采访来说明这些方法。
- en: Estimate What Is Possible
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估可能性
- en: Since ML models can tackle tasks without humans needing to give them step-by-step
    instructions, that means they are able to perform some tasks better than human
    experts (such as detecting tumors from radiology images or playing Go) and some
    that are entirely inaccessible to humans (such as recommending articles out of
    a pool of millions or changing the voice of a speaker to sound like someone else).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习模型能够处理任务而无需人类提供逐步指导，这意味着它们能够比人类专家更好地执行一些任务（例如从放射影像中检测肿瘤或打围棋），以及一些对人类完全无法接触的任务（例如从数百万篇文章中推荐文章或更改说话者的声音以听起来像其他人）。
- en: The ability of ML to learn directly from data makes it useful in a broad range
    of applications but makes it harder for humans to accurately distinguish which
    problems are solvable by ML. For each successful result published in a research
    paper or a corporate blog, there are hundreds of reasonable-sounding ideas that
    have entirely failed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习能够直接从数据中学习使其在广泛的应用领域中非常有用，但这也使人们更难准确地区分哪些问题可以通过机器学习来解决。在每篇发表在研究论文或公司博客上的成功结果背后，都有数百个听起来合理但却彻底失败的想法。
- en: While there is currently no surefire way to predict ML success, there are guidelines
    that can help you reduce the risk associated with tackling an ML project. Most
    importantly, you should always start with a product goal to then decide how best
    to solve it. At this stage, be open to any approach whether it requires ML or
    not. When considering ML approaches, make sure to evaluate those approaches based
    on how appropriate they are for the product, not simply on how interesting the
    methods are in a vacuum.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然目前没有确切的方法可以预测机器学习的成功，但有一些指导原则可以帮助您减少处理机器学习项目所涉及的风险。最重要的是，您应始终以产品目标为出发点，然后决定如何最好地解决它。在此阶段，要对任何方法持开放态度，无论是需要机器学习还是不需要。在考虑机器学习方法时，务必根据它们对产品的适用性来评估这些方法，而不仅仅是考虑方法本身是否有趣。
- en: 'The best way to do this is by following two successive steps: (1) framing your
    product goal in an ML paradigm, and (2) evaluating the feasibility of that ML
    task. Depending on your evaluation, you can readjust your framing until we are
    satisfied. Let’s explore what these steps really mean.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳方法是通过以下两个连续步骤来完成：(1) 将您的产品目标构建成一个机器学习范式，以及 (2) 评估该机器学习任务的可行性。根据您的评估结果，您可以重新调整您的构建，直到我们满意为止。让我们探讨一下这些步骤的实际含义。
- en: '*Framing a product goal in an ML paradigm:* When we build a product, we start
    by thinking of what service we want to deliver to users. As we mentioned in the
    introduction, we’ll illustrate concepts in this book using the case study of an
    editor that helps users write better questions. The goal of this product is clear:
    we want users to receive actionable and useful advice on the content they write.
    ML problems, however, are framed in an entirely different way. An ML problem concerns
    itself with *learning a function from data*. An example is learning to take in
    a sentence in one language and output it in another. For one product goal, there
    are usually many different ML formulations, with varying levels of implementation
    difficulty.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在机器学习范式中构建产品目标：* 当我们构建一个产品时，我们首先考虑要向用户提供什么样的服务。正如我们在介绍中提到的那样，我们将通过一个案例研究来说明本书中的概念，即一款帮助用户更好地提出问题的编辑器。这个产品的目标很明确：我们希望用户在他们写作的内容上获得可操作和有用的建议。然而，机器学习问题的构建方式完全不同。一个机器学习问题涉及*从数据中学习函数*。一个例子是学习将一种语言的句子输入并输出为另一种语言的句子。对于一个产品目标，通常会有许多不同的机器学习表述，其实施难度各不相同。'
- en: '*Evaluating ML feasibility:* All ML problems are not created equal! As our
    understanding of ML has evolved, problems such as building a model to correctly
    classify photos of cats and dogs have become solvable in a matter of hours, while
    others, such as creating a system capable of carrying out a conversation, remain
    open research problems. To efficiently build ML applications, it is important
    to consider multiple potential ML framings and start with the ones we judge as
    the simplest. One of the best ways to evaluate the difficulty of an ML problem
    is by looking at both the kind of data it requires and at the existing models
    that could leverage said data.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*评估机器学习可行性：* 所有机器学习问题并非一视同仁！随着我们对机器学习的理解逐步深入，例如正确分类猫和狗的照片的模型构建问题已经能够在几小时内解决，而创建能够进行对话的系统等问题则仍然是开放性研究问题。要高效地构建机器学习应用程序，重要的是考虑多种潜在的机器学习框架，并从我们认为最简单的开始。评估机器学习问题难度的最佳方法之一是查看它所需的数据类型以及可以利用该数据的现有模型。'
- en: 'To suggest different framings and evaluate their feasibility, we should examine
    two core aspects of an ML problem: data and models.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要提出不同的框架建议并评估其可行性，我们应该检查机器学习问题的两个核心方面：数据和模型。
- en: We will start with models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从模型开始。
- en: Models
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: There are many commonly used models in ML, and we will abstain from giving an
    overview of all of them here. Feel free to refer to the books listed in [“Additional
    Resources”](preface01.html#oth_books) for a more thorough overview. In addition
    to common models, many model variations, novel architectures, and optimization
    strategies are published on a weekly basis. In May 2019 alone, more than 13,000
    papers were submitted to [ArXiv](https://arxiv.org), a popular electronic archive
    of research where papers about new models are frequently submitted.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中有许多常用的模型，我们将在此不进行全部概述。您可以参考[“附加资源”](preface01.html#oth_books)中列出的书籍，以获取更全面的概述。除了常见模型外，每周都会发布许多模型变体、新颖架构和优化策略。仅在2019年5月，就有超过13,000篇论文被提交到[ArXiv](https://arxiv.org)，这是一个流行的电子研究存档，经常有关于新模型的论文被提交。
- en: It is useful, however, to share an overview of different categories of models
    and how they can be applied to different problems. To this end, I propose here
    a simple taxonomy of models based on how they approach a problem. You can use
    it as a guide for selecting an approach to tackle a particular ML problem. Because
    models and data are closely coupled in ML, you will notice some overlap between
    this section and [“Data types”](#data_types).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，分享不同类别模型的概述及其在不同问题中的应用是很有用的。为此，我在这里提出了一个简单的模型分类法，根据它们处理问题的方式。您可以将其作为选择解决特定机器学习问题方法的指南。由于模型和数据在机器学习中密切相关，您会注意到这一部分与[“数据类型”](#data_types)存在一些重叠。
- en: ML algorithms can be categorized based on whether they require labels. Here,
    a label refers to the presence in the data of an ideal output that a model should
    produce for a given example. Supervised algorithms leverage datasets that contain
    labels for inputs, and they aim to learn a mapping from inputs to labels. Unsupervised
    algorithms, on the other hand, do not require labels. Finally, weakly supervised
    algorithms leverage labels that aren’t exactly the desired output but that resemble
    it in some way.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可以根据是否需要标签进行分类。在这里，标签指的是数据中一个理想输出的存在，模型应该为给定示例产生该输出。监督算法利用包含输入标签的数据集，并旨在学习从输入到标签的映射。另一方面，无监督算法则不需要标签。最后，弱监督算法利用并非完全符合期望输出但在某种程度上类似的标签。
- en: Many product goals can be tackled by both supervised and unsupervised algorithms.
    A fraud detection system can be built by training a model to detect transactions
    that differ from the average one, requiring no labels. Such a system could also
    be built by manually labeling transactions as fraudulent or legitimate, and training
    a model to learn from said labels.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 许多产品目标可以通过监督和无监督算法来解决。例如，可以通过训练模型检测与平均交易不同的交易来构建欺诈检测系统，这不需要标签。此类系统也可以通过手动标记交易为欺诈或合法，并训练模型从这些标签中学习来构建。
- en: For most applications, supervised approaches are easier to validate since we
    have access to labels to assess the quality of a model’s prediction. This also
    makes it easier to train models since we have access to desired outputs. While
    creating a labeled dataset can sometimes be time-consuming initially, it makes
    it much easier to build and validate models. For this reason, this book will mostly
    cover supervised approaches.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数应用程序，监督方法更容易验证，因为我们可以访问标签来评估模型预测的质量。 这也使得训练模型变得更容易，因为我们可以访问所需的输出。 虽然创建带标签的数据集有时可能最初耗时，但这使得构建和验证模型变得更加容易。
    因此，本书大部分内容将涵盖监督方法。
- en: 'With that being said, determining which kind of inputs your model will take
    in and which outputs it will produce will help you narrow down potential approaches
    significantly. Based on these types, any of the following categories of ML approaches
    could be a good fit:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，确定您的模型将接受哪种类型的输入和产生哪些输出将有助于显著缩小可能的方法。 根据这些类型，以下任何ML方法类别都可能是一个很好的选择：
- en: Classification and regression
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类和回归
- en: Knowledge extraction
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识提取
- en: Catalog organization
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目录组织
- en: Generative models
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型
- en: I’ll expand on these further in the following section. As we explore these different
    modeling approaches, I recommend thinking about which kind of data you have available
    to you or could gather. Oftentimes, data availability ends up being the limiting
    factor in model selection.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在以下部分进一步展开。 当我们探索这些不同的建模方法时，我建议考虑您可以使用或可以收集的数据类型。 数据可用性通常最终成为模型选择的限制因素。
- en: Classification and regression
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类和回归
- en: Some projects are focused on effectively classifying data points between two
    or more categories or attributing them a value on a continuous scale (referred
    to as *regression* instead of *classification*). Regression and classification
    are technically different, but oftentimes methods to tackle them have significant
    overlap, so we lump them together here.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一些项目的重点是有效地在两个或多个类别之间分类数据点，或者在连续尺度上为它们赋予一个值（称为*回归*而不是*分类*）。 回归和分类在技术上是不同的，但通常处理它们的方法有显著的重叠，因此我们在这里将它们放在一起。
- en: One of the reasons classification and regression are similar is because most
    classification models output a probability score for a model to belong to a category.
    The classification aspect then boils down to deciding how to attribute an object
    to a category based on said scores. At a high level, a classification model can
    thus be seen as a regression on probability values.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 分类和回归之间相似的原因之一是大多数分类模型输出一个模型属于某一类别的概率分数。 然后，分类方面归结为根据这些分数决定如何将对象归因于类别。 从高层次来看，因此分类模型可以被看作是对概率值的回归。
- en: Commonly, we classify or score individual examples, such as spam filters that
    classify each email as valid or junk, fraud detection systems that classify users
    as fraudulent or legitimate, or computer vision radiology models that classify
    bones as fractured or healthy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们对单个示例进行分类或评分，例如将每封电子邮件分类为有效或垃圾的垃圾邮件过滤器，将用户分类为欺诈或合法的欺诈检测系统，或将骨骼分类为骨折或健康的计算机视觉放射学模型。
- en: In [Figure 1-2](#classification), you can see an example of classifying a sentence
    according to its sentiment, and the topic it covers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图1-2](#classification) 中，您可以看到一个根据其情感和主题对句子进行分类的示例。
- en: '![Classifying a sentence in multiple categories](assets/bmla_0102.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![将句子分类为多个类别](assets/bmla_0102.png)'
- en: Figure 1-2\. Classifying a sentence in multiple categories
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2\. 将句子分类为多个类别
- en: In regression projects, instead of attributing a class to each example, we give
    them a value. Predicting the sale price of a home based on attributes such as
    how many rooms it has and where it is located is an example of a regression problem.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归项目中，与为每个示例分配一个类别不同，我们给它们一个值。 根据诸如房间数量和位置等属性预测房屋销售价格是回归问题的一个示例。
- en: In some cases, we have access to a series of past data points (instead of one)
    to predict an event in the future. This type of data is often called a *time series*,
    and making predictions from a series of data points is referred to as *forecasting*.
    Time-series data could represent a patient’s medical history or a series of attendance
    measurements from national parks. These projects often benefit from models and
    features that can leverage this added temporal dimension.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可以访问一系列过去的数据点（而不是单个数据点）来预测未来的事件。这种类型的数据通常称为*时间序列*，从一系列数据点中进行预测被称为*预测*。时间序列数据可以代表患者的医疗历史或国家公园的一系列出勤测量。这些项目通常受益于可以利用这种增加的时间维度的模型和特征。
- en: In other cases, we attempt to detect unusual events from a dataset. This is
    called *anomaly detection*. When a classification problem is trying to detect
    events that represent a small minority of the data and thus are hard to detect
    accurately, a different set of methods is often required. Picking a needle out
    of a haystack is a good analogy here.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，我们尝试从数据集中检测异常事件。这被称为*异常检测*。当分类问题试图检测代表数据中小部分的事件时，因此很难准确检测到时，通常需要使用不同的方法集。这里用“从大数据集中找出一个小东西”来形容很贴切。
- en: Good classification and regression work most often requires significant feature
    selection and feature engineering work. Feature selection consists of identifying
    a subset of features that have the most predictive value. Feature generation is
    the task of identifying and generating good predictors of a target by modifying
    and combining existing features of a dataset. We will cover both of these topics
    in more depth in [Part III](part03.html#section_3).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的分类和回归工作通常需要进行重要的特征选择和特征工程工作。特征选择包括识别具有最高预测价值的特征子集。特征生成是通过修改和组合数据集的现有特征来识别和生成目标的良好预测器的任务。我们将在[第三部分](part03.html#section_3)更深入地讨论这两个主题。
- en: Recently, deep learning has shown a promising ability to automatically generate
    useful features from images, text, and audio. In the future, it may play a larger
    part in simplifying feature generation and selection, but for now, they remain
    integral parts of the ML workflow.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习表现出了自动从图像、文本和音频中生成有用特征的良好能力。未来，它可能在简化特征生成和选择方面发挥更大作用，但目前它们仍然是机器学习工作流的重要部分。
- en: Finally, we can often build on top of the classification or score described
    earlier to provide useful advice. This requires building an interpretable classification
    model and using its feature to generate actionable advice. More on this later!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通常可以在前述分类或评分的基础上提供有用的建议。这需要建立一个可解释的分类模型，并利用其特征生成可操作的建议。稍后会详细讨论这一点！
- en: Not all problems aim to attribute a set of categories or values to an example.
    In some cases, we’d like to operate at a more granular level and extract information
    from parts of an input, such as knowing where an object is in a picture, for example.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有问题都旨在为示例归因一组类别或值。在某些情况下，我们希望在更细粒度的级别上操作，并从输入的部分提取信息，比如知道图片中的物体位置。
- en: Knowledge extraction from unstructured data
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从非结构化数据中提取知识
- en: '*Structured data* is data that is stored in a tabular format. Database tables
    and Excel sheets are good examples of structured data. *Unstructured data* refers
    to datasets that are not in a tabular format. This includes text (from articles,
    reviews, Wikipedia, and so on), music, videos, and songs.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*结构化数据*是以表格格式存储的数据。数据库表和Excel表格是结构化数据的好例子。*非结构化数据*指的是不以表格格式存储的数据集。这包括文本（来自文章、评论、维基百科等）、音乐、视频和歌曲。'
- en: In [Figure 1-3](#structured_unstructured), you can see an example of structured
    data on the left and unstructured data on the right. Knowledge extraction models
    focus on taking a source of unstructured data and extracting structure out of
    it using ML.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图1-3](#structured_unstructured)中，您可以看到左侧是结构化数据的示例，右侧是非结构化数据的示例。知识提取模型专注于通过机器学习从非结构化数据源中提取结构。
- en: In the case of text, knowledge extraction can be used to add structure to reviews,
    for example. A model can be trained to extract aspects such as cleanliness, service
    quality, and price from reviews. Users could then easily access reviews that mention
    topics they are interested in.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本的情况下，知识提取可用于为评论添加结构。可以训练模型来从评论中提取例如清洁度、服务质量和价格等方面。用户随后可以轻松访问提及他们感兴趣主题的评论。
- en: '![Example types of structured and unstructured data](assets/bmla_0103.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![结构化和非结构化数据示例类型](assets/bmla_0103.png)'
- en: Figure 1-3\. Example types of structured and unstructured data
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 1-3\. 结构化和非结构化数据的示例类型
- en: In the medical domain, a knowledge extraction model could be built to take raw
    text from medical papers as input, and extract information such as the disease
    that is discussed in the paper, as well as the associated diagnosis and its performance.
    In [Figure 1-4](#extraction), a model takes a sentence as an input and extracts
    which words refer to a type of media and which words refer to the title of a media.
    Using such a model on comments in a fan forum, for example, would allow us to
    generate summaries of which movies frequently get discussed.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗领域，知识提取模型可以被建立为接受医学论文中的原始文本作为输入，并提取诸如论文讨论的疾病、相关的诊断及其性能等信息。在 [Figure 1-4](#extraction)
    中，一个模型以句子作为输入，并提取哪些词语指代媒体类型以及哪些词语指代媒体的标题。例如，在粉丝论坛的评论中使用这样的模型，我们可以生成关于经常被讨论的电影的摘要。
- en: '![Extracting information from a sentence](assets/bmla_0104.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![从句子中提取信息](assets/bmla_0104.png)'
- en: Figure 1-4\. Extracting media type and title from a sentence
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 1-4\. 从句子中提取媒体类型和标题
- en: 'For images, knowledge extraction tasks often consist of finding areas of interest
    in an image and categorizing them. Two common approaches are depicted in [Figure 1-5](#bounding_box):
    object detection is a coarser approach that consists of drawing rectangles (referred
    to as *bounding boxes*) around areas of interest, while segmentation precisely
    attributes each pixel of an image to a given category.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像，知识提取任务通常包括在图像中找到感兴趣的区域并对其进行分类。两种常见的方法如 [Figure 1-5](#bounding_box) 所示：对象检测是一种粗略的方法，它涉及在感兴趣的区域周围绘制矩形（称为*边界框*），而分割则精确地将图像的每个像素归属到特定的类别。
- en: '![Bounding boxes and segmentation masks](assets/bmla_0105.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![边界框和分割掩模](assets/bmla_0105.png)'
- en: Figure 1-5\. Bounding boxes and segmentation masks
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 1-5\. 边界框和分割掩模
- en: Sometimes, this extracted information can be used as an input to another model.
    An example is using a pose detection model to extract key points from a video
    of a yogi, and feeding those key points to a second model that classifies the
    pose as correct or not based on labeled data. [Figure 1-6](#cv_pipeline) shows
    an example of a series of two models that could do just this. The first model
    extracts structured information (the coordinates of joints) from unstructured
    data (a photo), and the second one takes these coordinates and classifies them
    as a yoga pose.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，这些提取的信息可以作为另一个模型的输入。例如，使用姿势检测模型从瑜伽视频中提取关键点，并将这些关键点馈送给第二个模型，根据标记数据对姿势进行分类。[Figure 1-6](#cv_pipeline)
    展示了这样一系列模型的示例。第一个模型从非结构化数据（照片）中提取结构化信息（关节坐标），第二个模型则接受这些坐标并将其分类为瑜伽姿势。
- en: '![Using multiple models in a sequence](assets/bmla_0106.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![在序列中使用多个模型](assets/bmla_0106.png)'
- en: Figure 1-6\. Yoga pose detection
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 1-6\. 瑜伽姿势检测
- en: The models we’ve seen so far focus on generating outputs conditioned on a given
    input. In some cases such as search engines or recommendation systems, the product
    goal is about surfacing relevant items. This is what we will cover in the following
    category.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的模型主要关注于根据给定输入生成输出。在某些情况下，例如搜索引擎或推荐系统，产品的目标是展示相关的项目。这是我们将在下一个类别中讨论的内容。
- en: Catalog organization
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目录组织
- en: Catalog organization models most often produce a set of results to present to
    users. These results can be conditioned on an input string typed into a search
    bar, an uploaded image, or a phrase spoken to a home assistant. In many cases
    such as streaming services, this set of results can also be proactively presented
    to the user as content they may like without them making a request at all.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 目录组织模型通常会生成一组结果供用户查看。这些结果可以基于键入搜索栏中的输入字符串、上传的图像或对家庭助手说的短语进行条件设定。在许多情况下，例如流媒体服务，这些结果也可以主动呈现给用户，作为他们可能喜欢的内容，而不需要他们发出任何请求。
- en: '[Figure 1-7](#organization) shows an example of such a system that volunteers
    potential candidate movies to watch based on a movie the user just viewed, but
    without having the user perform any form of search.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 1-7](#organization) 展示了这样一个系统的示例，它基于用户刚刚观看的电影主动提供潜在的候选电影，而无需用户进行任何形式的搜索。'
- en: '![Movie recommendations](assets/bmla_0107.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![电影推荐](assets/bmla_0107.png)'
- en: Figure 1-7\. Movie recommendations
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 1-7\. 电影推荐
- en: These models thus either *recommend* items that are related to an item the user
    already expressed interest in (similar Medium articles or Amazon products) or
    provide a useful way to *search* through a catalog (allowing users to search for
    items by typing text or submitting their own photos).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些模型要么*推荐*与用户已经表达兴趣的物品相关的物品（例如类似的Medium文章或Amazon产品），要么提供一个有用的*搜索*目录的方式（允许用户通过键入文本或提交他们自己的照片来搜索物品）。
- en: These recommendations are most often based on learning from previous user patterns,
    in which case they are called *collaborative* recommendation systems. Sometimes,
    they are based on particular attributes of items, in which case they are called
    *content-based* recommendation systems. Some systems leverage both collaborative
    and content-based approaches.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些推荐通常基于从先前用户模式学习的经验，这种情况下称为*协同*推荐系统。有时，它们基于物品的特定属性，这种情况下称为*基于内容*的推荐系统。一些系统结合了协同和基于内容的方法。
- en: Finally, ML can also be used for creative purposes. Models can learn to generate
    aesthetically pleasing images, audio, and even amusing text. Such models are referred
    to as generative models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，机器学习也可以用于创造性目的。模型可以学习生成审美良好的图像、音频，甚至有趣的文本。这样的模型被称为生成模型。
- en: Generative models
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成模型
- en: Generative models focus on generating data, potentially dependent on user input.
    Because these models focus on generating data rather than classifying it in categories,
    scoring it, extracting information from it, or organizing it, they usually have
    a wide range of outputs. This means that generative models are uniquely fit for
    tasks such as translation, where outputs are immensely varied.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型专注于生成数据，可能依赖于用户输入。因为这些模型侧重于生成数据而不是分类、评分、提取信息或组织信息，它们通常具有广泛的输出。这意味着生成模型非常适合像翻译这样输出多样的任务。
- en: On the other side, generative models are often used to train and have outputs
    that are less constrained, making them a riskier choice for production. For that
    reason, unless they are necessary to attain your goal, I recommend starting with
    other models first. For readers who would like to dive deeper into generative
    models, however, I recommend the book [*Generative Deep Learning*](https://learning.oreilly.com/library/view/generative-deep-learning/9781492041931/),
    by David Foster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成模型通常用于训练和输出不受限制的情况，这使得它们成为生产中更具风险的选择。因此，除非它们对实现目标必不可少，否则建议首先使用其他模型。然而，对于希望深入了解生成模型的读者，我推荐David
    Foster的书籍[*Generative Deep Learning*](https://learning.oreilly.com/library/view/generative-deep-learning/9781492041931/)。
- en: Practical examples include translation, which maps sentences in one language
    to another; summarization; subtitle generation, which maps videos and audio tracks
    to transcripts; and neural style transfer (see [Gatys et al., “A Neural Algorithm
    of Artistic Style”)](https://oreil.ly/XVwMs), which maps images to stylized renditions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的例子包括翻译，将一种语言中的句子映射到另一种语言；摘要生成；字幕生成，将视频和音轨映射到文本；以及神经风格转移（参见[Gatys等，“艺术风格的神经算法”](https://oreil.ly/XVwMs)），将图像映射到风格化的再现。
- en: '[Figure 1-8](#style_transfer) shows an example of a generative model transforming
    a photograph on the left by giving it a style similar to a painting shown in the
    vignette on the right side.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-8](#style_transfer)展示了一个生成模型的例子，通过给左侧的照片赋予与右侧小插图中的绘画类似风格的转换。'
- en: '![Style transfer example from Gatys Et al.](assets/bmla_0108.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Gatys Et al.的风格转移示例](assets/bmla_0108.png)'
- en: Figure 1-8\. Style transfer example from [Gatys et al., “A Neural Algorithm
    of Artistic Style”](https://oreil.ly/XVwMs)
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-8的风格转移示例来自[Gatys等，“艺术风格的神经算法”](https://oreil.ly/XVwMs)。
- en: As you can tell by now, each type of model requires a different type of data
    to be trained on. Commonly, a choice of a model will strongly depend on the data
    you are able to obtain—data availability often drives model selection.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，每种类型的模型都需要不同类型的训练数据。通常情况下，模型的选择很大程度上取决于您能够获取的数据，因为数据的可用性通常决定了模型的选择。
- en: Let’s cover a few common data scenarios and associated models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看几种常见的数据情景及其关联的模型。
- en: Data
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: Supervised ML models leverage patterns in data to learn useful mappings between
    inputs and outputs. If a dataset contains features that are predictive of the
    target output, it should be possible for an appropriate model to learn from it.
    Most often, however, we do not initially have the right data to train a model
    to solve a product use case from end-to-end.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习模型利用数据中的模式来学习输入和输出之间有用的映射关系。如果数据集包含预测目标输出的特征，适当的模型应该能够从中学习。然而，大多数情况下，我们最初没有正确的数据来训练模型从头到尾解决产品使用案例。
- en: For example, say we are training a *speech recognition* system that will listen
    for requests from customers, understand their intent, and perform actions depending
    on said intent. When we start working on this project, we may define a set of
    intents we would want to understand, such as “playing a movie on the television.”
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在训练一个*语音识别*系统，它将监听客户的请求，理解他们的意图，并根据该意图执行操作。当我们开始这个项目时，我们可以定义一组我们希望理解的意图，比如“在电视上播放电影”。
- en: To train an ML model to accomplish this task, we would need to have a dataset
    containing audio clips of users of diverse backgrounds asking in their own terms
    for the system to play a movie. Having a representative set of inputs is crucial,
    as any model will only be able to learn from the data that we present to it. If
    a dataset contains examples from only a subset of the population, a product will
    be useful to only that subset. With that in mind, because of the specialized domain
    we have selected, it is extremely unlikely that a dataset of such examples already
    exists.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个能够完成此任务的机器学习模型，我们需要有一个包含来自各种背景的用户的音频剪辑的数据集，这些用户用自己的术语请求系统播放电影。拥有代表性的输入集合至关重要，因为任何模型只能从我们提供给它的数据中学习。如果数据集仅包含某一人群的示例，产品将只对该人群有用。考虑到我们选择的专业领域，因此几乎不可能已经存在这样的示例数据集。
- en: For most applications we would want to tackle, we will need to search for, curate,
    and collect additional data. The data acquisition process can vary widely in scope
    and complexity depending on the specifics of a project, and estimating the challenge
    ahead of time is crucial in order to succeed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数我们想要解决的应用程序，我们需要搜索、策划和收集额外的数据。数据获取过程的范围和复杂性可能因项目的具体情况而异，提前估计面临的挑战是成功的关键。
- en: To start, let’s define a few different situations you can find yourself in when
    searching for a dataset. This initial situation should be a key factor in deciding
    how to proceed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一些在搜索数据集时可能遇到的不同情况。这个初始情况应该是决定如何继续的关键因素。
- en: Data types
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据类型
- en: Once we’ve defined a problem as *mapping inputs to outputs*, we can search for
    data sources that follow this mapping.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们把问题定义为*将输入映射到输出*，我们可以搜索遵循此映射的数据来源。
- en: For fraud detection, these could be examples of fraudulent and innocent users,
    along with features of their account that we could use to predict their behavior.
    For translation, this would be a corpus of sentence pairs in the source and target
    domains. For content organization and search, this could be a history of past
    searches and clicks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于欺诈检测，这些可能是欺诈和无辜用户的示例，以及他们账户的特征，我们可以用来预测他们的行为。对于翻译，这将是源语言和目标语言领域的句子对语料库。对于内容组织和搜索，这可能是过去搜索和点击的历史记录。
- en: We will rarely be able to find the exact mapping we are looking for. For this
    reason, it is useful to consider a few different cases. Think of this as a hierarchy
    of needs for data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很少能找到我们正在寻找的确切映射。因此，考虑几种不同情况是有用的。把这看作是数据需求的层次结构。
- en: Data availability
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据可用性
- en: There are roughly three levels of data availability, from best-case scenario
    to most challenging. Unfortunately, as with most other tasks, you can generally
    assume that the most useful type of data will be the hardest to find. Let’s go
    through them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 大致有三个数据可用性级别，从最理想的情况到最具挑战性的情况。不幸的是，与大多数其他任务一样，通常可以假设最有用的数据类型最难找到。让我们详细讨论一下。
- en: Labeled data exists
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 标记数据存在
- en: This is the leftmost category in [Figure 1-9](#data_avail). When working on
    a supervised model, finding a *labeled dataset* is every practitioner’s dream.
    Labeled here means that many data points contain the target value that the model
    is trying to predict. This makes training and judging model quality much easier,
    as labels provide ground truth answers. Finding a labeled dataset that fits your
    needs and is freely available on the web is rare in practice. It is common, however,
    to mistake the dataset that you find for the dataset that you need.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[图 1-9](#data_avail)中最左侧的类别。在处理监督模型时，找到一个*标记数据集*是每个从业者的梦想。这里的“标记”意味着许多数据点包含模型试图预测的目标值。这使得训练和评估模型质量变得更加容易，因为标签提供了真实的答案。在实践中，很难找到一个符合你需求并且在网上免费获取的标记数据集。然而，常见的情况是你可能会把你找到的数据集误认为是你所需要的数据集。
- en: Weakly labeled data exists
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 存在弱标记数据
- en: This is the middle category in [Figure 1-9](#data_avail). Some datasets contain
    labels that are not exactly a modeling target, but somewhat correlated with it.
    Playback and skip history for a music streaming service are examples of a weakly
    labeled dataset for predicting whether a user dislikes a song. While a listener
    may have not marked a song as disliked, if they skipped it as it was playing,
    it is an indication that they may have not been fond of it. Weak labels are less
    precise by definition but often easier to find than perfect labels.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[图 1-9](#data_avail)中间的类别。一些数据集包含的标签与建模目标不完全一致，但与之相关。例如，音乐流媒体服务的回放和跳过历史是一个弱标记的数据集示例，用于预测用户是否不喜欢一首歌曲。虽然听众可能没有标记一首歌曲为不喜欢，但如果他们在播放时跳过了它，这表明他们可能不喜欢这首歌。弱标签按定义不够精确，但通常比完美标签更容易找到。
- en: Unlabeled data exists
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 存在无标记数据
- en: This category is on the right side of [Figure 1-9](#data_avail). In some cases,
    while we do not have a labeled dataset mapping desired inputs to outputs, we at
    least have access to a dataset containing relevant examples. For the text translation
    example, we might have access to large collections of text in both languages,
    but with no direct mapping between them. This means we need to label the dataset,
    find a model that can learn from unlabeled data, or do a little bit of both.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[图 1-9](#data_avail)中右侧的类别。在某些情况下，虽然我们没有一个将期望输入映射到输出的标记数据集，但我们至少可以访问一个包含相关示例的数据集。以文本翻译为例，我们可能可以访问大量的文本集合，涵盖两种语言，但它们之间没有直接的映射。这意味着我们需要标记数据集，找到能够从无标记数据中学习的模型，或者两者兼而有之。
- en: We need to acquire data
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要获取数据
- en: In some cases, we are one step away from unlabeled data, as we need to first
    acquire it. In many cases, we do not have a dataset for what we need and thus
    will need to find a way to acquire such data. This is often seen as an insurmountable
    task, but many methods now exist to rapidly gather and label data. This will be
    the focus of [Chapter 4](ch04.html#initial_dataset).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们离无标记数据只有一步之遥，因为我们首先需要获取它。在许多情况下，我们没有我们所需的数据集，因此需要找到一种获取这些数据的方法。这通常被视为一项难以克服的任务，但现在存在许多快速收集和标记数据的方法。这将是[第四章](ch04.html#initial_dataset)的重点。
- en: For our case study, an ideal dataset would be a set of user-typed questions,
    along with a set of better worded questions. A *weakly labeled* dataset would
    be a dataset of many questions with some weak labels indicative of their quality
    such as “likes” or “upvotes.” This would help a model learn what makes for good
    and bad questions but would not provide side-by-side examples for the same question.
    You can see both of these examples in [Figure 1-9](#data_avail).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的案例研究，一个理想的数据集将是一组用户提出的问题，以及一组更好的措辞问题。一个*弱标记*的数据集将是包含许多问题并带有一些弱标签，表明它们质量的数据集，例如“喜欢”或“点赞”。这将有助于模型学习什么是好问题和坏问题，但不提供相同问题的并列示例。你可以在[图 1-9](#data_avail)中看到这两个例子。
- en: '![The most useful data is generally the least available](assets/bmla_0109A.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![最有用的数据通常是最稀缺的](assets/bmla_0109A.png)'
- en: Figure 1-9\. Data availability versus data usefulness
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-9\. 数据可用性与数据有用性
- en: In general in ML, a weakly labeled dataset refers to a dataset that contains
    information that will help a model learn, but not the exact ground truth. In practice,
    most datasets that we can gather are weakly labeled.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在机器学习中，弱标记数据集指的是包含可以帮助模型学习的信息，但不是精确的真实标签的数据集。实际上，我们可以收集到的大多数数据集都是弱标记的。
- en: Having an imperfect dataset is entirely fine and shouldn’t stop you. The ML
    process is iterative in nature, so starting with a dataset and getting some initial
    results is the best way forward, regardless of the data quality.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个不完美的数据集是完全可以接受的，不应该阻止你。ML过程是迭代的，所以从一个数据集开始并获得一些初步结果是前进的最佳方式，无论数据质量如何。
- en: Datasets are iterative
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集是迭代的
- en: In many cases, since you will not be able to immediately find a dataset containing
    a direct mapping from inputs to your desired output, I suggest progressively iterating
    on the way you formulate the problem, making it easier to find an adequate dataset
    to start with. Each dataset you explore and use will provide you with valuable
    information that you can use to curate the next version of your dataset and generate
    useful features for your models.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，由于您可能无法立即找到一个直接映射从输入到所需输出的数据集，我建议逐步迭代问题的表述方式，使得更容易找到一个足够的数据集作为起点。您探索和使用的每个数据集都将为您提供宝贵的信息，您可以利用这些信息策划下一个版本的数据集，并为您的模型生成有用的特征。
- en: Let’s now dive into the case study and see how we can use what we’ve learned
    to identify different models and datasets we could use, and choose the most appropriate.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入案例研究，看看我们如何利用所学知识识别不同的模型和数据集，并选择最合适的。
- en: Framing the ML Editor
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建ML编辑器
- en: Let’s see how we could iterate through a product use case to find the right
    ML framing. We’ll get through this process by outlining a method to progress from
    a product goal (helping users write better questions) to an ML paradigm.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何通过一个产品用例的迭代来找到正确的ML框架。我们将通过概述从产品目标（帮助用户写出更好问题）到ML范式的方法来完成这一过程。
- en: We would like to build an editor that accepts questions by users and improves
    them to be better written, but what does “better” mean in this case? Let’s start
    by defining the writing assistant’s product goal a little more clearly.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要建立一个编辑器，接受用户的问题并改进它们，使其写作更好，但在这种情况下，“更好”是什么意思呢？让我们从更清晰地定义写作助手的产品目标开始。
- en: Many people use forums, social networks, and websites such as [Stack Overflow](https://stackoverflow.com/)
    to find answers to their questions. However, the way that people ask questions
    has a dramatic impact on whether they receive a useful answer. This is unfortunate
    both for the user looking to get their question answered and for future users
    that may have the same problem and could have found an existing answer useful.
    To that end, our goal will be to *build an assistant that can help users write
    better questions*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人使用论坛、社交网络和网站如[Stack Overflow](https://stackoverflow.com/)来寻找答案。然而，人们提问的方式对于是否能得到有用的答案有着巨大的影响。这对于希望得到他们问题答案的用户和可能有相同问题的未来用户都是不利的。因此，我们的目标是*构建一个助手，可以帮助用户写出更好的问题*。
- en: We have a product goal and now need to decide which modeling approach to use.
    To make this decision, we will go through the iteration loop of model selection
    and data validation mentioned earlier.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个产品目标，需要决定使用哪种建模方法。为了做出这个决定，我们将进行前面提到的模型选择和数据验证的迭代循环。
- en: 'Trying to Do It All with ML: An End-to-End Framework'
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试使用ML来做所有事情：一个端到端的框架
- en: In this context, *end-to-end* means using a single model to go from input to
    output with no intermediary steps. Since most product goals are very specific,
    attempting to solve an entire use case by learning it from end-to-end often requires
    custom-built cutting-edge ML models. This may be the right solution for teams
    that have the resources to develop and maintain such models, but it is often worth
    it to start with more well-understood models first.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，*端到端*意味着使用单一模型从输入到输出，没有中间步骤。由于大多数产品目标非常具体，尝试通过端到端学习整个用例通常需要定制的尖端ML模型。这可能是那些有能力开发和维护这些模型的团队的正确解决方案，但通常最好先从更为成熟的模型开始。
- en: In our case, we could attempt to gather a dataset of poorly formulated questions,
    as well as their professionally edited versions. We could then use a generative
    model to go straight from one text to the other.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们可以尝试收集一个问题表述不佳及其专业编辑版本的数据集。然后，我们可以使用生成模型直接从一个文本生成另一个文本。
- en: '[Figure 1-10](#end_to_end) depicts what this would look like in practice. It
    shows a simple diagram with user input on the left, the desired output on the
    right, and a model in between.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-10](#end_to_end)展示了这在实践中的样子。它显示了一个简单的图表，左侧是用户输入，右侧是期望的输出，中间是一个模型。'
- en: '![An end-to-end approach](assets/bmla_0109.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![端到端方法](assets/bmla_0109.png)'
- en: Figure 1-10\. End-to-end approach
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-10\. 端到端方法
- en: 'As you’ll see, this approach comes with significant challenges:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这种方法存在着显著的挑战：
- en: Data
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 数据
- en: To acquire such a dataset, we would need to find pairs of questions with the
    same intent but of different wording quality. This is quite a rare dataset to
    find as is. Building it ourselves would be costly as well, as we would need to
    be assisted by professional editors to generate this data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取这样的数据集，我们需要找到意图相同但措辞不同的问题对。这种数据集很难找到。自行构建将会很昂贵，因为我们需要专业编辑的帮助来生成这些数据。
- en: Model
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 模型
- en: Models going from one sequence of text to another, seen in the generative models
    category discussed earlier, have progressed tremendously in recent years. Sequence-to-sequence
    models (as described in the paper by I. Sutskever et al., [“Sequence to Sequence
    Learning with Neural Networks”](https://arxiv.org/abs/1409.3215)) were originally
    proposed in 2014 for translation tasks and are closing the gap between machine
    and human translation. The success of these models, however, has mostly been on
    sentence-level tasks, and they have not been frequently used to process text longer
    than a paragraph. This is because so far, they have not been able to capture long-term
    context from one paragraph to another. Additionally, because they usually have
    a large number of parameters, they are some of the slowest models to train. If
    a model is trained only once, this is not necessarily an issue. If it needs to
    be retrained hourly or daily, training time can become an important factor.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从一段文本到另一段文本的模型，如前面讨论的生成模型类别中所见，近年来取得了巨大进展。序列到序列模型（如 I. Sutskever 等人在 [“Sequence
    to Sequence Learning with Neural Networks”](https://arxiv.org/abs/1409.3215) 论文中描述的那样）最初是2014年提出用于翻译任务，并且正在缩小机器翻译与人类翻译之间的差距。然而，这些模型的成功主要集中在句子级任务上，并且它们并不经常用于处理比段落更长的文本。这是因为迄今为止，它们尚未能够从一段到另一段捕捉长期上下文。此外，由于它们通常具有大量参数，它们是训练速度最慢的模型之一。如果一个模型只需要训练一次，这并不一定是问题。但如果它需要每小时或每天重新训练，训练时间可能会成为一个重要因素。
- en: Latency
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟
- en: Sequence-to-sequence models are often *autoregressive models*, meaning they
    require the model’s output of the previous word to start working on the next.
    This allows them to leverage information from neighboring words but causes them
    to be slower to train and slower to run inference on than the simpler models.
    Such models can take a few seconds to produce an answer at inference time, as
    opposed to subsecond latency for simpler models. While it is possible to optimize
    such a model to run quickly enough, it will require additional engineering work.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型通常是*自回归模型*，这意味着它们需要上一个词的模型输出来开始处理下一个词。这使它们能够利用邻近词的信息，但导致它们在训练和推理时比更简单的模型要慢。这样的模型可能需要几秒钟来在推理时生成答案，而简单模型的延迟则为亚秒级。虽然可以优化这样的模型以使其运行速度足够快，但这将需要额外的工程工作。
- en: Ease of implementation
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 实施的便利性
- en: Training complex end-to-end models is a very delicate and error-prone process,
    as they have many moving parts. This means that we need to consider the tradeoff
    between a model’s potential performance and the complexity it adds to a pipeline.
    This complexity will slow us down when building a pipeline, but it also introduces
    a maintenance burden. If we anticipate that other teammates may need to iterate
    on and improve on your model, it may be worthwhile to choose a set of simpler,
    more well-understood models.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 训练复杂的端到端模型是一个非常微妙且容易出错的过程，因为它们有许多移动部件。这意味着我们需要考虑模型潜在性能与其为流水线增加的复杂性之间的权衡。这种复杂性在构建流水线时会减慢我们的速度，同时也会增加维护负担。如果我们预期其他团队成员可能需要对您的模型进行迭代和改进，选择一组更简单、更为人熟知的模型可能是值得的。
- en: This end-to-end approach could work, but it will require a lot of upfront data
    gathering and engineering effort, with no success guarantee, so it would be worthwhile
    to explore other alternatives, as we will cover next.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这种端到端方法可能有效，但需要大量的前期数据收集和工程努力，并不能保证成功，因此探索其他替代方案是值得的，接下来我们将介绍。
- en: 'The Simplest Approach: Being the Algorithm'
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最简单的方法：成为算法
- en: As you’ll see in the interview at the end of this section, it is often a great
    idea for data scientists to *be the algorithm* before they implement it. In other
    words, to understand how to best automate a problem, start by attempting to solve
    it manually. So, if we were editing questions ourselves to improve readability
    and the odds of getting an answer, how would we go about it?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将在本节末尾的采访中看到的，对于数据科学家来说，*成为算法*通常是一个很好的想法，然后再实现它。换句话说，要想了解如何最好地自动化一个问题，首先尝试手动解决它是个不错的开始。那么，如果我们自己编辑问题以提高可读性和获取答案的几率，我们将如何进行？
- en: A first approach would be to not use data at all but leverage prior art to define
    what makes a question or a body of text well written. For general writing tips,
    we could reach out to a professional editor or research newspapers’ style guides
    to learn more.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一个初步的方法是完全不使用数据，而是利用先前的成果来定义什么样的问题或文本写得好。对于一般的写作技巧，我们可以向专业编辑或研究报纸的风格指南寻求帮助以了解更多信息。
- en: In addition, we should dive into a dataset to look at individual examples and
    trends and let those inform our modeling strategy. We will skip this for now as
    we will cover how to do this in more depth in [Chapter 4](ch04.html#initial_dataset).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们应该深入数据集以查看个别示例和趋势，并让它们指导我们的建模策略。由于我们将在[第四章](ch04.html#initial_dataset)中更深入地介绍如何做到这一点，现在我们将跳过此步骤。
- en: 'To start, we could look at existing [research](https://oreil.ly/jspYn) to identify
    a few attributes we might use to help people write more clearly. These features
    could include factors such as:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以查看现有的[研究](https://oreil.ly/jspYn)，以识别几个可能用于帮助人们更清晰地写作的属性。这些特征可能包括以下因素：
- en: Prose simplicity
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 散文简洁性
- en: We often give new writers the advice to use simpler words and sentence structures.
    We could thus establish a set of criteria on the appropriate sentence and word
    length, and recommend changes as needed.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常建议新作家使用更简单的词语和句子结构。因此，我们可以建立一套适合句子和词语长度的标准，并在需要时推荐修改。
- en: Tone
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 语气
- en: We could measure the use of adverbs, superlatives, and punctuation to measure
    the polarity of the text. Depending on the context, more opinionated questions
    may receive fewer answers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以测量副词、最高级和标点符号的使用情况来测量文本的极性。根据背景，更具意见的问题可能会得到较少的答案。
- en: Structural features
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 结构特征
- en: Finally, we could try to extract the presence of important structural attributes
    such as the use of greetings or question marks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以尝试提取重要结构属性的存在，如使用问候语或问号。
- en: 'Once we have identified and generated useful features, we can build a simple
    solution that uses them to provide recommendations. There is no ML involved here,
    but this phase is crucial for two reasons: it provides a baseline that is very
    quick to implement and will serve as a yardstick to measure models against.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定并生成了有用的特征，我们可以构建一个简单的解决方案，利用它们提供建议。这里没有涉及机器学习，但这个阶段有两个关键原因：它提供了一个非常快速实施的基准，并将作为衡量模型的标准。
- en: To validate our intuition about how to detect good writing, we can gather a
    dataset of “good” and “bad” text and see if we can tell the good from the bad
    using these features.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们对如何识别优秀写作的直觉，我们可以收集一个“好”和“坏”文本的数据集，并看看是否能够利用这些特征区分好的和坏的。
- en: 'Middle Ground: Learning from Our Experience'
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中间地带：从我们的经验中学习
- en: Now that we have a baseline set of features, we can attempt to use them to *learn
    a model of style from a body of data*. To do this we can gather a dataset, extract
    the features we described earlier from it, and train a classifier on it to separate
    good and bad examples.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一组基准特征集，我们可以尝试从数据集中*学习样式模型*。为此，我们可以收集一个数据集，从中提取我们之前描述的特征，并对其进行分类器训练，以区分好的和坏的示例。
- en: Once we have a model that can classify written text, we can inspect it to identify
    which features are highly predictive and use those as recommendations. We will
    see how to do this in practice in [Chapter 7](ch07.html#using_clas_recom).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了可以对书面文本进行分类的模型，我们就可以检查它，以确定哪些特征具有很高的预测性，并将其用作推荐。我们将在实践中看到如何做到这一点，详情请见[第七章](ch07.html#using_clas_recom)。
- en: '[Figure 1-11](#middle_ground) describes this approach. On the left side, a
    model is trained to classify a question as good or bad. On the right side, the
    trained model is given a question and scores candidate reformulations of this
    question that will lead to it receiving a better score. The reformulation with
    the highest score is recommended to the user.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-11](#middle_ground)描述了这种方法。左侧，一个模型被训练为将问题分类为好的或坏的。右侧，训练好的模型接受一个问题并对该问题的候选重构进行评分，以使其获得更高的分数。得分最高的重构建议给用户。'
- en: '![A middle ground between manual and end to end](assets/bmla_0111.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![手动与端到端之间的中间地带](assets/bmla_0111.png)'
- en: Figure 1-11\. A middle ground between manual and end-to-end
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-11\. 手动与端到端之间的中间地带
- en: 'Let’s examine the challenges we outlined in [“Trying to Do It All with ML:
    An End-to-End Framework”](#end_to_end_framework) and see whether the classifier
    approach makes them any easier:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查我们在[“试图用ML做所有事情：端到端框架”](#end_to_end_framework)中概述的挑战，并看看分类器方法是否能更轻松地解决它们：
- en: Dataset
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集
- en: We could obtain a dataset of good and bad examples by gathering questions from
    an online forum along with some measure of their quality, such as the number of
    views or upvotes. As opposed to the end-to-end approach, this does not require
    us to have access to revisions of the same questions. We simply need a set of
    good and bad examples we can hope to learn aggregate features from, which is an
    easier dataset to find.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过从在线论坛收集问题及其质量指标（如浏览量或赞数）来获取一组好的和坏的示例数据集。与端到端方法相反，这不需要我们访问相同问题的修订版本。我们只需一个可以学习聚合特征的好和坏示例集合，这是一个更容易找到的数据集。
- en: Model
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 模型
- en: 'We need to consider two things here: how predictive a model is (can it efficiently
    separate good and bad articles?) and how easily features can be extracted from
    it (can we see which attributes were used to classify an example?). There are
    many potential models that we could use here, along with different features we
    could extract from text to make it more explainable.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要考虑两件事情：模型的预测能力（它能有效区分好坏文章吗？）以及从中提取特征的便利性（我们能看到用于分类示例的哪些属性吗？）。在这里我们可以使用许多潜在的模型，以及从文本中提取不同特征以使其更具可解释性。
- en: Latency
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟
- en: Most text classifiers are quite quick. We could start with a simple model such
    as a random forest, which can return results in less than a tenth of a second
    on regular hardware, and move on to more complex architectures if needed.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数文本分类器非常迅速。我们可以从一个简单的模型开始，比如随机森林，它可以在普通硬件上少于十分之一秒返回结果，并根据需要转向更复杂的架构。
- en: Ease of implementation
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 实施的简易性
- en: Compared to text generation, text classification is relatively well understood,
    meaning building such a model should be relatively quick. Many examples of working
    text classification pipelines exist online, and many such models have already
    been deployed to production.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 与文本生成相比，文本分类相对较为成熟，这意味着构建这样一个模型应该相对迅速。在线上存在许多工作的文本分类流水线示例，并且许多这样的模型已经部署到生产环境中。
- en: If we start with a human heuristic and then build this simple model, we will
    quickly be able to have an initial baseline, and the first step toward a solution.
    Moreover, the initial model will be a great way to inform what to build next (more
    on this in [Part III](part03.html#section_3)).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从人类启发式开始，然后构建这个简单模型，我们将很快能够得到一个初始基准，并迈出解决问题的第一步。此外，初始模型将是指导接下来构建什么的良好方式（更多详情请参阅[第三部分](part03.html#section_3)）。
- en: For more on the importance of starting with simple baselines, I sat down with
    Monica Rogati, who shares some of the lessons she has learned helping data teams
    deliver products.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 关于从简单基线开始的重要性，我与Monica Rogati进行了交流，她分享了她在帮助数据团队交付产品过程中学到的一些经验教训。
- en: 'Monica Rogati: How to Choose and Prioritize ML Projects'
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Monica Rogati：如何选择和优先处理ML项目
- en: After getting her Ph.D. in computer science, Monica Rogati started her career
    at LinkedIn where she worked on core products such as integrating ML into the
    People You May Know algorithm and built the first version of job-to-candidate
    matching. She then became VP of data at Jawbone where she built and led the entire
    data team. Monica is now an adviser to dozens of companies whose number of employees
    ranges from 5 to 8,000\. She has kindly agreed to share some of the advice she
    often gives to teams when it comes to designing and executing on ML products.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 莫妮卡·罗加蒂在计算机科学博士学位后，从LinkedIn开始她的职业生涯，她在整合机器学习到“你可能认识的人”算法等核心产品上工作，并建立了职位到候选人匹配的第一个版本。然后她成为Jawbone的数据副总裁，领导整个数据团队。莫妮卡现在是几十家公司的顾问，这些公司的员工数量从5到8,000不等。她很慷慨地同意分享一些她在设计和执行机器学习产品时经常给团队的建议。
- en: 'Q: *How do you scope out an ML product?*'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Q：*如何确定机器学习产品的范围？*
- en: 'A: You have to remember that you are trying to use the best tools to solve
    a problem, and only use ML if it makes sense.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: A：您必须记住，您正在尝试使用最佳工具来解决问题，并且只有在有意义的情况下才使用机器学习。
- en: Let’s say you wanted to predict what a user of an application will do and show
    it to them as a suggestion. You should start by combining discussions on modeling
    and product. Among other things, this includes designing the product around handling
    ML failures gracefully.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想预测应用程序用户将要做什么，并将其显示给他们作为建议。您应该从模型和产品的讨论开始。除其他事项外，这还包括围绕优雅处理机器学习失败设计产品。
- en: You could start by taking into account the confidence our model has in its prediction.
    We could then formulate our suggestions differently based on the confidence score.
    If the confidence is above 90%, we present the suggestion prominently; if it is
    over 50%, we still display it but with less emphasis, and we do not display anything
    if the confidence is below this score.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从考虑我们的模型对其预测的信心开始。然后，根据信心分数不同，我们可以以不同方式表达我们的建议。如果信心高于90％，我们会突出显示建议；如果超过50％，我们仍然显示，但不那么强调；如果信心低于这个分数，我们不显示任何内容。
- en: 'Q: *How do you decide what to focus on in an ML project?*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Q：*在机器学习项目中如何决定关注什么？*
- en: 'A: You have to find the *impact bottleneck*, meaning the piece of your pipeline
    that could provide the most value if you improve on it. When working with companies,
    I often find that they may not be working on the right problem or not be at the
    right growth stage for this.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: A：您必须找到*影响瓶颈*，即如果您改进它，可能会提供最大价值的流程部分。在与公司合作时，我经常发现，它们可能没有解决正确的问题，或者不处于适合此问题的增长阶段。
- en: There are often problems around the model. The best way to find this out is
    to replace the model with something simple and debug the whole pipeline. Frequently,
    the issues will not be with the accuracy of your model. Frequently, your product
    is dead even if your model is successful.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型周围经常会出现问题。找出问题的最佳方法是用简单的东西替换模型，然后调试整个流程。经常情况下，问题并不在于您模型的准确性。即使您的模型成功，您的产品也可能失败。
- en: 'Q: *Why do you usually recommend starting with a simple model?*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Q：*为什么您通常建议从简单模型开始？*
- en: 'A: The goal of our plan should be to derisk our model somehow. The best way
    to do this is to start with a “strawman baseline” to evaluate worst-case performance.
    For our earlier example, this could be simply suggesting whichever action the
    user previously took.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: A：我们计划的目标应该是以某种方式减少我们模型的风险。这样做的最佳方式是从“稻草人基准”开始评估最坏情况下的性能。对于我们之前的例子，这可能仅仅是建议用户之前采取的任何行动。
- en: If we did this, how often would our prediction be correct, and how annoying
    would our model be to the user if we were wrong? Assuming that our model was not
    much better than this baseline, would our product still be valuable?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们这样做了，我们的预测有多少时会是正确的，如果我们错了，我们的模型对用户会有多烦人？假设我们的模型比这个基准好不了多少，我们的产品仍然有价值吗？
- en: This applies well to examples in natural language understanding and generation
    such as chatbots, translation, Q&A, and summarization. Oftentimes in summarization,
    for example, simply extracting the top keywords and categories covered by an article
    is enough to serve most users’ needs.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这对自然语言理解和生成的例子非常适用，比如聊天机器人、翻译、问答和摘要。例如，在摘要中，仅仅提取文章涵盖的顶级关键词和类别通常足以满足大多数用户的需求。
- en: 'Q: *Once you have your whole pipeline, how do you identify the impact bottleneck?*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Q：*一旦您有了整个流程，如何确定影响的瓶颈？*
- en: 'A: You should start with imagining that the impact bottleneck is solved, and
    ask yourself whether it was worth the effort you estimated it would take. I encourage
    data scientists to compose a tweet and companies to write a press release before
    they even start on a project. That helps them avoid working on something just
    because they thought it was cool and puts the impact of the results into context
    based on the effort.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 你应该从设想影响瓶颈已解决的情况开始，并问问自己预计的努力是否值得。我鼓励数据科学家在开始项目之前撰写一条推文，公司撰写一篇新闻稿。这有助于他们避免仅仅因为觉得酷而开始某些工作，并根据努力的影响将结果的影响置于上下文中。'
- en: 'The ideal case is that you can pitch the results regardless of the outcome:
    if you do not get the best outcome, is this still impactful? Have you learned
    something or validated some assumptions? A way to help with this is to build infrastructure
    to help lower the required effort for deployment.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况是，你可以无论结果如何都推销结果：即使你没有获得最佳结果，这仍然具有影响力吗？你学到了什么或验证了一些假设吗？一种有助于这一点的方法是建立基础设施，帮助降低部署所需的工作量。
- en: At LinkedIn, we had access to a very useful design element, a little window
    with a few rows of text and hyperlinks, that we could customize with our data.
    This made it easier to launch experiments for projects such as job recommendations,
    as the design was already approved. Because the resource investment was low, the
    impact did not have to be as large, which allowed for a faster iteration cycle.
    The barrier then becomes about nonengineering concerns, such as ethics, fairness,
    and branding.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LinkedIn，我们可以使用非常有用的设计元素，一个带有几行文本和超链接的小窗口，我们可以根据我们的数据进行自定义。这使得为诸如职位推荐之类的项目启动实验变得更容易，因为设计已经得到批准。由于资源投入低，影响不必如此大，这允许更快的迭代周期。障碍随后变成了非工程问题，如伦理、公平性和品牌。
- en: 'Q: *How do you decide which modeling techniques to use?*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: *你是如何决定使用哪些建模技术的？*'
- en: 'A: The first line of defense is looking at the data yourself. Let’s say we
    want to build a model to recommend groups to LinkedIn users. A naive way would
    be to recommend the most popular group containing their company’s name in the
    group title. After looking at a few examples, we found out one of the popular
    groups for the company Oracle was “Oracle sucks!” which would be a terrible group
    to recommend to Oracle employees.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 第一道防线是亲自查看数据。假设我们想建立一个模型来为 LinkedIn 用户推荐群组。一个天真的方法是推荐包含其公司名称的最流行群组。在查看了几个例子之后，我们发现公司
    Oracle 的一个流行群组是“Oracle sucks!”，这将是向 Oracle 员工推荐的一个糟糕选择。'
- en: It is always valuable to spend the manual effort to look at inputs and outputs
    of your model. Scroll past a bunch of examples to see if anything looks weird.
    The head of my department at IBM had this mantra of doing something manually for
    an hour before putting in any work.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 值得花费手动工作查看模型的输入和输出总是有价值的。浏览一堆示例以查看是否有异常情况。我在 IBM 的部门负责人有一个口头禅，在做任何工作之前，手动做一个小时的事情。
- en: Looking at your data helps you think of good heuristics, models, and ways to
    reframe the product. If you rank examples in your dataset by frequency, you might
    even be able to quickly identify and label 80% of your use cases.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 查看数据有助于你思考好的启发式、模型和重新构建产品的方法。如果你根据频率对数据集中的示例进行排名，甚至可以快速识别和标记80%的用例。
- en: At Jawbone, for example, people entered “phrases” to log the content of their
    meals. By the time we labeled the top 100 by hand, we had covered 80% of phrases
    and had strong ideas of what the main problems we would have to handle, such as
    varieties of text encoding and languages.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 Jawbone，人们输入“短语”来记录他们的饮食内容。在我们手工标记了前100个短语之后，我们已经涵盖了80%的短语，并且对我们需要处理的主要问题有了明确的想法，例如各种文本编码和语言的变化。
- en: The last line of defense is to have a diverse workforce that looks at the results.
    This will allow you to catch instances where a model is exhibiting discriminative
    behavior, such as tagging your friends as a gorilla, or is insensitive by surfacing
    painful past experiences with its smart “this time last year” retrospective.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的防线是拥有多样化的工作人员来查看结果。这将允许您捕捉到模型展示出歧视行为的情况，例如将您的朋友标记为大猩猩，或者通过其智能的“去年此时”的回顾功能展示令人痛苦的过去经历。
- en: Conclusion
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As we’ve seen, building an ML-powered application starts with judging feasibility
    and picking an approach. Most often, picking a supervised approach is the simplest
    way to get started. Among those, classification, knowledge extraction, catalog
    organization, or generative models are the most common paradigms in practice.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，构建一个由ML驱动的应用程序始于评估可行性并选择方法。在这些方法中，选择监督方法通常是开始的最简单方法。在这些方法中，分类、知识提取、目录组织或生成模型是实践中最常见的范式。
- en: As you are picking an approach, you should identify how easily you’ll be able
    to access strongly or weakly labeled data, or any data at all. You should then
    compare potential models and datasets by defining a product goal and choosing
    the modeling approach that best allows you to accomplish this goal.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择方法时，您应该确定如何轻松或困难地访问有标签或无标签数据，或者根本没有数据。然后，您应该通过定义产品目标并选择最能帮助您实现此目标的建模方法，来比较潜在的模型和数据集。
- en: We illustrated these steps for the ML Editor, opting to start with simple heuristics
    and a classification-based approach. And finally, we covered how leaders such
    as Monica Rogati have been applying these practices to successfully ship ML models
    to users.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为ML编辑器说明了这些步骤，选择从简单的启发式和基于分类的方法开始。最后，我们讲解了像Monica Rogati这样的领导者如何将这些实践应用到成功地推出ML模型给用户。
- en: Now that we have chosen an initial approach, it is time to define success metrics
    and create an action plan to make regular progress. This will involve setting
    minimal performance requirements, doing a deep dive into available modeling and
    data resources, and building a simple prototype.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们选择了一个初始方法，是时候定义成功的度量标准，并制定行动计划以保持定期进展了。这将涉及设置最低性能要求，深入了解可用的建模和数据资源，并构建一个简单的原型。
- en: We will cover all of those in [Chapter 2](ch02.html#setting_expectations).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第2章](ch02.html#setting_expectations)中覆盖所有这些内容。
