- en: Chapter 6\. Monitoring and Logging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 监控与日志记录
- en: By Alfredo Deza
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 阿尔弗雷多·德扎
- en: Not only is the cerebral anatomy double, and not only is it unarguable that
    one hemisphere is enough for consciousness; beyond that, two hemispheres following
    callosotomy have been shown to be conscious simultaneously and independently.
    As Nagel said of the split-brain, “What the right hemisphere can do on its own
    is too elaborate, too intentionally directed, and too psychologically intelligible
    to be regarded merely as a collection of unconscious automatic responses.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不仅大脑解剖学是双重的，而且不仅仅是一个半球足以形成意识；更重要的是，在隔离大脑半球后，已经显示出两个半球可以同时和独立地具有意识。正如纳格尔所说的分脑：“右半球独自完成的工作太复杂、太有意图性和太心理可理解，无法简单地看作是无意识的自动反应的集合。”
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dr. Joseph Bogen
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 约瑟夫·博根博士
- en: Both logging and monitoring are core pillars of DevOps principles that are crucial
    to robust ML practices. Useful logging and monitoring are hard to get right, and
    although you can leverage cloud services that take care of the heavy lifting,
    it is up to you to decide and come up with a sound strategy that makes sense.
    Most software engineers tend to prefer writing code and leave behind other tasks
    like testing, documentation, and very often logging and monitoring.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录和监控都是DevOps原则的核心支柱，对健壮的机器学习实践至关重要。有效的日志记录和监控很难做到，尽管您可以利用云服务来处理繁重的工作，但决策和制定一个合理的策略仍然取决于您自己。大多数软件工程师倾向于编写代码，忽略测试、文档编写以及往往也忽略日志记录和监控等其他任务。
- en: 'Don’t be surprised to hear suggestions about automated solutions that can “solve
    the logging problem.” A solid foundation is possible by thinking thoroughly about
    the problem at hand so that the information produced is usable. The hard work
    and solid foundation ideals I describe become crystal clear when you face useless
    information (it doesn’t help narrate a story) or cryptic (too hard to understand).
    A perfect example of this situation is a software issue I opened back in 2014
    that captured the following question from an online chat about the product:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 不要对能够“解决日志问题”的自动化解决方案的建议感到惊讶。通过深思熟虑地思考手头的问题，可以建立一个坚实的基础，使得产生的信息可用。我所描述的辛勤工作和坚实基础的理念在面对无用信息（它无法帮助叙述一个故事）或晦涩难懂（太难理解）时变得非常清晰。这种情况的一个完美例子是我在2014年提出的一个软件问题，从一个在线聊天关于产品的问题中捕捉到以下问题：
- en: '“Can anyone help me interpret this line:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “有人能帮我解释一下这句话吗：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'I’d been working with this software product for almost two years at the time,
    and I had no idea what that meant. Can you think of a possible answer? A knowledgeable
    engineer had the perfect translation: “The machine you are on is unable to contact
    the monitor at 172.16.17.55.” I was baffled by what the log statement meant. Why
    can’t we make a change to say that instead? The ticket capturing this issue from
    2014, as of this writing, is still open. Even more troubling is that engineering
    replied in that ticket that “The log message is fine.”'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当时我已经使用这款软件产品将近两年时间，对那意味着什么毫无概念。你能想出一个可能的答案吗？一位富有经验的工程师给出了完美的翻译：“您所在的机器无法与172.16.17.55的监视器联系。”我对日志声明的含义感到困惑。为什么我们不能做出改变来明确表达呢？截至本文撰写时，2014年捕捉到此问题的工单仍然未解决。更令人不安的是，工程部门在该工单中回复：“日志消息没有问题。”
- en: Logging and monitoring is hard work because it takes effort to produce meaningful
    output that helps us understand a program’s state.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录和监控是艰苦的工作，因为需要努力产生有意义的输出，帮助我们理解程序的状态。
- en: I’ve mentioned that it is crucial to have information that helps us narrate
    a story. This is true of both monitoring and logging. A few years ago, I worked
    in a large engineering group that delivered one of the world’s largest Python-based
    CMSs (Content Management System). After proposing adding metrics to the application,
    the general sentiment was that the CMS didn’t need it. Monitoring was already
    in place, and the Ops team had all kinds of utilities tied to thresholds for alerts.
    The engineering managers rewarded excellence by giving an engineer time to work
    on any relevant project (not just 20% like some famous tech company). Before working
    on a relevant project, one had to pitch the whole management team’s idea to get
    buy-in. When my time came, I, of course, chose to add metric facilities to the
    application.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到，拥有帮助我们叙述故事的信息至关重要。这对监控和日志记录都是如此。几年前，我曾在一个大型工程团队工作，该团队提供了全球最大的基于 Python 的
    CMS（内容管理系统）之一。在提议将指标添加到应用程序后，普遍的感觉是 CMS 并不需要它。监控已经就位，运维团队已经有各种与警报阈值相关的实用工具。工程经理通过给工程师时间来奖励卓越表现（不仅仅是像某些著名科技公司那样的
    20% 时间）。在开始相关项目之前，必须向整个管理团队提出理念以获得支持。轮到我时，我当然选择了向应用程序添加指标功能。
- en: '“Alfredo, we already have metrics, we know the disk usage, and we have memory
    alerts for every server. We don’t get what we gain by this initiative.” It is
    hard to be standing in front of a large senior management group, and trying to
    convince them of something they don’t believe in. My explanation started with
    the most important button on the website: the subscribe button. That subscribe
    button was the one in charge of producing paying users and crucial to the business.
    I explained that, “if we deploy a new version with a JavaScript issue that makes
    this button unusable, what metric or alert can tell us this is a problem?” Of
    course, disk usage would be the same, and memory usage would probably not change
    at all. And yet, the most important button in the application would go unnoticed
    in an unusable state. In this particular case, metrics can capture the click-through
    rate of every hour, day, and week for that button. And most importantly, it can
    help tell a story about how today the website generates more (or perhaps less!)
    revenue than last year on this same month. Disk usage and memory consumption of
    servers are worth keeping an eye on, but it isn’t the ultimate goal.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: “Alfredo，我们已经有了指标，我们知道磁盘使用情况，并且我们有每台服务器的内存警报。我们不明白通过这个举措我们能得到什么。” 站在一个大型高级管理团队面前，并试图说服他们相信一些他们不相信的事情是很困难的。我的解释从网站上最重要的按钮开始：订阅按钮。那个订阅按钮负责生成付费用户并对业务至关重要。我解释说，“如果我们部署了一个新版本，其中有一个
    JavaScript 问题使得这个按钮无法使用，哪个指标或警报可以告诉我们这是一个问题？” 当然，磁盘使用情况将保持不变，而内存使用可能根本不会改变。然而，应用程序中最重要的按钮将在无法使用的状态下被忽视。在这种特殊情况下，指标可以捕获该按钮的每小时、每天和每周的点击率。最重要的是，它可以帮助讲述今天网站如何比去年同月份产生更多（或者更少！）收入的故事。服务器的磁盘使用情况和内存消耗值得关注，但这不是最终目标。
- en: These stories aren’t explicitly tied to machine learning or delivering trained
    models to production. Still, as you will see in this chapter, it will help you
    and your company tell a story and increase confidence in your process by surfacing
    important issues and pointing at the reason why a model probably needs better
    data before hitting production. Identifying data drift and accuracy over time
    is critical in ML operations. Deploying a model into production with a significant
    change in accuracy should never happen and needs prevention. The earlier these
    problems are detected, the cheaper it is to fix them. The consequences of having
    an inaccurate model in production can be catastrophic.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些故事并没有明确与机器学习或将训练模型交付到生产环境有关。然而，正如您将在本章中看到的那样，它将帮助您和您的公司讲述一个故事，并通过揭示重要问题和指出模型可能需要更好数据之前的原因来增强您的过程的信心。在
    ML 运营中，随着时间的推移识别数据漂移和准确性至关重要。在生产环境中部署一个准确性显著变化的模型绝不能发生，并需要进行预防。一旦检测到这些问题，早期解决将更加经济。在生产中存在不准确的模型的后果可能是灾难性的。
- en: Observability for Cloud MLOps
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观测性能用于云 MLOps
- en: It is safe to say that most machine learning is taking place in a cloud environment.
    As such, there are special services from cloud providers that enable observability.
    For example, on AWS, they have [Amazon CloudWatch](https://oreil.ly/43bvM), on
    GCP they have the [Google Cloud operations suite](https://oreil.ly/3YVn1), and
    on Azure they have [Azure Monitor](https://oreil.ly/gDIud).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 可以肯定地说，大多数机器学习是在云环境中进行的。因此，云提供商提供了特殊的服务来实现可观测性。例如，在AWS上，他们有[Amazon CloudWatch](https://oreil.ly/43bvM)，在GCP上有[Google
    Cloud operations suite](https://oreil.ly/3YVn1)，在Azure上有[Azure Monitor](https://oreil.ly/gDIud)。
- en: In [Figure 6-1](#Figure-6-0-1), Amazon CloudWatch is a better example of how
    these monitoring services work. At a high level, every component of the cloud
    system sends metrics and logs to CloudWatch. These tasks include the servers,
    the application logs, the training metadata for machine learning jobs, and the
    results of production machine learning endpoints.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 6-1](#Figure-6-0-1)中，Amazon CloudWatch是展示这些监控服务如何工作的一个很好的例子。在高层次上，云系统的每个组件都将指标和日志发送到CloudWatch。这些任务包括服务器、应用程序日志、用于机器学习作业的训练元数据以及生产机器学习端点的结果。
- en: '![pmlo 0601](Images/pmlo_0601.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0601](Images/pmlo_0601.png)'
- en: Figure 6-1\. AWS CloudWatch
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. AWS CloudWatch
- en: Next, this information becomes part of many different tools, from dashboards
    to auto-scaling. For example, in AWS SageMaker, this could mean that the production
    ML service will scale automatically if the individual endpoints exceed more than
    75% of their total CPU or memory. Finally, all of this observability allows humans
    and machines alike to analyze what is going on with a production ML system and
    act on it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这些信息将成为许多不同工具的一部分，从仪表板到自动扩展。例如，在AWS SageMaker中，这意味着如果个别端点的CPU或内存超过其总量的75%，生产ML服务将自动扩展。最后，所有这些可观测性都允许人类和机器分析生产ML系统的运行情况并采取行动。
- en: The experienced cloud software engineer already knows that cloud observability
    tools are nonoptional components of a cloud software deployment. However, what
    is unique about MLOps in the cloud is that new components also need granular monitoring.
    For example, in [Figure 6-2](#Figure-6-0-2), a whole new series of actions take
    place in an ML model deployment. Notice, though, that again, CloudWatch collects
    these new metrics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 经验丰富的云软件工程师已经知道，云可观测工具是云软件部署中不可选的组件。然而，在云中的MLOps独特之处在于新组件也需要精细的监控。例如，在[图 6-2](#Figure-6-0-2)中，ML模型部署中发生了一系列全新的操作。请注意，CloudWatch再次收集了这些新的指标。
- en: '![pmlo 0602](Images/pmlo_0602.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0602](Images/pmlo_0602.png)'
- en: Figure 6-2\. AWS SageMaker model monitoring
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. AWS SageMaker模型监控
- en: 'As the rest of the chapter unfolds, keep in mind that at a high level on a
    master system like CloudWatch, it collects data, routes alerts, and engages with
    the dynamic components of cloud computing like elastic scaling. Next, let’s get
    into the details of a finer-grained member of observability: logging.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章的进展，请记住，在像CloudWatch这样的主系统上，在高层次上，它收集数据，路由警报，并与弹性扩展等云计算的动态组件进行交互。接下来，让我们深入了解更精细的可观测性成员：日志记录。
- en: Introduction to Logging
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录简介
- en: Most logging facilities have common aspects in how they work. Systems define
    the log levels they can operate with, and then the user can select at what level
    those statements should appear. For example, the Nginx web server comes with a
    default configuration for access logs saved at */var/log/nginx/access.log* and
    error logs to */var/log/nginx/error.log*. As an Nginx user, the first thing you
    need to do if you are troubleshooting the web server is to go into those fails
    and see the output.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数日志记录设施在其工作方式上具有共同的特征。系统定义了它们可以操作的日志级别，然后用户可以选择这些语句应出现在何种级别。例如，Nginx web服务器默认配置为将访问日志保存在*/var/log/nginx/access.log*，错误日志保存在*/var/log/nginx/error.log*。作为Nginx用户，如果您在解决Web服务器问题，首先要做的事情就是进入这些文件并查看输出。
- en: 'I installed Nginx in an Ubuntu server with default configurations and sent
    some HTTP requests. Right away, the access logs started getting some information:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我在Ubuntu服务器上使用默认配置安装了Nginx，并发送了一些HTTP请求。立即，访问日志开始获取一些信息：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The long log lines pack lots of useful (configurable) information that includes
    the server’s IP address, the time, and type of request alongside the user-agent
    information. The user-agent in my case is my browser running on a Macintosh computer.
    These aren’t errors, though. These lines show access to the server. To force Nginx
    to get into an error condition, I changed the permissions on a file to be unreadable
    by the server. Next, I sent new HTTP requests:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 长长的日志行包含了大量有用（可配置的）信息，包括服务器的IP地址、时间和请求类型，以及用户代理信息。在我的情况下，用户代理是运行在Macintosh计算机上的浏览器。然而，这些并不是错误。这些行显示的是对服务器的访问。为了强制Nginx进入错误条件，我改变了一个文件的权限，使其对服务器不可读。接下来，我发送了新的HTTP请求：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The log entry explicitly shows that the level of the information is of *“error.”*
    This allows a consumer, like myself, to identify the severity of the information
    produced by the web server. Back when I was a Systems Administrator and was getting
    started in necessary tasks like configuration and deployment of production environments,
    it wasn’t clear why these levels were useful. The main point for me was that I
    could identify an error from the logs’ informational content.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 日志条目明确显示信息级别为*“error”*。这使得像我这样的消费者能够识别出Web服务器生成的信息的严重性。当我是系统管理员时，刚开始进行配置和部署生产环境等必要任务时，我并不清楚为什么这些级别很有用。对我来说，主要的一点是我可以从日志的信息内容中识别出错误。
- en: Although the idea of making it easier for consumers to identify if an error
    is valid, it isn’t all there is to log levels. If you’ve tried to debug a program
    before, you’ve probably used `print()` statements to help you get useful information
    about a running program. There are many other ways to debug programs, but using
    `print()` is still valuable. One drawback is that you have to clean up and remove
    all those `print()` statements once the problem is solved. The next time you need
    to debug the same program, you will need to add all those statements back. This
    is not a good strategy, and it is one of the many situations where logging can
    help.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然让消费者更容易识别错误是否有效的想法是有效的，但日志级别并不仅限于此。如果你以前尝试过调试程序，可能已经使用过`print()`语句来帮助获取运行程序的有用信息。有许多其他调试程序的方法，但使用`print()`仍然是有价值的。一个缺点是，一旦问题解决，你必须清理和删除所有`print()`语句。下次再需要调试同一个程序时，你又需要添加所有这些语句。这不是一个好策略，也是日志记录可以帮助解决的许多情况之一。
- en: Now that some basics on logging are clear, I’ll discuss how to configure logging
    in an application, which tends to be complicated since there are so many different
    options and decisions to make.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在基本的日志记录原理已经清楚了，接下来我将讨论如何在应用程序中配置日志记录，这往往是复杂的，因为有很多不同的选项和决策需要做。
- en: Logging in Python
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的日志记录
- en: I’ll be working in Python, but most of the concepts in this section should apply
    cleanly to other languages and frameworks. Log levels, redirection of output,
    and other facilities are commonly available in other applications. To start applying
    logging, I will create a short Python script to process a CSV file. No functions
    or modules; the example script is as close to a Jupyter Notebook cell that you
    can find.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在Python中进行工作，但本节中的大多数概念应该可以干净地应用到其他语言和框架中。日志级别、输出重定向和其他功能在其他应用程序中通常也是可用的。为了开始应用日志记录，我将创建一个简短的Python脚本来处理一个CSV文件。没有函数或模块；示例脚本尽可能接近Jupyter
    Notebook单元格。
- en: 'Create a new file called *describe.py* to see how logging can help in a basic
    script:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为*describe.py*的新文件，看看日志记录如何帮助基本脚本：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The script will take the input from the last argument on the command line and
    tell the Pandas library to read it and describe it. The idea is to produce a description
    of a CSV file, but that isn’t what happens when you run it with no arguments:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本将从命令行上的最后一个参数中获取输入，并告诉Pandas库读取并描述它。这个想法是产生一个CSV文件的描述，但当你没有参数运行时并不会发生这种情况：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'What happens here is that the last argument in that example is the script itself,
    so Pandas is describing the contents of the script. This is not very useful, and
    to someone that hasn’t created the script, the results are shocking, to say the
    least. Let’s take this brittle script a step forward and pass an argument to a
    path that doesn’t exist:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的是，示例中的最后一个参数是脚本本身，因此Pandas正在描述脚本的内容。这对于那些没有创建过脚本的人来说并不是非常有用，结果至少会让人感到震惊。让我们将这个脆弱的脚本向前推进一步，并传递一个不存在的路径作为参数：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'There is no error checking to tell us if the input is valid and what the script
    expects. This problem is worse if you are waiting on a pipeline run or some remote
    data processing job to complete, and you are getting these types of errors. Some
    developers try to defend against these by catching all exceptions and obscuring
    the actual error, making it impossible to tell what is going on. This slightly
    modified version of the script highlights the problem better:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 没有错误检查告诉我们输入是否有效以及脚本期望的内容。如果正在等待管道运行或某些远程数据处理作业的完成，并且出现这些类型的错误，问题会更加严重。一些开发人员试图通过捕获所有异常并掩盖实际错误来防范这些问题，使得无法了解正在发生的情况。脚本的稍作修改版本更突显了问题：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Running it produces an error that would make me very upset to see in production
    code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 运行它会产生一个错误，在生产代码中看到这种情况会让我非常沮丧：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The example is trivial, and because the script is just a few lines long and
    you know its contents, it isn’t that difficult to point at the problem. But if
    this is running in an automated pipeline remotely, you don’t have any context,
    and it becomes challenging to understand the problem. Let’s use the Python logging
    module to provide more information about what is going on in this data processing
    script.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子很琐碎，因为脚本只有几行长，你知道它的内容，所以指出问题并不那么困难。但如果这是在一个远程的自动化流水线中运行，你没有任何上下文信息，理解问题就变得更具挑战性。让我们使用Python日志模块来提供更多关于这个数据处理脚本正在发生的事情的信息。
- en: 'The first thing to do is to configure logging. We don’t need anything too complicated
    here, and adding a few lines is more than enough. Modify the *describe.py* file
    to include these lines, then rerun the script:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是配置日志记录。在这里我们不需要太复杂的东西，添加几行就足够了。修改*describe.py*文件以包括这些行，然后重新运行脚本：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Rerunning it should look similar to this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 重新运行它应该看起来类似于这样：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Still, not very useful yet, but already informational. This might feel like
    a lot of boilerplate code for something that a simple `print()` statement could’ve
    also accomplished. The logging module is resilient to failures when constructing
    the message. For example, open a Python interpreter and try using fewer arguments
    to `print`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目前还不是非常有用，但已经包含了信息。对于一些简单的`print()`语句也能达到的效果，可能会感觉有点冗长。当构建消息时，日志模块对于失败是具有弹性的。例如，打开Python解释器并尝试使用更少的参数来调用`print`：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let’s try the same operation with the logging module:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用日志模块进行相同的操作：
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The last example would not break a production application. Logging should never
    break any application at runtime. In this case, the logging module is trying to
    do the variable replacement in the string and failing, but instead of breaking
    it is advertising the problem and then continuing. A print statement can’t do
    this. In fact, I see using `print()` in Python much like `echo` in shell scripts.
    There is no control, it is easy to break a production application, and it is hard
    to control the verbosity.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个例子不会破坏生产应用程序。记录日志在任何运行时都不应该破坏任何应用程序。在这种情况下，日志模块试图在字符串中进行变量替换并失败，但它并没有导致程序崩溃，而是通告了问题然后继续执行。`print`语句无法做到这一点。事实上，我认为在Python中使用`print()`很像在shell脚本中使用`echo`。它没有控制性，很容易破坏生产应用程序，并且很难控制详细程度。
- en: 'Verbosity is crucial when logging, and aside from the *error level* in the
    Nginx example, it has several facilities to empower log consumers to fine-tune
    the information needed. Before going into log-level granularity and verbosity
    control, give the script an upgrade in the log formatting to look nicer. The Python
    logging module is compelling and allows lots of configuration. Update the *describe.py*
    script where the logging configuration happens:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在日志记录时详细程度非常重要，除了Nginx示例中的错误级别外，它还具有多种设施来使日志消费者能够精细调节所需的信息。在深入探讨日志级别的细粒度和详细程度控制之前，给脚本的日志格式升级，使其看起来更加美观。Python日志模块非常强大，允许大量配置。更新发生日志配置的*describe.py*脚本：
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `log_format` is a template with some keywords used when constructing the
    log line. Note how I didn’t have a timestamp before, and although I still don’t
    have it with this update, the configuration does allow me to include it. For now,
    the logger’s name (`describe` in this case), the log level, and the message are
    all there with some padding and using square brackets to separate the information
    for better readability. Rerun the script once again to check how the output changes:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`log_format`是一个模板，其中包含构建日志行时使用的一些关键字。请注意，我以前没有时间戳，尽管我现在仍然没有它，但配置确实允许我包括它。目前，记录器的名称（在本例中为`describe`）、日志级别和消息都在那里，并使用方括号进行分隔，以便更好地阅读。再次运行脚本以检查输出如何变化：'
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Another superpower of logging is to provide the traceback information along
    with the error. Sometimes it is useful to capture (and show) the traceback without
    getting into an error condition. To do this, update the *describe.py* script in
    the `except` block:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 日志的另一个超级功能是在错误信息中提供追溯信息。有时捕获（并显示）追溯信息是有用的，而不需要陷入错误条件。为了实现这一点，在`except`块中更新*describe.py*脚本：
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It looks very similar to the `print()` statement from before. Rerun the script
    and check the results:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来与之前的`print()`语句非常相似。重新运行脚本并检查结果：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The traceback is the string representation of the error, not an error itself.
    To verify this, add another log line right after the `except` block:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 追溯是错误的字符串表示，而不是错误本身。为了验证这一点，在`except`块后添加另一行日志：
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Rerun to verify the outcome:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 重新运行以验证结果：
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Modifying Log Levels
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改日志级别
- en: 'These types of informational facilities provided by logging are not straightforward
    with `print()` statements. If you are writing a shell script, it would be borderline
    impossible. In the example, we now have three log levels: debug, error, and info.
    Another thing logging allows is to selectively set the level to what we are interested
    in. Before changing it, the levels have a *weight* associated with them, and it
    is essential to grasp them so that changes reflect the priority. From most to
    least verbose, the order is as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 日志提供的这些信息设施与`print()`语句并不直接。如果你在编写一个Shell脚本，这几乎是不可能的。在这个例子中，我们现在有三个日志级别：debug、error和info。日志允许的另一件事情是选择性地设置我们感兴趣的级别。在更改之前，这些级别有与之相关联的*权重*，理解它们以便更改能够反映出优先级是很重要的。从最详细到最少详细，顺序如下：
- en: '`debug`'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`debug`'
- en: '`info`'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`info`'
- en: '`warning`'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`warning`'
- en: '`error`'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`error`'
- en: '`critical`'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`critical`'
- en: 'Although this is the Python logging module, you should expect a similar weighted
    priority from other systems. A log level of `debug` will include every other level,
    as well as `debug`. A `critical` log level will include only `critical`-level
    messages. Update the script once again to set the log level to `error`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是Python的日志模块，但你应该期待其他系统中类似的加权优先级。`debug`日志级别将包含所有其他级别，以及`debug`级别本身。`critical`日志级别仅包含`critical`级别的消息。再次更新脚本将日志级别设置为`error`：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The change causes only the error log message to be displayed. This is useful
    when trying to reduce the amount of logging to what may be of interest. Debugging
    encompasses most (if not all) messages, while errors and critical situations are
    much more sporadic and usually not seen as often. I recommend setting debug log
    levels for new production code so that it is easier to catch potential problems.
    You should expect problems when producing and deploying new code. Highly verbose
    output is not that useful in the long run when an application has proven stable,
    and there aren’t many surprising issues. Fine-tuning the levels to `info` or `error`
    only is something you can do progressively.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此更改导致仅显示错误日志消息。当试图减少日志量以关注感兴趣的内容时，这是非常有用的。调试包含大多数（如果不是所有）消息，而错误和关键情况则要少得多且通常不会经常出现。我建议为新的生产代码设置调试日志级别，以便更容易捕捉潜在问题。在生成和部署新代码时，你应该预期会出现问题。当应用程序已经被证明是稳定的，并且没有太多令人惊讶的问题时，过于冗长的输出在长期内并不那么有用。逐步将级别调整为仅`info`或`error`是你可以做的事情。
- en: Logging Different Applications
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记录不同的应用程序
- en: So far, we’ve seen log levels of a script trying to load a CSV file. And I haven’t
    gone into the details of why the logger’s name (*“describe”* in the past examples)
    is significant. In Python, you import modules and packages, and many of these
    packages come with their own loggers. The logging facility allows you to set particular
    options for these loggers independently from your application. There is also a
    hierarchy for loggers, where the *“root”* logger is the parent of all loggers
    and can modify settings for all applications and loggers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到一个试图加载 CSV 文件的脚本的日志级别。我还没有详细说明为什么记录器的名称（在过去的示例中为“describe”）很重要。在
    Python 中，您导入模块和包，许多这些包都带有自己的记录器。日志记录设施允许您独立于应用程序为这些记录器设置特定选项。还有一个记录器的层次结构，其中 *“root”*
    记录器是所有记录器的父记录器，并且可以修改所有应用程序和记录器的设置。
- en: 'Changing the logging level is one of the many things you can configure. In
    one production application, I created two loggers for the same application: one
    would emit messages to the terminal, while the other would write to a logfile.
    This allowed having user-friendly messages go to the terminal, omitting large
    tracebacks and errors polluting the output. At the same time, the internal, developer-oriented
    logging would go to a file. This is another example of something that would be
    very difficult and complicated to do with a `print()` statement or an `echo` directive
    in a shell script. The more flexibility you have, the better you can craft applications
    and services.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 更改日志级别是您可以配置的众多功能之一。在一个生产应用程序中，我为同一应用程序创建了两个记录器：一个将消息输出到终端，而另一个将消息写入日志文件。这允许将用户友好的消息发送到终端，省略大型回溯和错误污染输出。同时，内部面向开发人员的日志记录将写入文件。这是另一个使用
    `print()` 语句或 shell 脚本中的 `echo` 指令非常难以实现和复杂的示例。您拥有的灵活性越大，您就可以更好地构建应用程序和服务。
- en: 'Create a new file and save it as *http-app.py* with the following contents:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为 *http-app.py* 的新文件，并保存以下内容：
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The script configures the logging facility with a basic configuration that
    emits messages to the terminal by default. It then tries to make a request using
    the *requests* library. Run it and check the output. It might feel surprising
    that nothing shows up in the terminal after executing the script. Before explaining
    exactly why this is happening, update the script:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本使用基本配置配置日志记录设施，默认情况下将消息输出到终端。然后尝试使用 *requests* 库发出请求。运行它并检查输出。执行脚本后在终端中什么也不显示可能会让人感到意外。在详细解释发生这种情况的确切原因之前，更新脚本：
- en: '[PRE21]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Rerun the script and take note of the output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 重新运行脚本并注意输出：
- en: '[PRE22]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The Python logging module can set configuration settings globally for every
    single logger in all packages and modules. This *parent* logger is called the
    *root* logger. The reason there is output is that the changes set the root logger
    level to debug. But there is more output than the single line of the *http-app*
    logger. This happens because the *urllib3* package has its logger as well. Since
    the root logger changed the global log level to debug, the *urllib3* package is
    now emitting those messages.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Python 日志模块可以全局设置每个包和模块中的每个记录器的配置设置。这个 *parent* 记录器称为 *root* 记录器。有输出的原因是更改了根记录器级别为
    debug。但是输出不止是单行 *http-app* 记录器。这是因为 *urllib3* 包也有自己的记录器。由于根记录器将全局日志级别更改为 debug，因此
    *urllib3* 包现在正在发出这些消息。
- en: 'It is possible to configure several different loggers and fine-tune the granularity
    and verbosity of the levels (as well as any other logging configuration). To demonstrate
    this, change the *urllib3* package’s logging level by adding these lines at the
    end of the *http-app.py* script:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可以配置多个不同的记录器，并调整级别的细粒度和详细程度（以及任何其他日志记录配置）。为了演示这一点，在 *http-app.py* 脚本的末尾添加以下行来更改
    *urllib3* 包的日志级别：
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The updated version retrieves the logger for *urllib3* and changes its log
    level to error. The script’s logger emits a new message before calling `requests.get()`,
    which in turn uses the *urllib3* package. Rerun the script once more to check
    the output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 更新版本检索 *urllib3* 的日志记录器，并将其日志级别更改为错误。在调用 `requests.get()` 之前，脚本的日志记录器发出了一条新消息，而
    `requests.get()` 又使用了 *urllib3* 包。再次运行脚本以检查输出：
- en: '[PRE24]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Since the log level of *urllib3* got changed before the last request, no more
    debug messages appear. The info-level message does show up because that logger
    is still configured at a debug level. These combinations of logging configurations
    are powerful because it allows you to select what is interesting from other information
    that can cause *noise* in the output.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在上次请求之前更改了*urllib3*的日志级别，不再显示调试消息。信息级别的消息确实显示，因为该记录器仍然配置为调试级别。这些日志配置的组合非常强大，因为它允许您选择从其他可能导致输出“噪音”的信息中选择感兴趣的内容。
- en: Imagine using a library that interacts with a cloud’s storage solution. The
    application you are developing performs thousands of interactions by downloading,
    listing, and uploading content to the storage server. Suppose the primary concern
    of the application is to manage datasets and offload them to the cloud provider.
    Do you think it would be interesting to see an informational message that says
    a request is about to be made to the cloud provider? In most situations, I would
    say that is far from useful. Instead, it would be critical to be alerted when
    the application fails to perform a particular operation with the storage. Further,
    it may be possible that you are dealing with timeouts when requesting the storage,
    in which case, changing the log level to indicate when the request happens is
    crucial to get the time delta.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下使用与云存储解决方案互动的库。您正在开发的应用程序通过下载、列出和上传内容到存储服务器来执行成千上万次的交互。假设应用程序的主要关注点是管理数据集并将其卸载到云提供商。您认为看到一条信息性消息，说一个请求即将发送到云提供商，会有趣吗？在大多数情况下，我会说这远非有用之处。相反，当应用程序未能执行与存储相关的特定操作时发出警报至关重要。此外，可能出现请求存储时超时的情况，此时改变日志级别以指示请求发生的时间差至关重要。
- en: It is all about flexibility and adapting to your application’s lifecycle needs.
    What is useful today can be information overload tomorrow. Logging goes hand-in-hand
    with monitoring (covered in the next section), and it is not uncommon to hear
    about observability in the same conversation. These are foundational to DevOps,
    and they should be part of the ML lifecycle.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都是关于灵活性和适应您应用程序生命周期需求的能力。今天有用的东西明天可能成为信息过载。日志记录与监控（在下一节中介绍）密切相关，谈论到可观察性也并非不寻常。这些是DevOps的基础，应该成为ML生命周期的一部分。
- en: Monitoring and Observability
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控与可观察性
- en: 'When I was a professional athlete, my dad, who was also my coach, added a particular
    chore that I despised: write every day, in a journal, about the workout that I’d
    just completed. The journal entry needed the following items:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾是一名职业运动员，我的父亲，也是我的教练，为我增加了一项特别讨厌的任务：每天在日记中写关于刚刚完成的训练内容。日记条目需要包括以下内容：
- en: The planned workout. For example, if it was 10 repetitions of 300 meters at
    a 42 second pace.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计划的训练内容。例如，如果是以42秒的速度进行10次300米的重复训练。
- en: The results of the workout. For the 300 meters, it would be the actual time
    performed at each repetition.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练结果。对于300米的距离，需要记录每次重复的实际完成时间。
- en: How I felt during and after the session.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中和之后的感受。
- en: Any other relevant information, like feeling sick or pain from an injury, for
    example.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他相关信息，比如感觉生病或因伤痛感到疼痛等。
- en: I started training professionally at 11 years old. As a teenager, this journaling
    task felt worse than the worst workouts. I didn’t understand the big deal about
    journaling and why it was vital for me to cover the workout details. I had the
    guts to tell my dad that this seemed his job, not mine. After all, I was already
    doing the workouts. That argument didn’t go very well for me. The problem is that
    I didn’t understand. It felt like a useless task, without any benefits. I couldn’t
    see or feel the benefits.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我11岁开始专业训练。作为一个十几岁的少年，这项写日记的任务比最糟糕的训练还要糟糕。我不明白为什么写日记如此重要，为什么必须详细记录训练内容。我有勇气告诉父亲，这似乎是他的工作，而不是我的。毕竟，我已经在做训练了。这个论点对我来说并不是一个很好的答复。问题在于我没有理解。这似乎是一项毫无用处的任务，没有任何好处。我无法看到或感受到其中的好处。
- en: It felt more like a disciplinary task at the end of the day instead of a crucial
    training aspect. A few years after journaling every day (I worked out 14 times
    a week on average) and having a great season behind me, I sat down with my dad
    to plan the next season. Instead of a couple of hours of hearing my dad talk to
    me about what was coming for the next season, he illuminated me by demonstrating
    how powerful journaling was. *“Alright, Alfredo, let me pull up the last two journals,
    to check what we did and how you felt, to adapt, increase, and have a great season
    once again.”*
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到了一天结束时，感觉更像是一个纪律性的任务，而不是一个关键的训练环节。在每天都记录日志几年后（我平均每周锻炼14次）并且在我身后有一个很棒的赛季后，我和父亲坐下来计划下一个赛季。而不是听我爸爸谈论接下来赛季要做什么几个小时，他启发了我，展示了记日志的强大之处。*“好的，阿尔弗雷多，让我查看最近的两本期刊，看看我们做了什么以及你的感受，以便调整、增加并再次度过一个伟大的赛季。”*
- en: 'The journals had every piece of information that we needed to plan. He used
    a phrase that stuck with me for years, and I hope it demonstrates why monitoring
    and metrics are critical in any situation: *“If we can measure, then we can compare.
    And if we can compare, only then can we improve.”*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些期刊中包含了我们计划所需的每一条信息。他用了一句话让我印象深刻多年，我希望这能说明为什么在任何情况下监控和指标都至关重要：*“如果我们能够衡量，那么我们就能比较。而如果我们能比较，那么我们才能改进。”*
- en: Machine learning operations are no different. When a new iteration of a model
    ships to production, you have to know if it performs better or worse. Not only
    do you have to know, but the information has to be accessible and straightforward
    to produce. Processes can create friction and make everything go slower than it
    should. Automation brings down silly processes and can effortlessly create information
    availability.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习运营也不例外。当一个模型的新迭代被送到生产中时，你必须知道它的表现是好还是差。不仅如此，信息必须是易于获取和简单明了的。流程可能会制造摩擦，使一切比应该慢。自动化可以消除愚蠢的流程，并轻松创建信息的可用性。
- en: A few years ago, I worked at a startup where the head of sales would send me
    a CSV file with new accounts to “run some numbers” on it and send him a PDF back
    with the results. This is horrible; it doesn’t scale. And it doesn’t survive the
    bus test.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，我在一家初创公司工作，销售主管会把一个CSV文件发送给我，让我“处理一些数据”，然后把结果以PDF格式发回给他。这太糟糕了，不具备可扩展性。也不会通过“巴士测试”。
- en: The bus test is where I can get hit by a bus today, and everything should still
    work, and everyone that works with me can pick up the slack. All effort in automation
    and producing metrics is instrumental for shipping robust models to production.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 巴士测试是如果今天我被公共汽车撞了，一切都应该还能正常运行，与我共事的每个人都能够接管我的工作。自动化和生成指标的所有努力对于将健壮的模型交付到生产中至关重要。
- en: Basics of Model Monitoring
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型监控的基础
- en: Monitoring in ML operations means everything and anything that has to do with
    getting a model into production—from capturing information about the systems and
    services to the performance of the model itself. There is no single silver bullet
    to implement monitoring that will make everything right. Monitoring and capturing
    metrics is somewhat similar to knowing the data before figuring out what features
    and algorithms to use to train a model. The more you know about the data, the
    better decisions you can make about training the model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习运营中，监控意味着任何与将模型投入生产有关的一切——从捕获系统和服务信息到模型本身的性能。没有一种单一的灵丹妙药可以实施监控以使一切正常。监控和捕获指标有些类似于在确定要使用哪些特征和算法来训练模型之前了解数据。你了解的数据越多，你在训练模型时做出的决策就越好。
- en: 'Similarly, the more you know about the steps involved in getting a model to
    production, the better the choices you make when capturing metrics and setting
    monitoring alerts. The answer to *“what metric should you capture when training
    a model?”* is one that I dislike, but is so accurate in this situation: it depends.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你了解将模型投入生产所涉及的步骤越多，当捕获指标和设置监控警报时，你做出的选择就越好。对于“在训练模型时应该捕获什么指标？”的答案我并不喜欢，但在这种情况下确实如此准确：这取决于情况。
- en: Although there are differences between the metrics and types of alerts you can
    set, there are some useful foundational patterns that you can default to. These
    patterns will help you clarify what data to collect, how often that collection
    should occur, and how to best visualize it. Finally, depending on the step in
    producing a model, the key metrics will be different. For example, when collecting
    data and cleaning it up, it might be substantial to detect the number of empty
    values per column and perhaps the time to completion when processing the data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管度量和设置警报类型之间存在差异，但有一些有用的基础模式可以作为默认选择。这些模式将帮助您澄清要收集的数据，收集频率以及如何最佳可视化它们。最后，根据生成模型的步骤不同，关键的度量标准也会有所不同。例如，在收集数据并清理数据时，检测每列的空值数量以及处理数据时的完成时间可能非常重要。
- en: The past week I was processing system vulnerability data. And I had to make
    some changes to the application in charge of reading JSON files and save the information
    in a database. After some changes, the database went from several gigabytes in
    size to just 100 megabytes. The code change wasn’t meant to reduce the size at
    all, so I immediately knew that I’d made a mistake that needed correction. Encountering
    these situations are excellent opportunities to determine what metrics (and when)
    to capture them.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 上个星期我在处理系统漏洞数据。我不得不对负责读取 JSON 文件并将信息保存到数据库的应用程序进行一些更改。在一些更改之后，数据库的大小从几个千兆字节减小到了只有
    100 兆字节。代码更改本意并不是为了减小大小，所以我立刻意识到自己犯了一个需要修正的错误。遇到这些情况是确定何时以及如何捕获这些度量的绝佳机会。
- en: 'There are a few types of metrics that you can find in most metric-capturing
    systems:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数度量捕获系统中可以找到的度量类型有几种：
- en: Counter
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 计数器
- en: As the name implies, this type of metric can be useful when counting any type
    of item. It is useful when iterating over items. For example, this could be useful
    for counting empty cell values per column.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，这种类型的度量在计数任何类型的项时非常有用。在迭代项时尤为有用。例如，这可能对于计算每列的空单元格值非常有用。
- en: Timer
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 计时器
- en: A timer is excellent when trying to determine how long some action will take.
    This is crucial for performance monitoring, as its core responsibility functionality
    is to measure time spent during an action. Time spent is commonly seen in monitoring
    graphs for hosted HTTP APIs. If you have a hosted model, a timer would help capture
    how long the model took to produce a prediction over HTTP.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试确定某些动作需要多长时间时，计时器是非常好的选择。这对性能监控至关重要，因为其核心功能是测量动作期间所花费的时间。通常在托管 HTTP API 的监控图表中可以看到时间花费。如果您有托管模型，计时器将帮助捕获模型通过
    HTTP 生成预测所需的时间。
- en: Value
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 值
- en: 'When counters and timers do not fit the metric to capture, values come in handy.
    I tend to think of values like parts of an algebra equation: even if I don’t know
    what *X* is, I want to capture its value and persist it. Reusing the example of
    processing JSON files at work and saving information to a database, a fair use
    for this metric would be the size (in gigabytes) of the resulting database.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当计数器和计时器不适合捕获度量时，值变得很有用。我倾向于将值视为代数方程的一部分：即使我不知道 *X* 是什么，我也希望捕获其值并持久化。重复利用在工作中处理
    JSON 文件并将信息保存到数据库时，这种度量的合理用途可能是生成的数据库大小（以千兆字节为单位）。
- en: There are two common operations specific to ML that cloud providers need to
    monitor and capture useful metrics. The first is the target dataset. This can
    be the same dataset you used to train a model, although special care needs to
    be put into ensuring that the number (and order) of features doesn’t change. The
    second one is the baseline. The baseline determines what differences may (or may
    not) be acceptable when a model gets trained. Think about baseline as the acceptable
    thresholds to determine how fit a model is for production usage.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种常见的与 ML 相关的操作，云服务提供商需要监视和捕获有用的度量。第一种是目标数据集。这可以是您用来训练模型的同一数据集，尽管需要特别注意确保特征的数量（和顺序）不变。第二种是基线。基线确定了在训练模型时可能（或可能不）接受的差异。将基线视为确定模型在生产环境中适用程度的可接受阈值。
- en: Now that the basics are clear and that a target dataset with a baseline is understood,
    let’s use them to capture useful metrics when training models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在基础知识已经清楚，并且理解了带有基线的目标数据集，让我们在训练模型时使用它们来捕获有用的度量。
- en: Monitoring Drift with AWS SageMaker
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS SageMaker 监控漂移
- en: As I mentioned already, cloud providers will usually need a target dataset and
    a baseline. This is no different with AWS. In this section, we will produce metrics
    and capture data violations from an already deployed model. SageMaker is an incredible
    tool for inspecting datasets, training models, and provisioning models into production
    environments. Since SageMaker tightly integrates with other AWS offerings like
    S3 storage, you can leverage saving target information that can quickly be processed
    elsewhere that has access.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，云提供商通常需要一个目标数据集和一个基线。AWS 也不例外。在本节中，我们将生成指标并从已部署的模型中捕获数据违规。SageMaker
    是检查数据集、训练模型并将模型提供到生产环境的不可思议工具。由于 SageMaker 与其他 AWS 提供的服务（如 S3 存储）紧密集成，您可以利用保存目标信息，以便在其他地方快速处理并具有访问权限。
- en: One thing in particular that I like about SageMaker is its Jupyter Notebook
    offering. The interface is not as polished as Google’s Colab, but it packs features
    like the AWS SDK preinstalled and substantial kernel types to choose from—from
    (the now deprecated) Python 2.7 to Conda environments running Python 3.6, as shown
    in [Figure 6-3](#Figure-6-1).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 一个我特别喜欢的地方是它的 Jupyter Notebook 提供。界面不像 Google 的 Colab 那样精致，但它装备了 AWS
    SDK 预安装和大量的内核类型可供选择——从（现已弃用的）Python 2.7 到运行 Python 3.6 的 Conda 环境，如 [图 6-3](#Figure-6-1)
    所示。
- en: Note
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This section doesn’t cover the specifics of deploying a model. If you want to
    dive into model deployment in AWS, see [Chapter 7](ch07.xhtml#Chapter7).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本节不涵盖部署模型的具体细节。如果您想深入了解 AWS 中的模型部署，请参阅 [第 7 章](ch07.xhtml#Chapter7)。
- en: '![pmlo 0603](Images/pmlo_0603.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0603](Images/pmlo_0603.png)'
- en: Figure 6-3\. SageMaker kernels
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. SageMaker 内核
- en: Use a SageMaker notebook for this section. Log in to the AWS console, and find
    the SageMaker service. Once that is loaded, on the left column, find the Notebook
    Instances link and click it. Create a new instance with a meaningful name. There
    is no need to change any of the defaults, including the machine type. I’ve named
    my notebook *practical-mlops-monitoring* (see [Figure 6-4](#Figure-6-2)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SageMaker 笔记本完成本节内容。登录 AWS 控制台，并找到 SageMaker 服务。加载后，在左侧列找到“笔记本实例”链接并点击。创建一个具有有意义名称的新实例。无需更改任何默认设置，包括机器类型。我将笔记本命名为
    *practical-mlops-monitoring*（见 [图 6-4](#Figure-6-2)）。
- en: 'When deploying the model, it is important to enable capturing of data. So make
    sure you use the `DataCaptureConfig` class to do so. This is a quick example that
    will save it to an S3 bucket:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署模型时，启用数据捕获非常重要。因此，请确保使用 `DataCaptureConfig` 类来执行此操作。以下是一个快速示例，将其保存到 S3 存储桶中：
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![pmlo 0604](Images/pmlo_0604.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0604](Images/pmlo_0604.png)'
- en: Figure 6-4\. SageMaker notebook instance
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. SageMaker 笔记本实例
- en: 'Use the `data_capture_config` when calling `model.deploy()`. In this example,
    I’ve previously created a `model` object using the `Model()` class, and assigned
    the data capture configuration to it, so that when the model gets exercised, the
    data gets saved to the S3 bucket:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用 `model.deploy()` 时，请使用 `data_capture_config`。在此示例中，我已经使用 `Model()` 类创建了一个
    `model` 对象，并将数据捕获配置分配给它，因此当模型被执行时，数据将保存到 S3 存储桶中。
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After the model is deployed and available, send some requests to start making
    predictions. By sending requests to the model, you are causing the capturing configuration
    to save critical data needed to create the baseline. You can send prediction requests
    to the model in any way. In this example, I’m using the SDK to send some requests
    using sample CSV data from a file. Each row represents data that the model can
    use to start predicting. Since the input is data, that is the reason I’m using
    a CSV deserializer, so that the endpoint understands how to consume that input:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署并可用后，发送一些请求以开始进行预测。通过向模型发送请求，您会导致捕获配置保存必要的数据，以创建基线所需的关键数据。您可以以任何方式向模型发送预测请求。在本例中，我使用
    SDK 使用来自文件的示例 CSV 数据发送一些请求。每行数据代表模型可以用来开始预测的数据。由于输入是数据，这就是我使用 CSV 反序列化程序的原因，以便端点了解如何处理该输入：
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After running, double-check that there is output captured in the S3 bucket.
    You can list the contents of that bucket to ensure there is actual data. In this
    case, I’m going to use the AWS command line tool, but you can use the web interface
    or the SDK (the method doesn’t matter in this case):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，请仔细检查 S3 存储桶中是否有输出被捕获。您可以列出该存储桶的内容以确保实际存在数据。在本例中，我将使用 AWS 命令行工具，但您也可以使用
    Web 界面或 SDK（在这种情况下，方法并不重要）：
- en: '[PRE28]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The bucket lists about 30 items, confirming that the prediction requests were
    successful and that data was captured and saved to the S3 bucket. Each file has
    a JSON entry with some information. The specifics in each entry are hard to grasp.
    This is what one of the entries looks like:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 存储桶列出约 30 个项目，确认了预测请求成功并且数据已经被捕获并保存到 S3 存储桶中。每个文件都有一个包含一些信息的 JSON 条目。每个条目的具体内容很难理解。一个条目看起来像这样：
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Once again, the entries reference the CSV content type throughout. This is crucial
    so other consumers of this data can consume the information correctly. So far,
    we configured the model to capture data and save it to an S3 bucket. This all
    happened after generating some predictions with test data. However, there is no
    baseline yet. The data captured in the previous step is required to create the
    baseline. The next step requires a target training dataset. As I’ve mentioned
    before, the training dataset can be the same one used to train the model. A subset
    of the dataset might be acceptable if the resulting model doesn’t change drastically.
    This *target dataset* has to have the same features (and in the same order) as
    the dataset used to train the production model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，条目在整个过程中引用 CSV 内容类型。这对其他数据消费者正确消费信息至关重要。到目前为止，我们已经配置模型来捕获数据并将其保存到 S3 存储桶中。这都是在使用测试数据生成一些预测之后发生的。然而，还没有基线。在之前的步骤中捕获的数据需要用于创建基线。下一步需要一个目标训练数据集。如我之前提到的，训练数据集可以是用于训练模型的相同数据集。如果生成的模型没有发生很大变化，则可能接受数据集的子集。这个*目标数据集*必须具有与用于训练生产模型的数据集相同的特征（并且顺序相同）。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It is common to find online documentation that refers to the *baseline dataset*
    interchangeably with *target dataset*, since both can initially be the same. This
    can make it confusing when trying to grasp some of these concepts. It is useful
    to think about the baseline dataset as the data used to create the gold standard
    (the baseline) and any newer data as the *target*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线文档中经常会发现将*基准数据集*与*目标数据集*互换使用，因为它们最初可能相同。这在试图理解这些概念时可能会令人困惑。将基线数据集视为用于创建基准（基线）的数据，将任何更新的数据视为*目标*是很有用的。
- en: 'SageMaker makes it easy to save and retrieve data by relying on S3\. I’ve defined
    locations throughout the SDK examples already, and for the baselining, I will
    do the same. Start by creating a monitor object; this object is able to produce
    a baseline and save it to S3:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 通过依赖 S3 轻松保存和检索数据。我已经在 SDK 示例中定义了各个位置，对于基线设定，我也会这样做。首先创建一个监视器对象；这个对象能够生成一个基线并将其保存到
    S3：
- en: '[PRE30]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that the monitor is available, use the `suggest_baseline()` method to produce
    a default baseline for the model:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在监视器可用，使用 `suggest_baseline()` 方法为模型生成一个默认基线：
- en: '[PRE31]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'When the run completes, a lot of output will get produced. The start of the
    output should be similar to this:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完成后，会产生大量输出。输出的开头应该类似于这样：
- en: '[PRE32]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'There should be two files saved in the configured S3 bucket: *constraints.json*
    and *statistics.json*. You can visualize the constraints with the Pandas library:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置的 S3 存储桶中应该保存有两个文件：*constraints.json* 和 *statistics.json*。你可以使用 Pandas 库可视化约束：
- en: '[PRE33]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This is a short subset of the constraints table that Pandas generates:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Pandas 生成的约束表的一个简短子集：
- en: '[PRE34]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now that we have a baseline made with a very similar dataset to the one used
    to train the production model, and we have captured relevant constraints, it is
    time to analyze it and monitor data drift. There have been several steps involved
    so far, but most of these steps will not change much from these examples to other
    more complex ones, which means there are many opportunities here to automate and
    abstract a lot. The initial collection of data happens once when setting the baseline,
    and then it shouldn’t change unless changes to the baseline are needed. These
    will probably not happen often, so the heavy lifting of setting the baseline shouldn’t
    feel like a burden.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经使用与训练生产模型相似的数据集制作了一个基线，并捕获了相关约束，现在是分析和监控数据漂移的时候了。到目前为止，已经涉及到几个步骤，但大多数这些步骤不会从这些示例到其他更复杂的示例进行很大变化，这意味着在这里有许多机会自动化和抽象化很多内容。首次收集数据发生在设置基线时，然后除非需要更改基准数据，否则不应更改。这种更改可能不经常发生，所以设置基线的繁重工作不应该感觉像负担。
- en: 'To analyze the collected data, we need a monitoring schedule. The example schedule
    will run every hour, using the baseline created in the previous steps to compare
    against traffic:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要分析收集的数据，我们需要一个监控计划。示例计划将每小时运行一次，使用在前面步骤中创建的基线与流量进行比较：
- en: '[PRE35]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Once you create the schedule, it will require traffic to generate reports. If
    the model is already in a production environment, then we can assume (and reuse)
    existing traffic. If you are testing out the baseline on a test model like I’ve
    done in these examples, you will need to generate traffic by invoking a request
    to the deployed model. A straightforward way to generate traffic is to reuse the
    training dataset to invoke the endpoint.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 创建计划后，需要流量生成报告。如果模型已经在生产环境中，我们可以假设（并重复使用）现有流量。如果您像我在这些示例中做的那样，在测试模型上测试基线，则需要通过调用请求来生成流量到部署的模型。生成流量的一种简单方法是重用训练数据集以调用端点。
- en: 'The model I deployed ran for a few hours. I used this script to generate some
    predictions from the previously deployed model:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我部署的模型运行了几个小时。我使用这个脚本从之前部署的模型生成了一些预测：
- en: '[PRE36]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Since I configured the monitoring schedule to capture every hour, SageMaker
    will not immediately generate reports onto the S3 bucket. After two hours, it
    is possible to list the S3 bucket and check if reports appear in there.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我配置了每小时运行一次的监控计划，SageMaker不会立即将报告生成到S3存储桶中。两小时后，可以列出S3存储桶并检查是否有报告出现。
- en: Note
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although the model monitor will run hourly, AWS has a 20-minute buffer that
    may cause a delay of up to 20 minutes after the hour mark. If you’ve seen other
    scheduling systems, this buffer might be surprising. This happens because, behind
    the scenes, AWS is load balancing the resources for scheduling.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型监控将每小时运行，但AWS有一个20分钟的缓冲区，可能会在整点后延迟高达20分钟。如果您看到其他调度系统，这种缓冲区可能会令人惊讶。这是因为在幕后，AWS正在为调度平衡资源。
- en: 'The reports consist of three JSON files:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 报告包括三个JSON文件：
- en: '*contraint_violations.json*'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*contraint_violations.json*'
- en: '*contraint.json*'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*contraint.json*'
- en: '*statistics.json*'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*statistics.json*'
- en: 'The interesting information related to monitoring and capturing drift is in
    the *constraint_violations.json* file. In my case, most of the violations look
    like this entry:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与监控和捕获漂移相关的有趣信息在*constraint_violations.json*文件中。在我的情况下，大多数违规看起来像这个条目：
- en: '[PRE37]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The suggested baseline required 100% of data integrity, and here we are seeing
    that the model is close enough at 99.7%. Because the constraint is to meet 100%,
    the violation gets generated and reported. Most of those numbers are similar in
    my case except for one row:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的基准要求数据完整性达到100%，而我们看到模型接近99.7%。由于约束是达到100%，所以会生成并报告违规情况。在我的情况下，这些数字大多数是相似的，除了一行：
- en: '[PRE38]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: A 0% is a critical situation here, and this is where all the hard work of setting
    up the systems to catch and report these changes in predictions is crucial. I
    want to emphasize that although it took several steps and boilerplate code with
    the AWS Python SDK, it isn’t too complicated to automate and start generating
    these reports automatically for target datasets. I created the baseline with an
    automated suggestion, and this will mostly require fine-tuning to define acceptable
    values to prevent generating violations that are not useful.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，0%是一个关键情况，这就是设置系统捕捉和报告这些预测变化的所有辛苦工作的地方。我想强调的是，尽管需要几个步骤和AWS Python SDK的样板代码，但自动化和开始为目标数据集生成这些报告并不复杂。我使用自动化建议创建了基线，这主要需要进行微调，以定义可接受的值，以防止生成没有用处的违规。
- en: Monitoring Drift with Azure ML
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Azure ML监控漂移
- en: MLOps plays a significant role in cloud providers. It isn’t surprising that
    monitoring and alerting (core pillars of DevOps) get offered with well-thought-out
    services. Azure can analyze data drift and set alerts to capture potential issues
    before models get into production. It is always useful to see how different cloud
    providers solve a problem like data drift—perspective is an invaluable asset and
    will make you a better engineer. The amount of thought, documentation, and examples
    in the Azure platform makes for a smoother onboarding process. It doesn’t take
    much effort to find learning resources to get up to speed with Azure’s offerings.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps 在云提供商中发挥着重要作用。监控和警报（DevOps 的核心支柱）被精心设计的服务所提供并不奇怪。Azure 可以分析数据漂移，并设置警报以捕获模型进入生产前的潜在问题。了解不同云提供商如何解决像数据漂移这样的问题总是有益的——视角是一种宝贵的资产，将使您成为更好的工程师。Azure
    平台中的思考量、文档和示例数量使得入职过程更加顺利。找到学习资源并快速了解 Azure 的各种提供不需要花费太多的精力。
- en: Note
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of this writing, data drift detection on Azure ML is on preview,
    and some minor issues still need to get worked out that prevent giving solid code
    examples to try out.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Azure ML 上的数据漂移检测仍处于预览阶段，还有一些小问题需要解决，这些问题阻碍了提供可靠的代码示例来尝试。
- en: 'Data drift detection in Azure works similarly to using AWS SageMaker. The objective
    is to be alerted when drift happens between training and serving datasets. As
    with most ML operations, it is critical to have a deep understanding of the data
    (and, therefore, the dataset). An overly simplistic example is a dataset that
    captures swimsuit sales over a year: if the number of sales per week drops to
    zero, does that mean the dataset has drifted and should not be used in production?
    Or that it is probably the middle of winter and no one is buying anything? These
    open questions are easy to answer when the details of the data are well understood.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 中，数据漂移检测的工作方式类似于使用 AWS SageMaker。其目标是在训练数据集和服务数据集之间发生漂移时发出警报。与大多数机器学习操作一样，深入了解数据（因此也是数据集）至关重要。一个过于简单化的例子是一个捕捉了一年内泳衣销售的数据集：如果每周的销售量降至零，这是否意味着数据集已经漂移，并且不应在生产中使用？还是说这可能是冬季的中间，没有人购买任何东西？当数据的细节被充分理解时，这些问题很容易回答。
- en: There are several causes for data drift, many of which can be wholly unacceptable.
    Some examples of these causes involve changes in value types (e.g., Fahrenheit
    to Celsius), empty or null values, or in the example of the swimsuit sale, a *natural
    drift*, where seasonal changes can affect predictions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 数据漂移有几个原因，其中许多原因可能是完全不可接受的。例如，值类型的变化（例如，华氏度到摄氏度），空值或空值，或者在泳衣销售的例子中，*自然漂移*，其中季节性变化可能会影响预测。
- en: The patterns for setting up and analyzing drift on Azure require a baseline
    dataset, a target dataset, and a monitor. These three requirements work together
    to produce metrics and create alerts whenever drift is detected. The monitoring
    and analysis workflow allows you to detect and alert data drift when there is
    new data in a dataset while allowing profiling new data over time. You will probably
    not use historical profiling as much as current drift detection because it is
    more common to use the most current comparison point than checking against several
    months ago. It is still useful to have as a comparator where the previous year’s
    performance is more meaningful than comparing against last month. This is particularly
    true with datasets that are affected by seasonal events. There is not much use
    in comparing Christmas tree sales against August metrics.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 上设置和分析漂移的模式需要一个基线数据集、一个目标数据集和一个监视器。这三个要求共同作用以生成度量并在检测到漂移时创建警报。监控和分析工作流程允许您在数据集中有新数据时检测和警报数据漂移，同时允许随时间对新数据进行分析。与使用历史数据分析漂移检测相比，您可能不会经常使用历史分析，因为与检查几个月前相比，使用最新的比较点更为常见。然而，与上一年的表现相比较比与上个月相比更有意义，特别是对于受季节性事件影响的数据集。比较圣诞树销售与八月的指标并没有太多的用处。
- en: To set up a data drift workflow in Azure, you must start by creating a target
    dataset. The target dataset requires a *time-series* set on it either using a
    timestamp column or a *virtual column*. The virtual column is a nice feature because
    it infers the timestamp from the path where the dataset is stored. This configuration
    attribute is called the *partition format*. And if you are configuring a dataset
    to use the virtual column, you will see a partition format referenced in both
    Azure ML Studio and the Python SDK (see [Figure 6-5](#Figure-6-3)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Azure 中设置数据漂移工作流程，您必须首先创建一个目标数据集。目标数据集需要设置一个*时间序列*，可以使用时间戳列或*虚拟列*。虚拟列是一个很好的功能，因为它可以从存储数据集的路径中推断出时间戳。这个配置属性称为*分区格式*。如果您正在配置一个数据集以使用虚拟列，您将在
    Azure ML Studio 和 Python SDK 中看到引用的分区格式（参见[图 6-5](#Figure-6-3)）。
- en: '![pmlo 0605](Images/pmlo_0605.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0605](Images/pmlo_0605.png)'
- en: Figure 6-5\. Azure partition format
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. Azure 分区格式
- en: In this case, I’m using a partition format helper so that Azure can use the
    path to infer the timestamp. This is nice because it instructs Azure that the
    convention is setting the standard. A path like */2021/10/14/dataset.csv* means
    that the dataset will end up with a virtual column of October 14th, 2021\. Configuration
    by convention is a golden nugget of automation. Whenever you see an opportunity
    to abstract (or entirely remove) configuration that can be inferred by a convention
    (like a path in this case), you should take advantage of it. Less configuration
    means less overhead, which enables speedy workflows.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我正在使用分区格式助手，以便 Azure 可以使用路径推断时间戳。这很好，因为它告诉 Azure 采用的惯例是设置标准的。例如路径 */2021/10/14/dataset.csv*
    意味着数据集将以虚拟列的形式得到 2021 年 10 月 14 日的日期。按照惯例进行配置是自动化的一大优势。每当您看到可以通过惯例来推断（或完全删除）配置（如本例中的路径）的机会时，您都应该利用它。较少的配置意味着较少的开销，这有助于加快工作流程。
- en: Once you have a time-series dataset, you can proceed by creating a dataset monitor.
    To get everything working, you will need a target dataset (in this case, the time-series
    dataset), the baseline dataset, and the monitor settings.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您拥有时间序列数据集，您可以通过创建数据集监视器来进行后续操作。为了使一切正常运行，您将需要一个目标数据集（在本例中是时间序列数据集）、基准数据集以及监视器设置。
- en: The baseline dataset has to have the same features (or as similar as possible)
    as those of the target dataset. One exciting feature is selecting a time range
    to slice the dataset with relevant data for the monitoring task. The monitor’s
    configuration is what brings all the datasets together. It allows you to create
    a schedule to run with a set of exciting features and set the threshold for the
    data drift percentage tolerable.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基准数据集必须具有与目标数据集相同（或尽可能相似）的特性。一个令人兴奋的特性是选择一个时间范围，以便切片数据集中与监控任务相关的数据。监视器的配置是将所有数据集整合在一起的关键。它允许您创建一个运行计划，其中包含一组令人兴奋的特性，并设置可容忍的数据漂移百分比的阈值。
- en: The drift results will be available at the Dataset Monitors tab in the Assets
    section in Azure ML Studio. All the configured monitors are available with highlighted
    metrics about drift magnitude and a sorted list of the top features with drift.
    The simplicity of exposing data drift is one thing I like about Azure ML Studio
    because it can quickly surface the essential pieces of information useful to take
    corrective decisions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 数据漂移结果将在 Azure ML Studio 的“资产”部分中的“数据集监视器”选项卡中显示。所有配置的监视器都会显示漂移的度量值以及漂移前几名特征的排序列表。我喜欢
    Azure ML Studio 展示数据漂移的简单性，因为它可以快速呈现对决策有用的关键信息。
- en: If there is a need to go deep into the metrics’ details, you can use Application
    Insights to query logs and metrics associated with the monitors. Enabling and
    setting up Application Insights is covered in [“Application Insights”](ch08.xhtml#Section-application-insights).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有必要深入了解指标的详细信息，您可以使用 Application Insights 查询与监视器相关的日志和指标。启用和设置 Application
    Insights 的步骤详见[“应用洞察”](ch08.xhtml#Section-application-insights)。
- en: Conclusion
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Logging, monitoring, and metrics are so critical that any model getting shipped
    into production *must implement them* at the risk of hard-to-recover catastrophic
    failure. High confidence in robust processes for reproducible results requires
    all of these components. As I’ve mentioned throughout this chapter, you must make
    decisions with accurate data that can tell you if accuracy is as high as ever
    or if the number of errors has increased significantly.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录、监控和度量如此关键，以至于任何投入生产的模型在风险硬性恢复灾难性失败之前**必须实施它们**。对于可复现结果的稳健过程有高度信心需要所有这些组件。正如我在本章中多次提到的，您必须根据准确的数据做出决策，这些数据可以告诉您准确率是否一直如此高，或者错误数量是否显著增加。
- en: Many examples can be tricky to grasp, but they can all be automated and abstracted
    away. Throughout this book, you will consistently read about DevOps’ core pillars
    and how they are relevant to operationalizing machine learning. Automation is
    what binds pillars like logging and monitoring together. Setting up logging and
    monitoring is usually not exciting work, especially if the idea is to get state-of-the-art
    prediction models doing exceptional work. But exceptional results cannot happen
    consistently with a broken foundation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 许多示例可能难以理解，但它们都可以自动化和抽象化。在本书中，您将持续阅读DevOps的核心支柱及其如何与机器学习运作相关。自动化是将像日志记录和监控这样的支柱绑在一起的关键。设置日志记录和监控通常不是令人兴奋的工作，特别是如果想要最先进的预测模型进行出色工作的话。但是，如果基础不稳固，卓越的结果无法保持一致。
- en: When I first started training to be a professional athlete, I always doubted
    my coach, who didn’t allow me to do the High Jump every day. *“Before becoming
    a High Jumper, you must become an athlete.”* Strong foundations enable strong
    results, and in DevOps and MLOps, this is no different.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始接受专业运动员训练时，我总是怀疑我的教练，他不允许我每天跳高。*“在成为跳高运动员之前，你必须成为一个运动员。”* 强大的基础支持强大的结果，在DevOps和MLOps中也是如此。
- en: In the next chapters, you get a chance to dive deeper into each of the three
    major cloud providers, their concepts, and their machine learning offerings.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，您将有机会深入了解三大主要云服务提供商及其概念和机器学习产品。
- en: Exercises
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Use a different dataset and create a drift report with violations using AWS
    SageMaker.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的数据集，在AWS SageMaker上创建一个违规漂移报告。
- en: Add Python logging to a script that will log errors to `STDERR`, info statements
    to `STDOUT`, and all levels to a file.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在脚本中添加Python日志记录，将错误记录到`STDERR`，将信息语句记录到`STDOUT`，并将所有级别记录到文件中。
- en: Create a time-series dataset on Azure ML Studio.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Azure ML Studio上创建一个时间序列数据集。
- en: Configure a dataset monitor in Azure ML Studio that will send an email when
    a drift is detected beyond the acceptable threshold.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Azure ML Studio中配置数据集监视器，当检测到超出可接受阈值的漂移时发送电子邮件。
- en: Critical Thinking Discussion Questions
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批判性思维讨论问题
- en: Why might it be desirable to log to multiple sources at the same time?
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么同时记录到多个源可能是可取的？
- en: Why is it critical to monitor data drift?
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监视数据漂移为何至关重要？
- en: Name three advantages of using logging facilities versus `print()` or `echo`
    statements.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列举使用日志记录设施与`print()`或`echo`语句的三个优势。
- en: List the five most common log levels, from least to most verbose.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出最常见的五个日志级别，从最少详细到最详细。
- en: What are three common metric types found in metric-capturing systems?
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在度量捕获系统中找到的三种常见度量类型是什么？
