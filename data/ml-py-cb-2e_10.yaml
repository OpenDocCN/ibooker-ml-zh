- en: Chapter 10\. Dimensionality Reduction Using Feature Selection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Chapter 10\. 使用特征选择进行降维
- en: 10.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.0 Introduction
- en: 'In [Chapter 9](ch09.xhtml#dimensionality-reduction-using-feature-extraction),
    we discussed how to reduce the dimensionality of our feature matrix by creating
    new features with (ideally) similar abilities to train quality models but with
    significantly fewer dimensions. This is called *feature extraction*. In this chapter
    we will cover an alternative approach: selecting high-quality, informative features
    and dropping less useful features. This is called *feature selection*.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.xhtml#dimensionality-reduction-using-feature-extraction)中，我们讨论了如何通过创建具有（理想情况下）类似能力的新特征来降低特征矩阵的维度。这称为*特征提取*。在本章中，我们将介绍一种替代方法：选择高质量、信息丰富的特征并丢弃不太有用的特征。这称为*特征选择*。
- en: 'There are three types of feature selection methods: filter, wrapper, and embedded.
    *Filter methods* select the best features by examining their statistical properties.
    Methods where we explicitly set a threshold for a statistic or manually select
    the number of features we want to keep are examples of feature selection by filtering.
    Wrapper methods use trial and error to find the subset of features that produces
    models with the highest quality predictions. *Wrapper methods* are often the most
    effective, as they find the best result through actual experimentation as opposed
    to naive assumptions. Finally, *embedded methods* select the best feature subset
    as part of, as an extension of, a learning algorithm’s training process.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种特征选择方法：过滤、包装和嵌入。*过滤方法*通过检查特征的统计属性选择最佳特征。我们明确设置统计量的阈值或手动选择要保留的特征数的方法是通过过滤进行特征选择的示例。包装方法使用试错法找到产生质量预测模型的特征子集。*包装方法*通常是最有效的，因为它们通过实际试验而非简单的假设来找到最佳结果。最后，*嵌入方法*在学习算法的培训过程中选择最佳特征子集作为其延伸部分。
- en: Ideally, we’d describe all three methods in this chapter. However, since embedded
    methods are closely intertwined with specific learning algorithms, they are difficult
    to explain prior to a deeper dive into the algorithms themselves. Therefore, in
    this chapter we cover only filter and wrapper feature selection methods, leaving
    the discussion of particular embedded methods until the chapters where those learning
    algorithms are discussed in depth.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们会在本章节中描述所有三种方法。然而，由于嵌入方法与特定的学习算法紧密相连，要在深入探讨算法本身之前解释它们是困难的。因此，在本章中，我们仅涵盖过滤和包装特征选择方法，将嵌入方法的讨论留到那些深入讨论这些学习算法的章节中。
- en: 10.1 Thresholding Numerical Feature Variance
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.1 数值特征方差阈值法
- en: Problem
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Problem
- en: You have a set of numerical features and want to filter out those with low variance
    (i.e., likely containing little information).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一组数值特征，并希望过滤掉那些方差低（即可能包含较少信息）的特征。
- en: Solution
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Solution
- en: 'Select a subset of features with variances above a given threshold:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 选择方差高于给定阈值的特征子集：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Discussion
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Discussion
- en: '*Variance thresholding* (VT) is an example of feature selection by filtering,
    and one of the most basic approaches to feature selection. It is motivated by
    the idea that features with low variance are likely less interesting (and less
    useful) than features with high variance. VT first calculates the variance of
    each feature:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*方差阈值法*（VT）是一种通过过滤进行特征选择的示例，也是特征选择的最基本方法之一。其动机是低方差特征可能不太有趣（并且不太有用），而高方差特征可能更有趣。VT
    首先计算每个特征的方差：'
- en: <math display="block"><mrow><mrow><mi>V</mi> <mi>a</mi> <mi>r</mi></mrow> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo><mi>μ</mi><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mrow><mi>V</mi> <mi>a</mi> <mi>r</mi></mrow> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo><mi>μ</mi><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: where <math display="inline"><mi>x</mi></math> is the feature vector, <math
    display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> is an individual feature
    value, and <math display="inline"><mi>μ</mi></math> is that feature’s mean value.
    Next, it drops all features whose variance does not meet that threshold.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><mi>x</mi></math> 是特征向量，<math display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
    是单个特征值，<math display="inline"><mi>μ</mi></math> 是该特征的平均值。接下来，它删除所有方差未达到该阈值的特征。
- en: 'Keep two things in mind when employing VT. First, the variance is not centered;
    that is, it is in the squared unit of the feature itself. Therefore, VT will not
    work when feature sets contain different units (e.g., one feature is in years
    while another is in dollars). Second, the variance threshold is selected manually,
    so we have to use our own judgment for a good value to select (or use a model
    selection technique described in [Chapter 12](ch12.xhtml#model-selection)). We
    can see the variance for each feature using `variances_`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 VT 时要牢记两点。首先，方差未居中；即，它位于特征本身的平方单位中。因此，当特征集包含不同单位时（例如，一个特征以年为单位，而另一个特征以美元为单位），VT
    将无法正常工作。其次，方差阈值是手动选择的，因此我们必须凭借自己的判断来选择一个合适的值（或者使用第 12 章中描述的模型选择技术）。我们可以使用 `variances_`
    查看每个特征的方差：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, if the features have been standardized (to mean zero and unit variance),
    then for obvious reasons VT will not work correctly:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果特征已经标准化（均值为零，方差为单位），那么很显然 VT 将无法正确工作：
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 10.2 Thresholding Binary Feature Variance
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.2 二进制特征方差的阈值处理
- en: Problem
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a set of binary categorical features and want to filter out those with
    low variance (i.e., likely containing little information).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您拥有一组二进制分类特征，并希望过滤掉方差低的特征（即可能包含少量信息）。
- en: Solution
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Select a subset of features with a Bernoulli random variable variance above
    a given threshold:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个伯努利随机变量方差高于给定阈值的特征子集：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Discussion
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'As with numerical features, one strategy for selecting highly informative categorical
    features and filtering out less informative ones is to examine their variances.
    In binary features (i.e., Bernoulli random variables), variance is calculated
    as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与数值特征类似，选择高信息二分类特征并过滤掉信息较少的策略之一是检查它们的方差。在二进制特征（即伯努利随机变量）中，方差计算如下：
- en: <math display="block"><mrow><mo form="prefix">Var</mo> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>=</mo> <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></math>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mo form="prefix">Var</mo> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>=</mo> <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></math>
- en: where <math display="inline"><mi>p</mi></math> is the proportion of observations
    of class `1`. Therefore, by setting <math display="inline"><mi>p</mi></math>,
    we can remove features where the vast majority of observations are one class.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math display="inline"><mi>p</mi></math> 是类 `1` 观察值的比例。因此，通过设置 <math display="inline"><mi>p</mi></math>，我们可以移除大多数观察值为一类的特征。
- en: 10.3 Handling Highly Correlated Features
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.3 处理高度相关的特征
- en: Problem
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a feature matrix and suspect some features are highly correlated.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个特征矩阵，并怀疑某些特征之间高度相关。
- en: Solution
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use a correlation matrix to check for highly correlated features. If highly
    correlated features exist, consider dropping one of the correlated features:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相关性矩阵检查高度相关特征。如果存在高度相关的特征，请考虑删除其中一个：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|  | 0 | 2 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 2 |'
- en: '| --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 1 | 1 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 |'
- en: '| 1 | 2 | 0 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 0 |'
- en: '| 2 | 3 | 1 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3 | 1 |'
- en: Discussion
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'One problem we often run into in machine learning is highly correlated features.
    If two features are highly correlated, then the information they contain is very
    similar, and it is likely redundant to include both features. In the case of simple
    models like linear regression, failing to remove such features violates the assumptions
    of linear regression and can result in an artificially inflated R-squared value.
    The solution to highly correlated features is simple: remove one of them from
    the feature set. Removing highly correlated features by setting a correlation
    threshold is another example of filtering.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们经常遇到的一个问题是高度相关的特征。如果两个特征高度相关，那么它们所包含的信息非常相似，同时包含这两个特征很可能是多余的。对于像线性回归这样简单的模型，如果不移除这些特征，则违反了线性回归的假设，并可能导致人为膨胀的
    R-squared 值。解决高度相关特征的方法很简单：从特征集中删除其中一个特征。通过设置相关性阈值来移除高度相关特征是另一种筛选的例子。
- en: 'In our solution, first we create a correlation matrix of all features:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的解决方案中，首先我们创建了所有特征的相关性矩阵：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|  | 0 | 1 | 2 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | 1.000000 | 0.976103 | 0.000000 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.000000 | 0.976103 | 0.000000 |'
- en: '| 1 | 0.976103 | 1.000000 | -0.034503 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.976103 | 1.000000 | -0.034503 |'
- en: '| 2 | 0.000000 | -0.034503 | 1.000000 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.000000 | -0.034503 | 1.000000 |'
- en: 'Second, we look at the upper triangle of the correlation matrix to identify
    pairs of highly correlated features:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们查看相关性矩阵的上三角来识别高度相关特征的成对：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|  | 0 | 1 | 2 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 1 | 2 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | NaN | 0.976103 | 0.000000 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 0 | NaN | 0.976103 | 0.000000 |'
- en: '| 1 | NaN | NaN | 0.034503 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 1 | NaN | NaN | 0.034503 |'
- en: '| 2 | NaN | NaN | NaN |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 2 | NaN | NaN | NaN |'
- en: Third, we remove one feature from each of those pairs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们从这些成对特征中移除一个特征。
- en: 10.4 Removing Irrelevant Features for Classification
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.4 删除分类中无关紧要的特征
- en: Problem
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You have a categorical target vector and want to remove uninformative features.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个分类目标向量，并希望删除无信息的特征。
- en: Solution
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'If the features are categorical, calculate a chi-square (<math display="inline"><msup><mi>χ</mi>
    <mn>2</mn></msup></math> ) statistic between each feature and the target vector:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征是分类的，请计算每个特征与目标向量之间的卡方统计量（<math display="inline"><msup><mi>χ</mi> <mn>2</mn></msup></math>)：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If the features are quantitative, compute the ANOVA F-value between each feature
    and the target vector:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征是数量的，请计算每个特征与目标向量之间的ANOVA F值：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Instead of selecting a specific number of features, we can use `SelectPercentile`
    to select the top *n* percent of features:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是选择特定数量的特征，我们可以使用`SelectPercentile`来选择顶部*n*百分比的特征：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Discussion
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Chi-square statistics examine the independence of two categorical vectors.
    That is, the statistic is the difference between the observed number of observations
    in each class of a categorical feature and what we would expect if that feature
    were independent (i.e., no relationship) of the target vector:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方统计检验两个分类向量的独立性。也就是说，统计量是类别特征中每个类别的观察次数与如果该特征与目标向量独立（即没有关系）时预期的观察次数之间的差异：
- en: <math display="block"><mrow><msup><mi>χ</mi> <mn>2</mn></msup> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <mfrac><msup><mrow><mo>(</mo><msub><mi>O</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>E</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <msub><mi>E</mi> <mi>i</mi></msub></mfrac></mrow></math>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msup><mi>χ</mi> <mn>2</mn></msup> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <mfrac><msup><mrow><mo>(</mo><msub><mi>O</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>E</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <msub><mi>E</mi> <mi>i</mi></msub></mfrac></mrow></math>
- en: where <math display="inline"><msub><mi>O</mi><mi>i</mi></msub></math> is the
    number of observed observations in class <math display="inline"><mi>i</mi></math>,
    and <math display="inline"><msub><mi>E</mi><mi>i</mi></msub></math> is the number
    of expected observations in class <math display="inline"><mi>i</mi></math>.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math display="inline"><msub><mi>O</mi><mi>i</mi></msub></math>是类别<math display="inline"><mi>i</mi></math>中观察到的观测次数，<math
    display="inline"><msub><mi>E</mi><mi>i</mi></msub></math>是类别<math display="inline"><mi>i</mi></math>中预期的观测次数。
- en: A chi-squared statistic is a single number that tells you how much difference
    exists between your observed counts and the counts you would expect if there were
    no relationship at all in the population. By calculating the chi-squared statistic
    between a feature and the target vector, we obtain a measurement of the independence
    between the two. If the target is independent of the feature variable, then it
    is irrelevant for our purposes because it contains no information we can use for
    classification. On the other hand, if the two features are highly dependent, they
    likely are very informative for training our model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方统计量是一个单一的数字，它告诉您观察计数和在整体人群中如果没有任何关系时预期计数之间的差异有多大。通过计算特征和目标向量之间的卡方统计量，我们可以得到两者之间独立性的度量。如果目标与特征变量无关，那么对我们来说它是无关紧要的，因为它不包含我们可以用于分类的信息。另一方面，如果两个特征高度依赖，它们可能对训练我们的模型非常有信息性。
- en: To use chi-squared in feature selection, we calculate the chi-squared statistic
    between each feature and the target vector, then select the features with the
    best chi-square statistics. In scikit-learn, we can use `SelectKBest` to select
    them. The parameter `k` determines the number of features we want to keep and
    filters out the least informative features.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要在特征选择中使用卡方，我们计算每个特征与目标向量之间的卡方统计量，然后选择具有最佳卡方统计量的特征。在scikit-learn中，我们可以使用`SelectKBest`来选择它们。参数`k`确定我们想要保留的特征数，并过滤掉信息最少的特征。
- en: It is important to note that chi-square statistics can be calculated only between
    two categorical vectors. For this reason, chi-squared for feature selection requires
    that both the target vector and the features are categorical. However, if we have
    a numerical feature we can use the chi-squared technique by first transforming
    the quantitative feature into a categorical feature. Finally, to use our chi-squared
    approach, all values need to be nonnegative.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，卡方统计只能在两个分类向量之间计算。因此，特征选择的卡方要求目标向量和特征都是分类的。然而，如果我们有一个数值特征，我们可以通过首先将定量特征转换为分类特征来使用卡方技术。最后，为了使用我们的卡方方法，所有值都需要是非负的。
- en: Alternatively, if we have a numerical feature, we can use `f_classif` to calculate
    the ANOVA F-value statistic with each feature and the target vector. F-value scores
    examine if, when we group the numerical feature by the target vector, the means
    for each group are significantly different. For example, if we had a binary target
    vector, gender, and a quantitative feature, test scores, the F-value score would
    tell us if the mean test score for men is different than the mean test score for
    women. If it is not, then test score doesn’t help us predict gender and therefore
    the feature is irrelevant.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们有一个数值特征，我们可以使用`f_classif`来计算ANOVA F值统计量和每个特征以及目标向量的相关性。F值分数检查如果我们按照目标向量对数值特征进行分组，每个组的平均值是否显著不同。例如，如果我们有一个二进制目标向量，性别和一个定量特征，测试分数，F值将告诉我们男性的平均测试分数是否与女性的平均测试分数不同。如果不是，则测试分数对我们预测性别没有帮助，因此该特征是无关的。
- en: 10.5 Recursively Eliminating Features
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.5 递归消除特征
- en: Problem
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to automatically select the best features to keep.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要自动选择保留的最佳特征。
- en: Solution
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use scikit-learn’s `RFECV` to conduct *recursive feature elimination* (RFE)
    using cross-validation (CV). That is, use the wrapper feature selection method
    and repeatedly train a model, each time removing a feature until model performance
    (e.g., accuracy) becomes worse. The remaining features are the best:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn的`RFECV`进行*递归特征消除*（RFE），使用交叉验证（CV）。也就是说，使用包装器特征选择方法，重复训练模型，每次删除一个特征，直到模型性能（例如准确性）变差。剩下的特征就是最好的：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once we have conducted RFE, we can see the number of features we should keep:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们进行了RFE，我们就可以看到我们应该保留的特征数量：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can also see which of those features we should keep:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到哪些特征应该保留：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can even view the rankings of the features:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以查看特征的排名：
- en: '[PRE23]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Discussion
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: This is likely the most advanced recipe in this book up to this point, combining
    a number of topics we have yet to address in detail. However, the intuition is
    straightforward enough that we can address it here rather than holding off until
    a later chapter. The idea behind RFE is to train a model repeatedly, updating
    the *weights* or *coefficients* of that model each time. The first time we train
    the model, we include all the features. Then, we find the feature with the smallest
    parameter (notice that this assumes the features are either rescaled or standardized),
    meaning it is less important, and remove that feature from the feature set.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是本书到目前为止最复杂的配方，结合了一些我们尚未详细讨论的主题。然而，直觉足够简单，我们可以在这里解释它，而不是推迟到以后的章节。RFE背后的想法是重复训练模型，每次更新该模型的*权重*或*系数*。第一次训练模型时，我们包括所有特征。然后，我们找到具有最小参数的特征（请注意，这假设特征已经重新缩放或标准化），意味着它不太重要，并从特征集中删除该特征。
- en: 'The obvious question then is: how many features should we keep? We can (hypothetically)
    repeat this loop until we only have one feature left. A better approach requires
    that we include a new concept called *cross-validation*. We will discuss CV in
    detail in the next chapter, but here is the general idea.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 那么显而易见的问题是：我们应该保留多少特征？我们可以（假设性地）重复此循环，直到我们只剩下一个特征。更好的方法要求我们包括一个新概念叫*交叉验证*。我们将在下一章详细讨论CV，但这里是一般的想法。
- en: 'Given data containing (1) a target we want to predict, and (2) a feature matrix,
    first we split the data into two groups: a training set and a test set. Second,
    we train our model using the training set. Third, we pretend that we do not know
    the target of the test set and apply our model to its features to predict the
    values of the test set. Finally, we compare our predicted target values with the
    true target values to evaluate our model.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 给定包含（1）我们想要预测的目标和（2）特征矩阵的数据，首先我们将数据分为两组：一个训练集和一个测试集。其次，我们使用训练集训练我们的模型。第三，我们假装不知道测试集的目标，并将我们的模型应用于其特征以预测测试集的值。最后，我们将我们预测的目标值与真实的目标值进行比较，以评估我们的模型。
- en: We can use CV to find the optimum number of features to keep during RFE. Specifically,
    in RFE with CV, after every iteration we use cross-validation to evaluate our
    model. If CV shows that our model improved after we eliminated a feature, then
    we continue on to the next loop. However, if CV shows that our model got worse
    after we eliminated a feature, we put that feature back into the feature set and
    select those features as the best.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用CV找到在RFE期间保留的最佳特征数。具体而言，在带有CV的RFE中，每次迭代后我们使用交叉验证评估我们的模型。如果CV显示在我们消除一个特征后模型改善了，那么我们继续下一个循环。然而，如果CV显示在我们消除一个特征后模型变差了，我们将该特征重新放回特征集，并选择这些特征作为最佳特征。
- en: In scikit-learn, RFE with CV is implemented using `RFECV`, which contains a
    number of important parameters. The `estimator` parameter determines the type
    of model we want to train (e.g., linear regression), the `step` parameter sets
    the number or proportion of features to drop during each loop, and the `scoring`
    parameter sets the metric of quality we use to evaluate our model during cross-validation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，使用`RFECV`实现了带有多个重要参数的RFE与CV。`estimator`参数确定我们想要训练的模型类型（例如线性回归），`step`参数在每个循环中设置要删除的特征数量或比例，`scoring`参数设置我们在交叉验证期间用于评估模型质量的度量标准。
- en: See Also
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[scikit-learn documentation: Recursive feature elimination with cross-validation](https://oreil.ly/aV-Fz)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn文档：带交叉验证的递归特征消除](https://oreil.ly/aV-Fz)'
