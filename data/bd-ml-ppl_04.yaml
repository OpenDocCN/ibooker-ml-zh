- en: Chapter 4\. Data Validation
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第四章。数据验证
- en: In [Chapter 3](index_split_008.html#filepos156116), we discussed how we can
    ingest data from various sources into our pipeline. In this chapter, we now want
    to start consuming the data by validating it, as shown in [Figure 4-1](#filepos295723).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](index_split_008.html#filepos156116)中，我们讨论了如何将数据从各种来源导入我们的管道。在本章中，我们现在想要通过验证数据开始消耗数据，如[图4-1](#filepos295723)所示。
- en: '![](images/00040.jpg)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00040.jpg)'
- en: Figure 4-1\. Data validation as part of ML pipelines
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图4-1\. 数据验证作为ML管道的一部分
- en: 'Data is the basis for every machine learning model, and the model’s usefulness
    and performance depend on the data used to train, validate, and analyze the model.
    As you can imagine, without robust data, we can’t build robust models. In colloquial
    terms, you might have heard the phrase: “garbage in, garbage out”—meaning that
    our models won’t perform if the underlying data isn’t curated and validated. This
    is the exact purpose of our first workflow step in our machine learning pipeline:
    data validation.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是每个机器学习模型的基础，模型的实用性和性能取决于用于训练、验证和分析模型的数据。正如你可以想象的那样，没有健壮的数据，我们就无法构建健壮的模型。用口头表达来说，你可能听过这样的短语：“垃圾进，垃圾出”—这意味着如果底层数据没有经过精心策划和验证，我们的模型将无法表现出色。这正是我们机器学习管道中第一个工作流步骤的确切目的：数据验证。
- en: In this chapter, we first motivate the idea of data validation, and then we
    introduce you to a Python package from the TensorFlow Extended ecosystem called
    TensorFlow Data Validation (TFDV). We show how you can set up the package in your
    data science projects, walk you through the common use cases, and highlight some
    very useful workflows.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先推动数据验证的概念，然后向您介绍了TensorFlow Extended生态系统中名为TensorFlow数据验证（TFDV）的Python包。我们展示了如何在您的数据科学项目中设置这个包，引导您通过常见的用例，并突出一些非常有用的工作流程。
- en: The data validation step checks that the data in your pipelines is what your
    feature engineering step expects. It assists you in comparing multiple datasets.
    It also highlights if your data changes over time, for example, if your training
    data is significantly different from the new data provided to your model for inference.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据验证步骤检查您的管道中的数据是否符合您的特征工程步骤期望的数据。它帮助您比较多个数据集。它还突出显示如果您的数据随时间变化，例如，如果您的训练数据与为您的模型提供推断的新数据显著不同。
- en: At the end of the chapter, we integrate our first workflow step into our TFX
    pipeline.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，我们将我们的第一个工作流步骤整合到我们的TFX管道中。
- en: Why Data Validation?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么需要数据验证？
- en: In machine learning, we are trying to learn from patterns in datasets and to
    generalize these learnings. This puts data front and center in our machine learning
    workflows, and the quality of the data becomes fundamental to the success of our
    machine learning projects.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们试图从数据集中学习模式并推广这些学习。这使得数据在我们的机器学习工作流程中占据了核心位置，数据的质量对我们的机器学习项目的成功至关重要。
- en: Every step in our machine learning pipeline determines whether the workflow
    can proceed to the next step or if the entire workflow needs to be abandoned and
    restarted (e.g., with more training data). Data validation is an especially important
    checkpoint because it catches changes in the data coming into the machine learning
    pipeline before it reaches the time-consuming preprocessing and training steps.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们机器学习管道中的每一步都决定了工作流是否可以进入下一步，或者整个工作流是否需要被放弃并重新启动（例如，使用更多的训练数据）。数据验证是一个特别重要的检查点，因为它在数据进入耗时的预处理和训练步骤之前捕捉到数据的变化。
- en: 'If our goal is to automate our machine learning model updates, validating our
    data is essential. In particular, when we say validating, we mean three distinct
    checks on our data:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的目标是自动化我们的机器学习模型更新，验证我们的数据就是必不可少的。特别是，当我们说验证时，我们指的是对我们的数据进行三个不同的检查：
- en: Check for data anomalies.
  id: totrans-12
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 检查数据异常。
- en: Check that the data schema hasn’t changed.
  id: totrans-13
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 检查数据模式是否发生了变化。
- en: Check that the statistics of our new datasets still align with statistics from
    our previous training datasets.
  id: totrans-14
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 检查我们新数据集的统计数据是否仍然与我们之前的训练数据集的统计数据保持一致。
- en: The data validation step in our pipeline performs these checks and highlights
    any failures. If a failure is detected, we can stop the workflow and address the
    data issue by hand, for example, by curating a new dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们管道中的数据验证步骤执行这些检查并突出显示任何失败。如果检测到失败，我们可以停止工作流程，并手动解决数据问题，例如，策划一个新的数据集。
- en: It is also useful to refer to the data validation step from the data processing
    step, the next step in our pipeline. Data validation produces statistics around
    your data features and highlights whether a feature contains a high percentage
    of missing values or if features are highly correlated. This is useful information
    when you are deciding which features should be included in the preprocessing step
    and what the form of the preprocessing should be.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据处理步骤到数据验证步骤，这也是有用的。数据验证可以生成关于数据特征的统计信息，并突出显示特征是否包含高百分比的缺失值或特征是否高度相关。在决定哪些特征应包含在预处理步骤中以及预处理的形式应该是什么时，这些信息非常有用。
- en: Data validation lets you compare the statistics of different datasets. This
    simple step can assist you in debugging your model issues. For example, data validation
    can compare the statistics of your training against your validation data. With
    a few lines of code, it brings any difference to your attention. You might train
    a binary classification model with a perfect label split of 50% positive labels
    and 50% negative labels, but the label split isn’t 50/50 in your validation set.
    This difference in the label distribution ultimately will affect your validation
    metrics.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数据验证让您可以比较不同数据集的统计信息。这一简单步骤可以帮助您调试模型问题。例如，数据验证可以比较您的训练数据与验证数据的统计信息。几行代码就可以把任何差异带给您的注意。您可能使用完美的标签分布训练二元分类模型，其中正标签和负标签分别占
    50%，但您的验证集中标签分布并非 50/50。标签分布的这种差异最终会影响您的验证指标。
- en: 'In a world where datasets continuously grow, data validation is crucial to
    make sure that our machine learning models are still up to the task. Because we
    can compare schemas, we can quickly detect if the data structure in newly obtained
    datasets has changed (e.g., when a feature is deprecated). It can also detect
    if your data starts to drift. This means that your newly collected data has different
    underlying statistics than the initial dataset used to train your model. This
    drift could mean that new features need to be selected or that the data preprocessing
    steps need to be updated (e.g., if the minimum or maximum of a numerical column
    changes). Drift can happen for a number of reasons: an underlying trend in the
    data, seasonality of the data, or as a result of a feedback loop, as we discuss
    in [Chapter 13](index_split_020.html#filepos1489635).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集持续增长的世界中，数据验证对确保我们的机器学习模型仍能胜任任务至关重要。因为我们可以比较模式，我们可以快速检测到新获得的数据集中的数据结构是否发生了变化（例如当特征被弃用时）。它还可以检测您的数据是否开始漂移。这意味着您新收集的数据具有与用于训练模型的初始数据集不同的底层统计信息。漂移可能意味着需要选择新特征或更新数据预处理步骤（例如，如果数值列的最小值或最大值发生变化）。数据漂移可能由多种原因引起：数据中的潜在趋势、数据的季节性，或者由于反馈循环而导致的，正如我们在
    [第 13 章](index_split_020.html#filepos1489635) 中讨论的那样。
- en: In the following sections, we will walk through these different use cases. However,
    before that, let’s take a look at the required installation steps to get TFDV
    up and running.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细讨论这些不同的使用案例。但在此之前，让我们看一下安装 TFDV 并使其运行所需的步骤。
- en: TFDV
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TFDV
- en: The TensorFlow ecosystem offers a tool that can assist you in data validation,
    TFDV. It is part of the TFX project. TFDV allows you to perform the kind of analyses
    we discussed previously (e.g., generating schemas and validating new data against
    an existing schema). It also offers visualizations based on the Google PAIR project
    [Facets](https://oreil.ly/ZXbqa), as shown in [Figure 4-2](#filepos302848).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 生态系统提供了一个可以帮助您进行数据验证的工具，TFDV。它是 TFX 项目的一部分。TFDV 允许您执行我们之前讨论过的各种分析（例如生成模式并验证新数据与现有模式的匹配性）。它还基于
    Google PAIR 项目 [Facets](https://oreil.ly/ZXbqa) 提供可视化，如 [图 4-2](#filepos302848)
    所示。
- en: 'TFDV accepts two input formats to start the data validation: TensorFlow’s TFRecord
    and CSV files. In common with other TFX components, it distributes the analysis
    using Apache Beam.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: TFDV 接受两种输入格式来开始数据验证：TensorFlow 的 TFRecord 和 CSV 文件。与其他 TFX 组件一样，它使用 Apache
    Beam 进行分析分布。
- en: '![](images/00050.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00050.jpg)'
- en: Figure 4-2\. Screenshot of a TFDV visualization
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-2\. TFDV 可视化的屏幕截图
- en: Installation
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 安装
- en: 'When we installed the `tfx` package introduced in [Chapter 2](index_split_007.html#filepos83150),
    TFDV was already installed as a dependency. If we would like to use TFDV as a
    standalone package, we can install it with this command:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们安装了 [第 2 章](index_split_007.html#filepos83150) 中介绍的 `tfx` 包时，TFDV 已经作为依赖包安装了。如果我们想单独使用
    TFDV 包，可以使用以下命令进行安装：
- en: '`$` `pip install tensorflow-data-validation`'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`$` `pip install tensorflow-data-validation`'
- en: After installing `tfx` or `tensorflow-data-validation`, we can now integrate
    our data validation into your machine learning workflows or analyze our data visually
    in a Jupyter Notebook. Let’s walk through a couple of use cases in the following
    sections.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了`tfx`或`tensorflow-data-validation`之后，我们现在可以将数据验证集成到您的机器学习工作流程中，或者在Jupyter
    Notebook中对数据进行可视化分析。我们将在以下几节中讨论一些用例。
- en: Generating Statistics from Your Data
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从您的数据生成统计信息。
- en: 'The first step in our data validation process is to generate some summary statistics
    for our data. As an example, we can load our consumer complaints CSV data directly
    with TFDV and generate statistics for each feature:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据验证流程的第一步是为数据生成一些摘要统计信息。例如，我们可以直接使用TFDV加载我们的消费者投诉CSV数据，并为每个特征生成统计信息。
- en: '`import``tensorflow_data_validation``as``tfdv``stats``=``tfdv``.``generate_statistics_from_csv``(``data_location``=``''/data/consumer_complaints.csv''``,``delimiter``=``'',''``)`'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`import``tensorflow_data_validation``as``tfdv``stats``=``tfdv``.``generate_statistics_from_csv``(``data_location``=``''/data/consumer_complaints.csv''``,``delimiter``=``'',''``)`'
- en: 'We can generate feature statistics from TFRecord files in a very similar way
    using the following code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码从TFRecord文件生成特征统计，方法类似。
- en: '`stats``=``tfdv``.``generate_statistics_from_tfrecord``(``data_location``=``''/data/consumer_complaints.tfrecord''``)`'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`stats``=``tfdv``.``generate_statistics_from_tfrecord``(``data_location``=``''/data/consumer_complaints.tfrecord''``)`'
- en: We discuss how to generate TFRecord files in [Chapter 3](index_split_008.html#filepos156116).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论如何在[第3章](index_split_008.html#filepos156116)中生成TFRecord文件。
- en: Both TFDV methods generate a data structure that stores the summary statistics
    for each feature, including the minimum, maximum, and average values.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: TFDV的两种方法都生成一个数据结构，用于存储每个特征的摘要统计信息，包括最小、最大和平均值。
- en: 'The data structure looks like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 数据结构如下所示：
- en: '`datasets``{``num_examples``:``66799``features``{``type``:``STRING``string_stats``{``common_stats``{``num_non_missing``:``66799``min_num_values``:``1``max_num_values``:``1``avg_num_values``:``1.0``num_values_histogram``{``buckets``{``low_value``:``1.0``high_value``:``1.0``sample_count``:``6679.9``...``}}}}}}`'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`datasets``{``num_examples``:``66799``features``{``type``:``STRING``string_stats``{``common_stats``{``num_non_missing``:``66799``min_num_values``:``1``max_num_values``:``1``avg_num_values``:``1.0``num_values_histogram``{``buckets``{``low_value``:``1.0``high_value``:``1.0``sample_count``:``6679.9``...``}}}}}}`'
- en: 'For numerical features, TFDV computes for every feature:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值特征，TFDV为每个特征计算：
- en: The overall count of data records
  id: totrans-39
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据记录的总数。
- en: The number of missing data records
  id: totrans-40
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 缺失数据记录的数量。
- en: The mean and standard deviation of the feature across the data records
  id: totrans-41
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据记录中特征的均值和标准差。
- en: The minimum and maximum value of the feature across the data records
  id: totrans-42
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据记录中特征的最小值和最大值。
- en: The percentage of zero values of the feature across the data records
  id: totrans-43
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据记录中特征零值的百分比。
- en: In addition, it generates a histogram of the values for each feature.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还为每个特征的值生成直方图。
- en: 'For categorical features, TFDV provides:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类特征，TFDV提供以下内容：
- en: The overall count of data records
  id: totrans-46
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据记录的总数。
- en: The percentage of missing data records
  id: totrans-47
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 缺失数据记录的百分比。
- en: The number of unique records
  id: totrans-48
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 唯一记录的数量。
- en: The average string length of all records of a feature
  id: totrans-49
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所有记录中特征的平均字符串长度。
- en: For each category, TFDV determines the sample count for each label and its rank
  id: totrans-50
  prefs:
  - PREF_UL
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于每个类别，TFDV确定每个标签的样本计数及其排名。
- en: In a moment, you’ll see how we can turn these statistics into something actionable.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，您将看到我们如何将这些统计数据转化为可操作的内容。
- en: Generating Schema from Your Data
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从您的数据生成模式。
- en: Once we have generated our summary statistics, the next step is to generate
    a schema of our dataset. Data schema are a form of describing the representation
    of your datasets. A schema defines which features are expected in your dataset
    and which type each feature is based on (float, integer, bytes, etc.). Besides,
    your schema should define the boundaries of your data (e.g., outlining minimums,
    maximums, and thresholds of allowed missing records for a feature).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 生成摘要统计信息后，下一步是生成数据集的模式。数据模式描述数据集的表示形式。模式定义了数据集中预期包含的特征以及每个特征的类型（浮点数、整数、字节等）。此外，您的模式应定义数据的边界（例如，特征的允许缺失记录的最小值、最大值和阈值）。
- en: The schema definition of your dataset can then be used to validate future datasets
    to determine if they are in line with your previous training sets. The schemas
    generated by TFDV can also be used in the following workflow step when you are
    preprocessing your datasets to convert them to data that can be used to train
    machine learning models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您数据集的模式定义随后可用于验证未来数据集，以确定它们是否符合先前训练集的要求。TFDV 生成的模式还可用于以下工作流步骤，即在预处理数据集以转换为可用于训练机器学习模型的数据时使用。
- en: 'As shown in the following, you can generate the schema information from your
    generated statistics with a single function call:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，您可以通过单个函数调用从生成的统计信息中生成模式信息：
- en: '`schema``=``tfdv``.``infer_schema``(``stats``)`'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`schema``=``tfdv``.``infer_schema``(``stats``)`'
- en: '`tfdv.infer_schema` generates a schema protocol defined by TensorFlow:[1](#filepos396063)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`tfdv.infer_schema` 生成了 TensorFlow 定义的模式协议：[1](#filepos396063)'
- en: '`feature``{``name``:``"product"``type``:``BYTES``domain``:``"product"``presence``{``min_fraction``:``1.0``min_count``:``1``}``shape``{``dim``{``size``:``1``}``}``}`'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`feature``{``name``:``"product"``type``:``BYTES``domain``:``"product"``presence``{``min_fraction``:``1.0``min_count``:``1``}``shape``{``dim``{``size``:``1``}``}``}`'
- en: 'You can display the schema with a single function call in any Jupyter Notebook:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在任何 Jupyter Notebook 中通过单个函数调用显示模式：
- en: '`tfdv``.``display_schema``(``schema``)`'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tfdv``.``display_schema``(``schema``)`'
- en: And the results are shown in [Figure 4-3](#filepos320159).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 并且结果显示在 [Figure 4-3](#filepos320159) 中。
- en: '![](images/00003.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00003.jpg)'
- en: Figure 4-3\. Screenshot of a schema visualization
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-3\. 模式可视化截图
- en: In this visualization, `Presence` means whether the feature must be present
    in 100% of data examples (`required`) or not (`optional`). `Valency` means the
    number of values required per training example. In the case of categorical features,
    `single` would mean each training example must have exactly one category for the
    feature.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在此可视化中，“Presence” 表示特征是否必须存在于100%的数据示例中（`required`）或不需要（`optional`）。“Valency”
    表示每个训练示例所需的值的数量。对于分类特征，`single` 意味着每个训练示例必须具有该特征的一个类别。
- en: The schema that has been generated here may not be exactly what we need because
    it assumes that the current dataset is exactly representative of all future data
    as well. If a feature is present in all training examples in this dataset, it
    will be marked as `required`, but in reality it may be `optional`. We will show
    you how to update the schema according to your own knowledge of the dataset in
    [“Updating the Schema”](#filepos333831).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此处生成的模式可能并不完全符合我们的需求，因为它假设当前数据集完全代表所有未来数据。如果某个特征在此数据集中的所有训练示例中都存在，它将被标记为`required`，但实际上它可能是`optional`。我们将展示如何根据您对数据集的了解更新模式在
    [“更新模式”](#filepos333831) 中。
- en: With the schema now defined, we can compare our training or evaluation datasets,
    or check our datasets for any problems that may affect our model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在定义了模式后，我们可以比较我们的训练或评估数据集，或者检查可能影响我们模型的任何问题。
- en: Recognizing Problems in Your Data
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 发现您数据中的问题
- en: In the previous sections, we discussed how to generate summary statistics and
    a schema for our data. These describe our data, but they don’t spot potential
    issues with it. In the next few sections, we will describe how TFDV can help us
    spot problems in our data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们讨论了如何为我们的数据生成摘要统计信息和模式。这些描述了我们的数据，但并没有发现其中的潜在问题。在接下来的几个部分中，我们将描述 TFDV
    如何帮助我们发现数据中的问题。
- en: Comparing Datasets
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 比较数据集
- en: 'Let’s say we have two datasets: training and validation datasets. Before training
    our machine learning model, we would like to determine how representative the
    validation set is in regards to the training set. Does the validation data follow
    our training data schema? Are any feature columns or a significant number of feature
    values missing? With TFDV, we can quickly determine the answer.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个数据集：训练数据集和验证数据集。在训练我们的机器学习模型之前，我们想确定验证集在数据结构方面与训练集的代表性如何。验证数据是否遵循我们的训练数据模式？是否缺少任何特征列或大量特征值？使用
    TFDV，我们可以快速确定答案。
- en: 'As shown in the following, we can load both datasets and then visualize both
    datasets. If we execute the following code in a Jupyter Notebook, we can compare
    the dataset statistics easily:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，我们可以加载两个数据集，然后可视化这两个数据集。如果我们在 Jupyter Notebook 中执行以下代码，我们可以轻松比较数据集的统计信息：
- en: '`train_stats``=``tfdv``.``generate_statistics_from_tfrecord``(``data_location``=``train_tfrecord_filename``)``val_stats``=``tfdv``.``generate_statistics_from_tfrecord``(``data_location``=``val_tfrecord_filename``)``tfdv``.``visualize_statistics``(``lhs_statistics``=``val_stats``,``rhs_statistics``=``train_stats``,``lhs_name``=``''VAL_DATASET''``,``rhs_name``=``''TRAIN_DATASET''``)`'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`train_stats``=``tfdv``.``generate_statistics_from_tfrecord``(``data_location``=``train_tfrecord_filename``)``val_stats``=``tfdv``.``generate_statistics_from_tfrecord``(``data_location``=``val_tfrecord_filename``)``tfdv``.``visualize_statistics``(``lhs_statistics``=``val_stats``,``rhs_statistics``=``train_stats``,``lhs_name``=``''VAL_DATASET''``,``rhs_name``=``''TRAIN_DATASET''``)`'
- en: '[Figure 4-4](#filepos326920) shows the difference between the two datasets.
    For example, the validation dataset (containing 4,998 records) has a lower rate
    of missing `sub_issue` values. This could mean that the feature is changing its
    distribution in the validation set. More importantly, the visualization highlighted
    that over half of all records don’t contain `sub_issue` information. If the `sub_issue`
    is an important feature for our model training, we need to fix our data-capturing
    methods to collect new data with the correct issue identifiers.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-4](#filepos326920)展示了两个数据集之间的差异。例如，验证数据集（包含 4,998 条记录）中缺少`sub_issue`值的比率较低。这可能意味着该特征在验证集中的分布发生了变化。更重要的是，可视化显示超过一半的记录不包含`sub_issue`信息。如果`sub_issue`对我们的模型训练很重要，我们需要修复数据采集方法，以收集具有正确问题标识符的新数据。'
- en: The schema of the training data we generated earlier now becomes very handy.
    TFDV lets us validate any data statistics against the schema, and it reports any
    anomalies.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先前生成的训练数据的架构现在非常有用。TFDV允许我们根据架构验证任何数据统计，并报告任何异常。
- en: '![](images/00016.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00016.jpg)'
- en: Figure 4-4\. Comparison between training and validation datasets
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-4\. 训练集和验证集之间的比较
- en: 'Anomalies can be detected using the following code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码检测异常：
- en: '`anomalies``=``tfdv``.``validate_statistics``(``statistics``=``val_stats``,``schema``=``schema``)`'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`anomalies``=``tfdv``.``validate_statistics``(``statistics``=``val_stats``,``schema``=``schema``)`'
- en: 'And we can then display the anomalies with:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下代码显示异常：
- en: '`tfdv``.``display_anomalies``(``anomalies``)`'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tfdv``.``display_anomalies``(``anomalies``)`'
- en: This displays the result shown in [Table 4-1](#filepos329030).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了在[表 4-1](#filepos329030)中显示的结果。
- en: Table 4-1\. Visualize the anomalies in a Jupyter Notebook
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-1\. 在Jupyter Notebook中可视化异常
- en: '|  Feature name  |  Anomaly short description  |  Anomaly long description 
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  Feature name  |  Anomaly short description  |  Anomaly long description 
    |'
- en: '|  “company”  |  Column dropped  |  The feature was present in fewer examples
    than expected.  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  “company”  |  Column dropped  |  The feature was present in fewer examples
    than expected.  |'
- en: 'The following code shows the underlying anomaly protocol. This contains useful
    information that we can use to automate our machine learning workflow:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了底层的异常协议。这包含了我们可以用来自动化机器学习工作流程的有用信息：
- en: '`anomaly_info``{``key``:``"company"``value``{``description``:``"The feature
    was present in fewer examples than expected."``severity``:``ERROR``short_description``:``"Column
    dropped"``reason``{``type``:``FEATURE_TYPE_LOW_FRACTION_PRESENT``short_description``:``"Column
    dropped"``description``:``"The feature was present in fewer examples than expected."``}``path``{``step``:``"company"``}``}``}`'
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`anomaly_info``{``key``:``"company"``value``{``description``:``"The feature
    was present in fewer examples than expected."``severity``:``ERROR``short_description``:``"Column
    dropped"``reason``{``type``:``FEATURE_TYPE_LOW_FRACTION_PRESENT``short_description``:``"Column
    dropped"``description``:``"The feature was present in fewer examples than expected."``}``path``{``step``:``"company"``}``}``}`'
- en: Updating the Schema
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 更新架构
- en: The preceding anomaly protocol shows us how to detect variations from the schema
    that is autogenerated from our dataset. But another use case for TFDV is manually
    setting the schema according to our domain knowledge of the data. Taking the `sub_issue`
    feature discussed previously, if we decide that we need to require this feature
    to be present in greater than 90% of our training examples, we can update the
    schema to reflect this.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的异常协议显示了如何从我们的数据集自动生成的架构中检测到变化。但TFDV的另一个用例是根据我们对数据的领域知识手动设置架构。根据先前讨论的`sub_issue`特征，如果我们决定需要在我们的训练示例中要求此特征出现的百分比大于90%，我们可以更新架构以反映这一点。
- en: 'First, we need to load the schema from its serialized location:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从其序列化位置加载架构：
- en: '`schema``=``tfdv``.``load_schema_text``(``schema_location``)`'
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`schema``=``tfdv``.``load_schema_text``(``schema_location``)`'
- en: 'Then, we update this particular feature so that it is required in 90% of cases:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们更新这个特定特征，以便在 90% 的情况下是必需的：
- en: '`sub_issue_feature``=``tfdv``.``get_feature``(``schema``,``''sub_issue''``)``sub_issue_feature``.``presence``.``min_fraction``=``0.9`'
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`sub_issue_feature``=``tfdv``.``get_feature``(``schema``,``''sub_issue''``)``sub_issue_feature``.``presence``.``min_fraction``=``0.9`'
- en: 'We could also update the list of US states to remove Alaska:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以更新美国州列表以删除阿拉斯加：
- en: '`state_domain``=``tfdv``.``get_domain``(``schema``,``''state''``)``state_domain``.``value``.``remove``(``''AK''``)`'
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`state_domain``=``tfdv``.``get_domain``(``schema``,``''state''``)``state_domain``.``value``.``remove``(``''AK''``)`'
- en: 'Once we are happy with the schema, we write the schema file to its serialized
    location with the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们满意架构，我们将架构文件写入其序列化位置，步骤如下：
- en: '`tfdv``.``write_schema_text``(``schema``,``schema_location``)`'
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tfdv``.``write_schema_text``(``schema``,``schema_location``)`'
- en: 'We then need to revalidate the statistics to view the updated anomalies:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要重新验证统计信息以查看更新的异常情况：
- en: '`updated_anomalies``=``tfdv``.``validate_statistics``(``eval_stats``,``schema``)``tfdv``.``display_anomalies``(``updated_anomalies``)`'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`updated_anomalies``=``tfdv``.``validate_statistics``(``eval_stats``,``schema``)``tfdv``.``display_anomalies``(``updated_anomalies``)`'
- en: In this way, we can adjust the anomalies to those that are appropriate for our
    dataset.[2](#filepos396439)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以调整适合我们数据集的异常情况。[2](#filepos396439)
- en: Data Skew and Drift
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据倾斜和漂移
- en: TFDV provides a built-in “skew comparator” that detects large differences between
    the statistics of two datasets. This isn’t the statistical definition of skew
    (a dataset that is asymmetrically distributed around its mean). It is defined
    in TFDV as the L-infinity norm of the difference between the `serving_statistics`
    of two datasets. If the difference between the two datasets exceeds the threshold
    of the L-infinity norm for a given feature, TFDV highlights it as an anomaly using
    the anomaly detection defined earlier in this chapter.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: TFDV提供了内置的“倾斜比较器”，用于检测两个数据集统计信息之间的大差异。这不是倾斜的统计定义（数据集围绕其均值不对称分布），在TFDV中定义为两个数据集的服务统计信息的L-infinity范数差异。如果两个数据集之间的差异超过给定特征的L-infinity范数阈值，TFDV将使用本章前述的异常检测将其标记为异常。
- en: L-INFINITY NORM
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: L-INFINITY NORM
- en: The L-infinity norm is an expression used to define the difference between two
    vectors (in our case, the serving statistics). The L-infinity norm is defined
    as the maximum absolute value of the vector’s entries.
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: L-infinity范数是用于定义两个向量（在我们的案例中是服务统计信息）之间差异的表达式。L-infinity范数定义为向量条目的最大绝对值。
- en: For example, the L-infinity norm of the vector [3, –10, –5] is 10\. Norms are
    often used to compare vectors. If we wish to compare the vectors [2, 4, –1] and
    [9, 1, 8], we first compute their difference, which is [–7, 3, –9], and then we
    compute the L-infinity norm of this vector, which is 9.
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 例如，向量[3, –10, –5]的L-infinity范数为10。范数通常用于比较向量。如果我们希望比较向量[2, 4, –1]和[9, 1, 8]，我们首先计算它们的差异，即[–7,
    3, –9]，然后计算此向量的L-infinity范数，结果为9。
- en: In the case of TFDV, the two vectors are the summary statistics of the two datasets.
    The norm returned is the biggest difference between these two sets of statistics.
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在TFDV的情况下，这两个向量是两个数据集的摘要统计信息。返回的范数是这两组统计信息之间的最大差异。
- en: 'The following code shows how you can compare the skew between datasets:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何比较数据集之间的倾斜：
- en: '`tfdv``.``get_feature``(``schema``,``''company''``)``.``skew_comparator``.``infinity_norm``.``threshold``=``0.01``skew_anomalies``=``tfdv``.``validate_statistics``(``statistics``=``train_stats``,``schema``=``schema``,``serving_statistics``=``serving_stats``)`'
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tfdv``.``get_feature``(``schema``,``''company''``)``.``skew_comparator``.``infinity_norm``.``threshold``=``0.01``skew_anomalies``=``tfdv``.``validate_statistics``(``statistics``=``train_stats``,``schema``=``schema``,``serving_statistics``=``serving_stats``)`'
- en: And [Table 4-2](#filepos345362) shows the results.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 而[表4-2](#filepos345362)展示了结果。
- en: Table 4-2\. Visualization of the data skew between the training and serving
    datasets
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-2\. 可视化展示训练和服务数据集之间的数据倾斜
- en: '|  Feature name  |  Anomaly short description  |  Anomaly long description 
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  特征名称  |  异常简短描述  |  异常详细描述  |'
- en: '|  “company”  |  High L-infinity distance between training and serving  |  The
    L-infinity distance between training and serving is 0.0170752 (up to six significant
    digits), above the threshold 0.01\. The feature value with maximum difference
    is: Experian  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  “公司”  |  训练和服务之间的高L-infinity距离  |  训练和服务之间的L-infinity距离为0.0170752（精确到六位有效数字），高于阈值0.01。具有最大差异的特征值为：Experian 
    |'
- en: TFDV also provides a `drift_comparator` for comparing the statistics of two
    datasets of the same type, such as two training sets collected on two different
    days. If drift is detected, the data scientist should either check the model architecture
    or determine whether feature engineering needs to be performed again.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: TFDV 还提供了一个 `drift_comparator`，用于比较同一类型的两个数据集的统计数据，例如在两个不同日期收集的两个训练集。如果检测到漂移，则数据科学家应检查模型架构或确定是否需要重新进行特征工程。
- en: 'Similar to this skew example, you should define your `drift_comparator` for
    the features you would like to watch and compare. You can then call `validate_statistics`
    with the two dataset statistics as arguments, one for your baseline (e.g., yesterday’s
    dataset) and one for a comparison (e.g., today’s dataset):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于这个偏斜的例子，你应该为你想观察和比较的特征定义你的 `drift_comparator`。然后，你可以使用两个数据集统计数据作为参数来调用 `validate_statistics`，一个用作基线（例如昨天的数据集），另一个用作比较（例如今天的数据集）：
- en: '`tfdv``.``get_feature``(``schema``,``''company''``)``.``drift_comparator``.``infinity_norm``.``threshold``=``0.01``drift_anomalies``=``tfdv``.``validate_statistics``(``statistics``=``train_stats_today``,``schema``=``schema``,``previous_statistics``=``\`
    `train_stats_yesterday``)`'
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tfdv``.``get_feature``(``schema``,``''公司''``)``.``drift_comparator``.``infinity_norm``.``threshold``=``0.01``drift_anomalies``=``tfdv``.``validate_statistics``(``statistics``=``train_stats_today``,``schema``=``schema``,``previous_statistics``=``\`
    `train_stats_yesterday``)'
- en: And this gives the result shown in [Table 4-3](#filepos350364).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了显示在 [表 4-3](#filepos350364) 中的结果。
- en: Table 4-3\. Visualization of the data drift between two training sets
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-3\. 两个训练集之间数据漂移的可视化
- en: '|  Feature name  |  Anomaly short description  |  Anomaly long description 
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  特征名称  |  异常简短描述  |  异常详细描述  |'
- en: '|  “company”  |  High L-infinity distance between current and previous  |  The
    L-infinity distance between current and previous is 0.0170752 (up to six significant
    digits), above the threshold 0.01\. The feature value with maximum difference
    is: Experian  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  “公司”  |  当前和上一次之间的高L-infinity距离  |  当前和上一次的L-infinity距离为 0.0170752（保留六个有效数字），超过了阈值
    0.01\. 具有最大差异的特征值是：Experian  |'
- en: The L-infinity norm in both the `skew_comparator` and the `drift_comparator`
    is useful for showing us large differences between datasets, especially ones that
    may show us that something is wrong with our data input pipeline. Because the
    L-infinity norm only returns a single number, the schema may be more useful for
    detecting variations between datasets.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `skew_comparator` 和 `drift_comparator` 中的L-infinity范数对于显示数据集之间的大差异非常有用，特别是可能显示我们的数据输入管道存在问题的情况。因为L-infinity范数只返回一个单一的数字，模式可能更有用来检测数据集之间的变化。
- en: Biased Datasets
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 偏倚数据集
- en: Another potential problem with an input dataset is bias. We define bias here
    as data that is in some way not representative of the real world. This is in contrast
    to fairness, which we define in [Chapter 7](index_split_012.html#filepos624151)
    as predictions made by our model that have disparate impacts on different groups
    of people.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据集的另一个潜在问题是偏见。我们在这里定义偏见为在某种程度上不代表真实世界的数据。这与我们在 [第 7 章](index_split_012.html#filepos624151)
    中定义的公平性形成对比，后者是我们模型在不同人群中产生不同影响的预测。
- en: Bias can creep into data in a number of different ways. A dataset is always,
    by necessity, a subset of the real world—we can’t hope to capture all the details
    about everything. The way that we sample the real world is always biased in some
    way. One of the types of bias we can check for is selection bias, in which the
    distribution of the dataset is not the same as the real-world distribution of
    data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见可以通过多种方式进入数据。数据集始终是真实世界的子集，我们不可能希望捕捉所有细节。我们对真实世界的采样方式总是以某种方式存在偏差。我们可以检查的偏见类型之一是选择偏见，其中数据集的分布与真实世界数据的分布不同。
- en: We can use TFDV to check for selection bias using the statistics visualizations
    that we described previously. For example, if our dataset contains `Gender` as
    a categorical feature, we can check that this is not biased toward the male category.
    In our consumer complaints dataset, we have `State` as a categorical feature.
    Ideally, the distribution of example counts across the different US states would
    reflect the relative population in each state.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 TFDV 来检查选择偏见，使用我们之前描述的统计可视化工具。例如，如果我们的数据集包含 `Gender` 作为分类特征，我们可以检查它是否偏向于男性类别。在我们的消费者投诉数据集中，我们有
    `State` 作为分类特征。理想情况下，不同美国州的示例计数分布应反映每个州的相对人口。
- en: We can see in [Figure 4-5](#filepos353872) that it doesn’t (e.g., Texas, in
    third place, has a larger population than Florida in second place). If we find
    this type of bias in our data and we believe this bias may harm our model’s performance,
    we can go back and collect more data or over/undersample our data to get the correct
    distribution.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图 4-5](#filepos353872)中看到，情况并非如此（例如，德克萨斯州排名第三，其人口比排名第二的佛罗里达州更多）。如果我们发现数据中存在这种类型的偏差，并且我们相信这种偏差可能会损害我们模型的性能，我们可以回头收集更多数据或对数据进行过采样/欠采样，以获取正确的分布。
- en: '![](images/00076.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00076.jpg)'
- en: Figure 4-5\. Visualization of a biased feature in our dataset
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-5\. 数据集中一个有偏的特征的可视化
- en: You can also use the anomaly protocol described previously to automatically
    alert you to these kinds of problems. Using the domain knowledge you have of your
    dataset, you can enforce limits on numeric values that mean your dataset is as
    unbiased as possible—for example, if your dataset contains people’s wages as a
    numeric feature, you can enforce that the mean of the feature value is realistic.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用先前描述的异常协议来自动警报您这类问题。利用您对数据集的领域知识，您可以强制执行限制数值的上限，以尽可能保证数据集的无偏性，例如，如果您的数据集包含人们的工资作为数值特征，则可以强制执行特征值的平均值是现实的。
- en: For more details and definitions of bias, Google’s [Machine Learning Crash Course](https://oreil.ly/JtX5b)
    has some useful material.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多详细信息和偏差的定义，请参阅谷歌的[机器学习入门课程](https://oreil.ly/JtX5b)提供的有用材料。
- en: Slicing Data in TFDV
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在TFDV中切片数据
- en: We can also use TFDV to slice datasets on features of our choice to help show
    whether they are biased. This is similar to the calculation of model performance
    on sliced features that we describe in [Chapter 7](index_split_012.html#filepos624151).
    For example, a subtle way for bias to enter data is when data is missing. If data
    is not missing at random, it may be missing more frequently for one group of people
    within the dataset than for others. This can mean that when the final model is
    trained, its performance is worse for these groups.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用TFDV来切片我们选择的特征数据集，以帮助显示它们是否存在偏差。这类似于我们在[第7章](index_split_012.html#filepos624151)中描述的对切片特征的模型性能计算。例如，数据出现缺失时，偏差可能悄然而至。如果数据不是随机缺失的，那么在数据集中的某些人群可能会更频繁地出现缺失情况。这可能意味着，当最终模型训练完成时，其性能对这些群体可能更差。
- en: 'In this example, we’ll look at data from different US states. We can slice
    the data so that we only get statistics from California using the following code:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将查看来自不同美国州的数据。我们可以切分数据，只获取加利福尼亚州的统计信息，使用以下代码：
- en: '`from``tensorflow_data_validation.utils``import``slicing_util``slice_fn1``=``slicing_util``.``get_feature_value_slicer``(``features``=``{``''state''``:``[``b``''CA''``]})`![](images/00002.jpg)`slice_options``=``tfdv``.``StatsOptions``(``slice_functions``=``[``slice_fn1``])``slice_stats``=``tfdv``.``generate_statistics_from_csv``(``data_location``=``''data/consumer_complaints.csv''``,``stats_options``=``slice_options``)`'
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tensorflow_data_validation.utils``import``slicing_util``slice_fn1``=``slicing_util``.``get_feature_value_slicer``(``features``=``{``''state''``:``[``b``''CA''``]})`![](images/00002.jpg)`slice_options``=``tfdv``.``StatsOptions``(``slice_functions``=``[``slice_fn1``])``slice_stats``=``tfdv``.``generate_statistics_from_csv``(``data_location``=``''data/consumer_complaints.csv''``,``stats_options``=``slice_options``)`'
- en: '![](images/00002.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Note that the feature value must be provided as a list of binary values.
  id: totrans-134
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，特征值必须以二进制值列表的形式提供。
- en: 'We need some helper code to copy the sliced statistics to the visualization:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些辅助代码来将切片后的统计数据复制到可视化中：
- en: '`from``tensorflow_metadata.proto.v0``import``statistics_pb2``def``display_slice_keys``(``stats``):``print``(``list``(``map``(``lambda``x``:``x``.``name``,``slice_stats``.``datasets``)))``def``get_sliced_stats``(``stats``,``slice_key``):``for``sliced_stats``in``stats``.``datasets``:``if``sliced_stats``.``name``==``slice_key``:``result``=``statistics_pb2``.``DatasetFeatureStatisticsList``()``result``.``datasets``.``add``()``.``CopyFrom``(``sliced_stats``)``return``result``print``(``''Invalid
    Slice key''``)``def``compare_slices``(``stats``,``slice_key1``,``slice_key2``):``lhs_stats``=``get_sliced_stats``(``stats``,``slice_key1``)``rhs_stats``=``get_sliced_stats``(``stats``,``slice_key2``)``tfdv``.``visualize_statistics``(``lhs_stats``,``rhs_stats``)`'
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tensorflow_metadata.proto.v0``import``statistics_pb2``def``display_slice_keys``(``stats``):``print``(``list``(``map``(``lambda``x``:``x``.``name``,``slice_stats``.``datasets``)))``def``get_sliced_stats``(``stats``,``slice_key``):``for``sliced_stats``in``stats``.``datasets``:``if``sliced_stats``.``name``==``slice_key``:``result``=``statistics_pb2``.``DatasetFeatureStatisticsList``()``result``.``datasets``.``add``()``.``CopyFrom``(``sliced_stats``)``return``result``print``(``''Invalid
    Slice key''``)``def``compare_slices``(``stats``,``slice_key1``,``slice_key2``):``lhs_stats``=``get_sliced_stats``(``stats``,``slice_key1``)``rhs_stats``=``get_sliced_stats``(``stats``,``slice_key2``)``tfdv``.``visualize_statistics``(``lhs_stats``,``rhs_stats``)`'
- en: 'And we can visualize the results with the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下代码来可视化结果：
- en: '`tfdv``.``visualize_statistics``(``get_sliced_stats``(``slice_stats``,``''state_CA''``))`'
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`tfdv``.``visualize_statistics``(``get_sliced_stats``(``slice_stats``,``''state_CA''``))`'
- en: 'And then compare the statistics for California with the overall results:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后比较加利福尼亚州的统计数据与总体结果：
- en: '`compare_slices``(``slice_stats``,``''state_CA''``,``''All Examples''``)`'
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`compare_slices``(``slice_stats``,``''state_CA''``,``''All Examples''``)`'
- en: The results of this are shown in [Figure 4-6](#filepos370070).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果显示在[图4-6](#filepos370070)中。
- en: '![](images/00058.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00058.jpg)'
- en: Figure 4-6\. Visualization of data sliced by feature values
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图4-6. 按特征值切片的数据可视化
- en: In this section, we have shown some useful features of TFDV that allow you to
    spot problems in your data. Next, we’ll look at how to scale up your data validation
    using a product from Google Cloud.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了TFDV的一些有用功能，可以帮助您发现数据中的问题。接下来，我们将看看如何利用Google Cloud的产品来扩展您的数据验证。
- en: Processing Large Datasets with GCP
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GCP处理大型数据集
- en: As we collect more data, the data validation becomes a more time-consuming step
    in our machine learning workflow. One way of reducing the time to perform the
    validation is by taking advantage of available cloud solutions. By using a cloud
    provider, we aren’t limited to the computation power of our laptop or on-premise
    computing resources.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们收集更多数据，数据验证成为机器学习工作流程中更耗时的步骤。减少验证时间的一种方法是利用可用的云解决方案。通过使用云提供商，我们不受限于笔记本电脑或本地计算资源的计算能力。
- en: As an example, we’ll introduce how to run TFDV on Google Cloud’s product Dataflow.
    TFDV runs on Apache Beam, which makes a switch to GCP Dataflow very easy.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我们将介绍如何在Google Cloud的Dataflow产品上运行TFDV。TFDV基于Apache Beam运行，这使得切换到GCP Dataflow非常容易。
- en: Dataflow lets us accelerate our data validation tasks by parallelizing and distributing
    them across the allocated nodes for our data-processing task. While Dataflow charges
    for the number of CPUs and the gigabytes of memory allocated, it can speed up
    our pipeline step.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Dataflow让我们通过并行化和分布在分配的节点上执行数据处理任务来加速数据验证任务。虽然Dataflow按照分配的CPU数量和内存的GB数收费，但它可以加快我们的管道步骤。
- en: We’ll demonstrate a minimal setup to distribute our data validation tasks. For
    more information, we highly recommend the extended GCP [documentation](https://oreil.ly/X3cdi).
    We assume that you have a Google Cloud account created, the billing details set
    up, and the `GOOGLE_APPLICATION_CREDENTIALS` environment variable set in your
    terminal shell. If you need help to get started, see [Chapter 3](index_split_008.html#filepos156116)
    or the Google Cloud [documentation](https://oreil.ly/p4VTx).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将演示一个最小的设置来分发我们的数据验证任务。有关更多信息，我们强烈建议查看扩展的GCP[文档](https://oreil.ly/X3cdi)。我们假设您已经创建了Google
    Cloud帐户，设置了计费详细信息，并在终端Shell中设置了`GOOGLE_APPLICATION_CREDENTIALS`环境变量。如果您需要帮助开始，请参阅[第3章](index_split_008.html#filepos156116)或Google
    Cloud[文档](https://oreil.ly/p4VTx)。
- en: 'We can use the same method we discussed previously (e.g., `tfdv.generate_statistics_from_tfrecord`),
    but the methods require the additional arguments `pipeline_options` and `output_path`.
    While `output_path` points at the Google Cloud bucket where the data validation
    results should be written, `pipeline_options` is an object that contains all the
    Google Cloud details to run our data validation on Google Cloud. The following
    code shows how we can set up such a pipeline object:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用之前讨论过的相同方法（例如，`tfdv.generate_statistics_from_tfrecord`），但这些方法需要额外的参数`pipeline_options`和`output_path`。其中，`output_path`指向Google
    Cloud存储桶，用于存放数据验证结果，`pipeline_options`是一个包含所有Google Cloud细节的对象，用于在Google Cloud上运行我们的数据验证。以下代码展示了如何设置这样的管道对象：
- en: '`from``apache_beam.options.pipeline_options``import``(``PipelineOptions``,``GoogleCloudOptions``,``StandardOptions``)``options``=``PipelineOptions``()``google_cloud_options``=``options``.``view_as``(``GoogleCloudOptions``)``google_cloud_options``.``project``=``''<YOUR_GCP_PROJECT_ID>''`![](images/00002.jpg)`google_cloud_options``.``job_name``=``''<YOUR_JOB_NAME>''`![](images/00075.jpg)`google_cloud_options``.``staging_location``=``''gs://<YOUR_GCP_BUCKET>/staging''`![](images/00064.jpg)`google_cloud_options``.``temp_location``=``''gs://<YOUR_GCP_BUCKET>/tmp''``options``.``view_as``(``StandardOptions``)``.``runner``=``''DataflowRunner''`'
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``apache_beam.options.pipeline_options``import``(``PipelineOptions``,``GoogleCloudOptions``,``StandardOptions``)``options``=``PipelineOptions``()``google_cloud_options``=``options``.``view_as``(``GoogleCloudOptions``)``google_cloud_options``.``project``=``''<YOUR_GCP_PROJECT_ID>''`![](images/00002.jpg)`google_cloud_options``.``job_name``=``''<YOUR_JOB_NAME>''`![](images/00075.jpg)`google_cloud_options``.``staging_location``=``''gs://<YOUR_GCP_BUCKET>/staging''`![](images/00064.jpg)`google_cloud_options``.``temp_location``=``''gs://<YOUR_GCP_BUCKET>/tmp''``options``.``view_as``(``StandardOptions``)``.``runner``=``''DataflowRunner''`'
- en: '![](images/00002.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00002.jpg)'
- en: Set your project’s identifier.
  id: totrans-153
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 设置你项目的标识符。
- en: '![](images/00075.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00075.jpg)'
- en: Give your job a name.
  id: totrans-155
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 给你的任务取一个名字。
- en: '![](images/00064.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00064.jpg)'
- en: Point toward a storage bucket for staging and temporary files.
  id: totrans-157
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 指向一个用于暂存和临时文件的存储桶。
- en: We recommend creating a storage bucket for your Dataflow tasks. The storage
    bucket will hold all the datasets and temporary files.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议为Dataflow任务创建一个存储桶。这个存储桶将容纳所有数据集和临时文件。
- en: Once we have configured the Google Cloud options, we need to configure the setup
    for the Dataflow workers. All tasks are executed on workers that need to be provisioned
    with the necessary packages to run their tasks. In our case, we need to install
    TFDV by specifying it as an additional package.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 配置好Google Cloud选项后，我们需要为Dataflow工作节点配置设置。所有任务都在工作节点上执行，这些节点需要预装运行任务所需的必要软件包。在我们的情况下，我们需要通过指定额外的软件包来安装TFDV。
- en: To do this, download the latest TFDV package (the binary `.whl` file)[3](#filepos396869)
    to your local system. Choose a version which can be executed on a Linux system
    (e.g., `tensorflow_data_validation-0.22.0-cp37-cp37m-manylinux2010_x86_64.whl`).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，请将最新的TFDV软件包（二进制的`.whl`文件）[3](#filepos396869)下载到你的本地系统。选择一个可以在Linux系统上执行的版本（例如，`tensorflow_data_validation-0.22.0-cp37-cp37m-manylinux2010_x86_64.whl`）。
- en: 'To configure the worker setup options, specify the path to the downloaded package
    in the `setup_options.extra_packages` list as shown:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置工作节点设置选项，请按照下面的示例将下载包的路径指定到`setup_options.extra_packages`列表中：
- en: '`from``apache_beam.options.pipeline_options``import``SetupOptions``setup_options``=``options``.``view_as``(``SetupOptions``)``setup_options``.``extra_packages``=``[``''/path/to/tensorflow_data_validation''``''-0.22.0-cp37-cp37m-manylinux2010_x86_64.whl''``]`'
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``apache_beam.options.pipeline_options``import``SetupOptions``setup_options``=``options``.``view_as``(``SetupOptions``)``setup_options``.``extra_packages``=``[``''/path/to/tensorflow_data_validation''``''-0.22.0-cp37-cp37m-manylinux2010_x86_64.whl''``]`'
- en: 'With all the option configurations in place, you can kick off the data validation
    tasks from your local machine. They are executed on the Google Cloud Dataflow
    instances:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 所有选项配置就绪后，你可以从本地机器启动数据验证任务。它们将在Google Cloud Dataflow实例上执行：
- en: '`data_set_path``=``''gs://<YOUR_GCP_BUCKET>/train_reviews.tfrecord''``output_path``=``''gs://<YOUR_GCP_BUCKET>/''``tfdv``.``generate_statistics_from_tfrecord``(``data_set_path``,``output_path``=``output_path``,``pipeline_options``=``options``)`'
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`data_set_path``=``''gs://<YOUR_GCP_BUCKET>/train_reviews.tfrecord''``output_path``=``''gs://<YOUR_GCP_BUCKET>/''``tfdv``.``generate_statistics_from_tfrecord``(``data_set_path``,``output_path``=``output_path``,``pipeline_options``=``options``)`'
- en: After you have started the data validation with Dataflow, you can switch back
    to the Google Cloud console. Your newly kicked off job should be listed in a similar
    way to the one in [Figure 4-7](#filepos383939).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当你启动Dataflow进行数据验证后，你可以切换回Google Cloud控制台。你新启动的任务应该以类似[图4-7](#filepos383939)的方式列出。
- en: '![](images/00115.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00115.jpg)'
- en: Figure 4-7\. Google Cloud Dataflow Jobs console
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-7\. Google Cloud Dataflow 作业控制台
- en: You can then check the details of the running job, its status, and its autoscaling
    details, as shown in [Figure 4-8](#filepos384724).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看运行作业的详细信息，其状态以及自动缩放的详细信息，如[图 4-8](#filepos384724)所示。
- en: You can see that with a few steps you can parallelize and distribute the data
    validation tasks in a cloud environment. In the next section, we’ll discuss the
    integration of the data validation tasks into our automated machine learning pipelines.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通过几个步骤，您可以在云环境中并行和分布数据验证任务。在下一节中，我们将讨论如何将数据验证任务集成到我们的自动化机器学习流水线中。
- en: '![](images/00021.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00021.jpg)'
- en: Figure 4-8\. Google Cloud Dataflow Job details
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-8\. Google Cloud Dataflow 作业详细信息
- en: Integrating TFDV into Your Machine Learning Pipeline
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 将 TFDV 集成到您的机器学习流水线中
- en: So far, all methods we have discussed can be used in a standalone setup. This
    can be helpful to investigate datasets outside of the pipeline setup.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的所有方法都可以在独立设置中使用。这对于在流水线设置之外研究数据集非常有帮助。
- en: 'TFX provides a pipeline component called `StatisticsGen`, which accepts the
    output of the previous `ExampleGen` components as input and then performs the
    generation of statistics:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: TFX 提供了一个名为`StatisticsGen`的流水线组件，它接受前一个`ExampleGen`组件的输出作为输入，然后执行统计信息的生成：
- en: '`from``tfx.components``import``StatisticsGen``statistics_gen``=``StatisticsGen``(``examples``=``example_gen``.``outputs``[``''examples''``])``context``.``run``(``statistics_gen``)`'
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tfx.components``import``StatisticsGen``statistics_gen``=``StatisticsGen``(``examples``=``example_gen``.``outputs``[``''examples''``])``context``.``run``(``statistics_gen``)`'
- en: 'Just like we discussed in [Chapter 3](index_split_008.html#filepos156116),
    we can visualize the output in an interactive context using:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在[第 3 章](index_split_008.html#filepos156116)中讨论的那样，我们可以使用以下方法在交互式环境中可视化输出：
- en: '`context``.``show``(``statistics_gen``.``outputs``[``''statistics''``])`'
  id: totrans-177
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`context``.``show``(``statistics_gen``.``outputs``[``''statistics''``])`'
- en: This gives us the visualization shown in [Figure 4-9](#filepos388487).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们得到了[图 4-9](#filepos388487)所示的可视化效果。
- en: '![](images/00033.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00033.jpg)'
- en: Figure 4-9\. Statistics generated by the `StatisticsGen` component
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4-9\. `StatisticsGen` 组件生成的统计信息
- en: 'Generating our schema is just as easy as generating the statistics:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模式与生成统计信息一样简单：
- en: '`from``tfx.components``import``SchemaGen``schema_gen``=``SchemaGen``(``statistics``=``statistics_gen``.``outputs``[``''statistics''``],``infer_feature_shape``=``True``)``context``.``run``(``schema_gen``)`'
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tfx.components``import``SchemaGen``schema_gen``=``SchemaGen``(``statistics``=``statistics_gen``.``outputs``[``''statistics''``],``infer_feature_shape``=``True``)``context``.``run``(``schema_gen``)`'
- en: The `SchemaGen` component only generates a schema if one doesn’t already exist.
    It’s a good idea to review the schema on the first run of this component and then
    manually adjust it if required as we discussed in [“Updating the Schema”](#filepos333831).
    We can then use this schema until it’s necessary to change it, for example, if
    we add a new feature.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`SchemaGen`组件仅在不存在模式时才生成模式。建议在此组件的首次运行时审查模式，如果需要，则手动调整模式，正如我们在[“更新模式”](#filepos333831)中讨论的那样。然后，我们可以使用此模式，直到需要更改它，例如，如果我们添加了新的特征。'
- en: 'With the statistics and schema in place, we can now validate our new dataset:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了统计数据和模式，我们可以验证我们的新数据集了：
- en: '`from``tfx.components``import``ExampleValidator``example_validator``=``ExampleValidator``(``statistics``=``statistics_gen``.``outputs``[``''statistics''``],``schema``=``schema_gen``.``outputs``[``''schema''``])``context``.``run``(``example_validator``)`'
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`from``tfx.components``import``ExampleValidator``example_validator``=``ExampleValidator``(``statistics``=``statistics_gen``.``outputs``[``''statistics''``],``schema``=``schema_gen``.``outputs``[``''schema''``])``context``.``run``(``example_validator``)`'
- en: NOTE
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: The `ExampleValidator` can automatically detect the anomalies against the schema
    by using the skew and drift comparators we described previously. However, this
    may not cover all the potential anomalies in your data. If you need to detect
    some other specific anomalies, you will need to write your own custom component
    as we describe in [Chapter 10](index_split_017.html#filepos1073133).
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`ExampleValidator`可以使用我们之前描述的偏移和漂移比较器自动检测针对模式的异常。然而，这可能无法涵盖数据中的所有潜在异常。如果您需要检测其他特定的异常，您需要按照我们在[第 10
    章](index_split_017.html#filepos1073133)中描述的方法编写自己的定制组件。'
- en: If the `ExampleValidator` component detects a misalignment in the dataset statistics
    or schema between the new and the previous dataset, it will set the status to
    failed in the metadata store, and the pipeline ultimately stops. Otherwise, the
    pipeline moves on to the next step, the data preprocessing.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`ExampleValidator`组件检测到新旧数据集之间的数据统计或模式不一致，它将在元数据存储中将状态设置为失败，并且管道最终会停止。否则，管道将继续到下一步，即数据预处理。
- en: Summary
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the importance of data validation and how you
    can efficiently perform and automate the process. We discussed how to generate
    data statistics and schemas and how to compare two different datasets based on
    their statistics and schemas. We stepped through an example of how you could run
    your data validation on Google Cloud with Dataflow, and ultimately we integrated
    this machine learning step in our automated pipeline. This is a really important
    go/no go step in our pipeline, as it stops dirty data getting fed through to the
    time-consuming preprocessing and training steps.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了数据验证的重要性以及如何有效执行和自动化此过程。我们讨论了如何生成数据统计和模式，并如何根据它们的统计和模式比较两个不同的数据集。我们通过一个示例演示了如何在Google
    Cloud上使用Dataflow运行数据验证，并最终将此机器学习步骤集成到我们的自动化管道中。这是我们管道中非常重要的一步，因为它可以阻止脏数据通过到耗时的预处理和训练步骤。
- en: In the following chapters, we will extend our pipeline setup by starting with
    data preprocessing.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将通过开始数据预处理来扩展我们的管道设置。
- en: '[1  ](#filepos316434) You can find the protocol buffer definitions for the
    schema protocol in the [TensorFlow repository](https://oreil.ly/Qi263).'
  id: totrans-192
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[1  ](#filepos316434) 您可以在[TensorFlow存储库](https://oreil.ly/Qi263)中找到模式协议的协议缓冲定义。'
- en: '[2  ](#filepos340427) You can also adjust the schema so that different features
    are required in the training and serving environments. See [the documentation](https://oreil.ly/iSgKL)
    for more details.'
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[2  ](#filepos340427) 你还可以调整模式，以便在训练和服务环境中需要不同的特性。参见[文档](https://oreil.ly/iSgKL)获取更多详细信息。'
- en: '[3  ](#filepos379120) Download [TFDV packages](https://oreil.ly/lhExZ).'
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[3  ](#filepos379120) 下载[TFDV包](https://oreil.ly/lhExZ)。'
