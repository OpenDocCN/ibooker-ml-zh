- en: Chapter 5\. Driving Value with Responsible Machine Learning Innovation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。以负责任的机器学习创新驱动价值
- en: “By far, the greatest danger of Artificial Intelligence is that people conclude
    too early that they understand it.”
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “远远最大的危险是人们过早地得出对人工智能的理解结论。”
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Eliezer Yudkowsky
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Eliezer Yudkowsky
- en: “Why do 87% of data science projects never make it into production?” asks a
    recent [VentureBeat article](https://oreil.ly/7XQr2). For many companies, getting
    ML models into production is where the rubber meets the road in terms of ML risks.
    And to many, the entire purpose of building a model is to ultimately deploy it
    for making live predictions, and anything else is a failure. For others, the ultimate
    goal of an ML model can simply be ad hoc predictions, valuations, categorizations,
    or alerts. This short chapter aims to provide an overview of key concepts companies
    should be aware of as they look to adopt and drive value from ML. Generally, there
    are much more significant implications for companies looking to make material,
    corporate decisions based on predictive algorithms, versus simply experimenting
    or prototyping exploratory ML exercises.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: “为什么87%的数据科学项目最终未能投入生产？”近期一篇[《VentureBeat》文章](https://oreil.ly/7XQr2)提出了这个问题。对于许多公司来说，将机器学习模型投入生产是衡量机器学习风险的关键时刻。对许多人而言，构建模型的整体目的在于最终部署它进行实时预测，其他任何目的都会被视为失败。对其他公司来说，机器学习模型的最终目标可以简单地是即席预测、估值、分类或警报。本短章旨在提供公司在采纳和从机器学习中获取价值时应注意的关键概念概述。一般来说，对于企业而言，基于预测算法做出重要决策会带来更为重大的影响，而不仅仅是进行试验或原型探索性机器学习活动。
- en: Trust and Risk
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信任与风险
- en: 'For smart organizations adopting AI, there are often two major questions that
    get asked: “How can I trust this model?” and “How risky is it?” These are critical
    questions for firms to ask before they put ML models into production. However,
    the thing to understand is there is a flywheel effect between the answers to these
    questions. The more you understand an ML system’s risks, the more you can trust
    it. We often find that executives and leaders jump to ask, “What is the risk?”
    whereas the data science practitioners are more focused on, “Can I trust this
    prediction?” But in the end, they are asking the same question.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于采用人工智能的智能组织来说，通常会有两个主要问题：“我如何能够信任这个模型？”和“它有多大风险？”这些对公司在将机器学习模型投入生产之前问的关键问题。然而，需要理解的是，这些问题的答案之间存在一个正反馈效应。你对一个机器学习系统的风险了解得越多，你就能越信任它。我们经常发现，高管和领导们会迫不及待地问：“风险是什么？”而数据科学从业者更专注于：“我能相信这个预测吗？”但最终，他们问的是同一个问题。
- en: 'The first and most obvious metrics to be analyzed are those around the risk
    that a given ML model may manifest. Below are a few questions informed decision
    makers need to ask regarding ML deployments:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，需要分析的最明显的指标是一个给定的机器学习模型可能表现出的风险。以下是决策者需要询问有关机器学习部署的几个问题的几个问题：
- en: What is the quality of the model? (Accuracy, AUC/ROC, F1)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的质量如何？（准确性、AUC/ROC、F1）
- en: What is the cost of an inevitable wrong outcome?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误结果的成本是多少？
- en: Are there secondary cost considerations? Legal or compliance concerns? Customer
    lifetime value? Operational risk? Brand or reputational risk? Harm to end users
    or the general public?
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有次要成本考虑吗？法律或合规问题？客户生命周期价值？运营风险？品牌或声誉风险？对终端用户或公众的伤害？
- en: Are we watching our models in real time for accuracy, discrimination, privacy,
    or security problems?
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否实时监控模型的准确性、歧视性、隐私或安全问题？
- en: Do we have specific AI incident response plans?
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否有具体的人工智能事件响应计划？
- en: How many predictions is this model making?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个模型正在做多少预测？
- en: What is the velocity of these predictions? How quickly does your company need
    to respond to errors or anomalous situations?
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些预测的速度是多少？你的公司需要多快地响应错误或异常情况？
- en: What is the materiality of this model to your company?
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个模型对你的公司有多大的重要性？
- en: As of today, there’s no silver bullet for ML risks. Much of the true nuance
    when it comes to data science and ML risk stems from understanding what happens
    when predictions go wrong. For the same reason, that is why having deep domain
    knowledge or contextual business background is imperative when designing and developing
    ML systems. The downside risk of an incorrect prediction is not simply lost profit
    or increased cost. It is a multilayered consideration that firms need to be rigorous
    in analyzing. The statistics quotation, “all models are wrong, but some are useful,”
    by George Box, should be a starting point for organizations with ML. Know that
    your model will be incorrect and understand thoroughly what that means for your
    organization.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，没有关于ML风险的银弹。在数据科学和ML风险方面，真正的细微差别主要源于了解预测出错时会发生什么。出于同样的原因，设计和开发ML系统时，深入的领域知识或背景上下文至关重要。错误预测的下行风险不仅仅是损失利润或增加成本，而是企业需要严格分析的多层考虑因素。乔治·博克斯的统计引用“所有模型都是错误的，但有些是有用的”，应该成为拥有ML的组织的一个起点。了解您的模型将会出错，并彻底理解这对您的组织意味着什么。
- en: Signal and Simplicity
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信号与简洁
- en: 'The most recent wave of popularity in ML and AI, starting roughly in the mid
    to late aughts, is often attributed to the explosion of deep neural networks and
    deep learning accelerated by high-performance computing. Originally, the big idea
    was that an appropriately tuned deep learning model with enough data could outperform
    any other method. The problem with this idea is that these models were arguably
    the most black-box of any leading method. This created a tradeoff: do I have to
    sacrifice accuracy (signal) for simplicity? How much? However, new research shows
    that with tabular data and methods like XNN and EBM, this tradeoff is probably
    small. For the time being, white-box methodologies can perform at the same accuracy
    as their black-box counterparts on standard business data sources. Remember that
    interpretability enables all kinds of risk mitigation processes around an ML system:
    improved effective challenge, better documentation, customer appeals, and more
    model debugging. If you’re a business leader presented with an ML system that
    your team can’t explain, this is a major red flag. You can have both signal and
    simplicity, especially for the most common types of traditional data mining problems.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年ML和AI领域的热潮很大程度上归因于深度神经网络和高性能计算的推动。最初的大想法是，适当调整的深度学习模型在有足够数据支持下，可以超越其他任何方法。这个想法的问题在于，这些模型可以说是所有领先方法中最黑盒的。这带来了一个折衷：为了简洁性（信号），我是否需要牺牲准确性？要牺牲多少？然而，新的研究表明，在表格数据和诸如XNN和EBM等方法中，这种折衷可能很小。目前而言，白盒方法在标准业务数据源上的准确性可以与其黑盒对应物相媲美。请记住，解释性可以围绕ML系统实施各种风险缓解过程：改进有效挑战、更好的文档编制、客户申诉以及更多模型调试。如果您作为业务领导者面对一个您的团队无法解释的ML系统，这是一个重要的警告信号。您可以同时兼顾信号和简洁，特别是对于最常见的传统数据挖掘问题。
- en: The Future of Responsible Machine Learning
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任机器学习的未来
- en: Over the last few years, there has been more demand for better understanding
    and trust of our ML systems—and that is a very good thing. In parts of Europe
    and Asia, governments have been more active in requiring organizations to consider
    these factors when deploying ML. In the US, responsible ML has largely been a
    grassroots push by data scientists, researchers, and industry practitioners aiming
    to encourage responsible innovation in AI and ML. Either way, the goal is increasing
    consumer trust and driving better practices in the field. We believe that responsibility
    in ML is going to continue to evolve and improve over time, particularly with
    serious regulatory oversight from government agencies in the US. With both the
    grassroots and future regulatory pressures increasing, organizations would be
    remiss to simply check off a series of boxes and consider responsible ML “done.”
    We hope organizations will aim to continually improve the practices they use to
    better understand and trust their ML systems until responsible machine learning
    is just machine learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，对于更好理解和信任我们的机器学习系统有了更多的需求，这是一件非常好的事情。在欧洲和亚洲的某些地区，政府在组织部署机器学习时更积极要求考虑这些因素。在美国，负责任的机器学习主要是由数据科学家、研究人员和行业从业者的基层推动，旨在促进AI和机器学习领域的负责任创新。无论如何，目标都是增强消费者信任，并推动该领域的更好实践。我们相信，在未来，机器学习的责任性将继续发展和改进，特别是在美国政府机构的严格监管下。随着基层压力和未来的法规压力增加，组织不能简单地打勾认为负责任的机器学习问题已解决。我们希望组织能不断改进其实践，以更好地理解和信任他们的机器学习系统，直到负责任的机器学习变成普通的机器学习。
- en: Further Reading
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[UK ICO AI Auditing Framework](https://oreil.ly/RDdNa)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[英国ICO AI审计框架](https://oreil.ly/RDdNa)'
- en: '[Singapore PDPC Model AI Governance Framework](https://oreil.ly/1UOSP)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[新加坡PDPC模型AI治理框架](https://oreil.ly/1UOSP)'
- en: '[Berryville Institute Interactive Machine Learning Risk Assessment](https://oreil.ly/EXYuN)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[Berryville Institute互动式机器学习风险评估](https://oreil.ly/EXYuN)'
- en: Acknowledgments
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: Thanks to our colleagues at H2O.ai, especially Ingrid Burton. Thanks to Michele
    Cronin, Michelle Houston, Beth Kelly, Mike Loukides, and Rebecca Novack at O’Reilly
    Media. Thanks also to our colleagues in the broader data science community, Andrew
    Burt, Hannes Hapke, Catherine Nelson, and Nicholas Schimdt.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢我们在H2O.ai的同事，尤其是Ingrid Burton。感谢O’Reilly Media的Michele Cronin、Michelle Houston、Beth
    Kelly、Mike Loukides和Rebecca Novack。同时也感谢我们更广泛的数据科学社区的同事Andrew Burt、Hannes Hapke、Catherine
    Nelson和Nicholas Schimdt。
- en: About the Authors
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于作者
- en: '**Patrick Hall** is principal scientist at bnh.ai, a boutique law firm focused
    on data analytics and AI. Patrick also serves as a visiting professor in the Department
    of Decision Sciences at the George Washington University and as an advisor to
    H2O.ai.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**Patrick Hall** 是bnh.ai的首席科学家，这是一家专注于数据分析和人工智能的精品法律公司。Patrick还是乔治·华盛顿大学决策科学系的客座教授，并担任H2O.ai的顾问。'
- en: '**Navdeep Gill** is a senior data scientist and software engineer at H2O.ai
    where he focuses mainly on responsible machine learning. Navdeep has also contributed
    to H2O.ai’s efforts in GPU-accelerated machine learning, automated machine learning,
    and to the core H2O-3 machine learning platform.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**Navdeep Gill** 是H2O.ai的高级数据科学家和软件工程师，主要专注于负责任的机器学习。Navdeep还为H2O.ai在GPU加速机器学习、自动化机器学习以及核心H2O-3机器学习平台方面做出了贡献。'
- en: '**Ben Cox** is a director of product marketing at H2O.ai where he leads responsible
    AI market research and thought leadership. Prior to H2O.ai, Ben held data science
    roles in high-profile teams at Ernst & Young, Nike, and NTT Data.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ben Cox** 是H2O.ai的产品营销总监，负责负责任的AI市场研究和思想领导。在加入H2O.ai之前，Ben在安永、耐克和NTT数据的高知名度团队中担任数据科学角色。'
