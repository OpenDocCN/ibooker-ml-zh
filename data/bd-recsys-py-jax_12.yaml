- en: Chapter 9\. Feature-Based and Counting-Based Recommendations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章。基于特征和基于计数的推荐
- en: 'Consider this oversimplified problem: given a bunch of new users, predict which
    will like our new mega-ultra-fancy-fun-item-of-novelty, or MUFFIN for short. You
    may start by asking which old users like MUFFIN; do those users have any aspects
    in common? If so, you could build a model that predicts MUFFIN affinity from those
    correlated user features.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个简化的问题：给定一群新用户，预测哪些用户会喜欢我们的新型超级超级有趣新品项，简称为马芬。您可以开始询问哪些老用户喜欢马芬；这些用户是否有任何共同点？如果有，您可以构建一个模型，从这些相关的用户特征预测马芬的亲和力。
- en: Alternatively, you could ask, “What are other items people buy with MUFFIN?”
    If you find that others frequently also ask for JAM (just-awesome-merch), then
    MUFFIN may be a good suggestion for those who already have JAM. This would be
    using the co-occurrence of MUFFIN and JAM as a predictor. Similarly, if your friend
    comes along with tastes similar to yours—you both like SCONE, JAM, BISCUIT, and
    TEA—but your friend hasn’t yet had the MUFFIN, if you like MUFFIN, it’s probably
    a good choice for your friend too. This is using the co-occurrence of items between
    you and your friend.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以问：“人们与马芬一起购买的其他物品是什么？”如果您发现其他人经常还购买果酱（just-awesome-merch），那么马芬对那些已经拥有果酱的人来说可能是一个不错的建议。这将使用马芬和果酱的共现作为预测器。同样，如果您的朋友有与您相似的口味——您都喜欢司康、果酱、饼干和茶——但您的朋友尚未品尝过马芬，如果您喜欢马芬，那么对于您的朋友来说，这可能是一个不错的选择。这是利用您和朋友之间的物品共现。
- en: These item relationship features will form our first ranking methods in this
    chapter; so grab a tasty snack and let’s dig in.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些物品关系特征将在本章形成我们的第一种排名方法；所以拿起一份美味的小吃，让我们深入了解一下。
- en: Bilinear Factor Models (Metric Learning)
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双线性因子模型（度量学习）
- en: As per the usual idioms about running in front of horses and walking after the
    cart, let’s start our journey into ranking systems with what can be considered
    the *naive* ML approaches. Via these approaches, we will start to get a sense
    of where the rub lies in building recommendation systems and why some of the forthcoming
    efforts are necessary at all.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 根据关于在马前奔跑和在车后行走的通俗语，让我们从可以视为*天真*的机器学习方法开始我们的排名系统之旅。通过这些方法，我们将开始感受构建推荐系统中的难点所在，以及为什么一些即将进行的努力是必要的。
- en: 'Let’s begin again with our basic premise of recommendation problems: to estimate
    ratings of item <math alttext="x"><mi>x</mi></math> by user <math alttext="i"><mi>i</mi></math>
    written as <math alttext="r Subscript i comma x"><msub><mi>r</mi> <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow></msub></math>
    . *Note the slight change in notation from earlier for reasons that will become
    clear momentarily.* In a usual ML paradigm, we might claim that estimating this
    score is done via properties of the item and the user, and frequently those properties
    would be described as features, and thus <math alttext="bold i"><mi>𝐢</mi></math>
    and <math alttext="bold x"><mi>𝐱</mi></math> can be the user and item vectors,
    respectively, composed of these features.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新从推荐问题的基本假设开始：估计用户<math alttext="i"><mi>i</mi></math>对物品<math alttext="x"><mi>x</mi></math>的评分，表示为<math
    alttext="r Subscript i comma x"><msub><mi>r</mi> <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow></msub></math>
    。*请注意符号与之前稍有不同的原因很快就会明白。* 在通常的机器学习范式中，我们可能会声称通过物品和用户的特性来估计这个分数，并且通常这些特性会被描述为特征，因此<math
    alttext="bold i"><mi>𝐢</mi></math> 和 <math alttext="bold x"><mi>𝐱</mi></math>
    可以是用户和物品向量，分别由这些特征组成。
- en: Now, we consider user <math alttext="i"><mi>i</mi></math> with their collection
    of previously interacted-with items <math alttext="script upper R Subscript i"><msub><mi>ℛ</mi>
    <mi>i</mi></msub></math> , and consider <math alttext="script upper I equals StartSet
    bold x vertical-bar x element-of script upper R Subscript i Baseline EndSet"><mrow><mi>ℐ</mi>
    <mo>=</mo> <mo>{</mo> <mi>𝐱</mi> <mo>|</mo> <mi>x</mi> <mo>∈</mo> <msub><mi>ℛ</mi>
    <mi>i</mi></msub> <mo>}</mo></mrow></math> the set of vectors associated to those
    items in this feature space. We can then map this collection of vectors to a representation
    to yield a *content-based feature vector for <math alttext="i"><mi>i</mi></math>
    .* [Figure 9-1](#fig:content-feature-vector) illustrates an example mapping.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们考虑用户<math alttext="i"><mi>i</mi></math>及其之前互动过的物品集合<math alttext="script
    upper R Subscript i"><msub><mi>ℛ</mi> <mi>i</mi></msub></math>，并考虑<math alttext="script
    upper I equals StartSet bold x vertical-bar x element-of script upper R Subscript
    i Baseline EndSet"><mrow><mi>ℐ</mi> <mo>=</mo> <mo>{</mo> <mi>𝐱</mi> <mo>|</mo>
    <mi>x</mi> <mo>∈</mo> <msub><mi>ℛ</mi> <mi>i</mi></msub> <mo>}</mo></mrow></math>，这些物品在特征空间中相关联的向量集合。然后，我们可以将这些向量映射到一个表示中，以生成一个*用户<math
    alttext="i"><mi>i</mi></math>的基于内容的特征向量*。[图 9-1](#fig:content-feature-vector)展示了一个映射示例。
- en: '![Map a user''s read books to a single feature vector](assets/brpj_0901.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![将用户阅读的书籍映射到单个特征向量](assets/brpj_0901.png)'
- en: Figure 9-1\. Content-to-feature vector
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 内容到特征向量
- en: This extremely simple approach can turn a collection of item features and user-item
    interactions into features of the user. Much of the following will be increasingly
    rich ways of doing this. Thinking very hard about the map, the features, and the
    requirements for *interaction* yields many of the key insights in the rest of
    the book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种极其简单的方法可以将一组物品特征和用户-物品交互转化为用户的特征。在接下来的内容中，将会探讨越来越丰富的方法。通过深入思考映射、特征以及*交互*的需求，为本书的许多关键洞察铺平道路。
- en: Let’s take the preceding mapping, <math alttext="bold i colon equals upper F
    left-parenthesis script upper I right-parenthesis"><mrow><mi>𝐢</mi> <mo>:</mo>
    <mo>=</mo> <mi>F</mi> <mfenced close=")" open="("><mi>ℐ</mi></mfenced></mrow></math>
    , to be a simple aggregation like dimension-wise average. Then recognize that
    the mapping will provide a vector of the same dimension as the items. Now we have
    a user vector in the same “space” as the items, and we can ask a similarity question
    as we did in our discussion of latent space in [Chapter 3](ch03.html#ch:math).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将前述映射<math alttext="bold i colon equals upper F left-parenthesis script upper
    I right-parenthesis"><mrow><mi>𝐢</mi> <mo>:</mo> <mo>=</mo> <mi>F</mi> <mfenced
    close=")" open="("><mi>ℐ</mi></mfenced></mrow></math>，视为一个简单的聚合，如维度平均。然后认识到该映射将提供与物品相同维度的向量。现在我们有了一个与物品相同“空间”的用户向量，我们可以像我们在[第3章](ch03.html#ch:math)中讨论潜在空间那样提出相似性问题。
- en: 'We need to move back to the mathematical framings to set up how to use these
    vectors. Ultimately, we’re now in a latent space with users and items, but how
    can we do anything with that? Well you may already remember how to compare vector
    similarity. Let’s define the similarity to be *cosine-similarity*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要回到数学框架中来建立如何使用这些向量。最终，我们现在处于一个潜在空间中，其中包括用户和物品，但我们怎么能够做任何事情呢？也许你已经记得如何比较向量相似性了。让我们定义相似性为*cosine-similarity*：
- en: <math alttext="s i m left-parenthesis bold i comma bold x right-parenthesis
    equals StartFraction bold i dot bold x Over StartAbsoluteValue bold i EndAbsoluteValue
    asterisk StartAbsoluteValue bold x EndAbsoluteValue EndFraction" display="block"><mrow><mi>s</mi>
    <mi>i</mi> <mi>m</mi> <mrow><mo>(</mo> <mi>𝐢</mi> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>𝐢</mi><mo>·</mo><mi>𝐱</mi></mrow>
    <mrow><mfenced close="|" open="|"><mi>𝐢</mi></mfenced><mo>*</mo><mfenced close="|"
    open="|"><mi>𝐱</mi></mfenced></mrow></mfrac></mstyle></mrow></math>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="s i m left-parenthesis bold i comma bold x right-parenthesis
    equals StartFraction bold i dot bold x Over StartAbsoluteValue bold i EndAbsoluteValue
    asterisk StartAbsoluteValue bold x EndAbsoluteValue EndFraction" display="block"><mrow><mi>s</mi>
    <mi>i</mi> <mi>m</mi> <mrow><mo>(</mo> <mi>𝐢</mi> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>𝐢</mi><mo>·</mo><mi>𝐱</mi></mrow>
    <mrow><mfenced close="|" open="|"><mi>𝐢</mi></mfenced><mo>*</mo><mfenced close="|"
    open="|"><mi>𝐱</mi></mfenced></mrow></mfrac></mstyle></mrow></math>
- en: 'If we precompose our similarity with vector normalization, this is simply the
    inner product—*and this is an essential first step toward recommendation systems*.
    For convenience, let’s always assume this space we’re working in is after normalization,
    so all similarity measures are done on the unit sphere:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用向量归一化预先构造我们的相似性，这就是简单的内积—*这是推荐系统的一个重要的第一步*。为方便起见，让我们假设我们工作的空间是经过归一化的，因此所有相似度测量都是在单位球上进行的：
- en: <math alttext="r Subscript i comma x Baseline tilde s i m left-parenthesis bold
    i comma bold x right-parenthesis equals sigma-summation Underscript k Endscripts
    bold i Subscript k Baseline asterisk bold x Subscript k" display="block"><mrow><msub><mi>r</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow></msub> <mo>∼</mo> <mi>s</mi> <mi>i</mi>
    <mi>m</mi> <mrow><mo>(</mo> <mi>𝐢</mi> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow>
    <mo>=</mo> <munder><mo>∑</mo> <mi>k</mi></munder> <msub><mi>𝐢</mi> <mi>k</mi></msub>
    <mo>*</mo> <msub><mi>𝐱</mi> <mi>k</mi></msub></mrow></math>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="r Subscript i comma x Baseline tilde s i m left-parenthesis bold
    i comma bold x right-parenthesis equals sigma-summation Underscript k Endscripts
    bold i Subscript k Baseline asterisk bold x Subscript k" display="block"><mrow><msub><mi>r</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow></msub> <mo>∼</mo> <mi>s</mi> <mi>i</mi>
    <mi>m</mi> <mrow><mo>(</mo> <mi>𝐢</mi> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow>
    <mo>=</mo> <munder><mo>∑</mo> <mi>k</mi></munder> <msub><mi>𝐢</mi> <mi>k</mi></msub>
    <mo>*</mo> <msub><mi>𝐱</mi> <mi>k</mi></msub></mrow></math>
- en: 'This now approximates our ratings. But wait, dear reader, where are the learnable
    parameters? Let’s go ahead and make this a weighted summation, via a diagonal
    matrix <math alttext="upper A"><mi>A</mi></math> :'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这近似了我们的评分。但等等，亲爱的读者，学习参数在哪里？让我们继续，通过一个对角矩阵<math alttext="upper A"><mi>A</mi></math>，将其变为加权求和：
- en: <math alttext="r Subscript i comma x Baseline tilde s i m Superscript upper
    A Baseline left-parenthesis bold i comma bold x right-parenthesis equals sigma-summation
    Underscript k Endscripts a Subscript k Baseline asterisk bold i Subscript k Baseline
    asterisk bold x Subscript k" display="block"><mrow><msub><mi>r</mi> <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>∼</mo> <mi>s</mi> <mi>i</mi> <msup><mi>m</mi> <mi>A</mi></msup> <mrow><mo>(</mo>
    <mi>𝐢</mi> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo>
    <mi>k</mi></munder> <msub><mi>a</mi> <mi>k</mi></msub> <mo>*</mo> <msub><mi>𝐢</mi>
    <mi>k</mi></msub> <mo>*</mo> <msub><mi>𝐱</mi> <mi>k</mi></msub></mrow></math>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="r Subscript i comma x Baseline tilde s i m Superscript upper
    A Baseline left-parenthesis bold i comma bold x right-parenthesis equals sigma-summation
    Underscript k Endscripts a Subscript k Baseline asterisk bold i Subscript k Baseline
    asterisk bold x Subscript k" display="block"><mrow><msub><mi>r</mi> <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>∼</mo> <mi>s</mi> <mi>i</mi> <msup><mi>m</mi> <mi>A</mi></msup> <mrow><mo>(</mo>
    <mi>𝐢</mi> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo>
    <mi>k</mi></munder> <msub><mi>a</mi> <mi>k</mi></msub> <mo>*</mo> <msub><mi>𝐢</mi>
    <mi>k</mi></msub> <mo>*</mo> <msub><mi>𝐱</mi> <mi>k</mi></msub></mrow></math>
- en: 'This slight generalization already puts us in the world of statistical learning.
    You can probably already see how <math alttext="upper A"><mi>A</mi></math> can
    be used to learn which of the dimensions in this space are most important for
    approximating the ratings, but before we make that precise, let’s generalize yet
    once more:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个轻微的泛化已经将我们置于统计学习的世界中。你可能已经看到<math alttext="upper A"><mi>A</mi></math>如何用来学习这个空间中哪些维度对于逼近评分最重要，但在我们确立之前，让我们再次泛化：
- en: <math alttext="r Subscript i comma x Baseline tilde s i m Superscript upper
    A Baseline left-parenthesis bold i comma bold x right-parenthesis equals sigma-summation
    Underscript k comma l Endscripts a Subscript k l Baseline asterisk bold i Subscript
    k Baseline asterisk bold x Subscript l" display="block"><mrow><msub><mi>r</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow></msub> <mo>∼</mo> <mi>s</mi> <mi>i</mi>
    <msup><mi>m</mi> <mi>A</mi></msup> <mrow><mo>(</mo> <mi>𝐢</mi> <mo>,</mo> <mi>𝐱</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow></munder>
    <msub><mi>a</mi> <mrow><mi>k</mi><mi>l</mi></mrow></msub> <mo>*</mo> <msub><mi>𝐢</mi>
    <mi>k</mi></msub> <mo>*</mo> <msub><mi>𝐱</mi> <mi>l</mi></msub></mrow></math>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="r Subscript i comma x Baseline tilde s i m Superscript upper
    A Baseline left-parenthesis bold i comma bold x right-parenthesis equals sigma-summation
    Underscript k comma l Endscripts a Subscript k l Baseline asterisk bold i Subscript
    k Baseline asterisk bold x Subscript l" display="block"><mrow><msub><mi>r</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow></msub> <mo>∼</mo> <mi>s</mi> <mi>i</mi>
    <msup><mi>m</mi> <mi>A</mi></msup> <mrow><mo>(</mo> <mi>𝐢</mi> <mo>,</mo> <mi>𝐱</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo> <mrow><mi>k</mi><mo>,</mo><mi>l</mi></mrow></munder>
    <msub><mi>a</mi> <mrow><mi>k</mi><mi>l</mi></mrow></msub> <mo>*</mo> <msub><mi>𝐢</mi>
    <mi>k</mi></msub> <mo>*</mo> <msub><mi>𝐱</mi> <mi>l</mi></msub></mrow></math>
- en: 'This nets us even more parameters! We see that now <math alttext="s i m Superscript
    upper A Baseline left-parenthesis bold i comma bold x right-parenthesis equals
    bold i upper A bold x"><mrow><mi>s</mi> <mi>i</mi> <msup><mi>m</mi> <mi>A</mi></msup>
    <mrow><mo>(</mo> <mi>𝐢</mi> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>𝐢</mi> <mi>A</mi> <mi>𝐱</mi></mrow></math> , and we are only one step away
    from the familiar ground of linear regression. Currently, our model is in the
    form of a *bilinear regression*, so let’s utilize a little linear algebra. For
    the sake of exposition, let <math alttext="bold i element-of double-struck upper
    R Superscript n"><mrow><mi>𝐢</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mi>n</mi></msup></mrow></math>
    , <math alttext="bold x element-of double-struck upper R Superscript m"><mrow><mi>𝐱</mi>
    <mo>∈</mo> <msup><mi>ℝ</mi> <mi>m</mi></msup></mrow></math> , and <math alttext="upper
    A element-of double-struck upper R Superscript n times m"><mrow><mi>A</mi> <mo>∈</mo>
    <msup><mi>ℝ</mi> <mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow></msup></mrow></math>
    , and then we have this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了更多的参数！现在我们看到 <math alttext="s i m Superscript upper A Baseline left-parenthesis
    bold i comma bold x right-parenthesis equals bold i upper A bold x"><mrow><mi>s</mi>
    <mi>i</mi> <msup><mi>m</mi> <mi>A</mi></msup> <mrow><mo>(</mo> <mi>𝐢</mi> <mo>,</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>𝐢</mi> <mi>A</mi> <mi>𝐱</mi></mrow></math>
    ，我们只差一步就到达了线性回归的熟悉领域。目前，我们的模型处于*bilinear regression*的形式，因此让我们利用一点线性代数知识。为了阐述清楚，让我们设定
    <math alttext="bold i element-of double-struck upper R Superscript n"><mrow><mi>𝐢</mi>
    <mo>∈</mo> <msup><mi>ℝ</mi> <mi>n</mi></msup></mrow></math> ， <math alttext="bold
    x element-of double-struck upper R Superscript m"><mrow><mi>𝐱</mi> <mo>∈</mo>
    <msup><mi>ℝ</mi> <mi>m</mi></msup></mrow></math> ，以及 <math alttext="upper A element-of
    double-struck upper R Superscript n times m"><mrow><mi>A</mi> <mo>∈</mo> <msup><mi>ℝ</mi>
    <mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow></msup></mrow></math> ，然后我们有如下结果：
- en: <math alttext="bold vect left-parenthesis bold i asterisk bold x Superscript
    upper T Baseline right-parenthesis element-of double-struck upper R Superscript
    n asterisk m" display="block"><mrow><mi>𝐯𝐞𝐜𝐭</mi> <mfenced close=")" open="("
    separators=""><mi>𝐢</mi> <mo>*</mo> <msup><mi>𝐱</mi> <mi>T</mi></msup></mfenced>
    <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mi>n</mi><mo>*</mo><mi>m</mi></mrow></msup></mrow></math>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold vect left-parenthesis bold i asterisk bold x Superscript
    upper T Baseline right-parenthesis element-of double-struck upper R Superscript
    n asterisk m" display="block"><mrow><mi>𝐯𝐞𝐜𝐭</mi> <mfenced close=")" open="("
    separators=""><mi>𝐢</mi> <mo>*</mo> <msup><mi>𝐱</mi> <mi>T</mi></msup></mfenced>
    <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mi>n</mi><mo>*</mo><mi>m</mi></mrow></msup></mrow></math>
- en: 'We can simplify to the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简化为以下形式：
- en: <math alttext="s i m Superscript upper A Baseline left-parenthesis bold i comma
    bold x right-parenthesis equals bold i upper A bold x equals bold vect left-parenthesis
    bold i asterisk bold x Superscript upper T Baseline right-parenthesis asterisk
    bold vect left-parenthesis upper A right-parenthesis" display="block"><mrow><mi>s</mi>
    <mi>i</mi> <msup><mi>m</mi> <mi>A</mi></msup> <mrow><mo>(</mo> <mi>𝐢</mi> <mo>,</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>𝐢</mi> <mi>A</mi> <mi>𝐱</mi> <mo>=</mo>
    <mi>𝐯𝐞𝐜𝐭</mi> <mfenced close=")" open="(" separators=""><mi>𝐢</mi> <mo>*</mo>
    <msup><mi>𝐱</mi> <mi>T</mi></msup></mfenced> <mo>*</mo> <mi>𝐯𝐞𝐜𝐭</mi> <mfenced
    close=")" open="("><mi>A</mi></mfenced></mrow></math>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="s i m Superscript upper A Baseline left-parenthesis bold i comma
    bold x right-parenthesis equals bold i upper A bold x equals bold vect left-parenthesis
    bold i asterisk bold x Superscript upper T Baseline right-parenthesis asterisk
    bold vect left-parenthesis upper A right-parenthesis" display="block"><mrow><mi>s</mi>
    <mi>i</mi> <msup><mi>m</mi> <mi>A</mi></msup> <mrow><mo>(</mo> <mi>𝐢</mi> <mo>,</mo>
    <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>𝐢</mi> <mi>A</mi> <mi>𝐱</mi> <mo>=</mo>
    <mi>𝐯𝐞𝐜𝐭</mi> <mfenced close=")" open="(" separators=""><mi>𝐢</mi> <mo>*</mo>
    <msup><mi>𝐱</mi> <mi>T</mi></msup></mfenced> <mo>*</mo> <mi>𝐯𝐞𝐜𝐭</mi> <mfenced
    close=")" open="("><mi>A</mi></mfenced></mrow></math>
- en: 'If we make up notation for the right-hand side, you’ll find your friend linear
    regression waiting for you:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们为右侧的符号构造记号，你会发现你的好朋友线性回归正等待着你：
- en: <math alttext="bold v Subscript i x Baseline colon equals bold vect left-parenthesis
    bold i asterisk bold x Superscript upper T Baseline right-parenthesis comma beta
    colon equals bold vect left-parenthesis upper A right-parenthesis" display="block"><mrow><msub><mi>𝐯</mi>
    <mrow><mi>i</mi><mi>x</mi></mrow></msub> <mo>:</mo> <mo>=</mo> <mi>𝐯𝐞𝐜𝐭</mi> <mfenced
    close=")" open="(" separators=""><mi>𝐢</mi> <mo>*</mo> <msup><mi>𝐱</mi> <mi>T</mi></msup></mfenced>
    <mo>,</mo> <mi>β</mi> <mo>:</mo> <mo>=</mo> <mi>𝐯𝐞𝐜𝐭</mi> <mfenced close=")" open="("><mi>A</mi></mfenced></mrow></math>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i x Baseline colon equals bold vect left-parenthesis
    bold i asterisk bold x Superscript upper T Baseline right-parenthesis comma beta
    colon equals bold vect left-parenthesis upper A right-parenthesis" display="block"><mrow><msub><mi>𝐯</mi>
    <mrow><mi>i</mi><mi>x</mi></mrow></msub> <mo>:</mo> <mo>=</mo> <mi>𝐯𝐞𝐜𝐭</mi> <mfenced
    close=")" open="(" separators=""><mi>𝐢</mi> <mo>*</mo> <msup><mi>𝐱</mi> <mi>T</mi></msup></mfenced>
    <mo>,</mo> <mi>β</mi> <mo>:</mo> <mo>=</mo> <mi>𝐯𝐞𝐜𝐭</mi> <mfenced close=")" open="("><mi>A</mi></mfenced></mrow></math>
- en: 'Thus:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此：
- en: <math alttext="r Subscript i comma x Baseline tilde s i m Superscript upper
    A Baseline left-parenthesis bold i comma bold x right-parenthesis equals bold
    v Subscript i x Baseline beta" display="block"><mrow><msub><mi>r</mi> <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>∼</mo> <mi>s</mi> <mi>i</mi> <msup><mi>m</mi> <mi>A</mi></msup> <mrow><mo>(</mo>
    <mi>𝐢</mi> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝐯</mi>
    <mrow><mi>i</mi><mi>x</mi></mrow></msub> <mi>β</mi></mrow></math>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="r Subscript i comma x Baseline tilde s i m Superscript upper
    A Baseline left-parenthesis bold i comma bold x right-parenthesis equals bold
    v Subscript i x Baseline beta" display="block"><mrow><msub><mi>r</mi> <mrow><mi>i</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>∼</mo> <mi>s</mi> <mi>i</mi> <msup><mi>m</mi> <mi>A</mi></msup> <mrow><mo>(</mo>
    <mi>𝐢</mi> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝐯</mi>
    <mrow><mi>i</mi><mi>x</mi></mrow></msub> <mi>β</mi></mrow></math>
- en: With this computation behind us, we see that whether we wish to compute binary
    ratings, ordinal ratings, or likelihood estimation, the tools in our linear models
    toolbox can enter the party. We have available to us regularization and optimizers
    and any other fun we’re interested in from the linear models world.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些计算，我们看到，无论我们想计算二进制评分、序数评分还是概率估计，我们线性模型工具箱中的工具都可以派上用场。我们可以利用正则化和优化器，以及线性模型领域中其他任何有趣的东西。
- en: 'If these equations feel frustrating or painful, let me try to offer you a geometric
    mental model. Each item and user is in a high-dimensional space, and ultimately
    we’re trying to figure out which ones are closest to one another. People frequently
    misunderstand these geometries by imagining the tips of the vectors being near
    one another; this is not the case. These spaces are extremely high-dimensional,
    which results in the analogy being far from the truth. Instead, ask if *the values
    are similarly large in some of the vector indices.* This is a much simpler, but
    also more accurate, geometric view: there are some subspaces in the extremely
    high-dimensional space where the vectors point in the same direction.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些方程让你感到沮丧或痛苦，让我试着给你提供一个几何心理模型。每个项目和用户都处在一个高维空间中，最终我们试图弄清楚哪些项目彼此最接近。人们经常误解这些几何结构，认为向量的顶点彼此靠近；但事实并非如此。这些空间是极高维的，这导致这种类比与实际情况相去甚远。相反，问问“向量的某些分量是否相似地大”，这是一个更简单但更准确的几何视角：在这极高维的空间中，存在一些子空间，向量指向相同方向。
- en: This forms the foundation for where we are going but has serious limitations
    for large-scale recommender problems. You will see, however, that the feature-based
    learning still has its place in the cold-start regime.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这构成了我们接下来要探索的基础，但在大规模推荐问题中存在严重限制。然而，基于特征的学习仍然在冷启动阶段有其用武之地。
- en: Note that in addition to the preceding approach of building content-based features
    for a user, we may also have obvious user features that are obtained via queries
    to the user, or implicitly via other data collection; examples of these features
    include location, age range, and height.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，除了前面介绍的为用户构建基于内容的特征的方法外，我们还可以通过查询用户或通过其他数据收集隐式获取明显的用户特征；这些特征的示例包括位置、年龄范围和身高。
- en: Feature-Based Warm Starting
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于特征的启动
- en: As you saw in [Chapter 7](ch07.html#serving-and-architecture), there are a variety
    of ways to use features alongside some of the collaborative filtering (CF) and
    MF approaches we’ve presented. In particular, you saw how encoders built via a
    two-towers architecture can be used for fast feature-based recommendations in
    the cold-start scenario. Let’s look into this deeper and think carefully about
    features for new users or items.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第 7 章](ch07.html#serving-and-architecture)中看到的，除了我们提出的一些协同过滤（CF）和 MF 方法之外，还有多种方法可以在特征旁边使用它们。特别是，您看到了通过两塔架构构建的编码器如何在冷启动场景下用于快速基于特征的推荐。让我们深入研究这一点，认真考虑一下新用户或项目的特征。
- en: 'In [Chapter 9](#feature-counting), we built our bilinear factor model as a
    simple regression and, in fact, saw that all the standard ML modeling approaches
    would apply. However, we took the user embedding to be features learned from item
    interactions: that is, the content-based feature vector. If our goal is to build
    a recommendation algorithm that does not need a history of user ratings, obviously
    this construction will not suffice.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 9 章](#feature-counting)中，我们将我们的双线性因子模型构建为简单的回归，并且事实上，看到所有标准的 ML 建模方法都适用。然而，我们将用户嵌入视为从项目交互中学到的特征：也就是说，基于内容的特征向量。如果我们的目标是构建一个不需要用户评分历史记录的推荐算法，显然这种构建方法不够。
- en: 'We might begin by asking if the preceding factor regression approach could
    work in the pure user-feature setting—leave aside worries about the inner product
    that depended on a mutual embedding and just take everything to be pure matrices.
    While this is a reasonable idea that can yield some results, we may quickly identify
    the coarseness of this model: each user would then need to provide answers to
    queries <math alttext="q Subscript k"><msub><mi>q</mi> <mi>k</mi></msub></math>
    such that <math alttext="bold i element-of double-struck upper R Superscript k"><mrow><mi>𝐢</mi>
    <mo>∈</mo> <msup><mi>ℝ</mi> <mi>k</mi></msup></mrow></math> . Because the dimensionality
    of these user vectors scales linearly with the number of questions we’re willing
    and able to ask the user, we are passing along the difficulty of the problem to
    our user experience.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能首先要问的是在纯用户特征设置中前面的因子回归方法是否可行——暂且不管依赖于相互嵌入的内积的担忧，只把一切都看作是纯矩阵。虽然这是一个可以产生一些结果的合理想法，但我们可能很快就会识别出这个模型的粗糙性：每个用户都需要回答一些查询
    <math alttext="q Subscript k"><msub><mi>q</mi> <mi>k</mi></msub></math>，使得 <math
    alttext="bold i element-of double-struck upper R Superscript k"><mrow><mi>𝐢</mi>
    <mo>∈</mo> <msup><mi>ℝ</mi> <mi>k</mi></msup></mrow></math> 。因为这些用户向量的维度随着我们愿意和能够问用户的问题数量呈线性增长，我们正在将问题的难度传递给我们的用户体验。
- en: Because we intend on using CF via MF as our core model, we’d really like to
    find a way to smoothly transition from the feature-based model into this MF, ensuring
    we take advantage of user/item ratings as they emerge. In [“The Evaluation Flywheel”](ch07.html#sec:eval_flywheel),
    we discussed using inference results and their subsequent outcomes in real time
    to update the model, but how do we account for that in the modeling paradigm?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们打算将 CF 通过 MF 作为我们的核心模型，所以我们真的希望找到一种方法，可以从基于特征的模型平滑地过渡到这个 MF，确保我们利用用户/项目评分的出现。在[“评估飞轮”](ch07.html#sec:eval_flywheel)中，我们讨论了使用推理结果及其随后的实时结果来更新模型，但在建模范式中如何考虑这一点呢？
- en: 'In a latent-factor model obtained via MF, we have the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过 MF 获得的潜在因子模型中，我们有以下内容：
- en: <math alttext="bold u Subscript i Baseline bold v Subscript x" display="block"><mrow><msub><mi>𝐮</mi>
    <mi>i</mi></msub> <msub><mi>𝐯</mi> <mi>x</mi></msub></mrow></math>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold u Subscript i Baseline bold v Subscript x" display="block"><mrow><msub><mi>𝐮</mi>
    <mi>i</mi></msub> <msub><mi>𝐯</mi> <mi>x</mi></msub></mrow></math>
- en: Here, <math alttext="bold u Subscript i"><msub><mi>𝐮</mi> <mi>i</mi></msub></math>
    has a Gaussian prior with zero mean; this is why new users won’t yield useful
    ratings before they have interaction data. We thus say that the *user-matrix*
    has *zero-concentrated priors.* Our first strategy to including features in our
    MF is to simply build a better priors distribution.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，<math alttext="bold u Subscript i"><msub><mi>𝐮</mi> <mi>i</mi></msub></math>
    具有零均值的高斯先验；这就是为什么新用户在没有交互数据之前不会产生有用的评分。因此，我们说 *用户矩阵* 具有 *零集中先验*。我们包含特征在我们的 MF
    中的第一个策略是简单地构建一个更好的先验分布。
- en: 'More mathematically: we learn a regression model <math alttext="upper G left-parenthesis
    bold i right-parenthesis tilde bold u Subscript i"><mrow><mi>G</mi> <mrow><mo>(</mo>
    <mi>𝐢</mi> <mo>)</mo></mrow> <mo>∼</mo> <msub><mi>𝐮</mi> <mi>i</mi></msub></mrow></math>
    for initialization of our learned factor matrix, and this means we’re learning
    the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 更具数学性：我们学习一个回归模型 <math alttext="upper G left-parenthesis bold i right-parenthesis
    tilde bold u Subscript i"><mrow><mi>G</mi> <mrow><mo>(</mo> <mi>𝐢</mi> <mo>)</mo></mrow>
    <mo>∼</mo> <msub><mi>𝐮</mi> <mi>i</mi></msub></mrow></math> 用于初始化我们学到的因子矩阵，这意味着我们学习以下内容：
- en: <math alttext="s left-parenthesis i comma x right-parenthesis tilde bold w Subscript
    i x Baseline gamma plus alpha Subscript i Baseline plus beta Subscript x Baseline
    plus bold u Subscript i Baseline bold v Subscript x" display="block"><mrow><mi>s</mi>
    <mrow><mo>(</mo> <mi>i</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>∼</mo>
    <msub><mi>𝐰</mi> <mrow><mi>i</mi><mi>x</mi></mrow></msub> <mi>γ</mi> <mo>+</mo>
    <msub><mi>α</mi> <mi>i</mi></msub> <mo>+</mo> <msub><mi>β</mi> <mi>x</mi></msub>
    <mo>+</mo> <msub><mi>𝐮</mi> <mi>i</mi></msub> <msub><mi>𝐯</mi> <mi>x</mi></msub></mrow></math>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="s left-parenthesis i comma x right-parenthesis tilde bold w Subscript
    i x Baseline gamma plus alpha Subscript i Baseline plus beta Subscript x Baseline
    plus bold u Subscript i Baseline bold v Subscript x" display="block"><mrow><mi>s</mi>
    <mrow><mo>(</mo> <mi>i</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>∼</mo>
    <msub><mi>𝐰</mi> <mrow><mi>i</mi><mi>x</mi></mrow></msub> <mi>γ</mi> <mo>+</mo>
    <msub><mi>α</mi> <mi>i</mi></msub> <mo>+</mo> <msub><mi>β</mi> <mi>x</mi></msub>
    <mo>+</mo> <msub><mi>𝐮</mi> <mi>i</mi></msub> <msub><mi>𝐯</mi> <mi>x</mi></msub></mrow></math>
- en: Here, our <math alttext="bold w Subscript i x Baseline gamma"><mrow><msub><mi>𝐰</mi>
    <mrow><mi>i</mi><mi>x</mi></mrow></msub> <mi>γ</mi></mrow></math> is now a standard
    bilinear feature regression from user and item features, the bias terms are learned
    to estimate popularity or *rank inflation*, and our familiar MF terms are <math
    alttext="bold u Subscript i Baseline bold v Subscript x"><mrow><msub><mi>𝐮</mi>
    <mi>i</mi></msub> <msub><mi>𝐯</mi> <mi>x</mi></msub></mrow></math> .
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的<math alttext="bold w Subscript i x Baseline gamma"><mrow><msub><mi>𝐰</mi>
    <mrow><mi>i</mi><mi>x</mi></mrow></msub> <mi>γ</mi></mrow></math>现在是从用户和项目特征的标准双线性特征回归，偏差项被学习以估计流行度或*等级膨胀*，我们熟悉的MF术语是<math
    alttext="bold u Subscript i Baseline bold v Subscript x"><mrow><msub><mi>𝐮</mi>
    <mi>i</mi></msub> <msub><mi>𝐯</mi> <mi>x</mi></msub></mrow></math> 。
- en: Note that this approach provides a general strategy for including features into
    an MF model. How we fit the factors-features model is totally up to us, as are
    the optimization methods we wish to employ.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种方法提供了一种将特征包含到MF模型中的一般策略。我们如何拟合因子-特征模型完全取决于我们，以及我们希望采用的优化方法。
- en: 'Also note that instead of regression-based approaches, priors can be established
    via *k*-nearest neighbors in a purely feature-based embedding space. This modeling
    strategy is explored in great detail in [“Eliciting Auxiliary Information for
    Cold Start User Recommendation: A Survey”](https://oreil.ly/N4Ast) by Nor Aniza
    Abdullah et al. Compare this with the item-item content-based recommender from
    [Chapter 5](ch05.html#ch:pinterest-content), where the query is an item and similarity
    in item space is the link between the last item and the next.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，与基于回归的方法不同，先验可以通过在纯粹基于特征的嵌入空间中的*k*最近邻来建立。这种建模策略在Nor Aniza Abdullah等人的[“为冷启动用户推荐获取辅助信息：一项调查”](https://oreil.ly/N4Ast)中得到了详细探讨。与第5章中的基于项目内容的项目-项目推荐器进行比较，其中查询是一个项目，并且在项目空间中的相似性是上一个项目和下一个项目之间的联系。
- en: We have established a strategy and a collection of approaches to building our
    models via features. We’ve even seen how our MF will fall over for new users,
    only to be saved by a feature-based model. So why not stick to features? Why introduce
    factors at all?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定了一种通过特征构建我们的模型的策略和一系列方法。我们甚至看到我们的MF将为新用户服务，只能通过基于特征的模型来拯救它。那么为什么不坚持使用特征呢？为什么要引入因子？
- en: Segmentation Models and Hybrids
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割模型和混合模型
- en: Similar to our preceding discussion of warm-starting via features is the closely
    related concept of *demographic-based systems.* Note that *demographic* in this
    context need not refer explicitly to personally identifiable information and can
    refer to the user data collected during the sign-up process. Simple examples from
    book recommendations might include a user’s favorite genres, self-identified price
    preference, book-length preferences, and favorite author. Standard methods of
    clustering-based regression can be helpful in converting a small set of user features
    into recommendations for new users. For these coarse user features, building simple
    feature-based models like naive Bayes, can be especially effective.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们之前讨论的通过特征进行热启动的概念是与之密切相关的*基于人口统计的系统*。请注意，在这种情况下，“人口统计”不一定指的是个人可识别信息，可以指的是在注册过程中收集到的用户数据。关于书籍推荐的简单例子可能包括用户喜爱的流派、自我识别的价格偏好、书籍长度偏好和喜欢的作者。基于聚类的回归的标准方法可以帮助将一小组用户特征转换为对新用户的推荐。对于这些粗略的用户特征，构建像朴素贝叶斯这样的简单基于特征的模型可能特别有效。
- en: More generally, given user feature vectors, we can formulate a similarity measure
    and then user segments to make new-user recommendations. This should feel similar
    to feature-based recommenders, but instead of requiring usage of user features,
    we model the user’s containment in a segment and then build our factor model from
    the segment to different items.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，鉴于用户特征向量，我们可以制定一个相似度度量，然后使用用户分段进行新用户推荐。这应该与基于特征的推荐器相似，但我们不要求使用用户特征，而是对用户在段中的包含进行建模，然后从段到不同项目构建我们的因子模型。
- en: 'One way to imagine this approach is to consider the modeling problem as estimating
    the following for C, a user cluster:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 想象这种方法的一种方式是将建模问题看作是为C，一个用户群体，估计以下内容：
- en: <math alttext="r Subscript upper C comma x Baseline colon equals Avg left-parenthesis
    r Subscript bold i comma x Baseline bar bold i element-of upper C right-parenthesis"
    display="block"><mrow><msub><mi>r</mi> <mrow><mi>C</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>:</mo> <mo>=</mo> <mtext>Avg</mtext> <mrow><mo>(</mo> <msub><mi>r</mi> <mrow><mi>𝐢</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>∣</mo> <mi>𝐢</mi> <mo>∈</mo> <mi>C</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="r Subscript upper C comma x Baseline colon equals Avg left-parenthesis
    r Subscript bold i comma x Baseline bar bold i element-of upper C right-parenthesis"
    display="block"><mrow><msub><mi>r</mi> <mrow><mi>C</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>:</mo> <mo>=</mo> <mtext>Avg</mtext> <mrow><mo>(</mo> <msub><mi>r</mi> <mrow><mi>𝐢</mi><mo>,</mo><mi>x</mi></mrow></msub>
    <mo>∣</mo> <mi>𝐢</mi> <mo>∈</mo> <mi>C</mi> <mo>)</mo></mrow></mrow></math>
- en: Then we estimate <math alttext="upper P left-parenthesis bold j element-of upper
    C right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>𝐣</mi> <mo>∈</mo> <mi>C</mi>
    <mo>)</mo></mrow></math> , the probability a user <math alttext="bold j"><mi>𝐣</mi></math>
    is a member of <math alttext="upper C"><mi>C</mi></math> . We can easily imagine
    that we instead wish to use the probability associated with each cluster to build
    a bagging model, and have each cluster contributed to a weighted average rating.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们估计 <math alttext="upper P left-parenthesis bold j element-of upper C right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>𝐣</mi> <mo>∈</mo> <mi>C</mi> <mo>)</mo></mrow></math> ，即用户 <math
    alttext="bold j"><mi>𝐣</mi></math> 是属于 <math alttext="upper C"><mi>C</mi></math>
    的概率。我们可以轻松想象，我们反而希望使用与每个聚类相关联的概率来构建一个装袋模型，并且每个聚类都对加权平均评分做出贡献。
- en: While these ideas may not seem like interesting extensions to what we’ve built
    previously, in practice they can be enormously useful for fast, explainable recommendations
    for new users.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些想法可能看起来不像是我们之前构建的有趣扩展，但在实践中，它们对于新用户的快速、可解释的推荐非常有用。
- en: Also note that nothing in this construction is particular to the users; we can
    consider the *dual model* that takes the clustering to be at the level of the
    items and performs a similar process. Combining these models can provide the coarsest
    model of simply user segments to item groups, and utilizing several of these modeling
    approaches simultaneously can provide important and flexible models.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，这种构建方式并不特定于用户；我们可以考虑将聚类作为物品的层次结构来进行类似的过程。同时使用这些建模方法可以提供简单的用户段到物品组的模型，并且同时利用多种建模方法可以提供重要且灵活的模型。
- en: Tag-Based Recommenders
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于标签的推荐器
- en: One special case of the segmentation model for item-based recommenders is a
    *tag-based recommender*. This is a quite common first recommender to try when
    you have some human labels and need to quickly turn it into a working recommender.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 项目推荐模型中基于项目的推荐器的一个特殊案例是*基于标签的推荐器*。当你有一些人工标签并且需要快速将其转换为可工作的推荐器时，这是一个相当常见的首选推荐器。
- en: 'Let’s talk through a toy example: you have a personal digital wardrobe, where
    you’ve logged many features about each article of clothing in your personal closet.
    You want your fashion recommender to give you suggestions for what else to wear,
    given that you’ve selected one piece for the day. You wake up and see that it’s
    rainy outside, so you start by choosing a cozy cardigan. The model you’ve trained
    has found that cardigan has tags *outerwear* and *cozy*, which it knows correlate
    well with *bottoms* and *warm*—so it’s likely to recommend heavier jeans today.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个玩具例子来详细讨论：你有一个个人数字衣柜，你已经记录了每件衣物的许多特征。你希望你的时尚推荐器给出建议，告诉你在你选择了一件当天穿的衣服后，还可以穿什么。你醒来看到外面下雨了，所以你开始选择一件舒适的羊毛衫。你训练的模型发现羊毛衫具有*外套*和*舒适*这两个标签，它知道这些标签与*裤子*和*温暖*很好地相关，因此今天可能会建议你穿厚重的牛仔裤。
- en: The upside of a tag recommender is how explainable and understandable the recommendations
    are. The downside is that performance is directly tied to the amount of effort
    that’s put into tagging items.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 标签推荐器的优势在于推荐解释性强且容易理解。缺点是性能直接取决于标记物品所投入的工作量。
- en: Let’s discuss a slightly more involved example of a tag-based recommender that
    one of the authors built in collaboration with Ashraf Shaik and Eric Bunch for
    recommending blog posts.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一个稍微更复杂的基于标签的推荐器的示例，这是作者之一与Ashraf Shaik和Eric Bunch合作建立的，用于推荐博客文章。
- en: The goal was to warm-start the blog-post recommender by utilizing high-quality
    tags that classified the blogs into themes. One special aspect of this system
    was its rich hierarchical tagging maintained by the marketing team. In particular,
    each *tag type* had several values, and there were 11 tag types with up to 10
    values each. Blogs had values for each tag type and sometimes had multiple tags
    in a single tag type for the blog. This may sound a bit complicated, but suffice
    it to say that each blog post could have some of the 47 tags, and the tags were
    further grouped into types.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是通过利用由营销团队维护的高质量标签对博客进行分类主题来启动博文推荐器。该系统的一个特殊方面是其富有层次的标签化。特别是，每个*标签类型*都有多个值，共有11种标签类型，每种最多有10个值。博客对每个标签类型都有值，有时在一个标签类型的博客中有多个标签。这听起来可能有点复杂，但可以说每篇博文可能包含47个标签之一，并且这些标签进一步分组为类型。
- en: 'One of the first potential tasks is to use those tags to build a simple recommender,
    and we did, but doing so would mean missing a significant additional opportunity
    when afforded such high-quality tag data: evaluating our embeddings.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个首要任务是利用这些标签构建一个简单的推荐系统，我们确实做了，但这样做意味着在提供了如此高质量的标签数据时会错过一个重要的额外机会：评估我们的嵌入。
- en: First, we needed to understand how we could build user embeddings. Our plan
    was to average the blog embeddings a user had seen, a simple CF approach when
    you have a clear item embedding. Thus we wanted to train the best embedding model
    possible for these blogs. We started by considering models like BERT but were
    unsure whether the highly technical content would be meaningfully captured by
    our embedding model. This led us to realize that we could use the tags as a classifier
    dataset for our embedding. If we could test several embedding models by training
    a simple multilayer perceptron (MLP) to perform multilabel multiclassification
    for each tag type, where the input features were the embedding dimensions, then
    our embedding space would capture the content well.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要了解如何构建用户嵌入。我们的计划是对用户已经看过的博客嵌入进行平均，这是一种简单的 CF 方法，当您拥有明确的项嵌入时。因此，我们希望为这些博客训练最好的嵌入模型。我们开始考虑像
    BERT 这样的模型，但我们不确定高度技术性的内容是否会被我们的嵌入模型有意义地捕获。这使我们意识到，我们可以将标签用作嵌入的分类器数据集。如果我们可以通过训练一个简单的多层感知器（MLP）为每个标签类型执行多标签多分类，其中输入特征是嵌入的维度，那么我们的嵌入空间将很好地捕获内容。
- en: Some of the embedding models were of varying dimensions, and some were quite
    large, so we also first used a dimension reduction (UMAP) to a standard size before
    we trained the MLP. We used [F1 scores](https://oreil.ly/rYGsU) to determine which
    of the embedding models led to the best classification model for tags, and we
    used visual inspection to ensure the groups were as we’d hoped. This worked quite
    well and showed that some embeddings were much better than others.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一些嵌入模型的维度不同，有些非常大，因此我们首先使用了尺寸缩减（UMAP）到一个标准尺寸，然后再训练 MLP。我们使用 [F1 分数](https://oreil.ly/rYGsU)
    确定哪个嵌入模型导致了最佳的标签分类模型，并使用视觉检查确保分组符合我们的期望。这项工作做得相当好，并显示了一些嵌入比其他嵌入好得多。
- en: Hybridization
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合化
- en: 'You saw in the previous section how to blend our MF with simpler models by
    taking priors from the simpler models and learning how to transition away. Coarser
    approaches to this process of *hybridization* exist:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您看到了如何通过从较简单的模型中获取先验并学习如何过渡来将我们的 MF 与简单模型混合。存在更粗糙的方法来进行此过程的*混合*：
- en: Weighted combinations of models
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的加权组合
- en: This approach is incredibly powerful, and the weights can be learned in a standard
    Bayesian framework.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法非常强大，权重可以在标准的贝叶斯框架中学习。
- en: Multilevel modeling
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 多层建模
- en: This approach can include learning a model to select which recommendation model
    should be used, and then learning models in each regime. For example, we could
    use a tree-based model on user features when the user has fewer than 10 historical
    ratings and then use MF after that. A variety of multilevel approaches exist,
    including *switching* and *cascading*, which correspond roughly to voting and
    boosting, respectively.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以包括学习一个模型来选择应该使用哪个推荐模型，然后在每个区域学习模型。例如，当用户的历史评分少于 10 个时，我们可以在用户特征上使用基于树的模型，然后在此之后使用
    MF。存在各种多层次的方法，包括*切换*和*级联*，它们分别大致对应于投票和增强。
- en: Feature augmentation
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 特征增强
- en: This allows multiple vectors of features to be concatenated and a larger model
    to be learned. By definition, if we wish to combine feature vectors with factor
    vectors, like those coming from a CF, we will expect substantial nullity. Learning
    despite that nullity allows a somewhat naive combination of the different kinds
    of features to be fed into the model and operated on in all regimes of user activity.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许将多个特征向量串联起来，并学习一个更大的模型。根据定义，如果我们希望将来自 CF 的因子向量与特征向量组合，我们将期望有实质性的零度。尽管存在这种零度，但学习使得可以将不同类型的特征相对简单地组合到模型中，并在用户活动的所有区域进行操作。
- en: 'We can combine these models in a variety of useful ways. However, we take the
    position that instead of more complicated combinations of several models that
    work well in different paradigms, we will attempt to stick to a relatively straightforward
    model-service architecture by doing the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以多种有用的方式组合这些模型。然而，我们的立场是，与其使用在不同范式中表现良好的几种模型的更复杂组合，我们将尝试坚持使用相对简单的模型-服务架构，方法如下：
- en: Training the best model we can by using MF-based CF
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过基于MF的CF来训练我们能够得到的最佳模型
- en: Using user and item feature-based models for cold start
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于用户和物品特征的模型来解决冷启动问题
- en: Let’s see why we think feature-based modeling might not be the best strategy,
    even if we do it via neural networks and latent factor models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 看看为什么我们认为基于特征的建模可能不是最佳策略，即使我们通过神经网络和潜在因子模型来实现它。
- en: Limitations of Bilinear Models
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bilinear Models的局限性
- en: We started this chapter by describing *bilinear modeling* approaches, and immediately
    you should take warning—they’re linear relationships. You can immediately wonder,
    “Are there really linear relationships between the features of my users and items
    and the pairwise affinity?”
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从描述*bilinear modeling*方法开始这一章节，但请立即注意——这些都是线性关系。你可能会立即想到：“我的用户和物品的特征之间真的存在线性关系吗？”
- en: The answer to this question might depend on the number of features, or it might
    not. Either way, skepticism is appropriate, and in practice the answer is overwhelmingly
    *no*. You might think, “Well then, as it is a linear approximation, MF also cannot
    succeed,” but that’s not so clear-cut. In fact, MF suggests that the linear relationship
    is *between the latent factors*, not the actual features. This subtle difference
    makes a world of difference.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个问题的答案可能取决于特征的数量，或者也可能不取决于它。无论哪种情况，持怀疑态度是恰当的，在实践中答案是压倒性地*否定*的。你可能会想，“那么，作为线性近似，MF也不能成功”，但事实并非如此明确。实际上，MF表明线性关系是*在潜在因子之间*，而不是实际特征之间。这种微妙的差别产生了天壤之别的效果。
- en: One important callout before we move on to simpler ideas is that neural networks
    with nonlinear activation functions can be used to build feature-based methods.
    This domain has had some successes, but ultimately a surprising and important
    result is that [neural CF does not outperform matrix factorization](https://oreil.ly/rFWaS).
    This doesn’t suggest that there are no useful approaches for feature-based models
    utilizing MLPs, but it does defray some of our worries about MF being *too linear*.
    So why not use more feature-based approaches?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们转向更简单的想法之前，需要强调的一点是，带有非线性激活函数的神经网络可以用来构建基于特征的方法。这个领域取得了一些成功，但最终一个令人惊讶且重要的结果是，[神经网络CF并没有超越矩阵分解](https://oreil.ly/rFWaS)。这并不意味着没有利用MLP的基于特征模型的有效方法，但它确实减少了我们对MF“过于线性”的担忧。那么，为什么不使用更多基于特征的方法呢？
- en: 'The first most obvious challenge for content-based, demographic-based, and
    any other feature-based method is *getting the features*. Let’s consider the dual
    problems:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 面向内容、人口统计学和其他基于特征的方法的第一个最明显的挑战是*获取这些特征*。让我们考虑以下双重问题：
- en: Features for users
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 用户的特征
- en: If we want to collect features for users, we need to either ask them a series
    of queries or infer those features implicitly. Inferring these via exogenous signals
    is noisy and limited, but each query that we ask the user increases the likelihood
    of onboarding drop-off. When we think of user-onboarding funnels, we know that
    each additional prompt or question incurs another chance that the user will not
    complete the onboarding. This effect accumulates quickly, and without users making
    it through the funnel, the recommendation system won’t be very useful.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要收集用户的特征，我们需要向他们提出一系列查询或隐含地推断这些特征。通过外部信号推断这些特征是嘈杂且有限的，但每次向用户提问都增加了用户流失的可能性。当我们考虑用户引导漏斗时，我们知道每个额外的提示或问题都增加了用户未能完成引导的机会。这种效应迅速累积，如果用户不能通过引导，推荐系统将变得不太有用。
- en: Features for items
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 物品的特征
- en: On the flip side, creating features for items is a heavily manual task. While
    many businesses need to do this task to serve other purposes as well, it still
    incurs a significant cost in many cases. If the features are to be useful, they
    need to be of high quality, which incurs more debt. But most importantly, if the
    number of items is extremely large, the cost may quickly get out of reach. For
    large-scale recommendation problems, manually adding features is simply infeasible.
    This is where automatic feature-engineering models can help.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一方面，为物品创建特征是一项非常手动的任务。尽管许多企业需要执行此任务以服务其他目的，但在许多情况下，这仍然会产生显著成本。如果要使特征发挥作用，它们需要是高质量的，这会增加更多的成本。但更重要的是，如果物品数量极其庞大，成本可能会迅速超出承受范围。对于大规模推荐问题，手动添加特征根本是不可行的。这就是自动特征工程模型可以发挥作用的地方。
- en: Another significant issue in these feature-based models is *separability* or
    *distinguishability*. These models are not useful if the features cannot separate
    the items or users well. This leads to compounding problems as the cardinality
    increases.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基于特征的模型中另一个重要问题是*可分离性*或*可区分性*。如果特征不能很好地区分物品或用户，这些模型就没有用。随着基数的增加，这会导致复合问题。
- en: Finally, in many recommendation problems, we start with the assumption that
    taste or preference is extremely personal. We fundamentally believe that our interest
    in a book will have less to do with the number of pages and publication date than
    how it connects with us and our personal experience (*our deepest apologies to
    anyone who bought this book based on page number and publication date*). CF—while
    simple in concept—speaks better to these connections via a *shared experience
    network*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在许多推荐问题中，我们假设品味或偏好是非常个人化的。我们基本上认为我们对一本书的兴趣与页数和出版日期的关系要比它与我们个人经历的连接更少（*我们深表歉意，如果有人是基于页数和出版日期购买了这本书*）。虽然CF在概念上很简单，但通过*共享体验网络*更好地传达了这些联系。
- en: Counting Recommenders
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数推荐
- en: Here we will use the simplest feature type, simple counting. Counting the frequency
    and pairwise frequencies will provide a simple but useful set of initial models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用最简单的特征类型，即简单计数。计算频率和成对频率将提供一组简单但有用的初始模型。
- en: Return to the Most-Popular-Item Recommender
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 返回到最受欢迎的物品推荐器
- en: Our super simple scheme from before, implementing the MPIR, provided us with
    a convenient toy model, but what are the practical considerations of deploying
    an MPIR? It turns out that the MPIR provides an excellent framework for getting
    started on a Bayesian approximation approach to recommendations. Note that in
    this section, we’re not even considering a personalized recommender; everything
    here is reward maximization across the entire user population. We follow the treatment
    in [*Statistical Methods for Recommender Systems*](https://oreil.ly/kKulC) by
    Deepak K. Agarwal and Bee-Chung Chen (Cambridge University Press).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前的超简单方案，实现MPIR，为我们提供了一个方便的玩具模型，但部署MPIR的实际考虑是什么？事实证明，MPIR为开始基于贝叶斯逼近方法的奖励最大化提供了一个优秀的框架。请注意，在本节中，我们甚至没有考虑个性化推荐；这里的一切都是跨整个用户群体的奖励最大化。我们遵循了Deepak
    K. Agarwal和Bee-Chung Chen（剑桥大学出版社）在[*推荐系统的统计方法*](https://oreil.ly/kKulC)中的处理。
- en: 'For the sake of simplicity, let’s consider *click-through rate* (*CTR*) as
    our simple metric to optimize. Our formulation is as follows: we have <math alttext="script
    upper I equals StartSet i EndSet"><mrow><mi>ℐ</mi> <mo>=</mo> <mfenced close="}"
    open="{"><mi>i</mi></mfenced></mrow></math> items available to recommend and initially
    *only one time period* in which to do it, and we’re interested in an *allocation
    plan*, or a set of proportions <math alttext="x Subscript i Baseline comma sigma-summation
    Underscript i element-of script upper I Endscripts x Subscript i Baseline equals
    1 comma"><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mo>∑</mo>
    <mrow><mi>i</mi><mo>∈</mo><mi>ℐ</mi></mrow></msub> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>=</mo> <mn>1</mn> <mo>,</mo></mrow></math> for how to recommend items. This
    can be seen as a very simple multiarmed bandit problem with the reward given by
    the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，让我们将*点击率*（*CTR*）作为我们要优化的简单度量标准。我们的公式如下：我们有 <math alttext="script upper
    I equals StartSet i EndSet"><mrow><mi>ℐ</mi> <mo>=</mo> <mfenced close="}" open="{"><mi>i</mi></mfenced></mrow></math>
    可推荐的项目，并且最初*仅有一个时间段*来进行推荐，我们对*分配计划*或比例集 <math alttext="x Subscript i Baseline
    comma sigma-summation Underscript i element-of script upper I Endscripts x Subscript
    i Baseline equals 1 comma"><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo>
    <msub><mo>∑</mo> <mrow><mi>i</mi><mo>∈</mo><mi>ℐ</mi></mrow></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>=</mo> <mn>1</mn> <mo>,</mo></mrow></math> 感兴趣。这可以看作是一个非常简单的多臂赌博机问题，其奖励由以下给出：
- en: <math alttext="upper R left-parenthesis bold x bold comma bold c right-parenthesis
    equals sigma-summation Underscript i element-of script upper I Endscripts c Subscript
    i Baseline asterisk left-parenthesis upper N asterisk x Subscript i Baseline right-parenthesis"
    display="block"><mrow><mi>R</mi> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>,</mo> <mi>𝐜</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo> <mrow><mi>i</mi><mo>∈</mo><mi>ℐ</mi></mrow></munder>
    <msub><mi>c</mi> <mi>i</mi></msub> <mo>*</mo> <mrow><mo>(</mo> <mi>N</mi> <mo>*</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R left-parenthesis bold x bold comma bold c right-parenthesis
    equals sigma-summation Underscript i element-of script upper I Endscripts c Subscript
    i Baseline asterisk left-parenthesis upper N asterisk x Subscript i Baseline right-parenthesis"
    display="block"><mrow><mi>R</mi> <mrow><mo>(</mo> <mi>𝐱</mi> <mo>,</mo> <mi>𝐜</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mo>∑</mo> <mrow><mi>i</mi><mo>∈</mo><mi>ℐ</mi></mrow></munder>
    <msub><mi>c</mi> <mi>i</mi></msub> <mo>*</mo> <mrow><mo>(</mo> <mi>N</mi> <mo>*</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: Here, <math alttext="c Subscript i"><msub><mi>c</mi> <mi>i</mi></msub></math>
    represents prior distributions of CTR for each item. It’s plain to see that maximizing
    this reward is achieved by allocating all recommendations to the item with greatest
    <math alttext="p Subscript i"><msub><mi>p</mi> <mi>i</mi></msub></math> , i.e.,
    picking the most popular item in terms of CTR.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，<math alttext="c Subscript i"><msub><mi>c</mi> <mi>i</mi></msub></math> 表示每个项目的点击率先验分布。显而易见，通过将所有推荐分配给具有最大
    <math alttext="p Subscript i"><msub><mi>p</mi> <mi>i</mi></msub></math> 的项目来最大化奖励，即选择在点击率上最受欢迎的项目。
- en: This setup makes it obvious that if we have strong confidence in our priors,
    this problem seems trivial. So let’s move to a case where we have a mismatch in
    confidence.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置明显表明，如果我们对先验有强烈的信心，这个问题似乎很简单。因此，让我们转向一个我们在信心上存在不匹配的案例。
- en: 'Let’s consider *two time periods*, <math alttext="upper N 0"><msub><mi>N</mi>
    <mn>0</mn></msub></math> and <math alttext="upper N 1"><msub><mi>N</mi> <mn>1</mn></msub></math>
    , as indicating the number of user visits. Note that we think of 0 as the past
    and 1 as the future in this model. Let’s assume that we offer *only two items*
    and that, somewhat mysteriously, for one item we have 100% confidence in its CTR
    in each time period: <math alttext="q 0"><msub><mi>q</mi> <mn>0</mn></msub></math>
    and <math alttext="q 1"><msub><mi>q</mi> <mn>1</mn></msub></math> will denote
    these rates, respectively. In contrast, we have only priors for our second item:
    <math alttext="p 0 tilde script upper P left-parenthesis theta 0 right-parenthesis"><mrow><msub><mi>p</mi>
    <mn>0</mn></msub> <mo>∼</mo> <mi>𝒫</mi> <mrow><mo>(</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></mrow></math> and <math alttext="p 1 tilde script upper P left-parenthesis
    theta 1 right-parenthesis"><mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>∼</mo>
    <mi>𝒫</mi> <mrow><mo>(</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    will denote these rates, respectively, and we regard <math alttext="theta Subscript
    i"><msub><mi>θ</mi> <mi>i</mi></msub></math> as a state vector. We again notate
    the allocations with <math alttext="x Subscript i comma t"><msub><mi>x</mi> <mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></math>
    , where now the second index refers to time period. Then we can simply compute
    the expected number of clicks as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑 *两个时间段*，<math alttext="upper N 0"><msub><mi>N</mi> <mn>0</mn></msub></math>
    和 <math alttext="upper N 1"><msub><mi>N</mi> <mn>1</mn></msub></math> ，表示用户访问的次数。请注意，在这个模型中，我们将
    0 视为过去，1 视为未来。让我们假设我们只提供 *两个物品*，并且有点神秘地，对于一个物品，我们对其在每个时间段内的 CTR 有 100% 的信心：分别用
    <math alttext="q 0"><msub><mi>q</mi> <mn>0</mn></msub></math> 和 <math alttext="q
    1"><msub><mi>q</mi> <mn>1</mn></msub></math> 表示这些比率。相反，我们只有第二个物品的先验：分别用 <math
    alttext="p 0 tilde script upper P left-parenthesis theta 0 right-parenthesis"><mrow><msub><mi>p</mi>
    <mn>0</mn></msub> <mo>∼</mo> <mi>𝒫</mi> <mrow><mo>(</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></mrow></math> 和 <math alttext="p 1 tilde script upper P left-parenthesis
    theta 1 right-parenthesis"><mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>∼</mo>
    <mi>𝒫</mi> <mrow><mo>(</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    表示这些比率，我们把 <math alttext="theta Subscript i"><msub><mi>θ</mi> <mi>i</mi></msub></math>
    视为状态向量。我们再次用 <math alttext="x Subscript i comma t"><msub><mi>x</mi> <mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></math>
    表示分配，这里第二个索引现在指的是时间段。然后我们可以简单地计算预期点击数如下：
- en: <math alttext="double-struck upper E left-bracket upper N 0 asterisk x 0 left-parenthesis
    p 0 minus q 0 right-parenthesis plus upper N 1 asterisk x 1 left-parenthesis p
    1 minus q 1 right-parenthesis right-bracket plus q 0 upper N 0 plus q 1 upper
    N 1" display="block"><mrow><mi>𝔼</mi> <mfenced close="]" open="[" separators=""><msub><mi>N</mi>
    <mn>0</mn></msub> <mo>*</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mfenced close=")"
    open="(" separators=""><msub><mi>p</mi> <mn>0</mn></msub> <mo>-</mo> <msub><mi>q</mi>
    <mn>0</mn></msub></mfenced> <mo>+</mo> <msub><mi>N</mi> <mn>1</mn></msub> <mo>*</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mfenced close=")" open="(" separators=""><msub><mi>p</mi>
    <mn>1</mn></msub> <mo>-</mo> <msub><mi>q</mi> <mn>1</mn></msub></mfenced></mfenced>
    <mo>+</mo> <msub><mi>q</mi> <mn>0</mn></msub> <msub><mi>N</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>q</mi> <mn>1</mn></msub> <msub><mi>N</mi> <mn>1</mn></msub></mrow></math>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="double-struck upper E left-bracket upper N 0 asterisk x 0 left-parenthesis
    p 0 minus q 0 right-parenthesis plus upper N 1 asterisk x 1 left-parenthesis p
    1 minus q 1 right-parenthesis right-bracket plus q 0 upper N 0 plus q 1 upper
    N 1" display="block"><mrow><mi>𝔼</mi> <mfenced close="]" open="[" separators=""><msub><mi>N</mi>
    <mn>0</mn></msub> <mo>*</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mfenced close=")"
    open="(" separators=""><msub><mi>p</mi> <mn>0</mn></msub> <mo>-</mo> <msub><mi>q</mi>
    <mn>0</mn></msub></mfenced> <mo>+</mo> <msub><mi>N</mi> <mn>1</mn></msub> <mo>*</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mfenced close=")" open="(" separators=""><msub><mi>p</mi>
    <mn>1</mn></msub> <mo>-</mo> <msub><mi>q</mi> <mn>1</mn></msub></mfenced></mfenced>
    <mo>+</mo> <msub><mi>q</mi> <mn>0</mn></msub> <msub><mi>N</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>q</mi> <mn>1</mn></msub> <msub><mi>N</mi> <mn>1</mn></msub></mrow></math>
- en: This is maximized by assuming a distribution for <math alttext="p 1"><msub><mi>p</mi>
    <mn>1</mn></msub></math> as a function of <math alttext="x 0"><msub><mi>x</mi>
    <mn>0</mn></msub></math> and <math alttext="p 0"><msub><mi>p</mi> <mn>0</mn></msub></math>
    . With distributional assumptions that <math alttext="p 0"><msub><mi>p</mi> <mn>0</mn></msub></math>
    is gamma distributed and <math alttext="p 1"><msub><mi>p</mi> <mn>1</mn></msub></math>
    is normally distributed, we can treat this as a convex optimization problem to
    maximize the clicks. See *Statistical Methods for Recommender Systems* for a full
    treatment of the statistics.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过假设 <math alttext="p 1"><msub><mi>p</mi> <mn>1</mn></msub></math> 关于 <math
    alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math> 和 <math alttext="p 0"><msub><mi>p</mi>
    <mn>0</mn></msub></math> 的分布进行优化的。通过假设 <math alttext="p 0"><msub><mi>p</mi> <mn>0</mn></msub></math>
    为 gamma 分布，<math alttext="p 1"><msub><mi>p</mi> <mn>1</mn></msub></math> 为正态分布，我们可以将其视为最大化点击的凸优化问题。详见《推荐系统的统计方法》以获取统计学的全面处理。
- en: This toy example extends in both dimensions to model larger item sets and more
    time windows and provides us with relatively straightforward intuition about the
    relationship between our priors for each item and time step during this step-forward
    optimization.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个玩具示例在两个维度上扩展，以建模更大的项目集和更多时间窗口，并且为我们提供了相对直观的关于在这一步优化中每个项目和时间步骤的先验关系的理解。
- en: 'Let’s put this recommender in context: we’ve started with item popularity and
    generalized to a Bayesian recommender that learns with respect to user feedback.
    You might consider a recommender like this for a very trend-based recommendations
    context like news; popular stories are often important, but that can change rapidly,
    and we want to be learning from user behavior.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个推荐系统放在环境中：我们从项目流行度开始，并推广到一个基于贝叶斯推荐系统，该系统根据用户反馈进行学习。你可以考虑像这样的推荐系统用于非常趋势导向的推荐环境，比如新闻；热门故事通常很重要，但是这可能会迅速改变，我们希望从用户行为中学习。
- en: Correlation Mining
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关挖掘
- en: We’ve seen ways to use correlations between features of items and recommendations,
    but we should not forget to use correlations between items themselves. Think back
    to our early discussions of cheese in [Chapter 2](ch02.html#ch:user-item) ([Figure 2-1](ch02.html#fig:cheese-ratings));
    we said that our CF gave us a way to find mutual cheese tastes to recommend new
    cheeses. This was built on the notion of ratings, but we can abstract away from
    the ratings and simply look at the correlations of items a user chooses. You can
    imagine for an ecommerce bookseller that a user’s choice of one book to read may
    be useful in recommending others—even if that user chooses not to rate the first
    book. We also saw this phenomena in [Chapter 8](ch08.html#ch:wikipedia-e2e) as
    we used the co-occurrence of tokens in Wikipedia entries.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何利用物品特征与推荐之间的相关性，但我们不应忽视物品本身之间的相关性。回想一下我们在[第二章](ch02.html#ch:user-item)中关于奶酪的早期讨论（[图2-1](ch02.html#fig:cheese-ratings)）；我们说我们的协同过滤提供了一种找到相互喜欢的奶酪品味以推荐新奶酪的方法。这建立在评分的概念上，但我们可以从评分中抽象出来，只需看用户选择物品的相关性。你可以想象对于一个电子商务书店来说，用户选择阅读一本书可能对推荐其他书籍有用，即使用户选择不给第一本书评分。我们在[第八章](ch08.html#ch:wikipedia-e2e)中也看到了这种现象，当我们使用维基百科条目中的词元共现时。
- en: We introduced the co-occurrence matrix as the multidimensional array of counts
    where two items, <math alttext="i"><mi>i</mi></math> and <math alttext="j"><mi>j</mi></math>
    , co-occur. Let’s take a moment to discuss co-occurrence a bit more deeply.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了共现矩阵作为计数的多维数组，其中两个物品 <math alttext="i"><mi>i</mi></math> 和 <math alttext="j"><mi>j</mi></math>
    的共现。让我们花点时间更深入地讨论共现。
- en: Co-occurrence is context dependent; for our Wikipedia articles, we considered
    co-occurrence of tokens in an article. In the case of ecommerce, co-occurrence
    can be two items purchased by the same user. For ads, co-occurrence can be two
    things that the user clicked, and so on. Mathematically, given users and items,
    we construct an *incidence vector* for each user, the binary vector of one-hot
    encoded features for each item that they interacted with. Those vectors are stacked
    into a vector to yield a <math alttext="number-sign left-parenthesis u s e r s
    right-parenthesis times number-sign left-parenthesis i t e m s right-parenthesis"><mrow><mo>#</mo>
    <mo>(</mo> <mi>u</mi> <mi>s</mi> <mi>e</mi> <mi>r</mi> <mi>s</mi> <mo>)</mo> <mo>×</mo>
    <mo>#</mo> <mo>(</mo> <mi>i</mi> <mi>t</mi> <mi>e</mi> <mi>m</mi> <mi>s</mi> <mo>)</mo></mrow></math>
    matrix in which each row is a user, each column is an item, and the elements equal
    1 when a user-item pair has interacted.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 共现是依赖于上下文的；对于我们的维基百科文章，我们考虑了文章中词元的共现。在电子商务的情况下，共现可以是同一用户购买的两个物品。对于广告，共现可以是用户点击的两个物品，等等。从数学上讲，给定用户和物品，我们为每个用户构建一个*关联向量*，即一个二元向量，其中每个与之交互的物品是一个独热编码的特征。这些向量堆叠成一个向量，生成一个<math
    alttext="number-sign left-parenthesis u s e r s right-parenthesis times number-sign
    left-parenthesis i t e m s right-parenthesis"><mrow><mo>#</mo> <mo>(</mo> <mi>u</mi>
    <mi>s</mi> <mi>e</mi> <mi>r</mi> <mi>s</mi> <mo>)</mo> <mo>×</mo> <mo>#</mo> <mo>(</mo>
    <mi>i</mi> <mi>t</mi> <mi>e</mi> <mi>m</mi> <mi>s</mi> <mo>)</mo></mrow></math>
    矩阵，其中每行是一个用户，每列是一个物品，当用户-物品对有交互时元素等于1。
- en: To be mathematically precise, a *user-item incidence structure* is a collection
    of sets of user interactions, <math alttext="StartSet y Subscript u Baseline EndSet
    Subscript u element-of upper U"><msub><mfenced close="}" open="{" separators=""><msub><mi>y</mi>
    <mi>u</mi></msub></mfenced> <mrow><mi>u</mi><mo>∈</mo><mi>U</mi></mrow></msub></math>
    , with items <math alttext="StartSet x Subscript i Baseline EndSet Subscript i
    element-of upper I"><msub><mfenced close="}" open="{" separators=""><msub><mi>x</mi>
    <mi>i</mi></msub></mfenced> <mrow><mi>i</mi><mo>∈</mo><mi>I</mi></mrow></msub></math>
    , where <math alttext="upper U"><mi>U</mi></math> indexes users and <math alttext="upper
    I"><mi>I</mi></math> indexes items.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了数学上的精确性，*用户-物品关联结构*是用户交互集合的集合，<math alttext="StartSet y Subscript u Baseline
    EndSet Subscript u element-of upper U"><msub><mfenced close="}" open="{" separators=""><msub><mi>y</mi>
    <mi>u</mi></msub></mfenced> <mrow><mi>u</mi><mo>∈</mo><mi>U</mi></mrow></msub></math>
    ，具有物品 <math alttext="StartSet x Subscript i Baseline EndSet Subscript i element-of
    upper I"><msub><mfenced close="}" open="{" separators=""><msub><mi>x</mi> <mi>i</mi></msub></mfenced>
    <mrow><mi>i</mi><mo>∈</mo><mi>I</mi></mrow></msub></math> 的集合，其中 <math alttext="upper
    U"><mi>U</mi></math> 索引用户，<math alttext="upper I"><mi>I</mi></math> 索引物品。
- en: 'The associated *user-item incidence matrix*, <math alttext="script upper U"><mi>𝒰</mi></math>
    , is the binary matrix with rows indexed by sets, and columns indexed by nodes,
    such that elements are as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的*用户-物品关联矩阵*，<math alttext="script upper U"><mi>𝒰</mi></math> ，是二进制矩阵，其行由集合索引，列由节点索引，元素如下：
- en: <math alttext="e Subscript y Sub Subscript u Subscript comma x Sub Subscript
    i Subscript Baseline equals StartLayout Enlarged left-brace 1st Row 1st Column
    1 2nd Column x Subscript i Baseline element-of y Subscript u Baseline 2nd Row
    1st Column 0 2nd Column otherwise EndLayout" display="block"><mrow><msub><mi>e</mi>
    <mrow><msub><mi>y</mi> <mi>u</mi></msub> <mo>,</mo><msub><mi>x</mi> <mi>i</mi></msub></mrow></msub>
    <mo>=</mo> <mfenced close="" open="{" separators=""><mtable><mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>∈</mo> <msub><mi>y</mi>
    <mi>u</mi></msub></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mtext>otherwise</mtext></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="e Subscript y Sub Subscript u Subscript comma x Sub Subscript
    i Subscript Baseline equals StartLayout Enlarged left-brace 1st Row 1st Column
    1 2nd Column x Subscript i Baseline element-of y Subscript u Baseline 2nd Row
    1st Column 0 2nd Column otherwise EndLayout" display="block"><mrow><msub><mi>e</mi>
    <mrow><msub><mi>y</mi> <mi>u</mi></msub> <mo>,</mo><msub><mi>x</mi> <mi>i</mi></msub></mrow></msub>
    <mo>=</mo> <mfenced close="" open="{" separators=""><mtable><mtr><mtd columnalign="left"><mn>1</mn></mtd>
    <mtd columnalign="left"><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>∈</mo> <msub><mi>y</mi>
    <mi>u</mi></msub></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mn>0</mn></mtd>
    <mtd columnalign="left"><mtext>otherwise</mtext></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'The *co-occurrence of* <math alttext="x Subscript a"><msub><mi>x</mi> <mi>a</mi></msub></math>
    *and* <math alttext="x Subscript b"><msub><mi>x</mi> <mi>b</mi></msub></math>
    is the order of the set <math alttext="StartSet y Subscript u Baseline bar x Subscript
    a Baseline element-of y Subscript u Baseline and x Subscript b Baseline element-of
    y Subscript u Baseline EndSet"><mfenced close="}" open="{" separators=""><msub><mi>y</mi>
    <mi>u</mi></msub> <mo>∣</mo> <msub><mi>x</mi> <mi>a</mi></msub> <mo>∈</mo> <msub><mi>y</mi>
    <mi>u</mi></msub> <mtext>and</mtext> <msub><mi>x</mi> <mi>b</mi></msub> <mo>∈</mo>
    <msub><mi>y</mi> <mi>u</mi></msub></mfenced></math> . We can also write that as
    a matrix that can be computed via a simple formula; let <math alttext="upper C
    Subscript script upper I"><msub><mi>C</mi> <mi>ℐ</mi></msub></math> be the co-occurrences
    matrix—i.e., the matrix with rows and columns indexed by <math alttext="StartSet
    x Subscript i Baseline EndSet Subscript i element-of upper I"><msub><mfenced close="}"
    open="{" separators=""><msub><mi>x</mi> <mi>i</mi></msub></mfenced> <mrow><mi>i</mi><mo>∈</mo><mi>I</mi></mrow></msub></math>
    and with elements that are the co-occurrences of the indices. Then we use the
    following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="x Subscript a"><msub><mi>x</mi> <mi>a</mi></msub></math> 的 *共现*
    和 <math alttext="x Subscript b"><msub><mi>x</mi> <mi>b</mi></msub></math> 的 *共现*
    是集合 <math alttext="StartSet y Subscript u Baseline bar x Subscript a Baseline
    element-of y Subscript u Baseline and x Subscript b Baseline element-of y Subscript
    u Baseline EndSet"><mfenced close="}" open="{" separators=""><msub><mi>y</mi>
    <mi>u</mi></msub> <mo>∣</mo> <msub><mi>x</mi> <mi>a</mi></msub> <mo>∈</mo> <msub><mi>y</mi>
    <mi>u</mi></msub> <mtext>和</mtext> <msub><mi>x</mi> <mi>b</mi></msub> <mo>∈</mo>
    <msub><mi>y</mi> <mi>u</mi></msub></mfenced></math> 的顺序。我们也可以将其写为一个可以通过简单公式计算的矩阵；令
    <math alttext="upper C Subscript script upper I"><msub><mi>C</mi> <mi>ℐ</mi></msub></math>
    为共现矩阵——即行和列的索引由 <math alttext="StartSet x Subscript i Baseline EndSet Subscript
    i element-of upper I"><msub><mfenced close="}" open="{" separators=""><msub><mi>x</mi>
    <mi>i</mi></msub></mfenced> <mrow><mi>i</mi><mo>∈</mo><mi>I</mi></mrow></msub></math>
    的元素组成，这些元素是索引的共现。然后我们使用以下公式：
- en: <math alttext="upper C Subscript script upper I Baseline equals script upper
    I Superscript upper T Baseline asterisk script upper I" display="block"><mrow><msub><mi>C</mi>
    <mi>ℐ</mi></msub> <mo>=</mo> <msup><mi>ℐ</mi> <mi>T</mi></msup> <mo>*</mo> <mi>ℐ</mi></mrow></math>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper C Subscript script upper I Baseline equals script upper
    I Superscript upper T Baseline asterisk script upper I" display="block"><mrow><msub><mi>C</mi>
    <mi>ℐ</mi></msub> <mo>=</mo> <msup><mi>ℐ</mi> <mi>T</mi></msup> <mo>*</mo> <mi>ℐ</mi></mrow></math>
- en: As mentioned in [“Customers Also Bought”](ch08.html#CaB), we can build a new
    variant of our MPIR by considering the rows or columns of the co-occurence matrix.
    The *conditional MPIR* is the recommender that returns the max of the elements
    in the row corresponding to <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    , given the user’s last interaction was the item <math alttext="x Subscript i"><msub><mi>x</mi>
    <mi>i</mi></msub></math> .
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在 [“顾客也买了”](ch08.html#CaB) 中提到的，我们可以通过考虑共现矩阵的行或列来构建 MPIR 的新变体。*条件 MPIR* 是返回与用户上次交互的项目
    <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> 对应行中元素最大值的推荐系统。
- en: 'In practice, we often think of the row corresponding to <math alttext="x Subscript
    i"><msub><mi>x</mi> <mi>i</mi></msub></math> as a *basis vector*, i.e., a vector
    <math alttext="q Subscript x Sub Subscript i"><msub><mi>q</mi> <msub><mi>x</mi>
    <mi>i</mi></msub></msub></math> with one nonzero element in the <math alttext="x
    Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> th position:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们通常将对应于 <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    的行视为 *基向量*，即一个在 <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    位置有一个非零元素的向量 <math alttext="q Subscript x Sub Subscript i"><msub><mi>q</mi> <msub><mi>x</mi>
    <mi>i</mi></msub></msub></math> ：
- en: <math alttext="q Subscript x Sub Subscript i Subscript comma j Baseline equals
    StartLayout Enlarged left-brace 1st Row 1st Column 1 2nd Column j equals x Subscript
    i Baseline 2nd Row 1st Column 0 2nd Column otherwise EndLayout equals Start 5
    By 1 Matrix 1st Row  0 2nd Row  vertical-ellipsis 3rd Row  1 4th Row  vertical-ellipsis
    5th Row  0 EndMatrix" display="block"><mrow><msub><mi>q</mi> <mrow><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo><mi>j</mi></mrow></msub> <mo>=</mo> <mfenced close=""
    open="{" separators=""><mtable><mtr><mtd columnalign="left"><mn>1</mn></mtd> <mtd
    columnalign="left"><mrow><mi>j</mi> <mo>=</mo> <msub><mi>x</mi> <mi>i</mi></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mn>0</mn></mtd> <mtd columnalign="left"><mtext>otherwise</mtext></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="[" separators=""><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="q Subscript x Sub Subscript i Subscript comma j Baseline equals
    StartLayout Enlarged left-brace 1st Row 1st Column 1 2nd Column j equals x Subscript
    i Baseline 2nd Row 1st Column 0 2nd Column otherwise EndLayout equals Start 5
    By 1 Matrix 1st Row  0 2nd Row  vertical-ellipsis 3rd Row  1 4th Row  vertical-ellipsis
    5th Row  0 EndMatrix" display="block"><mrow><msub><mi>q</mi> <mrow><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo><mi>j</mi></mrow></msub> <mo>=</mo> <mfenced close=""
    open="{" separators=""><mtable><mtr><mtd columnalign="left"><mn>1</mn></mtd> <mtd
    columnalign="left"><mrow><mi>j</mi> <mo>=</mo> <msub><mi>x</mi> <mi>i</mi></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mn>0</mn></mtd> <mtd columnalign="left"><mtext>otherwise</mtext></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="[" separators=""><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Then we can consider max—or even softmax—of the preceding dot products:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以考虑前述点   然后我们可以考虑前面点积的最大值——或者甚至是 softmax：
- en: <math alttext="upper C Subscript script upper I Baseline equals script upper
    I Superscript upper T Baseline dot script upper I asterisk q Subscript x Sub Subscript
    i" display="block"><mrow><msub><mi>C</mi> <mi>ℐ</mi></msub> <mo>=</mo> <msup><mi>ℐ</mi>
    <mi>T</mi></msup> <mo>·</mo> <mi>ℐ</mi> <mo>*</mo> <msub><mi>q</mi> <msub><mi>x</mi>
    <mi>i</mi></msub></msub></mrow></math>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper C Subscript script upper I Baseline equals script upper
    I Superscript upper T Baseline dot script upper I asterisk q Subscript x Sub Subscript
    i" display="block"><mrow><msub><mi>C</mi> <mi>ℐ</mi></msub> <mo>=</mo> <msup><mi>ℐ</mi>
    <mi>T</mi></msup> <mo>·</mo> <mi>ℐ</mi> <mo>*</mo> <msub><mi>q</mi> <msub><mi>x</mi>
    <mi>i</mi></msub></msub></mrow></math>
- en: This yields the vector of co-occurrence counts between <math alttext="x Subscript
    i"><msub><mi>x</mi> <mi>i</mi></msub></math> and each other item. Here we frequently
    will call <math alttext="q Subscript x Sub Subscript i"><msub><mi>q</mi> <msub><mi>x</mi>
    <mi>i</mi></msub></msub></math> a *query* to indicate that it’s the input to our
    co-occurrence recommendation model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这得出了 <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    和其他每个项目之间的共现计数向量。在这里，我们经常称 <math alttext="q Subscript x Sub Subscript i"><msub><mi>q</mi>
    <msub><mi>x</mi> <mi>i</mi></msub></msub></math> 为 *查询*，以表明它是我们共现推荐模型的输入。
- en: How Do You Store This Data?
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你如何存储这些数据？
- en: We can think about co-occurrence data in a *lot* of ways. The main reason is
    because we expect that co-occurrences for recommendation systems are incredibly
    sparse. This means that the preceding method of matrix multiplication—which is
    approximately <math alttext="upper O left-parenthesis n cubed right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <msup><mi>n</mi> <mn>3</mn></msup> <mo>)</mo></mrow></math> —is going
    to be relatively slow to compute fewer nonzero entries. Because of this and concerns
    about storing huge matrices full of zeros, computer scientists have taken seriously
    the problem of representing sparse matrices.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以*许多*方式思考共现数据。主要原因是因为我们预计用于推荐系统的共现非常稀疏。这意味着矩阵乘法的前述方法——大约是<math alttext="upper
    O left-parenthesis n cubed right-parenthesis"><mrow><mi>O</mi> <mo>(</mo> <msup><mi>n</mi>
    <mn>3</mn></msup> <mo>)</mo></mrow></math>——计算非零条目会相对较慢。由于这个原因以及对存储大量充满零的矩阵的担忧，计算机科学家们认真对待了表示稀疏矩阵的问题。
- en: '[Max Grossman](https://oreil.ly/c3Gif) claims there are 101 ways, but in practice
    there are only a few. JAX supports [BCOO](https://oreil.ly/AB5vk), or *batched
    coordinate format*, which is essentially a list of coordinates for nonzero elements,
    and then what those elements are.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[马克斯·格罗斯曼](https://oreil.ly/c3Gif)声称有101种方法，但实际上只有几种。JAX支持[BCOO](https://oreil.ly/AB5vk)，即*批处理坐标格式*，其本质上是非零元素的坐标列表，以及这些元素的内容。'
- en: 'In our binary case of interactions, those are 1s, and for the co-occurrence
    matrix, those are the counts. The structure of these matrices can be written as
    follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的二进制交互案例中，那些是1，而在共现矩阵中，那些是计数。这些矩阵的结构可以写成如下形式：
- en: '[PRE0]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Pointwise Mutual Information via Co-occurrences
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过共现计算的点对点互信息
- en: 'An early recommendation system for articles used *pointwise mutual information*,
    or PMI, which is closely related to co-occurrences. In the context of NLP, PMI
    attempts to express how much more frequent co-occurrence is than random chance.
    Given what we’ve seen before, you can think of this as a normalized co-occurrences
    model. Computational linguists frequently use PMI as an estimator for word similarity
    or word meaning following from the distributional hypothesis:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的早期推荐系统使用*点对点互信息*或PMI，这与共现密切相关。在自然语言处理的上下文中，PMI试图表达共现比随机事件更频繁的程度。根据我们之前看到的内容，你可以把这看作是一种归一化的共现模型。计算语言学家经常使用PMI作为词汇相似性或词义的估计器，遵循分布假设：
- en: You shall know a word by the company it keeps.
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你可以通过它交往的人了解一个词。
- en: ''
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: John R. Firth, British linguist
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 约翰·R·弗斯，英国语言学家
- en: 'In the context of recommendation ranking, items with very high PMI are said
    to have a highly meaningful co-occurrence. This can thus be used as an estimator
    for *complementary* items: given you’ve interacted with one of them, you should
    interact with the other.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐排序的上下文中，具有非常高PMI的物品被认为具有高度有意义的共现。因此，可以将其用作*互补*物品的估计器：一旦您与其中一个互动，您应该与另一个互动。
- en: 'PMI is computed for two items, <math alttext="x Subscript i Baseline comma
    x Subscript j Baseline"><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>x</mi>
    <mi>j</mi></msub></mrow></math> , via the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: PMI通过以下方式计算两个项目的值，<math alttext="x Subscript i Baseline comma x Subscript j
    Baseline"><mrow><msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>x</mi>
    <mi>j</mi></msub></mrow></math> 。
- en: <math alttext="StartFraction p left-parenthesis x Subscript i Baseline comma
    x Subscript j Baseline right-parenthesis Over p left-parenthesis x Subscript i
    Baseline right-parenthesis asterisk p left-parenthesis x Subscript j Baseline
    right-parenthesis EndFraction equals StartFraction left-parenthesis upper C Subscript
    script upper I Baseline right-parenthesis Subscript x Sub Subscript i Subscript
    comma x Sub Subscript j Subscript Baseline asterisk number-sign left-parenthesis
    normal t normal o normal t normal a normal l normal i normal n normal t normal
    e normal r normal a normal c normal t normal i normal o normal n normal s right-parenthesis
    Over number-sign left-parenthesis x Subscript i Baseline right-parenthesis asterisk
    number-sign left-parenthesis x Subscript j Baseline right-parenthesis EndFraction"
    display="block"><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub> <mo>)</mo></mrow>
    <mrow><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow><mo>*</mo><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow></mrow></mfrac></mstyle> <mo>=</mo> <mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mrow><msub><mfenced close=")" open="("
    separators=""><msub><mi>C</mi> <mi>ℐ</mi></msub></mfenced> <mrow><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub></mrow></msub> <mo>*</mo><mo>#</mo><mrow><mo>(</mo>
    <mi>total</mi> <mi>interactions</mi> <mo>)</mo></mrow></mrow> <mrow><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow><mo>*</mo><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow></mrow></mfrac></mstyle></mrow></math>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction p left-parenthesis x Subscript i Baseline comma
    x Subscript j Baseline right-parenthesis Over p left-parenthesis x Subscript i
    Baseline right-parenthesis asterisk p left-parenthesis x Subscript j Baseline
    right-parenthesis EndFraction equals StartFraction left-parenthesis upper C Subscript
    script upper I Baseline right-parenthesis Subscript x Sub Subscript i Subscript
    comma x Sub Subscript j Subscript Baseline asterisk number-sign left-parenthesis
    normal t normal o normal t normal a normal l normal i normal n normal t normal
    e normal r normal a normal c normal t normal i normal o normal n normal s right-parenthesis
    Over number-sign left-parenthesis x Subscript i Baseline right-parenthesis asterisk
    number-sign left-parenthesis x Subscript j Baseline right-parenthesis EndFraction"
    display="block"><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub> <mo>)</mo></mrow>
    <mrow><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow><mo>*</mo><mi>p</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow></mrow></mfrac></mstyle> <mo>=</mo> <mstyle
    displaystyle="true" scriptlevel="0"><mfrac><mrow><msub><mfenced close=")" open="("
    separators=""><msub><mi>C</mi> <mi>ℐ</mi></msub></mfenced> <mrow><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi> <mi>j</mi></msub></mrow></msub> <mo>*</mo><mo>#</mo><mrow><mo>(</mo>
    <mi>total</mi> <mi>interactions</mi> <mo>)</mo></mrow></mrow> <mrow><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow><mo>*</mo><mo>#</mo><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow></mrow></mfrac></mstyle></mrow></math>
- en: The PMI calculation allows us to modify all our work on co-occurrence to a more
    normalized computation, and thus is a bit more meaningful. This process is related
    to the GloVE model we learned in [“GloVE Model Definition”](ch08.html#Glove).
    The negative PMI values allow us to understand when two things are not often witnessed
    together.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: PMI计算允许我们将所有关于共现的工作修改为更规范的计算，因此更有意义。这个过程与我们在[“GloVE模型定义”](ch08.html#Glove)中学到的GloVE模型相关。负PMI值使我们能够理解两个事物不经常一起见到的时候。
- en: These PMI calculations can be used to recommend *another item in a cart* when
    an item has been added and you find those with very high PMI. It can be used as
    a retrieval method by looking at the set of items a user has already interacted
    with and finding items that have high PMI with several of them.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这些PMI计算可以用于推荐*购物车中的另一件物品*，当添加了一件物品并发现与之具有非常高PMI的物品时。可以通过查看用户已经互动过的物品集合，并找到与其中多个物品具有高PMI的物品，作为检索方法使用。
- en: Let’s look at how to turn co-occurrences into other similarity measures.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将共现转化为其他相似性度量。
- en: Is PMI a Distance Measurement?
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PMI是一种距离测量吗？
- en: A good question to consider at this point is “Is PMI between two objects a measurement
    of distance? Can I define similarity directly as the PMI between two items, and
    thus yield a convenient geometry in which to consider distances?” The answer is
    no. Recall that one of the axioms of a distance function is the triangle inequality;
    a useful exercise is to consider why the triangle inequality would not be true
    for PMI.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此时需要考虑的一个重要问题是：“两个对象之间的PMI是否是一种距离度量？我是否可以直接将相似性定义为两个项目之间的PMI，从而产生一个便于考虑距离的几何结构？”
    答案是否定的。回想一下距离函数的公理之一是三角不等式；一个有用的练习是思考为什么PMI不满足三角不等式。
- en: But all is not lost. In the next section, we’ll show you how to formulate some
    important similarity measurements from co-occurrence structures. Further, in the
    next chapter, we’ll discuss Wasserstein distance, which allows you to turn the
    co-occurrence counts into a distance metric directly. The key difference will
    be considering the co-occurrence counts of all other items simultaneously as a
    distribution.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 但一切并未失去。在下一节中，我们将向您展示如何从共现结构中制定一些重要的相似度测量。此外，在下一章中，我们将讨论Wasserstein距离，它允许您将共现计数直接转换为距离度量。关键的区别在于同时考虑所有其他项目的共现计数作为一个分布。
- en: Similarity from Co-occurrence
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自共现的相似性
- en: Earlier, we discussed similarity measures and how they come from the Pearson
    correlation. The Pearson correlation is a special case of similarity when we have
    explicit ratings, so let’s instead look at when we don’t.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了相似性度量及其如何来自皮尔逊相关性。当我们有明确评级时，皮尔逊相关性是相似性的一个特例，所以让我们看看在没有明确评级时的情况。
- en: 'Consider incidence sets associated to users, <math alttext="StartSet y Subscript
    u Baseline EndSet Subscript u element-of upper U"><msub><mfenced close="}" open="{"
    separators=""><msub><mi>y</mi> <mi>u</mi></msub></mfenced> <mrow><mi>u</mi><mo>∈</mo><mi>U</mi></mrow></msub></math>
    , as we define three distance metrics:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑与用户关联的发生集合，<math alttext="StartSet y Subscript u Baseline EndSet Subscript
    u element-of upper U"><msub><mfenced close="}" open="{" separators=""><msub><mi>y</mi>
    <mi>u</mi></msub></mfenced> <mrow><mi>u</mi><mo>∈</mo><mi>U</mi></mrow></msub></math>
    ，我们定义三个距离度量：
- en: Jaccard similarity, <math alttext="upper J a c left-parenthesis minus right-parenthesis"><mrow><mi>J</mi>
    <mi>a</mi> <mi>c</mi> <mo>(</mo> <mo>-</mo> <mo>)</mo></mrow></math>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard相似度，<math alttext="upper J a c left-parenthesis minus right-parenthesis"><mrow><mi>J</mi>
    <mi>a</mi> <mi>c</mi> <mo>(</mo> <mo>-</mo> <mo>)</mo></mrow></math>
- en: The ratio of shared items by two users to the total items those users have interacted
    with
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 两个用户共享项目的比例与这些用户互动的总项目数之比。
- en: Sørensen-Dice similarity, <math alttext="upper D upper S upper C left-parenthesis
    minus right-parenthesis"><mrow><mi>D</mi> <mi>S</mi> <mi>C</mi> <mo>(</mo> <mo>-</mo>
    <mo>)</mo></mrow></math>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Sørensen-Dice 相似性，<math alttext="upper D upper S upper C left-parenthesis minus
    right-parenthesis"><mrow><mi>D</mi> <mi>S</mi> <mi>C</mi> <mo>(</mo> <mo>-</mo>
    <mo>)</mo></mrow></math>
- en: Twice the ratio of shared items by two users to the sum of total items each
    user has interacted with
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 两个用户共享项目比例的两倍，除以每个用户互动的总项目数之和。
- en: Cosine similarity, <math alttext="upper C o s i m left-parenthesis minus right-parenthesis"><mrow><mi>C</mi>
    <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>m</mi> <mo>(</mo> <mo>-</mo> <mo>)</mo></mrow></math>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度，<math alttext="upper C o s i m left-parenthesis minus right-parenthesis"><mrow><mi>C</mi>
    <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>m</mi> <mo>(</mo> <mo>-</mo> <mo>)</mo></mrow></math>
- en: The ratio of shared items by two users to the product of total items each user
    has interacted with
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 两个用户共享项目的比例与每个用户互动的总项目数的乘积之比。
- en: 'These are all very related metrics with slightly different strengths. Here
    are some points to consider:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是非常相关的度量指标，具有略有不同的优势。以下是一些需要考虑的要点：
- en: Jaccard similarity is a real distance metric that has some nice properties for
    geometry; neither of the other two is.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaccard相似度是一个真实的距离度量，具有几何上的一些良好特性；其他两者都不是。
- en: All three are on the interval <math alttext="left-parenthesis 0 comma 1 right-parenthesis"><mfenced
    close=")" open="(" separators=""><mn>0</mn> <mo>,</mo> <mn>1</mn></mfenced></math>
    , but you’ll often see cosine extended to <math alttext="left-parenthesis negative
    1 comma 1 right-parenthesis"><mfenced close=")" open="(" separators=""><mo>-</mo>
    <mn>1</mn> <mo>,</mo> <mn>1</mn></mfenced></math> by including negative ratings.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有三者都在区间 <math alttext="left-parenthesis 0 comma 1 right-parenthesis"><mfenced
    close=")" open="(" separators=""><mn>0</mn> <mo>,</mo> <mn>1</mn></mfenced></math>
    内，但您经常会看到余弦相似度通过包括负评级来扩展到 <math alttext="left-parenthesis negative 1 comma 1 right-parenthesis"><mfenced
    close=")" open="(" separators=""><mo>-</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn></mfenced></math>。
- en: Cosine can accommodate “thumbs-up/thumbs-down” by merely extending all interactions
    to have a polarity of <math alttext="plus-or-minus 1"><mrow><mo>±</mo> <mn>1</mn></mrow></math>
    .
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cosine 可以通过将所有交互的极性扩展为 <math alttext="plus-or-minus 1"><mrow><mo>±</mo> <mn>1</mn></mrow></math>
    ，来适应“赞/踩”的情况。
- en: Cosine can accommodate “multiple interactions” if you allow the vectors to be
    nonbinary and count the number of times a user interacts with an item.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cosine 可以适应“多次交互”，如果允许向量是非二进制的，并计算用户与物品交互的次数。
- en: Jaccard and Dice are related by the simple equation <math alttext="upper S equals
    2 upper J slash left-parenthesis 1 plus upper J right-parenthesis"><mrow><mi>S</mi>
    <mo>=</mo> <mn>2</mn> <mi>J</mi> <mo>/</mo> <mo>(</mo> <mn>1</mn> <mo>+</mo> <mi>J</mi>
    <mo>)</mo></mrow></math> , and you can easily compute one from the other.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaccard 和 Dice 之间有简单的关系式 <math alttext="upper S equals 2 upper J slash left-parenthesis
    1 plus upper J right-parenthesis"><mrow><mi>S</mi> <mo>=</mo> <mn>2</mn> <mi>J</mi>
    <mo>/</mo> <mo>(</mo> <mn>1</mn> <mo>+</mo> <mi>J</mi> <mo>)</mo></mrow></math>
    ，你可以轻松地从一个计算出另一个。
- en: Notice that we’ve defined all these similarity measures between users. We’ll
    show in the next section how to extend these definitions to items and how to turn
    these into recommendations.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经定义了所有这些用户之间的相似度量。我们将在下一节中展示如何将这些定义扩展到物品，并将其转化为推荐。
- en: Similarity-Based Recommendations
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于相似度的推荐
- en: In each of the preceding distance metrics, we’ve defined a similarity measure,
    but we haven’t yet discussed how similarity measures turn into recommendations.
    As we discussed in [“Nearest Neighbors”](ch03.html#nearest_neighbors), we utilize
    similarity measures in our retrieval step; we wish to find a space in that items
    that are *close* to one another are good recommendations. In the context of ranking,
    our similarity measure can be used directly to order the recommendations in terms
    of how likely the recommendation is relevant. In the next chapter, we’ll talk
    more about metrics of relevance.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述每个距离度量中，我们定义了一个相似度量，但我们还没有讨论相似度量如何转化为推荐。正如我们在 [“最近邻居”](ch03.html#nearest_neighbors)
    中讨论的那样，我们在检索步骤中利用相似度量；我们希望找到一个空间，在该空间中彼此“接近”的物品是良好的推荐。在排名的背景下，我们的相似度量可以直接用来按照推荐的相关性顺序排序。在下一章中，我们将更多地讨论相关性度量。
- en: In the preceding section, we looked at three similarity scores, but we need
    to expand our notion of the relevant sets for these measures. Let’s consider Jaccard
    similarity as a prototype.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们看了三种相似度分数，但我们需要扩展这些度量的相关集合的概念。让我们以 Jaccard 相似度为原型考虑一下。
- en: 'Given a user <math alttext="y Subscript u"><msub><mi>y</mi> <mi>u</mi></msub></math>
    and an unseen item <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    , let’s ask, “What is the Jaccard similarity between this user and item?” Let’s
    remember that Jaccard similarity is the similarity between two sets, and in the
    definition those sets were both *incidence sets of users’ interactions*. Here
    are three ways to use this approach for recommendations:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个用户 <math alttext="y Subscript u"><msub><mi>y</mi> <mi>u</mi></msub></math>
    和一个未见的物品 <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    ，让我们问：“这个用户和物品之间的 Jaccard 相似度是多少？” 让我们记住，Jaccard 相似度是两个集合之间的相似度，在定义中，这些集合都是 *用户交互的发生集*。以下是使用这种方法进行推荐的三种方式：
- en: User-user
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 用户-用户
- en: Using our preceding definition, find the <math alttext="k"><mi>k</mi></math>
    users with maximum Jaccard similarity. Compute the percentage of these users who
    have interacted with <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    . You may also wish to normalize this by popularity of the item <math alttext="x
    Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> .
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们前面的定义，找到具有最大 Jaccard 相似度的 <math alttext="k"><mi>k</mi></math> 个用户。计算这些用户中与
    <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> 互动的百分比。您可能还希望按照物品
    <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> 的流行度对此进行归一化。
- en: Item-item
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 物品-物品
- en: Compute the set of users that each item has interacted with, and compute the
    <math alttext="k"><mi>k</mi></math> items most similar to <math alttext="x Subscript
    i"><msub><mi>x</mi> <mi>i</mi></msub></math> with respect to Jaccard similarity
    of these item-user incidence sets. Compute the percentage of these items that
    are in <math alttext="y Subscript u"><msub><mi>y</mi> <mi>u</mi></msub></math>
    ’s set of interactions. You may also wish to normalize this by total interactions
    of <math alttext="y Subscript u"><msub><mi>y</mi> <mi>u</mi></msub></math> or
    the popularity of the similar items.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每个项目与之交互的用户集合，并根据这些项目-用户发生集的Jaccard相似性计算与<math alttext="x Subscript i"><msub><mi>x</mi>
    <mi>i</mi></msub></math>最相似的<math alttext="k"><mi>k</mi></math>个项目。计算这些项目中属于<math
    alttext="y Subscript u"><msub><mi>y</mi> <mi>u</mi></msub></math>的交互集的百分比。您可能还希望通过<math
    alttext="y Subscript u"><msub><mi>y</mi> <mi>u</mi></msub></math>的总交互或相似项目的流行度对其进行归一化。
- en: User-item
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 用户-项目
- en: Compute the user <math alttext="y Subscript u"><msub><mi>y</mi> <mi>u</mi></msub></math>
    ’s set of items they’ve interacted with, and the set of items co-occurring with
    <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> in any
    user’s incidence set of interaction. Compute the Jaccard similarity between these
    two sets.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 计算用户<math alttext="y Subscript u"><msub><mi>y</mi> <mi>u</mi></msub></math>的已互动项目集合，以及任何用户互动集的<math
    alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>共现的项目集。计算这两个集合之间的Jaccard相似性。
- en: Frequently in designing ranking systems, we specify the *query*, which refers
    to which nearest neighbors you’re looking for. We then specify how you use those
    neighbors to yield a recommendation. The items that may become the recommendation
    are the candidates, but as you saw in the preceding example, the neighbors may
    not be the candidates themselves. An additional complication is that you usually
    need to compute many candidate scores simultaneously, which requires optimized
    computations that we’ll see in [Chapter 16](ch16.html#acceleration_structures).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计排名系统时，我们经常指定*查询*，这指的是您要查找的最近邻居。然后我们指定如何使用这些邻居来生成推荐。可能成为推荐的项目是候选项，但正如您在前面的例子中看到的，邻居可能不是候选项本身。另一个复杂之处在于，您通常需要同时计算许多候选项分数，这需要我们将在[第16章](ch16.html#acceleration_structures)中看到的优化计算。
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we’ve begun to dig deeper into notions of similarity—building
    on our intuition from retrieval that users’ preferences might be captured by the
    interactions they’ve already demonstrated.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经开始深入探讨相似性的概念——建立在我们从检索中的直觉上，即用户的偏好可能通过他们已经展示的互动来捕捉。
- en: We started out with simple models based on features about users and built linear
    models relating them to our target outcomes. We then combined those simple models
    with other aspects of feature modeling and hybrid systems.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从基于用户特征的简单模型开始，并建立线性模型将其与我们的目标结果相关联。然后，我们将这些简单模型与特征建模的其他方面和混合系统结合起来。
- en: Next, we moved into discussing counting—in particular, counting the co-occurrence
    of items, users, or baskets. By looking at frequent co-occurrence, we can build
    models that capture “If you liked *a*, you may like *b*.” These models are simple
    to understand, but we can use these basic correlation structures to build similarity
    measures, and thus latent spaces where ANN-based retrieval can yield good candidates
    for recommendations.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开始讨论计数，特别是计算项目、用户或购物篮的共现次数。通过查看频繁共现，我们可以构建捕捉“如果你喜欢*a*，你可能会喜欢*b*”的模型。这些模型简单易懂，但我们可以使用这些基本的相关结构构建相似度度量，从而建立ANN检索能够产生良好推荐候选项的潜在空间。
- en: 'One point that you may have noticed about the featurization of all the items
    and the building of our co-occurrence matrices is that the number of features
    is astronomically large—one dimension for each item! This is the area of investigation
    we’ll tackle in the next chapter: how to reduce the dimensionality of your *latent
    space*.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到有关所有项目的特征化和构建我们的共现矩阵的一点是特征的数量 astronomically large ——每个项目一个维度！这是我们将在下一章中探讨的研究领域：如何减少您的*潜在空间*的维度。
