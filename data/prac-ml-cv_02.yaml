- en: Chapter 2\. ML Models for Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。视觉的 ML 模型
- en: In this chapter, you will learn how to represent images and train basic machine
    learning models to classify images. You will discover that the performance of
    linear and fully connected neural networks is poor on images. However, along the
    way, you will learn how to use the Keras API to implement ML primitives and train
    ML models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何表示图像并训练基本的机器学习模型来分类图像。您将发现线性和全连接神经网络在图像上的表现很差。然而，在此过程中，您将学习如何使用Keras
    API来实现ML基元并训练ML模型。
- en: Tip
  id: totrans-2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The code for this chapter is in the *02_ml_models* folder of the book’s [GitHub
    repository](https://github.com/GoogleCloudPlatform/practical-ml-vision-book).
    We will provide file names for code samples and notebooks where applicable.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于该书的 [GitHub 代码库](https://github.com/GoogleCloudPlatform/practical-ml-vision-book)的
    *02_ml_models* 文件夹中。在适用的情况下，我们将提供代码示例和笔记本文件名。
- en: A Dataset for Machine Perception
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于机器感知的数据集
- en: For the purposes of this book, it will be helpful if we take a single practical
    problem and build a variety of machine learning models to solve it. Assume that
    we have collected and labeled a dataset of nearly four thousand photographs of
    flowers. There are five types of flowers in the *5-flowers* dataset (see [Figure 2-1](#the_photographs_in_the_five-flowers_data)),
    and each image in the dataset has already been labeled with the type of flower
    it depicts.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本书的目的，如果我们选择一个实际问题，并构建多种机器学习模型来解决它将会很有帮助。假设我们已经收集并标记了近四千张花卉的照片数据集。*5-flowers*
    数据集中有五种类型的花（见 [图 2-1](#the_photographs_in_the_five-flowers_data)），数据集中的每张图像都已经标记了它所描述的花的类型。
- en: '![](Images/pmlc_0201.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0201.png)'
- en: 'Figure 2-1\. The photographs in the 5-flowers dataset are of five types of
    flowers: daisies, dandelions, roses, sunflowers, and tulips.'
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1。5-Flowers 数据集中的照片包括五种花的照片：雏菊、蒲公英、玫瑰、向日葵和郁金香。
- en: Suppose we want to create a computer program that will, when provided an image,
    tell us what type of flower is in the image. We are asking the machine learning
    model to learn to perceive what’s in the image, so you might see this type of
    task called *machine perception*. Specifically, the type of perception is analogous
    to human sight, so the problem is termed *computer vision*, and in this case we
    will solve it through image classification.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要创建一个计算机程序，当提供一张图像时，能告诉我们图像中是什么类型的花。我们要求机器学习模型学会感知图像中的内容，因此您可能会看到这种类型的任务称为*机器感知*。具体而言，这种感知类型类似于人类视觉，所以这个问题被称为*计算机视觉*，在这种情况下，我们将通过图像分类来解决它。
- en: 5-Flowers Dataset
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5-Flowers 数据集
- en: The 5-flowers dataset was created by Google and placed in the public domain
    with a Creative Commons license. It is published as a [TensorFlow dataset](https://oreil.ly/tqwFi)
    and available in a public Google Cloud Storage bucket (`gs://cloud-ml-data/`)
    in the form of JPEG files. This makes the dataset both realistic (it consists
    of JPEG photographs of the sort collected by off-the-shelf cameras) and readily
    accessible. Therefore, we will use it as an ongoing example in this book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 5-Flowers 数据集由 Google 创建，并在公共领域中以创作共用许可证发布。它作为[TensorFlow 数据集](https://oreil.ly/tqwFi)发布，并以
    JPEG 文件的形式存储在公共 Google Cloud Storage 存储桶 (`gs://cloud-ml-data/`) 中。这使得数据集既逼真（它由即插即用相机收集的
    JPEG 照片组成），又易于访问。因此，在本书中，我们将以此数据集作为持续的示例。
- en: In [Figure 2-2](#these_five_photographs_of_tulips_vary_wi), you can see several
    of the tulip photographs. Note that they range from close-up photographs to photographs
    of fields of tulips. All of these are photographs that a human would have no problem
    labeling as tulips, but it’s a difficult problem for us to capture using simple
    rules—if we were to say a tulip is an elongated flower, for example, only the
    first and fourth images would qualify.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 2-2](#these_five_photographs_of_tulips_vary_wi) 中，您可以看到几张郁金香的照片。请注意，这些照片从近距离拍摄的照片到郁金香花田的照片不等。所有这些照片对于人类来说都很容易标记为郁金香，但对我们来说，这是一个使用简单规则难以捕捉的问题——例如，如果我们说郁金香是一个细长的花朵，那么只有第一张和第四张照片符合条件。
- en: '![](Images/pmlc_0202.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0202.png)'
- en: Figure 2-2\. These five photographs of tulips vary widely in terms of zoom,
    color of the tulips, and what’s in the frame.
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2。这五张郁金香的照片在变焦、郁金香的颜色以及画面内容方面有很大的差异。
- en: Reading Image Data
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取图像数据
- en: 'To train image models, we need to read image data into our programs. There
    are four steps to reading an image in a standard format like JPEG or PNG and getting
    it ready to train machine learning models with it (the complete code is available
    in [*02a_machine_perception.ipynb* in the GitHub repository for the book](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/02_ml_models/02a_machine_perception.ipynb)):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练图像模型，我们需要将图像数据读入我们的程序中。在标准格式（如JPEG或PNG）中读取图像并准备好训练机器学习模型有四个步骤（完整代码在[*02a_machine_perception.ipynb*的GitHub存储库中可用](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/02_ml_models/02a_machine_perception.ipynb)）：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We first read the image data from persistent storage into memory as a sequence
    of bytes:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从持久存储中读取图像数据到内存中，作为一系列字节的序列：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The variable `img` here is a tensor (see [“What’s a Tensor?”](#whatapostrophes_a_tensorquestion_mark))
    that contains an array of bytes. We parse these bytes to convert them into the
    pixel data—this is also called *decoding* the data because image formats like
    JPEG require you to decode the pixel values from lookup tables:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的变量`img`是一个张量（见[“什么是张量？”](#whatapostrophes_a_tensorquestion_mark)），其中包含一个字节数组。我们解析这些字节以将它们转换为像素数据——这也称为*解码*数据，因为像JPEG这样的图像格式需要你从查找表中解码像素值：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we specify that we want only the three color channels (red, green, and
    blue) from the JPEG image and not the opacity, which is the fourth channel. The
    channels you have available depend on the file itself. Grayscale images may have
    only one channel.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定我们只想从JPEG图像中获取三个颜色通道（红色、绿色和蓝色），而不是不透明度，这是第四个通道。你可以根据文件本身拥有的通道来选择使用哪些通道。灰度图像可能只有一个通道。
- en: 'The pixels will consist of RGB values that are of type `uint8` and are in the
    range [0,255]. So, in the third step, we convert them to floats and scale the
    values to lie in the range [0,1]. This is because machine learning optimizers
    are tuned to work well with small numbers:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 像素将由类型为`uint8`的RGB值组成，范围在[0,255]之间。因此，在第三步中，我们将它们转换为浮点数并将值缩放到[0,1]的范围内。这是因为机器学习优化器通常表现良好与小数字一起工作：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we resize the image to the desired size. Machine learning models are
    built to work with inputs of known sizes. Since images in the real world are likely
    to come in arbitrary sizes, you might have to shrink, crop, or expand them to
    fit your desired size. For example, to resize the image to be 256 pixels wide
    and 128 pixels tall, we’d specify:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将图像调整为所需大小。机器学习模型是建立在已知输入大小的基础上工作的。因此，由于现实世界中的图像可能是任意大小的，你可能需要缩小、裁剪或扩展它们以适应所需的大小。例如，要将图像调整为256像素宽和128像素高，我们可以指定：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In [Chapter 6](ch06.xhtml#preprocessing), we’ll see that this method does not
    preserve the aspect ratio and we’ll look at other options to resize images.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.xhtml#preprocessing)中，我们会看到这种方法不保留纵横比，并探讨其他调整图像大小的选项。
- en: These steps are not set in stone. If your input data consists of remotely sensed
    images from a satellite that are provided in a band interleaved format or brain
    scan images provided in Digital Imaging and Communications in Medicine (DICOM)
    format, you obviously wouldn’t decode those using `decode_jpeg()`. Similarly,
    you may not always resize the data. In some instances, you might choose to crop
    the data to the desired size or pad it with zeros. In other cases, you might resize,
    keeping the aspect ratio constant, and then pad the remaining pixels. These preprocessing
    operations are discussed in [Chapter 6](ch06.xhtml#preprocessing).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤并非一成不变。如果你的输入数据包括来自卫星的遥感图像，这些图像以波段交织格式提供，或者提供的是数字影像和医学通信（DICOM）格式的脑部扫描图像，显然不能使用`decode_jpeg()`来解码它们。同样地，你可能不总是调整数据大小。在某些情况下，你可能选择将数据裁剪到所需大小或用零填充。在其他情况下，你可能会保持纵横比进行调整大小，然后填充剩余的像素。这些预处理操作在[第6章](ch06.xhtml#preprocessing)中有详细讨论。
- en: Visualizing Image Data
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化图像数据
- en: Always visualize a few of the images to ensure that you are reading the data
    correctly—a common mistake is to read the data in such a way that the images are
    rotated or mirrored. Visualizing the images is also useful to get a sense of how
    challenging a machine perception problem is.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 始终要想象几幅图像，以确保你正确阅读数据——一个常见的错误是以一种使图像旋转或镜像的方式读取数据。想象这些图像也有助于感受机器感知问题的挑战性。
- en: We can use Matplotlib’s `imshow()` function to visualize an image, but in order
    to do so we must first convert the image, which is a TensorFlow tensor, into a
    `numpy` array using the `numpy()` function.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Matplotlib 的`imshow()`函数来可视化图像，但是为了做到这一点，我们必须先将图像（一个 TensorFlow 张量）转换为`numpy`数组，使用`numpy()`函数。
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Trying it out on one of our daisy images, we get what’s shown in [Figure 2-3](#make_sure_to_visualize_the_data_to_ensur).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的一张雏菊图像上试验，我们得到了[图 2-3](#make_sure_to_visualize_the_data_to_ensur)中显示的结果。
- en: '![](Images/pmlc_0203.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0203.png)'
- en: Figure 2-3\. Make sure to visualize the data to ensure that you are reading
    it correctly.
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 确保可视化数据以确保正确读取。
- en: 'Notice from [Figure 2-3](#make_sure_to_visualize_the_data_to_ensur) that the
    filename contains the type of flower (daisy). This means we can use wildcard matching
    with TensorFlow’s `glob()` function to get, say, all the tulip images:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意来自[图 2-3](#make_sure_to_visualize_the_data_to_ensur)，文件名包含花的类型（雏菊）。这意味着我们可以使用
    TensorFlow 的`glob()`函数进行通配符匹配，例如，获取所有郁金香图像：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The result of running this code and visualizing a panel of five tulip photographs
    was shown in [Figure 2-2](#these_five_photographs_of_tulips_vary_wi).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码并可视化五张郁金香照片面板的结果显示在[图 2-2](#these_five_photographs_of_tulips_vary_wi)中。
- en: Reading the Dataset File
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取数据集文件
- en: 'We now know how to read an image. In order to train a machine model, though,
    we need to read many images. We also have to obtain the labels for each of the
    images. We could obtain a list of all the images by carrying out a wildcard match
    using `glob()`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何读取图像了。但是，为了训练机器模型，我们需要读取许多图像。我们还必须获取每个图像的标签。我们可以使用`glob()`进行通配符匹配来获取所有图像的列表：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, knowing that the images in our dataset have a naming convention, we could
    take a filename and extract the label using string operations. For example, we
    can remove the prefix using:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，知道我们数据集中的图像有一个命名约定，我们可以使用字符串操作获取文件名并提取标签。例如，我们可以使用以下操作去除前缀：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'and get the category name using:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 并获取类别名称使用：
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As usual, please refer to the GitHub repository for this book for the full code.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，请参阅此书的 GitHub 代码库获取完整代码。
- en: 'However, for reasons of generalization and reproducibility (explained further
    in [Chapter 5](ch05.xhtml#creating_vision_datasets)), it is better to set aside
    in advance the images that we will retain for evaluation. That has already been
    done in the 5-flowers dataset, and the images to use for training and evaluation
    are listed in two files in the same Cloud Storage bucket as the images:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，出于泛化和可重复性的原因（在[第 5 章](ch05.xhtml#creating_vision_datasets)中进一步解释），最好提前设置保留用于评估的图像。在5-花卉数据集中已经完成了这一点，用于训练和评估的图像在与图像相同的
    Cloud Storage 存储桶中的两个文件中列出：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: These are comma-separated values (CSV) files where each line contains a filename
    followed by the label.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是逗号分隔值（CSV）文件，每行包含文件名及其标签。
- en: 'One way to read a CSV file is to read in text lines using `TextLineDataset`,
    passing in a function to handle each line as it is read through the `map()` function:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 读取 CSV 文件的一种方法是使用`TextLineDataset`读取文本行，并通过`map()`函数传递处理每一行的函数：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We are using the `tf.data` API, which makes it possible to handle large amounts
    of data (even if it doesn’t all fit into memory) by reading only a handful of
    data elements at a time, and performing transformations as we are reading the
    data. It does this by using an abstraction called `tf.data.Dataset` to represent
    a sequence of elements. In our pipeline, each element is a training example that
    contains two tensors. The first tensor is the image and the second is the label.
    Many types of `Dataset`s correspond to many different file formats. We’re using
    `TextLineDataset`, which reads text files and assumes that each line is a different
    element.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用`tf.data`API，通过仅读取少量数据元素并在读取数据时执行转换，使其能够处理大量数据（即使不全部装入内存）。它通过使用名为`tf.data.Dataset`的抽象来表示一系列元素。在我们的流水线中，每个元素是一个包含两个张量的训练示例。第一个张量是图像，第二个是标签。许多类型的`Dataset`对应于许多不同的文件格式。我们正在使用`TextLineDataset`，它读取文本文件，并假设每行是一个不同的元素。
- en: '`parse_csvline()` is a function that we supply in order to parse the line,
    extract the filename of the image, read the image, and return the image and its
    label:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`parse_csvline()`是我们提供的函数，用于解析行，提取图像的文件名，读取图像并返回图像及其标签：'
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `record_defaults` that are passed into the `parse_csvline()` function specify
    what TensorFlow needs to replace in order to handle a line where one or more values
    are missing.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 `parse_csvline()` 函数的 `record_defaults` 指定了 TensorFlow 需要替换以处理遗漏一个或多个值的行。
- en: 'To verify that this code works, we can print out the average pixel value for
    each channel of the first three images in the training dataset:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这段代码是否工作，我们可以打印出训练数据集中前三幅图像每个通道的平均像素值：
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this code snippet, the `take()` method truncates the dataset to three items.
    Notice that because `decode_csv()` returns a tuple `(img, label)`, that’s what
    we obtain when we iterate through the dataset. Printing out the entire image is
    a terrible idea, so we are printing out the average pixel value in the image using
    `tf.reduce_mean()`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码片段中，`take()` 方法截断数据集到三个项。注意因为 `decode_csv()` 返回一个元组 `(img, label)`，所以当我们迭代数据集时得到的就是这些值。打印整个图像是一个糟糕的主意，因此我们使用
    `tf.reduce_mean()` 打印图像中像素值的平均值。
- en: 'The first line of the result is (with line breaks added for readability):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的第一行是（为了易读性添加换行符）：
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that the label is a string tensor and the average is a 1D tensor of length
    3\. Why did we get a 1D tensor? That’s because we passed in an `axis` parameter
    to `reduce_mean()`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意标签是一个字符串张量，平均值是一个长度为3的1D张量。为什么我们得到一个1D张量？因为我们向 `reduce_mean()` 传递了一个 `axis`
    参数：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Had we not supplied an axis, then TensorFlow would have computed the mean along
    all the dimensions and returned a scalar value. Recall that the shape of the image
    is `[IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS]`. Therefore, by providing `axis=[0,
    1]`, we are asking TensorFlow to compute the average of all columns (`axis=0`)
    and all rows (`axis=1`), but not to average the RGB values (see [Figure 2-4](#we_compute_the_reduce_meanleft_parenthes)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有提供轴，那么 TensorFlow 会沿着所有维度计算平均值并返回一个标量值。请记住图像的形状是 `[IMG_HEIGHT, IMG_WIDTH,
    NUM_CHANNELS]`。因此，通过提供 `axis=[0, 1]`，我们要求 TensorFlow 计算所有列的平均值（`axis=0`）和所有行的平均值（`axis=1`），但不要计算RGB值的平均值（见[图 2-4](#we_compute_the_reduce_meanleft_parenthes)）。
- en: Tip
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Printing out statistics of the image like this is helpful for another reason.
    If your input data is corrupt and there is unrepresentable floating-point data
    (technically called [`NaN`](https://oreil.ly/E0xc2)`)` in your images, the mean
    will itself be `NaN`. This is a handy way to ensure that you haven’t made a mistake
    when reading data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 类似这样打印图像的统计信息对另一个原因也是有帮助的。如果你的输入数据损坏并且图像中有不可表示的浮点数据（技术上称为[`NaN`](https://oreil.ly/E0xc2)），那么平均值本身将是`NaN`。这是一个方便的方式来确保在读取数据时没有出错。
- en: '![](Images/pmlc_0204.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0204.png)'
- en: Figure 2-4\. We compute the `reduce_mean()` along the row and column axes of
    the image.
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4. 我们在图像的行和列轴上计算 `reduce_mean()`。
- en: A Linear Model Using Keras
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 的线性模型
- en: As [Figure 2-4](#we_compute_the_reduce_meanleft_parenthes) demonstrates, the
    `reduce_mean()` function weights each pixel value in the image the same. What
    if we were to apply a different weight to each of the width * height * 3 pixel-channel
    points in the image?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 2-4](#we_compute_the_reduce_meanleft_parenthes)所示，`reduce_mean()` 函数对图像中的每个像素值都进行了相同的加权。如果我们在图像的每个宽度
    * 高度 * 3像素通道点上应用不同的权重会怎样？
- en: Given a new image, we can compute the weighted average of all its pixel values.
    We can then use this value to choose between the five types of flowers. Therefore,
    we will compute five such weighted averages (so that we are actually learning
    width * height * 3 * 5 weight values; see [Figure 2-5](#in_the_linear_modelcomma_there_are_five)),
    and choose the flower type based on which output is the largest.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一幅新图像，我们可以计算其所有像素值的加权平均值。然后，我们可以使用这个值来在五种花之间进行选择。因此，我们将计算五个这样的加权平均值（实际上是学习宽度
    * 高度 * 3 * 5个权重值；见[图 2-5](#we_compute_the_reduce_meanleft_parenthes)），并根据输出最大的选择花的类型。
- en: '![](Images/pmlc_0205.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0205.png)'
- en: Figure 2-5\. In the linear model, there are five outputs, one for each category;
    each of the output values is a weighted sum of the input pixel values.
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5. 在线性模型中，有五个输出，每个输出值都是输入像素值的加权和。
- en: 'In practice, a constant term called a *bias* is also added, so that we can
    represent each output value as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，还会添加一个称为*偏差*的常数项，以便我们可以将每个输出值表示为：
- en: <math><mrow><mrow><mtable><mtr><mtd><mrow><msub><mrow><mi>Y</mi></mrow><mrow><mi>j</mi></mrow></msub><mo>=</mo><msub><mrow><mi>b</mi></mrow><mrow><mi>j</mi></mrow></msub><mo>+</mo><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>r</mi><mi>o</mi><mi>w</mi><mi>s</mi></mrow></munder></mrow></mtd><mtd><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>l</mi><mi>u</mi><mi>m</mi><mi>n</mi><mi>s</mi></mrow></munder></mtd><mtd><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>c</mi><mi>h</mi><mi>a</mi><mi>n</mi><mi>n</mi><mi>e</mi><mi>l</mi><mi>s</mi></mrow></munder><mrow><mo>(</mo><msub><mrow><mi>w</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>*</mo><msub><mrow><mi>x</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>)</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mtable><mtr><mtd><mrow><msub><mrow><mi>Y</mi></mrow><mrow><mi>j</mi></mrow></msub><mo>=</mo><msub><mrow><mi>b</mi></mrow><mrow><mi>j</mi></mrow></msub><mo>+</mo><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>r</mi><mi>o</mi><mi>w</mi><mi>s</mi></mrow></munder></mrow></mtd><mtd><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>l</mi><mi>u</mi><mi>m</mi><mi>n</mi><mi>s</mi></mrow></munder></mtd><mtd><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>c</mi><mi>h</mi><mi>a</mi><mi>n</mi><mi>n</mi><mi>e</mi><mi>l</mi><mi>s</mi></mrow></munder><mrow><mo>(</mo><msub><mrow><mi>w</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>*</mo><msub><mrow><mi>x</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>)</mo></mrow></mtd></mtr></mtable></mrow></mrow></math>
- en: Without the bias, we’d be forcing the output to be zero if all the pixels are
    black.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有偏差，当所有像素都是黑色时，我们会强制输出为零。
- en: Keras Model
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras 模型
- en: Rather than write the preceding equation using low-level TensorFlow functions,
    it will be more convenient to use a higher-level abstraction. TensorFlow 1.1 shipped
    with one such abstraction, the Estimator API, and Estimators are still supported
    for backward compatibility. However, the Keras API has been part of TensorFlow
    since TensorFlow 2.0, and it’s what we recommend that you use.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用低级别的TensorFlow函数编写前述方程，使用更高级别的抽象会更方便。 TensorFlow 1.1附带了一个这样的抽象，即Estimator
    API，并且Estimators仍然支持向后兼容性。 但是，自TensorFlow 2.0以来，Keras API一直是TensorFlow的一部分，这是我们建议您使用的。
- en: 'A linear model can be represented in Keras as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，线性模型可以表示如下：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A *Sequential model* consists of *layers* that are connected such that the output
    of one layer is the input to the next. A layer is a Keras component that takes
    a tensor as input, applies some TensorFlow operations to that input, and outputs
    a tensor.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sequential 模型*由*层*连接而成，一个层的输出是下一个层的输入。 层是一个Keras组件，接受张量作为输入，对该输入应用一些TensorFlow操作，并输出一个张量。'
- en: The first layer, which is implicit, is the input layer, which asks for a 3D
    image tensor. The second layer (the `Flatten` layer) takes a 3D image tensor as
    input and reshapes it to be a 1D tensor with the same number of values. The `Flatten`
    layer is connected to a `Dense` layer with one output node for each class of flower.
    The name `Dense` means that every output is a weighted sum of every input and
    no weights are shared. We will encounter other common types of layers later in
    this chapter.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层，隐含的输入层，要求一个3D图像张量。 第二层（`Flatten`层）接受一个3D图像张量作为输入，并将其重新整形为具有相同数值的1D张量。 `Flatten`层连接到一个`Dense`层，每个花类别都有一个输出节点。
    `Dense`的名称意味着每个输出都是每个输入的加权和，没有权重共享。 在本章后面，我们将遇到其他常见类型的层。
- en: 'To use the Keras model defined here, we need to call `model.fit()` with the
    training dataset and `model.predict()` with each image we want to classify. To
    train the model, we need to tell Keras how to optimize the weights based on the
    training dataset. The way to do this is to *compile* the model, specifying an
    *optimizer* to use, the *loss* to minimize, and *metrics* to report. For example:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此处定义的 Keras 模型，我们需要用训练数据集调用 `model.fit()`，并用要分类的每个图像调用 `model.predict()`。
    要训练模型，我们需要告诉 Keras 如何基于训练数据集优化权重。 方法是*编译*模型，指定要使用的*优化器*，要*最小化*的*损失*，以及要报告的*指标*。
    例如：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The Keras `predict()` function will do the model computation on the image. The
    parameters to the `compile()` function make more sense if we look at the prediction
    code first, so let’s start there.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的 `predict()` 函数将在图像上进行模型计算。 如果我们先看预测代码，那么 `compile()` 函数的参数将更加有意义，所以我们从那里开始。
- en: Prediction function
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测函数
- en: 'Because the model has the trained set of weights in its internal state, we
    can compute the predicted value for an image by calling `model.predict()` and
    passing in the image:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因为模型在其内部状态中具有经过训练的权重集，所以我们可以通过调用 `model.predict()` 并传入图像来计算图像的预测值：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The reason for the `reshape()` is that `predict()` expects a batch of images,
    so we reshape the `img` tensor as a batch consisting of one image.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`reshape()` 的原因是 `predict()` 需要一个批量的图片，所以我们将 `img` 张量重塑为一个包含一张图片的批量。'
- en: What does the `pred` tensor that is output from `model.predict()` look like?
    Recall that the final layer of the model was a `Dense` layer with five outputs,
    so the shape of `pred` is (5)—that is, it consists of five numbers corresponding
    to the five flower types. The first output is the model’s confidence that the
    image in question is a daisy, the second output is the model’s confidence that
    the image is a dandelion, and so on. The predicted confidence values are called
    *logits* and are in the range –infinity to +infinity.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.predict()` 输出的 `pred` 张量是什么样子？回想一下，模型的最后一层是一个 `Dense` 层，有五个输出，因此 `pred`
    的形状是 (5) — 即，它由五个数字组成，分别对应五种花的类型。第一个输出是模型对所讨论图片为雏菊的置信度，第二个输出是模型对图片为蒲公英的置信度，以此类推。预测的置信值称为*logits*，其范围为负无穷到正无穷。'
- en: 'The model’s prediction is the label in which it has the highest confidence:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的预测是它最有信心的标签：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can convert the logits to probabilities by applying a function called the
    *softmax* function to them. So, the probability corresponding to the predicted
    label is:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过应用一个称为*softmax*函数的函数将 logits 转换为概率。因此，对应于预测标签的概率是：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Activation function
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'It is not sufficient to simply call `model.predict()`, because `model.predict()`
    returns a weighted sum that is unbounded. We can treat this weighted sum as logits
    and apply either the sigmoid or the softmax function (depending on whether we
    have a binary classification problem or a multiclass one) to obtain the probability:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 简单调用 `model.predict()` 是不够的，因为 `model.predict()` 返回一个未经限制的加权和。我们可以将这个加权和视为 logits，并应用
    sigmoid 或 softmax 函数（取决于我们是否有二元分类问题或多类分类问题）来获得概率：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can make things more convenient for end users if we add an *activation function*
    to the last layer of the model:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在模型的最后一层添加一个*激活函数*，可以为最终用户提供更方便的操作：
- en: '[PRE22]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If we do this, then `model.predict()` will return five probabilities (not logits),
    one for each class. There is no need for the client code to call `softmax()`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们这样做，`model.predict()` 将返回五个概率值（而不是 logits），每个类别一个。客户端代码无需调用 `softmax()`。
- en: Any layer in Keras can have an activation function applied to its output. Supported
    activation functions include `linear`, `sigmoid`, and `softmax`. We’ll look at
    other activation functions later in this chapter.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 中的任何层都可以对其输出应用激活函数。支持的激活函数包括 `linear`、`sigmoid` 和 `softmax`。我们将在本章后面探讨其他激活函数。
- en: Optimizer
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器
- en: 'Keras allows us to choose the optimizer that we wish to use to tune the weights
    based on the training dataset. Available optimizers include:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 允许我们选择优化器来根据训练数据集调整权重。可用的优化器包括：
- en: Stochastic gradient descent (SGD)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）
- en: The most basic optimizer.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的优化器。
- en: Adagrad (adaptive gradients) and Adam
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad（自适应梯度）和 Adam
- en: Improve upon the basic optimizer by adding features that allow for faster convergence.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加允许更快收敛的特性来改进基本优化器。
- en: Ftrl
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Ftrl
- en: An optimizer that tends to work well on extremely sparse datasets with many
    categorical features.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个倾向于在具有许多分类特征的极度稀疏数据集上表现良好的优化器。
- en: Adam is the tried-and-proven choice for deep learning models. We recommend using
    Adam as your optimizer for computer vision problems unless you have a strong reason
    not to.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 是深度学习模型的经过验证的选择。我们建议在计算机视觉问题中使用 Adam 作为优化器，除非你有充分的理由不这样做。
- en: 'SGD and all its variants, including Adam, rely on receiving mini-batches (often
    just called *batches*) of data. For each batch of data, we feed forward through
    the model and calculate the error and the *gradient*, or how much each weight
    contributed to the error; then the optimizer updates the weights with this information,
    ready for the next batch of data. Therefore, when we read the training dataset,
    we have to also batch it:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 及其所有变体，包括 Adam，依赖于接收数据的小批量（通常简称为*批次*）。对于每个数据批次，我们通过模型进行前向传播，计算误差和*梯度*，即每个权重对误差的贡献；然后优化器使用这些信息更新权重，准备好处理下一个数据批次。因此，当我们读取训练数据集时，我们也需要对其进行分批处理：
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Training loss
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练损失
- en: 'The optimizer tries to choose the weights that minimize the model’s error on
    the training dataset. For classification problems, there are strong mathematical
    reasons to choose cross-entropy as the error to be minimized. To calculate the
    cross-entropy, we compare the output probability (*p[j]* for the *j*th class)
    of the model against the true label for that class (*L[j]*) and sum this up over
    all the classes using the formula:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器试图选择能够最小化训练数据集上模型误差的权重。对于分类问题，选择交叉熵作为要最小化的错误具有强大的数学原因。为了计算交叉熵，我们使用以下公式比较模型对于第
    *j* 类的输出概率 (*p[j]*) 与该类的真实标签 (*L[j]*)，并将其总和计算所有类别：
- en: <math><mrow><mrow><munder><mrow><mi mathvariant="normal">Σ</mi></mrow><mrow><mi>j</mi></mrow></munder><mo>−</mo><msub><mrow><mi>L</mi></mrow><mrow><mi>j</mi></mrow></msub><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><msub><mrow><mi>p</mi></mrow><mrow><mi>j</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow></math>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><munder><mrow><mi mathvariant="normal">Σ</mi></mrow><mrow><mi>j</mi></mrow></munder><mo>−</mo><msub><mrow><mi>L</mi></mrow><mrow><mi>j</mi></mrow></msub><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><msub><mrow><mi>p</mi></mrow><mrow><mi>j</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow></math>
- en: In other words, we take the logarithm of the probability for predicting the
    correct label. If the model gets it exactly correct, this probability will be
    1; log(1) is 0, and so the loss is 0\. If the model gets it exactly wrong, this
    probability will be 0; log(0) is –infinity, and so the loss is +infinity, the
    worst possible loss. Using cross-entropy as our error measure allows us to tune
    the weights based on small improvements in the probability assigned to the correct
    label.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们取预测正确标签的概率的对数。如果模型完全正确，这个概率将为 1；log(1) 等于 0，因此损失为 0。如果模型完全错误，这个概率将为 0；log(0)
    是负无穷，因此损失是正无穷，即最差的损失。使用交叉熵作为我们的错误度量允许我们根据正确标签的概率小幅调整权重。
- en: In order to compute the loss, the optimizer will need to compare the label (returned
    by the `parse_csvline()` function) with the output of `model.predict()`. The specific
    loss you use will depend on how you are representing the label and what the last
    layer of your model returns.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算损失，优化器将需要将 `parse_csvline()` 函数返回的标签与 `model.predict()` 的输出进行比较。您使用的具体损失将取决于您如何表示标签以及您的模型的最后一层返回什么。
- en: 'If your labels are one-hot encoded (e.g., if the label is encoded as [1 0 0
    0 0] for daisy images), then, you should use *categorical cross-entropy* as your
    loss function. This will show up in your `decode_csv()` as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的标签是独热编码的（例如，如果标签编码为 [1 0 0 0 0] 用于雏菊图像），那么您应该使用*categorical cross-entropy*作为损失函数。这将在您的
    `decode_csv()` 中显示如下：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Because `CLASS_NAMES` is an array of strings, comparing to a single label will
    return a one-hot-encoded array where the Boolean value is 1 in the corresponding
    position. You will specify the loss as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 `CLASS_NAMES` 是一个字符串数组，与单个标签进行比较将返回一个独热编码数组，其中布尔值在相应位置为 1。您将如下指定损失：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that the constructor takes a parameter which specifies whether the last
    layer of the model returns logits of probabilities, or whether you have done a
    softmax.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意构造函数需要一个参数，指定模型的最后一层是返回对数几率还是已经进行了 softmax。
- en: 'On the other hand, if your labels will be represented as integer indices (e.g.,
    4 indicates tulips), then your `decode_csv()` will represent the label by the
    position of the correct class:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您的标签将表示为整数索引（例如，4 表示郁金香），则您的 `decode_csv()` 将通过正确类的位置来表示标签：
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'And the loss will be specified as:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 并且损失将被指定为：
- en: '[PRE27]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Again, take care to specify the value of `from_logits` appropriately.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意要适当地指定 `from_logits` 的值。
- en: Error metrics
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 错误度量
- en: While we can use the cross-entropy loss to minimize the error on the training
    dataset, business users will typically want a more understandable error metric.
    The most common error metric that is used for this purpose is *accuracy*, which
    is simply the fraction of instances that are classified correctly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以使用交叉熵损失来最小化训练数据集上的误差，但业务用户通常希望有一个更易理解的错误度量。用于此目的最常见的错误度量是*准确度*，它简单地是正确分类的实例的比例。
- en: 'However, the accuracy metric fails when one of the classes is very rare. Suppose
    you are trying to identify fake ID cards, and your model has the following performance
    characteristics:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当一个类别非常罕见时，精度度量会失败。假设您试图识别假身份证，您的模型具有以下性能特征：
- en: '|   | Card identified as fake | Card identified as genuine |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|   | 被识别为假身份证 | 被识别为真实身份证 |'
- en: '| Actual fake ID cards | 8 (TP) | 2 (FN) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 实际假身份证 | 8 (TP) | 2 (FN) |'
- en: '| Actual genuine ID cards | 140 (FP) | 850 (TN) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 实际真实身份证 | 140（FP） | 850（TN） |'
- en: 'The dataset has 990 genuine ID cards and 10 fake ID cards—there is a *class
    imbalance*. Of the fake ID cards, 8 were correctly identified as fake. These are
    the true positives (TP). The accuracy on this dataset would thus be (850 + 8)
    / 1,000, or 0.858\. It can be immediately seen that because fake ID cards are
    so rare, the model’s performance on this class has very little impact on its overall
    accuracy score—even if the model correctly identified only 2 of the 10 fake ID
    cards, the accuracy would remain nearly the same: 0.852\. Indeed, the model can
    achieve an accuracy of 0.99 simply by identifying all cards as being valid! In
    such cases, it is common to report two other metrics:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含 990 张真实身份证和 10 张假身份证——存在*类别不平衡*。其中，有 8 张假身份证被正确识别。这些是真正例（TP）。因此，该数据集的准确率为
    (850 + 8) / 1,000，即 0.858。可以立即看出，由于假身份证非常罕见，模型在这一类别上的表现对其整体准确率几乎没有影响——即使模型仅正确识别了
    10 张假身份证中的 2 张，准确率仍几乎不变：0.852。实际上，模型可以通过将所有卡片识别为有效卡片而达到 0.99 的准确率！在这种情况下，通常会报告另外两个指标：
- en: Precision
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 精度
- en: 'The fraction of true positives in the set of identified positives: TP / (TP
    + FP). Here, the model has identified 8 true positives and 140 false positives,
    so the precision is only 8/148\. This model is very imprecise.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在已识别正例集合中的真正例的比例：TP / (TP + FP)。这里，模型识别了 8 个真正例和 140 个假正例，因此精度仅为 8/148。该模型非常不精确。
- en: Recall
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 召回
- en: 'The fraction of true positives identified among all the positives in the dataset:
    TP / (TP + FN). Here, there are 10 positives in the full dataset and the model
    has identified 8 of them, so the recall is 0.8.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中所有正例中已识别出的真正例的比例：TP / (TP + FN)。这里，全数据集中有 10 个正例，模型已识别了其中的 8 个，因此召回率为 0.8。
- en: 'In addition to the precision and recall, it is also common to report the F1
    score, which is the harmonic mean of the two numbers:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除了精度和召回率外，通常还会报告 F1 分数，即两个数字的调和平均数：
- en: <math><mrow><mrow><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>/</mo><mrow><mrow><mo>[</mo><mfrac><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mn>1</mn></mrow></mstyle><mstyle displaystyle="true"
    scriptlevel="0"><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mstyle></mfrac><mo>+</mo><mfrac><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mn>1</mn></mrow></mstyle><mstyle displaystyle="true"
    scriptlevel="0"><mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mstyle></mfrac><mo>]</mo></mrow></mrow></mrow></mrow></math>
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>F</mi><mn>1</mn><mo>=</mo><mn>2</mn><mo>/</mo><mrow><mrow><mo>[</mo><mfrac><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mn>1</mn></mrow></mstyle><mstyle displaystyle="true"
    scriptlevel="0"><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mstyle></mfrac><mo>+</mo><mfrac><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mn>1</mn></mrow></mstyle><mstyle displaystyle="true"
    scriptlevel="0"><mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mstyle></mfrac><mo>]</mo></mrow></mrow></mrow></mrow></math>
- en: In a binary classification problem such as the one we’re considering here (identifying
    fake ID cards), the accuracy, precision, and recall all rely on the probability
    threshold we choose to determine whether to classify an instance in one category
    or the other. By varying the probability threshold, we can obtain different trade-offs
    in terms of precision and recall. The resulting curve is called the *precision-recall
    curve* (see [Figure 2-9](#by_varying_the_thresholdcomma_it_is_poss)). Another
    variant of this curve, where the true positive rate is plotted against the false
    positive rate, is called the *receiver operating characteristic* (ROC) curve.
    The area under the ROC curve (commonly shortened as *AUC*) is also often used
    as an aggregate measure of performance.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类问题中，例如我们正在考虑的问题（识别假身份证），准确率、精度和召回率都依赖于我们选择的概率阈值，以确定是否将实例分类为一类或另一类。通过调整概率阈值，我们可以在精度和召回率之间获得不同的权衡。结果曲线称为*精度-召回率曲线*（见[图
    2-9](#by_varying_the_thresholdcomma_it_is_p)）。这条曲线的另一个变体，其中真正例率绘制为假正例率，称为*接收者操作特征曲线*（ROC
    曲线）。ROC 曲线下的面积（通常缩写为*AUC*）也经常用作性能的综合衡量指标。
- en: '![](Images/pmlc_0209.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0209.png)'
- en: Figure 2-9\. By varying the threshold, it is possible to get different precision
    and recall measures.
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9。通过调整阈值，可以获得不同的精度和召回率度量。
- en: We normally want to report these metrics not on the training dataset, but on
    an independent evaluation dataset. This is to verify that the model hasn’t simply
    memorized the answers for the training dataset.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常不希望在训练数据集上报告这些指标，而是希望在独立的评估数据集上报告。这是为了验证模型是否仅仅记住了训练数据集的答案。
- en: Training the Model
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: Let’s now put all the concepts that we covered in the previous section together
    to create and train a Keras model.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们把前面章节涵盖的所有概念综合起来，创建并训练一个 Keras 模型。
- en: Creating the datasets
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建数据集
- en: To train a linear model, we need a training dataset. Actually, we want two datasets—a
    training dataset and an evaluation dataset—to verify whether the trained model
    *generalizes*, or works on data that it has not seen during training.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个线性模型，我们需要一个训练数据集。实际上，我们希望有两个数据集——一个训练数据集和一个评估数据集——以验证训练出的模型*泛化*能力，即是否能在未在训练中见过的数据上运行。
- en: 'So, we first obtain the training and evaluation datasets:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们首先获取训练和评估数据集：
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'where `decode_csv()` reads and decodes JPEG images:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`decode_csv()`读取并解码 JPEG 图像：
- en: '[PRE29]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `label` that is returned in this code is the sparse representation—i.e.,
    the number 4 for tulips, that class’s index—and not the one-hot-encoded one. We
    batch the training dataset because the optimizer class expects batches. We also
    batch the evaluation dataset to avoid creating two versions of all our methods
    (one that operates on batches, and another that requires one image at a time).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码返回的`label`是稀疏表示——即郁金香的编号 4，该类的索引——而不是独热编码。我们对训练数据集进行分批处理，因为优化器类期望批次处理。我们还对评估数据集进行批处理，以避免创建所有方法的两个版本（一个批次操作，另一个需要逐个图像处理）。
- en: Creating and viewing the model
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建和查看模型
- en: 'Now that the datasets have been created, we need to create the Keras model
    that is to be trained using those datasets:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集已创建，我们需要创建要使用这些数据集进行训练的 Keras 模型：
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can view the model using:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下方法查看模型：
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This yields the diagram in [Figure 2-10](#a_keras_linear_model_to_classify_flowers).
    Note that the input layer takes a batch (that’s the ?) of [224, 224, 3] images.
    The question mark indicates that the size of this dimension is undefined until
    runtime; this way, the model can dynamically adapt to any batch size. The `Flatten`
    layer takes this input and returns a batch of 224 * 224 * 3 = 150,528 numbers
    that are then connected to five outputs in the `Dense` layer.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生[图 2-10](#a_keras_linear_model_to_classify_flowers)中的图表。请注意，输入层接受一个批次（这是？）的[224,
    224, 3]图像。问号表示此维度的大小在运行时未定义；这样，模型可以动态适应任何批次大小。`Flatten`层接受此输入并返回一批 224 * 224 *
    3 = 150,528 个数字，然后连接到`Dense`层的五个输出。
- en: '![](Images/pmlc_0210.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0210.png)'
- en: Figure 2-10\. A Keras linear model to classify flowers.
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-10\. 用于分类花卉的 Keras 线性模型。
- en: 'We can verify that the `Flatten` operation does not need any trainable weights,
    but the `Dense` layer has 150,528 * 5 = 752,645 weights that need to be trained
    by using `model.summary()`, which yields:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以验证`Flatten`操作不需要任何可训练的权重，但`Dense`层有 150,528 * 5 = 752,645 个需要通过`model.summary()`训练的权重，如下所示：
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Fitting the model
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'Next, we train the model using `model.fit()` and pass in the training and validation
    datasets:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`model.fit()`训练模型，并传入训练和验证数据集：
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Note that we are passing in the training dataset to train on, and the validation
    dataset to report accuracy metrics on. We are asking the optimizer to go through
    the training data 10 times (an *epoch* is a full pass through the dataset). We
    hope that 10 epochs will be sufficient for the loss to converge, but we should
    verify this by plotting the history of the loss and error metrics. We can do that
    by looking at what the history has been tracking:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将训练数据集传入以进行训练，并将验证数据集传入以报告准确度指标。我们要求优化器通过训练数据 10 次（一个*epoch*是对整个数据集的完整遍历）。我们希望
    10 个 epochs 足够使损失收敛，但我们应通过绘制损失和错误指标的历史记录来验证这一点。我们可以通过查看历史记录来进行：
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We obtain the following list:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得以下列表：
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can then plot the loss and validation loss using:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以绘制损失和验证损失：
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This yields the graph shown in the lefthand panel of [Figure 2-11](#loss_and_accuracy_curves_on_the_training).
    Note that the loss does not go down smoothly; instead, it is quite choppy. This
    is an indication that our choices of batch size and optimizer settings can be
    improved—unfortunately, this part of the ML process is trial and error. The validation
    loss goes down, and then starts to increase. This is an indication that *overfitting*
    is starting to happen: the network has started to memorize details of the training
    dataset (such details are called *noise*) that do not occur in the validation
    dataset. Either 10 epochs is too long, or we need to add regularization. Overfitting
    and regularization are topics that we will address in more detail in the next
    section.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了[图 2-11](#loss_and_accuracy_curves_on_the_training)左侧面板上显示的图表。请注意，损失值不会平稳下降；相反，它非常波动。这表明我们的批次大小和优化器设置的选择可以改进——不幸的是，这部分ML过程是试错的。验证损失下降，然后开始增加。这表明开始发生*过拟合*：网络已经开始记忆训练数据集中不发生的细节（这些细节称为*噪声*）。10个时期要么太长，要么我们需要添加正则化。过拟合和正则化是我们将在下一节详细讨论的主题。
- en: '![](Images/pmlc_0211.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0211.png)'
- en: Figure 2-11\. Loss and accuracy curves on the training (solid) and validation
    (dashed) sets.
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-11\. 训练（实线）和验证（虚线）集上的损失和准确率曲线。
- en: 'It is also possible to plot the accuracy on the training dataset and the validation
    dataset using:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用以下方法绘制训练数据集和验证数据集上的准确率：
- en: '[PRE37]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The resulting graph is shown in the righthand panel of [Figure 2-11](#loss_and_accuracy_curves_on_the_training).
    Notice that the accuracy on the training dataset goes on increasing the longer
    we train, while the accuracy on the validation dataset plateaus.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图 2-11](#loss_and_accuracy_curves_on_the_training)的右侧面板上。请注意，在训练数据集上的准确率随着训练时间的增加而增加，而在验证数据集上的准确率则趋于平稳。
- en: These lines are also choppy, providing us with the same insights we got from
    the loss curves. However, the accuracy that we have obtained (0.4) on the evaluation
    dataset is better than what we would have gotten from random chance (0.2). This
    indicates the model has been able to learn and become somewhat skillful at the
    task.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这些曲线也是波动的，为我们提供了与损失曲线相同的见解。然而，我们在评估数据集上获得的准确率（0.4）比随机选择（0.2）要好。这表明模型已经能够学习，并在任务上变得有些熟练。
- en: Plotting predictions
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绘制预测
- en: 'We can look at what the model has learned by plotting its predictions on a
    few images from the training dataset:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制模型在训练数据集中的几个图像的预测来查看模型学到了什么：
- en: '[PRE38]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Note that we need to take the single image that we have and make it a batch,
    because that’s what the model was trained on and what it expects. Fortunately,
    we don’t need to pass in exactly 10 images (our batch size was 10 during training)
    because the model was designed to take any batch size (recall that the first dimension
    in [Figure 2-10](#a_keras_linear_model_to_classify_flowers) was a ?).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要将我们有的单个图像变成一个批次，因为这是模型训练和预期的内容。幸运的是，我们不需要精确地传递10个图像（我们的批次大小在训练期间是10），因为模型被设计为接受任何批次大小（回想一下[图
    2-10](#a_keras_linear_model_to_classify_flowers)中的第一个维度是一个？）。
- en: The first few predictions from the training and evaluation datasets are shown
    in [Figure 2-12](Images/#the_first_few_images_from_the_training_l).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练和评估数据集中显示的前几个预测如[图 2-12](Images/#the_first_few_images_from_the_training_l)所示。
- en: '![](Images/pmlc_0212.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0212.png)'
- en: Figure 2-12\. The first few images from the training (top row) and evaluation
    (bottom row) datasets—the first image, which is actually a daisy, has been wrongly
    classified as a dandelion with a probability of 0.74.
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-12\. 训练（顶行）和评估（底行）数据集中的前几个图像——第一个图像实际上是一朵雏菊，但被错误地分类为蒲公英，概率为0.74。
- en: A Neural Network Using Keras
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras的神经网络
- en: 'In the linear model that we covered in the previous section, we wrote the Keras
    model as:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中我们介绍的线性模型中，我们将Keras模型写成了：
- en: '[PRE39]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is the softmax of the weighted average of the flattened input pixel
    values:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是输入像素值的加权平均值的softmax：
- en: <math><mrow><mrow><mi>Y</mi><mo>=</mo><mo>softmax</mo><mo>⁡</mo><mrow><mrow><mo>(</mo></mrow><mi>B</mi><mo>+</mo><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>p</mi><mi>i</mi><mi>x</mi><mi>e</mi><mi>l</mi><mi>s</mi></mrow></munder><msub><mrow><mi>W</mi></mrow><mrow><mi>i</mi></mrow></msub><msub><mrow><mi>X</mi></mrow><mrow><mi>i</mi></mrow></msub><mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>Y</mi><mo>=</mo><mo>softmax</mo><mo>⁡</mo><mrow><mrow><mo>(</mo></mrow><mi>B</mi><mo>+</mo><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>p</mi><mi>i</mi><mi>x</mi><mi>e</mi><mi>l</mi><mi>s</mi></mrow></munder><msub><mrow><mi>W</mi></mrow><mrow><mi>i</mi></mrow></msub><msub><mrow><mi>X</mi></mrow><mrow><mi>i</mi></mrow></msub><mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>
- en: '*B* is the bias tensor, *W* the weights tensor, *X* the input tensor, and *Y*
    the output tensor. This is usually written in matrix form as (using § to represent
    the softmax):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*B* 是偏置张量，*W* 是权重张量，*X* 是输入张量，*Y* 是输出张量。通常这被写成矩阵形式（使用 § 代表 softmax）：'
- en: <math><mrow><mrow><mi>Y</mi><mo>=</mo><mi>§</mi><mrow><mo>(</mo><mi>B</mi><mo>+</mo><mi>W</mi><mi>X</mi><mo>)</mo></mrow></mrow></mrow></math>
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>Y</mi><mo>=</mo><mi>§</mi><mrow><mo>(</mo><mi>B</mi><mo>+</mo><mi>W</mi><mi>X</mi><mo>)</mo></mrow></mrow></mrow></math>
- en: 'As shown in [Figure 2-10](#a_keras_linear_model_to_classify_flowers), and in
    the following model summary, there is only one trainable layer, the `Dense` one.
    The `Flatten` operation is a reshaping operation and does not contain any trainable
    weights:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2-10](#a_keras_linear_model_to_classify_flowers) 所示，并且在以下模型摘要中，只有一个可训练层，即
    `Dense` 层。`Flatten` 操作是一种重塑操作，并不包含任何可训练权重：
- en: '[PRE40]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Linear models are great, but they are limited in what they can model. How do
    we obtain more complex models?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型很好，但它们在建模能力上有所限制。我们如何获得更复杂的模型？
- en: Neural Networks
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: One way to get a more complex model is to interpose one or more `Dense` layers
    in between the input and output layers. This results in a machine learning model
    called a *neural network*, for reasons that we will explain shortly.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 获得更复杂模型的一种方式是在输入和输出层之间插入一个或多个 `Dense` 层。这导致了一种被称为 *神经网络* 的机器学习模型，我们稍后会解释原因。
- en: Hidden layers
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏层
- en: 'Suppose that we interpose one more `Dense` layer using:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们插入一个额外的 `Dense` 层：
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The model now has three layers (see [Figure 2-14](#a_neural_network_with_one_hidden_layerdo)).
    A layer with trainable weights, such as the one we added that is neither the input
    nor the output layer, is called a *hidden* layer.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型有三层（见 [Figure 2-14](#a_neural_network_with_one_hidden_layerdo)）。具有可训练权重的层，例如我们添加的那一层，既不是输入也不是输出层，称为
    *隐藏* 层。
- en: '![](Images/pmlc_0214.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0214.png)'
- en: Figure 2-14\. A neural network with one hidden layer.
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-14\. 具有一个隐藏层的神经网络。
- en: 'Mathematically, the output is now:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，输出现在是：
- en: <math><mrow><mrow><mi>Y</mi><mo>=</mo><mi>§</mi><mrow><mo>(</mo><msub><mrow><mi>B</mi></mrow><mrow><mn>2</mn></mrow></msub><mo>+</mo><msub><mrow><mi>W</mi></mrow><mrow><mn>2</mn></mrow></msub><mrow><mo>(</mo><msub><mrow><mi>B</mi></mrow><mrow><mn>1</mn></mrow></msub><mo>+</mo><msub><mrow><mi>W</mi></mrow><mrow><mn>1</mn></mrow></msub><mi>X</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mrow></math>
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>Y</mi><mo>=</mo><mi>§</mi><mrow><mo>(</mo><msub><mrow><mi>B</mi></mrow><mrow><mn>2</mn></mrow></msub><mo>+</mo><msub><mrow><mi>W</mi></mrow><mrow><mn>2</mn></mrow></msub><mrow><mo>(</mo><msub><mrow><mi>B</mi></mrow><mrow><mn>1</mn></mrow></msub><mo>+</mo><msub><mrow><mi>W</mi></mrow><mrow><mn>1</mn></mrow></msub><mi>X</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mrow></math>
- en: 'Simply wrapping multiple layers like this is pointless, since we might as well
    have multiplied the second layer’s weight (*W*[2]) into the equation—the model
    remains a linear model. However, if we add a nonlinear *activation* function *A*(*x*)
    to transform the output of the hidden layer:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地包装多个层如此是没有意义的，因为我们可以直接将第二层的权重 (*W*[2]) 乘到方程中——模型仍然是线性模型。然而，如果我们在隐藏层的输出上添加非线性
    *激活* 函数 *A*(*x*) 来变换输出：
- en: <math><mrow><mrow><mi>Y</mi><mo>=</mo><mi>§</mi><mrow><mo>(</mo><msub><mrow><mi>B</mi></mrow><mrow><mn>2</mn></mrow></msub><mo>+</mo><msub><mrow><mi>W</mi></mrow><mrow><mn>2</mn></mrow></msub><mi>A</mi><mrow><mo>(</mo><msub><mrow><mi>B</mi></mrow><mrow><mn>1</mn></mrow></msub><mo>+</mo><msub><mrow><mi>W</mi></mrow><mrow><mn>1</mn></mrow></msub><mi>X</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mrow></math>
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>Y</mi><mo>=</mo><mi>§</mi><mrow><mo>(</mo><msub><mrow><mi>B</mi></mrow><mrow><mn>2</mn></mrow></msub><mo>+</mo><msub><mrow><mi>W</mi></mrow><mrow><mn>2</mn></mrow></msub><mi>A</mi><mrow><mo>(</mo><msub><mrow><mi>B</mi></mrow><mrow><mn>1</mn></mrow></msub><mo>+</mo><msub><mrow><mi>W</mi></mrow><mrow><mn>1</mn></mrow></msub><mi>X</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mrow></math>
- en: then the output becomes capable of representing more complex relationships than
    a simple linear function.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 那么输出就能表示比简单线性函数更复杂的关系。
- en: 'In Keras, we introduce the activation function as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，我们如下引入激活函数：
- en: '[PRE42]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The rectified linear unit (ReLU) is the most commonly used activation function
    for hidden layers (see [Figure 2-15](#a_few_nonlinear_activation_functionsdot)).
    Other commonly used activation functions include *sigmoid*, *tanh*, and *elu*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 整流线性单元（ReLU）是隐藏层中最常用的激活函数（见[图 2-15](#a_few_nonlinear_activation_functionsdot)）。其他常用的激活函数包括*sigmoid*、*tanh*和*elu*。
- en: '![](Images/pmlc_0215.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0215.png)'
- en: Figure 2-15\. A few nonlinear activation functions.
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-15\. 一些非线性激活函数。
- en: All three of the activation functions shown in [Figure 2-15](#a_few_nonlinear_activation_functionsdot)
    are loosely based on how neurons in the human brain fire if the input from the
    dendrites together exceeds some minimum threshold (see [Figure 2-16](#neurons_in_the_brain_fire_when_the_sum_o)).
    Thus, a model that has a hidden layer with a nonlinear activation function is
    called a “neural network.”
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 2-15](#a_few_nonlinear_activation_functionsdot)中展示的所有三个激活函数都与人脑中神经元的工作方式
    loosly 相关，如果来自树突的输入共同超过一些最小阈值，则模型具有具有非线性激活函数的隐藏层，称为“神经网络”。
- en: '![](Images/pmlc_0216.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0216.png)'
- en: 'Figure 2-16\. Neurons in the brain fire when the sum of the inputs exceeds
    some minimum threshold. Image credit: Allen Institute for Brain Science, Allen
    Human Brain Atlas, available from [human.brain-map.org](http://human.brain-map.org).'
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-16\. 当输入总和超过一些最小阈值时，人脑中的神经元会发出。 图片来源：艾伦大脑科学研究所，艾伦人类大脑图谱，可从[human.brain-map.org](http://human.brain-map.org)获取。
- en: The sigmoid is a continuous function that behaves most similarly to how brain
    neurons work—the output saturates at both extremes. However, the sigmoid function
    suffers from slow convergence because the weight update at each step is proportional
    to the gradient, and the gradient near the extremes is very small. The ReLU is
    more often used so that the weight updates remain the same size in the active
    part of the function. In a `Dense` layer with a ReLU activation function, the
    activation function “fires” if the weighted sum of the inputs is greater than
    –*b*, where *b* is the bias. The strength of the firing is proportional to the
    weighted sum of the inputs. The issue with a ReLU is that it is zero for half
    its domain. This leads to a problem called *dead ReLUs*, where no weight update
    ever happens. The elu activation function (see [Figure 2-15](#a_few_nonlinear_activation_functionsdot))
    solves this problem by having a small exponential negative value instead of zero.
    It is, however, expensive to compute because of the exponential. Therefore, some
    ML practitioners instead use the Leaky ReLU, which uses a small negative slope.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 是一个连续函数，其行为最类似于脑神经元的工作方式——输出在两个极端处饱和。然而，Sigmoid 函数收敛速度较慢，因为每一步的权重更新与梯度成正比，并且在极端值附近的梯度非常小。ReLU
    更常用，因为在函数的活跃部分权重更新保持相同大小。在具有 ReLU 激活函数的 `Dense` 层中，如果输入的加权和大于-*b*，其中 *b* 是偏差，激活函数就会“触发”。触发的强度与输入的加权和成正比。ReLU
    的问题在于其域的一半为零。这导致了一个称为*死亡ReLU*的问题，即从不进行权重更新。elu 激活函数（见[图 2-15](#a_few_nonlinear_activation_functionsdot)）通过在零的地方使用一个小的指数负值来解决这个问题。然而，由于指数函数的存在，elu
    的计算成本较高。因此，一些机器学习从业者选择使用带有小负斜率的Leaky ReLU。
- en: Training the neural network
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'Training the neural network is similar to training the linear model.  We compile
    the model, passing in the optimizer, the loss, and the metrics. Then we call `model.fit()`,
    passing in the datasets:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络类似于训练线性模型。我们编译模型，传入优化器、损失函数和度量标准。然后调用`model.fit()`，传入数据集：
- en: '[PRE43]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The result, shown in [Figure 2-17](#loss_and_accuracy_on_the_training_and_va),
    reveals that the best validation accuracy that we have obtained (0.45) is similar
    to what we obtained with a linear model. The curves are also not smooth.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[图 2-17](#loss_and_accuracy_on_the_training_and_va)中，我们获得的最佳验证准确率（0.45）与线性模型的结果相似。曲线也不够平滑。
- en: '![](Images/pmlc_0217.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0217.png)'
- en: Figure 2-17\. Loss and accuracy on the training and validation datasets when
    training a neural network.
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-17\. 在训练神经网络时的训练和验证数据集上的损失和准确率。
- en: We would normally expect that adding layers to a model will improve the ability
    of the model to fit the training data, and thus lower the loss. That is, indeed,
    the case—whereas the cross-entropy loss for the linear model is on the order of
    10, it is on the order of 2 for the neural network. However, the accuracies are
    pretty similar, indicating that much of the improvement is obtained by the model
    driving probabilities like 0.7 to be closer to 1.0 than by getting items misclassified
    by the linear model correct.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常期望向模型添加层次将提高模型拟合训练数据的能力，从而降低损失。实际上，这种情况确实如此——线性模型的交叉熵损失约为10，而神经网络的交叉熵损失约为2。然而，准确率相似，表明模型的改进很大程度上是通过使概率接近1.0来获得，而不是通过纠正线性模型错误分类的项目。
- en: There are still some improvements that we can try, though. For example, we can
    change the learning rate and the loss function, and make better use of the validation
    dataset. We’ll look at these next.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，还有一些改进方法可以尝试。例如，我们可以调整学习率和损失函数，并更好地利用验证数据集。我们将在接下来进行探讨。
- en: Learning rate
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率
- en: A gradient descent optimizer works by looking in all directions at each point
    and picking the direction where the error function is decreasing the most rapidly.
    Then it makes a step in that direction and tries again. For example, in [Figure 2-18](#how_a_gradient_descent_optimizer_worksdo),
    starting at the first point (the circle marked 1), the optimizer looks in two
    directions (actually 2*^N* directions, where *N* is the dimension of the weight
    tensor to be optimized) and chooses direction 2, because it is the direction in
    which the loss function decreases the fastest. Then, the optimizer updates the
    weight value by making a *step* in that direction, as indicated by the dashed
    curved line. The size of this step for every weight value is proportional to a
    model hyperparameter called the *learning rate*.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降优化器通过在每个点查看所有方向并选择错误函数下降最快的方向来工作。然后，它沿着该方向迈出一步，并再次尝试。例如，在[图2-18](#how_a_gradient_descent_optimizer_worksdo)中，从第一个点（标记为圆圈1）开始，优化器查看两个方向（实际上是2*^N*个方向，其中*N*是要优化的权重张量的维度），选择方向2，因为损失函数在该方向上下降最快。然后，优化器通过制定的虚线曲线更新权重值。每个权重值的步长大小与称为“学习率”的模型超参数成比例。
- en: '![](Images/pmlc_0218.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0218.png)'
- en: Figure 2-18\. How a gradient descent optimizer works.
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-18. 梯度下降优化器的工作原理。
- en: As you can see, if the learning rate is too high, the optimizer might skip over
    the minima completely. After this step (denoted by the circle marked 2 in the
    figure), the optimizer again looks in two directions and then continues to the
    third point, because the loss curve is dropping faster in that direction. After
    this step, the gradient is evaluated again. Now the direction points backward,
    and the optimizer manages to find the local minimum in between the second and
    third points. The global minimum which was between the first and second steps
    has, however, been missed.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，如果学习速率过高，优化器可能会完全跳过最小值。在此步骤之后（在图中标记为圆圈2），优化器再次朝两个方向查找，然后继续到第三点，因为损失曲线在那个方向下降得更快。在这一步之后，再次评估梯度。现在方向指向后方，优化器成功地在第二和第三点之间找到了局部最小值。然而，原本位于第一和第二步之间的全局最小值被错过了。
- en: In order to not skip over minima, we should use a small value for the learning
    rate. But if the learning rate is too small, the model will get stuck in a local
    minimum. Also, the smaller the value of the learning rate, the slower the model
    will converge. Thus, there’s a trade-off between not missing minima and getting
    the model to converge quickly.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了不跳过最小值，我们应该使用较小的学习率。但如果学习率太小，模型会陷入局部最小值。此外，学习率值越小，模型收敛速度就越慢。因此，在不错过最小值和使模型快速收敛之间存在权衡。
- en: 'The default learning rate for the Adam optimizer is 0.001\. We can change it
    by changing the optimizer passed into the `compile()` function:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 优化器的默认学习率为 0.001。我们可以通过更改传递给 `compile()` 函数的优化器来更改它：
- en: '[PRE44]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Repeating the training with this lower training rate, we get the same end result
    in terms of accuracy, but the curves are noticeably less choppy (see [Figure 2-19](#the_loss_and_accuracy_curves_when_the_le)).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较低的训练率重复训练后，我们在准确性方面获得了相同的最终结果，但曲线明显更为平稳（参见[图2-19](#the_loss_and_accuracy_curves_when_the_le)）。
- en: '![](Images/pmlc_0219.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0219.png)'
- en: Figure 2-19\. The loss and accuracy curves when the learning rate is lowered
    to 0.0001.
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-19。当学习率降低到0.0001时的损失和准确率曲线。
- en: Regularization
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: It is also worth noting that the number of trainable parameters in the neural
    network is 128 times the number of trainable parameters in the linear model (19
    million versus 750,000). Yet, we have only about 3,700 images. The more complex
    model might perform better, but we’d probably need much more data—on the order
    of hundreds of thousands of images. Later in this book, we will look at data augmentation
    techniques to make the most out of the data that we do have.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，神经网络中可训练参数的数量是线性模型的128倍（1900万对比75万）。然而，我们只有大约3700张图像。更复杂的模型可能表现更好，但我们可能需要更多的数据——数十万张图像。本书后面，我们将介绍数据增强技术，以充分利用我们所拥有的数据。
- en: Given that we have a relatively small dataset for the complexity of the model
    that we are using, it is possible that the model will start to use individual
    trainable weights to “memorize” the classification answers for individual images
    in the training dataset—this is the overfitting that we can see happening in [Figure 2-19](#the_loss_and_accuracy_curves_when_the_le)
    (the loss on the validation set started to increase even though the training accuracy
    was still decreasing). When this happens, the weight values start to become highly
    tuned to very specific pixel values and attain very high values.^([1](ch02.xhtml#ch02fn01))
    Therefore, we can reduce the incidence of overfitting by changing the loss to
    apply a penalty on the weight values themselves. This sort of penalty applied
    to the loss function is called *regularization*.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们使用的模型相对复杂，数据集相对较小，模型可能开始使用单独的可训练权重来“记住”训练数据集中每个图像的分类答案——这就是我们可以在[图2-19](#the_loss_and_accuracy_curves_when_the_le)中观察到的过拟合现象（尽管训练准确率仍在降低，验证集上的损失开始增加）。当发生这种情况时，权重值开始高度调整到非常特定的像素值，并获得非常高的值。^([1](ch02.xhtml#ch02fn01))
    因此，我们可以通过改变损失函数应用在权重值本身上的惩罚来减少过拟合的发生。这种在损失函数上应用的惩罚称为*正则化*。
- en: 'Two common forms are:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 两种常见形式是：
- en: <math><mrow><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mi>c</mi><mi>r</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>-</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>+</mo><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>i</mi></mrow></munder><mrow><mo>|</mo><msub><mrow><mi>w</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>|</mo></mrow></mrow></mrow></math>
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mi>c</mi><mi>r</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>-</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>+</mo><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>i</mi></mrow></munder><mrow><mo>|</mo><msub><mrow><mi>w</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>|</mo></mrow></mrow></mrow></math>
- en: 'and:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 和：
- en: <math><mrow><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mi>c</mi><mi>r</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>-</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>+</mo><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>i</mi></mrow></munder><mrow><msubsup><mrow><mi>w</mi></mrow><mrow><mi>i</mi></mrow><mrow><mn>2</mn></mrow></msubsup></mrow></mrow></mrow></math>
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mi>c</mi><mi>r</mi><mi>o</mi><mi>s</mi><mi>s</mi><mi>-</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mo>+</mo><munder><mrow><mi
    mathvariant="normal">Σ</mi></mrow><mrow><mi>i</mi></mrow></munder><mrow><msubsup><mrow><mi>w</mi></mrow><mrow><mi>i</mi></mrow><mrow><mn>2</mn></mrow></msubsup></mrow></mrow></mrow></math>
- en: The first type of penalty is called an *L1 regularization term*, and the second
    is called an *L2 regularization term*. Either penalty will cause the optimizer
    to prefer smaller weight values. L1 regularization drives many of the weight values
    to zero but is more tolerant of individual large weight values than L2 regularization,
    which tends to drive all the weights to small but nonzero values. The mathematical
    reasons why this is the case are beyond the scope of this book, but it’s useful
    to understand that we use L1 if we want a compact model (because we can prune
    zero weights), whereas we use L2 if we want to limit overfitting to the maximum
    possible.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种惩罚类型称为*L1正则化项*，第二种称为*L2正则化项*。任何一种惩罚都会使优化器倾向于更小的权重值。L1正则化驱使许多权重值为零，但对于个别大权重值更具有宽容性，而L2正则化则倾向于驱使所有权重值为小但非零值。为什么会出现这种情况的数学原因超出了本书的范围，但了解L1用于紧凑模型（因为我们可以修剪零权重），而L2用于尽可能限制过拟合很有用。
- en: 'This is how we apply a regularization term to the `Dense` layers:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何在`Dense`层应用正则化项的方法：
- en: '[PRE45]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: With L2 regularization turned on, we see from [Figure 2-20](#the_loss_and_accuracy_curves_when_ltwo_r)
    that the loss values are higher (because they include the penalty term). However,
    it is clear that overfitting is still happening after epoch 6\. This indicates
    that we need to increase the regularization amount. Again, this is trial and error.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 开启L2正则化后，从[图2-20](#the_loss_and_accuracy_curves_when_ltwo_r)可以看出，损失值较高（因为包含了惩罚项）。然而，明显看出，在第6个epoch后仍然存在过拟合现象。这表明我们需要增加正则化的量。再次强调，这是一个试验和错误的过程。
- en: '![](Images/pmlc_0220.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![Images/pmlc_0220.png](Images/pmlc_0220.png)'
- en: Figure 2-20\. The loss and accuracy curves when L2 regularization is added.
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-20. 当添加L2正则化时的损失和准确率曲线。
- en: Early stopping
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提前停止
- en: Look carefully at the righthand panel in [Figure 2-20](#the_loss_and_accuracy_curves_when_ltwo_r).
    Both the training and validation set accuracies increase smoothly until the sixth
    epoch. After that, even though the training set accuracy continues to increase,
    the validation accuracy starts to drop. This is a classic sign that the model
    has stopped generalizing to unseen data, and is now starting to fit noise in the
    training dataset.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察[图2-20](#the_loss_and_accuracy_curves_when_ltwo_r)右侧面板。训练集和验证集的准确率都平稳增加直到第六个epoch。之后，尽管训练集准确率继续增加，验证集准确率开始下降。这是模型停止泛化到未见数据，开始拟合训练数据中的噪声的典型迹象。
- en: 'It would be good if we could stop the training once the validation accuracy
    stops increasing. In order to do that, we pass in a callback to the `model.fit()`
    function:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能在验证准确率停止增加时停止训练，那将是很好的。为了做到这一点，我们将一个回调函数传递给`model.fit()`函数：
- en: '[PRE46]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Because convergence can be a bit bumpy, the `patience` parameter allows us to
    configure the number of epochs for which we want the validation accuracy to not
    decrease before training is stopped.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 由于收敛可能会有些波动，`patience`参数允许我们配置在停止训练之前希望验证准确率不下降的epoch数。
- en: Note
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Add in the `EarlyStopping()` callback only after you have tuned the learning
    rate and regularization to get smooth, well-behaved training curves. If your training
    curves are choppy, it is possible that you will miss out on obtaining better performance
    by stopping early.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在调整了学习率和正则化以获得平滑且良好的训练曲线后，才添加`EarlyStopping()`回调函数。如果你的训练曲线波动较大，可能会错过通过提前停止获得更好性能的机会。
- en: Hyperparameter tuning
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数调整
- en: 'We chose a number of parameters for our model: the number of hidden nodes,
    the learning rate, the L2 regularization, and so on. How do we know that these
    are optimal? We don’t. We need to *tune* these hyperparameters.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为模型选择了一些参数：隐藏节点数、学习率、L2正则化等等。我们怎么知道这些是最优的呢？我们不知道。我们需要*调整*这些超参数。
- en: 'One way to do this is to use the Keras Tuner. To use the Keras Tuner, we implement
    the model-building function to use hyperparameters (the full code is in a [*02_ml_models/02b_neural_network.ipynb*
    on GitHub)](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/02_ml_models/02b_neural_network.ipynb):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一个使用Keras Tuner的方法是实现模型构建函数来使用超参数（完整代码在[*02_ml_models/02b_neural_network.ipynb*
    on GitHub)](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/02_ml_models/02b_neural_network.ipynb)。
- en: '[PRE47]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As you can see, we defined the space from which the hyperparameters are drawn.
    The learning rate (`lrate`) is a floating-point value between 1e-4 and 1e-1, chosen
    logarithmically (not linearly). The L2 regularization value is chosen from a set
    of five predefined values (0.0, 1e-1, 1e-2, 1e-3, and 1e-4). The number of hidden
    nodes (`num_hidden`) is an integer chosen from the range 32 to 256 in increments
    of 32\. These values are then used in the model-building code as normal.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们定义了超参数的取值空间。学习率（`lrate`）是一个浮点值，选择对数范围内的数值（而非线性范围）。L2正则化值是从五个预定义值中选择的一个（0.0,
    1e-1, 1e-2, 1e-3, 和 1e-4）。隐藏节点数（`num_hidden`）是一个整数，从32到256，以32为增量选择。然后，这些值像平常一样在模型构建代码中使用。
- en: 'We pass the `build_model()` function into a Keras Tuner optimization algorithm.
    Several algorithms are [supported](https://keras-team.github.io/keras-tuner/documentation/tuners/),
    but Bayesian optimization is an old standby that works well for computer vision
    problems:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`build_model()`函数传递给Keras Tuner优化算法。支持多种算法，但贝叶斯优化是一个适用于计算机视觉问题的老牌算法，效果良好：
- en: '[PRE48]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Here, we are specifying that our objective is to maximize the validation accuracy
    and that we want the Bayesian optimizer to run 10 trials starting from 2 randomly
    chosen seed points. The tuner can pick up where it left off, and we are asking
    Keras to do so by telling it to reuse information learned in preexisting trials
    and not start with a blank slate.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们明确指出我们的目标是最大化验证准确率，并且我们希望贝叶斯优化器从2个随机选择的种子点开始运行10次试验。调参器可以从上次停下的地方继续，我们要求Keras这样做，告诉它重复使用在现有试验中学到的信息，而不是从头开始。
- en: 'Having created the tuner, we can then run the search:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了调参器后，我们可以运行搜索：
- en: '[PRE49]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'At the end of the run, we can get the top *N* trials (the ones that ended with
    the highest validation accuracy) using:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行结束时，我们可以使用以下方法获取前*N*个试验（以最高验证准确率结束的试验）：
- en: '[PRE50]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'When we did hyperparameter tuning for the 5-flowers problem, we determined
    that the best set of parameters was:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为5-花问题进行超参数调优时，我们确定最佳参数集如下：
- en: '[PRE51]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The best validation accuracy obtained was 0.46.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的最佳验证准确率为0.46。
- en: Deep Neural Networks
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: The linear model gave us an accuracy of 0.4\. The neural network with one hidden
    layer gave us an accuracy of 0.46\. What if we add more hidden layers?
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型给了我们0.4的准确率。具有一个隐藏层的神经网络给了我们0.46的准确率。如果我们增加更多隐藏层呢？
- en: A *deep neural network* (DNN) is a neural network with more than one hidden
    layer. Each time we add a layer, the number of trainable parameters increases.
    Therefore, we will need a larger dataset. We still have only 3,700 flower images,
    but, as you’ll see, there are a few tricks (namely dropout and batch normalization)
    that we can use to limit the amount of overfitting that happens.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*深度神经网络*（DNN）是一个具有超过一个隐藏层的神经网络。每次增加一层，可训练参数的数量就会增加。因此，我们将需要一个更大的数据集。尽管如此，正如你将看到的，有几个技巧（即dropout和batch
    normalization）可以用来限制过拟合的发生。
- en: Building a DNN
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建一个DNN
- en: 'We can parameterize the creation of a DNN as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将创建DNN的参数化如下：
- en: '[PRE52]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Notice that we are providing readable names for the layers. This shows up when
    we print a summary of the model and is also useful to get a layer by name. For
    example, here is the model where `num_hidden` is [64, 16]:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们为各层提供了可读的名称。这在打印模型摘要时会显示出来，同时也有助于通过名称获取层。例如，这是一个`num_hidden`为[64, 16]的模型：
- en: '[PRE53]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The model, once created, is trained just as before. Unfortunately, as shown
    in [Figure 2-21](#the_loss_and_accuracy_curves_for_a_deep), the resulting validation
    accuracy is worse than what was obtained with either the linear model or the neural
    network.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了模型，就像以前一样进行训练。不幸的是，如[图2-21](#the_loss_and_accuracy_curves_for_a_deep)所示，得到的验证准确率比线性模型或神经网络获得的要差。
- en: '![](Images/pmlc_0221.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0221.png)'
- en: Figure 2-21\. The loss and accuracy curves for a deep neural network with two
    hidden layers.
  id: totrans-282
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-21. 具有两个隐藏层的深度神经网络的损失和准确率曲线。
- en: The 5-flowers dataset is too small for us to take advantage of the additional
    modeling capability provided by the DNN’s extra layer. Recall that we had a similar
    situation when we started with the neural network. Initially, we did not do better
    than the linear model, but by adding regularization and lowering the learning
    rate we were able to get better performance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对于5-花问题来说，数据集太小，我们无法利用DNN额外层提供的额外建模能力。回顾一下，当我们从神经网络开始时，情况类似。最初，我们并没有比线性模型做得更好，但通过添加正则化并降低学习率，我们能够获得更好的性能。
- en: Are there some tricks that we can apply to improve the performance of the DNN?
    Glad you asked! There are two ideas—*dropout layers* and *batch normalization*—that
    are worth trying.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 有没有一些技巧我们可以应用来提高DNN的性能？很高兴你问！有两个想法——*dropout层* 和 *batch normalization*——值得一试。
- en: Dropout
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout
- en: Dropout is one of the oldest regularization techniques in deep learning. At
    each training iteration, the dropout layer drops random neurons from the network,
    with a probability *p* (typically 25% to 50%). In practice, the dropped neurons’
    outputs are set to zero. The net result is that these neurons will not participate
    in the loss computation this time around, and they will not get weight updates
    (see [Figure 2-22](#dropout_layers_are_applied_during_traini)). Different neurons
    will be dropped at each training iteration.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是深度学习中最古老的正则化技术之一。在每次训练迭代中，dropout层以概率*p*（通常为25%到50%）随机丢弃网络中的神经元。实际上，被丢弃的神经元的输出被设置为零。其结果是，这些神经元在本次训练迭代中不会参与损失计算，并且它们不会得到权重更新（见[图2-22](#dropout_layers_are_applied_during_traini)）。不同的神经元会在每次训练迭代中被丢弃。
- en: '![](Images/pmlc_0222.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![批量归一化](Images/pmlc_0222.png)'
- en: Figure 2-22\. Dropout layers are applied during training—here, with a dropout
    rate of 0.4, 40% of the nodes in the layer are randomly dropped at each step of
    training.
  id: totrans-288
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-22。训练期间应用的辍学层——这里使用0.4的辍学率，在每一步训练中随机丢弃层中的40%节点。
- en: 'When testing the performance of the network, all the neurons need to be considered
    (dropout rate=0). Keras does this automatically, so all you have to do is add
    a `tf.keras.layers.Dropout` layer. It will automatically have the correct behavior
    at training and evaluation time: during training, layers are randomly dropped;
    but during evaluation and prediction, no layers are dropped.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试网络性能时，需要考虑所有神经元（辍学率=0）。Keras会自动执行此操作，因此您只需添加一个`tf.keras.layers.Dropout`层。它在训练和评估时会自动具有正确的行为：在训练期间，层会随机丢弃节点；但在评估和预测期间，不会丢弃任何节点。
- en: Tip
  id: totrans-290
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The theory behind dropout is that neural networks have so much freedom between
    their numerous layers that it is entirely possible for one layer to evolve a bad
    behavior and for the next layer to compensate for it. This is not an ideal use
    of neurons. With dropout, there is a high probability that the neurons “fixing”
    the problem will not be there in a given training round. The bad behavior of the
    offending layer therefore becomes obvious, and weights evolve toward a better
    behavior. Dropout also helps spread the information flow throughout the network,
    giving all weights fairly equal amounts of training, which can help keep the model
    balanced.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中辍学的理论是，神经网络在其众多层之间有很大的自由度，因此一个层可能会演变出不良行为，而下一个层则会对其进行补偿。这不是神经元理想的使用方式。通过辍学，有很高的概率在给定的训练轮次中，修复问题的神经元将不存在。因此，问题层的不良行为变得显而易见，权重逐渐朝向更好的行为演变。辍学还有助于在整个网络中传播信息流，使所有权重都能够获得相对均等的训练量，这有助于保持模型的平衡。
- en: Batch normalization
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量归一化
- en: Our input pixel values are in the range [0, 1], and this is compatible with
    the dynamic range of the typical activation functions and optimizers. However,
    once we add a hidden layer, the resulting output values will no longer lie in
    the dynamic range of the activation function for subsequent layers (see [Figure 2-23](#the_output_values_of_hidden_layer_neuron)).
    When this happens, the neuron’s output is zero, and because moving a small amount
    in either direction makes no difference, the gradient is zero. There is no way
    for the network to escape from the dead zone.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入像素值范围为[0, 1]，这与典型激活函数和优化器的动态范围兼容。然而，一旦我们添加了隐藏层，生成的输出值将不再位于激活函数的动态范围内，用于后续层的（见[图2-23](#the_output_values_of_hidden_layer_neuron)）输出值。当这种情况发生时，神经元的输出为零，因为向任何方向移动一小步都不会有差别，梯度为零。网络无法从死区中逃脱。
- en: '![](Images/pmlc_0223.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![批量归一化](Images/pmlc_0223.png)'
- en: Figure 2-23\. The output values of hidden layer neurons may not be in the dynamic
    range of the activation function. They might be (A) too far to the left (after
    sigmoid activation, this neuron almost always outputs zero), (B) too narrow (after
    sigmoid activation, this neuron never outputs a clear 0 or 1), or (C) not too
    bad (after sigmoid activation, this neuron will output a fair range of outputs
    between 0 and 1 across a mini-batch).
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-23。隐藏层神经元的输出值可能不在激活函数的动态范围内。它们可能（A）偏向左侧过多（经sigmoid激活后，该神经元几乎总是输出零），（B）过窄（经sigmoid激活后，该神经元从不清楚地输出0或1），或者（C）还不错（经sigmoid激活后，该神经元会在小批量中输出0到1之间的合理范围）。
- en: 'To fix this, batch normalization normalizes neuron outputs across a training
    batch of data by subtracting the average and dividing by the standard deviation.
    However, doing just that could be swinging the pendulum too far in one direction—with
    a perfectly centered and normally wide distribution everywhere, all neurons would
    have the same behavior. The trick is to introduce two additional learnable parameters
    per neuron, called *scale* and *center*, and to normalize the input data to the
    neuron using these values:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，批量归一化通过减去平均值并除以标准差来标准化训练数据批次中的神经元输出。然而，仅仅这样做可能会在某个方向上过于偏斜——通过引入每个神经元的两个额外可学习参数，称为*缩放*和*中心*，并使用这些值来对神经元的输入数据进行归一化：
- en: <math><mrow><mrow><mi>n</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>z</mi><mi>e</mi><mi>d</mi><mo>=</mo><mfrac><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mrow><mo>(</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>−</mo><mi>c</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mo>)</mo></mrow></mrow></mstyle><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi></mrow></mstyle></mfrac></mrow></mrow></math>
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>n</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>l</mi><mi>i</mi><mi>z</mi><mi>e</mi><mi>d</mi><mo>=</mo><mfrac><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mrow><mo>(</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>−</mo><mi>c</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mo>)</mo></mrow></mrow></mstyle><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi></mrow></mstyle></mfrac></mrow></mrow></math>
- en: 'This way, the network decides, through machine learning, how much centering
    and rescaling to apply at each neuron. In Keras, you can selectively use one or
    the other. For example:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，网络通过机器学习决定在每个神经元上应用多少中心化和重新缩放。在Keras中，你可以选择性地使用其中一个。例如：
- en: '[PRE54]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The problem with batch normalization is that at prediction time you do not have
    training batches over which you can compute the statistics of your neurons’ outputs,
    but you still need those values. Therefore, during training, neurons’ output statistics
    are computed across a “sufficient” number of batches using a running exponential
    average. These stats are then used at inference time.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化的问题在于，在预测时，你没有训练批次来计算神经元输出的统计数据，但你仍然需要这些值。因此，在训练期间，神经元输出的统计数据通过运行指数加权平均值在“足够数量”的批次上计算。这些统计数据随后在推断时使用。
- en: 'The good news is that in Keras you can use a `tf.keras.layers.BatchNormalization`
    layer and all this accounting will happen automatically. When using batch normalization,
    remember that:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，在Keras中，你可以使用`tf.keras.layers.BatchNormalization`层，所有这些计算都会自动完成。在使用批归一化时，请记住：
- en: Batch normalization is performed on the output of a layer before the activation
    function is applied. So, rather than set `activation='relu'` in the `Dense` layer’s
    constructor, we’d omit the activation function there and then add a separate `Activation`
    layer.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用激活函数之前，批归一化在层的输出上执行。因此，不要在`Dense`层的构造函数中设置`activation='relu'`，而是在那里省略激活函数，然后添加一个单独的`Activation`层。
- en: If you use `center=True` in batch norm, you do not need biases in your layer.
    The batch norm offset plays the role of a bias.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在批归一化中使用`center=True`，则你的层不需要偏置。批归一化的偏移量起到了偏置的作用。
- en: If you use an activation function that is scale-invariant (i.e., does not change
    shape if you zoom in on it), then you can set `scale=False`. ReLu is scale-invariant.
    Sigmoid is not.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果使用的激活函数是尺度不变的（即，如果你对其进行放大，形状不会改变），那么你可以设置`scale=False`。ReLU是尺度不变的，而Sigmoid不是。
- en: 'With dropout and batch normalization, the hidden layers now become:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了dropout和批归一化后，隐藏层现在变成了：
- en: '[PRE55]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Note that we have moved the activation out of the `Dense` layer and into a
    separate layer that comes after batch normalization:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已经将激活函数移出了`Dense`层，并放入了一个在批归一化之后的单独层中。
- en: '[PRE56]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The resulting training indicates that these two tricks have improved the ability
    of the model to generalize and to converge faster, as shown in [Figure 2-24](#the_loss_and_accuracy_curves_for-id00001).
    We now get an accuracy of 0.48, as opposed to 0.40 without batch norm and dropout.
    Fundamentally, though, the DNN is not much better than a linear model (0.48 vs.
    0.46), because a dense network is not the correct way to go deeper.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这些技巧已经提高了模型泛化能力并使其收敛速度更快，如[图2-24](#the_loss_and_accuracy_curves_for-id00001)所示。我们现在的准确率为0.48，而没有批归一化和dropout时为0.40。然而，从根本上讲，这个DNN并没有比线性模型更好（0.48对比0.46），因为密集网络不是更深入的正确方式。
- en: '![](Images/pmlc_0224.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0224.png)'
- en: Figure 2-24\. The loss and accuracy curves for a deep neural network with two
    hidden layers with dropout and batch normalization.
  id: totrans-311
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-24。具有两个隐藏层、使用了dropout和批归一化的深度神经网络的损失和准确率曲线。
- en: The curves are not yet well behaved (note the choppiness of the validation curves).
    To smooth them out, we will have to experiment with different values of regularization
    and then do hyperparameter tuning as before. In general, you’ll have to experiment
    with all of these ideas (regularization, early stopping, dropout, batch normalization)
    for any model you pick. In the rest of the book, we’ll simply show the code, but,
    in practice, model creation will always be followed by a period of experimentation
    and hyperparameter tuning.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线尚未表现良好（请注意验证曲线的不稳定性）。为了平滑它们，我们将不得不尝试不同的正则化值，然后像以前一样进行超参数调整。总的来说，你将不得不为你选择的任何模型尝试所有这些想法（正则化、早停、dropout、批归一化）。在本书的其余部分，我们将仅展示代码，但在实践中，模型的创建总是紧随一段实验和超参数调整的时期。
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored how to build a simple data pipeline that reads
    image files and creates 2D floating-point arrays. These arrays were used as inputs
    into fully connected machine learning models. We started with a linear model,
    and then added more `Dense` layers. We discovered that regularization was important
    to limit overfitting, and that changing the learning rate had an impact on learnability.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何构建一个简单的数据流水线，读取图像文件并创建2D浮点数组。这些数组被用作全连接的机器学习模型的输入。我们从一个线性模型开始，然后添加了更多的`Dense`层。我们发现正则化对限制过拟合很重要，改变学习率对可学性有影响。
- en: The models that we built in this chapter did not take advantage of the special
    structure of images, where adjacent pixels are highly correlated. That is what
    we will do in [Chapter 3](ch03.xhtml#image_vision). Nevertheless, the tools that
    we introduced in this chapter for reading images, visualizing them, creating ML
    models, and predicting using ML models will remain applicable even as the models
    themselves become more complex. The techniques that you learned about here—hidden
    layers, changing the learning rate, regularization, early stopping, hyperparameter
    tuning, dropout, and batch normalization—are used in all the models we discuss
    in this book.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中建立的模型并没有利用图像的特殊结构，即相邻像素高度相关。这是我们将在[第三章](ch03.xhtml#image_vision)中做的事情。尽管如此，本章介绍的用于读取图像、可视化、创建机器学习模型和使用机器学习模型进行预测的工具，即使模型本身变得更复杂，仍然适用。你在这里学到的技术——隐藏层、调整学习率、正则化、早停、超参数调整、dropout和批归一化——在本书讨论的所有模型中都有应用。
- en: This chapter introduced a lot of important terminology. For quick reference,
    a short glossary of terms follows.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了许多重要的术语。以下是一个术语简短词汇表供快速参考。
- en: Glossary
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语表
- en: Accuracy
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度
- en: 'An error metric that measures the fraction of correct predictions in a classification
    model: (TP + TN) / (TP + FP + TN + FN) where, for example, TP is true positives.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 一个错误度量，用于衡量分类模型中正确预测的分数：（TP + TN）/（TP + FP + TN + FN），例如，TP是真正例。
- en: Activation function
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数
- en: A function applied to the weighted sum of the inputs to a node in a neural network.
    This is the way that nonlinearity is added to a neural network. Common activation
    functions include ReLU and sigmoid.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于神经网络节点输入加权和的函数。这是向神经网络添加非线性的方法。常见的激活函数包括ReLU和sigmoid。
- en: AUC
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: AUC
- en: Area under the curve of true positive rate plotted against false positive rate.
    The AUC is a threshold-independent error metric.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线下的面积。AUC是一个与阈值无关的错误度量。
- en: Batch or mini-batch
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理或小批量
- en: Training is always performed on batches of training data and labels. Doing so
    helps the algorithm converge. The batch dimension is typically the first dimension
    of data tensors. For example, a tensor of shape [100, 192, 192, 3] contains 100
    images of 192x192 pixels with three values per pixel (RGB).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 训练总是在训练数据和标签的批次上执行。这有助于算法收敛。数据张量的批次维度通常是数据张量的第一维。例如，形状为[100, 192, 192, 3]的张量包含100张192x192像素的图像，每个像素有三个值（RGB）。
- en: Batch normalization
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化
- en: Adding two additional learnable parameters per neuron to normalize the input
    data to the neuron during training.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 添加每个神经元两个额外可学习参数来规范输入数据到神经元在训练期间。
- en: Cross-entropy loss
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: A special loss function often used in classifiers.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 一种在分类器中经常使用的特殊损失函数。
- en: Dense layer
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: Dense层
- en: A layer of neurons where each neuron is connected to all the neurons in the
    previous layer.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经元层，其中每个神经元连接到前一层的所有神经元。
- en: Dropout
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout
- en: A regularization technique in deep learning where, during each training iteration,
    randomly chosen neurons from the network are dropped.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中的一种正则化技术，每次训练迭代中，从网络中随机丢弃选择的神经元。
- en: Early stopping
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止
- en: Stopping a training run when the validation set error starts to get worse.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 当验证集错误开始恶化时停止训练运行。
- en: Epoch
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 纪元
- en: A full pass through the training dataset during training.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 训练期间对整个训练数据集的完整遍历。
- en: Error metric
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 错误度量
- en: The error function comparing neural network outputs to the correct answers.
    The error on the evaluation dataset is what is reported. Common error metrics
    include precision, recall, accuracy, and AUC.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络输出与正确答案进行比较的错误函数。在评估数据集上报告的是错误。常见的错误度量包括精度、召回率、准确度和AUC。
- en: Features
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 特征
- en: A term used to refer to the inputs of a neural network. In modern image models,
    the pixel values form the features.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 用于指代神经网络输入的术语。在现代图像模型中，像素值形成了特征。
- en: Feature engineering
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程
- en: The art of figuring out which parts of a dataset (or combinations of parts)
    to feed into a neural network to get good predictions. In modern image models,
    no feature engineering is required.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 确定将数据集（或其组合）的哪些部分馈送到神经网络以获得良好预测的技艺。在现代图像模型中，不需要进行特征工程。
- en: Flattening
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 展平
- en: Converting a multidimensional tensor to a 1D tensor that contains all the values.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 将多维张量转换为包含所有值的一维张量。
- en: Hyperparameter tuning
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优
- en: An “outer” optimization loop where multiple models with different values of
    model hyperparameters (like learning rate and number of nodes) are trained, and
    the best of these models chosen. In the “inner” optimization loop, which we call
    the *training loop*, the model’s parameters (weights and biases) are optimized.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 一个“外部”优化循环，在此循环中，使用不同的模型超参数值（如学习率和节点数量）训练多个模型，并选择最佳模型。在我们称之为“内部”优化循环或*训练循环*中，优化模型的参数（权重和偏差）。
- en: Labels
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 标签
- en: Another name for “classes,” or correct answers in a supervised classification
    problem.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: “类别”或监督分类问题中的正确答案的另一个名称。
- en: Learning rate
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率
- en: The fraction of the gradient by which weights and biases are updated at each
    iteration of the training loop.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环的每次迭代中，权重和偏差更新的梯度分数。
- en: Logits
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: Logits（逻辑值）
- en: The outputs of a layer of neurons before the activation function is applied.
    The term comes from the *logistic function*, a.k.a. the *sigmoid function*, which
    used to be the most popular activation function. “Neuron outputs before logistic
    function” was shortened to “logits.”
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 应用激活函数之前神经元层的输出。该术语来自于*逻辑函数*，又名*Sigmoid函数*，它曾是最流行的激活函数。“逻辑函数之前的神经元输出”被简化为“logits”。
- en: Loss
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 损失
- en: The error function comparing neural network outputs to the correct answers.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络输出与正确答案进行比较的错误函数。
- en: Neuron
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元
- en: The most basic unit of a neural network, which computes the weighted sum of
    its inputs, adds a bias, and feeds the result through an activation function.
    The loss on the training dataset is what is minimized during training.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的最基本单元，计算其输入的加权和，加上偏差，并通过激活函数输出结果。在训练过程中，会最小化训练数据集上的损失。
- en: One-hot encoding
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码
- en: 'A representation of categorical values as binary vectors. For example, class
    3 out of 5 is encoded as a vector of five elements, which are all 0s except the
    third one, which is a 1: [0 0 1 0 0].'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 将分类值表示为二进制向量的方法。例如，5个类别中的第3类被编码为包含五个元素的向量，其中除了第三个元素为1外，其他元素均为0：[0 0 1 0 0]。
- en: Precision
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 精度
- en: 'An error metric that measures the fraction of true positives in the set of
    identified positives: TP / (TP + FP).'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 一个衡量真正例在被识别的正例集中所占比例的错误度量：TP / (TP + FP)。
- en: Recall
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率
- en: 'An error metric that measures the fraction of true positives identified among
    all the positives in the dataset: TP / (TP + FN).'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 一个衡量在数据集中所有正例中真正例所占比例的错误度量：TP / (TP + FN)。
- en: Regularization
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化
- en: A penalty imposed on weights or model function during training to limit the
    amount of overfitting. *L1 regularization* drives many of the weight values to
    zero but is more tolerant of individual large weight values than *L2 regularization*,
    which tends to drive all the weights to small but nonzero values.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中对权重或模型函数施加的惩罚，以限制过拟合的数量。*L1正则化*会将许多权重值推向零，但对个别大权重值更宽容，而*L2正则化*倾向于将所有权重推向小但非零的值。
- en: ReLU
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU（整流线性单元）
- en: Rectified linear unit. A popular activation function for neurons.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 整流线性单元。神经元的流行激活函数之一。
- en: Sigmoid
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid（S形函数）
- en: An activation function that acts on an unbounded scalar and converts it into
    a value that lies between [0,1]. It is used as the last step of a binary classifier.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 一种作用于无界标量并将其转换为位于 [0,1] 区间内值的激活函数。用作二元分类器的最后一步。
- en: Softmax
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax
- en: A special activation function that acts on a vector. It increases the difference
    between the largest component and all others, and also normalizes the vector to
    have a sum of 1 so that it can be interpreted as a vector of probabilities. Used
    as the last step in multiclass classifiers.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特殊的激活函数，作用于向量上。它增加了最大分量与所有其他分量之间的差异，并且将向量归一化为和为1，以便可以解释为概率向量。用作多类分类器中的最后一步。
- en: Tensor
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor
- en: A tensor is like a matrix but with an arbitrary number of dimensions. A 1D tensor
    is a vector, a 2D tensor is a matrix, and you can have tensors with three, four,
    five or more dimensions. In this book, we will use the term tensor to refer to
    the numerical type that supports GPU-accelerated TensorFlow operations.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 张量类似于矩阵，但具有任意数量的维度。1D 张量是向量，2D 张量是矩阵，您可以有三、四、五或更多维的张量。在本书中，我们将使用张量一词来指代支持 GPU
    加速 TensorFlow 操作的数值类型。
- en: Training
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 训练
- en: Optimizing the parameters of a machine learning model to attain lower loss on
    a training dataset.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 优化机器学习模型的参数以在训练数据集上实现更低的损失。
- en: ^([1](ch02.xhtml#ch02fn01-marker)) A good non-mathematical explanation of this
    phenomenon can be found at [DataCamp.com](https://oreil.ly/N2qH5).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.xhtml#ch02fn01-marker)) 这种现象的良好非数学解释可以在[DataCamp.com](https://oreil.ly/N2qH5)找到。
