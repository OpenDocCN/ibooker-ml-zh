- en: Chapter 20\. Tensors with PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第20章 PyTorch 中的张量
- en: 20.0 Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.0 简介
- en: Just as NumPy is a foundational tool for data manipulation in the machine learning
    stack, PyTorch is a foundational tool for working with tensors in the deep learning
    stack. Before moving on to deep learning itself, we should familiarize ourselves
    with PyTorch tensors and create many operations analogous to those performed with
    NumPy in [Chapter 1](ch01.xhtml#vectors-matrices-and-arrays).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 NumPy 是机器学习堆栈中数据操作的基础工具一样，PyTorch 是深度学习堆栈中处理张量的基础工具。在深入学习之前，我们应该熟悉 PyTorch
    张量，并创建许多与 NumPy 中执行的操作类似的操作（见 [第1章](ch01.xhtml#vectors-matrices-and-arrays)）。
- en: Although PyTorch is just one of multiple deep learning libraries, it is significantly
    popular both within academia and industry. PyTorch tensors are *very* similar
    to NumPy arrays. However, they also allow us to perform tensor operations on GPUs
    (hardware specialized for deep learning). In this chapter, we’ll familiarize ourselves
    with the basics of PyTorch tensors and many common low-level operations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 PyTorch 只是多个深度学习库之一，但在学术界和工业界都非常流行。PyTorch 张量与 NumPy 数组非常相似。然而，它们还允许我们在 GPU（专门用于深度学习的硬件）上执行张量操作。在本章中，我们将熟悉
    PyTorch 张量的基础知识和许多常见的低级操作。
- en: 20.1 Creating a Tensor
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.1 创建张量
- en: Problem
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to create a tensor.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要创建一个张量。
- en: Solution
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use Pytorch to create a tensor:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 创建张量：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Discussion
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The main data structure within PyTorch is a tensor, and in many ways tensors
    are exactly like the multidimensional NumPy arrays used in [Chapter 1](ch01.xhtml#vectors-matrices-and-arrays).
    Just like vectors and arrays, these tensors can be represented horizontally (i.e.,
    rows) or vertically (i.e., columns).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 中的主要数据结构是张量，在许多方面，张量与多维 NumPy 数组（见 [第1章](ch01.xhtml#vectors-matrices-and-arrays)）完全相同。就像向量和数组一样，这些张量可以水平（即行）或垂直（即列）表示。
- en: See Also
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[PyTorch documentation: Tensors](https://oreil.ly/utaTD)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 文档：张量](https://oreil.ly/utaTD)'
- en: 20.2 Creating a Tensor from NumPy
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.2 从 NumPy 创建张量
- en: Problem
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to create PyTorch tensors from NumPy arrays.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要从 NumPy 数组创建 PyTorch 张量。
- en: Solution
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the PyTorch `from_numpy` function:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 的 `from_numpy` 函数：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Discussion
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: As we can see, PyTorch is very similar to NumPy syntactically. In addition,
    it easily allows us to convert NumPy arrays to PyTorch tensors that we can use
    on GPUs and other accelerated hardware. At the time of writing, NumPy is mentioned
    frequently in the PyTorch documentation, and PyTorch itself even offers a way
    that PyTorch tensors and NumPy arrays can share the same memory to reduce overhead.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，PyTorch 在语法上与 NumPy 非常相似。此外，它还允许我们轻松地将 NumPy 数组转换为可以在 GPU 和其他加速硬件上使用的
    PyTorch 张量。在撰写本文时，PyTorch 文档中频繁提到 NumPy，并且 PyTorch 本身甚至提供了一种使 PyTorch 张量和 NumPy
    数组可以共享内存以减少开销的方式。
- en: See Also
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[PyTorch documentation: Bridge with NumPy](https://oreil.ly/zEJo6)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 文档：与 NumPy 的桥接](https://oreil.ly/zEJo6)'
- en: 20.3 Creating a Sparse Tensor
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.3 创建稀疏张量
- en: Problem
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Given data with very few nonzero values, you want to efficiently represent it
    with a tensor.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据，其中非零值非常少，您希望以张量的方式高效表示它。
- en: Solution
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the PyTorch `to_sparse` function:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 的 `to_sparse` 函数：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Discussion
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Sparse tensors are memory-efficient ways to represent data composed of mostly
    0s. In [Chapter 1](ch01.xhtml#vectors-matrices-and-arrays) we used `scipy` to
    create a compressed sparse row (CSR) matrix that was no longer a NumPy array.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏张量是表示由大多数 0 组成的数据的内存高效方法。在 [第1章](ch01.xhtml#vectors-matrices-and-arrays) 中，我们使用
    `scipy` 创建了一个压缩稀疏行（CSR）矩阵，它不再是 NumPy 数组。
- en: 'The `torch.Tensor` class allows us to create both regular and sparse matrices
    using the same object. If we inspect the types of the two tensors we just created,
    we can see they’re actually both of the same class:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Tensor` 类允许我们使用同一个对象创建常规矩阵和稀疏矩阵。如果我们检查刚刚创建的两个张量的类型，我们可以看到它们实际上都属于同一类：'
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: See Also
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[PyTorch documentation: Sparse Tensor](https://oreil.ly/8J3IO)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 文档：稀疏张量](https://oreil.ly/8J3IO)'
- en: 20.4 Selecting Elements in a Tensor
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.4 在张量中选择元素
- en: Problem
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: We need to select specific elements of a tensor.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要选择张量的特定元素。
- en: Solution
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use NumPy-like indexing and slicing to return elements:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类似于 NumPy 的索引和切片返回元素：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Discussion
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Like NumPy arrays and most everything in Python, PyTorch tensors are zero-indexed.
    Both indexing and slicing are supported as well. One key difference is that indexing
    a PyTorch tensor to return a single element still returns a tensor as opposed
    to the value of the object itself (which would be in the form of an integer or
    float). Slicing syntax also has parity with NumPy and will return objects of type
    tensor in PyTorch:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 像 NumPy 数组和 Python 中的大多数内容一样，PyTorch 张量也是从零开始索引的。索引和切片也都受支持。一个关键区别是，对 PyTorch
    张量进行索引以返回单个元素仍然会返回一个张量，而不是对象本身的值（该值将是整数或浮点数）。切片语法也与 NumPy 相同，并将以张量对象的形式返回：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'One key difference is that PyTorch tensors do not yet support negative steps
    when slicing. Therefore, attempting to reverse a tensor using slicing yields an
    error:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键区别是，PyTorch 张量在切片时不支持负步长。因此，尝试使用切片反转张量会产生错误：
- en: '[PRE21]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Instead, if we wish to reverse a tensor we can use the `flip` method:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果我们希望反转张量，我们可以使用 `flip` 方法：
- en: '[PRE23]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: See Also
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[PyTorch documentation: Operations on Tensors](https://oreil.ly/8-xj7)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 文档：张量操作](https://oreil.ly/8-xj7)'
- en: 20.5 Describing a Tensor
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.5 描述张量
- en: Problem
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to describe the shape, data type, and format of a tensor along with
    the hardware it’s using.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您想描述张量的形状、数据类型和格式以及它所使用的硬件。
- en: Solution
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Inpect the `shape`, `dtype`, `layout`, and `device` attributes of the tensor:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 检查张量的 `shape`、`dtype`、`layout` 和 `device` 属性：
- en: '[PRE25]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Discussion
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'PyTorch tensors provide a number of helpful attributes for gathering information
    about a given tensor, including:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 张量提供了许多有用的属性，用于收集关于给定张量的信息，包括：
- en: Shape
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 形状
- en: Returns the dimensions of the tensor
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 返回张量的维度
- en: Dtype
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Dtype
- en: Returns the data type of objects within the tensor
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 返回张量中对象的数据类型
- en: Layout
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 布局
- en: Returns the memory layout (most common is `strided` used for dense tensors)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 返回内存布局（最常见的是用于稠密张量的 `strided`）
- en: Device
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 设备
- en: Returns the hardware the tensor is being stored on (CPU/GPU)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 返回张量存储的硬件（CPU/GPU）
- en: Again, the key differentiator between tensors and arrays is an attribute like
    *device*, because tensors provide us with hardware-accelerated options like GPUs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，张量与数组的主要区别在于 *设备* 这样的属性，因为张量为我们提供了像 GPU 这样的硬件加速选项。
- en: 20.6 Applying Operations to Elements
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.6 对元素应用操作
- en: Problem
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to apply an operation to all elements in a tensor.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 您想对张量中的所有元素应用操作。
- en: Solution
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Take advantage of *broadcasting* with PyTorch:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 PyTorch 进行 *广播*：
- en: '[PRE33]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Discussion
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Basic operations in PyTorch will take advantage of broadcasting to parallelize
    them using accelerated hardware such as GPUs. This is true for supported mathematical
    operators in Python (+, -, ×, /) and other functions inherent to PyTorch. Unlike
    NumPy, PyTorch doesn’t include a `vectorize` method for applying a function over
    all elements in a tensor. However, PyTorch comes equipped with all of the mathematical
    tools necessary to distribute and accelerate the usual operations required for
    deep learning workflows.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 中的基本操作将利用广播并行化，使用像 GPU 这样的加速硬件。这对于 Python 中支持的数学运算符（+、-、×、/）和 PyTorch
    内置函数是真实的。与 NumPy 不同，PyTorch 不包括用于在张量上应用函数的 `vectorize` 方法。然而，PyTorch 配备了所有必要的数学工具，以分发和加速深度学习工作流程中所需的常规操作。
- en: See Also
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[PyTorch documentation: Broadcasting Semantics](https://oreil.ly/NsPpa)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 文档：广播语义](https://oreil.ly/NsPpa)'
- en: '[Vectorization and Broadcasting with PyTorch](https://oreil.ly/dfzIJ)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 中的向量化和广播](https://oreil.ly/dfzIJ)'
- en: 20.7 Finding the Maximum and Minimum Values
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.7 查找最大值和最小值
- en: Problem
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to find the maximum or minimum value in a tensor.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在张量中找到最大值或最小值。
- en: Solution
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the PyTorch `max` and `min` methods:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 的 `max` 和 `min` 方法：
- en: '[PRE35]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Discussion
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'The `max` and `min` methods of a tensor help us find the largest or smallest
    values in that tensor. These methods work the same across multidimensional tensors
    as well:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的 `max` 和 `min` 方法帮助我们找到该张量中的最大值或最小值。这些方法在多维张量上同样适用：
- en: '[PRE39]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 20.8 Reshaping Tensors
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.8 改变张量的形状
- en: Problem
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You want to change the shape (number of rows and columns) of a tensor without
    changing the element values.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望改变张量的形状（行数和列数）而不改变元素值。
- en: Solution
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the PyTorch `reshape` method:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 的 `reshape` 方法：
- en: '[PRE41]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Discussion
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Manipulating the shape of a tensor can be common in the field of deep learning,
    as neurons in a neural network often require tensors of a very specific shape.
    Since the required shape of a tensor can change between neurons in a given neural
    network, it is good to have a low-level understanding of our inputs and outputs
    in deep learning.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习领域中，操作张量的形状可能很常见，因为神经网络中的神经元通常需要具有非常特定形状的张量。由于给定神经网络中的神经元之间所需的张量形状可能会发生变化，因此了解深度学习中输入和输出的低级细节是很有好处的。
- en: 20.9 Transposing a Tensor
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.9 转置张量
- en: Problem
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to transpose a tensor.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要转置一个张量。
- en: Solution
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the `mT` method:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `mT` 方法：
- en: '[PRE43]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Discussion
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Transposing with PyTorch is slightly different from NumPy. The `T` method used
    for NumPy arrays is supported in PyTorch only with tensors of two dimensions and
    at the time of writing is deprecated for tensors of other shapes. The `mT` method
    used to transpose batches of tensors is preferred, as it scales to greater than
    two dimensions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch 进行转置与 NumPy 略有不同。用于 NumPy 数组的 `T` 方法仅支持二维张量，在 PyTorch 中对于其他形状的张量时，该方法在写作时已被弃用。用于转置批处理张量的
    `mT` 方法更受欢迎，因为它适用于超过两个维度的张量。
- en: 'An additional way to transpose PyTorch tensors of any shape is to use the `permute`
    method:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用 `permute` 方法之外，还可以使用 PyTorch 中的另一种方式来转置任意形状的张量：
- en: '[PRE45]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This method also works for one-dimensional tensors (for which the value of the
    tranposed tensor is the same as the original tensor).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法也适用于一维张量（其中转置张量的值与原始张量相同）。
- en: 20.10 Flattening a Tensor
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.10 张量展平
- en: Problem
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to transform a tensor into one dimension.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要将张量转换为一维。
- en: Solution
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the `flatten` method:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `flatten` 方法：
- en: '[PRE47]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Discussion
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Flattening a tensor is a useful technique for reducing a multidimensional tensor
    into one dimension.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 张量展平是将多维张量降维为一维的一种有用技术。
- en: 20.11 Calculating Dot Products
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.11 计算点积
- en: Problem
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to calculate the dot product of two tensors.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要计算两个张量的点积。
- en: Solution
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use the `dot` method:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `dot` 方法：
- en: '[PRE49]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Discussion
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Calculating the dot product of two tensors is a common operation useful in the
    deep learning space as well as the information retrieval space. You may remember
    earlier in the book where we used the dot product of two vectors to perform a
    cosine similarity-based search. Doing this in PyTorch on GPU (instead of with
    NumPy or scikit-learn on CPU) can yield impressive performance benefits on information
    retrieval problems.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 计算两个张量的点积是深度学习空间以及信息检索空间中常用的操作。您可能还记得本书中我们使用两个向量的点积执行基于余弦相似度的搜索。在 PyTorch 上使用
    GPU（而不是在 CPU 上使用 NumPy 或 scikit-learn）执行此操作可以在信息检索问题上获得显著的性能优势。
- en: See Also
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Vectorization and Broadcasting with PyTorch](https://oreil.ly/lIjtB)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 进行向量化和广播](https://oreil.ly/lIjtB)'
- en: 20.12 Multiplying Tensors
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20.12 乘法张量
- en: Problem
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to multiply two tensors.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要将两个张量相乘。
- en: Solution
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Use basic Python arithmetic operators:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基本的 Python 算术运算符：
- en: '[PRE51]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Discussion
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: PyTorch supports basic arithmetic operators such as ×, +, - and /. Although
    multiplying tensors is probably one of the most common operations used in deep
    learning, it’s useful to know tensors can also be added, subtracted, and divided.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 支持基本算术运算符，如 ×、+、- 和 /。虽然在深度学习中，张量乘法可能是最常用的操作之一，但了解张量也可以进行加法、减法和除法是很有用的。
- en: 'Add one tensor to another:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个张量加到另一个张量中：
- en: '[PRE53]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Subtract one tensor from another:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个张量中减去另一个张量：
- en: '[PRE55]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Divide one tensor by another:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个张量除以另一个张量：
- en: '[PRE57]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
