- en: Chapter 5\. Architecting a Data Lake
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章 架构设计数据湖
- en: A data lake is the part of the data platform that captures raw, ungoverned data
    from across an organization and supports compute tools from the Apache ecosystem.
    In this chapter, we will go into more detail about this concept, which is important
    when designing modern data platforms. The cloud can provide a boost to the different
    use cases that can be implemented on top of it, as you will read throughout the
    chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是数据平台的一部分，从整个组织中捕获原始、未经管理的数据，并支持Apache生态系统中的计算工具。在本章中，我们将详细讨论这个概念，这在设计现代数据平台时非常重要。云计算可以为其上可实现的各种用例提供支持，这一点将贯穿本章始终。
- en: We will start with a recap of why you might want to store raw, ungoverned data
    that only supports basic compute. Then, we discuss architecture design and implementation
    details in the cloud. Even though data lakes were originally intended only for
    basic data processing, it is now possible to democratize data access and reporting
    using just a data lake—because of integrations with other solutions through APIs
    and connectors, the data within a data lake can be made much more fit for purpose.
    We will finally take a bird’s-eye perspective on a very common way to speed up
    analysis and experimentation with data within an organization by leveraging data
    science notebooks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从为何希望存储仅支持基本计算的原始、未经管理数据开始进行回顾。然后，我们讨论云中的架构设计和实施细节。尽管最初数据湖仅用于基本数据处理，现在可以通过API和连接器与其他解决方案集成，仅使用数据湖就可以民主化数据访问和报告——因为数据湖中的数据可以更适合特定目的。最后，我们将从鸟瞰视角看待通过利用数据科学笔记本在组织内加速分析和实验的一种非常普遍的方法。
- en: Data Lake and the Cloud—A Perfect Marriage
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖与云——完美的结合
- en: Data helps organizations make better decisions, faster. It’s the center of everything
    from applications to security, and more data means more need for processing power,
    which cloud solutions can provide.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据帮助组织更快做出更好的决策。它是一切应用程序和安全性的中心，更多的数据意味着需要更多的处理能力，而云解决方案可以提供这种能力。
- en: Challenges with On-Premises Data Lakes
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地数据湖的挑战
- en: Organizations need a place to store all types of data, including unstructured
    data (images, video, text, logs, binary files, web content). This was the main
    reason that enterprises adopted data lakes. Initially, enterprises believed that
    data lakes were just pure raw storage.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 组织需要一个地方来存储各种类型的数据，包括非结构化数据（图片、视频、文本、日志、二进制文件、网页内容）。这是企业采用数据湖的主要原因。最初，企业认为数据湖只是纯粹的原始存储。
- en: Business units wanted to extract insights and value from the data stored by
    IT departments, rather than simply storing it. Thanks to the evolution of the
    Hadoop ecosystem, data lakes enabled organizations capable of doing big data analytics
    to go beyond the mere concept of storage offload. Data lakes brought advanced
    analytics and ML capabilities within reach. Hadoop and related technologies jump-started
    a massive adoption of data lakes in the 2010s.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 业务部门希望从IT部门存储的数据中提取洞察和价值，而不仅仅是存储数据。多亏了Hadoop生态系统的演进，数据湖使得能够进行大数据分析的组织能够超越存储卸载的概念。数据湖带来了先进的分析和机器学习能力。在2010年代，Hadoop及相关技术推动了数据湖的大规模采用。
- en: However, companies have struggled to get a sufficient return on investment from
    their data lake efforts because of drawbacks around TCO, scalability, governance,
    and agility. The resource utilization and overall cost of managing an on-premises
    data lake can become unmanageable. Resource-intensive data and analytics processing
    often leads to missed SLAs. Data governance and security issues can lead to compliance
    concerns. Analytics experimentation is slowed due to the time required to provision
    resources.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，公司在获取数据湖成效方面面临着挑战，因为在总拥有成本（TCO）、可伸缩性、治理和敏捷性等方面存在缺陷。管理本地数据湖的资源利用率和总体成本可能变得难以管理。资源密集型的数据和分析处理经常导致未能达到服务级别协议。数据治理和安全问题可能引发合规性问题。由于资源配置所需的时间，分析实验速度变慢。
- en: With estimates that by 2025, [80% of organizations’ data will be unstructured](https://oreil.ly/KxCbZ),
    the on-premises world is no longer able to provide adequate environments at an
    affordable price. Cloud solutions, as you saw in [Chapter 2](ch02.html#strategic_steps_to_innovate_with_data),
    allow organizations to first reduce the TCO and then build a platform for innovation
    because people within the company can focus on business value instead of hardware
    management.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 据估计，到 2025 年，[80% 的组织数据将为非结构化数据](https://oreil.ly/KxCbZ)，本地环境已不再能够以可承受的价格提供充足的环境。如您在[第二章](ch02.html#strategic_steps_to_innovate_with_data)中所见，云解决方案使组织能够首先降低
    TCO，然后构建创新平台，因为公司内部的人员可以专注于业务价值而不是硬件管理。
- en: Benefits of Cloud Data Lakes
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云数据湖的好处
- en: 'The cloud paradigm is hugely beneficial for data lakes because:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算范式对数据湖非常有益，因为：
- en: It is not necessary to store all data in an expensive, always-on Hadoop Distributed
    File System (HDFS) cluster. Object storage solutions (e.g., AWS S3, Azure Blob
    Storage, or Google Cloud Storage) are fully managed, infinitely scalable, and
    a fraction of the cost.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不必将所有数据存储在昂贵的、始终开启的 Hadoop 分布式文件系统（HDFS）集群中。对象存储解决方案（例如 AWS S3、Azure Blob 存储或
    Google Cloud Storage）是完全托管的、无限可扩展的，成本仅为传统方案的一小部分。
- en: Hadoop clusters, which provide not only storage capabilities but even processing
    compute power, can be created on demand in a short amount of time (a few minutes
    or seconds), generating an immediate cost saving due to the fact that they do
    not need to be always on. These Hadoop clusters can read/write data directly from
    object storage—even though such data access is slower than read⁠ing/​writing to
    HDFS, the cost savings of having ephemeral clusters can make the overall trade-off
    worthwhile.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 集群不仅提供存储能力，还能够在短时间内按需创建处理计算能力（几分钟或几秒钟），由于无需始终开启，这带来了立即的成本节约。这些 Hadoop
    集群可以直接从对象存储读取/写入数据，尽管这种数据访问速度比读取/写入 HDFS 要慢，但通过使用临时集群带来的成本节约可能使整体折衷更为合理。
- en: Hyperscalers usually offer capabilities to leverage less-expensive virtual machines
    (VMs), called *spot instances* or *preemptible instances*, for worker nodes. These
    VMs have the drawback that they can be evicted by the system at any time (generally
    with a 30-to-60-second notice), but they can easily be substituted with new ones
    thanks to the underlying Hadoop technology that is worker-node-failure tolerant.
    Thanks to this approach, additional costs can be saved.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超大规模云服务提供商通常提供利用更便宜的虚拟机（称为*竞价实例*或*抢先实例*）作为工作节点的能力。这些虚拟机的缺点是它们可能随时被系统驱逐（通常提前
    30 到 60 秒通知），但由于底层的 Hadoop 技术具有工作节点故障容忍性，因此可以轻松地用新的替换。这种方法带来了额外的成本节约。
- en: Nowadays, the majority of the Hadoop services available on hyperscalers are
    fully managed and offered in PaaS mode (even though you can build your own Hadoop
    cluster leveraging a pure IaaS service). PaaS means that you do not need to manage
    by yourself all the VMs needed for master and worker nodes but that you can instead
    focus on building processing to extract value from the data.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，超大规模云服务提供商上提供的大多数 Hadoop 服务都是完全托管的，并以平台即服务（PaaS）模式提供（尽管您可以通过纯 IaaS 服务构建自己的
    Hadoop 集群）。PaaS 意味着您无需自行管理所有需要的主节点和工作节点的虚拟机，而是可以专注于构建处理流程以从数据中提取价值。
- en: On-premises Hadoop clusters tend to generate data silos within the organization
    because of the fact that the HDFS pools were not designed to be prone to data
    sharing. The fact that instead in the cloud we can have an effective separation
    between storage (i.e., data in object storage that can be injected in any HDFS
    cluster) and compute (i.e., VMs created on demand) gives organizations better
    flexibility in handling the challenges in data governance.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地的 Hadoop 集群往往在组织内部产生数据孤岛，因为 HDFS 池并未设计为易于数据共享。云计算中，存储（例如，任意 HDFS 集群中可注入的对象存储中的数据）和计算（例如，按需创建的虚拟机）能够有效分离，使组织在处理数据治理挑战时拥有更大的灵活性。
- en: 'Cloud is the perfect habitat for data lakes because it addresses all the challenges
    with on-premises data lakes: TCO, scalability, elasticity, and consistent data
    governance.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算是数据湖的理想栖息地，因为它解决了所有本地数据湖的挑战：总体成本拥有（TCO）、可伸缩性、弹性和一致的数据治理。
- en: The market is strongly investing in data lakes and especially in the cloud-based
    ones. Amazon EMR, Azure Data Lake, and Google Cloud Dataproc are available on
    the hyperscalers. Databricks (the developer behind the open source Spark engine
    that is part of all major Hadoop distributions) has built a complete multicloud
    data platform that offers not only storage and compute but also a full set of
    features to handle the entire data lifecycle.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 市场强烈投资于数据湖，特别是基于云的数据湖。亚马逊EMR、Azure数据湖和谷歌云Dataproc可在超大规模计算机上使用。Databricks（开源Spark引擎的开发者，是所有主要Hadoop分发的一部分）构建了一个完整的多云数据平台，不仅提供存储和计算，还提供处理整个数据生命周期所需的全部功能。
- en: Next, let’s look at the design and implementation details of a data lake on
    the cloud.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看云上数据湖的设计与实施细节。
- en: Design and Implementation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计与实施
- en: The design of a data lake depends on whether you need streaming, how you will
    do data governance, what Hadoop capabilities you use, and which hyperscaler you
    are building on. Let’s take these one by one.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖的设计取决于是否需要流处理，如何进行数据治理，使用哪些Hadoop功能，以及构建在哪个超大规模计算机上。让我们一一讨论这些问题。
- en: Batch and Stream
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理和流处理
- en: 'When analyzing data workloads, the first question to answer is the age of the
    data to be processed: is it data that has been stored for some time, or is it
    data that has just arrived in the system? Based on the answer, choose between
    the two main approaches to data processing: batch and streaming. Batch processing
    has been the dominant approach for over 20 years, but streaming has become increasingly
    popular in recent years, especially with the advent of the cloud. Streaming processing
    is better suited for handling large amounts of data in real time, while batch
    processing is better for processing large amounts of data offline.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析数据工作负载时，首要问题是要回答要处理的数据的年龄：它是存储已久的数据，还是刚刚到达系统的数据？根据答案，选择数据处理的两种主要方法之一：批处理和流处理。批处理在过去20年中一直是主流方法，但随着云计算的出现，流处理在近年来日益流行。流处理更适合实时处理大量数据，而批处理更适合离线处理大量数据。
- en: 'Whether batch or stream, there are four storage areas in a data lake:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是批处理还是流处理，在数据湖中有四个存储区域：
- en: Raw/Landing/Bronze
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 原始/落地/青铜
- en: Where raw data is collected and ingested directly from source systems.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据从源系统直接收集和摄取。
- en: Staging/Dev/Silver
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 暂存/开发/银
- en: Where more advanced users (e.g., data engineers, data scientists) process the
    data to prepare it for final users.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 更高级用户（如数据工程师、数据科学家）处理数据，为最终用户准备数据。
- en: Production/Gold
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 生产/黄金
- en: Where the final data used by production systems is stored.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生产系统使用的最终数据存储位置。
- en: Sensitive (optional)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感（可选）
- en: Where sensitive data resides. It is connected to all other stages and facilitates
    data access governance to ensure compliance with company and government regulations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感数据所在之处。它连接到所有其他阶段，并促进数据访问治理，以确保符合公司和政府规定。
- en: 'In 2014, two new architectures were proposed to allow both batch and stream
    processing at scale: Lambda (by Nathan Marz) and Kappa (by Jay Kreps). The Lambda
    architecture (shown in [Figure 5-1](#lambda_architecture)) uses separate technology
    stacks for the *batch layer* that spans all the (historical) facts and the *speed
    layer* for the real-time data.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，提出了两种新的架构，以允许规模化的批处理和流处理：Lambda（由Nathan Marz提出）和Kappa（由Jay Kreps提出）。Lambda架构（见[图5-1](#lambda_architecture)）使用独立的技术堆栈，用于*批处理层*（涵盖所有（历史）事实）和用于实时数据的*速度层*。
- en: '![Lambda architecture](assets/adml_0501.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Lambda架构](assets/adml_0501.png)'
- en: Figure 5-1\. Lambda architecture
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. Lambda架构
- en: New data ingested into the system is stored both in a persistent dataset (*batch
    layer*) and in a volatile cache (*speed layer)*. The first one is then indexed
    and made available to the serving layer for the *batch views* while the second
    is exposed by the speed layer via *real-time views.* Both datasets (persistent
    and volatile) can be queried in parallel or disjointly to answer the needed question.
    This architecture is generally deployed in a Hadoop environment where HDFS can
    be leveraged as a batch layer while technologies like Spark, Storm, or Beam can
    be used for the speed layer. Hives can then finally be the solution for the implementation
    of the serving layer, for example.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 新数据进入系统后，同时存储在持久数据集（*批处理层*）和易失缓存（*速度层*）。第一个随后被索引，并可供*批处理视图*的服务层使用，而第二个通过速度层提供*实时视图*。这两个数据集（持久和易失）可以并行或不交叉查询以回答所需问题。这种架构通常部署在
    Hadoop 环境中，其中可以利用 HDFS 作为批处理层，而像 Spark、Storm 或 Beam 这样的技术可以用于速度层。例如，Hive 最终可以成为服务层实现的解决方案。
- en: In the Kappa architecture (shown in [Figure 5-2](#kappa_architecture)), you
    can perform both real-time and batch processing with a single technology stack
    (e.g., Beam/Spark). The core is the streaming architecture. First, the event streaming
    platform stores the incoming data. From there, a stream processing engine processes
    the data in real time, making data available for real-time views. At this stage,
    the data can be persisted into a database to perform batch analysis when needed
    and leveraging the same technology stack.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kappa 架构（显示在 [图 5-2](#kappa_architecture) 中），您可以使用单一技术堆栈（例如，Beam/Spark）同时进行实时和批处理。核心是流式架构。首先，事件流平台存储传入数据。从那里，流处理引擎实时处理数据，使数据可用于实时视图。在此阶段，数据可以持久化到数据库，以便在需要时执行批量分析并利用相同的技术堆栈。
- en: '![Kappa architecture](assets/adml_0502.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![Kappa 架构](assets/adml_0502.png)'
- en: Figure 5-2\. Kappa architecture
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. Kappa 架构
- en: Data Catalog
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据目录
- en: 'Data is scattered across multiple sites, including databases, DWHs, filesystems,
    and blob storage. In addition, data could be stored in different areas of the
    data lake: raw, staging, production, or sensitive. This makes it difficult for
    data scientists to find the data they need and for business users to access the
    latest data. We need a solution to make all of these data sources discoverable
    and accessible.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分散在多个站点，包括数据库、数据仓库、文件系统和 Blob 存储。此外，数据可能存储在数据湖的不同区域：原始、暂存、生产或敏感。这使得数据科学家难以找到所需的数据，业务用户难以访问最新数据。我们需要一个解决方案来使所有这些数据源可发现和可访问。
- en: A data catalog is a repository of all the metadata that describes the datasets
    of the organization, and it will help data scientists, data engineers, business
    analysts, and other users to find a path toward the desired dataset. In [Figure 5-3](#the_data_catalog_supports_data_processi),
    you can see a possible high-level architecture that describes how the data catalog
    can be connected to the data lakes and the other solutions of the data platform.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据目录是描述组织数据集所有元数据的存储库，将帮助数据科学家、数据工程师、业务分析师及其他用户找到通向所需数据集的路径。在 [图 5-3](#the_data_catalog_supports_data_processi)
    中，您可以看到一个可能的高级架构，描述了数据目录如何连接数据湖和数据平台的其他解决方案。
- en: '![The data catalog supports data processing](assets/adml_0503.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![数据目录支持数据处理](assets/adml_0503.png)'
- en: Figure 5-3\. The data catalog supports data processing
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 数据目录支持数据处理
- en: If the data lives across several data repositories (one of them being the data
    lake) but the processing engine and the Gold repository for the analytical workloads
    are within the data lake, ensure that the data catalog is comprehensive and that
    the data is not duplicated. Make sure the metadata contains information about
    the level of the dataset (e.g., master or replica).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据分布在多个数据存储库中（其中一个是数据湖），但处理引擎和用于分析工作负载的 Gold 存储库位于数据湖内，请确保数据目录是全面的，并且数据没有重复。确保元数据包含有关数据集级别的信息（例如，主数据或副本）。
- en: 'When bringing and transforming a master dataset within a data lake, there could
    be a need to sync after the computations. In [Figure 5-3](#the_data_catalog_supports_data_processi),
    you can see this high-level integration with data catalog of data processing:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在将主数据集引入和转换到数据湖时，可能需要在计算后进行同步。在 [图 5-3](#the_data_catalog_supports_data_processi)
    中，您可以看到这种与数据处理的数据目录高级集成：
- en: Metadata catalog fulfillment.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 元数据目录的履行。
- en: Search for the desired data asset.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索所需的数据资产。
- en: If the data asset is *not* already in the data lake, then make a copy of it
    into the Bronze storage area.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果数据资产尚未存储在数据湖中，则将其复制到 Bronze 存储区域。
- en: Execute your desired transformation on the copied data asset.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在复制的数据资产上执行所需的转换。
- en: Update the original data asset if needed.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要更新原始数据资产。
- en: A data catalog can help with rationalizing the various datasets an organization
    may have because it can help find duplicated, unused, or similar datasets and
    delete, decommission, or merge them as appropriate. A data catalog can help organizations
    focus on the data that is most relevant to them and avoid using resources to store
    and process data that is not useful, all of which could lead to cost savings.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据目录可以帮助理清组织可能拥有的各种数据集，因为它可以帮助找到重复的、未使用的或类似的数据集，并根据需要删除、废弃或合并它们。数据目录可以帮助组织专注于对它们最相关的数据，并避免使用资源来存储和处理无用的数据，这可能会带来成本节约。
- en: Whenever data is shared across an organization, it is beneficial to have associated
    data contracts. Data contracts tend to be JSON/YAML files that capture agreement
    between data producers and consumers on aspects such as the schema of the data,
    ingestion/publication frequency, ownership, and levels of data access, including
    anonymization and masking.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据在组织内共享时，拥有关联的数据合约是有益的。数据合约通常是捕获数据生产者和消费者之间协议的 JSON/YAML 文件，涵盖数据的模式、摄取/发布频率、所有权以及数据访问级别，包括匿名化和数据屏蔽。
- en: Hadoop Landscape
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop 景观
- en: 'Hadoop remains the de facto standard for data lakes both on premises and in
    the cloud. The idea of the data lake started with MapReduce and Hadoop technology
    (see [“Antipattern: Data Marts and Hadoop”](ch01.html#antipattern_data_marts_and_hadoop)).
    Hadoop’s popularity has grown over the past 15 years, creating a rich ecosystem
    of projects for data ingestion, storage, processing, and visualization. This led
    companies like IBM and Cloudera to develop commercial distributions. Databricks
    provides a multicloud Hadoop capability. In [Table 5-1](#hadoop_solutions_by_environment),
    we have listed some popular tools in the framework split by use case.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 仍然是数据湖在本地和云上的事实标准。数据湖的概念始于 MapReduce 和 Hadoop 技术（参见 [“反模式：数据集市与 Hadoop”](ch01.html#antipattern_data_marts_and_hadoop)）。Hadoop
    的流行已经持续了 15 年，为数据摄取、存储、处理和可视化创建了丰富的项目生态系统。这导致像 IBM 和 Cloudera 这样的公司开发了商业发行版。Databricks
    提供了多云 Hadoop 能力。在 [Table 5-1](#hadoop_solutions_by_environment) 中，我们列出了框架中一些常用工具按用例划分。
- en: The solutions listed in [Table 5-1](#hadoop_solutions_by_environment) can be
    deployed in an on-premises environment, but they can also be easily deployed in
    the cloud using an IaaS approach. Hyperscalers turned these popular products into
    fully managed solutions to reduce the burden of provisioning and infrastructure
    management on the user side, increase the inherent scalability and elasticity,
    and reduce costs. [Table 5-1](#hadoop_solutions_by_environment) also outlines
    the most popular solutions available in the cloud to facilitate the cloud migration
    and adoption of the most popular on-premises Hadoop technologies.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [Table 5-1](#hadoop_solutions_by_environment) 中列出的解决方案可以部署在本地环境中，但也可以通过 IaaS
    方法轻松部署在云中。超大规模云计算服务商将这些热门产品转化为完全托管的解决方案，以减轻用户在供应和基础设施管理方面的负担，增加固有的可伸缩性和弹性，并降低成本。
    [Table 5-1](#hadoop_solutions_by_environment) 还概述了云中提供的最流行解决方案，以促进流行的本地 Hadoop
    技术的云迁移和采用。
- en: Table 5-1\. Hadoop solutions by environment
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Table 5-1\. Hadoop 环境下的解决方案
- en: '| Use case | On premises, Databricks | AWS | Azure | Google Cloud Platform
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 用例 | 本地部署、Databricks | AWS | Azure | Google Cloud Platform |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Workflows | Airflow, Oozie | Data Pipeline, Airflow on EC2, EMR | HDInsight,
    Data Factory | Cloud Composer, Cloud Dataproc |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 工作流 | Airflow, Oozie | Data Pipeline, Airflow on EC2, EMR | HDInsight, Data
    Factory | Cloud Composer, Cloud Dataproc |'
- en: '| Streaming ingest | Apache Kafka, MapR Streams | Kinesis, Kinesis Data Streams,
    Managed Kafka | Event Hubs | Cloud Pub/Sub, Confluent Apache Kafka |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 流式摄取 | Apache Kafka, MapR Streams | Kinesis, Kinesis Data Streams, Managed
    Kafka | Event Hubs | Cloud Pub/Sub, Confluent Apache Kafka |'
- en: '| Streaming computation | Beam, Storm | Beam on Flink, Kinesis Data Streams
    | Beam on HDInsight, Stream Analytics | Cloud Dataflow |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 流计算 | Beam, Storm | Beam on Flink, Kinesis Data Streams | Beam on HDInsight,
    Stream Analytics | Cloud Dataflow |'
- en: '| SQL | Drill, Hive, Impala | Athena, Redshift | Synapse, HDInsight | BigQuery
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| SQL | Drill, Hive, Impala | Athena, Redshift | Synapse, HDInsight | BigQuery
    |'
- en: '| NoSQL | HBase, Cassandra | DynamoDB | Cosmos DB | Cloud Bigtable |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| NoSQL | HBase, Cassandra | DynamoDB | Cosmos DB | Cloud Bigtable |'
- en: '| Filesystem | HDFS, Iceberg, Delta Lake | EMR | HDInsight, Data Lake Storage
    | Cloud Dataproc |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 文件系统 | HDFS, Iceberg, Delta Lake | EMR | HDInsight, 数据湖存储 | 云 Dataproc |'
- en: '| Security | Sentry, Ranger, Knox | AWS IAM | Azure IAM | Cloud IAM, Dataplex
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 安全性 | Sentry, Ranger, Knox | AWS IAM | Azure IAM | 云 IAM, Dataplex |'
- en: '| Batch computation | Spark | EMR | HDInsight, Databricks | Cloud Dataproc,
    Serverless Spark |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 批量计算 | Spark | EMR | HDInsight, Databricks | 云 Dataproc, 无服务器 Spark |'
- en: Refer to the table as we review the data lake reference architectures for the
    three main cloud providers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们审查三大主要云服务提供商的数据湖参考架构时，请参阅表格。
- en: Cloud Data Lake Reference Architecture
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云数据湖参考架构
- en: In this section we are going to review some reference architectures for the
    implementation of a data lake in the public cloud leveraging services of the main
    three hyperscalers. As with any cloud architecture, there is no *one-size-fits-all*
    design. There will always be several different options available that could suit
    your specific needs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将审查在公共云中利用三大主要超级扩展者的服务实施数据湖的一些参考架构。与任何云架构一样，没有 *一刀切* 的设计。始终会有几种不同的选择可供选择，可能适合您特定的需求。
- en: Amazon Web Services
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 亚马逊网络服务
- en: 'The managed Hadoop service on AWS is Amazon Elastic MapReduce (EMR). However,
    that provides only analytics data processing. AWS recommends that we think of
    data lakes more holistically and consider that analytics of structured data is
    better carried out using Amazon Athena or Amazon Redshift. Also, the raw data
    may not already exist in cloud storage (Amazon S3) and will need to be ingested.
    Therefore, the recommended way to implement a data lake on top of AWS is to leverage
    AWS Lake Formation. It is a fully managed service that enables the development
    and the automation of data collection, data cleansing/processing, and data movement
    to make the data available for analytics and ML workloads. It comes with a permissions
    service that extends the AWS IAM capabilities and allows for better data governance
    and security (i.e., fine-grade policies, column- and row-level access control,
    etc.). Looking at the architecture in [Figure 5-4](#aws_data_lake_architecture),
    we can identify the following main components:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 的托管 Hadoop 服务是 Amazon Elastic MapReduce (EMR)。然而，这仅提供分析数据处理。AWS 建议我们更全面地考虑数据湖，并考虑使用
    Amazon Athena 或 Amazon Redshift 更好地进行结构化数据的分析。此外，原始数据可能尚未存在于云存储 (Amazon S3) 中，需要进行摄入。因此，在
    AWS 上实现数据湖的推荐方式是利用 AWS Lake Formation。这是一个完全托管的服务，可以实现数据收集、数据清洗/处理和数据移动的自动化，以便为分析和机器学习工作负载提供数据。它配备了一个权限服务，扩展了
    AWS IAM 的能力，并允许更好地进行数据治理和安全管理（例如，细粒度策略、列级和行级访问控制等）。查看图 [5-4](#aws_data_lake_architecture)
    中的架构，我们可以识别以下主要组件：
- en: Data sources, which, in this case, are Amazon S3, relational, and NoSQL databases
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源，本例中为 Amazon S3、关系数据库和 NoSQL 数据库
- en: Storage zones on top of Amazon S3 buckets
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储区域位于 Amazon S3 存储桶之上
- en: Data catalog, data governance, security, and process engine orchestrated by
    AWS Lake Formation
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 AWS Lake Formation 协调的数据目录、数据治理、安全性和流程引擎
- en: Analytical services such as Amazon Athena, Amazon RedShift, and Amazon EMR to
    provide access to the data
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析服务，例如 Amazon Athena、Amazon RedShift 和 Amazon EMR，以提供对数据的访问
- en: '![AWS data lake architecture](assets/adml_0504.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![AWS 数据湖架构](assets/adml_0504.png)'
- en: Figure 5-4\. AWS data lake architecture
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4\. AWS 数据湖架构
- en: This architecture should work for any kind of use case managing structured,
    semistructured, and unstructured data. Once the data has been prepared and transformed,
    it can easily be made available even to solutions (such as Databricks or Snowflake)
    outside of the data lake thanks to the pervasive nature of AWS S3 service, which
    can potentially be connected to any other service of the platform.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此架构适用于管理结构化、半结构化和非结构化数据的任何用例。一旦数据准备和转换完成，即使对于数据湖之外的解决方案（如 Databricks 或 Snowflake），也可以通过
    AWS S3 服务的广泛性轻松提供数据，后者可能与平台上的任何其他服务连接。
- en: Microsoft Azure
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微软 Azure
- en: 'On the Azure platform, there are several ways to implement a data lake because
    there are different solutions that can help with designing a possible target architecture.
    Typically, we’ll see the architecture shown in [Figure 5-5](#azure_data_lake_architecture),
    in which we can identify the following main components:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 平台上，有几种实现数据湖的方法，因为有不同的解决方案可以帮助设计可能的目标架构。通常情况下，我们会看到图 [5-5](#azure_data_lake_architecture)，在此架构中，我们可以识别以下主要组件：
- en: Azure Data Lake Storage Gen2 (ADLSG2)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 数据湖存储 Gen2 (ADLSG2)
- en: Object storage optimized for huge volumes of data, fully compatible with HDFS
    and where users typically implement all the data zones of the data lake
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 针对大量数据进行优化的对象存储，与 HDFS 完全兼容，并且通常由用户实现数据湖的所有数据区域
- en: Azure Data Factory
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Data Factory
- en: Serverless solution to ingest, transform, and manipulate the data
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器解决方案，用于摄取、转换和操作数据
- en: Azure Purview
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Purview
- en: Solution that provides governance for finding, classifying, defining, and enforcing
    policies and standards across data
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 提供治理的解决方案，用于在数据范围内查找、分类、定义和执行策略和标准
- en: Azure Synapse Analytics
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Synapse Analytics
- en: The analytics service used to issue SQL queries against the data stored in ADLSG2
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 用于针对存储在 ADLSG2 中的数据发出 SQL 查询的分析服务
- en: Databricks
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks
- en: Complete analytics and ML solution based on the Spark processing engine
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Spark 处理引擎的完整分析和 ML 解决方案
- en: Power BI
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Power BI
- en: Business intelligence (BI) reporting and visualization tool
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 业务智能（BI）报告和可视化工具
- en: '![Azure data lake architecture](assets/adml_0505.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![Azure 数据湖架构](assets/adml_0505.png)'
- en: Figure 5-5\. Azure data lake architecture
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. Azure 数据湖架构
- en: It has to be noted that Azure Databricks can be interchanged with Azure HDInsight.
    The main difference between the two is that Databricks is an Apache Spark–based
    analytics platform optimized for the Microsoft Azure cloud services platform while
    HDInsight is a managed full Apache Hadoop distribution (i.e., more than just Spark
    tools, but less tuned to Azure). If you want to work with the standard Hadoop
    ecosystem solutions, you should leverage HDInsight, while if you prefer to leverage
    a complete analytics and ML solution based on Spark, you should go for Databricks.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Azure Databricks 可以与 Azure HDInsight 互换使用。两者的主要区别在于，Databricks 是基于 Apache
    Spark 的分析平台，经过优化以适配 Microsoft Azure 云服务平台，而 HDInsight 是一个托管的完整 Apache Hadoop 分发版（即不仅限于
    Spark 工具，但对 Azure 的适配程度较低）。如果您想使用标准的 Hadoop 生态系统解决方案，应选择 HDInsight；而如果您倾向于利用基于
    Spark 的完整分析和 ML 解决方案，应选择 Databricks。
- en: Note
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While Databricks is available on all major cloud providers, its native and tight
    integration with Azure makes it unique in that it can be considered a first-party
    service rather than a third-party one.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Databricks 可在所有主要云提供商上使用，但其与 Azure 的本地紧密集成使其独特之处在于，它可以被视为一种第一方服务，而不是第三方服务。
- en: Google Cloud Platform
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud Platform
- en: 'On Google Cloud Platform (see [Figure 5-6](#google_cloud_platform_data_lake_archite)),
    the different serverless and fully managed components are communicating with one
    another via APIs. We can identify the following main components:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Cloud Platform（见[图 5-6](#google_cloud_platform_data_lake_archite)），各个无服务器和完全托管的组件通过
    API 相互通信。我们可以识别出以下主要组件：
- en: Data Fusion
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Data Fusion
- en: A solution to ingest and transform data both in batch and in streaming
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于批处理和流处理数据摄取和转换的解决方案
- en: Pub/Sub
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Pub/Sub
- en: Messaging middleware to integrate the input arriving from Data Fusion and the
    Hadoop cluster delivered by Dataproc
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 用于集成从 Data Fusion 输入的输入和由 Dataproc 交付的 Hadoop 集群的消息中间件
- en: Dataproc
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Dataproc
- en: On-demand Apache Hadoop cluster delivering HDFS, Spark, and non-Spark capabilities
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 提供按需 Apache Hadoop 集群，提供 HDFS、Spark 和非 Spark 能力
- en: Cloud Storage
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 云存储
- en: Object storage solution where data zones are implemented
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实现数据区域的对象存储解决方案
- en: Bigtable and BigQuery
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Bigtable 和 BigQuery
- en: Analytical and real-time data processing engine
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 分析和实时数据处理引擎
- en: Looker/Looker Studio
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Looker/Looker Studio
- en: BI and visualization solutions
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: BI 和可视化解决方案
- en: Dataplex
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Dataplex
- en: Single pane of glass to handle data governance, data catalog, and security
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个统一的界面，用于处理数据治理、数据目录和安全性
- en: Composer
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Composer
- en: Data workflow orchestration service based on Apache Airflow that empowers users
    to author, schedule, and monitor pipelines
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Apache Airflow 的数据工作流编排服务，使用户能够编写、调度和监控管道
- en: '![Google Cloud Platform data lake architecture](assets/adml_0506.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![Google Cloud Platform 数据湖架构](assets/adml_0506.png)'
- en: Figure 5-6\. Google Cloud Platform data lake architecture
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. Google Cloud Platform 数据湖架构
- en: Depending on your use case, Hadoop or an HDFS cluster may not always be the
    best fit. Having the data in Cloud Storage gives you the freedom to start selecting
    the right tool for the job at hand, instead of being limited to the tools and
    compute resources available on the HDFS cluster. For instance, a lot of ad hoc
    Hive SQL queries can be easily migrated to BigQuery, which can employ either its
    native storage or read/query directly off Cloud Storage. Similarly, streaming
    applications may fit better into Apache Beam pipelines, and these can be run on
    Dataflow, a fully managed streaming analytics service that minimizes latency,
    processing time, and cost through autoscaling and batch processing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的使用案例，Hadoop或HDFS集群可能并不总是最合适的选择。将数据存储在云存储中使您能够根据手头工作选择合适的工具，而不是仅限于HDFS集群上可用的工具和计算资源。例如，许多临时Hive
    SQL查询可以轻松迁移到BigQuery，后者可以使用其本地存储或直接从云存储读取/查询。类似地，流式应用程序可能更适合于Apache Beam流水线，这些可以在Dataflow上运行，Dataflow是一个完全托管的流式分析服务，通过自动缩放和批处理大大减少了延迟、处理时间和成本。
- en: Note
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: When choosing a cloud vendor, you should also consider the datasets that are
    natively available through their offerings. In the marketing or advertising space,
    for example, it can be very impactful to implement your business solutions using
    these datasets. Some examples of datasets include Open Data on AWS, the Google
    Cloud Public Datasets, and the Azure Open Datasets. If you advertise on Google
    or sell on Amazon, the ready-built integrations between the different divisions
    of these companies and their respective cloud platforms can be particularly helpful.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择云供应商时，您还应考虑其提供的本地数据集。例如，在市场营销或广告领域，利用这些数据集实施业务解决方案可能会产生非常大的影响。一些数据集的例子包括AWS上的开放数据、Google
    Cloud的公共数据集和Azure的开放数据集。如果您在Google上进行广告投放或在亚马逊上销售产品，这些公司不同部门之间及其各自云平台之间的现成集成可能特别有帮助。
- en: Now that you’re familiar with the reference architecture of cloud data lakes,
    let’s delve into how we can extend the data lake with third-party solutions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经熟悉了云数据湖的参考架构，让我们深入探讨如何通过第三方解决方案扩展数据湖。
- en: 'Integrating the Data Lake: The Real Superpower'
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成数据湖：真正的超能力
- en: Data lake technology has become popular because it can handle any type of data,
    from structured tabular formats to unstructured files like text or images. This
    enables a wide range of use cases that were not possible before, such as analyzing
    suppliers by performing text analysis on invoices, identifying actors in a film
    scene, or implementing a real-time dashboard to monitor sales on an ecommerce
    website. The superpower of a data lake is its ability to connect the data with
    an unlimited number of process engines to activate potentially any use case you
    have in mind.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖技术因其能够处理任何类型的数据而变得流行，从结构化的表格格式到文本或图像等非结构化文件。这使得许多以前不可能的用例得以实现，例如通过对发票进行文本分析来分析供应商、识别电影场景中的演员，或实现实时仪表盘以监控电子商务网站上的销售情况。数据湖的超能力在于它能够将数据与无限数量的处理引擎连接起来，从而激活您可能有的任何用例。
- en: APIs to Extend the Lake
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展湖的API
- en: In a data lake, the data ingestion process begins with raw data being imported
    into the landing zone. After the data has been ingested, it must be processed
    (which may involve multiple passes) before it can be visualized and activated.
    Each of these steps may involve a variety of engines that are either part of the
    data lake itself or third-party products that are hosted in the cloud or, in some
    cases, on premises.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据湖中，数据摄取过程始于原始数据被导入到登陆区。数据摄取后，必须进行处理（可能涉及多次传递），然后才能进行可视化和激活。每个步骤可能涉及数据湖本身或云中托管的各种引擎，或者在某些情况下是本地第三方产品。
- en: To make different systems, which may be located in hybrid environments, communicate
    and exchange data, you can use APIs. APIs are pieces of software that allow two
    or more systems to communicate via shared protocols such as HTTPS and gRPC. The
    majority of modern applications use APIs to integrate services and exchange data.
    Even when ad hoc connectors are available, they are usually built on top of APIs.
    You can think of APIs as highways where data can flow from one system to another.
    The security measures implemented to protect the data traffic are the toll booths,
    and the rate limits are the speed limits. Thanks to these highways, data can flow
    between multiple systems and be processed by any type of process engine, from
    a standard ETL to a modern ML framework like TensorFlow or PyTorch.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使位于混合环境中的不同系统能够通信和交换数据，可以使用API。API是允许两个或更多系统通过共享的协议（例如HTTPS和gRPC）进行通信的软件组件。大多数现代应用程序使用API来集成服务和交换数据。即使存在特定连接器，它们通常也是构建在API之上的。您可以将API视为数据可以从一个系统流向另一个系统的高速公路。用于保护数据流量的安全措施是收费站，速率限制则是速度限制。由于这些高速公路，数据可以在多个系统之间流动，并且可以由任何类型的处理引擎处理，从标准ETL到现代ML框架如TensorFlow或PyTorch。
- en: Organizations can evolve their data lakes using APIs to desired external services.
    APIs can also be used to monitor, configure, tweak, automate, and, of course,
    access and query the lake itself.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 组织可以通过API将其数据湖演化到所需的外部服务。API还可用于监视、配置、调整、自动化，当然还包括访问和查询湖本身。
- en: The Evolution of Data Lake with Apache Iceberg, Apache Hudi, and Delta Lake
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Apache Iceberg、Apache Hudi和Delta Lake推动数据湖的演进
- en: 'The primary goal of integrating the data lake with other technologies is to
    expand its capability beyond the out-of-the-box features made available by the
    Hadoop framework. When we consider a standard Hadoop stack, there is one missing
    element that is typically handled with other technologies (for example, online
    transaction processing [OLTP] databases): ACID transactions management. Apache
    Iceberg, Apache Hudi, and Delta Lake are open source storage layers sitting on
    top of HDFS that have been implemented to address this key aspect. While they
    each come with a set of different features (e.g., Apache Iceberg supports more
    file formats than Apache Hudi and Delta Lake), there are some elements in common:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据湖与其他技术集成的主要目标是扩展其能力，超越Hadoop框架提供的开箱即用功能。在考虑标准的Hadoop堆栈时，通常有一个缺失的元素，这通常由其他技术（例如在线事务处理[OLTP]数据库）处理：ACID事务管理。Apache
    Iceberg、Apache Hudi和Delta Lake是构建在HDFS之上的开源存储层，专门解决了这一关键方面。虽然它们各自具有一系列不同的功能（例如，Apache
    Iceberg支持比Apache Hudi和Delta Lake更多的文件格式），但也有一些共同的元素：
- en: ACID compliance, giving users the certainty that the information they are querying
    is consistent
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符合ACID标准，确保用户查询的信息一致性
- en: Overcoming the inherent limitation of HDFS in terms of file size—with this approach,
    even a small file can work perfectly
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克服HDFS在文件大小方面的固有限制——使用这种方法，即使是小文件也可以完美运作
- en: Logging of every single change made on data, guaranteeing a complete audit if
    ever necessary and enabling time travel queries
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录对数据所做的每一次更改，确保在必要时进行完整审计，并支持时间旅行查询
- en: No difference in handling batch and streaming ingestions and elaboration
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理批处理和流式输入和处理没有差别
- en: Full compatibility with the Spark engine
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Spark引擎完全兼容
- en: Storage based on Parquet format that can achieve a high level of compression
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Parquet格式的存储，能够实现高水平的压缩
- en: Stream processing enablement
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持流处理
- en: Ability to treat the object store as a database
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够将对象存储视为数据库
- en: Applying governance at the row and column level
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在行和列级别应用治理
- en: These storage solutions can enable several use cases that were usually handled
    by other technologies (such as the DWH, which we will investigate more in [Chapter 6](ch06.html#innovating_with_an_enterprise_data_ware)),
    mainly because of the ability to prevent data corruption, execute queries in a
    very fast mode, and frequently update the data. There are very specific scenarios
    where the transactional feature of this new enabled storage (i.e., update/delete)
    plays a crucial role—these are related to the GDPR and the California Consumer
    Privacy Act (CCPA). Organizations are forced by these regulations to have the
    ability to purge personal information related to a specific user in case of a
    specific request. Performing this operation in a standard HDFS environment could
    be time- and resource-consuming because all of the files that pertain to the personal
    data being requested must be identified, ingested, filtered, and written out as
    new files, and the original ones deleted. These new HDFS extensions simplify these
    operations, making them easy and fast to execute and, more importantly, reliable.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些存储解决方案可以启用通常由其他技术处理的几个用例（例如数据仓库，我们将在[第6章](ch06.html#innovating_with_an_enterprise_data_ware)中详细研究），主要是因为能够防止数据损坏、以非常快的模式执行查询并频繁更新数据。这些新的启用存储的事务特性非常特定，发挥着至关重要的作用——这些特性与GDPR和加利福尼亚消费者隐私法案（CCPA）相关。根据这些法规，组织被迫具备根据特定请求清除特定用户相关个人信息的能力。在标准HDFS环境中执行此操作可能是耗时且资源消耗大的，因为必须识别、摄入、过滤和写入新文件，并删除原始文件中与请求的个人数据相关的所有文件。这些新的HDFS扩展简化了这些操作，使其易于快速执行，更重要的是，可靠性高。
- en: These open source projects have been widely adopted by the community, which
    is heavily investing in their development. For example, Apache Iceberg is widely
    used by Netflix, while Apache Hudi powers Uber’s data platform. Even though Delta
    Lake is a Linux Foundation project, the main contributor is Databricks, the company
    behind the Spark engine that developed and commercialized an entire suite to handle
    big data workloads based on a vendor-proprietary version of Spark and Delta Lake.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些开源项目已被社区广泛采用，并且社区正在大力投资它们的开发。例如，Netflix广泛使用Apache Iceberg，而Uber的数据平台则由Apache
    Hudi驱动。尽管Delta Lake是Linux基金会的项目，但其主要贡献者是Databricks，这是开发和商业化了整个基于供应商专有版本的Spark和Delta
    Lake的大数据工作负载处理套件的Spark引擎背后的公司。
- en: 'In addition to ACID, there are two other features that are evolving the way
    users can handle data in a data lake:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 除了ACID之外，还有两个功能正在改变用户在数据湖中处理数据的方式：
- en: Partition evolution
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 分区演化
- en: This is the ability to update the partition definition over a file (i.e., table).
    The partition is the information that allows a filesystem to split the content
    of a file into several chunks to avoid completing a full scan when retrieving
    information (e.g., extracting sales figures of Q1 2022—you should be able to query
    only data belonging to the first quarter of the year instead of querying the entire
    dataset and then filtering out the data you do not need). Considering the fact
    that the definition of a partition in a file can evolve because of a business
    need (e.g., working hours of a device for which we want to collect insights),
    it is critical to have a filesystem that is able to handle these changes in a
    transparent and fast way. HDFS (via Hive) can achieve this from a logical point
    of view, but the computational effort required makes it practically unachievable.
    Please note that, at the time of writing, this feature is offered only by Apache
    Iceberg.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这是更新文件（即表格）上的分区定义的能力。分区是允许文件系统将文件内容分割为多个块的信息，以避免在检索信息时完成全面扫描（例如，提取2022年第一季度的销售数据时，您应该能够仅查询属于该年度第一季度的数据，而不是查询整个数据集然后过滤掉您不需要的数据）。考虑到文件中分区的定义可能因业务需求（例如，我们想要收集洞察力的设备的工作时间）而演变，因此具有能够以透明且快速的方式处理这些变化的文件系统至关重要。从逻辑角度来看，HDFS（通过Hive）可以实现这一点，但所需的计算工作使其实际上难以实现。请注意，在撰写本文时，此功能仅由Apache
    Iceberg提供。
- en: Schema evolution
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 模式演化
- en: Just like the partition, the schema may need to be updated over time. You may
    want to add, remove, or update columns in a table, and the filesystem should be
    able to do that at scale without the need to retransform the data (i.e., without
    an ELT/ETL approach). Please note that while, at the time of writing, this is
    fully supported only by Apache Iceberg, all the solutions are able to handle schema
    evolution when leveraging Apache Spark as the engine.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 就像分区一样，模式可能需要随时间更新。您可能希望在表中添加、删除或更新列，而文件系统应能够在不重新转换数据（即无需ELT/ETL方法）的情况下大规模执行此操作。请注意，尽管在撰写本文时，只有Apache
    Iceberg完全支持此功能，但所有解决方案都能够在利用Apache Spark作为引擎时处理模式演化。
- en: Now that you have seen how you can expand the capabilities of a data lake and
    enrich the broad set of use cases that can be solved with them, let’s have a look
    at how to actually interact with data lakes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到如何扩展数据湖的能力并丰富可以用它们解决的广泛用例集，让我们看看如何实际与数据湖进行交互。
- en: Interactive Analytics with Notebooks
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用笔记本进行交互式分析
- en: 'One of the most important things when dealing with data is the ability to interactively
    get access to it and perform analysis in a very easy and fast manner. When leveraging
    a data lake, data engineers, data scientists, and business users can leverage
    a plethora of services like Spark, Pig, Hive, Presto, etc., to handle the data.
    A solution that has grown greatly in popularity in several communities, primarily
    with data scientists, is what we consider the best Swiss Army knife an organization
    may leverage for data analysis: Jupyter Notebook.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据时最重要的一点是能够以非常简单且快速的方式交互式地访问数据并执行分析。在利用数据湖时，数据工程师、数据科学家和业务用户可以利用大量服务（如Spark、Pig、Hive、Presto等）处理数据。在几个社区中，尤其是在数据科学家中，一种大受欢迎的解决方案被认为是组织在数据分析中可以利用的最佳瑞士军刀：Jupyter
    Notebook。
- en: Jupyter Notebook is an open source application that can be used to write *live
    documents* containing a mix of text, code to be executed, plots, and charts. It
    can be considered a live book where, in addition to the text that you write using
    a markup language like Markdown, there is code that you execute that performs
    some activities on data (e.g., query, transformation, etc.) and eventually plots
    some results, generating charts or tables.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook是一个开源应用程序，用于编写包含文本、待执行代码、绘图和图表的*实时文档*。它可以被视为一个实时书籍，在您使用类似Markdown的标记语言编写文本的同时，还有执行一些数据操作（例如查询、转换等）的代码，并最终绘制一些结果，生成图表或表格。
- en: From an architectural perspective, you can think of the Jupyter Notebook running
    on top of a data lake as three different components, as presented on the lefthand
    side of [Figure 5-7](#jupyter_general_notebook_architecture_a). HDFS is the storage,
    the kernel the process engine, and Jupyter the server that leverages the process
    engine to access the data and, via the browser, provide users with the user interface
    to write and execute the content of the notebooks. There are several kernels that
    you can leverage (for ML, you’d often use PyTorch), but the most common choice
    for data engineering is Spark, which can be accessed using PySpark via code written
    in Python programming language (as shown on the righthand side of [Figure 5-7](#jupyter_general_notebook_architecture_a)).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构角度来看，您可以将Jupyter Notebook视为在数据湖顶层运行的三个不同组件，如[图5-7](#jupyter_general_notebook_architecture_a)左侧所示。HDFS是存储，内核是处理引擎，而Jupyter是利用处理引擎访问数据，并通过浏览器为用户提供笔记本内容编写和执行的服务器的界面。有几种内核可供利用（例如机器学习常用的PyTorch），但数据工程中最常见的选择是Spark，可以通过Python编程语言编写的PySpark访问（如[图5-7](#jupyter_general_notebook_architecture_a)右侧所示）。
- en: '![Jupyter general notebook architecture and Spark-based kernel](assets/adml_0507.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![Jupyter通用笔记本架构和基于Spark的内核](assets/adml_0507.png)'
- en: Figure 5-7\. Jupyter general notebook architecture and Spark-based kernel
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7. Jupyter通用笔记本架构和基于Spark的内核
- en: 'Once properly configured, you can start immediately writing text and code directly
    into the notebook and interact with datasets as you would while leveraging the
    Spark command-line interface. You can use notebooks to develop and share code
    with other people within the organization, perform quick tests and analysis, or
    quickly visualize data to get some extra insights. It is important to highlight
    that results are not *static* because they are *live* documents: this means that
    if the underlying dataset changes or if the piece of code changes, results will
    be different when the person with whom the notebook is shared reexecutes the notebook.
    Once you’ve finalized the analysis, usually there are two different paths that
    you can take:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦正确配置，您可以立即在笔记本中直接编写文本和代码，并与数据集进行交互，就像在利用 Spark 命令行界面时那样。您可以使用笔记本开发和与组织内的其他人员共享代码，执行快速测试和分析，或快速可视化数据以获取额外的洞察。需要强调的是结果不是
    *静态* 的，因为它们是 *活动* 文档：这意味着如果底层数据集发生变化或代码段变更，当与笔记本共享的人重新执行笔记本时，结果将不同。分析完成后，通常有两条不同的路径可供选择：
- en: Share the source code of the notebook (Jupyter Notebook produces *.ipynb* files)
    with other data scientists, data engineers, developers, or anyone who wants to
    contribute. They can load the file in their environment and rerun the code (it
    is of course necessary to have the right access to the underlying storage system
    and to any other solution integrated via APIs).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他数据科学家、数据工程师、开发人员或任何希望贡献的人分享笔记本的源代码（Jupyter Notebook 生成 *.ipynb* 文件）。他们可以在其环境中加载文件并重新运行代码（当然需要具备对底层存储系统及通过
    API 集成的任何其他解决方案的正确访问权限）。
- en: Make results static via the generation of an HTML page or a PDF document that
    can be shared to a broader set of users.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过生成 HTML 页面或 PDF 文档使结果静态化，可以与更广泛的用户群共享。
- en: Notebooks have become the de facto standard solution for interactive data analysis,
    testing, and experimentation because of their extreme flexibility, both in terms
    of programming languages that you can leverage (thanks to the numerous available
    kernels) and in terms of activity that you can do (e.g., you can generate a chart
    starting from data in your data lake, or you can train a complex ML algorithm
    on a small subset of data before moving that to production).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本已经成为互动数据分析、测试和实验的事实标准解决方案，这是因为它们极其灵活，无论是在可以利用的编程语言（多亏了众多可用的内核），还是在可以进行的活动方面（例如，您可以从数据湖中的数据生成图表，或者在将复杂的机器学习算法应用于数据的小子集之前进行训练）。
- en: 'What we have seen working with several organizations is that the *notebook
    approach* is a first step in putting in place a *journey to data democratization*
    (as we will discuss in the next section) because it allows the more technically
    savvy people to have an immediate and standardized access to the data, fostering
    an approach to collaborations. While data scientists were the pioneers of notebook
    use, data engineers and analytical engineers continue to adopt them as well. We
    have even seen companies developing custom libraries to be included in *notebook
    templates* with the aim to facilitate the integration with several other solutions
    (off the shelf or custom developed), bringing the standardization to another level
    and reducing the learning curve for the final users (and even the pain). This
    standardization, thanks to the container technology, has been brought even at
    the compute level: every time users within the company launch a notebook they
    are, behind the scenes, launching a container with an adequate number of computing
    resources and a set of tools that are immediately at their disposal for their
    data analysis.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与几家组织合作时看到的情况是，*笔记本方法* 是推动 *数据民主化之旅* 的第一步（我们将在下一节讨论），因为它允许技术上更有能力的人立即和标准化地访问数据，促进协作方式。虽然数据科学家是笔记本使用的先驱，但数据工程师和分析工程师也在继续采用它们。我们甚至看到一些公司开发了自定义库，以便包含在
    *笔记本模板* 中，旨在促进与多种其他解决方案（现成或自定义开发的）的集成，将标准化提升到另一个层次，并减少最终用户（甚至是痛苦的）学习曲线。由于容器技术的存在，这种标准化甚至已经扩展到计算层面：每当公司内的用户启动笔记本时，实际上是在后台启动一个具有适当数量计算资源和一组工具的容器，这些工具立即可供其用于数据分析。
- en: Cloud hyperscalers are making some solutions available for managed versions
    of Jupyter Notebook—AWS SageMaker, Azure ML Studio, and Google Cloud Vertex AI
    Workbench, to name a few—that can help get rid of headaches related to the management
    of the underlying infrastructure thanks to the fact they are fully managed.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 云超大规模供应商正在提供一些解决方案的托管版本，如AWS SageMaker、Azure ML Studio和Google Cloud Vertex AI
    Workbench，这些解决方案可以帮助消除与基础架构管理相关的头疼问题，因为它们是完全托管的。
- en: Now that you have a better understanding of how you can leverage Jupyter Notebook
    to expand the data lake, we’ll turn our attention to helping you understand how
    people within organizations can handle data ingestion up to data visualization
    reporting, moving from a fully IT model to a more democratic approach.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你更好地理解了如何利用Jupyter Notebook扩展数据湖，我们将把注意力转向帮助你理解组织内部如何处理从数据摄入到数据可视化报告的数据，从完全的IT模式转向更为民主的方法。
- en: Democratizing Data Processing and Reporting
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理和报告民主化
- en: The best value that data provides to an organization is that it enables decision
    makers and users in general to make informed decisions. To that end, data should
    be accessible and usable by any authorized individual without the need for ad
    hoc expertise and specialism. Even if it is possible to implement democratization
    of the data access within a company from a technical point of view, it is not
    sufficient to focus just on the technology. Organizations need to also implement
    data cataloging and governance operations. Having good metadata describing proper
    datasets will enable users to find the right information they need and to activate
    correct data processing and reporting. In this section we will explore the key
    technologies that can help an organization with switching from a fully IT-driven
    approach to a more *democratic* approach when building a cloud data lake.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 数据为组织提供的最大价值在于它使决策者和普通用户能够做出知情决策。为此，任何授权的个人都应能够访问和使用数据，而无需临时专业知识和专长。即使从技术角度来看实现数据访问的民主化在公司内是可能的，但仅专注于技术是不够的。组织还需要实施数据目录和治理操作。拥有描述正确数据集的良好元数据将使用户能够找到他们所需的正确信息，并激活正确的数据处理和报告。在本节中，我们将探讨关键技术，帮助组织从完全由IT驱动的方法转向更为*民主化*的方法，建设云数据湖。
- en: Build Trust in the Data
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在数据中建立信任
- en: 'When one of the authors of the book was working as a consultant for an important
    retail customer several years ago, he worked to develop solutions to automate
    the data extraction from the sales database to generate reports to be leveraged
    by the business users. He needed to be available every time decision makers had
    questions like the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当这本书的一位作者几年前为一家重要的零售客户担任顾问时，他努力开发解决方案，自动从销售数据库中提取数据以生成报告，供业务用户利用。每当决策者有以下问题时，他都需要随时可用：
- en: Where did you get this data from?
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你从哪里获取这些数据的？
- en: How did you transform that data?
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是如何转换这些数据的？
- en: What are the operations you made before generating the report?
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成报告之前，你进行了哪些操作？
- en: Once he was assigned to another customer, the organization’s IT team took over
    the entire end-to-end process. While they could fix bugs and monitor the report
    generation, they were unable to convince the decision makers that the data and
    reports were trustworthy because they did not have the right level of knowledge
    to answer these questions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦他被分配到另一个客户那里，组织的IT团队接管了整个端到端的过程。虽然他们可以修复错误并监控报告生成，但他们无法说服决策者数据和报告是可信的，因为他们没有足够的知识水平来回答这些问题。
- en: This trust was clearly a bottleneck that would have been removed by having final
    users be able to dig into their data in an autonomous way and audit it from ingest
    to the final reported value. This approach may have been unrealistic in the past,
    but the fact that people are now more *digitally experienced* helps a lot in shifting
    this method.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信任显然是一个瓶颈，通过使最终用户能够自主地挖掘数据并从摄入到最终报告的值进行审计，这个瓶颈可以消除。过去这种方法可能是不现实的，但现在人们更加*数字化经验丰富*，有助于转变这种方法。
- en: As illustrated in [Figure 5-8](#data_access_approaches_left_parenthesis), there
    is a clear transition in ownership and responsibility from the old world represented
    to the left to the new one represented to the right. While the majority of the
    work used to be carried out by the IT department, nowadays final users have in
    their hands tools that enable them to handle the majority of the activities from
    the data cataloging up to the data visualization with a great level of autonomy.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 5-8 所示，从左边代表的旧世界向右边代表的新世界，责任和所有权的明显转变。尽管大部分工作过去由 IT 部门完成，但如今最终用户手中拥有能够处理从数据编目到数据可视化的大部分活动的工具，具备很高的自治权。
- en: '![Data access approaches (centrally managed versus democratic data access)](assets/adml_0508.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![数据访问方法（集中管理 versus 民主数据访问）](assets/adml_0508.png)'
- en: Figure 5-8\. Data access approaches (centrally managed versus democratic data
    access)
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-8\. 数据访问方法（集中管理 versus 民主数据访问）
- en: Tools like Atlan, Collibra, Informatica, AWS Glue, Azure Purview, and Google
    Dataplex are making the process of metadata collection easier and faster. On one
    side, a lot of automation has been built to enable data crawling in an automated
    way, especially thanks to the integration with several database and storage engines
    (e.g., AWS Redshift and Athena, Azure Synapse, Google BigQuery, Snowflake, etc.),
    and on the other, facilitating data entry activities leveraging rich and easy-to-use
    user interfaces lets businesspeople carry out the majority of the work.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 工具如 Atlan、Collibra、Informatica、AWS Glue、Azure Purview 和 Google Dataplex 正在使元数据收集过程变得更加简单和快速。一方面，已经构建了大量自动化功能，以实现自动化数据爬取，特别是通过与多个数据库和存储引擎集成（例如
    AWS Redshift 和 Athena、Azure Synapse、Google BigQuery、Snowflake 等），另一方面，通过使用丰富且易于使用的用户界面促进数据录入活动，让业务人员能够执行大部分工作。
- en: 'Moving up through the chain, we find that even the data processing, preparation,
    cleansing, and wrangling steps, which have always been for expert users (i.e.,
    data engineers), can be handled directly by the final users. Of course, a subset
    of the data processing may still need to be fulfilled by data engineers/data scientists
    leveraging advanced process engines like Spark. For the activities that require
    subject matter experts (SMEs), tools like Talend Data Preparation or Trifacta
    Dataprep have been developed and made available with the clear goal to support
    non-data-engineering/data scientist users: exploration, transformation, and filtering
    are just a few of the activities that can be achieved, leveraging a very intuitive
    interface that delegates the processing to an underlying engine (e.g., Beam) that
    can apply all the mutations to vast datasets. This approach is even emphasized
    by the fact that the access to the underlying data, which sits on the HDFS cluster
    or directly on the object storage like AWS S3, Azure Data Lake, or Google Cloud
    Storage, can be achieved via several different tools offering a broad set of features
    and controls. This means that different kinds of users can leverage different
    kinds of tools to deal with the data. Data engineers, for example, can leverage
    Spark to develop their data transformation pipelines; data scientists can implement
    ML algorithms using scikit-learn, TensorFlow, or PyTorch; and business analysts
    can use Hive or PrestoSQL to execute what-if analysis using SQL queries.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 向上走一步，我们发现即使是数据处理、准备、清洗和整理步骤，这些步骤通常是专为专业用户（即数据工程师）设计的，现在也可以直接由最终用户处理。当然，数据处理的一部分仍然可能需要数据工程师/数据科学家利用像
    Spark 这样的先进流程引擎来完成。对于需要主题专家（SMEs）参与的活动，开发和提供的工具如 Talend Data Preparation 或 Trifacta
    Dataprep 旨在支持非数据工程师/数据科学家用户：探索、转换和过滤只是可以实现的几个活动，利用非常直观的界面将处理委托给底层引擎（例如 Beam），该引擎可以对大型数据集应用所有变更。这种方法甚至强调了访问基础数据的事实，这些数据位于
    HDFS 集群上或直接存储在像 AWS S3、Azure Data Lake 或 Google Cloud Storage 这样的对象存储上，可以通过多种不同的工具实现，这些工具提供了广泛的功能和控制。这意味着不同类型的用户可以利用不同类型的工具处理数据。例如，数据工程师可以利用
    Spark 开发其数据转换管道；数据科学家可以使用 scikit-learn、TensorFlow 或 PyTorch 实施机器学习算法；业务分析师可以使用
    Hive 或 PrestoSQL 使用 SQL 查询执行假设分析。
- en: The last step is the data visualization/reporting, which is typically the easiest
    for business users to carry out themselves. Here there are a ton of solutions
    that can be leveraged (e.g., Microsoft Power BI, Looker, Tableau, and Qlik, just
    to name a few) that give users the flexibility they need. Users tend to be naturally
    autonomous at this stage because the tools are similar in the approach, to some
    extent, to what they are used to with Excel, so users don’t have a steep learning
    curve to become proficient. Data visualization, BI, and what-if scenarios are
    the typical analysis that business users can easily carry out.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是数据可视化/报告，通常是企业用户自行执行的最简单步骤。在这里，有许多解决方案可以利用（例如，Microsoft Power BI、Looker、Tableau
    和 Qlik，仅举几例），这些解决方案为用户提供了所需的灵活性。用户在这个阶段往往自然而然地独立进行，因为这些工具在某种程度上与他们在 Excel 中所熟悉的方法相似，所以用户不需要很陡的学习曲线即可精通。数据可视化、商业智能和假设情景分析是企业用户通常可以轻松执行的分析工作。
- en: 'Even if the raw data is always managed by the IT department, there could be
    some situations where, thanks to the integrations with third-party services, business
    users can ingest data into the data lake in an autonomous way. Because of this,
    there is a growing need for trust concerning data content and related quality
    and correctness of datasets ingested by different business units. The concept
    of *stewardship* is gaining traction to help with this. Stewardship, which is
    the process of managing and overseeing an organization’s data assets to ensure
    that business users have access to high-quality, consistent, and easily accessible
    data, can be seen as a combination of three key factors:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 即使原始数据始终由IT部门管理，但在与第三方服务集成的某些情况下，企业用户可以自主将数据摄入数据湖。因此，对数据内容的信任以及不同业务单位摄入的数据集的质量和正确性的相关需求正在增长。管理数据资产以确保企业用户可以访问高质量、一致且易于访问的数据的过程，即“管理”，正在成为解决这一问题的关键因素的组合。
- en: '*Identify* the key stakeholders who have the right information on a timely
    basis.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 及时识别具有正确信息的关键利益相关者。
- en: '*Defend* the data from any kind of exfiltration, both internal and external,
    with a focus on personnel.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护数据免受任何内部和外部的外泄，重点放在人员方面。
- en: '*Cooperate* with other people inside or outside the company to unlock the value
    of data.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与公司内外的其他人合作，解锁数据的价值。
- en: What we are seeing in many companies is that stewardship is not necessarily
    a role that people are assigned to, but it is more a title that they earn in the
    field thanks to their interactions with the tools and the quality of their information
    that they provide to the internal community. In fact, there are several tools
    (like Informatica Data Catalog, for example) that allow users to rate stewards
    in the same way purchasers can rate sellers on Amazon.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多公司中我们所看到的是，管理并不一定是人们被分配的角色，而更多是由于他们与工具的互动以及他们提供给内部社区的信息质量而获得的头衔。事实上，有几种工具（例如
    Informatica Data Catalog）允许用户像亚马逊上的买家评价卖家一样评价数据管理者。
- en: 'Now that you’ve seen the various options available to bring the organization
    toward a more modern and democratic approach, let’s discuss the part of the process
    that is still mainly in the hands of the IT team: the data ingestion.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经看到了将组织引向更现代和民主方法的各种选择，让我们讨论这个过程中仍然主要由IT团队掌控的部分：数据摄入。
- en: Data Ingestion Is Still an IT Matter
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄入仍然是IT事务
- en: 'One of the most critical and important steps in a data lake is the data ingestion
    process. If poorly planned, the data lake may become a “data swamp” with a lot
    of unused data. This typically occurs because organizations tend to load every
    kind of raw data into the data lake, even without having a complete understanding
    of *why* they loaded the data or *how* they will leverage it. This approach will
    bring the creation of massive *heaps of data* that will be unused for the majority
    of the time. But even unused data remains in the lake and uses space that should
    be free for other activities. Stale data also tends to have gaps and inconsistencies
    and causes hard-to-troubleshoot errors when it is eventually used by someone.
    Because of that, it is important to follow the following best practices, which
    you can leverage during ingestion processes:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据湖中，最关键和重要的步骤之一是数据摄取过程。如果规划不当，数据湖可能会变成“数据沼泽”，积累大量未被使用的数据。这通常发生在组织倾向于将各种原始数据加载到数据湖中，即使他们并没有完全理解为什么要加载这些数据，或者如何利用这些数据。这种方法会导致大量未被使用的*数据堆积*，这些数据大部分时间都不会被使用。但即使未被使用的数据仍然留在湖中，并占用本应为其他活动所空闲的空间。陈旧的数据也往往存在缺失和不一致性，并在最终被使用时引发难以解决的错误。因此，在摄取过程中遵循以下最佳实践非常重要，您可以在摄取过程中利用这些实践：
- en: File format
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 文件格式
- en: 'There are several file formats that can be leveraged that come with pros and
    cons. If the primary goal is readability, CSV, JSON, and XML are the most common
    ones. If instead performance is the most relevant goal, then Avro, Parquet, or
    Optimized Row Columnar (ORC) are the ones that suit better:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种文件格式可以利用，各有优缺点。如果主要目标是可读性，则CSV、JSON和XML是最常见的格式。如果性能是最重要的目标，则Avro、Parquet或优化行列（ORC）更为合适：
- en: Avro (row-based format) works better when intense I/O and low-latency operations
    are required (e.g., event-based source, streaming dashboards).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当需要强大的I/O和低延迟操作时（例如基于事件的源、流式仪表盘），Avro（基于行的格式）表现更好。
- en: Parquet and ORC (columnar-based format) are more suitable for query use case.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet和ORC（基于列的格式）更适合查询使用情况。
- en: File size
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 文件大小
- en: Typically, in the Hadoop world, *larger is better*. Files usually have a size
    of dozens of gigabytes, and when working with very small files (e.g., IoT use
    case scenario), it is a good idea to leverage a streaming engine (such as Apache
    Beam or Spark Streaming) to consolidate the data into a few large files.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在Hadoop世界中，*越大越好*。文件通常具有数十GB的大小，当处理非常小的文件（例如IoT使用场景）时，利用流引擎（如Apache Beam或Spark
    Streaming）将数据合并为少量大文件是一个好主意。
- en: Data compression
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 数据压缩
- en: Data compression is incredibly important to save space, especially in data lakes
    that are potentially ingesting petabytes of data every single day. Additionally,
    to guarantee performance it is crucial to select a compression algorithm that
    is fast enough to compress/decompress the data on the fly, saving a decent amount
    of space. The standard ZIP compression algorithm is not generally suitable for
    such applications, and what we are seeing is that organizations tend to leverage
    Snappy, an open source algorithm developed by Google that provides performance
    aligned with data lakes’ needs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 数据压缩对于节省空间非常重要，特别是在每天可能摄取PB级数据的数据湖中。此外，为了保证性能，选择一个能够在运行时快速压缩/解压数据的压缩算法至关重要，从而节省大量空间。标准的ZIP压缩算法通常不适合此类应用，我们看到组织倾向于利用由Google开发的Snappy，这是一个开源算法，提供与数据湖需求对齐的性能。
- en: Directory structure
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 目录结构
- en: 'This is a very important topic because, based on the use case and on the process
    engine to be leveraged, your directory structure can vary a lot. Let’s take as
    an example an IoT use case handled via HBase: imagine you are collecting data
    from devices globally distributed and you want to extract information of a specific
    day from messages generated by devices in a specific location. One good example
    of directory structure might be */country/city/device_id/year/month/day/hours/minute/second.*
    If instead the use case is a batch operation, it may be useful to put the input
    data in a folder called IN and the output data in a folder called OUT (of course,
    it has to be identified with the best possible prefix to avoid misunderstanding
    and data duplication).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常重要的话题，因为根据用例和要利用的处理引擎，你的目录结构可能会有很大变化。举个例子，通过HBase处理的物联网用例：假设你正在全球各地的设备中收集数据，并且想要从特定位置的设备生成的消息中提取特定日期的信息。一个好的目录结构示例可能是*/country/city/device_id/year/month/day/hours/minute/second*。如果用例是批处理操作，将输入数据放入名为IN的文件夹中，并将输出数据放入名为OUT的文件夹中可能会很有用（当然，必须用最佳前缀标识以避免误解和数据重复）。
- en: 'Depending on the nature of the data (batch versus streaming) and the target
    of the data (e.g., AWS S3, Azure Data Lake Storage, Google Cloud Storage), it
    will be possible to leverage several solutions: users can load the data leveraging
    scripts (running in PowerShell or bash, for example), or they can ingest the data
    directly using APIs or native connectors. Users can stream the data directly into
    the platform (e.g., AWS Kinesis, Azure Event Hub, or Google Pub/Sub), or they
    can upload it as a raw file into HDFS for future processing. IT departments usually
    manage these activities because they can involve a lot of automation and at the
    same time a lot of integration/data processing that require ad hoc skills. However,
    there are solutions like Fivetran that are simplifying the way users can configure
    data ingestion into the cloud. For such a solution, even less-skilled people (e.g.,
    business users) could potentially load data in the lake, extending the democratization
    concept we discussed earlier.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据的性质（批处理与流处理）和数据的目标（如AWS S3、Azure Data Lake Storage、Google Cloud Storage），可以利用多种解决方案：用户可以使用脚本（例如PowerShell或bash）加载数据，也可以使用API或本地连接器直接将数据摄入。用户可以将数据直接流入平台（如AWS
    Kinesis、Azure Event Hub或Google Pub/Sub），或者将其作为原始文件上传到HDFS以供将来处理。IT部门通常管理这些活动，因为它们可能涉及大量自动化以及需要专门技能的集成/数据处理。然而，像Fivetran这样的解决方案正在简化用户配置数据摄入到云端的方式。对于这样的解决方案，甚至较少技术技能的人（例如业务用户）也可能加载数据到数据湖中，扩展我们之前讨论过的民主化概念。
- en: Now that you have seen how to democratize access to the data, let’s see from
    a high-level view how data lakes can facilitate the training and prediction of
    ML algorithms. You will learn more about that in [Chapter 11](ch11.html#architecting_an_ml_platform).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何使数据访问民主化，让我们从高层次来看看数据湖如何促进机器学习算法的训练和预测。你将在[第11章](ch11.html#architecting_an_ml_platform)中了解更多相关内容。
- en: ML in the Data Lake
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖中的机器学习
- en: As we discussed before, it is possible to store any type of data in its raw
    format in a data lake—this is unlike a DWH, where data needs to be structured
    or semistructured. In particular, it is possible to store unstructured data (images,
    videos, natural language, etc.) in their typical formats (JPEG, MPEG, PDF, etc.).
    This makes it extremely convenient to ingest data of all types into the data lake,
    which can then be used to develop, train, and predict ML algorithms, especially
    the ones based on deep learning (refer to [“Applying AI”](ch01.html#applying_ai)).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论过的，数据湖可以存储任何类型的数据以其原始格式存在，这与数据仓库不同，数据仓库中的数据需要结构化或半结构化。特别是，可以存储非结构化数据（图片、视频、自然语言等）以其典型格式（JPEG、MPEG、PDF等）存储。这使得将各种类型的数据引入数据湖变得非常方便，然后可以用于开发、训练和预测机器学习算法，特别是基于深度学习的算法（参见[“应用AI”](ch01.html#applying_ai)）。
- en: Training on Raw Data
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于原始数据的训练
- en: The typical ML framework you would use varies based on the type of data. For
    structured data, you can leverage libraries like Spark, XGBoost, and LightGBM.
    These frameworks can directly read and process data in CSV files, which can be
    stored as-is in a data lake. For unstructured data, the most commonly used ML
    frameworks are TensorFlow and PyTorch. These frameworks can read most image and
    video formats in their native form, which is the form that the raw data will be
    stored in. Thus, in [Figure 5-9](#steps_of_training_ml_models), where we walk
    through the steps in training ML models, the prepared data can be identical to
    the collected and stored data, and the labels can be part of the data itself—either
    a column in the CSV files or based on how images/videos are organized (for example,
    all images of screws can be stored in a folder named *screws*). This makes training
    ML models on a data lake extremely straightforward.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用的典型 ML 框架取决于数据的类型。对于结构化数据，可以利用类似 Spark、XGBoost 和 LightGBM 的库。这些框架可以直接读取和处理
    CSV 文件中的数据，而这些文件可以原样存储在数据湖中。对于非结构化数据，最常用的 ML 框架是 TensorFlow 和 PyTorch。这些框架可以以其原生形式读取大多数图像和视频格式，而原始数据也将以这种形式存储。因此，在
    [图 5-9](#steps_of_training_ml_models) 中，我们详细介绍了 ML 模型训练步骤时，准备的数据可以与收集和存储的数据完全相同，标签可以作为数据的一部分——可以是
    CSV 文件中的一列，也可以基于图像/视频的组织方式（例如，所有螺丝图像可以存储在名为 *screws* 的文件夹中）。这使得在数据湖上训练 ML 模型变得非常简单。
- en: '![Steps of training ML models](assets/adml_0509.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![ML 模型训练步骤](assets/adml_0509.png)'
- en: Figure 5-9\. Steps of training ML models
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. ML 模型训练步骤
- en: There are some efficiency considerations, however—directly reading JPEG or MPEG
    files will lead to the ML training programs being I/O bound. Therefore, the data
    is often extracted and stored in formats such as TensorFlow Records in the data
    lake. Because these formats optimize for throughput when reading from cloud storage,
    they help maximize GPU utilization. GPU manufacturers also provide capabilities
    to read common formats, such as Apache Arrow, directly from cloud storage into
    GPU memory, thus speeding up the ML process.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还需要考虑一些效率问题——直接读取 JPEG 或 MPEG 文件将导致 ML 训练程序受到 I/O 限制。因此，数据通常会被提取并以 TensorFlow
    Records 等格式存储在数据湖中。这些格式优化了从云存储中读取时的吞吐量，有助于最大化 GPU 利用率。GPU 制造商还提供了能力，可以直接从云存储读取常见格式，如
    Apache Arrow，到 GPU 内存中，从而加速 ML 过程。
- en: Predicting in the Data Lake
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在数据湖中进行预测
- en: Because the ML frameworks support directly reading data in the raw formats,
    invoking models on the raw data is also very straightforward. For example, in
    [Figure 5-10](#ml_inference_carried_out_on_an_image_up), we show you an image
    classification workflow on AWS. Note how the image is ingested as-is to the cloud
    storage bucket and the ML model is invoked on the uploaded image. Similar capabilities
    are available in GCP and Azure as well, which we’ll cover more extensively in
    [Chapter 11](ch11.html#architecting_an_ml_platform).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 ML 框架直接支持原始格式数据的读取，因此在原始数据上调用模型也非常简单。例如，在 [图 5-10](#ml_inference_carried_out_on_an_image_up)
    中，我们展示了在 AWS 上的图像分类工作流程。请注意图像原封不动地被摄入到云存储桶中，并且 ML 模型在上传的图像上被调用。类似的功能也在 GCP 和 Azure
    中提供，我们将在 [第 11 章](ch11.html#architecting_an_ml_platform) 中更详细地讨论。
- en: '![ML inference carried out on an image uploaded into a data lake on AWS](assets/adml_0510.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![在 AWS 上上传到数据湖中的图像进行的 ML 推断](assets/adml_0510.png)'
- en: Figure 5-10\. ML inference carried out on an image uploaded into a data lake
    on AWS
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-10\. 在 AWS 上上传到数据湖中的图像进行的 ML 推断
- en: If choosing a data lake as a primary platform, also consider using MLflow, an
    open source project, to implement end-to-end ML workflows. The data is stored
    in the data lake, and distributed training is typically carried out using Spark,
    although integrations with TensorFlow and PyTorch also exist. Besides training
    and deployment, MLflow also supports advanced features such as model registries
    and feature stores. This capability to directly process raw data in ML frameworks
    is one of the compelling advantages of data lakes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择数据湖作为主要平台，还应考虑使用开源项目 MLflow 来实现端到端的 ML 工作流程。数据存储在数据湖中，通常使用 Spark 进行分布式训练，尽管也存在与
    TensorFlow 和 PyTorch 的集成。除了训练和部署外，MLflow 还支持高级功能，如模型注册表和特征存储。在 ML 框架中直接处理原始数据的能力是数据湖的一个引人注目的优势之一。
- en: Summary
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter you have seen a little more in detail what a real data lake
    is, what the challenges are, and the patterns that you can adopt to make it the
    core pillar of the data platform of an organization. The key takeaways are as
    follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您更详细地了解了真实数据湖的概念，挑战以及可以采用的模式，使其成为组织数据平台的核心支柱。关键点如下：
- en: Data plays a key role in every organization to help make robust decisions within
    a short time frame and possibly in real time.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据在每个组织中发挥关键作用，以帮助在短时间内做出健壮的决策，可能是实时的。
- en: A data lake provides more flexibility than a DWH because it allows users to
    play with any kind of data (i.e., structured, semistructured, and even unstructured)
    and to leverage a wide variety of solutions and tools coming from the open source
    ecosystems.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖比数据仓库更灵活，因为它允许用户处理任何类型的数据（即结构化、半结构化甚至非结构化数据），并利用来自开源生态系统的各种解决方案和工具。
- en: Organizations struggled in managing data lakes in old on-premises environments.
    Cloud environments are a great solution for organizations to store data, reduce
    TCO, and focus on business value instead of hardware management.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织在旧的本地环境中管理数据湖时遇到了困难。云环境是组织存储数据、降低TCO并专注于业务价值而非硬件管理的良好解决方案。
- en: 'The main advantages to cloud adoption are: (1) reduce the TCO by saving money
    on storage and compute; (2) obtain scalability by leveraging the reach of hyperscalers;
    (3) adopt a fail-fast approach to speed up experimentations by leveraging the
    elasticity of the underlying platform; and (4) benefit from a consistent data
    governance approach in the platform across several products to comply with security
    and control requirements.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云采用的主要优势包括：（1）通过节省存储和计算成本来降低TCO；（2）通过利用超大规模的扩展性来获得可扩展性；（3）采用快速失败的方法，通过利用底层平台的弹性加快实验速度；以及（4）通过平台上一致的数据治理方法从多个产品中获益，以符合安全性和控制要求。
- en: In data lakes there is a repository of all the metadata that describes the datasets
    of the organization, which will help data scientists, data engineers, business
    analysts, and any users who may need to leverage data to find a path toward the
    desired dataset.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据湖中有描述组织数据集的所有元数据存储库，这将帮助数据科学家、数据工程师、业务分析师以及任何可能需要利用数据找到所需数据集路径的用户。
- en: Data lakes enable batch and streaming operations and facilitate the implementation
    of Lambda/Kappa patterns.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖实现了批处理和流处理操作，并促进了Lambda/Kappa模式的实施。
- en: The market is strongly investing in data lakes, particularly in those that are
    cloud based. So a data lake can be extended by leveraging APIs or integration
    with third-party solutions.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场正在大力投资于数据湖，特别是基于云的数据湖。因此，可以通过利用API或与第三方解决方案集成来扩展数据湖。
- en: One common integration is the adoption of Iceberg or Delta Lake on top of HDFS
    to make the storage ACID compliant and enable other kinds of use cases (mainly
    those that request a strong consistency in data).
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个常见的集成是在HDFS顶部采用Iceberg或Delta Lake，使存储符合ACID，并启用其他类型的用例（主要是那些要求数据强一致性的用例）。
- en: Jupyter Notebook is the most common approach used to implement a data analysis,
    experimentation, and test-based approach within organizations.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter Notebook是在组织内实现数据分析、实验和基于测试方法的最常见途径。
- en: Data lakes facilitate data democratization access within the organization because
    the majority of the activity can be carried out on a self-service basis.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖促进了组织内数据民主化的访问，因为大部分活动可以基于自助服务进行。
- en: Because data is stored in native formats in a data lake, and because ML frameworks
    support reading these formats, data lakes facilitate ML without any special hooks.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为数据在数据湖中以原生格式存储，并且因为ML框架支持读取这些格式，所以数据湖可以在没有任何特殊钩子的情况下促进ML。
- en: In the following chapter, we will look at the alternative to a data lake, a
    data warehouse.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨数据湖的替代方案，即数据仓库。
