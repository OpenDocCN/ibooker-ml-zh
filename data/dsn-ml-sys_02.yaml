- en: Chapter 2\. Introduction to Machine Learning Systems Design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。机器学习系统设计介绍
- en: Now that we’ve walked through an overview of ML systems in the real world, we
    can get to the fun part of actually designing an ML system. To reiterate from
    the first chapter, ML systems design takes a system approach to MLOps, which means
    that we’ll consider an ML system holistically to ensure that all the components—the
    business requirements, the data stack, infrastructure, deployment, monitoring,
    etc.—and their stakeholders can work together to satisfy the specified objectives
    and requirements.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经浏览了现实世界中机器学习系统的概述，我们可以进入实际设计机器学习系统的有趣部分。再次强调，机器学习系统设计采用了一种系统化的方法，这意味着我们将全面考虑机器学习系统，以确保所有组成部分——业务需求、数据堆栈、基础设施、部署、监控等等——及其利益相关者能够共同工作，以满足指定的目标和要求。
- en: We’ll start the chapter with a discussion on objectives. Before we develop an
    ML system, we must understand why this system is needed. If this system is built
    for a business, it must be driven by business objectives, which will need to be
    translated into ML objectives to guide the development of ML models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从讨论目标开始这一章节。在我们开发机器学习系统之前，我们必须理解为什么需要这个系统。如果这个系统是为了业务而建立的，那么它必须以业务目标为驱动，这些目标将需要转化为机器学习目标，以指导机器学习模型的开发。
- en: 'Once everyone is on board with the objectives for our ML system, we’ll need
    to set out some requirements to guide the development of this system. In this
    book, we’ll consider the four requirements: reliability, scalability, maintainability,
    and adaptability. We will then introduce the iterative process for designing systems
    to meet those requirements.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦大家对我们的机器学习系统的目标达成共识，我们将需要制定一些需求来指导该系统的开发。在本书中，我们将考虑四个要求：可靠性、可扩展性、可维护性和可适应性。然后我们将介绍用于设计系统以满足这些要求的迭代过程。
- en: 'You might wonder: with all these objectives, requirements, and processes in
    place, can I finally start building my ML model yet? Not so soon! Before using
    ML algorithms to solve your problem, you first need to frame your problem into
    a task that ML can solve. We’ll continue this chapter with how to frame your ML
    problems. The difficulty of your job can change significantly depending on how
    you frame your problem.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：有了这么多目标、要求和流程，我可以开始构建我的机器学习模型了吗？还不太快！在使用机器学习算法解决问题之前，您首先需要将问题框定为机器学习可以解决的任务。我们将继续讨论如何框定您的机器学习问题。根据您如何框定问题，您的工作难度可能会发生显著变化。
- en: 'Because ML is a data-driven approach, a book on ML systems design will be amiss
    if it fails to discuss the importance of data in ML systems. The last part of
    this chapter touches on a debate that has consumed much of the ML literature in
    recent years: which is more important—data or intelligent algorithms?'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习是一种数据驱动的方法，如果一本讨论机器学习系统设计的书籍未能讨论数据在机器学习系统中的重要性，那么将是遗漏的。本章的最后部分涉及近年来占据了大量机器学习文献的辩论：什么更重要——数据还是智能算法？
- en: Let’s get started!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Business and ML Objectives
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务与机器学习目标
- en: 'We first need to consider the objectives of the proposed ML projects. When
    working on an ML project, data scientists tend to care about the ML objectives:
    the metrics they can measure about the performance of their ML models such as
    accuracy, F1 score, inference latency, etc. They get excited about improving their
    model’s accuracy from 94% to 94.2% and might spend a ton of resources—data, compute,
    and engineering time—to achieve that.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要考虑提议的机器学习项目的目标。在开展机器学习项目时，数据科学家们倾向于关注机器学习目标：他们可以衡量其机器学习模型性能的指标，例如准确率、F1分数、推断延迟等等。他们对将模型的准确率从94%提高到94.2%感到兴奋，并可能投入大量资源——数据、计算和工程时间来实现这一目标。
- en: 'But the truth is: most companies don’t care about the fancy ML metrics. They
    don’t care about increasing a model’s accuracy from 94% to 94.2% unless it moves
    some business metrics. A pattern I see in many short-lived ML projects is that
    the data scientists become too focused on hacking ML metrics without paying attention
    to business metrics. Their managers, however, only care about business metrics
    and, after failing to see how an ML project can help push their business metrics,
    kill the projects prematurely (and possibly let go of the data science team involved).^([1](ch02.xhtml#ch01fn35))'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但事实是：大多数公司不关心花哨的机器学习指标。他们不在乎将模型的准确性从94%提高到94.2%，除非这些变化影响了某些业务指标。我在许多短命机器学习项目中看到的一个模式是，数据科学家过于专注于突破机器学习指标，而忽视了业务指标。然而，他们的管理者只关心业务指标，如果看不到机器学习项目如何帮助推动他们的业务指标，就会过早终止项目（并可能解雇涉及的数据科学团队）。^([1](ch02.xhtml#ch01fn35))
- en: So what metrics do companies care about? While most companies want to convince
    you otherwise, the sole purpose of businesses, according to the Nobel-winning
    economist Milton Friedman, is to maximize profits for shareholders.^([2](ch02.xhtml#ch01fn36))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么公司关心哪些指标呢？尽管大多数公司想让你相信其他理由，但根据诺贝尔奖获得者经济学家米尔顿·弗里德曼的说法，企业的唯一目的是为股东最大化利润。^([2](ch02.xhtml#ch01fn36))
- en: 'The ultimate goal of any project within a business is, therefore, to increase
    profits, either directly or indirectly: directly such as increasing sales (conversion
    rates) and cutting costs; indirectly such as higher customer satisfaction and
    increasing time spent on a website.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，商业项目的最终目标是直接或间接地增加利润：直接的方式如增加销售（转化率）和降低成本；间接的方式如提高客户满意度和增加在网站上的停留时间。
- en: For an ML project to succeed within a business organization, it’s crucial to
    tie the performance of an ML system to the overall business performance. What
    business performance metrics is the new ML system supposed to influence, e.g.,
    the amount of ads revenue, the number of monthly active users?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在商业组织内成功实施一个机器学习项目，将机器学习系统的表现与整体业务表现紧密联系起来至关重要。新的机器学习系统应该影响哪些业务绩效指标，例如广告收入的数量，月活跃用户的数量？
- en: Imagine that you work for an ecommerce site that cares about purchase-through
    rate and you want to move your recommender system from batch prediction to online
    prediction.^([3](ch02.xhtml#ch01fn37)) You might reason that online prediction
    will enable recommendations more relevant to users right now, which can lead to
    a higher purchase-through rate. You can even do an experiment to show that online
    prediction can improve your recommender system’s predictive accuracy by *X*% and,
    historically on your site, each percent increase in the recommender system’s predictive
    accuracy led to a certain increase in purchase-through rate.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你在一家关注购买转化率的电子商务网站工作，你希望将推荐系统从批量预测转移到在线预测。^([3](ch02.xhtml#ch01fn37)) 你可能会认为在线预测将使得当前用户更相关的推荐变得可能，从而提高购买转化率。你甚至可以进行实验，以显示在线预测可以提高你的推荐系统预测准确性约*X*%，而在你的网站上，推荐系统预测准确性的每一次百分比增加都导致购买转化率的某种增加。
- en: 'One of the reasons why predicting ad click-through rates and fraud detection
    are among the most popular use cases for ML today is that it’s easy to map ML
    models’ performance to business metrics: every increase in click-through rate
    results in actual ad revenue, and every fraudulent transaction stopped results
    in actual money saved.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 预测广告点击率和欺诈检测成为当今机器学习最受欢迎的应用案例之一的原因之一是，可以轻松将机器学习模型的表现映射到业务指标：点击率的每次增加都会导致实际广告收入的增加，每次阻止欺诈交易都会导致实际节省的资金。
- en: 'Many companies create their own metrics to map business metrics to ML metrics.
    For example, Netflix measures the performance of their recommender system using
    *take-rate*: the number of quality plays divided by the number of recommendations
    a user sees.^([4](ch02.xhtml#ch01fn38)) The higher the take-rate, the better the
    recommender system. Netflix also put a recommender system’s take-rate in the context
    of their other business metrics like total streaming hours and subscription cancellation
    rate. They found that a higher take-rate also results in higher total streaming
    hours and lower subscription cancellation rates.^([5](ch02.xhtml#ch01fn39))'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司创建自己的指标，将业务指标映射到机器学习指标上。例如，Netflix使用“接受率”来衡量推荐系统的性能：优质播放量除以用户看到的推荐数目。[^4]
    接受率越高，推荐系统越好。Netflix还将推荐系统的接受率与总流媒体小时数和订阅取消率等其他业务指标联系在一起。他们发现，更高的接受率也导致了更高的总流媒体小时数和较低的订阅取消率。[^5]
- en: The effect of an ML project on business objectives can be hard to reason about.
    For example, an ML model that gives customers more personalized solutions can
    make them happier, which makes them spend more money on your services. The same
    ML model can also solve their problems faster, which makes them spend less money
    on your services.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习项目对业务目标的影响可能很难理解。例如，一个为客户提供更个性化解决方案的机器学习模型可能会使客户更满意，进而使他们在你的服务上花更多钱。同样的机器学习模型也可以更快地解决他们的问题，从而使他们在你的服务上花费更少的钱。
- en: To gain a definite answer on the question of how ML metrics influence business
    metrics, experiments are often needed. Many companies do that with experiments
    like A/B testing and choose the model that leads to better business metrics, regardless
    of whether this model has better ML metrics.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得关于机器学习指标如何影响业务指标的确切答案，通常需要进行实验。许多公司通过A/B测试等实验来进行这些实验，并选择导致更好业务指标的模型，而不管这些模型是否具有更好的机器学习指标。
- en: Yet, even rigorous experiments might not be sufficient to understand the relationship
    between an ML model’s outputs and business metrics. Imagine you work for a cybersecurity
    company that detects and stops security threats, and ML is just a component in
    their complex process. An ML model is used to detect anomalies in the traffic
    pattern. These anomalies then go through a logic set (e.g., a series of if-else
    statements) that categorizes whether they constitute potential threats. These
    potential threats are then reviewed by security experts to determine whether they
    are actual threats. Actual threats will then go through another, different process
    aimed at stopping them. When this process fails to stop a threat, it might be
    impossible to figure out whether the ML component has anything to do with it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使进行了严格的实验，也可能不足以理解机器学习模型的输出与业务指标之间的关系。想象一下，你为一家检测和阻止安全威胁的网络安全公司工作，机器学习只是其复杂流程的一个组成部分。机器学习模型用于检测流量模式中的异常。然后，这些异常经过一套逻辑设置（例如一系列if-else语句）分类，以确定它们是否构成潜在威胁。安全专家会审核这些潜在威胁，以确定它们是否真正构成威胁。如果这一过程未能阻止威胁，可能无法确定机器学习组件是否与此有关。
- en: Many companies like to say that they use ML in their systems because “being
    AI-powered” alone already helps them attract customers, regardless of whether
    the AI part actually does anything useful.^([6](ch02.xhtml#ch01fn40))
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司喜欢说他们在系统中使用机器学习，因为“仅仅是AI驱动”就可以帮助他们吸引客户，而不管这部分AI是否真正有用。[^6]
- en: When evaluating ML solutions through the business lens, it’s important to be
    realistic about the expected returns. Due to all the hype surrounding ML, generated
    both by the media and by practitioners with a vested interest in ML adoption,
    some companies might have the notion that ML can magically transform their businesses
    overnight.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过业务视角评估机器学习解决方案时，对预期收益保持现实态度非常重要。由于媒体和有利于机器学习采纳的从业者们的炒作，围绕机器学习存在大量炒作，一些公司可能认为机器学习能够神奇般地在一夜之间改变他们的业务。
- en: 'Magically: possible. Overnight: no.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 神奇地：可能。一夜之间：不可能。
- en: There are many companies that have seen payoffs from ML. For example, ML has
    helped Google search better, sell more ads at higher prices, improve translation
    quality, and build better Android applications. But this gain hardly happened
    overnight. Google has been investing in ML for decades.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司已经看到了机器学习的回报。例如，ML已经帮助谷歌改善搜索质量、以更高的价格出售广告、提高翻译质量以及构建更好的安卓应用。但这些收益并非一夜之间实现的。谷歌已经在机器学习领域投资了几十年。
- en: Returns on investment in ML depend a lot on the maturity stage of adoption.
    The longer you’ve adopted ML, the more efficient your pipeline will run, the faster
    your development cycle will be, the less engineering time you’ll need, and the
    lower your cloud bills will be, which all lead to higher returns. According to
    a 2020 survey by Algorithmia, among companies that are more sophisticated in their
    ML adoption (having had models in production for over five years), almost 75%
    can deploy a model in under 30 days. Among those just getting started with their
    ML pipeline, 60% take over 30 days to deploy a model (see [Figure 2-1](#how_long_it_takes_for_a_company_to_brin)).^([7](ch02.xhtml#ch01fn41))
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习投资的回报很大程度上取决于采用成熟度阶段。你采用机器学习的时间越长，你的流水线效率就会越高，开发周期就会越快，你需要的工程时间就会越少，云账单也会越低，这些都会导致更高的回报。根据Algorithmia在2020年进行的调查，对于那些在机器学习采纳方面更为成熟的公司（已经将模型投入生产超过五年），几乎有75%的公司能够在30天内部署一个模型。而那些刚开始启动他们的机器学习流水线的公司中，60%的公司需要超过30天来部署一个模型（参见[图2-1](#how_long_it_takes_for_a_company_to_brin)）。^([7](ch02.xhtml#ch01fn41))
- en: '![](Images/dmls_0201.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0201.png)'
- en: 'Figure 2-1\. How long it takes for a company to bring a model to production
    is proportional to how long it has used ML. Source: Adapted from an image by Algorithmia'
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。公司将模型投入生产所需的时间与其使用机器学习的时间成正比。来源：Algorithmia图像经过调整
- en: Requirements for ML Systems
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习系统的要求
- en: 'We can’t say that we’ve successfully built an ML system without knowing what
    requirements the system has to satisfy. The specified requirements for an ML system
    vary from use case to use case. However, most systems should have these four characteristics:
    reliability, scalability, maintainability, and adaptability. We’ll walk through
    each of these concepts in detail. Let’s take a closer look at reliability first.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能说我们已经成功地构建了一个机器学习系统，而不知道这个系统必须满足什么要求。指定机器学习系统的要求因用例而异。然而，大多数系统应该具备这四个特性：可靠性、可扩展性、可维护性和适应性。我们将详细讨论每一个概念。让我们首先仔细看看可靠性。
- en: Reliability
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠性
- en: The system should continue to perform the correct function at the desired level
    of performance even in the face of adversity (hardware or software faults, and
    even human error).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统应该在面对逆境（硬件或软件故障，甚至人为错误）时，仍然能够以所需的性能水平继续执行正确的功能。
- en: “Correctness” might be difficult to determine for ML systems. For example, your
    system might call the predict function—e.g., `model.predict()`—correctly, but
    the predictions are wrong. How do we know if a prediction is wrong if we don’t
    have ground truth labels to compare it with?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: “正确性”对于机器学习系统可能很难确定。例如，你的系统可能正确调用了预测函数，比如`model.predict()`，但预测结果却是错误的。如果我们没有地面真实标签来比较，我们如何知道预测是否错误呢？
- en: With traditional software systems, you often get a warning, such as a system
    crash or runtime error or 404\. However, ML systems can fail silently. End users
    don’t even know that the system has failed and might have kept on using it as
    if it were working. For example, if you use Google Translate to translate a sentence
    into a language you don’t know, it might be very hard for you to tell even if
    the translation is wrong. We’ll discuss how ML systems fail in production in [Chapter 8](ch08.xhtml#data_distribution_shifts_and_monitoring).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用传统软件系统时，通常会收到警告，比如系统崩溃、运行时错误或404。然而，机器学习系统可能会默默失败。最终用户甚至不知道系统已经失败，可能会继续使用它，就好像它正在工作一样。例如，如果你使用Google翻译将一句话翻译成你不懂的语言，你可能很难分辨翻译是否错误。我们将在[第8章](ch08.xhtml#data_distribution_shifts_and_monitoring)讨论机器学习系统在生产中的失败情况。
- en: Scalability
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: There are multiple ways an ML system can grow. It can grow in complexity. Last
    year you used a logistic regression model that fit into an Amazon Web Services
    (AWS) free tier instance with 1 GB of RAM, but this year, you switched to a 100-million-parameter
    neural network that requires 16 GB of RAM to generate predictions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统可以通过多种方式增长。它可以在复杂性上增长。去年你使用了一个逻辑回归模型，它适合Amazon Web Services（AWS）的免费套餐实例，具有1
    GB的RAM，但今年，你切换到一个需要16 GB RAM才能生成预测的1亿参数神经网络模型。
- en: Your ML system can grow in traffic volume. When you started deploying an ML
    system, you only served 10,000 prediction requests daily. However, as your company’s
    user base grows, the number of prediction requests your ML system serves daily
    fluctuates between 1 million and 10 million.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你的机器学习系统可以在流量量上增长。当你开始部署一个机器学习系统时，你每天只处理1万个预测请求。然而，随着公司用户基数的增长，你的机器学习系统每天服务的预测请求数量在100万到1000万之间波动。
- en: An ML system might grow in ML model count. Initially, you might have only one
    model for one use case, such as detecting the trending hashtags on a social network
    site like Twitter. However, over time, you want to add more features to this use
    case, so you’ll add one more to filter out NSFW (not safe for work) content and
    another model to filter out tweets generated by bots. This growth pattern is especially
    common in ML systems that target enterprise use cases. Initially, a startup might
    serve only one enterprise customer, which means this startup only has one model.
    However, as this startup gains more customers, they might have one model for each
    customer. A startup I worked with had 8,000 models in production for their 8,000
    enterprise customers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统可能在ML模型数量上增长。最初，您可能只有一个用例的一个模型，例如检测社交网络（如Twitter）上流行的标签。然而，随着时间的推移，您希望为这个用例添加更多功能，因此您将添加一个用于过滤不安全内容（NSFW）的模型，以及另一个用于过滤机器生成的推文的模型。这种增长模式在面向企业用例的ML系统中特别常见。最初，一家初创公司可能只为一个企业客户提供服务，这意味着这家初创公司只有一个模型。然而，随着这家初创公司获得更多客户，他们可能为每个客户都有一个模型。我曾与一家初创公司合作，他们在生产环境中有8000个模型，对应其8000个企业客户。
- en: Whichever way your system grows, there should be reasonable ways of dealing
    with that growth. When talking about scalability most people think of resource
    scaling, which consists of up-scaling (expanding the resources to handle growth)
    and down-scaling (reducing the resources when not needed).^([8](ch02.xhtml#ch01fn42))
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您的系统如何增长，都应该有合理的处理增长的方式。谈论可扩展性时，大多数人会考虑资源扩展，包括上扩展（扩展资源以处理增长）和下扩展（在不需要时减少资源）。^([8](ch02.xhtml#ch01fn42))
- en: For example, at peak, your system might require 100 GPUs (graphics processing
    units). However, most of the time, it needs only 10 GPUs. Keeping 100 GPUs up
    all the time can be costly, so your system should be able to scale down to 10
    GPUs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在高峰期，您的系统可能需要100个GPU（图形处理单元）。然而，大部分时间只需要10个GPU。保持100个GPU一直开启可能成本高昂，因此您的系统应该能够缩减至10个GPU。
- en: 'An indispensable feature in many cloud services is autoscaling: automatically
    scaling up and down the number of machines depending on usage. This feature can
    be tricky to implement. Even Amazon fell victim to this when their autoscaling
    feature failed on Prime Day, causing their system to crash. An hour of downtime
    was estimated to cost Amazon between $72 million and $99 million.^([9](ch02.xhtml#ch01fn43))'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 许多云服务中不可或缺的功能是自动扩展：根据使用情况自动增加或减少机器数量。这一功能可能难以实现。即使是亚马逊在Prime Day时也遇到了这个问题，导致系统崩溃。据估计，亚马逊每小时的停机时间可能造成7200万至9900万美元的损失。^([9](ch02.xhtml#ch01fn43))
- en: However, handling growth isn’t just resource scaling, but also artifact management.
    Managing one hundred models is very different from managing one model. With one
    model, you can, perhaps, manually monitor this model’s performance and manually
    update the model with new data. Since there’s only one model, you can just have
    a file that helps you reproduce this model whenever needed. However, with one
    hundred models, both the monitoring and retraining aspect will need to be automated.
    You’ll need a way to manage the code generation so that you can adequately reproduce
    a model when you need to.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，应对增长不仅仅是资源扩展，还包括工件管理。管理一百个模型与管理一个模型大不相同。对于一个模型，您可以手动监控其性能并手动更新新数据。由于只有一个模型，您只需拥有一个文件，在需要时帮助您重新生成此模型。然而，对于一百个模型，监控和重新训练方面都需要自动化。您需要一种管理代码生成的方式，以便在需要时能够充分复制模型。
- en: Because scalability is such an important topic throughout the ML project workflow,
    we’ll discuss it in different parts of the book. Specifically, we’ll touch on
    the resource scaling aspect in the section [“Distributed Training”](ch06.xhtml#distributed_training),
    the section [“Model optimization”](ch07.xhtml#model_optimization), and the section
    [“Resource Management”](ch10.xhtml#resource_management). We’ll discuss the artifact
    management aspect in the section [“Experiment Tracking and Versioning”](ch06.xhtml#experiment_tracking_and_versioning)
    and the section [“Development Environment”](ch10.xhtml#development_environment).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因为可扩展性在整个机器学习项目工作流中非常重要，我们将在本书的不同部分讨论它。具体来说，我们将在章节["分布式训练"](ch06.xhtml#distributed_training)中触及资源扩展方面，在章节["模型优化"](ch07.xhtml#model_optimization)中，在章节["资源管理"](ch10.xhtml#resource_management)中进行讨论。我们将在章节["实验追踪和版本管理"](ch06.xhtml#experiment_tracking_and_versioning)中讨论工件管理方面，以及在章节["开发环境"](ch10.xhtml#development_environment)中。
- en: Maintainability
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可维护性
- en: There are many people who will work on an ML system. They are ML engineers,
    DevOps engineers, and subject matter experts (SMEs). They might come from very
    different backgrounds, with very different programming languages and tools, and
    might own different parts of the process.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多人会参与ML系统的工作。他们是ML工程师、DevOps工程师和主题专家（SME）。他们可能来自非常不同的背景，使用非常不同的编程语言和工具，并可能拥有流程的不同部分。
- en: It’s important to structure your workloads and set up your infrastructure in
    such a way that different contributors can work using tools that they are comfortable
    with, instead of one group of contributors forcing their tools onto other groups.
    Code should be documented. Code, data, and artifacts should be versioned. Models
    should be sufficiently reproducible so that even when the original authors are
    not around, other contributors can have sufficient contexts to build on their
    work. When a problem occurs, different contributors should be able to work together
    to identify the problem and implement a solution without finger-pointing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要结构化你的工作负载，并建立基础设施，使不同的贡献者可以使用他们熟悉的工具，而不是让一组贡献者强迫其他组使用他们的工具。代码应该被文档化。代码、数据和工件应该被版本化。模型应该具备足够的可再现性，以便即使原始作者不在身边，其他贡献者也能够有足够的背景来构建他们的工作。当出现问题时，不同的贡献者应该能够共同合作，识别问题并实施解决方案，而不是互相指责。
- en: We’ll go more into this in the section [“Team Structure”](ch11.xhtml#team_structure).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[“团队结构”](ch11.xhtml#team_structure)部分进一步讨论这个问题。
- en: Adaptability
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适应性
- en: To adapt to shifting data distributions and business requirements, the system
    should have some capacity for both discovering aspects for performance improvement
    and allowing updates without service interruption.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应数据分布和业务需求的变化，系统应具备一定的能力，既能发现性能改进的方面，又能在不中断服务的情况下进行更新。
- en: Because ML systems are part code, part data, and data can change quickly, ML
    systems need to be able to evolve quickly. This is tightly linked to maintainability.
    We’ll discuss changing data distributions in the section [“Data Distribution Shifts”](ch08.xhtml#data_distribution_shifts),
    and how to continually update your model with new data in the section [“Continual
    Learning”](ch09.xhtml#continual_learning).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因为ML系统既是代码部分，又是数据部分，而数据可能会快速变化，所以ML系统需要能够快速演变。这与可维护性密切相关。我们将在[“数据分布变化”](ch08.xhtml#data_distribution_shifts)部分讨论数据分布的变化，以及如何在[“持续学习”](ch09.xhtml#continual_learning)部分不断更新您的模型。
- en: Iterative Process
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代过程
- en: Developing an ML system is an iterative and, in most cases, never-ending process.^([10](ch02.xhtml#ch01fn44))
    Once a system is put into production, it’ll need to be continually monitored and
    updated.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 开发一个ML系统是一个迭代的过程，在大多数情况下是永无止境的。^([10](ch02.xhtml#ch01fn44)) 一旦系统投入生产，就需要持续监控和更新。
- en: Before deploying my first ML system, I thought the process would be linear and
    straightforward. I thought all I had to do was to collect data, train a model,
    deploy that model, and be done. However, I soon realized that the process looks
    more like a cycle with a lot of back and forth between different steps.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署我的第一个ML系统之前，我认为这个过程会是线性和简单的。我以为我只需要收集数据、训练模型、部署模型，就完成了。然而，我很快意识到这个过程更像是一个循环，不同步骤之间来回反复。
- en: For example, here is one workflow that you might encounter when building an
    ML model to predict whether an ad should be shown when users enter a search query:^([11](ch02.xhtml#ch01fn45))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在构建一个用于预测用户输入搜索查询时是否应显示广告的ML模型时，你可能会遇到以下工作流程:^([11](ch02.xhtml#ch01fn45))
- en: Choose a metric to optimize. For example, you might want to optimize for impressions—the
    number of times an ad is shown.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个优化指标。例如，你可能想优化印象数——广告显示的次数。
- en: Collect data and obtain labels.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集数据并获取标签。
- en: Engineer features.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工程化特征。
- en: Train models.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: During error analysis, you realize that errors are caused by the wrong labels,
    so you relabel the data.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在错误分析中，你意识到错误是由错误的标签引起的，因此你重新标记数据。
- en: Train the model again.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次训练模型。
- en: During error analysis, you realize that your model always predicts that an ad
    shouldn’t be shown, and the reason is because 99.99% of the data you have have
    NEGATIVE labels (ads that shouldn’t be shown). So you have to collect more data
    of ads that should be shown.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在错误分析过程中，你意识到你的模型总是预测不应该显示广告，原因是你的数据中有99.99%的数据都是负标签（不应该显示的广告）。因此，你需要收集更多应该展示的广告数据。
- en: Train the model again.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次训练模型。
- en: The model performs well on your existing test data, which is by now two months
    old. However, it performs poorly on the data from yesterday. Your model is now
    stale, so you need to update it on more recent data.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型在你现有的测试数据上表现良好，这些数据已经两个月没有更新了。然而，它在昨天的数据上表现不佳。你的模型现在已经过时，因此需要用更新的数据来更新它。
- en: Train the model again.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次训练模型。
- en: Deploy the model.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署模型。
- en: The model seems to be performing well, but then the businesspeople come knocking
    on your door asking why the revenue is decreasing. It turns out the ads are being
    shown, but few people click on them. So you want to change your model to optimize
    for ad click-through rate instead.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型看起来表现良好，但商业人士却找上门来问为什么收入在减少。原来广告确实展示出来了，但点击率很低。因此，你希望改变你的模型，以优化广告点击率。
- en: Go to step 1.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到步骤1。
- en: '[Figure 2-2](#the_process_of_developing_an_ml_system) shows an oversimplified
    representation of what the iterative process for developing ML systems in production
    looks like from the perspective of a data scientist or an ML engineer. This process
    looks different from the perspective of an ML platform engineer or a DevOps engineer,
    as they might not have as much context into model development and might spend
    a lot more time on setting up infrastructure.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-2](#the_process_of_developing_an_ml_system)展示了从数据科学家或ML工程师的角度看，开发ML系统的迭代过程的简化表示。从ML平台工程师或DevOps工程师的角度来看，这个过程可能会有所不同，因为他们可能没有那么多关于模型开发的背景，并且可能会花更多时间设置基础设施。'
- en: '![](Images/dmls_0202.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0202.png)'
- en: Figure 2-2\. The process of developing an ML system looks more like a cycle
    with a lot of back and forth between steps
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. 开发ML系统的过程更像是一个循环，各个步骤之间来回交替。
- en: 'Later chapters will dive deeper into what each of these steps requires in practice.
    Here, let’s take a brief look at what they mean:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 后续章节将深入探讨每个步骤在实践中的具体要求。在这里，让我们简要地看看它们的含义：
- en: Step 1\. Project scoping
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤1\. 项目范围
- en: A project starts with scoping the project, laying out goals, objectives, and
    constraints. Stakeholders should be identified and involved. Resources should
    be estimated and allocated. We already discussed different stakeholders and some
    of the foci for ML projects in production in [Chapter 1](ch01.xhtml#overview_of_machine_learning_systems).
    We also already discussed how to scope an ML project in the context of a business
    earlier in this chapter. We’ll discuss how to organize teams to ensure the success
    of an ML project in [Chapter 11](ch11.xhtml#the_human_side_of_machine_learning).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 项目始于明确项目范围，制定目标、目的和限制条件。应识别并参与相关利益相关者。应估算和分配资源。我们已经在[第1章](ch01.xhtml#overview_of_machine_learning_systems)中讨论了ML项目生产中的不同利益相关者和一些重点。我们也已经在本章前面的商业背景下讨论了如何在ML项目中确定范围。我们将在[第11章](ch11.xhtml#the_human_side_of_machine_learning)中讨论如何组织团队以确保ML项目的成功。
- en: Step 2\. Data engineering
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2\. 数据工程
- en: A vast majority of ML models today learn from data, so developing ML models
    starts with engineering data. In [Chapter 3](ch03.xhtml#data_engineering_fundamentals),
    we’ll discuss the fundamentals of data engineering, which covers handling data
    from different sources and formats. With access to raw data, we’ll want to curate
    training data out of it by sampling and generating labels, which is discussed
    in [Chapter 4](ch04.xhtml#training_data).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当今大多数ML模型都是从数据中学习的，因此开发ML模型始于数据工程。在[第3章](ch03.xhtml#data_engineering_fundamentals)中，我们将讨论数据工程的基础知识，涵盖了处理来自不同来源和格式的数据。有了原始数据的访问权限，我们希望通过采样和生成标签来筛选训练数据，这在[第4章](ch04.xhtml#training_data)中有所讨论。
- en: Step 3\. ML model development
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤3\. ML模型开发
- en: With the initial set of training data, we’ll need to extract features and develop
    initial models leveraging these features. This is the stage that requires the
    most ML knowledge and is most often covered in ML courses. In [Chapter 5](ch05.xhtml#feature_engineering),
    we’ll discuss feature engineering. In [Chapter 6](ch06.xhtml#model_development_and_offline_evaluatio),
    we’ll discuss model selection, training, and evaluation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最初的训练数据，我们需要提取特征并开发初步模型，利用这些特征。这是需要最多ML知识的阶段，也是ML课程中经常涵盖的内容。在[第5章](ch05.xhtml#feature_engineering)中，我们将讨论特征工程。在[第6章](ch06.xhtml#model_development_and_offline_evaluatio)中，我们将讨论模型选择、训练和评估。
- en: Step 4\. Deployment
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤4\. 部署
- en: After a model is developed, it needs to be made accessible to users. Developing
    an ML system is like writing—you will never reach the point when your system is
    done. But you do reach the point when you have to put your system out there. We’ll
    discuss different ways to deploy an ML model in [Chapter 7](ch07.xhtml#model_deployment_and_prediction_service).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 开发完模型后，需要使其对用户可访问。开发机器学习系统就像写作一样——你永远不会达到系统完成的时刻。但你确实会达到需要将系统投入使用的时刻。我们将在第 [7](ch07.xhtml#model_deployment_and_prediction_service)
    章讨论不同的机器学习模型部署方式。
- en: Step 5\. Monitoring and continual learning
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第 5 步。监控与持续学习
- en: Once in production, models need to be monitored for performance decay and maintained
    to be adaptive to changing environments and changing requirements. This step will
    be discussed in Chapters [8](ch08.xhtml#data_distribution_shifts_and_monitoring)
    and [9](ch09.xhtml#continual_learning_and_test_in_producti).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型投入生产后，需要监控其性能衰减，并进行维护以适应不断变化的环境和需求。这一步骤将在第 [8](ch08.xhtml#data_distribution_shifts_and_monitoring)
    章和第 [9](ch09.xhtml#continual_learning_and_test_in_producti) 章讨论。
- en: Step 6\. Business analysis
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第 6 步。业务分析
- en: Model performance needs to be evaluated against business goals and analyzed
    to generate business insights. These insights can then be used to eliminate unproductive
    projects or scope out new projects. This step is closely related to the first
    step.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能需要根据业务目标进行评估，并进行分析以生成业务洞见。这些洞见随后可以用来消除无效项目或规划新项目的范围。这一步与第一步密切相关。
- en: Framing ML Problems
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 框定机器学习问题
- en: Imagine you’re an ML engineering tech lead at a bank that targets millennial
    users. One day, your boss hears about a rival bank that uses ML to speed up their
    customer service support that supposedly helps the rival bank process their customer
    requests two times faster. He orders your team to look into using ML to speed
    up your customer service support too.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一家以年轻用户为目标的银行的机器学习工程技术负责人。有一天，你的老板听说竞争对手银行正在利用机器学习加快客户服务支持速度，据说可以使竞争对手银行的客户请求处理速度提高两倍。他命令你的团队也研究利用机器学习来加速你们的客户服务支持。
- en: Slow customer support is a problem, but it’s not an ML problem. An ML problem
    is defined by inputs, outputs, and the objective function that guides the learning
    process—none of these three components are obvious from your boss’s request. It’s
    your job, as a seasoned ML engineer, to use your knowledge of what problems ML
    can solve to frame this request as an ML problem.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 缓慢的客户支持是一个问题，但这不是一个机器学习的问题。机器学习问题由输入、输出和指导学习过程的目标函数定义——而你老板的请求中这三个组成部分都不明显。作为一名经验丰富的机器学习工程师，你的工作是利用你对机器学习可以解决的问题的了解，将这个请求框定为一个机器学习问题。
- en: 'Upon investigation, you discover that the bottleneck in responding to customer
    requests lies in routing customer requests to the right department among four
    departments: accounting, inventory, HR (human resources), and IT. You can alleviate
    this bottleneck by developing an ML model to predict which of these four departments
    a request should go to. This makes it a classification problem. The input is the
    customer request. The output is the department the request should go to. The objective
    function is to minimize the difference between the predicted department and the
    actual department.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 调查后，您发现响应客户请求的瓶颈在于将客户请求路由到四个部门中的正确部门：会计、库存、人力资源（HR）和信息技术（IT）。您可以通过开发一个机器学习模型来预测请求应该发送到这四个部门中的哪一个，从而缓解这一瓶颈。这使得它成为一个分类问题。输入是客户请求，输出是请求应该发送到的部门。目标函数是最小化预测部门与实际部门之间的差异。
- en: 'We’ll discuss extensively how to extract features from raw data to input into
    your ML model in [Chapter 5](ch05.xhtml#feature_engineering). In this section,
    we’ll focus on two aspects: the output of your model and the objective function
    that guides the learning process.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细讨论如何从原始数据中提取特征以输入到您的机器学习模型中，具体见[第 5 章](ch05.xhtml#feature_engineering)。在本节中，我们将重点关注您的模型的输出和指导学习过程的目标函数。
- en: Types of ML Tasks
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习任务类型
- en: The output of your model dictates the task type of your ML problem. The most
    general types of ML tasks are classification and regression. Within classification,
    there are more subtypes, as shown in [Figure 2-3](#common_task_types_in_ml). We’ll
    go over each of these task types.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型的输出决定了您的机器学习问题的任务类型。机器学习任务的最常见类型是分类和回归。在分类中，还有更多的子类型，如图[2-3](#common_task_types_in_ml)所示。我们将逐一讨论这些任务类型。
- en: '![](Images/dmls_0203.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0203.png)'
- en: Figure 2-3\. Common task types in ML
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3。机器学习中常见的任务类型
- en: Classification versus regression
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类与回归
- en: Classification models classify inputs into different categories. For example,
    you want to classify each email to be either spam or not spam. Regression models
    output a continuous value. An example is a house prediction model that outputs
    the price of a given house.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型将输入分类为不同的类别。例如，你想要将每封电子邮件分类为垃圾邮件或非垃圾邮件。回归模型则输出连续值。一个例子是预测给定房屋价格的房屋预测模型。
- en: A regression model can easily be framed as a classification model and vice versa.
    For example, house prediction can become a classification task if we quantize
    the house prices into buckets such as under $100,000, $100,000–$200,000, $200,000–$500,000,
    and so forth and predict the bucket the house should be in.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型可以轻松地转换为分类模型，反之亦然。例如，房屋预测可以成为分类任务，如果我们将房价量化为诸如小于10万美元、10万至20万美元、20万至50万美元等区间，并预测房屋应属于的区间。
- en: The email classification model can become a regression model if we make it output
    values between 0 and 1, and decide on a threshold to determine which values should
    be SPAM (for example, if the value is above 0.5, the email is spam), as shown
    in [Figure 2-4](#the_email_classification_task_can_also).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将电子邮件分类模型输出值转化为0到1之间的值，并决定一个阈值来确定哪些值应为垃圾邮件（例如，如果值大于0.5，则邮件为垃圾邮件），如[图2-4](#the_email_classification_task_can_also)所示，邮件分类模型也可以成为回归模型。
- en: '![](Images/dmls_0204.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0204.png)'
- en: Figure 2-4\. The email classification task can also be framed as a regression
    task
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. 邮件分类任务也可以作为回归任务来构建。
- en: Binary versus multiclass classification
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二元分类与多类分类
- en: Within classification problems, the fewer classes there are to classify, the
    simpler the problem is. The simplest is *binary classification*, where there are
    only two possible classes. Examples of binary classification include classifying
    whether a comment is toxic, whether a lung scan shows signs of cancer, whether
    a transaction is fraudulent. It’s unclear whether this type of problem is common
    in the industry because they are common in nature or simply because ML practitioners
    are most comfortable handling them.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，类别越少，问题就越简单。最简单的是*二元分类*，只有两种可能的类别。二元分类的例子包括判断评论是否有毒、肺部扫描是否显示癌症迹象、交易是否欺诈等。目前尚不清楚这种类型的问题在工业界是否普遍存在，因为它们在本质上普遍存在还是因为机器学习从业者更习惯处理这些问题。
- en: When there are more than two classes, the problem becomes *multiclass classification*.
    Dealing with binary classification problems is much easier than dealing with multiclass
    problems. For example, calculating F1 and visualizing confusion matrices are a
    lot more intuitive when there are only two classes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在两个以上的类别时，问题变为*多类分类*。处理二分类问题比处理多类问题要简单得多。例如，在只有两个类别时，计算F1分数和可视化混淆矩阵更加直观。
- en: When the number of classes is high, such as disease diagnosis where the number
    of diseases can go up to thousands or product classifications where the number
    of products can go up to tens of thousands, we say the classification task has
    *high cardinality*. High cardinality problems can be very challenging. The first
    challenge is in data collection. In my experience, ML models typically need at
    least 100 examples for each class to learn to classify that class. So if you have
    1,000 classes, you already need at least 100,000 examples. The data collection
    can be especially difficult for rare classes. When you have thousands of classes,
    it’s likely that some of them are rare.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当类别数量很高时，比如疾病诊断，疾病数量可能达到上千种，或者产品分类，产品数量可能达到数万种，我们称该分类任务具有*高基数*。高基数问题可能非常具有挑战性。首先挑战在于数据收集。根据我的经验，机器学习模型通常需要至少100个示例来学习分类该类别。因此，如果你有1000个类别，你至少需要10万个示例。对于稀有类别，数据收集尤其困难。当你有数千个类别时，很可能其中一些是稀有的。
- en: 'When the number of classes is large, hierarchical classification might be useful.
    In hierarchical classification, you have a classifier to first classify each example
    into one of the large groups. Then you have another classifier to classify this
    example into one of the subgroups. For example, for product classification, you
    can first classify each product into one of the four main categories: electronics,
    home and kitchen, fashion, or pet supplies. After a product has been classified
    into a category, say fashion, you can use another classifier to put this product
    into one of the subgroups: shoes, shirts, jeans, or accessories.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当类别数量较多时，分层分类可能会有用。在分层分类中，首先有一个分类器将每个例子分类为大类中的一类。然后有另一个分类器将这个例子分类为子类中的一类。例如，对于产品分类，可以首先将每个产品分类为四个主要类别之一：电子产品、家居厨房用品、时尚服饰或宠物用品。当产品被分类为某一类别，比如时尚服饰，可以使用另一个分类器将此产品分类为鞋子、衬衫、牛仔裤或配饰中的一种。
- en: Multiclass versus multilabel classification
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多类别与多标签分类
- en: In both binary and multiclass classification, each example belongs to exactly
    one class. When an example can belong to multiple classes, we have a *multilabel
    classification* problem. For example, when building a model to classify articles
    into four topics—tech, entertainment, finance, and politics—an article can be
    in both tech and finance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类和多类别分类中，每个例子属于且仅属于一个类别。当一个例子可以属于多个类别时，我们面临一个*多标签分类*问题。例如，在构建一个将文章分类为四个主题——技术、娱乐、财经和政治的模型时，一篇文章可以同时属于技术和财经。
- en: There are two major approaches to multilabel classification problems. The first
    is to treat it as you would a multiclass classification. In multiclass classification,
    if there are four possible classes [tech, entertainment, finance, politics] and
    the label for an example is entertainment, you represent this label with the vector
    [0, 1, 0, 0]. In multilabel classification, if an example has both labels entertainment
    and finance, its label will be represented as [0, 1, 1, 0].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 解决多标签分类问题的两种主要方法。第一种方法是将其视为多类别分类。在多类别分类中，如果有四个可能的类别 [技术, 娱乐, 财经, 政治]，并且一个例子的标签是娱乐，则用向量
    [0, 1, 0, 0] 表示此标签。在多标签分类中，如果一个例子同时具有娱乐和财经两个标签，则其标签表示为 [0, 1, 1, 0]。
- en: The second approach is to turn it into a set of binary classification problems.
    For the article classification problem, you can have four models corresponding
    to four topics, each model outputting whether an article is in that topic or not.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是将其转化为一组二元分类问题。对于文章分类问题，可以有四个模型对应四个主题，每个模型输出文章是否属于该主题。
- en: Out of all task types, multilabel classification is usually the one that I’ve
    seen companies having the most problems with. Multilabel means that the number
    of classes an example can have varies from example to example. First, this makes
    it difficult for label annotation since it increases the label multiplicity problem
    that we discuss in [Chapter 4](ch04.xhtml#training_data). For example, an annotator
    might believe an example belongs to two classes while another annotator might
    believe the same example to belong in only one class, and it might be difficult
    resolving their disagreements.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有任务类型中，我通常看到公司最多遇到问题的是多标签分类。多标签意味着一个例子可以具有的类别数从例子到例子不同。首先，这增加了标签注释的难度，因为它增加了我们在[第四章](ch04.xhtml#training_data)讨论的标签多重性问题。例如，一个标注者可能认为一个例子属于两个类别，而另一个标注者可能认为同一个例子只属于一个类别，解决他们的分歧可能很困难。
- en: 'Second, this varying number of classes makes it hard to extract predictions
    from raw probability. Consider the same task of classifying articles into four
    topics. Imagine that, given an article, your model outputs this raw probability
    distribution: [0.45, 0.2, 0.02, 0.33]. In the multiclass setting, when you know
    that an example can belong to only one category, you simply pick the category
    with the highest probability, which is 0.45 in this case. In the multilabel setting,
    because you don’t know how many categories an example can belong to, you might
    pick the two highest probability categories (corresponding to 0.45 and 0.33) or
    three highest probability categories (corresponding to 0.45, 0.2, and 0.33).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，这种不断变化的类别数量使得从原始概率中提取预测变得困难。考虑将文章分类为四个主题的相同任务。想象一下，给定一篇文章，您的模型输出了这个原始概率分布：[0.45,
    0.2, 0.02, 0.33]。在多类别设置中，当您知道一个示例只能属于一个类别时，您简单地选择概率最高的类别，本例中为0.45。在多标签设置中，因为您不知道一个示例可以属于多少个类别，您可能选择两个最高概率的类别（对应于0.45和0.33）或三个最高概率的类别（对应于0.45，0.2和0.33）。
- en: Multiple ways to frame a problem
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 改变问题框架的多种方式
- en: Changing the way you frame your problem might make your problem significantly
    harder or easier. Consider the task of predicting what app a phone user wants
    to use next. A naive setup would be to frame this as a multiclass classification
    task—use the user’s and environment’s features (user demographic information,
    time, location, previous apps used) as input, and output a probability distribution
    for every single app on the user’s phone. Let *N* be the number of apps you want
    to consider recommending to a user. In this framing, for a given user at a given
    time, there is only one prediction to make, and the prediction is a vector of
    the size *N*. This setup is visualized in [Figure 2-5](#given_the_problem_of_predicting_the_ap).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 修改问题框架的方式可能会使问题变得更加困难或更容易。考虑预测用户下一次想使用的应用程序的任务。一个天真的设置可能是将其作为多类别分类任务来框定 - 使用用户和环境的特征（用户人口统计信息，时间，位置，以前使用的应用程序）作为输入，并为用户手机上的每个应用程序输出一个概率分布。让*N*表示您要考虑向用户推荐的应用的数量。在这种框架中，对于特定用户在特定时间，只有一个要进行的预测，并且预测是大小为*N*的向量。这种设置在[图 2-5](#given_the_problem_of_predicting_the_ap)中可视化。
- en: '![](Images/dmls_0205.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/dmls_0205.png)'
- en: Figure 2-5\. Given the problem of predicting the app a user will most likely
    open next, you can frame it as a classification problem. The input is the user’s
    features and environment’s features. The output is a distribution over all apps
    on the phone.
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 鉴于预测用户下次最有可能打开的应用程序的问题，您可以将其作为分类问题来框定。输入是用户的特征和环境的特征。输出是手机上所有应用程序的分布。
- en: This is a bad approach because whenever a new app is added, you might have to
    retrain your model from scratch, or at least retrain all the components of your
    model whose number of parameters depends on *N*. A better approach is to frame
    this as a regression task. The input is the user’s, the environment’s, and the
    app’s features. The output is a single value between 0 and 1; the higher the value,
    the more likely the user will open the app given the context. In this framing,
    for a given user at a given time, there are *N* predictions to make, one for each
    app, but each prediction is just a number. This improved setup is visualized in
    [Figure 2-6](#given_the_problem_of_predicting_the_app).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种不好的方法，因为每当添加新的应用程序时，您可能需要从头开始重新训练模型，或者至少重新训练所有参数数量取决于*N*的模型组件。一个更好的方法是将其作为回归任务来框定。输入是用户，环境和应用程序的特征。输出是介于0和1之间的单个值；值越高，用户打开应用的可能性就越大。在这种框架中，对于特定用户在特定时间，有*N*个要进行的预测，每个应用程序一个，但每个预测只是一个数字。这种改进的设置在[图 2-6](#given_the_problem_of_predicting_the_app)中可视化。
- en: '![](Images/dmls_0206.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/dmls_0206.png)'
- en: Figure 2-6\. Given the problem of predicting the app a user will most likely
    open next, you can frame it as a regression problem. The input is the user’s features,
    environment’s features, and an app’s features. The output is a single value between
    0 and 1 denoting how likely the user will be to open the app given the context.
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 鉴于预测用户下次最有可能打开的应用程序的问题，您可以将其作为回归问题来框定。输入是用户的特征，环境的特征和应用程序的特征。输出是介于0和1之间的单个值，表示用户在给定上下文中打开应用的可能性有多大。
- en: In this new framing, whenever there’s a new app you want to consider recommending
    to a user, you simply need to use new inputs with this new app’s feature instead
    of having to retrain your model or part of your model from scratch.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种新的框架中，每当有一个新的应用程序您想考虑推荐给用户时，您只需使用新应用程序的特性来替代旧的输入，而不必从头开始重新训练您的模型或模型的一部分。
- en: Objective Functions
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标函数
- en: To learn, an ML model needs an objective function to guide the learning process.^([12](ch02.xhtml#ch01fn46))
    An objective function is also called a loss function, because the objective of
    the learning process is usually to minimize (or optimize) the loss caused by wrong
    predictions. For supervised ML, this loss can be computed by comparing the model’s
    outputs with the ground truth labels using a measurement like root mean squared
    error (RMSE) or cross entropy.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要学习，ML模型需要一个目标函数来指导学习过程。[^12] 目标函数也称为损失函数，因为学习过程的目标通常是通过减少（或优化）由错误预测引起的损失来实现的。对于监督学习，可以通过使用像均方根误差（RMSE）或交叉熵这样的测量方法，通过将模型的输出与地面真实标签进行比较来计算这种损失。
- en: 'To illustrate this point, let’s again go back to the previous task of classifying
    articles into four topics [tech, entertainment, finance, politics]. Consider an
    article that belongs to the politics class, e.g., its ground truth label is [0,
    0, 0, 1]. Imagine that, given this article, your model outputs this raw probability
    distribution: [0.45, 0.2, 0.02, 0.33]. The cross entropy loss of this model, given
    this example, is the cross entropy of [0.45, 0.2, 0.02, 0.33] relative to [0,
    0, 0, 1]. In Python, you can calculate cross entropy with the following code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们再次回到将文章分类为四个主题[技术、娱乐、金融、政治]的前一任务。考虑一篇属于政治类的文章，例如其地面真实标签为[0, 0, 0,
    1]。假设给定这篇文章，您的模型输出的原始概率分布是[0.45, 0.2, 0.02, 0.33]。对于这个例子，这个模型的交叉熵损失是相对于[0, 0,
    0, 1]的[0.45, 0.2, 0.02, 0.33]的交叉熵。在Python中，您可以使用以下代码计算交叉熵：
- en: '[PRE0]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Choosing an objective function is usually straightforward, though not because
    objective functions are easy. Coming up with meaningful objective functions requires
    algebra knowledge, so most ML engineers just use common loss functions like RMSE
    or MAE (mean absolute error) for regression, logistic loss (also log loss) for
    binary classification, and cross entropy for multiclass classification.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通常选择目标函数是很直接的，尽管不是因为目标函数很容易。提出有意义的目标函数需要代数知识，所以大多数ML工程师只使用像RMSE或MAE（平均绝对误差）用于回归，逻辑损失（也称为对数损失）用于二元分类，以及交叉熵用于多类分类这样的常见损失函数。
- en: Decoupling objectives
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解耦目标
- en: 'Framing ML problems can be tricky when you want to minimize multiple objective
    functions. Imagine you’re building a system to rank items on users’ newsfeeds.
    Your original goal is to maximize users’ engagement. You want to achieve this
    goal through the following three objectives:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当您希望最小化多个目标函数时，构建ML问题可能会很棘手。想象一下，您正在构建一个系统来排列用户新闻提要中的项目。您最初的目标是最大化用户的参与度。您希望通过以下三个目标实现这一目标：
- en: Filter out spam
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤垃圾邮件
- en: Filter out NSFW content
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤成人内容
- en: 'Rank posts by engagement: how likely users will click on it'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据用户的参与度对帖子进行排名：用户点击的可能性有多大
- en: 'However, you quickly learned that optimizing for users’ engagement alone can
    lead to questionable ethical concerns. Because extreme posts tend to get more
    engagements, your algorithm learned to prioritize extreme content.^([13](ch02.xhtml#ch01fn47))
    You want to create a more wholesome newsfeed. So you have a new goal: maximize
    users’ engagement while minimizing the spread of extreme views and misinformation.
    To obtain this goal, you add two new objectives to your original plan:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您很快学到，仅优化用户参与度可能会引发疑问的伦理问题。因为极端的帖子往往能获得更多的参与度，您的算法学会了优先考虑极端内容。[^13] 您希望创建一个更全面的新闻提要。因此，您有了一个新的目标：在最大化用户参与度的同时，最小化极端观点和错误信息的传播。为了实现这个目标，您将两个新目标添加到您的原始计划中：
- en: Filter out spam
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤垃圾邮件
- en: Filter out NSFW content
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤成人内容
- en: Filter out misinformation
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤错误信息
- en: Rank posts by quality
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据质量对帖子进行排名
- en: 'Rank posts by engagement: how likely users will click on it'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据用户的参与度对帖子进行排名：用户点击的可能性有多大
- en: Now two objectives are in conflict with each other. If a post is engaging but
    it’s of questionable quality, should that post rank high or low?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在两个目标彼此冲突。如果一篇文章引人入胜，但质量可疑，那么这篇文章应该排名高还是低呢？
- en: 'An objective is represented by an objective function. To rank posts by quality,
    you first need to predict posts’ quality, and you want posts’ predicted quality
    to be as close to their actual quality as possible. Essentially, you want to minimize
    *quality_loss*: the difference between each post’s predicted quality and its true
    quality.^([14](ch02.xhtml#ch01fn48))'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 目标由目标函数表示。要按质量排列帖子，首先需要预测帖子的质量，希望预测的质量尽可能接近其实际质量。实质上，你希望最小化*quality_loss*：每篇帖子预测质量与实际质量之间的差异。^([14](ch02.xhtml#ch01fn48))
- en: 'Similarly, to rank posts by engagement, you first need to predict the number
    of clicks each post will get. You want to minimize *engagement_loss*: the difference
    between each post’s predicted clicks and its actual number of clicks.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，要按参与度排列帖子，首先需要预测每篇帖子将获得的点击数。你希望最小化*engagement_loss*：每篇帖子预测点击数与实际点击数之间的差异。
- en: 'One approach is to combine these two losses into one loss and train one model
    to minimize that loss:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是将这两种损失合并为一种损失，并训练一个模型来最小化该损失：
- en: '*loss* = *ɑ* *quality_loss* + *β* *engagement_loss*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*loss* = *ɑ* *quality_loss* + *β* *engagement_loss*'
- en: You can randomly test out different values of *α* and *β* to find the values
    that work best. If you want to be more systematic about tuning these values, you
    can check out Pareto optimization, “an area of multiple criteria decision making
    that is concerned with mathematical optimization problems involving more than
    one objective function to be optimized simultaneously.”^([15](ch02.xhtml#ch01fn49))
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可以随机测试不同的*α*和*β*值，找到最有效的值。如果想更系统地调整这些值，可以查看帕累托优化，“涉及多个同时优化的数学优化问题的多目标决策领域”。^([15](ch02.xhtml#ch01fn49))
- en: A problem with this approach is that each time you tune *α* and *β*—for example,
    if the quality of your users’ newsfeeds goes up but users’ engagement goes down,
    you might want to decrease *α* and increase *β*—you’ll have to retrain your model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个问题是，每次调整*α*和*β*时——例如，如果用户新闻源的质量提高但用户的参与度降低，你可能希望减少*α*并增加*β*——都需要重新训练模型。
- en: 'Another approach is to train two different models, each optimizing one loss.
    So you have two models:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是训练两个不同的模型，每个模型优化一个损失。因此，你有两个模型：
- en: quality_model
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: quality_model
- en: Minimizes *quality_loss* and outputs the predicted quality of each post
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化*quality_loss*并输出每篇帖子的预测质量
- en: engagement_model
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: engagement_model
- en: Minimizes *engagement_loss* and outputs the predicted number of clicks of each
    post
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化*engagement_loss*并输出每篇帖子预测的点击数
- en: 'You can combine the models’ outputs and rank posts by their combined scores:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 可以结合模型的输出，按其组合分数排列帖子：
- en: '*ɑ* *quality_score* + *β* *engagement_score*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*ɑ* *quality_score* + *β* *engagement_score*'
- en: Now you can tweak *α* and *β* without retraining your models!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以在不重新训练模型的情况下调整*α*和*β*！
- en: In general, when there are multiple objectives, it’s a good idea to decouple
    them first because it makes model development and maintenance easier. First, it’s
    easier to tweak your system without retraining models, as previously explained.
    Second, it’s easier for maintenance since different objectives might need different
    maintenance schedules. Spamming techniques evolve much faster than the way post
    quality is perceived, so spam filtering systems need updates at a much higher
    frequency than quality-ranking systems.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，在存在多个目标时，首先将它们解耦是一个好主意，因为这样可以更轻松地进行模型开发和维护。首先，可以在不重新训练模型的情况下轻松调整系统，如前所述。其次，由于不同的目标可能需要不同的维护计划，因此在维护方面也更容易。垃圾邮件技术的演变速度远快于帖子质量感知的方式，因此垃圾邮件过滤系统需要比质量排名系统更频繁地进行更新。
- en: Mind Versus Data
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 心灵与数据
- en: Progress in the last decade shows that the success of an ML system depends largely
    on the data it was trained on. Instead of focusing on improving ML algorithms,
    most companies focus on managing and improving their data.^([16](ch02.xhtml#custom_ch02fn1))
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 十年来的进展表明，ML系统的成功在很大程度上取决于其训练数据。大多数公司不是专注于改进ML算法，而是专注于管理和改进其数据。^([16](ch02.xhtml#custom_ch02fn1))
- en: Despite the success of models using massive amounts of data, many are skeptical
    of the emphasis on data as the way forward. In the last five years, at every academic
    conference I attended, there were always some public debates on the power of mind
    versus data. *Mind* might be disguised as inductive biases or intelligent architectural
    designs. *Data* might be grouped together with computation since more data tends
    to require more computation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用大量数据的模型取得了成功，但许多人对将数据视为前进的道路持怀疑态度。在我参加的每一场学术会议上，在过去五年中，总是有一些关于心力对抗数据力量的公开辩论。*心*可能被伪装为归纳偏见或智能建筑设计。*数据*可能与计算一样被归纳到一起，因为更多的数据往往需要更多的计算。
- en: In theory, you can both pursue architectural designs and leverage large data
    and computation, but spending time on one often takes time away from another.^([17](ch02.xhtml#ch01fn50))
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，你可以同时追求建筑设计和利用大数据和计算能力，但花在其中一个方面的时间往往会削弱另一个方面的发展。^([17](ch02.xhtml#ch01fn50))
- en: 'In the mind-over-data camp, there’s Dr. Judea Pearl, a Turing Award winner
    best known for his work on causal inference and Bayesian networks. The introduction
    to his book *The Book of Why* is entitled “Mind over Data,” in which he emphasizes:
    “Data is profoundly dumb.” In one of his more controversial posts on Twitter in
    2020, he expressed his strong opinion against ML approaches that rely heavily
    on data and warned that data-centric ML people might be out of a job in three
    to five years: “ML will not be the same in 3–5 years, and ML folks who continue
    to follow the current data-centric paradigm will find themselves outdated, if
    not jobless. Take note.”^([18](ch02.xhtml#ch01fn51))'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在“心胜于数据”的阵营中，有图灵奖获得者朱迪亚·珀尔博士，他以因果推断和贝叶斯网络的工作而著称。他的书《为什么》的介绍标题为“心胜于数据”，他在其中强调：“数据非常愚蠢。”在他2020年在Twitter上的一篇更具争议性的帖子中，他强烈反对依赖大量数据的机器学习方法，并警告称，依赖数据的机器学习人员可能在三到五年内失业：“机器学习在3-5年内将不再相同，如果继续遵循当前以数据为中心的范式，机器学习的人可能会过时，甚至失业。请注意。”^([18](ch02.xhtml#ch01fn51))
- en: There’s also a milder opinion from Professor Christopher Manning, director of
    the Stanford Artificial Intelligence Laboratory, who argued that huge computation
    and a massive amount of data with a simple learning algorithm create incredibly
    bad learners. The structure allows us to design systems that can learn more from
    less data.^([19](ch02.xhtml#ch01fn52))
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种温和的观点来自斯坦福人工智能实验室主任克里斯托弗·曼宁教授，他认为大量计算和海量数据加上简单的学习算法会导致非常糟糕的学习者。这种结构使我们能够设计能够从少量数据中学习更多的系统。^([19](ch02.xhtml#ch01fn52))
- en: 'Many people in ML today are in the data-over-mind camp. Professor Richard Sutton,
    a professor of computing science at the University of Alberta and a distinguished
    research scientist at DeepMind, wrote a great blog post in which he claimed that
    researchers who chose to pursue intelligent designs over methods that leverage
    computation will eventually learn a bitter lesson: “The biggest lesson that can
    be read from 70 years of AI research is that general methods that leverage computation
    are ultimately the most effective, and by a large margin.… Seeking an improvement
    that makes a difference in the shorter term, researchers seek to leverage their
    human knowledge of the domain, but the only thing that matters in the long run
    is the leveraging of computation.”^([20](ch02.xhtml#ch01fn53))'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当今许多机器学习领域的人都支持数据胜于心的观点。阿尔伯塔大学计算科学教授、DeepMind杰出研究科学家理查德·萨顿教授在一篇博客中写道，选择追求智能设计而非利用计算的研究人员最终会吃到苦果：“从70年的人工智能研究中可以得出的最重要的教训是，利用计算的通用方法最终是最有效的，效果要大得多……寻求在短期内取得改进的研究人员试图利用他们对领域的人类知识，但从长远来看，唯一重要的是利用计算。”^([20](ch02.xhtml#ch01fn53))
- en: 'When asked how Google Search was doing so well, Peter Norvig, Google’s director
    of search quality, emphasized the importance of having a large amount of data
    over intelligent algorithms in their success: “We don’t have better algorithms.
    We just have more data.”^([21](ch02.xhtml#ch01fn54))'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当被问及谷歌搜索表现如此出色的原因时，谷歌搜索质量总监彼得·诺维格强调了在他们成功中大数据的重要性而不是智能算法：“我们并没有更好的算法。我们只有更多的数据。”^([21](ch02.xhtml#ch01fn54))
- en: Dr. Monica Rogati, former VP of data at Jawbone, argued that data lies at the
    foundation of data science, as shown in [Figure 2-7](#the_data_science_hierarchy_of_needsmoni).
    If you want to use data science, a discipline of which ML is a part of, to improve
    your products or processes, you need to start with building out your data, both
    in terms of quality and quantity. Without data, there’s no data science.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Monica Rogati 博士，前 Jawbone 公司的数据副总裁，认为数据是数据科学的基础，如 [图 2-7](#the_data_science_hierarchy_of_needsmoni)
    所示。如果您希望利用数据科学（其中包括机器学习）来改进产品或流程，必须首先建立和扩展您的数据，无论是在质量还是数量上。没有数据，就没有数据科学。
- en: The debate isn’t about whether finite data is necessary, but whether it’s sufficient.
    The term *finite* here is important, because if we had infinite data, it might
    be possible for us to look up the answer. Having a lot of data is different from
    having infinite data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 辩论不在于有限数据是否必要，而在于其是否足够。这里的术语*有限*非常重要，因为如果我们有无限的数据，也许可以查找答案。拥有大量数据不同于拥有无限数据。
- en: '![](Images/dmls_0207.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0207.png)'
- en: 'Figure 2-7\. The data science hierarchy of needs. Source: Adapted from an image
    by Monica Rogati^([22](ch02.xhtml#ch01fn55))'
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7. 数据科学需求层次结构。来源：根据 Monica Rogati 的图片改编^([22](ch02.xhtml#ch01fn55))
- en: Regardless of which camp will prove to be right eventually, no one can deny
    that data is essential, for now. Both the research and industry trends in the
    recent decades show the success of ML relies more and more on the quality and
    quantity of data. Models are getting bigger and using more data. Back in 2013,
    people were getting excited when the One Billion Word Benchmark for Language Modeling
    was released, which contains 0.8 billion tokens.^([23](ch02.xhtml#ch01fn56)) Six
    years later, OpenAI’s GPT-2 used a dataset of 10 billion tokens. And another year
    later, GPT-3 used 500 billion tokens. The growth rate of the sizes of datasets
    is shown in [Figure 2-8](#the_size_of_the_datasets_left_parenthes).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 无论最终哪一方会被证明是正确的，都无人能否认数据目前是至关重要的。近几十年来，无论是研究还是行业趋势，都显示出机器学习的成功越来越依赖于数据的质量和数量。模型变得越来越大，并使用越来越多的数据。回到2013年，当发布了包含
    0.8 亿个标记的一亿字语言建模基准时，人们变得兴奋起来。六年后，OpenAI 的 GPT-2 使用了 100 亿个标记的数据集。又过了一年，GPT-3 使用了
    5000 亿个标记的数据集。数据集大小的增长速度如 [图 2-8](#the_size_of_the_datasets_left_parenthes) 所示。
- en: '![](Images/dmls_0208.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_0208.png)'
- en: Figure 2-8\. The size of the datasets (log scale) used for language models over
    time
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-8. 随时间推移语言模型使用的数据集大小（对数刻度）
- en: Even though much of the progress in deep learning in the last decade was fueled
    by an increasingly large amount of data, more data doesn’t always lead to better
    performance for your model. More data at lower quality, such as data that is outdated
    or data with incorrect labels, might even hurt your model’s performance.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管过去十年中深度学习的许多进展是由于数据量的增加，但更多的数据并不总是会提高模型的性能。低质量的更多数据，例如过时的数据或带有错误标签的数据，甚至可能损害模型的性能。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: I hope that this chapter has given you an introduction to ML systems design
    and the considerations we need to take into account when designing an ML system.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望本章为您介绍了机器学习系统设计以及在设计机器学习系统时需要考虑的因素。
- en: Every project must start with why this project needs to happen, and ML projects
    are no exception. We started the chapter with an assumption that most businesses
    don’t care about ML metrics unless they can move business metrics. Therefore,
    if an ML system is built for a business, it must be motivated by business objectives,
    which need to be translated into ML objectives to guide the development of ML
    models.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 每个项目都必须从为什么需要这个项目开始，机器学习项目也不例外。我们在本章开始时假设，除非可以推动业务指标，否则大多数企业不关心机器学习指标。因此，如果为企业构建了一个机器学习系统，必须由业务目标驱动，这些目标需要转化为机器学习目标，以指导机器学习模型的开发。
- en: 'Before building an ML system, we need to understand the requirements that the
    system needs to meet to be considered a good system. The exact requirements vary
    from use case to use case, and in this chapter, we focused on the four most general
    requirements: reliability, scalability, maintainability, and adaptability. Techniques
    to satisfy each of these requirements will be covered throughout the book.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建机器学习系统之前，我们需要了解系统需要满足的要求，以被视为良好系统。具体要求因用例而异，在本章中，我们专注于四个最一般的要求：可靠性、可扩展性、可维护性和适应性。满足每个要求的技术将在本书中进行详细介绍。
- en: Building an ML system isn’t a one-off task but an iterative process. In this
    chapter, we discussed the iterative process to develop an ML system that met those
    preceding requirements.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 建立机器学习系统不是一次性任务，而是一个迭代过程。在本章中，我们讨论了开发满足前述要求的机器学习系统的迭代过程。
- en: We ended the chapter on a philosophical discussion of the role of data in ML
    systems. There are still many people who believe that having intelligent algorithms
    will eventually trump having a large amount of data. However, the success of systems
    including AlexNet, BERT, and GPT showed that the progress of ML in the last decade
    relies on having access to a large amount of data.^([24](ch02.xhtml#custom_ch2fn2))
    Regardless of whether data can overpower intelligent design, no one can deny the
    importance of data in ML. A nontrivial part of this book will be devoted to shedding
    light on various data questions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一章结束时对数据在机器学习系统中的角色进行了哲学性的讨论。仍然有很多人相信拥有智能算法最终会超过拥有大量数据。然而，包括AlexNet、BERT和GPT在内的系统的成功表明，过去十年中机器学习的进展依赖于大量数据的获取^([24](ch02.xhtml#custom_ch2fn2))。无论数据是否能压倒智能设计，没有人能否认数据在机器学习中的重要性。本书的一个非平凡部分将专注于阐明各种数据问题。
- en: Complex ML systems are made up of simpler building blocks. Now that we’ve covered
    the high-level overview of an ML system in production, we’ll zoom in to its building
    blocks in the following chapters, starting with the fundamentals of data engineering
    in the next chapter. If any of the challenges mentioned in this chapter seem abstract
    to you, I hope that specific examples in the following chapters will make them
    more concrete.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的机器学习系统由更简单的构建块组成。现在我们已经介绍了生产中机器学习系统的高级概述，接下来的章节将详细讨论其构建块，从下一章的数据工程基础开始。如果本章提到的任何挑战对您来说显得抽象，希望以下章节中的具体示例能使其更加具体化。
- en: ^([1](ch02.xhtml#ch01fn35-marker)) Eugene Yan has [a great post](https://oreil.ly/thQCV)
    on how data scientists can understand the business intent and context of the projects
    they work on.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.xhtml#ch01fn35-marker)) Eugene Yan在他的[一篇很棒的文章](https://oreil.ly/thQCV)中解释了数据科学家如何理解他们参与的项目的业务意图和背景。
- en: ^([2](ch02.xhtml#ch01fn36-marker)) Milton Friedman, “A Friedman Doctrine—The
    Social Responsibility of Business Is to Increase Its Profits,” *New York Times
    Magazine,* September 13, 1970, [*https://oreil.ly/Fmbem*](https://oreil.ly/Fmbem).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.xhtml#ch01fn36-marker)) Milton Friedman，“弗里德曼主义——企业的社会责任是增加其利润”，*《纽约时报》杂志*，1970年9月13日，[*https://oreil.ly/Fmbem*](https://oreil.ly/Fmbem)。
- en: ^([3](ch02.xhtml#ch01fn37-marker)) We’ll cover batch prediction and online prediction
    in [Chapter 7](ch07.xhtml#model_deployment_and_prediction_service).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.xhtml#ch01fn37-marker)) 我们将在[第7章](ch07.xhtml#model_deployment_and_prediction_service)中介绍批处理预测和在线预测。
- en: ^([4](ch02.xhtml#ch01fn38-marker)) Ashok Chandrashekar, Fernando Amat, Justin
    Basilico, and Tony Jebara, “Artwork Personalization at Netflix,” *Netflix Technology
    Blog*, December 7, 2017, [*https://oreil.ly/UEDmw*](https://oreil.ly/UEDmw).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.xhtml#ch01fn38-marker)) Ashok Chandrashekar，Fernando Amat，Justin
    Basilico和Tony Jebara，“Netflix艺术品个性化”，*Netflix技术博客*，2017年12月7日，[*https://oreil.ly/UEDmw*](https://oreil.ly/UEDmw)。
- en: '^([5](ch02.xhtml#ch01fn39-marker)) Carlos A. Gomez-Uribe and Neil Hunt, “The
    Netflix Recommender System: Algorithms, Business Value, and Innovation,” *ACM
    Transactions on Management Information Systems* 6, no. 4 (January 2016): 13, [*https://oreil.ly/JkEPB*](https://oreil.ly/JkEPB).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch02.xhtml#ch01fn39-marker)) Carlos A. Gomez-Uribe和Neil Hunt，“Netflix推荐系统：算法、商业价值和创新”，*《ACM管理信息系统交易》*，2016年1月，13页，[*https://oreil.ly/JkEPB*](https://oreil.ly/JkEPB)。
- en: ^([6](ch02.xhtml#ch01fn40-marker)) Parmy Olson, “Nearly Half of All ‘AI Startups’
    Are Cashing In on Hype,” *Forbes*, March 4, 2019, [*https://oreil.ly/w5kOr*](https://oreil.ly/w5kOr).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch02.xhtml#ch01fn40-marker)) Parmy Olson，“几乎一半的‘AI初创企业’在炒作中获利”，*福布斯*，2019年3月4日，[*https://oreil.ly/w5kOr*](https://oreil.ly/w5kOr)。
- en: ^([7](ch02.xhtml#ch01fn41-marker)) “2020 State of Enterprise Machine Learning,”
    Algorithmia, 2020, [*https://oreil.ly/FlIV1*](https://oreil.ly/FlIV1).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch02.xhtml#ch01fn41-marker)) “2020年企业机器学习现状”，Algorithmia，2020年，[*https://oreil.ly/FlIV1*](https://oreil.ly/FlIV1)。
- en: '^([8](ch02.xhtml#ch01fn42-marker)) Up-scaling and down-scaling are two aspects
    of “scaling out,” which is different from “scaling up.” Scaling out is adding
    more equivalently functional components in parallel to spread out a load. Scaling
    up is making a component larger or faster to handle a greater load (Leah Schoeb,
    “Cloud Scalability: Scale Up vs Scale Out,” *Turbonomic Blog*, March 15, 2018,
    [*https://oreil.ly/CFPtb*](https://oreil.ly/CFPtb)).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch02.xhtml#ch01fn42-marker)) 上扩展和下扩展是“扩展”的两个方面，与“扩大”不同。扩展是并行添加更多等效功能组件以分散负载。扩大是使组件更大或更快以处理更大的负载（Leah
    Schoeb，“云可扩展性：扩展与扩大的比较”，*Turbonomic Blog*，2018年3月15日，[*https://oreil.ly/CFPtb*](https://oreil.ly/CFPtb)）。
- en: ^([9](ch02.xhtml#ch01fn43-marker)) Sean Wolfe, “Amazon’s One Hour of Downtime
    on Prime Day May Have Cost It up to $100 Million in Lost Sales,” *Business Insider*,
    July 19, 2018, [*https://oreil.ly/VBezI*](https://oreil.ly/VBezI).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch02.xhtml#ch01fn43-marker)) Sean Wolfe，“亚马逊在Prime Day的一小时停机可能导致高达1亿美元的销售损失”，*Business
    Insider*，2018年7月19日，[*https://oreil.ly/VBezI*](https://oreil.ly/VBezI)。
- en: ^([10](ch02.xhtml#ch01fn44-marker)) Which, as an early reviewer pointed out,
    is a property of traditional software.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch02.xhtml#ch01fn44-marker)) 早期评论者指出，这是传统软件的一个特性。
- en: ^([11](ch02.xhtml#ch01fn45-marker)) Praying and crying not featured, but present
    through the entire process.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch02.xhtml#ch01fn45-marker)) 虽然没有介绍祈祷和哭泣，但这些元素贯穿整个过程。
- en: ^([12](ch02.xhtml#ch01fn46-marker)) Note that objective functions are mathematical
    functions, which are different from the business and ML objectives we discussed
    earlier in this chapter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch02.xhtml#ch01fn46-marker)) 注意，客观函数是数学函数，与本章前面讨论的业务和ML目标不同。
- en: ^([13](ch02.xhtml#ch01fn47-marker)) Joe Kukura, “Facebook Employee Raises Powered
    by ‘Really Dangerous’ Algorithm That Favors Angry Posts,” *SFist*, September 24,
    2019, [*https://oreil.ly/PXtGi*](https://oreil.ly/PXtGi); Kevin Roose, “The Making
    of a YouTube Radical,” *New York Times*, June 8, 2019, [*https://oreil.ly/KYqzF*](https://oreil.ly/KYqzF).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch02.xhtml#ch01fn47-marker)) Joe Kukura，“Facebook员工加薪背后的‘非常危险’算法偏爱愤怒的帖子”，*SFist*，2019年9月24日，[*https://oreil.ly/PXtGi*](https://oreil.ly/PXtGi)；Kevin
    Roose，“YouTube极端分子的制造”，*纽约时报*，2019年6月8日，[*https://oreil.ly/KYqzF*](https://oreil.ly/KYqzF)。
- en: ^([14](ch02.xhtml#ch01fn48-marker)) For simplicity, let’s pretend for now that
    we know how to measure a post’s quality.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch02.xhtml#ch01fn48-marker)) 为简单起见，让我们暂时假设我们知道如何衡量帖子的质量。
- en: '^([15](ch02.xhtml#ch01fn49-marker)) Wikipedia, s.v. “Pareto optimization,”
    [*https://oreil.ly/NdApy*](https://oreil.ly/NdApy). While you’re at it, you might
    also want to read Jin and Sendhoff’s great paper on applying Pareto optimization
    for ML, in which the authors claimed that “machine learning is inherently a multiobjective
    task” (Yaochu Jin and Bernhard Sendhoff, “Pareto-Based Multiobjective Machine
    Learning: An Overview and Case Studies,” *IEEE Transactions on Systems, Man, and
    Cybernetics—Part C: Applications and Reviews* 38, no. 3 [May 2008], [*https://oreil.ly/f1aKk*](https://oreil.ly/f1aKk)).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '^([15](ch02.xhtml#ch01fn49-marker)) 维基百科，“Pareto优化”，[*https://oreil.ly/NdApy*](https://oreil.ly/NdApy)。此外，您可能还想阅读Jin和Sendhoff关于将Pareto优化应用于ML的优秀论文，其中作者声称“机器学习本质上是一个多目标任务”（Yaochu
    Jin和Bernhard Sendhoff，“基于Pareto的多目标机器学习：概述和案例研究”，*IEEE Transactions on Systems,
    Man, and Cybernetics—Part C: Applications and Reviews* 38, no. 3 [2008年5月]，[*https://oreil.ly/f1aKk*](https://oreil.ly/f1aKk)）。'
- en: ^([16](ch02.xhtml#custom_ch02fn1-marker)) Anand Rajaraman, “More Data Usually
    Beats Better Algorithms,” *Datawocky*, March 24, 2008, [*https://oreil.ly/wNwhV*](https://oreil.ly/wNwhV).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch02.xhtml#custom_ch02fn1-marker)) Anand Rajaraman，“更多数据通常胜过更好的算法”，*Datawocky*，2008年3月24日，[*https://oreil.ly/wNwhV*](https://oreil.ly/wNwhV)。
- en: ^([17](ch02.xhtml#ch01fn50-marker)) Rich Sutton, “The Bitter Lesson,” March
    13, 2019, [*https://oreil.ly/RhOp9*](https://oreil.ly/RhOp9).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch02.xhtml#ch01fn50-marker)) Rich Sutton，“苦涩的教训”，2019年3月13日，[*https://oreil.ly/RhOp9*](https://oreil.ly/RhOp9)。
- en: ^([18](ch02.xhtml#ch01fn51-marker)) Tweet by Dr. Judea Pearl (@yudapearl), September
    27, 2020, [*https://oreil.ly/wFbHb*](https://oreil.ly/wFbHb).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch02.xhtml#ch01fn51-marker)) 2020年9月27日，Judea Pearl博士（@yudapearl）的推文，[*https://oreil.ly/wFbHb*](https://oreil.ly/wFbHb)。
- en: ^([19](ch02.xhtml#ch01fn52-marker)) “Deep Learning and Innate Priors” (Chris
    Manning versus Yann LeCun debate), February 2, 2018, video, 1:02:55, [*https://oreil.ly/b3hb1*](https://oreil.ly/b3hb1).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch02.xhtml#ch01fn52-marker)) “深度学习与先验知识”（Chris Manning与Yann LeCun辩论），2018年2月2日，视频，1:02:55，[*https://oreil.ly/b3hb1*](https://oreil.ly/b3hb1)。
- en: ^([20](ch02.xhtml#ch01fn53-marker)) Sutton, “The Bitter Lesson.”
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch02.xhtml#ch01fn53-marker)) Sutton，“苦涩的教训”。
- en: ^([21](ch02.xhtml#ch01fn54-marker)) Alon Halevy, Peter Norvig, and Fernando
    Pereira, “The Unreasonable Effectiveness of Data,” *IEEE Computer Society*, March/April
    2009, [*https://oreil.ly/WkN6p*](https://oreil.ly/WkN6p).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch02.xhtml#ch01fn54-marker)) Alon Halevy，Peter Norvig和Fernando Pereira，“The
    Unreasonable Effectiveness of Data”，*IEEE Computer Society*，2009年3月/4月，[*https://oreil.ly/WkN6p*](https://oreil.ly/WkN6p)。
- en: ^([22](ch02.xhtml#ch01fn55-marker)) Monica Rogati, “The AI Hierarchy of Needs,”
    *Hackernoon Newsletter*, June 12, 2017, [*https://oreil.ly/3nxJ8*](https://oreil.ly/3nxJ8).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch02.xhtml#ch01fn55-marker)) Monica Rogati，“The AI Hierarchy of Needs”，*Hackernoon
    Newsletter*，2017年6月12日，[*https://oreil.ly/3nxJ8*](https://oreil.ly/3nxJ8)。
- en: ^([23](ch02.xhtml#ch01fn56-marker)) Ciprian Chelba, Tomas Mikolov, Mike Schuster,
    Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson, “One Billion Word Benchmark
    for Measuring Progress in Statistical Language Modeling,” *arXiv*, December 11,
    2013, [*https://oreil.ly/1AdO6*](https://oreil.ly/1AdO6).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch02.xhtml#ch01fn56-marker)) Ciprian Chelba，Tomas Mikolov，Mike Schuster，Qi
    Ge，Thorsten Brants，Phillipp Koehn和Tony Robinson，“One Billion Word Benchmark for
    Measuring Progress in Statistical Language Modeling”，*arXiv*，2013年12月11日，[*https://oreil.ly/1AdO6*](https://oreil.ly/1AdO6)。
- en: '^([24](ch02.xhtml#custom_ch2fn2-marker)) Alex Krizhevsky, Ilya Sutskever, and
    Geoffrey E Hinton, “ImageNet Classification with Deep Convolutional Neural Networks,”
    in *Advances in Neural Information Processing Systems*, vol. 25, ed. F. Pereira,
    C.J. Burges, L. Bottou, and K.Q. Weinberger (Curran Associates, 2012), [*https://oreil.ly/MFYp9*](https://oreil.ly/MFYp9);
    Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding,” *arXiv*, 2019,
    [*https://oreil.ly/TN8fN*](https://oreil.ly/TN8fN); “Better Language Models and
    Their Implications,” OpenAI blog, February 14, 2019, [*https://oreil.ly/SGV7g*](https://oreil.ly/SGV7g).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '^([24](ch02.xhtml#custom_ch2fn2-marker)) Alex Krizhevsky, Ilya Sutskever, and
    Geoffrey E Hinton，“ImageNet Classification with Deep Convolutional Neural Networks”，收录于*Advances
    in Neural Information Processing Systems*，第25卷，由F. Pereira，C.J. Burges，L. Bottou和K.Q.
    Weinberger编辑（Curran Associates，2012），[*https://oreil.ly/MFYp9*](https://oreil.ly/MFYp9)；Jacob
    Devlin，Ming-Wei Chang，Kenton Lee和Kristina Toutanova，“BERT: Pre-training of Deep
    Bidirectional Transformers for Language Understanding”，*arXiv*，2019年，[*https://oreil.ly/TN8fN*](https://oreil.ly/TN8fN)；“Better
    Language Models and Their Implications”，OpenAI博客，2019年2月14日，[*https://oreil.ly/SGV7g*](https://oreil.ly/SGV7g)。'
