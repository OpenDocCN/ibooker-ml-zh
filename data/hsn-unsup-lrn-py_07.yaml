- en: Chapter 5\. Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 聚类
- en: In [Chapter 3](ch03.html#Chapter_3), we introduced the most important dimensionality
    reduction algorithms in unsupervised learning and highlighted their ability to
    densely capture information. In [Chapter 4](ch04.html#Chapter_4), we used the
    dimensionality reduction algorithms to build an anomaly detection system. Specifically,
    we applied these algorithms to detect credit card fraud without using any labels.
    These algorithms learned the underlying structure in the credit card transactions.
    Then, we separated the normal transactions from the rare, potentially fraudulent
    ones based on the reconstruction error.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html#Chapter_3)中，我们介绍了无监督学习中最重要的降维算法，并突出它们密集捕捉信息的能力。在[第4章](ch04.html#Chapter_4)中，我们使用了降维算法构建了一个异常检测系统。具体来说，我们应用这些算法来检测信用卡欺诈，而不使用任何标签。这些算法学习了信用卡交易中的潜在结构。然后，我们根据重构误差将正常交易与罕见的、潜在的欺诈交易分开。
- en: In this chapter, we will build on these unsupervised learning concepts by introducing
    *clustering*, which attempts to group objects together based on similarity. Clustering
    achieves this without using any labels, comparing how similar the data for one
    observation is to data for other observations and groups.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在无监督学习的概念基础上进一步讨论*聚类*，它试图根据相似性将对象组合在一起。聚类在不使用任何标签的情况下实现这一点，比较一个观察数据与其他观察数据的相似性并进行分组。
- en: Clustering has many applications. For example, in credit card fraud detection,
    clustering can group fraudulent transactions together, separating them from normal
    transactions. Or, if we had only a few labels for the observations in our dataset,
    we could use clustering to group the observations first (without using labels).
    Then, we could transfer the labels of the few labeled observations to the rest
    of the observations within the same group. This is a form of *transfer learning*,
    a rapidly growing field in machine learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类有许多应用。例如，在信用卡欺诈检测中，聚类可以将欺诈交易分组在一起，与正常交易分开。或者，如果我们的数据集中只有少数几个标签的观察结果，我们可以使用聚类首先对观察结果进行分组（而不使用标签）。然后，我们可以将少数标记观察结果的标签转移到同一组内的其余观察结果上。这是*迁移学习*的一种形式，是机器学习中一个快速发展的领域。
- en: In areas such as online and retail shopping, marketing, social media, recommender
    systems for movies, music, books, dating, etc., clustering can group similar people
    together based on their behavior. Once these groups are established, business
    users will have better insight into their user base and can craft targeted business
    strategies for each of the distinct groups.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线购物、零售、市场营销、社交媒体、电影、音乐、书籍、约会等领域，聚类可以根据用户行为将相似的人群组合在一起。一旦建立了这些群体，业务用户就能更好地洞察他们的用户群体，并为每个独特的群体制定有针对性的业务战略。
- en: As we did with dimensionality reduction, let’s introduce the concepts first
    in this chapter, and then we will build an applied unsupervised learning solution
    in the next chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在降维中所做的那样，让我们先在本章中介绍概念，然后在下一章中构建一个应用的无监督学习解决方案。
- en: MNIST Digits Dataset
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST手写数字数据集
- en: To keep things simple, we will continue to work with the MNIST image dataset
    of digits that we introduced in [Chapter 3](ch03.html#Chapter_3).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们将继续使用我们在[第3章](ch03.html#Chapter_3)中介绍的手写数字MNIST图像数据集。
- en: Data Preparation
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Let’s first load the necessary libraries:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先加载必要的库：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, let’s load the dataset and create Pandas DataFrames:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们加载数据集并创建Pandas数据框：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Clustering Algorithms
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法
- en: Before we perform clustering, we will reduce the dimensionality of the data
    using PCA. As shown in [Chapter 3](ch03.html#Chapter_3), dimensionality reduction
    algorithms capture the salient information in the original data while reducing
    the size of the dataset.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行聚类之前，我们将使用PCA减少数据的维度。正如在[第3章](ch03.html#Chapter_3)中所示，降维算法捕捉了原始数据中的显著信息，同时减少了数据集的大小。
- en: As we move from a high number of dimensions to a lower number, the noise in
    the dataset is minimized because the dimensionality reduction algorithm (PCA,
    in this case) needs to capture the most important aspects of the original data
    and cannot devote attention to infrequently occurring elements (such as the noise
    in the dataset).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从高维度向低维度移动时，数据集中的噪声会被最小化，因为降维算法（在本例中是PCA）需要捕捉原始数据的最重要的方面，而不能将注意力放在频繁出现的元素（例如数据集中的噪声）上。
- en: Recall that dimensionality reduction algorithms are very powerful in learning
    the underlying structure in data. In [Chapter 3](ch03.html#Chapter_3), we showed
    that it was possible to meaningfully separate the MNIST images based on the digits
    they displayed using just two dimensions after dimensionality reduction.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 记得降维算法在学习数据中的潜在结构方面非常强大。在 [第三章](ch03.html#Chapter_3) 中，我们展示了仅使用两个维度进行降维后，可以根据
    MNIST 图像所显示的数字有意义地分开它们。
- en: 'Let’s apply PCA to the MNIST dataset again:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们再次将 PCA 应用于 MNIST 数据集：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Although we did not reduce the dimensionality, we will designate the number
    of principal components we will use during the clustering stage, effectively reducing
    the dimensionality.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们没有降低维度，但我们将在聚类阶段指定我们将使用的主成分数目，从而有效地降低维度。
- en: Now let’s move to clustering. The three major clustering algorithms are *k-means*,
    *hierarchical clustering*, and *DBSCAN*. We will introduce and explore each now.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向聚类。三个主要的聚类算法是 *k-means*、*层次聚类* 和 *DBSCAN*。我们将逐一介绍并探讨每一个。
- en: k-Means
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-Means
- en: The objective of clustering is to identify distinct groups in a dataset such
    that the observations within a group are similar to each other but different from
    observations in other groups. In *k*-means clustering, we specify the number of
    desired clusters *k*, and the algorithm will assign each observation to exactly
    one of these *k* clusters. The algorithm optimizes the groups by minimizing the
    *within-cluster variation* (also known as *inertia*) such that the sum of the
    within-cluster variations across all *k* clusters is as small as possible.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的目标是在数据集中识别出不同的组，使得组内的观察值彼此相似，但与其他组的观察值不同。在 *k*-means 聚类中，我们指定所需的簇数 *k*，算法将每个观察值精确分配到这
    *k* 个簇中的一个。该算法通过最小化 *簇内变化*（也称为 *惯性*）来优化这些组，从而使得所有 *k* 个簇内的变化总和尽可能小。
- en: Different runs of *k*-means will result in slightly different cluster assignments
    because *k*-means randomly assigns each observation to one of the *k* clusters
    to kick off the clustering process. *k*-means does this random initialization
    to speed up the clustering process. After this random initialization, *k*-means
    reassigns the observations to different clusters as it attempts to minimize the
    Euclidean distance between each observation and its cluster’s center point, or
    *centroid*. This random initialization is a source of randomness, resulting in
    slightly different clustering assignments, from one *k*-means run to another.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的 *k*-means 运行会导致略有不同的簇分配，因为 *k*-means 随机地将每个观察值分配给 *k* 个簇中的一个来启动聚类过程。*k*-means
    通过这种随机初始化来加速聚类过程。在此随机初始化后，*k*-means 将重新将观察值分配给不同的簇，以尽量减小每个观察值与其簇中心点（或 *质心*）之间的欧氏距离。这种随机初始化是随机性的来源，导致从一个
    *k*-means 运行到另一个运行略有不同的聚类分配。
- en: Typically, the *k*-means algorithm does several runs and chooses the run that
    has the best separation, defined as the lowest total sum of within-cluster variations
    across all *k* clusters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 典型情况下，*k*-means 算法会进行多次运行，并选择具有最佳分离效果的运行，这里分离效果定义为所有 *k* 个簇内部变化总和最低。
- en: k-Means Inertia
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-Means 惯性
- en: Let’s introduce the algorithm. We need to set the number of clusters we would
    like (`n_clusters`), the number of initializations we would like to perform (`n_init`),
    the maximum number of iterations the algorithm will run to reassign observations
    to minimize inertia (`max_iter`), and the tolerance to declare convergence (`tol`).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们介绍算法。我们需要设置我们想要的簇数目 (`n_clusters`)，我们希望执行的初始化次数 (`n_init`)，算法将运行以重新分配观察值以最小化惯性的最大迭代次数
    (`max_iter`)，以及声明收敛的容差 (`tol`)。
- en: We will keep the default values for number of initializations (10), maximum
    number of iterations (300), and tolerance (0.0001). Also, for now, we will use
    the first 100 principal components from PCA (`cutoff`). To test how the number
    of clusters we designate affects the inertia measure, let’s run *k*-means for
    cluster sizes 2 through 20 and record the inertia for each.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保留默认值，即初始化次数（10）、最大迭代次数（300）和容差（0.0001）。此外，目前我们将从 PCA 中选择前 100 个主成分 (`cutoff`)。为了测试我们指定的簇数目如何影响惯性度量，让我们对簇大小从
    2 到 20 运行 *k*-means，并记录每个簇的惯性。
- en: 'Here is the code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As [Figure 5-1](#k_means_inertia_for_cluster_sizes_2_through_20) shows, the
    inertia decreases as the number of clusters increases. This makes sense. The more
    clusters we have, the greater the homogeneity among observations within each cluster.
    However, fewer clusters are easier to work with than more, so finding the right
    number of clusters to generate is an important consideration when running *k*-means.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 5-1](#k_means_inertia_for_cluster_sizes_2_through_20)所示，随着群集数量的增加，惯性在减少。这是有道理的。群集越多，每个群集内观察结果的同质性就越大。然而，比起更多的群集，较少的群集更容易处理，因此在运行*k*-means时找到正确的群集数量是一个重要考虑因素。
- en: '![k-Means Inertia for Cluster Sizes 2 through 20](assets/hulp_0501.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![群集大小为2至20的*k*-means惯性](assets/hulp_0501.png)'
- en: Figure 5-1\. k-means inertia for cluster sizes 2 through 20
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 群集大小为2至20的*k*-means惯性
- en: Evaluating the Clustering Results
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估聚类结果
- en: To demonstrate how *k*-means works and how increasing the number of clusters
    results in more homogeneous clusters, let’s define a function to analyze the results
    of each experiment we do. The cluster assignments—generated by the clustering
    algorithm—will be stored in a Pandas DataFrame called `clusterDF`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示*k*-means的工作原理以及增加群集数量如何导致更加同质的群集，让我们定义一个函数来分析我们每次实验的结果。聚类算法生成的群集分配将存储在名为`clusterDF`的Pandas
    DataFrame中。
- en: 'Let’s count the number of observations in each cluster and store these in a
    Pandas DataFrame called `countByCluster`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们统计每个群集中的观察结果数量，并将这些存储在名为`countByCluster`的Pandas DataFrame中：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, let’s join the `clusterDF` with the true labels array, which we will
    call `labelsDF`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将`clusterDF`与称为`labelsDF`的真实标签数组结合起来：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s also count the number of observations for each true label in the training
    set (this won’t change but is good for us to know):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还统计训练集中每个真实标签的观察结果数量（这不会改变，但我们需要了解）：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, for each cluster, we will count the number of observations for each distinct
    label within a cluster. For example, if a given cluster has three thousand observations,
    two thousand may represent the number two, five hundred may represent the number
    one, three hundred may represent the number zero, and the remaining two hundred
    may represent the number nine.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于每个群集，我们将计算每个不同标签在群集内的观察结果数量。例如，如果给定的群集有三千个观察结果，其中两千可能代表数字二，五百可能代表数字一，三百可能代表数字零，其余的两百可能代表数字九。
- en: 'Once we calculate these, we will store the count for the most frequently occurring
    number for each cluster. In the example above, we would store a count of two thousand
    for this cluster:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算这些，我们将为每个群集存储最频繁出现数字的计数。在上述示例中，我们将为此群集存储两千的计数：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finally, we will judge the success of each clustering run based on how tightly
    grouped the observations are within each cluster. For example, in the example
    above, the cluster has two thousand observations that have the same label out
    of a total of three thousand observations in the cluster.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将根据每次聚类运行中观察结果在每个群集内的紧密程度来评估每次聚类运行的成功程度。例如，在上述示例中，群集中有两千个观察结果具有相同的标签，总共有三千个观察结果在该群集中。
- en: This cluster is not great since we ideally want to group similar observations
    together in the same cluster and exclude dissimilar ones.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们理想情况下希望将相似的观察结果聚集在同一个群集中并排除不相似的观察结果，因此这个群集并不理想。
- en: 'Let’s define the overall accuracy of the clustering as the sum of the counts
    of the most frequently occuring observations across all the clusters divided by
    the total number of observations in the training set (i.e., 50,000):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义聚类的总体准确性，即通过总体训练集观察结果中最频繁出现的观察结果的计数之和除以总观察结果数（即50,000）：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can also assess the accuracy by cluster:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过群集评估准确性：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For the sake of conciseness, we have all this code in a single function, available
    on [GitHub](http://bit.ly/2Gd4v7e).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们将所有这些代码放在一个单独的函数中，可以在[GitHub](http://bit.ly/2Gd4v7e)上找到。
- en: k-Means Accuracy
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*k*-means准确性'
- en: 'Let’s now perform the experiments we did earlier, but instead of calculating
    inertia, we will calculate the overall homogeneity of the clusters based on the
    accuracy measure we’ve defined for this MNIST digits dataset:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行之前的实验，但是不计算惯性，而是根据我们为MNIST数字数据集定义的准确性度量来计算群集的整体同质性：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Figure 5-2](#k_means_accuracy_for_cluster_sizes_2_through_20) shows the plot
    of the overall accuracy for different cluster sizes.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-2](#k_means_accuracy_for_cluster_sizes_2_through_20)显示了不同群集大小的整体准确性的图表。'
- en: '![k-Means Accuracy for Cluster Sizes 2 through 20](assets/hulp_0502.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![簇大小为 2 到 20 的 k-Means 准确性](assets/hulp_0502.png)'
- en: Figure 5-2\. k-means accuracy for cluster sizes 2 through 20
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 5-2\. 簇大小为 2 到 20 的 k-means 准确性
- en: As [Figure 5-2](#k_means_accuracy_for_cluster_sizes_2_through_20) shows, the
    accuracy improves as the number of clusters increases. In other words, clusters
    become more homogeneous as we increase the number of clusters because each cluster
    becomes smaller and more tightly formed.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [Figure 5-2](#k_means_accuracy_for_cluster_sizes_2_through_20) 所示，随着簇数的增加，准确性也会提高。换句话说，随着簇数的增加，簇变得更加同质化，因为每个簇变得更小且更紧凑。
- en: 'Accuracy by cluster varies quite a bit, with some clusters exhibiting a high
    degree of homogeneity and others exhibiting less. For example, in some clusters,
    over 90% of the images have the same digit; in other clusters, less than 50% of
    the images have the same digit:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 按簇计算的准确度差异很大，有些簇表现出高度的同质性，而其他簇则较少。例如，某些簇中超过 90% 的图像具有相同的数字；在其他簇中，少于 50% 的图像具有相同的数字：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: k-Means and the Number of Principal Components
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-Means 和主成分数量
- en: Let’s perform yet another experiment—this time, let’s assess how varying the
    number of principal components we use in the clustering algorithm impacts the
    homogeneity of the clusters (defined as *accuracy*).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行另一个实验——这次，让我们评估在聚类算法中使用的主成分数量如何影响簇的同质性（定义为 *准确性*）。
- en: In the experiments earlier, we used one hundred principal components, derived
    from normal PCA. Recall that the original number of dimensions for the MNIST digits
    dataset is 784\. If PCA does a good job of capturing the underlying structure
    in the data as compactly as possible, the clustering algorithm will have an easy
    time grouping similar images together, regardless of whether the clustering happens
    on just a fraction of the principal components or many more. In other words, clustering
    should perform just as well using 10 or 50 principal components as it does using
    one hundred or several hundred principal components.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的实验中，我们使用了一百个主成分，从正常的 PCA 中推导出来。回想一下，MNIST 数字数据集的原始维度是 784。如果 PCA 能够很好地捕捉数据中的基础结构并尽可能紧凑地表示，那么聚类算法将更容易将相似的图像分组在一起，无论是在少量主成分上还是在更多主成分上进行聚类。换句话说，聚类在使用
    10 或 50 个主成分时应该和使用一百或几百个主成分时一样好。
- en: 'Let’s test this hypothesis. We will pass along 10, 50, 100, 200, 300, 400,
    500, 600, 700, and 784 principal components and gauge the accuracy of each clustering
    experiment. We will then plot these results to see how varying the number of principal
    components affects the clustering accuracy:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来验证这个假设。我们将使用 10、50、100、200、300、400、500、600、700 和 784 个主成分，并评估每个聚类实验的准确性。然后我们将绘制这些结果，看看主成分数量的变化如何影响聚类的准确性：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Figure 5-3](#k_means_clustering_accuracy_as_number_of_principal_components_varies)
    shows the plot of the clustering accuracy for the different number of principal
    components.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 5-3](#k_means_clustering_accuracy_as_number_of_principal_components_varies)
    显示了不同主成分数量下聚类准确性的图表。'
- en: '![k-means Clustering Accuracy As Number of Principal Components Varies](assets/hulp_0503.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![k-means 聚类准确性随主成分数量变化](assets/hulp_0503.png)'
- en: Figure 5-3\. k-means clustering accuracy with varying number of principal components
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 5-3\. 随着主成分数量变化的 k-means 聚类准确性
- en: This plot supports our hypothesis. As the number of principal components varies
    from 10 to 784, the clustering accuracy remains stable and consistent around 70%.
    This is one reason why clustering should be performed on dimensionality-reduced
    datasets—the clustering algorithms generally perform better, both in terms of
    time and clustering accuracy, on dimensionality-reduced datasets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表支持我们的假设。随着主成分数量从 10 变化到 784，聚类的准确性保持稳定在约 70% 左右。这也是为什么应该在降维后的数据集上执行聚类的一个原因——聚类算法通常在降维后的数据集上表现更好，无论是在时间还是聚类准确性方面。
- en: In our case, for the MNIST dataset, the original 784 dimensions are manageable
    for a clustering algorithm, but imagine if the original dataset were thousands
    or millions of dimensions large. The case for reducing the dimensionality before
    performing clustering is even stronger in such a scenario.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MNIST 数据集而言，原始的 784 维度对于聚类算法来说是可以管理的，但是想象一下如果原始数据集的维度是成千上万的话。在这种情况下，进行聚类之前降低维度的理由更加强烈。
- en: k-Means on the Original Dataset
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在原始数据集上的 k-Means
- en: To make this point clearer, let’s perform clustering on the original dataset
    and measure how varying the number of dimensions we pass into the clustering algorithm
    affects clustering accuracy.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地说明这一点，让我们在原始数据集上执行聚类，并测量我们传递到聚类算法中的维度数量如何影响聚类准确性。
- en: For the PCA-reduced dataset in the previous section, varying the number of principal
    components that we passed into the clustering algorithm did not affect the clustering
    accuracy, which remained stable and consistent at approximately 70%. Is this true
    for the original dataset, too?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前一节中的PCA降维数据集，我们传递给聚类算法的主成分数量变化并不影响聚类准确性，其保持稳定且一致，约为70%。这对原始数据集也适用吗？
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Figure 5-4](#k_means_clustering_accuracy_as_number_of_original_dimension_varies)
    plots the clustering accuracy at the different original dimensions.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-4](#k_means_clustering_accuracy_as_number_of_original_dimension_varies)
    显示了在不同原始维度下的聚类准确性。'
- en: '![k-means Clustering Accuracy As Number of Original Dimensions Varies](assets/hulp_0504.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![随着原始维度数量变化的k-means聚类准确性](assets/hulp_0504.png)'
- en: Figure 5-4\. k-means clustering accuracy with varying number of original dimensions
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4\. 随着原始维度数量变化的k-means聚类准确性
- en: As the plot shows, clustering accuracy is very poor at lower dimensions but
    improves to nearly 70% only as the number of dimensions climbs to six hundred
    dimensions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图表所示，低维度下的聚类准确性非常低，但仅当维度数量提升至六百时，聚类准确性才接近70%。
- en: In the PCA case, clustering accuracy was approximately 70% even at 10 dimensions,
    demonstrating the power of dimensionality reduction to densely capture salient
    information in the original dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在PCA案例中，即使在10个维度下，聚类准确性也约为70%，展示了降维在原始数据集中密集捕捉显著信息的能力。
- en: Hierarchical Clustering
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Let’s move to a second clustering approach called *hierarchical clustering*.
    This approach does not require us to precommit to a particular number of clusters.
    Instead, we can choose how many clusters we would like after hierarchical clustering
    has finished running.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来介绍一种叫做*层次聚类*的第二种聚类方法。这种方法不要求我们预先确定特定数量的簇。相反，我们可以在层次聚类运行完成后选择我们想要的簇的数量。
- en: Using the observations in our dataset, the hierarchical clustering algorithm
    will build a *dendrogram*, which can be depicted as an upside-down tree where
    the leaves are at the bottom and the tree trunk is at the top.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们数据集中的观察结果，层次聚类算法将构建一个*树形图*，它可以被描绘为一个倒置的树，叶子位于底部，树干位于顶部。
- en: The leaves at the very bottom are individual instances in the dataset. Hierarchical
    clustering then joins the leaves together—as we move vertically up the upside-down
    tree—based on how similar they are to each other. The instances (or groups of
    instances) that are most similar to each other are joined sooner, while the instances
    that are not as similar are joined later.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 底部的叶子是数据集中的个别实例。随着我们沿着倒置树向上移动，层次聚类会将这些叶子根据它们彼此的相似程度连接在一起。最相似的实例（或实例组）会更早地连接在一起，而不那么相似的实例则会较晚连接。
- en: With this iterative process, all the instances are eventually linked together
    forming the single trunk of the tree.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个迭代过程，所有实例最终都连接在一起，形成树的单一主干。
- en: This vertical depiction is very helpful. Once the hierarchical clustering algorithm
    has finished running, we can view the dendrogram and determine where we want to
    cut the tree—the lower we cut, the more individual branches we are left with (i.e.,
    more clusters). If we want fewer clusters, we can cut higher on the dendrogram,
    closer to the single trunk at the very top of this upside-down tree.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这种垂直表示非常有帮助。一旦层次聚类算法运行完成，我们可以查看树状图，并确定我们想要切割树的位置——我们切割得越低，我们留下的个别分支（即更多簇）就越多。如果我们想要更少的簇，我们可以在树状图上部更高处切割，接近这个倒置树顶部的单一主干。
- en: The placement of this vertical cut is similar to choosing the number of *k*
    clusters in the *k*-means clustering algorithm.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个垂直切割的位置类似于在*k*-means聚类算法中选择*k*个簇的数量。
- en: Agglomerative Hierarchical Clustering
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合式层次聚类
- en: The version of hierarchical clustering we will explore is called *agglomerative
    clustering*. Although Scikit-Learn has a library for this, it performs very slowly.
    Instead, we will choose to use another version of hierarchical clustering called
    *fastcluster*. This package is a C++ library with an interface in Python/SciPy.^([1](ch05.html#idm140637545889136))
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索的层次聚类版本称为*聚合聚类*。虽然Scikit-Learn有一个库可以实现这一点，但执行速度非常慢。相反，我们选择使用另一个名为*fastcluster*的层次聚类版本。这个包是一个C++库，有Python/SciPy接口。^([1](ch05.html#idm140637545889136))
- en: The main function that we will use in this package is `fastcluster.linkage_vector`.
    This requires several arguments, including the training matrix *X*, the *method*,
    and the *metric*. The method—which can be set to `single`, `centroid`, `median`,
    or `ward`—specifies which clustering scheme to use to determine the distance from
    a new node in the dendrogram to the other nodes. The metric should be set to `euclidean`
    in most cases, and it is required to be `euclidean` if the method is `centroid`,
    `median`, or `ward`. For more on these arguments, refer to the fastcluster documentation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本包中我们将使用的主要函数是`fastcluster.linkage_vector`。这需要几个参数，包括训练矩阵*X*，*method*和*metric*。*method*可以设置为`single`、`centroid`、`median`或`ward`，指定用于确定树枝图中新节点到其他节点距离的聚类方案。在大多数情况下，*metric*应设置为`euclidean`，并且如果*method*是`centroid`、`median`或`ward`，则必须为`euclidean`。有关这些参数的更多信息，请参阅fastcluster文档。
- en: Let’s set up the hierarchical clustering algorithm for our data. As before,
    we will train the algorithm on the first one hundred principal components from
    the PCA-reduced MNIST image dataset. We will set the method to `ward` (which performed
    the best, by far, in the experimentation), and the metric to `euclidean`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的数据设置层次聚类算法。与之前一样，我们将在PCA降维的MNIST图像数据集的前一百个主成分上训练算法。我们将把*method*设置为`ward`（在实验中表现得非常好），*metric*设置为`euclidean`。
- en: Ward stands for *Ward’s minimum variance method*. You can learn more about this
    method [online](http://bit.ly/2WwOJK5). Ward is a good default choice to use in
    hierarchical clustering, but, as always, it is best to experiment on your specific
    datasets in practice.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Ward代表*Ward最小方差法*。您可以在[在线](http://bit.ly/2WwOJK5)了解更多关于这种方法的信息。在层次聚类中，Ward是一个很好的默认选择，但是，根据特定数据集的实际情况进行实验是最好的。
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The hierarchical clustering algorithm will return a matrix *Z*. The algorithm
    treats each observation in our 50,000 MNIST digits dataset as a single-point cluster,
    and, in each iteration of training, the algorithm will merge the two clusters
    that have the smallest distance between them.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法将返回一个矩阵*Z*。该算法将我们的50,000个MNIST数字数据集中的每个观察视为单点聚类，并且在每次训练迭代中，算法将合并距离最小的两个聚类。
- en: Initially, the algorithm is just merging single-point clusters together, but
    as it proceeds, it will merge multipoint clusters with either single-point or
    multipoint clusters. Eventually, through this iterative process, all the clusters
    are merged together, forming the trunk in the upside-down tree (dendrogram).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，算法仅合并单点聚类，但随着进行，它将单点或多点聚类与单点或多点聚类合并。最终，通过这个迭代过程，所有的聚类被合并在一起，形成了倒置树（树枝图）的主干。
- en: The Dendrogram
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树枝图
- en: '[Table 5-1](#first_few_rows_of_z_matrix_of_hierarchical_clustering) shows the
    Z matrix that was generated by the clustering algorithm, showing what the algorithm
    can accomplish.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5-1](#first_few_rows_of_z_matrix_of_hierarchical_clustering)展示了聚类算法生成的Z矩阵，显示了算法的成就。'
- en: Table 5-1\. First few rows of Z matrix of hierarchical clustering
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-1\. 层次聚类的Z矩阵的前几行
- en: '|  | clusterOne | clusterTwo | distance | newClusterSize |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | clusterOne | clusterTwo | distance | newClusterSize |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 42194.0 | 43025.0 | 0.562682 | 2.0 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 42194.0 | 43025.0 | 0.562682 | 2.0 |'
- en: '| 1 | 28350.0 | 37674.0 | 0.590866 | 2.0 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 28350.0 | 37674.0 | 0.590866 | 2.0 |'
- en: '| 2 | 26696.0 | 44705.0 | 0.621506 | 2.0 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 26696.0 | 44705.0 | 0.621506 | 2.0 |'
- en: '| 3 | 12634.0 | 32823.0 | 0.627762 | 2.0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 12634.0 | 32823.0 | 0.627762 | 2.0 |'
- en: '| 4 | 24707.0 | 43151.0 | 0.637668 | 2.0 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 24707.0 | 43151.0 | 0.637668 | 2.0 |'
- en: '| 5 | 20465.0 | 24483.0 | 0.662557 | 2.0 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 20465.0 | 24483.0 | 0.662557 | 2.0 |'
- en: '| 6 | 466.0 | 42098.0 | 0.664189 | 2.0 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 466.0 | 42098.0 | 0.664189 | 2.0 |'
- en: '| 7 | 46542.0 | 49961.0 | 0.665520 | 2.0 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 46542.0 | 49961.0 | 0.665520 | 2.0 |'
- en: '| 8 | 2301.0 | 5732.0 | 0.671215 | 2.0 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 2301.0 | 5732.0 | 0.671215 | 2.0 |'
- en: '| 9 | 37564.0 | 47668.0 | 0.675121 | 2.0 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 37564.0 | 47668.0 | 0.675121 | 2.0 |'
- en: '| 10 | 3375.0 | 26243.0 | 0.685797 | 2.0 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 3375.0 | 26243.0 | 0.685797 | 2.0 |'
- en: '| 11 | 15722.0 | 30368.0 | 0.686356 | 2.0 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 15722.0 | 30368.0 | 0.686356 | 2.0 |'
- en: '| 12 | 21247.0 | 21575.0 | 0.694412 | 2.0 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 21247.0 | 21575.0 | 0.694412 | 2.0 |'
- en: '| 13 | 14900.0 | 42486.0 | 0.696769 | 2.0 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 14900.0 | 42486.0 | 0.696769 | 2.0 |'
- en: '| 14 | 30100.0 | 41908.0 | 0.699261 | 2.0 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 30100.0 | 41908.0 | 0.699261 | 2.0 |'
- en: '| 15 | 12040.0 | 13254.0 | 0.701134 | 2.0 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 12040.0 | 13254.0 | 0.701134 | 2.0 |'
- en: '| 16 | 10508.0 | 25434.0 | 0.708872 | 2.0 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 10508.0 | 25434.0 | 0.708872 | 2.0 |'
- en: '| 17 | 30695.0 | 30757.0 | 0.710023 | 2.0 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 17 | 30695.0 | 30757.0 | 0.710023 | 2.0 |'
- en: '| 18 | 31019.0 | 31033.0 | 0.712052 | 2.0 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 18 | 31019.0 | 31033.0 | 0.712052 | 2.0 |'
- en: '| 19 | 36264.0 | 37285.0 | 0.713130 | 2.0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 19 | 36264.0 | 37285.0 | 0.713130 | 2.0 |'
- en: The first two columns in this table, `clusterOne` and `clusterTwo`, list which
    two clusters—could be single-point clusters (i.e., the original observations)
    or multipoint clusters—are being merged given their distance relative to each
    other. The third column, `distance`, displays this distance, which was determined
    by the Ward method and `euclidean` metric that we passed into the clustering algorithm.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表格中，前两列`clusterOne`和`clusterTwo`列出了两个簇——可以是单点簇（即原始观测数据）或多点簇——在彼此之间的距离下被合并。第三列`distance`显示了由我们传入聚类算法的Ward方法和`euclidean`度量计算出的距离。
- en: As you can see, the distance is monotonically increasing. In other words, the
    shortest-distance clusters are merged first, and the algorithm iteratively merges
    the next shortest-distance clusters until all the points have been joined into
    a single cluster at the top of the dendrogram.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，距离是单调递增的。换句话说，最短距离的簇首先合并，然后算法迭代地合并下一个最短距离的簇，直到所有点都合并为顶部树形图中的单一簇。
- en: Initially, the algorithm merges single-point clusters together, forming new
    clusters with a size of two, as shown in the fourth column, `newClusterSize`.
    However, as we get much further along, the algorithm joins large multipoint clusters
    with other large multipoint clusters, as shown in [Table 5-2](#last_few_rows_of_z_matrix_of_hierarchical_clustering).
    At the very last iteration (49,998), two large clusters are joined together, forming
    a single cluster—the top tree trunk—with all 50,000 original observations.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，算法将单点簇合并在一起，形成大小为两个的新簇，如第四列`newClusterSize`所示。然而，随着算法的进展，算法将大型多点簇与其他大型多点簇合并，如[表格5-2](#last_few_rows_of_z_matrix_of_hierarchical_clustering)所示。在最后一次迭代（49,998），两个大型簇合并在一起，形成单一簇——顶部树干，包含所有50,000个原始观测数据。
- en: Table 5-2\. Last few rows of Z matrix of hierarchical clustering
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表格5-2\. 分层聚类Z矩阵的最后几行
- en: '|  | clusterOne | clusterTwo | distance | newClusterSize |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | clusterOne | clusterTwo | distance | newClusterSize |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 49980 | 99965.0 | 99972.0 | 161.106998 | 5197.0 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 49980 | 99965.0 | 99972.0 | 161.106998 | 5197.0 |'
- en: '| 49981 | 99932.0 | 99980.0 | 172.070003 | 6505.0 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 49981 | 99932.0 | 99980.0 | 172.070003 | 6505.0 |'
- en: '| 49982 | 99945.0 | 99960.0 | 182.840860 | 3245.0 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 49982 | 99945.0 | 99960.0 | 182.840860 | 3245.0 |'
- en: '| 49983 | 99964.0 | 99976.0 | 184.475761 | 3683.0 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 49983 | 99964.0 | 99976.0 | 184.475761 | 3683.0 |'
- en: '| 49984 | 99974.0 | 99979.0 | 185.027847 | 7744.0 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 49984 | 99974.0 | 99979.0 | 185.027847 | 7744.0 |'
- en: '| 49985 | 99940.0 | 99975.0 | 185.345207 | 5596.0 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 49985 | 99940.0 | 99975.0 | 185.345207 | 5596.0 |'
- en: '| 49986 | 99957.0 | 99967.0 | 211.854714 | 5957.0 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 49986 | 99957.0 | 99967.0 | 211.854714 | 5957.0 |'
- en: '| 49987 | 99938.0 | 99983.0 | 215.494857 | 4846.0 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 49987 | 99938.0 | 99983.0 | 215.494857 | 4846.0 |'
- en: '| 49988 | 99978.0 | 99984.0 | 216.760365 | 11072.0 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 49988 | 99978.0 | 99984.0 | 216.760365 | 11072.0 |'
- en: '| 49989 | 99970.0 | 99973.0 | 217.355871 | 4899.0 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 49989 | 99970.0 | 99973.0 | 217.355871 | 4899.0 |'
- en: '| 49990 | 99969.0 | 99986.0 | 225.468298 | 8270.0 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 49990 | 99969.0 | 99986.0 | 225.468298 | 8270.0 |'
- en: '| 49991 | 99981.0 | 99982.0 | 238.845135 | 9750.0 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 49991 | 99981.0 | 99982.0 | 238.845135 | 9750.0 |'
- en: '| 49992 | 99968.0 | 99977.0 | 266.146782 | 5567.0 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 49992 | 99968.0 | 99977.0 | 266.146782 | 5567.0 |'
- en: '| 49993 | 99985.0 | 99989.0 | 270.929453 | 10495.0 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 49993 | 99985.0 | 99989.0 | 270.929453 | 10495.0 |'
- en: '| 49994 | 99990.0 | 99991.0 | 346.840948 | 18020.0 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 49994 | 99990.0 | 99991.0 | 346.840948 | 18020.0 |'
- en: '| 49995 | 99988.0 | 99993.0 | 394.365194 | 21567.0 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 49995 | 99988.0 | 99993.0 | 394.365194 | 21567.0 |'
- en: '| 49996 | 99987.0 | 99995.0 | 425.142387 | 26413.0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 49996 | 99987.0 | 99995.0 | 425.142387 | 26413.0 |'
- en: '| 49997 | 99992.0 | 99994.0 | 440.148301 | 23587.0 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 49997 | 99992.0 | 99994.0 | 440.148301 | 23587.0 |'
- en: '| 49998 | 99996.0 | 99997.0 | 494.383855 | 50000.0 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 49998 | 99996.0 | 99997.0 | 494.383855 | 50000.0 |'
- en: You may be a bit confused by the `clusterOne` and `clusterTwo` entries in this
    table. For example, in the last row—49,998—cluster 99,996 is joined with cluster
    99,997\. But as you know, there are only 50,000 observations in the MNIST digits
    dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表格中，你可能对`clusterOne`和`clusterTwo`的条目感到有些困惑。例如，在最后一行——49,998行——cluster 99,996与cluster
    99,997合并。但是你知道，在MNIST数字数据集中只有50,000个观测数据。
- en: '`clusterOne` and `clusterTwo` refer to the original observations for numbers
    0 through 49,999\. For numbers above 49,999, the cluster numbers refer to previously
    clustered points. For example, 50,000 refers to the newly formed cluster in row
    0, 50,001 refers to the newly formed cluster in row 1, etc.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`clusterOne`和`clusterTwo`指的是数字0至49,999的原始观测值。对于超过49,999的数字，聚类编号指的是先前聚类的点。例如，50,000指的是在第0行形成的新聚类，50,001指的是在第1行形成的新聚类，依此类推。'
- en: In row 49,998, `clusterOne`, 99,996 refers to the cluster formed in row 49,996,
    and `clusterTwo`, 99,997, refers to the cluster formed in row 49,997\. You can
    continue to work your way through this table using this formula to see how the
    clusters are being joined.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在第49,998行，`clusterOne`，99,996指的是在第49,996行形成的聚类，而`clusterTwo`，99,997指的是在第49,997行形成的聚类。你可以继续使用这个公式来查看聚类是如何被合并的。
- en: Evaluating the Clustering Results
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估聚类结果
- en: Now that we have the dendrogram in place, let’s determine where to cut off the
    dendrogram to make the number of clusters we desire. To more easily compare hierarchical
    clustering results with those of *k*-means, let’s cut the dendrogram to have exactly
    20 clusters. We will then use the clustering accuracy metric—defined in the `k-means
    section`—to judge how homogenous the hierarchical clustering clusters are.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了树状图，请确定在哪里切断树状图以获得我们想要的聚类数目。为了更容易地将层次聚类的结果与*k*-means的结果进行比较，让我们将树状图切割成恰好20个聚类。然后，我们将使用聚类准确度指标——在`k-means`部分定义——来评估层次聚类的聚类的同质性。
- en: To create the clusters we desire from the dendrogram, let’s pull in the *fcluster*
    library from SciPy. We need to specify the *distance threshold* of the dendrogram
    to determine how many distinct clusters we are left with. The larger the distance
    threshold, the fewer clusters we will have. Data points within the distance threshold
    we set will belong to the same cluster. A large distance threshold is akin to
    cutting the upside-down tree at a very high vertical point. Since more and more
    of the points are grouped together the higher up the tree we go, the fewer clusters
    we will have.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要从树状图中创建我们想要的聚类，让我们从SciPy引入*fcluster*库。我们需要指定树状图的*距离阈值*，以确定我们剩下多少个不同的聚类。距离阈值越大，我们得到的聚类就越少。在我们设定的距离阈值内的数据点将属于同一个聚类。较大的距离阈值类似于在非常高的垂直点剪切倒置树。因为随着树的高度越来越高，越来越多的点被分组在一起，我们得到的聚类就越少。
- en: 'To get exactly 20 clusters, we need to experiment with the distance threshold,
    as done here. The *fcluster* library will take our dendrogram and cut it with
    the distance threshold we specify. Each observation in the 50,000 observations
    MNIST digits dataset will get a cluster label, and we will store these in a Pandas
    DataFrame:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得确切的20个聚类，我们需要尝试不同的距离阈值，就像这里做的一样。*fcluster*库将使用我们指定的距离阈值对我们的树状图进行切割。MNIST手写数字数据集中的每一个观测值将获得一个聚类标签，并且我们将这些标签存储在一个Pandas
    DataFrame中：
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s verify that there are exactly 20 distinct clusters, given our choice
    of distance threshold:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证确实有恰好20个不同的聚类，考虑到我们选择的距离阈值：
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As expected, this confirms the 20 clusters:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，这证实了20个聚类：
- en: '[PRE17]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let’s evaluate the results:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估结果：
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We find that the overall accuracy is approximately 77%, even better than the
    approximately 70% accuracy from *k*-means:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现总体准确度约为77%，甚至比*k*-means的约70%准确度更好：
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Let’s also assess the accuracy by cluster.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也评估每个聚类的准确度。
- en: 'As shown here, the accuracy varies quite a bit. For some clusters, the accuracy
    is remarkably high, nearly 100%. For some, the accuracy is shy of 50%:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，准确度变化相当大。对于一些聚类，准确度非常高，接近100%。对于一些聚类，准确度略低于50%：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Overall, hierarchical clustering performs well on the MNIST digits dataset.
    Remember that we accomplished this without using any labels.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，层次聚类在MNIST手写数字数据集上表现良好。请记住，这是在不使用任何标签的情况下完成的。
- en: 'This is how it would work on real-world examples: we would apply dimensionality
    reduction first (such as PCA), then we would perform clustering (such as hierarchical
    clustering), and finally we would hand-label a few points per cluster. For example,
    for this MNIST digits dataset, if we did not have any labels, we would look at
    a few images per cluster and label those images based on the digits they displayed.
    So long as the clusters were homogeneous enough, the few hand labels we generated
    could be applied automatically to all the other images in the cluster.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际示例中，它将如何工作：首先我们会应用降维（如PCA），然后执行聚类（如层次聚类），最后我们会为每个聚类手动标记几个点。例如，对于MNIST数字数据集，如果我们没有任何标签，我们会查看每个聚类中的几幅图像，并基于它们显示的数字对这些图像进行标记。只要聚类足够同质，我们生成的少量手动标签就可以自动应用于聚类中的所有其他图像。
- en: All of a sudden, without much effort, we could have labeled all the images in
    our 50,000 dataset with a near 77% accuracy. This is impressive and highlights
    the power of unsupervised learning.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 突然之间，我们几乎可以以77%的准确率对我们50,000个数据集中的所有图像进行标记。这令人印象深刻，并突显了无监督学习的力量。
- en: DBSCAN
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN
- en: Now let’s turn to the third and final major clustering algorithm, *DBSCAN*,
    which stands for *density-based spatial clustering of applications with noise*.
    As the name implies, this clustering algorithm groups based on the density of
    points.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向第三个也是最后一个主要的聚类算法，*DBSCAN*，它代表*具有噪声的基于密度的空间聚类*。正如其名称所示，这种聚类算法基于点的密度进行分组。
- en: DBSCAN will group together closely packed points, where close together is defined
    as a minimum number of points that must exist within a certian distance. If the
    point is within a certain distance of multiple clusters, it will be grouped with
    the cluster to which it is most densely located. Any instance that is not within
    this certain distance of another cluster is labeled an outlier.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN将紧密排列的点分组在一起，其中“紧密”定义为在一定距离内存在最少数量的点。如果点在多个聚类的一定距离内，则将其与其最密集的聚类分组在一起。不在任何其他聚类的一定距离内的任何实例被标记为离群点。
- en: In *k*-means and hierarchical clustering, all points had to be clustered, and
    outliers were poorly dealt with. In DBSCAN, we can explicitly label points as
    outliers and avoid having to cluster them. This is powerful. Compared to the other
    clustering algorithms, DBSCAN is much less prone to the distortion typically caused
    by outliers in the data. Also, like hierarchical clustering—and unlike *k*-means—we
    do not need to prespecify the number of clusters.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在*k*-means和层次聚类中，所有点都必须被聚类，而且离群点处理不当。在DBSCAN中，我们可以明确将点标记为离群点，避免必须将它们聚类。这非常强大。与其他聚类算法相比，DBSCAN在数据中通常由离群点引起的失真问题上要少得多。此外，像层次聚类一样，但不像*k*-means，我们不需要预先指定聚类的数量。
- en: DBSCAN Algorithm
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DBSCAN算法
- en: Let’s first use the DBSCAN library from Scikit-Learn. We need to specify the
    *maximum distance* (called `eps`) between two points for them to be considered
    in the same neighborhood and the *minimum samples* (called `min_samples`) for
    a group to be called a cluster. The default value for `eps` is 0.5, and the default
    value for `min_samples` is 5\. If `eps` is set too low, no points may be close
    enough to other points for them to be considered in the same neighborhood. Hence,
    all the points would remain unclustered. If `eps` is set too high, many points
    may be clustered and only a handful of points would remain unclustered, effectively
    being labeled as outliers in the dataset.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们首先使用Scikit-Learn中的DBSCAN库。我们需要指定两点之间被视为相邻的最大距离（称为`eps`）和称为`min_samples`的最小样本数以被称为聚类的组。`eps`的默认值是0.5，`min_samples`的默认值是5。如果`eps`设置得太低，可能没有足够的点接近其他点以被视为相邻。因此，所有点将保持未聚类状态。如果`eps`设置得太高，可能会将许多点聚类在一起，只有少数点会保持未聚类状态，实际上被标记为数据集中的离群点。
- en: We need to search for the optimal `eps` for our MNIST digits dataset. `min_samples`
    designates how many points need to be within the `eps` distance in order for the
    points to be called a cluster. Once there are `min_samples` number of closely
    located points, any other point that is within the `eps` distance of any of these
    so-called *core points* is part of that cluster, even if those other points do
    not have the `min_samples` number of points within `eps` distance around them.
    These other points—if they do not have the *min_samples* number of points within
    `eps` distance around them—are called the *border points* of the cluster.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为我们的MNIST数字数据集寻找最佳的`eps`。`min_samples`指定在`eps`距离内需要多少点才能称为一个簇。一旦有足够数量的紧密排列的点，任何距离这些所谓的*核心点*的`eps`距离内的其他点都属于该簇，即使这些其他点周围没有达到`eps`距离内的*min_samples*数量的点。如果这些其他点周围没有*min_samples*数量的点在`eps`距离内，它们被称为该簇的*边界点*。
- en: Generally, as the `min_samples` increases, the number of clusters decreases.
    As with `eps`, we need to search for the optimal `min_samples` for our MNIST digits
    dataset. As you can see, the clusters have core points and border points, but
    for all intents and purposes, they belong to the same group. All points that do
    not get grouped—either as the core or border points of a cluster—are labeled as
    outliers.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，随着`min_samples`的增加，簇的数量减少。与`eps`类似，我们需要为我们的MNIST数字数据集寻找最佳的`min_samples`。正如您所见，这些簇有核心点和边界点，但就所有意图和目的而言，它们都属于同一组。所有未被分组的点——无论是簇的核心点还是边界点——都被标记为离群点。
- en: Applying DBSCAN to Our Dataset
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用DBSCAN到我们的数据集
- en: 'Let’s now move to our specific problem. As before, we will apply DBSCAN to
    the first one hundred principal components of the PCA-reduced MNIST digits dataset:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向我们的具体问题。与以前一样，我们将对经过PCA降维的MNIST数字数据集的前100个主成分应用DBSCAN算法：
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We will keep the `min_samples` at the default value of five, but we will adjust
    the `eps` to three to avoid having too few points clustered.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保持`min_samples`的默认值为5，但我们将调整`eps`为3，以避免集群中点数过少。
- en: 'Here is the overall accuracy:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是总体精度：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see, the accuracy is very poor compared to *k*-means and hierarchical
    clustering. We can fidget with the parameters `eps` and `min_samples` to improve
    the results, but it appears that DBSCAN is poorly suited to cluster the observations
    for this particular dataset.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，与*k*-means和层次聚类相比，准确率非常低。我们可以调整参数`eps`和`min_samples`来改善结果，但似乎DBSCAN不适合为这个特定数据集的观测进行聚类。
- en: To explore why, let’s look at the clusters ([Table 5-3](#cluster_results_for_dbscan)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索原因，让我们看看簇（[表5-3](#cluster_results_for_dbscan)）。
- en: Table 5-3\. Cluster results for DBSCAN
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-3：DBSCAN的簇结果
- en: '|  | cluster | clusterCount |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | cluster | clusterCount |'
- en: '| --- | --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | –1 | 39575 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 0 | –1 | 39575 |'
- en: '| 1 | 0 | 8885 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 8885 |'
- en: '| 2 | 8 | 720 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8 | 720 |'
- en: '| 3 | 5 | 92 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 5 | 92 |'
- en: '| 4 | 18 | 51 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 18 | 51 |'
- en: '| 5 | 38 | 38 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 38 | 38 |'
- en: '| 6 | 41 | 22 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 41 | 22 |'
- en: '| 7 | 39 | 22 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 39 | 22 |'
- en: '| 8 | 4 | 16 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 4 | 16 |'
- en: '| 9 | 20 | 16 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 20 | 16 |'
- en: Most of the points are unclustered. You can see this in the plot. 39,651 points—out
    of the 50,000 observations in the training set—are in cluster -1, which means
    that they do not belong to any cluster. They are labeled as outliers—noise, in
    other words.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数点都未被聚类。您可以在图中看到这一点。在训练集中的50,000个观测中，39,651个点属于簇-1，这意味着它们不属于任何簇。它们被标记为离群点——即噪声。
- en: 8,885 points belong in cluster 0\. Then, there is a long tail of smaller-sized
    clusters. It appears that DBSCAN has a hard time finding distinct dense groups
    of points, and, therefore, does a poor job of clustering the MNIST images based
    on the digits they display.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 8,885个点属于0号簇。然后是一长串较小规模的簇。看起来DBSCAN在找到明显的密集点组时有困难，因此在基于MNIST图像展示的数字进行聚类方面表现不佳。
- en: HDBSCAN
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HDBSCAN
- en: Let’s try another version of DBSCAN and see if the results improve. This one
    is known as *HDBSCAN*, or *hierarchical DBSCAN*. The takes the DBSCAN algorithm
    we introduced and converts it into a hierarchical clustering algorithm. In other
    words, it groups based on density and then links the density-based clusters based
    on distance iteratively, like in the hierarchical clustering algorithm we introduced
    in an earlier section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试DBSCAN的另一个版本，看看结果是否会改善。这个版本被称为*HDBSCAN*，或者*层次DBSCAN*。它采用我们介绍过的DBSCAN算法，并将其转换为层次聚类算法。换句话说，它基于密度进行分组，然后迭代地根据距离链接基于密度的簇，就像我们在前一节介绍的层次聚类算法中做的那样。
- en: 'The two main parameters for this algorithm are `min_cluster_size` and `min_samples`,
    which defaults to `min_cluster_size` when set to `None`. Let’s use the out-of-the-box
    parameter selections and gauge if HDBSCAN performs better than DBSCAN did for
    our MNIST digits dataset:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的两个主要参数是`min_cluster_size`和`min_samples`，当设置为`None`时，默认为`min_cluster_size`。让我们使用开箱即用的参数选择，评估HDBSCAN在我们的MNIST数字数据集上是否比DBSCAN表现更好：
- en: '[PRE23]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here is the overall accuracy:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是总体准确率：
- en: '[PRE24]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: At 25%, this is only marginally better than that of DBSCAN and well short of
    the 70%-plus achieved by *k*-means and hierarchical clustering. [Table 5-4](#cluster_results_for_hdbscan)
    displays the accuracy of the various clusters.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在25%时，这比DBSCAN略好一点，但远远低于*k*-means和层次聚类超过70%的表现。[表5-4](#cluster_results_for_hdbscan)展示了各个聚类的准确率。
- en: Table 5-4\. Cluster results for HDBSCAN
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-4. HDBSCAN的聚类结果
- en: '|  | cluster | clusterCount |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | 聚类 | 聚类数量 |'
- en: '| --- | --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | –1 | 42570 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 0 | –1 | 42570 |'
- en: '| 1 | 4 | 5140 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 4 | 5140 |'
- en: '| 2 | 7 | 942 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 7 | 942 |'
- en: '| 3 | 0 | 605 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 605 |'
- en: '| 4 | 6 | 295 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 6 | 295 |'
- en: '| 5 | 3 | 252 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 3 | 252 |'
- en: '| 6 | 1 | 119 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 | 119 |'
- en: '| 7 | 5 | 45 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 5 | 45 |'
- en: '| 8 | 2 | 32 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 2 | 32 |'
- en: We see a similar phenomenon as we did for DBSCAN. Most points are unclustered,
    and then there is a long tail of small-sized clusters. The results do not improve
    much.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到与DBSCAN类似的现象。大多数点未被聚类，然后是一长串小规模的聚类。结果并未有太大改进。
- en: Conclusion
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we introduced three major types of clustering algorithms—*k*-means,
    hierarchical clustering, and DBSCAN—and applied them to a dimensionality-reduced
    version of the MNIST digits dataset. The first two clustering algorithms performed
    very well on the dataset, grouping the images well enough to have a 70%-plus consistency
    in labels across the clusters.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了三种主要类型的聚类算法——*k*-means、层次聚类和DBSCAN，并将它们应用于MNIST数字数据集的降维版本。前两种聚类算法在数据集上表现非常好，能够很好地对图像进行分组，使得跨聚类的标签一致性超过70%。
- en: DBSCAN did not perform quite so well for this dataset but remains a viable clustering
    algorithm. Now that we’ve introduced the clustering algorithms, let’s build an
    applied unsupervised learning solution using these algorithms in [Chapter 6](ch06.html#Chapter_6).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，DBSCAN的表现并不太理想，但它仍然是一个可行的聚类算法。既然我们已经介绍了这些聚类算法，让我们在[第6章](ch06.html#Chapter_6)中构建一个应用的无监督学习解决方案。
- en: ^([1](ch05.html#idm140637545889136-marker)) For more on [fastcluster](https://pypi.org/project/fastcluster/),
    check out the project’s web page.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.html#idm140637545889136-marker)) 关于[fastcluster](https://pypi.org/project/fastcluster/)的更多信息，请访问该项目的网页。
