- en: Chapter 6\. More State-of-the-Art Research Questions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章。更多的最新研究问题
- en: Much of what we’ve covered in previous chapters has ranged from standard practices
    to practical yet underutilized methods. This chapter is dedicated to methods that
    are just exiting the research phase. We will also go into which of these methods
    are becoming practical for the real world and ask how different they are from
    the various trustworthiness metrics we’ve discussed in the previous chapters.
    This is by no means an exhaustive list of some of the various bleeding-edge ML
    techniques in the works right now. However, these are some of the more interesting
    techniques the authors have seen come up in discussions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中我们涵盖的内容大多从标准实践到实际但未被充分利用的方法。本章专注于刚刚走出研究阶段的方法。我们还将探讨这些方法中哪些正在变得适用于实际世界，并询问它们与我们在前几章中讨论过的各种可信度指标有何不同。这绝不是当前正在进行的各种尖端机器学习技术的详尽列表，然而，这些都是作者在讨论中看到的一些更有趣的技术。
- en: First, we want to go over how to watch out for overhyped reports and articles
    about machine learning techniques (which will be extremely relevant in [“Quantum
    Machine Learning”](#quantum-ml-sect)).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们希望了解如何警惕关于机器学习技术过度宣传的报告和文章（这在[“量子机器学习”](#quantum-ml-sect)中将非常相关）。
- en: Making Sense of Improperly Overhyped Research Claims
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解不当宣传研究声明的含义
- en: In general, all of these techniques are still in their research and proof-of-concept
    stages. It would help if we had more examples of how to judge research techniques
    and their readiness for the real world. This might sound like an oxymoron, since
    the whole point of machine learning research is to produce insights and techniques
    beyond what was previously possible.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，所有这些技术仍处于研究和概念验证阶段。如果我们有更多关于如何评估研究技术及其是否适用于实际应用的例子，将会很有帮助。这可能听起来像是一个自相矛盾，因为机器学习研究的整个目的是产生超出以往可能的见解和技术。
- en: A more helpful approach might be to look for red flags in reports on new machine
    learning advances. Indeed, given how fast the field of machine learning has been
    moving, it can be difficult to enforce a standard or culture of quality that one
    might see in other types of reporting or journalism. Fortunately, ML journalism
    mistakes are common enough that you can keep in mind several high-level antipatterns
    (both intentional and not intentional on the part of the writer) to look out for.^([1](ch06.html#idm45621833148960))
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更有帮助的方法可能是寻找关于新机器学习进展的报告中的警示信号。确实，考虑到机器学习领域的快速发展，很难强制执行一种标准或质量文化，这在其他类型的报道或新闻报道中可能会看到。幸运的是，机器学习新闻报道的错误是足够常见的，你可以记住几个高级别的反模式（无论是意图的还是非意图的，作者本人也可能会犯的）来警惕。^([1](ch06.html#idm45621833148960))
- en: Shallow Human-AI Comparison Antipattern
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 浅层次的人类-人工智能比较反模式
- en: This antipattern refers to all the ways in which a report or article might unjustly
    claim that an ML system is “just like” or “surpassing” a human being. One of the
    most egregious examples involves anthropomorphizing the ML system. A report might
    describe an ML system as having agency while completely ignoring the fact that
    its actions were always done with human supervision and cannot be done otherwise.
    Such articles often use images of humanoid robots to give the false impression
    that the ML system is embodied (or to make comparisons to the *Terminator* franchise),
    even when the ML system in question is just pattern-matching software on a cloud
    server or a laptop.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这一反模式涉及报告或文章可能不公正地声称某个机器学习系统“就像”或“超越”了人类的所有方式。其中一个最过分的例子是将机器学习系统拟人化。一篇报道可能描述一个机器学习系统具有代理能力，却完全忽视其行动始终在人类监督下进行，不能单独完成。这类文章经常使用人形机器人的图像，给人一种错误的印象，即所讨论的机器学习系统具有实体化（或与《终结者》系列进行比较），即使所涉及的机器学习系统只是云服务器或笔记本上的模式匹配软件。
- en: Some discussions about AI may compare deep learning algorithms to how living
    brains work. While a small minority of teams are trying to build AI systems based
    on observations of neurons (like [Numenta](https://numenta.com) and some academic
    labs), everyone else is using AI techniques that are extremely dissimilar from
    how brains or even living neurons work.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于人工智能的讨论可能会将深度学习算法与生物大脑的工作方式进行比较。虽然少数团队正在尝试基于对神经元的观察构建人工智能系统（例如[Numenta](https://numenta.com)和一些学术实验室），但其他人使用的人工智能技术与大脑甚至生物神经元的工作方式极为不同。
- en: Even when it comes to more specific benchmarking, there is still a tendency
    to compare how well a new technique performs against a human. Such comparisons
    neglect that humans and ML algorithms are competing in extremely limited domains,
    and they ignore how much more adaptable and generalizable humans are.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 即使涉及更具体的基准测试，仍然存在将新技术的表现与人类进行比较的倾向。这种比较忽视了人类和机器学习算法在极其有限的领域中的竞争，忽略了人类更具适应性和普适性的特点。
- en: Downplaying the Limitations of the Technique Antipattern
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 淡化技术局限性的反模式
- en: Describing the benefits of a new ML technique is to be expected. After all,
    you want not only to present the benefits to readers and funders but also to explain
    the research motivation in the first place. What is less excusable is downplaying
    or ignoring the limitations of the technique in a very one-sided analysis.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 描述新的机器学习技术的好处是可以预期的。毕竟，你不仅要向读者和资助者介绍好处，还要解释研究动机。更不可原谅的是，在非常片面的分析中淡化或忽视技术的局限性。
- en: This book has discussed a lot of cases of information leakage, bias, lack of
    understanding of the internals, poor validation, and potential for unintended
    uses. We’ve discussed all these topics in this book precisely because they are
    often omitted in discussions about taking a machine learning system from development
    to production.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本书已经讨论了许多信息泄露、偏见、对内部机制的不理解、验证不足和潜在的意外使用案例。我们之所以在本书中讨论所有这些主题，正是因为在将机器学习系统从开发到生产过程中，这些问题通常被忽略。
- en: When numbers about performance or accuracy are presented, there’s often omissions
    about how those accuracy metrics are calculated, or even what the uncertainty
    on those measurements looks like. This is a problem, because as [Chapter 4](ch04.html#chapter4)
    discussed, a lot of machine learning models are extremely brittle and fail to
    achieve the same performance when their testing environment is changed even slightly.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当呈现有关性能或准确度的数字时，通常会省略如何计算这些准确度指标，甚至省略这些测量的不确定性是什么样子。这是一个问题，因为正如[第四章](ch04.html#chapter4)所讨论的那样，许多机器学习模型非常脆弱，在其测试环境稍作改变时，它们无法达到相同的性能。
- en: In many reports on developments in ML, the limitations of the technique may
    be omitted entirely. If they are discussed, they might be downplayed in the structure
    of the article, perhaps by giving the discussion limited space or burying it in
    a footnote or addendum that most readers might not even spot in their first pass.
    Even without this de-emphasis, the limitations of the technique might be minimized
    by phrasing them as only being brought up by “skeptics” or “naysayers” who are
    “unwilling to accept the benefits of the new technique.”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多关于机器学习发展的报告中，技术的局限性可能完全被省略。如果有讨论，它们可能会在文章的结构中被淡化，也许只是给予有限的空间，或者将其埋在大多数读者第一遍阅读时甚至都不会注意到的脚注或附录中。即使没有这种淡化，技术的局限性也可能会通过将其表述为仅由“怀疑论者”或“反对者”提出来，试图将它们最小化。
- en: In addition, when a machine learning model is robust, the amount of human labor
    that went into making it so is often downplayed. Paradoxically, producing the
    data to train more advanced automation to replace humans is often extremely labor
    intensive.^([2](ch06.html#idm45621833133088)) Ignoring the contributions of human
    labelers, annotators, or even the developers in the worst cases gives the false
    impression that the ML system is more autonomous than it actually is.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当一个机器学习模型很健壮时，投入到使其如此的人力往往被低估。具有讽刺意味的是，为了训练更先进的自动化系统以替代人类，生产数据通常是极其劳动密集的。^([2](ch06.html#idm45621833133088))
    忽视人类标记员、注释员甚至在最糟糕的情况下开发者的贡献，会给人们一种错误的印象，认为机器学习系统比实际上更自主。
- en: Even the simple act of referring to a model as a “black box” can be deceptive.
    We have discussed “black box” models in [Chapter 3](ch03.html#chapter3) to refer
    to models that are not inherently interpretable but can be made easier to inspect
    using certain tools and techniques. In some cases, the use of the “black box”
    descriptor is intended to dissuade people from trying to scrutinize the model
    or hold the developers accountable.^([3](ch06.html#idm45621833127024))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至简单地将模型称为“黑箱”也可能是具有误导性的。我们在[第三章](ch03.html#chapter3)讨论了“黑箱”模型，指的是那些本质上不可解释的模型，但可以通过某些工具和技术来更易于检查。在某些情况下，“黑箱”描述符的使用旨在阻止人们审查模型或追究开发者的责任。^([3](ch06.html#idm45621833127024))
- en: Uncritical PR Piece Antipattern
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不加批判的公关文案反模式
- en: Some machine learning papers might sound more like PR press releases than actual
    research papers. If you have read such a paper, you are not imagining things.
    Most companies and research labs generally have an interest in hyping the impact
    of their work to attract more funding, customers, and talent. The article serving
    as a PR piece can come in many forms, ranging from a paper containing more marketing
    terms than technical terms, to a paper flooded with difficult-to-understand jargon
    (e.g., poorly defined acronyms or an abundance of new technical terms serving
    the same purpose as existing ones) and unnecessary difficult-to-parse mathematical
    notation (e.g., reimplementing k-nearest neighbors on a new domain, but filling
    one or more pages with intentionally obtuse notation about the k-nearest neighbors
    algorithm).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习论文听起来更像是公关新闻稿，而不是真正的研究论文。如果你读过这样的论文，那并非杞人忧天。大多数公司和研究实验室通常都有兴趣夸大他们工作的影响，以吸引更多的资金、客户和人才。作为公关文章的论文可以采用多种形式，从含有更多市场术语而不是技术术语的论文，到充斥着难以理解的术语（例如，定义不清的首字母缩写词或大量新的技术术语，其目的与现有的相同）和不必要难以解析的数学符号（例如，在新的领域重新实现k最近邻算法，但用故意晦涩的符号填充了一页或多页）。
- en: This goes beyond just the misuse of ArXiV and ML conferences. News articles
    often use terms from companies’ PR statements, which can often give wrong impressions
    about what the AI is capable of (and they may be even more prone to using the
    misleading humanoid robot imagery even when it’s irrelevant). If there is an interview
    or quote, it might be from a company spokesperson rather than a researcher or
    technical expert. The spokesperson might be intentionally exaggerating the claim,
    or they might genuinely be unaware of how the new system actually works.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这超出了仅仅滥用ArXiV和机器学习会议的范围。新闻报道经常使用公司公关声明中的术语，这往往会对AI的能力造成错误印象（即使在与人类机器人形象毫不相关时，他们可能更容易使用误导性的形象）。如果有采访或引述，可能是来自公司发言人而不是研究人员或技术专家。发言人可能有意夸大声明，或者他们可能真的不了解这个新系统的实际运作方式。
- en: Hyperbolic or Just Plain Wrong Antipattern
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反应超大或者完全错误的反范式
- en: One of the most troublesome antipatterns is that of the ML claims that are sensational,
    speculative, or even just outright false.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最令人头疼的反范式之一是那些对机器学习能力进行夸张、推测甚至是纯粹错误的声明。
- en: There is no shortage of hyperbolic claims about AI. Some of these claims might
    involve ham-fisted comparisons to major historical transformations like the Industrial
    Revolution or the invention of electricity. They might talk up an ML system’s
    relevance to a domain or industry without evidence, or even claim that it’s already
    being useful there. News articles often misunderstand what a report about a new
    ML system describes, either due to a lack of technical expertise on the team or
    deliberate misrepresentation by the organization or spokesperson. This can lead
    to phrasing the more mundane abilities of the ML system as much more groundbreaking
    or transformative than they actually are.^([4](ch06.html#idm45621833110816)) There
    is a lot of overlap with the previously described “uncritical PR piece” antipattern.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有关人工智能的夸大宣称层出不穷。其中一些宣称可能会将其与工业革命或电力发明等重大历史变革进行牵强比较。它们可能会吹捧某个机器学习系统对某个领域或行业的相关性，但缺乏证据，甚至声称它已经在那里发挥作用。新闻报道经常会误解关于新机器学习系统的报告，这既可能是团队缺乏技术专业知识，也可能是组织或发言人故意歪曲。这可能导致将机器学习系统的更为普通的能力表述为比它们实际上更具突破性或变革性的内容。^([4](ch06.html#idm45621833110816))
    这与先前描述的“不加批判的公关文章”反范式有很大重叠。
- en: The worst case of this antipattern involves a claim about the capabilities of
    a new ML system that is just a complete fabrication. The claim may be based on
    false numbers or metrics or even falsely describe performance in a specific domain.
    This might be discoverable by testing the technique yourself, but because refuting
    it might require such time and computation investment, the damage may already
    be done.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种反范式的最坏情况涉及对新的机器学习系统能力的声明，这些声明完全是捏造的。这些声明可能基于虚假的数字或指标，甚至虚假地描述了特定领域的性能。尽管可以通过自行测试这种技术来发现问题，但因为驳斥可能需要如此多的时间和计算投入，造成的损害可能已经难以挽回。
- en: Getting Past These Antipatterns
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克服这些反范式
- en: Having these antipatterns listed out might make you feel like none of the claims
    about ML systems can be trusted. This is not to say that you should be skeptical
    of all claims about ML systems, but you should at least keep an eye out for these
    common errors. Sayash Kapoor and Arvind Narayanan have created a handy checklist
    for inspecting AI claims.^([5](ch06.html#idm45621833106448)) In general, it’s
    a good idea to keep a security mindset (see [Chapter 3](ch03.html#chapter3)) when
    reading about ML systems.^([6](ch06.html#idm45621833104096))
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 把这些反模式列出来可能会让你觉得关于机器学习系统的所有声明都不可信。这并不是说你应该对所有关于机器学习系统的声明持怀疑态度，但至少你应该注意这些常见错误。Sayash
    Kapoor 和 Arvind Narayanan 创造了一个检查AI声明的便捷清单。^([5](ch06.html#idm45621833106448))
    总体上，在阅读关于机器学习系统的内容时保持安全意识是个好主意。参见[第三章](ch03.html#chapter3)。^([6](ch06.html#idm45621833104096))
- en: To show that not all new ML research claims are nonsense, we will go into more
    detail about a few newly emerging technologies and techniques in machine learning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明并非所有新的机器学习研究声明都是无稽之谈，我们将更详细地讨论一些新兴的技术和技巧。
- en: Quantized ML
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**量化机器学习**'
- en: It’s been known for a long time that you don’t always need the full numerical
    precision of the standard single or double precision floating-point numbers (32-bit
    floats or 64-bit floats) to get good results. Sure, these numbers are used in
    numerical computations in lots of other fields, but in machine learning, you might
    have opportunities to make up for the reduced precision with the sheer volume
    of numbers you’re computing when your neural network updates its parameters. *Quantized
    ML* refers to the space of removing or reducing the precision of weights and/or
    biases and/or activations in a model to remove the memory footprint or inference
    time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来已知，并非总是需要标准的单精度或双精度浮点数（32位浮点数或64位浮点数）的完整数值精度才能获得良好的结果。当然，这些数字在许多其他领域的数值计算中被使用，但在机器学习中，当神经网络更新其参数时，你可能有机会通过计算大量数字来弥补降低的精度。*量化机器学习*指的是在模型中移除或减少权重和/或偏差和/或激活的精度，以减少内存占用或推理时间的空间。
- en: Note
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Quantized ML, while it might sound similar, has nothing to do with quantum machine
    learning. It does often have heavy overlap with *sparse ML*, which involves pruning
    away the less important activations and weights of a model to reduce the overall
    size.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**量化机器学习**听起来可能与量子机器学习相似，但实际上毫无关系。它通常与*sparse ML*有很大的重叠，后者涉及修剪模型中较不重要的激活和权重，以减少整体大小。'
- en: If you have a device you want to deploy a machine learning model to, and you
    are very constrained by space (e.g., a Raspberry Pi) or have very low latency
    requirements (e.g., something running on live video) or have power constraints
    (e.g., a mobile app), quantized ML can work great. Google Brain, when developing
    its networks for running on TPUs and various NVIDIA GPUs, developed its own bfloat16
    (the *b* stands for *brain*) 16-bit floating-point format. This offered better
    energy, memory, and runtime efficiency for a model than did the full-precision
    alternatives. Still, other ML engineers wanted to go further.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个设备想要部署机器学习模型，并且空间非常有限（例如，树莓派）或有非常低的延迟要求（例如，运行在实时视频上的东西），或者有功耗约束（例如，移动应用程序），量化机器学习可以很好地发挥作用。Google
    Brain 在开发用于在TPUs和各种NVIDIA GPU上运行的网络时，开发了自己的bfloat16（*b*代表*brain*）16位浮点格式。这比完整精度的替代方案提供了更好的能源、内存和运行时效率。但其他机器学习工程师想要更进一步。
- en: The 8-bit precision formats were a much tougher nut to crack. As precision decreased
    to this level, there were bigger degradations in numerical stability and downstream
    performance of the model. Training divergence from full-precision-weight models
    was noticeable but manageable with the 16-bit bfloat16 format. However, with early
    8-bit quantization attempts, the training diverged so much that the model was
    unusable. Also, while the TPUs and GPUs supported the 16-bit float and 8-bit integer
    formats, they generally did not support the 8-bit floating-point operations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 8位精度格式是一个更为棘手的问题。随着精度降低到这个级别，模型的数值稳定性和下游性能都会有更大的降低。训练与完整精度权重模型的偏差是可以察觉到的，但用16位bfloat16格式可以管理。然而，早期的8位量化尝试导致训练偏离得如此之多，以至于模型无法使用。此外，虽然TPUs和GPUs支持16位浮点和8位整数格式，但它们通常不支持8位浮点运算。
- en: A few groups tried to create dedicated neural network architectures that would
    work better with 8-bit precision and below. One of the more famous examples of
    this was XNOR-net, which was a convolutional neural network that used binary weights
    and activations. This architecture and its successors were cleverly built, but
    they often did not have accuracy comparable to that of full-precision image classifiers.
    Implementing anything less than 16-bit precision in Transformer models was also
    much trickier than reduced-precision convolutional networks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 少数研究小组尝试创建专用的神经网络架构，以更好地支持8位精度及以下。其中更著名的例子是XNOR-net，这是一个卷积神经网络，使用二进制权重和激活。这种架构及其后继版本构建得很巧妙，但它们往往没有全精度图像分类器的准确性。在Transformer模型中实现低于16位精度也比降低精度卷积网络要困难得多。
- en: Researchers at Meta Research released methods for making working 8-bit-precision
    large language models with their technique of [8-bit optimizers via block-wise
    quantization](https://arxiv.org/abs/2110.02861) ([GitHub](https://oreil.ly/UcbYu)).
    The memory savings from their quantization method allow 175 billion parameter
    models (i.e., models of the size [GPT-3](https://oreil.ly/HESYM), [OPT-175B](https://oreil.ly/mtDr1),
    or [BLOOM](https://oreil.ly/sp4FT)) to be run on consumer hardware—around 8 RTX
    3090 GPUs with 24 GB RAM each. [Figure 6-1](#int-process-summary) shows int8 quantization
    for Transformers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Meta Research的研究人员发布了通过块级量化实现工作中的8位精度大语言模型的方法，其量化方法节省的内存允许在消费者硬件上运行1750亿参数模型（即[GPT-3](https://oreil.ly/HESYM)、[OPT-175B](https://oreil.ly/mtDr1)或[BLOOM](https://oreil.ly/sp4FT)等模型），大约需要8个配备每个24
    GB RAM的RTX 3090 GPU。图6-1（#int-process-summary）展示了Transformer的int8量化。
- en: '![ptml 0601](assets/ptml_0601.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0601](assets/ptml_0601.png)'
- en: 'Figure 6-1\. Diagram of the int8 quantization method for Transformers (credit:
    based on an image from the [8-bit CUDA functions for PyTorch project](https://oreil.ly/UcbYu))'
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. Transformer的int8量化方法示意图（来源：基于[8-bit CUDA functions for PyTorch项目](https://oreil.ly/UcbYu)的图像）
- en: The quantization method is based on decomposing matrix multiplications into
    two parts. One part of the matrix multiplication is done using int8 multiplication,
    where each row of the input left matrix and each column of the input right matrix
    is quantized to int8 with different scaling constants. The second part treats
    certain outlier feature dimensions, multiplying them using higher-precision 16-bit
    matrix multiplication. The authors found that these outlier feature dimensions,
    which contain values that are large in magnitude, very consistently emerge in
    Transformer language models of a certain size (at least 6.7 billion parameters)
    and that are sufficiently good at language modeling (as measured by perplexity).
    Entries of large magnitude must be dealt with carefully in quantization, because
    they can cause large errors when quantizing to lower precision data types, as
    shown in [Figure 6-2](#emergence-of-outlier-features).^([7](ch06.html#idm45621833075600))
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 量化方法基于将矩阵乘法分解为两部分。其中一部分矩阵乘法使用`int8`乘法完成，输入左矩阵的每一行和输入右矩阵的每一列都用不同的缩放常数量化为`int8`。第二部分处理某些异常特征维度，使用更高精度的`16-bit`矩阵乘法进行乘法。作者发现，这些包含数值较大的异常特征维度在某些规模的Transformer语言模型中（至少67亿参数）非常一致地出现，并且在语言建模方面表现良好（如困惑度所测量的）。由于将大幅度数值量化到低精度数据类型可能导致较大误差，量化时必须仔细处理，如[图 6-2](#emergence-of-outlier-features)所示。^([7](ch06.html#idm45621833075600))
- en: The LLM.int8() work leveraged the int8 operations available in many accelerators,
    but the authors note in their paper that they “believe FP8 data types offer superior
    performance compared to the Int8 data type, but currently, neither GPUs nor TPUs
    support this data type.”
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLM.int8()工作利用了许多加速器中可用的`int8`操作，但作者在论文中指出他们“认为FP8数据类型比Int8数据类型具有更好的性能，但目前GPU和TPU都不支持这种数据类型。”
- en: This is a technique that can be used for both inference and training.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种既可以用于推断又可以用于训练的技术。
- en: For example, if you wanted to use the 8-bit precision optimizer for training
    your PyTorch model, you can simply comment out the original optimizer `#torch.optim.Adam(....)`,
    add the 8-bit optimizer of your choice such as `bnb.optim.Adam8bit(....)` (arguments
    stay the same), and if necessary replace embedding layer (`torch.nn.Embedding(..)
    -> bnb.nn.Embedding(..)`).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想要在训练 PyTorch 模型时使用 8 位精度优化器，您可以简单地注释掉原始的优化器 `#torch.optim.Adam(....)`，添加您选择的
    8 位优化器，例如 `bnb.optim.Adam8bit(....)`（参数保持不变），如果需要替换嵌入层（`torch.nn.Embedding(..)
    -> bnb.nn.Embedding(..)`）。
- en: If you wanted to use `bitsandbytes` for inference, there are a few more steps.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要为推理使用 `bitsandbytes`，则需要进行一些额外步骤。
- en: 'Comment out `torch.nn.Linear`: `#linear = torch.nn.Linear(…)`'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `torch.nn.Linear` 注释掉：`#linear = torch.nn.Linear(…)`
- en: 'Add the bnb 8-bit linear light module: `linear = bnb.nn.Linear8bitLt(…)` (base
    arguments stay the same)'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 bnb 8 位线性光模块：`linear = bnb.nn.Linear8bitLt(…)`（基础参数保持不变）
- en: 'There are two modes:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有两种模式：
- en: 'Mixed 8-bit training with 16-bit main weights: pass the argument `use_fp16_weights=True`
    (default)'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 16 位主权重进行混合 8 位训练：传递参数 `use_fp16_weights=True`（默认）
- en: 'Int8 inference: pass the argument `use_fp16_weights=False`'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Int8 推理：传递参数 `use_fp16_weights=False`
- en: 'To use the full LLM.int8() method, use the `threshold=k` argument (the authors
    recommend `k=6.0`):'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用完整的 LLM.int8() 方法，请使用 `threshold=k` 参数（作者建议 `k=6.0`）：
- en: '[PRE0]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![ptml 0602](assets/ptml_0602.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0602](assets/ptml_0602.png)'
- en: 'Figure 6-2\. Emergence of outlier features in Transformers with many parameters
    and/or low perplexity (credit: based on an image from Dettmers et al.)^([8](ch06.html#idm45621833026688))'
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. Transformers 中的异常特征的出现与参数较多和/或低困惑度有关（来源：基于 Dettmers 等人的图像）
- en: 'Another recent paper, “FP8 Quantization: The Power of the Exponent” on 8-bit
    floating-point quantization,^([9](ch06.html#idm45621833024384)) builds an efficient
    implementation of a simulator for 8-bit floating-point arithmetic (see [Figure 6-3](#int-process-summary2)).
    With this simulator, they study 8-bit floating-point performance for training
    and post-training inference; the researchers show that 8-bit floating-point formats
    can achieve better performance on learning tasks than int8 for post-training inference,
    but int8 performs about as well for training.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一篇最近的论文，“FP8 量化：指数的力量”讨论了 8 位浮点量化的有效实现，^([9](ch06.html#idm45621833024384))
    构建了一个 8 位浮点算术模拟器的高效实现（参见 [图 6-3](#int-process-summary2)）。借助这个模拟器，他们研究了 8 位浮点格式在训练和后训练推理中的性能；研究人员表明，对于学习任务，8
    位浮点格式可以比 int8 实现更好的性能，但 int8 在训练中表现几乎相同。
- en: '![ptml 0603](assets/ptml_0603.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0603](assets/ptml_0603.png)'
- en: 'Figure 6-3\. Illustration of an 8-bit floating-point quantization procedure
    (credit: based on an image from Kuzmin et al.)'
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 8 位浮点量化过程的示意图（来源：基于 Kuzmin 等人的图像）
- en: Quantized ML has only just been made practical at large scale with these advancements.
    While this means that all the edge cases and failure cases of this kind of quantization
    haven’t been fully explored yet, this is already a huge step forward. One of the
    stated goals of this research was to reduce the need for giant, energy-hungry
    enterprise data centers to train large models. For example, prior to this quantization,
    hosting a model the size of GPT-3 would have required a server with eight A100
    GPUs, each with 80 GB RAM. Now, eight RTX 3090s will suffice, perhaps allowing
    such large models to fit in the resources of a Google Colab Pro account.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 量化 ML 刚刚通过这些进展在大规模上变得实用。虽然这意味着这种量化的所有边缘情况和失败案例尚未完全探索，但这已经是一个巨大的进步。这项研究的声明目标之一是减少需要大型、能耗巨大的企业数据中心来训练大型模型的需求。例如，在这种量化之前，托管像
    GPT-3 大小的模型需要一个具有 8 个 A100 GPU 的服务器，每个 GPU 配备 80 GB RAM。现在，八个 RTX 3090 就足够了，或许可以让这样大型模型适应
    Google Colab Pro 账户的资源。
- en: Tooling for Quantized ML
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化 ML 的工具
- en: There are a few tools and service that allow you to perform quantized ML.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些工具和服务，可以让您执行量化 ML。
- en: '[Larq](https://oreil.ly/reFK5) is another project dedicated to making binarized
    neural networks (you may know the makers as the people behind [OctoML](https://octoml.ai),
    a company that makes an SDK for optimizing neural networks for given hardware).
    Unfortunately, it is only available in Keras (not PyTorch or JAX), and the researchers
    made clear that they don’t have any plans to make it available in PyTorch. There
    are a few projects that have [implemented binarized neural networks in PyTorch](https://oreil.ly/eQSXt),
    but they haven’t achieved the same level of usability as the LLM.int8() research.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[Larq](https://oreil.ly/reFK5) 是另一个致力于创建二值化神经网络的项目（您可能知道其创作者是[OctoML](https://octoml.ai)背后的人，该公司制作了一款用于优化神经网络的
    SDK）。不幸的是，它仅在 Keras 中可用（而不是 PyTorch 或 JAX），研究人员明确表示他们没有计划将其在 PyTorch 中推出。有一些项目在
    PyTorch 中[实现了二值化神经网络](https://oreil.ly/eQSXt)，但它们尚未达到 LLM.int8() 研究的同等可用性水平。'
- en: As for other tools, Microsoft Research has made its DeepSpeed Compression, a
    framework for compression and system optimization in deep learning models, open
    source.^([10](ch06.html#idm45621833008720))
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 至于其他工具，微软研究院已经将其深度学习模型压缩和系统优化框架 DeepSpeed Compression 开源。^([10](ch06.html#idm45621833008720))
- en: Privacy, Bias, Interpretability, and Stability in Quantized ML
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在量化机器学习中的隐私、偏见、解释性和稳定性
- en: In terms of the areas we’ve described in the book, quantized ML has noticeable
    effects, sometimes good and sometimes bad.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 就我们在本书中描述的领域而言，量化机器学习产生了显著的影响，有时是好的，有时是不好的。
- en: If you are running a model on a device, and that device is not connected to
    the internet, then you have effectively air-gapped your ML model. This means that
    most of the attacks that require internet access will not work. As long as your
    device isn’t also displaying in-depth information about the model internals, you’ve
    eliminated the majority of machine learning–specific attacks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在设备上运行一个模型，而该设备没有连接到互联网，那么您实际上已经空隔离了您的机器学习模型。这意味着大多数需要互联网访问的攻击将无法奏效。只要您的设备没有显示关于模型内部的详细信息，您就已经消除了大多数机器学习特定攻击。
- en: Of course, you can still run a quantized model in an environment connected to
    the internet (which is probably what most readers of this book are going to be
    doing). For example, if you create a smaller model and deploy it to a Wasmer runtime,
    then you’ve got a bunch more security concerns.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您仍然可以在连接到互联网的环境中运行量化模型（这可能是本书大多数读者将要做的）。例如，如果您创建了一个较小的模型并部署到 Wasmer 运行时，那么您就会面临更多的安全问题。
- en: When it comes to interpretability and fairness, quantized ML can make things
    slightly easier to understand. After all, when looking at landscapes or any technique
    for mapping gradients, it’s easier to get away with a coarse-grained visualization
    because said visualization is already closer to the more coarse-grained nature
    of the network. Unfortunately, compression of models can hurt performance in a
    way that amplifies bias and hurts performance relative to underrepresented training
    examples.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及解释性和公平性时，量化机器学习可以使事情稍微容易理解一些。毕竟，当观察景观或任何映射梯度的技术时，通过粗粒度的可视化更容易逃脱，因为该可视化已经更接近网络的粗粒度特性。不幸的是，模型的压缩可能会损害性能，这样会增加偏见并相对于未充分训练的例子损害性能。
- en: However, when it comes to out-of-distribution attacks and uncertainty, quantized
    models fare much worse. As mentioned earlier in the section, reducing weight precision
    introduces numerical instability. For 8-bit quantization using techniques like
    LLM.int8() or FP8, the numerical instability has been controlled to the point
    where the model is at least usable in some of the desired applications, but it
    is not gone. Models created with lower precision weights, like binary neural networks,
    are often brittle and can be easily broken by adversarial attacks. While 8-bit
    precision networks aren’t vulnerable to the same degree, they are still more vulnerable
    than full-weight precision networks.^([11](ch06.html#idm45621832947760))
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当涉及到超出分布攻击和不确定性时，量化模型表现得要差得多。正如本节前面提到的，降低权重精度会引入数值不稳定性。对于使用 LLM.int8() 或
    FP8 等技术的 8 位量化，数值不稳定性已经得到控制，以至于模型至少在某些所需的应用中可以使用，但问题并未完全解决。像二值神经网络这样使用较低精度权重创建的模型通常是脆弱的，容易受到对抗性攻击的影响。虽然
    8 位精度网络的漏洞不及全精度网络，但仍比全精度网络更易受攻击。^([11](ch06.html#idm45621832947760))
- en: Diffusion-Based Energy Models
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于扩散的能量模型
- en: For most machine learning models, there’s usually a loss or optimization function
    that we try to minimize during training. Diffusion-based energy models can be
    thought of as trying to minimize a function during test/inference time. More recently,
    they have been the go-to technique for making highly capable text-to-image models.
    Recall, we discussed the potential of using diffusion models for synthetic data
    generation back in [Chapter 4](ch04.html#chapter4) (though with some caveats).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数机器学习模型，通常在训练过程中会尝试最小化某个损失或优化函数。基于扩散的能量模型可以被视为在测试/推理时尝试最小化函数。最近，它们已成为制造高度能力文本到图像模型的首选技术。回想一下，我们在[第四章](ch04.html#chapter4)讨论过使用扩散模型进行合成数据生成的潜力（尽管有一些注意事项）。
- en: When we describe text-to-image models as being diffusion models, we’re typically
    referring to the diffusion part making up the “image” part of the text-to-image
    model. Diffusion models were originally proposed in 2015, but seemed to take off
    only recently. A diffusion model is trained by taking an input like an image and
    incrementally corrupting the data with Gaussian noise until there’s nothing but
    noise. Once this is done, the diffusion process involves training a neural network
    to create the original input image through “de-noising,” or reversing the noising
    process The result is a model that can generate images from a starting random
    noise.^([12](ch06.html#idm45621832937856)) [Figure 6-4](#diffusion-overview) shows
    the process of training a diffusion model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将文本到图像模型描述为扩散模型时，通常指的是扩散部分构成文本到图像模型的“图像”部分。扩散模型最初是在2015年提出的，但直到最近才开始流行起来。扩散模型通过逐步用高斯噪声损坏数据来训练，直到只剩下噪声为止。一旦完成这个过程，扩散过程涉及训练神经网络通过去噪过程创建原始输入图像。其结果是可以从起始随机噪声生成图像的模型。^([12](ch06.html#idm45621832937856))
    [图6-4](#diffusion-overview)展示了训练扩散模型的过程。
- en: '![ptml 0604](assets/ptml_0604.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0604](assets/ptml_0604.png)'
- en: 'Figure 6-4\. Rough overview of the training process of an image-generating
    diffusion model (credit: based on an image from the University of California,
    Berkeley)'
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4。图像生成扩散模型训练过程的粗略概述（来源：加利福尼亚大学伯克利分校）
- en: Without context, text embeddings often look like random noise if you try to
    display them as images using a Python library like PIL. As such, it’s possible
    to use a diffusion model to generate images from text embeddings using the text
    embeddings as the initial noise.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 没有上下文的情况下，尝试使用Python库（如PIL）将文本嵌入显示为图像时，文本嵌入通常看起来像是随机噪声。因此，可以使用扩散模型从文本嵌入中生成图像，将文本嵌入作为初始噪声。
- en: As generative models, diffusion models have two main advantages over generative
    adversarial networks. Firstly, they do not require adversarial samples as training
    data (this has been one of the big limiters of GANs). Secondly, they are relatively
    straightforward to parallelize and scale.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作为生成模型，扩散模型比生成对抗网络具有两个主要优势。首先，它们不需要对抗样本作为训练数据（这一直是GAN的一个重大限制）。其次，它们相对容易进行并行化和扩展。
- en: 'Since the original proposal for diffusion models, there have been many variants,
    but the concepts immediately covered still apply:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 自扩散模型最初提出以来，出现了许多变体，但立即涵盖的概念仍然适用：
- en: '*Latent diffusion*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*潜在扩散*'
- en: This type of diffusion model implements the denoising process using the latent
    space of an image rather than the image itself.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的扩散模型实现去噪过程时使用图像的潜在空间，而不是图像本身。
- en: '*CLOOB conditional latent diffusion*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*CLOOB 条件潜在扩散*'
- en: CLOOB is a model that can map text and images to a shared embedding space. Using
    CLOOB embedding as conditioning, CLOOB conditional latent diffusion methods can
    be trained on both text and images to produce the final image outputs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: CLOOB 是一个能够将文本和图像映射到共享嵌入空间的模型。利用CLOOB嵌入作为条件，可以在文本和图像上训练CLOOB条件潜在扩散方法，以生成最终的图像输出。
- en: '*Guided diffusion with classifier guidance*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*带分类器指导的引导扩散*'
- en: This optimization of diffusion models uses a classifier to produce labels that
    guide the diffusion model toward a specific label.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这种扩散模型的优化使用分类器生成标签，指导扩散模型朝向特定标签发展。
- en: '*Guided diffusion with classifier-free guidance*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*无分类器指导的引导扩散*'
- en: 'This is a second optimization that alternates the guidance to the diffusion
    model to outputs produced with and without the classifier. The model proposes
    two updates: one produced using text conditioning and one without. Classifier-free
    guidance emphasizes a direction by taking the difference between the two possible
    updates and scaling that by some factor to push the process even further in that
    direction.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第二种优化方法，它改变了对扩散模型的引导，使其输出同时包含分类器和不含分类器的更新。模型提出了两种更新：一种是使用文本条件，一种是不使用文本条件。无分类器引导强调通过获取两种可能更新之间的差异并通过某个因子缩放来进一步推动该过程。
- en: A lot of these models can be imported using the HuggingFace diffusers and Transformers
    libraries.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用HuggingFace的扩散器和Transformers库可以导入许多这些模型。
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Fewer than 20 lines of text is all it takes to import one of these models and
    start creating images from little more than a text prompt.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 少于20行文本就足以导入其中一个模型，并从几乎只有文本提示开始创建图像。
- en: Consider the example of Stable Diffusion. Stable Diffusion is heavily based
    on DALL·E 2 created by OpenAI, which is a Transformer-based model that generates
    images conditioned on text descriptions. DALL·E 2 was only accessible to most
    users through an API, and training a new version of DALL·E 2 would take a long
    time (and require a lot of computing resources). However, one such organization
    decided to spend the resources needed, and this time they made both the architecture
    and the weights of the model open source. As a result, the internet exploded with
    use cases for the model ranging from video editing to image editing to creating
    3D models based on the images that the model generates.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑稳定扩散的例子。稳定扩散严重依赖于由OpenAI创建的DALL·E 2，这是一种基于Transformer的模型，根据文本描述生成图像。DALL·E
    2仅通过API对大多数用户可访问，并且训练新版本的DALL·E 2需要很长时间（并且需要大量计算资源）。然而，某个组织决定投入所需资源，并且这一次他们开放了模型的架构和权重。结果，网络上涌现出了从视频编辑到图像编辑再到基于模型生成的图像创建3D模型的用例。
- en: By making such a capable model available to seemingly anyone, how does one reduce
    the risks associated with it?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向几乎任何人提供如此强大的模型，如何降低相关风险？
- en: One of the approaches was to build a content filter into the model. In the HuggingFace
    implementation of Stable Diffusion, there is a [`StableDiffusionSafetyChecker`
    module](https://oreil.ly/gCzou). This model contains everything from the CLIP
    configuration (parameter of the CLIP model), to the visual projection (a linear
    projection layer), to a concept embeddings parameter and a *special care* embeddings
    parameter.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种方法是将内容过滤器集成到模型中。在HuggingFace的稳定扩散实现中，有一个[`StableDiffusionSafetyChecker`模块](https://oreil.ly/gCzou)。该模型包括从CLIP配置（CLIP模型的参数）到视觉投影（线性投影层），再到概念嵌入参数和*特别关注*嵌入参数的一切。
- en: The Stable Diffusion Safety Checker takes in the CLIP inputs and calculates
    the cosine distances to a few “bad concepts.” This cosine distance to the “bad
    concept” is combined with a concept threshold along with an adjustment factor
    to determine whether the image is safe or not. The “bad concept” detection is
    calculated by rounding to the third decimal place the concept cosine distance,
    minus the concept threshold, and adding the adjustment factor. The benefit of
    the adjustment factor can be changed to make the NSFW filter stronger, though
    at the cost of increasing the possibility of filtering benign images. When there
    is a hit for the “bad concept,” the Stable Diffusion pipeline just outputs an
    image made of nothing but black pixels.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定的扩散安全检查器接受CLIP输入，并计算与几个“坏概念”的余弦距离。将这个与概念阈值和调整因子相结合，以确定图像是否安全。通过将概念余弦距离四舍五入到第三位小数，减去概念阈值，并加上调整因子来计算“坏概念”检测。调整因子的好处可以通过增强NSFW过滤器来改变，尽管这会增加过滤良性图像的可能性。当“坏概念”出现时，稳定扩散流水线只会输出由纯黑像素组成的图像。
- en: This approach could be expanded to include filters for other concepts not included
    in the original implementation. In the future, it could also be expanded to check
    for embeddings of new words or concepts.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以扩展到包括原始实现中未包含的其他概念的过滤器。未来，还可以扩展到检查新单词或概念的嵌入。
- en: Homomorphic Encryption
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同态加密
- en: We briefly touched upon homomorphic encryption (HE) back in [Chapter 1](ch01.html#chapter1).
    The goal behind this domain of private ML is to run operations on encrypted data.
    The result of decrypting the result of the operation is the same as if the operation
    was run on the unencrypted data. [Figure 6-5](#he-overview) gives an overview
    of HE.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](ch01.html#chapter1)简要提到了同态加密（HE）。私人ML领域的目标是在加密数据上运行操作。解密操作的结果与在未加密数据上运行操作的结果相同。[图6-5](#he-overview)概述了HE的概况。
- en: '![ptml 0605](assets/ptml_0605.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0605](assets/ptml_0605.png)'
- en: Figure 6-5\. Visual overview of addition of homomorphically encrypted numbers
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5。同态加密数字相加的视觉概述
- en: Running operations on encrypted data sounds like a privacy dream come true,
    so why hasn’t it been more widely implemented? Part of the problem with homomorphic
    encryption is that it’s long been plagued by high computation time and the level
    of math expertise it requires. The first HE schemes were created back in 2009,
    and several improved schemes have come about over the years. These have existed
    as both partial HE and full HE. While partial HE could be used to go faster, it
    did not offer the same level of security guarantees as full HE.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在加密数据上运行操作听起来像是隐私的美梦成真，那为什么它没有被更广泛地实施呢？同态加密的问题之一是长期以来一直困扰着它的高计算时间和所需的数学专业知识水平。第一个HE方案是在2009年创建的，多年来出现了几种改进的方案。这些方案存在作为部分HE和完全HE。虽然部分HE可以用来加快速度，但它没有提供与完全HE相同级别的安全保证。
- en: Fast-forward to 2018, and Microsoft released the [SEAL library](https://oreil.ly/k2lBo).
    This is a C++ library that makes it easier to use homomorphic encryption while
    providing an abstraction layer around much of the underlying cryptography math.
    SEAL supports addition, subtraction, and multiplication of encrypted vectors of
    either integers (using the BFV algorithm) or real numbers (using the CKKS algorithm).
    These HE algorithms cover most of the operations needed for running a deep neural
    network. Fast-forward again to 2020, and the OpenMined team created [TenSEAL](https://oreil.ly/LURFK),
    a Python wrapper for SEAL. Since most machine learning engineers are working in
    Python, this is a huge step toward making HE machine learning more accessible
    to the general public.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 快进到2018年，微软发布了[SEAL库](https://oreil.ly/k2lBo)。这是一个C++库，使得在使用同态加密时更加方便，同时提供了一个在底层密码学数学周围的抽象层。SEAL支持使用BFV算法进行整数或使用CKKS算法进行实数的加法、减法和乘法操作。这些HE算法涵盖了运行深度神经网络所需的大多数操作。再快进到2020年，OpenMined团队创建了[TenSEAL](https://oreil.ly/LURFK)，这是SEAL的Python封装。由于大多数机器学习工程师使用Python，这是使HE机器学习更易于公众访问的重要一步。
- en: Part of what makes TenSEAL so handy is the `TenSEALContext` object that contains
    the various encryption keys and parameters for HE operations.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使TenSEAL如此方便的部分是包含各种加密密钥和HE操作参数的`TenSEALContext`对象。
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Having all the parameters and keys in one place makes it much easier to prototype
    HE operations. However, the security-minded might be nervous about having the
    all-important encryption keys in one place. If an unwanted outsider can still
    access the keys, that defeats the purpose of using HE in the first place. Fortunately,
    TenSEAL also provides the option of dropping the key from the `TenSEALContext`
    object so you can manage it separately. In this case, the key will need to be
    passed into the HE functions that require it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有参数和密钥集中在一个地方，大大简化了HE操作的原型制作。然而，注重安全的人可能会对将所有重要的加密密钥集中在一个地方感到紧张。如果一个不受欢迎的外部人士仍然可以访问这些密钥，这将抵消使用HE的初衷。幸运的是，TenSEAL还提供了通过`TenSEALContext`对象删除密钥的选项，这样您可以单独管理它。在这种情况下，需要将密钥传递给需要它的HE函数。
- en: For example, in the following code block, you can create a `public_context`
    that holds a secret key, then drop the secret key with the `make_context_public()`
    method to truly make it appropriate for public use.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在以下代码块中，您可以创建一个`public_context`来保存一个秘密密钥，然后使用`make_context_public()`方法丢弃秘密密钥，使其真正适合公共使用。
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here are the outputs of testing the context.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是测试上下文输出的结果。
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you are building anything with TenSEAL, it is important that you take care
    of the keys before releasing the code to production.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用TenSEAL构建任何东西，重要的是在发布代码到生产环境之前，确保您处理好密钥。
- en: Warning
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Homomorphic encryption does not automatically guarantee the protection of the
    data or model. If your system has a vulnerability that allows an unwanted outsider
    to access cryptographic keys, then your encrypted data/model can be decrypted
    by them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 同态加密并不自动保证数据或模型的保护。如果您的系统存在漏洞，允许不受欢迎的外部人员访问加密密钥，那么他们可以解密您的加密数据/模型。
- en: You need to take the proper steps to make sure your cryptographic keys are properly
    encapsulated and protected.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要采取适当的步骤，确保您的加密密钥被正确封装和保护。
- en: Once setting up the private or public `TenSEALContext` object is done, you can
    run addition and multiplication operations on encrypted vectors. Let’s see some
    examples with the BFV scheme. You can take a ciphertext vector created with the
    BFV scheme and either do arithmetic on it and a plaintext vector (c2p) or do arithmetic
    on it and another ciphertext vector (c2c).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了私有或公共的`TenSEALContext`对象，您就可以对加密向量进行加法和乘法运算。让我们来看看BFV方案的一些示例。您可以使用BFV方案创建一个密文向量，并对其进行算术运算和一个明文向量（c2p），或者对其进行算术运算和另一个密文向量（c2c）。
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can confirm that the decrypted results match the results of the unencrypted
    vector arithmetic.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以确认解密的结果与未加密向量算术的结果相匹配。
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Libraries like TenSEAL make working with homomorphic encryption much easier,
    but there are still some challenges to overcome. While the computations for homomorphic
    encryption have gotten much more efficient since 2009, they still add a lot of
    overhead to the existing computation. If we want to use HE to protect our dataset,
    or even encrypt the weights and biases of our model, even the smallest models
    will take much longer to train. For example, let’s compare the c2c and c2p multiplication
    times. Here is the code that outputs the times.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 类似TenSEAL的库使得使用同态加密变得更加容易，但仍然存在一些需要克服的挑战。虽然自2009年以来，同态加密的计算效率已经大大提高，但它们仍然给现有计算增加了很多开销。如果我们想要使用HE来保护我们的数据集，甚至加密模型的权重和偏差，即使是最小的模型训练时间也会更长。例如，我们来比较一下c2c和c2p乘法的时间。以下是输出时间的代码。
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: And here are the c2p and c2c multiplication times.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是c2p和c2c乘法的时间。
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, arithmetic operations run on two ciphertexts take much longer
    than operations run on one ciphertext and one plaintext. The takeaway is that
    if you don’t actually need to encrypt a certain plaintext (for example if the
    plaintext is common knowledge or not a secret), then there’s no need to encrypt
    it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，对两个密文运行的算术操作比对一个密文和一个明文运行的操作时间长得多。结论是，如果您实际上不需要加密某个明文（例如，如果该明文是公共知识或者不是机密），那么就没有必要对其进行加密。
- en: Tip
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For a full guide on how to make a basic neural network using homomorphic encryption,
    check out the Jupyter Notebook [*Chapter_6_Homomorphic_Encryption_NN.ipynb*](https://oreil.ly/Kh64k).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何使用同态加密制作基本神经网络的完整指南，请查看Jupyter笔记本[*Chapter_6_Homomorphic_Encryption_NN.ipynb*](https://oreil.ly/Kh64k)。
- en: Understanding these fundamentals of homomorphic encryption, such as the encryption
    schemes used for the different arithmetic operation and number types, to the trade-offs
    with runtimes, to the all-important step of carefully managing your secret key,
    will already make you far more knowledgeable on the subject of HE than most of
    the machine learning engineers out there.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 理解同态加密的这些基础知识，比如用于不同算术操作和数字类型的加密方案，与运行时的权衡，以及仔细管理您的秘密密钥这一至关重要的步骤，将使您在HE主题上比大多数机器学习工程师更具知识。
- en: Note
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We’ve shown we can reduce the precision of the weights of our network. If we
    switch out Float32 weights for Float16 or lower, or remove 20% or more of the
    total weights, the model training and inference speedup will carry over even if
    we’re using HE. This approach has not been widely implemented, but the tools for
    both HE and practical 8-bit quantization of neural networks have been made practical
    for some real-world use cases. Still, while TenSEAL supports addition, subtraction,
    and multiplication of encrypted vectors of either integers (using BFV) or real
    numbers (using CKKS), it may take more work to implement support for the reduced-weight
    precision formats.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了如何减少网络权重的精度。如果我们将Float32权重替换为Float16或更低的精度，或者移除总权重的20%或更多，即使使用HE，模型的训练和推断速度也会加快。尽管这种方法尚未广泛实施，但HE和神经网络实用的8位量化工具已经为一些实际用例提供了实用性。尽管TenSEAL支持对整数（使用BFV）或实数（使用CKKS）的加法、减法和乘法进行加密向量的操作，但是实现对减少权重精度格式的支持可能需要更多工作。
- en: Some HE libraries do implement quantization, such as [Concrete ML’s implementation](https://oreil.ly/BwnlH),
    but this is out of necessity for an encryption scheme that only works on 8-bit
    integers. We suggest the more general usefulness of quantization in different
    types of encrypted ML.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一些HE库确实实现了量化，例如[Concrete ML的实现](https://oreil.ly/BwnlH)，但这是对一个只能处理8位整数的加密方案的必要性。我们建议在不同类型的加密ML中更广泛地使用量化技术。
- en: Homomorphic encryption is definitely an exciting field. In addition to using
    homomorphic encryption for machine learning operations, there are several ongoing
    projects also leveraging HE. For example, Microsoft has been doing research on
    [homomorphically encrypted database queries](https://oreil.ly/j6NsQ).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 同态加密绝对是一个令人兴奋的领域。除了将同态加密用于机器学习操作外，还有几个正在进行的项目也在利用HE。例如，微软一直在研究[同态加密数据库查询](https://oreil.ly/j6NsQ)。
- en: While all of this brings homomorphic encryption into the realm of practicality,
    it is still an immature subfield compared to many of the other privacy-preserving
    ML tools we discussed in [Chapter 1](ch01.html#chapter1). We also want to stress
    that many jurisdictions have laws governing data privacy and as of the time of
    writing, HE has yet to be added to any of those standards lists (for example,
    using HE in your pipeline will not automatically make you HIPAA compliant in the
    US).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有这些使同态加密进入了实用领域，与我们在[第1章](ch01.html#chapter1)中讨论的许多其他保护隐私的ML工具相比，它仍然是一个不成熟的子领域。我们还要强调，许多司法管辖区都有关于数据隐私的法律，并且在撰写本文时，HE尚未被列入任何这些标准清单中（例如，在美国使用HE并不会自动使您符合HIPAA的要求）。
- en: Tip
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For homomorphically encrypted machine learning, TenSEAL is likely the best option
    so far. It’s part of the larger set of OpenMined privacy-preserving ML tools.
    For an example of an alternative HE tool for ML in Python, check out the [Concrete
    HE Rust Library’s Python wrapper](https://oreil.ly/47YgG).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于同态加密机器学习而言，TenSEAL目前可能是最佳选择。它是OpenMined隐私保护ML工具的一部分。例如，要了解Python中用于ML的备选HE工具，可以查看[Concrete
    HE Rust Library的Python包装器](https://oreil.ly/47YgG)。
- en: Simulating Federated Learning
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟联邦学习
- en: '[Chapter 1](ch01.html#chapter1) covers federated learning (FL) and describes
    techniques that can be applied and tested using a much smaller set of servers
    and clients. For example, you may have a set of a few sharded databases that you
    want your machine learning model to learn on. Federated learning can often refer
    to much larger cases such as splitting up machine learning models, training them
    separately, and then recombining the weights into a combined model.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](ch01.html#chapter1)涵盖了联邦学习（FL），并描述了可以使用较小一组服务器和客户端进行应用和测试的技术。例如，您可能有一组分片的数据库，您希望您的机器学习模型在上面学习。联邦学习通常可以指更大规模的案例，例如将机器学习模型分割、分别训练它们，然后将权重重新组合成一个合并模型。'
- en: Federated learning algorithms can range from centralized to decentralized. They
    can operate on many different model types ranging from decision trees to support
    vector machines to neural networks. The issue is that these techniques can have
    enormous differences in implementation. Neural networks aren’t updated in the
    exact same fashion as tree-based models, and protocols vary widely between centralized
    FL with a central oracle and decentralized FL on many internet-of-things (IoT)
    devices (such as with [Meta’s approach to protecting data on mobile devices](https://oreil.ly/91EPp)).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习算法可以从集中式到分散式范围。它们可以在许多不同的模型类型上操作，从决策树到支持向量机再到神经网络。问题在于这些技术在实施上可能有巨大差异。神经网络的更新方式与基于树的模型并不完全相同，协议在集中式FL与中央预言机和分散式FL在许多物联网（IoT）设备（例如[Meta在移动设备上保护数据的方法](https://oreil.ly/91EPp)）之间差异巨大。
- en: 'This is the core problem with FL: you only start to see the benefits of implementing
    it after you do so on a large network of devices or users. This makes it extremely
    difficult to implement for many reasons, ranging from enforcing privacy guarantees
    to complex updating schemes to the larger computation cost involved in simply
    testing your federated learning setup.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是FL的核心问题：只有在在大量设备或用户网络上实施后，您才会开始看到其带来的好处。这使得实施变得极其困难，原因包括强制实施隐私保证、复杂的更新方案以及简单测试联邦学习设置所涉及的更大计算成本。
- en: Compared to some of the privacy techniques discussed in [Chapter 1](ch01.html#chapter1),
    there are surprisingly few open source tools for testing federated learning. One
    of the few useful and mature tools out there is Microsoft’s [Federated Learning
    Utilities and Tools for Experimentation (FLUTE)](https://oreil.ly/4khyN) framework.
    FLUTE was built to make it easier to test and prototype federated learning algorithms
    using offline simulations. A few of the features that make it attractive are the
    ability to simulate federated learning on a large scale (millions of clients,
    sampling tens of thousands of instances per round), multi-GPU and multi-node orchestration,
    as well as the ability to simulate federated learning on architectures. Performance
    and scalability of the communications between separate devices are enforced through
    the use of Open Modeling Interface (OpenMI). The only downside is that, while
    it might support native integration with Azure, it will take more work to get
    FLUTE set up on other cloud providers. Also, while FLUTE is based on an extensible
    programming model and is written in PyTorch, it still requires a more in-depth
    understanding of federated learning to take full advantage of it.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与[第一章](ch01.html#chapter1)中讨论的某些隐私技术相比，用于测试联邦学习的开源工具令人惊讶地少。其中少数几个有用且成熟的工具之一是微软的[Federated
    Learning Utilities and Tools for Experimentation (FLUTE)](https://oreil.ly/4khyN)框架。FLUTE的建立旨在通过离线模拟来更轻松地测试和原型化联邦学习算法。一些使其具有吸引力的特性包括能够在大规模上模拟联邦学习（数百万客户端，每轮采样数万个实例）、多GPU和多节点协同，以及在架构上模拟联邦学习的能力。通过使用开放建模接口（OpenMI）来强化设备之间通信的性能和可扩展性。唯一的缺点是，虽然它可能支持与Azure的本地集成，但在其他云提供商上设置FLUTE将需要更多工作。此外，虽然FLUTE基于可扩展的编程模型并且是用PyTorch编写的，但要充分利用它仍需要对联邦学习有更深入的理解。
- en: A typical FLUTE architecture consists of a number of nodes—physical or virtual
    machines—that execute a number of workers. One of the nodes acts as an orchestrator
    distributing the model and tasks to the different workers. Each worker processes
    the tasks sequentially, calculates the model delta, and sends the gradients back
    to the orchestrator, which federates it into the centralized model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的FLUTE架构包括多个节点——物理或虚拟机器——执行多个工作器。其中一个节点充当协调器，将模型和任务分配给不同的工作器。每个工作器按顺序处理任务，计算模型增量，并将梯度发送回协调器，协调器将其联合到集中模型中。
- en: 'A federated learning workflow generally involves the following steps:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习工作流通常包括以下步骤：
- en: Transmit the initial model (the global model) to client devices.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将初始模型（全局模型）传输到客户端设备。
- en: Train instances of the global model with locally available data on each client.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个客户端上使用本地可用数据训练全局模型的实例。
- en: Transmit the information from training to the central orchestrator server (information
    like adapted local models, logits, and pseudo-gradients of those models).
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练信息从客户端传输到中央协调器服务器（例如适应的本地模型、对数和这些模型的伪梯度）。
- en: Combine the information on the models from all the clients to create a new model
    on the central server.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将来自所有客户端模型的信息合并，创建一个新模型放在中央服务器上。
- en: Optionally, make the central server update the global model with a server-side
    rehearsal step.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，让中央服务器通过服务器端的排练步骤更新全局模型。
- en: Send the updated global model back to the clients.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将更新后的全局模型发送回客户端。
- en: After sampling a new subset of clients for the next training iteration, repeat
    steps 2 through 6.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个训练迭代中对新的客户端子集进行采样后，重复步骤2至6。
- en: If you’re simulating federated learning instead of deploying it to a lot of
    devices, the simulation will ideally emulate the hurdles the algorithm will face
    in terms of updates and coordination. While it’s also important to simulate and
    calculate latency, this won’t be necessary to emulate perfectly in all tests.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在模拟联邦学习而不是将其部署到大量设备上，那么模拟将理想地模拟算法在更新和协调方面面临的障碍。虽然模拟和计算延迟同样重要，但并非所有测试都需要完美地模拟。
- en: Note
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For an example of how to use FLUTE for federated learning simulation, check
    out the example notebook [*Chapter_6_Federated_Learning_Simulations.ipynb*](https://oreil.ly/daGhX).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何使用FLUTE进行联邦学习模拟的示例，请查看示例笔记本[*Chapter_6_Federated_Learning_Simulations.ipynb*](https://oreil.ly/daGhX)。
- en: It’s also worth keeping in mind that federated learning is not a cure-all for
    issues in machine learning privacy. If you’re building products or software that
    is going to be operating over the internet, you should always assume there will
    be adversarial actors somewhere.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，要记住联邦学习并不能解决机器学习隐私问题的所有问题。如果你正在开发将在互联网上运行的产品或软件，你应该始终假设会存在某些对抗性行为者。
- en: Quantum Machine Learning
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量子机器学习
- en: If you’ve been paying attention to the state-of-the-art in ML, you’ve most likely
    come across all the hype surrounding quantum computing. If you’ve heard of this,
    you’ve probably also encountered the concept of “quantum machine learning” (QML).
    But what are these concepts, not just quantum machine learning but quantum computing
    in general?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你关注过机器学习的最新进展，你很可能已经听说了围绕量子计算的炒作。如果你听说过这一点，你可能也遇到了“量子机器学习”（QML）的概念。但这些概念是什么，不仅仅是量子机器学习，而是量子计算总体？
- en: Put simply, *quantum computers* are computers built out of things like photons,
    subatomic particles, and super-cooled atoms. The behavior of these systems is
    often difficult for classical computers to easily simulate. Part of the reason
    to develop quantum machine learning is just to build the computational substrate
    itself out of the very things that are hard to simulate. The benefit of building
    computers in this way is that the new computers can add new types of low-level
    computational operations to the set that classical computers can do. This is like
    taking the set of known [classical logic gates such as AND, OR, XOR, NAND](https://oreil.ly/ihhaE)
    and adding on entirely new fundamental gates, as shown in [Figure 6-6](#qbit-overview).
    These are [quantum logic gates](https://oreil.ly/CpvK3) that operate on qubits
    (quantum bits). The *quantum logic gate* operations (for example Pauli-X, Pauli-Y,
    and Pauli-Z, Hadamard, Phase, 𝜋/8, Controlled NOT, Controlled Z, SWAP, and Toffoli),
    unlike regular logic gates, are all reversible. All the gates are unitary operations
    that are often represented as unitary matrices. This is why you’ll often see the
    gate operations as matrices of 1s and 0s and complex numbers and unit-sphere values.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，*量子计算机*是由光子、亚原子粒子和超冷原子等组成的计算机。这些系统的行为通常难以被经典计算机轻松模拟。开发量子机器学习的一部分原因就是为了将计算基底建立在那些难以模拟的东西上。以这种方式构建计算机的好处在于，新计算机可以添加经典计算机无法进行的新类型低级计算操作。这类似于添加已知的[经典逻辑门，如与门、或门、异或门、非与门](https://oreil.ly/ihhaE)，并添加完全新的基本门，如[图
    6-6](#qbit-overview)中所示。这些是操作在量子比特（qubits）上的[量子逻辑门](https://oreil.ly/CpvK3)。*量子逻辑门*操作（例如
    Pauli-X、Pauli-Y 和 Pauli-Z、Hadamard、Phase、𝜋/8、Controlled NOT、Controlled Z、SWAP
    和 Toffoli），与常规逻辑门不同，都是可逆的。所有这些门都是幺正操作，通常被表示为单位矩阵。这就是为什么你经常会看到门操作作为由 1 和 0、复数和单位球值组成的矩阵。
- en: '![ptml 0606](assets/ptml_0606.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0606](assets/ptml_0606.png)'
- en: Figure 6-6\. Overview of the difference between classical and quantum bit states
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. 经典比特和量子比特状态之间差异的概述
- en: 'The inclusion of complex numbers and unit-sphere numbers in a table of logic
    gate values might seem a bit odd. We’ve mentioned that quantum computing adds
    entirely new types of low-level operations. This is due to the unique nature of
    qubits over classical bits. Regular bits can only be in one of two states: 0 or
    1. Quantum bits, on the other hand, can be in a linear combination of both states.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑门值表中包含复数和单位球数可能看起来有点奇怪。我们已经提到量子计算添加了完全新的低级操作类型。这是由于量子比特比经典比特的独特性质。常规比特只能处于两种状态之一：0
    或 1。另一方面，量子比特可以处于两种状态的线性组合之中。
- en: Warning
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Quantum bits are often described as a “superposition” of both states. *Superposition*
    is literally just a synonym for *linear combination*. This is one of those words
    or phrases that often comes up not only in quantum computing research literature
    but also in PR and marketing materials meant to make quantum computing sound more
    exciting. The “superposition” itself is often describing a probability distribution
    of possible states, rather than something that has been measured to be in a combination
    of states.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 量子比特经常被描述为“叠加”两种状态的状态。“叠加”实际上只是“线性组合”的同义词。这是一个在量子计算研究文献中经常出现的词语或短语，也出现在旨在使量子计算听起来更加令人兴奋的公关和市场材料中。“叠加”本身通常描述的是可能状态的概率分布，而不是已经测量到的处于状态组合中的东西。
- en: On a similar note, we recommend not putting too much weight on the words of
    anyone who takes “Schrödinger’s cat” too seriously. The cat thought experiment,
    which is probably more famous than the concept of quantum computing at this point,
    was created by Erwin Schrödinger precisely to illustrate the absurdity of the
    idea that a quantum system can truly be in a superposition of states. A superposition/linear
    combination is just a mathematical tool to describe the unintuitive behavior of
    a quantum system.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的是，我们建议不要对那些过于认真对待“薛定谔的猫”的人言听计从。这只猫的思想实验，目前可能比量子计算的概念更为人知晓，其目的正是为了说明一个量子系统真的可以处于超态的概念的荒谬性。超态/线性组合只是描述量子系统不直观行为的数学工具。
- en: When measuring the state of a quantum system, whether that be the spin of a
    particle or the polarity of a photon, you can imagine it as being somewhere on
    the surface of a Bloch sphere. That Bloch sphere is a 3D unit-sphere (think of
    it like a 3D version of the 2D unit sphere from your introductory trigonometry
    course). A 3D sphere is used because the state of the qubit can be described by
    how close it is to the poles of three planes of the sphere, as shown in [Figure 6-7](#qbit-overview2).
    The X plane represents the state of the qubit being (0) or (1). The Y plane represents
    the state of the qubit being positive (+) or negative (–). The Z plane represents
    the state of the qubit being in phase (i) or anti-phase (–i). The various operations
    of quantum logic gates manipulate the qubit’s state by rotating it around the
    sphere, bringing it closer or further away from these various poles.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当测量量子系统的状态时，无论是粒子的自旋还是光子的极性，你可以将其想象成位于布洛赫球面的某处。那个布洛赫球是一个三维单位球体（就像你初学三角法时的二维单位球面的三维版本）。选择三维球体是因为量子比特的状态可以通过它与球体三个平面极点的距离来描述，如图
    [6-7](#qbit-overview2) 所示。X 平面表示量子比特状态为 (0) 或 (1)。Y 平面表示量子比特状态为正（+）或负（–）。Z 平面表示量子比特状态为相位（i）或反相位（–i）。量子逻辑门的各种操作通过围绕球体旋转量子比特的状态，使其接近或远离这些不同的极点。
- en: '![ptml 0607](assets/ptml_0607.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0607](assets/ptml_0607.png)'
- en: Figure 6-7\. Visualizations of the bases for measurement of a quantum bit
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. 量子比特测量基础的可视化
- en: This is certainly a simplification of the overall process, but it should be
    a good way to give you the intuition that at least the basic operations of quantum
    computing are understandable if you have basic knowledge of matrices or trigonometry.
    This is a far cry from quantum computing being magic like so many reports and
    fictional depictions would have you believe.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是整个过程的简化，但这应该是一个很好的方式，至少让你了解量子计算的基本操作是可以理解的，如果你具备矩阵或三角法的基础知识的话。这与许多报道和虚构描绘中量子计算如魔术般的描述有很大不同。
- en: Note
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: For a high-level intuitive introduction to quantum computing, check out Michael
    Neilsen’s [“Quantum Computing for the very Curious”](https://oreil.ly/Q81Xa).
    To learn more about quantum computing in depth for the sake of coding quantum
    computing applications, we recommend O’Reilly’s’ resources. For example, you can
    read about programming quantum computers in *Programming Quantum Computers* by
    Eric R. Johnston et al. (O’Reilly, 2019).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 想要对量子计算进行高层次直观介绍，请查看迈克尔·尼尔森的 [“非常好奇的量子计算”](https://oreil.ly/Q81Xa)。要深入了解编写量子计算应用程序的量子计算，请参阅奥莱利的资源。例如，你可以阅读埃里克·R·约翰斯顿等人编写的《编程量子计算机》（奥莱利，2019年）。
- en: Quantum computing is in a sort of indeterminate state, as we have a lot of research
    prototypes but no real-world quantum computers that you’d want to actually use
    for business purposes (other than for PR). Still, there’s a possibility that this
    situation will flip and they’ll suddenly start working, much like what happened
    with machine learning after the introduction of GPU-powered deep neural networks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 量子计算处于一种不确定状态，因为我们有很多研究原型，但没有真正可以用于商业目的的量子计算机（除了用于公关）。不过，有可能情况会发生逆转，它们会突然开始工作，就像
    GPU 强化的深度神经网络引入后机器学习的情况一样。
- en: Quantum computing has gotten a lot of attention due to its ability to improve
    upon classical algorithms. One of the more famous quantum algorithms is Shor’s
    algorithm, which is a quantum algorithm that finds the shortest non-trivial prime
    factor of a number. Why does this earn such fame? Efficient prime factorization
    weakens the encryption-based security that most of modern IT security relies on.
    Quantum computers with Shor’s algorithm break elliptic curve encryption schemes
    completely. Hashes (like SHA256) survive quantum computers just fine, though security
    degrades somewhat and longer hash lengths are recommended. This is why there are
    so many efforts at discovering “post-quantum” encryption strategies.^([13](ch06.html#idm45621832210912))
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 量子计算因其能够改进经典算法的能力而引起了广泛关注。其中较著名的量子算法之一是Shor算法，这是一种可以找到数字最短非平凡质因数的量子算法。为何它如此著名？高效的质因数分解削弱了现代IT安全大多依赖的基于加密的安全性。使用Shor算法的量子计算机可以完全破解椭圆曲线加密方案。哈希函数（如SHA256）在量子计算机面前表现得相当不错，尽管安全性有所下降，建议使用更长的哈希长度。这就是为什么有如此多的努力在寻找“后量子”加密策略的原因。^([13](ch06.html#idm45621832210912))
- en: Quantum computing and quantum communication have also gotten a lot of attention
    for the (highly misleading) claim that such quantum systems are “unhackable.”
    Quantum computer operations are done by manipulating probabilities. These probabilities
    collapse into definite states when the qubits are measured. Quantum computation
    systems tell you more readily *if* they’ve been hacked, but they’d still be one
    careless sysadmin away from a hack.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 量子计算和量子通信也因（极具误导性的）声称这些量子系统是“无法被黑客攻击的”而受到了很多关注。量子计算操作通过操纵概率来完成。当量子比特被测量时，这些概率会坍缩为确定状态。量子计算系统更容易告诉你它们是否被黑客攻击过，但它们仍可能因为一个粗心的系统管理员而遭遇攻击。
- en: Why are the current quantum machine learning systems so small? Part of the reason
    is that the sensitive quantum systems are often besieged on all sides by error
    from the rest of the universe. For example, [cosmic rays induce widespread errors
    in superconducting quantum computers](https://arxiv.org/abs/2104.05219). This
    is quite bad for superconducting quantum computing. Approaches like neutral atom
    or ion-trap quantum computing might be relatively better, but even they have their
    own sources of error.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当前量子机器学习系统为何如此小？部分原因在于敏感的量子系统常常受到来自宇宙其他部分的误差侵袭。例如，[宇宙射线在超导量子计算机中引起广泛误差](https://arxiv.org/abs/2104.05219)。这对超导量子计算非常不利。像中性原子或离子阱量子计算可能相对更好些，但它们也有自己的误差来源。
- en: Getting better at correcting one source of error usually involves adding another
    source of error. Remember when we mentioned that quantum gates add new operation
    types? Quantum circuits are made of quantum gates (where you do want to change
    the state) and quantum wires (where you do *not* want to change the state). Systems
    that make good quantum gates usually make bad quantum wires and vice versa. This
    conflict between quantum gates and quantum wires is one of the big unsolved problems
    preventing quantum computers from getting bigger. Until that problem is solved,
    we don’t recommend betting too much of your product on quantum ML’s success.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要改进一个误差来源通常意味着添加另一个误差来源。还记得我们提到量子门引入新的操作类型吗？量子电路由量子门组成（你希望在这里改变状态）和量子线（你*不*希望改变状态）组成。制造良好的量子门的系统通常制造不良的量子线，反之亦然。量子门和量子线之间的这种冲突是阻止量子计算机变得更大的主要未解决问题之一。在解决这个问题之前，我们不建议过多地依赖量子机器学习的成功。
- en: Tooling and Resources for Quantum Machine Learning
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量子机器学习的工具和资源
- en: This section is by no means an exhaustive resource on quantum computing, or
    even quantum machine learning specifically. Of all the various approaches to quantum
    machine learning, photonic chips seem to be the closest to becoming usable for
    machine learning purposes (though “closest” is still relative, as all of these
    are still a long way from common real-world usage).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本节绝不是量子计算的详尽资源，甚至不是专门针对量子机器学习的。在各种量子机器学习方法中，光子芯片似乎最接近成为用于机器学习目的的可用技术（尽管“接近”仍然是相对的，因为所有这些技术离普通实际应用还有很长的路要走）。
- en: Strawberry Fields is a full-stack Python library for designing, simulating,
    and optimizing continuous-variable quantum optical circuits. PennyLane, another
    library by Xanadu, is optimized for differentiable programming of quantum computers.
    The Qiskit team, who is behind the most popular quantum computing framework in
    the market, [released a course about quantum ML](https://oreil.ly/v4lwi).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Strawberry Fields是一个全栈Python库，用于设计、模拟和优化连续变量量子光学电路。Xanadu的另一个库PennyLane则专为量子计算机的可微分编程进行优化。Qiskit团队，作为市场上最受欢迎的量子计算框架背后的团队，[发布了一门关于量子ML的课程](https://oreil.ly/v4lwi)。
- en: For specific algorithms and quantum versions of classical machine learning,
    see the resources listed in [Table 6-1](#table-quantum-other).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于经典机器学习的具体算法和量子版本，请参阅列在[表6-1](#table-quantum-other)中的资源。
- en: Table 6-1\. Quantum machine learning resources and their frameworks/providers
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-1\. 量子机器学习资源及其框架/供应商
- en: '| Tutorial | Frameworks/libraries | Overview |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 教程 | 框架/库 | 概述 |'
- en: '| --- | --- | --- |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| [Quantum models as Fourier series](https://oreil.ly/81DXb) | Xanadu (PennyLane)
    | Understand the link between variational quantum models and Fourier series |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| [将量子模型作为傅里叶级数](https://oreil.ly/81DXb) | Xanadu (PennyLane) | 理解变分量子模型与傅里叶级数之间的联系
    |'
- en: '| [Training and evaluating quantum kernels](https://oreil.ly/3A1py) | Xanadu
    (PennyLane), scikit-learn | Kernels and alignment training with PennyLane |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| [训练和评估量子内核](https://oreil.ly/3A1py) | Xanadu (PennyLane), scikit-learn |
    使用PennyLane进行内核和对齐训练 |'
- en: '| [Kernel-based training of quantum models with Scikit-learn](https://oreil.ly/DX8LU)
    | Xanadu (PennyLane), scikit-learn, PyTorch | Kernel-based training with scikit-learn
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| [使用Scikit-learn进行量子模型基于内核的训练](https://oreil.ly/DX8LU) | Xanadu (PennyLane),
    scikit-learn, PyTorch | 使用scikit-learn进行基于内核的训练 |'
- en: '| [Variational classifier](https://oreil.ly/kZSf6) | Xanadu (PennyLane) | A
    quantum variational classifier |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| [变分分类器](https://oreil.ly/kZSf6) | Xanadu (PennyLane) | 量子变分分类器 |'
- en: '| [Data-reuploading classifier](https://oreil.ly/LLqQW) | Xanadu (PennyLane)
    | Universal quantum classifier with data reuploading |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| [数据重新上传分类器](https://oreil.ly/LLqQW) | Xanadu (PennyLane) | 使用数据重新上传的通用量子分类器
    |'
- en: '| [Quantum transfer learning](https://oreil.ly/fsCwD) | Xanadu (PennyLane),
    PyTorch | Quantum transfer learning |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| [量子迁移学习](https://oreil.ly/fsCwD) | Xanadu (PennyLane), PyTorch | 量子迁移学习 |'
- en: '| [Quantum Generative Adversarial Networks with Cirq + TensorFlow](https://oreil.ly/Ywu70)
    | Xanadu (PennyLane), TensorFlow, Cirq | Create a simple QGAN with Cirq and TensorFlow
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| [Cirq + TensorFlow的量子生成对抗网络](https://oreil.ly/Ywu70) | Xanadu (PennyLane),
    TensorFlow, Cirq | 使用Cirq和TensorFlow创建简单的量子生成对抗网络 |'
- en: '| [Function fitting with a photonic quantum neural network](https://oreil.ly/y4B74)
    | Xanadu (PennyLane) | Fit one-dimensional noisy data with a quantum neural network
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| [用光子量子神经网络进行函数拟合](https://oreil.ly/y4B74) | Xanadu (PennyLane) | 使用量子神经网络拟合一维噪声数据
    |'
- en: '| [The quantum graph recurrent neural network](https://oreil.ly/7kEOx) | Xanadu
    (PennyLane) | Use a quantum graph recurrent neural network to learn quantum dynamics
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| [量子图递归神经网络](https://oreil.ly/7kEOx) | Xanadu (PennyLane) | 使用量子图递归神经网络学习量子动态
    |'
- en: '| [Learning to learn with quantum neural networks](https://oreil.ly/gsc8E)
    | Xanadu (PennyLane), TensorFlow | Meta-learning technique for variational quantum
    algorithms |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| [使用量子神经网络学习学习](https://oreil.ly/gsc8E) | Xanadu (PennyLane), TensorFlow |
    用于变分量子算法的元学习技术 |'
- en: '| [Quanvolutional neural networks](https://oreil.ly/AQzFz) | Xanadu (PennyLane),
    TensorFlow | Pre-process images with a quantum convolution |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| [量子卷积神经网络](https://oreil.ly/AQzFz) | Xanadu (PennyLane), TensorFlow | 使用量子卷积预处理图像
    |'
- en: '| [Ensemble classification with Forest and Qiskit devices](https://oreil.ly/tfGBM)
    | Xanadu (PennyLane), scikit-learn, Rigetti (Forest), IBM (QSKit) | Use multiple
    QPUs to ensemble quantum classification |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| [使用Forest和Qiskit设备进行集成分类](https://oreil.ly/tfGBM) | Xanadu (PennyLane), scikit-learn,
    Rigetti (Forest), IBM (QSKit) | 使用多个QPUs进行集成量子分类 |'
- en: '| [Quantum GANs](https://oreil.ly/Zbe5n) | Xanadu (PennyLane), PyTorch | Generate
    images with quantum GANs |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| [量子GANs](https://oreil.ly/Zbe5n) | Xanadu (PennyLane), PyTorch | 使用量子GAN生成图像
    |'
- en: '| [How to approximate a classical kernel with a quantum computer](https://oreil.ly/9bQlA)
    | Xanadu (PennyLane) | Estimate a classical kernel function on a quantum computer
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| [如何用量子计算机逼近经典核](https://oreil.ly/9bQlA) | Xanadu (PennyLane) | 在量子计算机上估算经典核函数
    |'
- en: '| [Tensor-network quantum circuits](https://oreil.ly/NiwVn) | Xanadu (PennyLane)
    | Tensor network quantum circuits |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| [张量网络量子电路](https://oreil.ly/NiwVn) | Xanadu (PennyLane) | 张量网络量子电路 |'
- en: '| [Quantum advantage in learning from experiments](https://oreil.ly/pnt2I)
    | Xanadu (PennyLane) | Quantum advantage in learning from experiments |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| [从实验中学习的量子优势](https://oreil.ly/pnt2I) | Xanadu (PennyLane) | 从实验中学习的量子优势
    |'
- en: '| [Machine learning for quantum many-body problems](https://oreil.ly/dARKj)
    | Xanadu (PennyLane) | Machine learning for quantum many-body problems |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| [量子多体问题的机器学习](https://oreil.ly/dARKj) | Xanadu (PennyLane) | 量子多体问题的机器学习
    |'
- en: '| [Function fitting using quantum signal processing](https://oreil.ly/8XCFg)
    | Xanadu (PennyLane), PyTorch | Train polynomial approximations to functions using
    QSP |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| [使用量子信号处理进行函数拟合](https://oreil.ly/8XCFg) | Xanadu (PennyLane), PyTorch |
    使用 QSP 训练多项式近似函数 |'
- en: At first glance, [Table 6-1](#table-quantum-other) seems to be trying to list
    everything about quantum machine learning as a field. That assessment is more
    correct than you realize. In fact, all the resources in [Table 6-1](#table-quantum-other),
    all of these small algorithms run on toy datasets, are roughly the extent of what
    we can do with quantum machine learning currently.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，[Table 6-1](#table-quantum-other) 似乎试图列出量子机器学习作为一个领域的所有内容。这种评估比你意识到的更正确。实际上，[Table 6-1](#table-quantum-other)
    中的所有资源，所有这些小算法在玩具数据集上运行，大致是我们目前能够通过量子机器学习所能做到的。
- en: Why QML Will Not Solve Your Regular ML Problems
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么 QML 不能解决你的常规 ML 问题
- en: Even if quantum computing becomes practical, one should not consider it a panacea
    for all the problems and roadblocks facing classical ML that we’ve described earlier.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 即使量子计算变得实用，也不应将其视为解决我们之前描述的所有经典机器学习问题和障碍的灵丹妙药。
- en: Proponents of quantum computing have excitedly pointed out the application to
    fields like protein folding. However, as demonstrated by [AlphaFold 2](https://oreil.ly/SoraC),
    classical machine algorithms were able to create valid solutions to the problem
    without quantum computing.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 量子计算的支持者们兴奋地指出其在蛋白质折叠等领域的应用。然而，正如 [AlphaFold 2](https://oreil.ly/SoraC) 所示，经典机器算法已能够为问题创建有效的解决方案，而无需量子计算的参与。
- en: Note
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Not only is protein design possible with classical ML, but it can even be run
    in the browser using HuggingFace Spaces. One such space is [ProteinMPNN](https://oreil.ly/xNEZP).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅可以使用经典机器学习进行蛋白质设计，而且可以在浏览器中使用HuggingFace Spaces运行。其中一个空间是 [ProteinMPNN](https://oreil.ly/xNEZP)。
- en: True, quantum information in graph neural networks is important for drug discovery,
    but there is a difference between adding that information as a feature in classical
    ML and using quantum phenomena as the substrate your neural network is running
    on. As a lot of AI drug discovery companies have shown, there is a lot that can
    be done with [just adding that information as a feature](https://oreil.ly/4Faog).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，图神经网络中的量子信息对药物发现很重要，但将该信息作为经典 ML 的特征添加与使用量子现象作为神经网络运行的基质之间存在差异。正如许多人工智能药物发现公司所展示的，仅通过
    [添加该信息作为特征](https://oreil.ly/4Faog) 就能做很多事情。
- en: 'Don’t expect too much from quantum computing to fix the issues of machine learning.
    If anything, it’s currently the opposite: machine learning is being used to solve
    the problems of quantum computing.^([14](ch06.html#idm45621832136768))'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 不要期望量子计算能够解决机器学习的问题。事实上，当前情况恰恰相反：机器学习正被用来解决量子计算的问题。^([14](ch06.html#idm45621832136768))
- en: Coupled with the difficulty of thermodynamic limits in increasing computer size,
    the only likely use for quantum computing in the next 10 to 15 years is to improve
    classical algorithms. Classical ML might end up supporting solutions to quantum
    problems rather than the reverse.^([15](ch06.html#idm45621832131504))
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 再加上增加计算机尺寸中热力学极限的困难，未来10到15年内量子计算唯一可能的用途是改进经典算法。经典机器学习最终可能支持解决量子问题的解决方案，而不是相反。^([15](ch06.html#idm45621832131504))
- en: Making the Leap from Theory to Practice
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跃迁从理论到实践
- en: This book so far has focused on some of the lesser-known techniques for improving
    the performance of machine learning in the real world. This chapter so far has
    focused on the bleeding edge of those techniques, where the distinction between
    theory and practice is still very blurry. In the next chapter, we will go into
    more detail about the general philosophies and testing approaches to keep in mind
    when you’re trying to make the leap from theory to practice.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书侧重于改善现实世界中机器学习性能的一些较少为人知的技术。本章迄今侧重于这些技术的前沿，其中理论与实践的区别仍非常模糊。在下一章中，我们将更详细地讨论在试图从理论到实践时应牢记的一般哲学和测试方法。
- en: ^([1](ch06.html#idm45621833148960-marker)) Sayash Kapoor and Arvind Narayanan,
    [“Eighteen Pitfalls to Beware of in AI Journalism”](https://oreil.ly/qtXa0), *AI
    Snake Oil* (blog), September 30, 2022.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.html#idm45621833148960-marker)) Sayash Kapoor和Arvind Narayanan，[“AI新闻报道中的十八个陷阱”](https://oreil.ly/qtXa0)，*AI蛇油*（博客），2022年9月30日。
- en: ^([2](ch06.html#idm45621833133088-marker)) Mary L. Gray, [“Paradox of Automation’s
    Last Mile”](https://oreil.ly/lQRRg), *Social Media Collective* (blog), November
    12, 2015.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.html#idm45621833133088-marker)) Mary L. Gray，[“自动化最后一英里的悖论”](https://oreil.ly/lQRRg)，*社交媒体集体*（博客），2015年11月12日。
- en: ^([3](ch06.html#idm45621833127024-marker)) Joshua A. Kroll, [“The Fallacy of
    Inscrutability”](http://dx.doi.org/10.1098/rsta.2018.0084), *Phil. Trans. R. Soc.
    A.* 376 (October 15, 2018).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.html#idm45621833127024-marker)) Joshua A. Kroll，[“不可测性的谬误”](http://dx.doi.org/10.1098/rsta.2018.0084)，*Phil.
    Trans. R. Soc. A.* 376（2018年10月15日）。
- en: '^([4](ch06.html#idm45621833110816-marker)) Emily M. Bender, [“On NYT Magazine
    on AI: Resist the Urge to Be Impressed”](https://oreil.ly/UH22m), *Medium* (blog),
    April 17, 2022.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06.html#idm45621833110816-marker)) Emily M. Bender，[“关于《纽约时报》上的人工智能：抵制被印象深刻的冲动”](https://oreil.ly/UH22m)，*Medium*（博客），2022年4月17日。
- en: ^([5](ch06.html#idm45621833106448-marker)) Sayash Kapoor and Arvind Narayanan,
    [“A Checklist of Eighteen Pitfalls in AI Journalism”](https://bit.ly/3rp3u26),
    September 30, 2022.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch06.html#idm45621833106448-marker)) Sayash Kapoor和Arvind Narayanan，[“AI新闻报道中的十八个陷阱清单”](https://bit.ly/3rp3u26)，2022年9月30日。
- en: ^([6](ch06.html#idm45621833104096-marker)) For a good example of this “security
    mindset” in ML, the [*Inverse Scaling Prize*](https://oreil.ly/phCXa) is aimed
    at finding tasks that larger models perform worse on than smaller models.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch06.html#idm45621833104096-marker)) 想了解机器学习中“安全思维”更深入的例子，请看[*Inverse
    Scaling Prize*](https://oreil.ly/phCXa)，旨在发现大型模型表现比小型模型更差的任务。
- en: ^([7](ch06.html#idm45621833075600-marker)) Younes Belkada and Tim Dettmers,
    [“A Gentle Introduction to 8-bit Matrix Multiplication for Transformers at Scale
    Using Hugging Face Transformers, Accelerate and bitsandbytes”](https://oreil.ly/KKTUG),
    *HuggingFace* (blog), August 18, 2022.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch06.html#idm45621833075600-marker)) Younes Belkada和Tim Dettmers，[“使用Hugging
    Face Transformers，Accelerate和bitsandbytes进行大规模Transformer的8位矩阵乘法简介”](https://oreil.ly/KKTUG)，*HuggingFace*（博客），2022年8月18日。
- en: '^([8](ch06.html#idm45621833026688-marker)) Tim Dettmers et al., [“LLM.int8():
    8-bit Matrix Multiplication for Transformers at Scale”](https://arxiv.org/abs/2208.07339),
    *NeurIPS 2022* (2022).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch06.html#idm45621833026688-marker)) Tim Dettmers等，[“LLM.int8(): 大规模Transformer的8位矩阵乘法”](https://arxiv.org/abs/2208.07339)，*NeurIPS
    2022*（2022年）。'
- en: '^([9](ch06.html#idm45621833024384-marker)) Andrey Kuzmin et al., [“FP8 Quantization:
    The Power of the Exponent”](https://arxiv.org/abs/2208.09225), *arXiv preprint*
    (2022).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch06.html#idm45621833024384-marker)) Andrey Kuzmin等，[“FP8量化：指数幂的威力”](https://arxiv.org/abs/2208.09225)，*arXiv预印本*，2022年。
- en: '^([10](ch06.html#idm45621833008720-marker)) DeepSpeed Team and Andrey Proskurin,
    [“DeepSpeed Compression: A Composable Library for Extreme Compression and Zero-Cost
    Quantization,”](https://oreil.ly/dwsOU) *Microsoft Research Blog*, July 20, 2022.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch06.html#idm45621833008720-marker)) DeepSpeed团队和Andrey Proskurin，[“DeepSpeed压缩：极端压缩和零成本量化的可组合库”](https://oreil.ly/dwsOU)，*Microsoft
    Research Blog*，2022年7月20日。
- en: ^([11](ch06.html#idm45621832947760-marker)) Alireza Mohammadshahi et al., [“What
    Do Compressed Multilingual Machine Translation Models Forget?”](https://arxiv.org/abs/2205.10828),
    *Findings of EMNLP 2022* (2022).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch06.html#idm45621832947760-marker)) Alireza Mohammadshahi等，[“压缩多语言机器翻译模型忽略了什么？”](https://arxiv.org/abs/2205.10828)，*EMNLP
    2022发现*（2022年）。
- en: ^([12](ch06.html#idm45621832937856-marker)) For a more in-depth code exploration
    of how diffusion models work, check out [“Denoising Diffusion Probabilistic Models
    (DDPM)”](https://oreil.ly/D4xTZ).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch06.html#idm45621832937856-marker)) 欲深入探索扩散模型如何工作的代码，请查看[“去噪扩散概率模型（DDPM）”](https://oreil.ly/D4xTZ)。
- en: ^([13](ch06.html#idm45621832210912-marker)) Bruce Schneier, [“NIST Post-Quantum
    Cryptography Standards”](https://oreil.ly/bW2Tc), *Schneier on Security* (blog),
    August 8, 2022.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch06.html#idm45621832210912-marker)) Bruce Schneier，[“NIST后量子密码标准”](https://oreil.ly/bW2Tc)，*Schneier
    on Security*（博客），2022年8月8日。
- en: ^([14](ch06.html#idm45621832136768-marker)) Max G. Levy, [“Machine Learning
    Gets a Quantum Speedup”](https://oreil.ly/E7ivo), *Quanta Magazine*, February
    4, 2022.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch06.html#idm45621832136768-marker)) Max G. Levy，[“机器学习获得量子加速”](https://oreil.ly/E7ivo)，*Quanta
    Magazine*，2022年2月4日。
- en: ^([15](ch06.html#idm45621832131504-marker)) Hsin-Yuan Huang et al., [“Provably
    Efficient Machine Learning for Quantum Many-Body Problems”](https://oreil.ly/X6Emj),
    *Science*, September 23, 2022.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch06.html#idm45621832131504-marker)) Hsin-Yuan Huang等，[“量子多体问题的可证明高效机器学习”](https://oreil.ly/X6Emj)，*Science*，2022年9月23日。
