- en: Chapter 1\. Introducing Synthesis and Simulation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 介绍合成和模拟
- en: The world is hungry for data. Machine learning and artificial intelligence are
    some of the most data-hungry domains around. Algorithms and models are growing
    ever bigger, and the real world is insufficient. Manual creation of data and real-world
    systems are not scalable, and we need new approaches. That’s where Unity, and
    software traditionally used for video game development, steps in.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 世界对数据需求迫切。机器学习和人工智能是最需要数据的领域之一。算法和模型不断增大，而现实世界的数据却是有限的。手动创建数据和现实世界系统并不可扩展，我们需要新的方法。这就是Unity以及传统用于视频游戏开发的软件发挥作用的地方。
- en: This book is all about synthesis and simulation, and leveraging the power of
    modern video game engines for machine learning. Combining machine learning with
    simulations and synthetic data sounds relatively straightforward on the surface,
    but the reality is the idea of including *video game* technology in the serious
    business world of machine learning scares an unreasonable number of companies
    and businesses away from the idea.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书关注合成和模拟，并利用现代视频游戏引擎在机器学习中的强大力量。表面上，将机器学习与模拟及合成数据结合起来似乎相对简单，但事实上，将*视频游戏*技术引入机器学习的严肃商业世界，却令许多公司和企业望而却步。
- en: We hope this book will steer you into this world and alleviate your concerns.
    Three of the authors of this book are video game developers with a significant
    background in computer science, and one is a serious machine learning and data
    scientist. Our combined perspectives and knowledge, built over many years in a
    variety of industries and approaches, are presented here for you.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望本书能引导你进入这个世界，并减少你的顾虑。本书的三位作者是具有丰富计算机科学背景的视频游戏开发者，还有一位是认真的机器学习和数据科学家。我们在多年间在各种行业和方法中积累的综合视角和知识，在这里呈现给你。
- en: 'This book will take you on a journey through the approaches and techniques
    that can be used to build and train machine learning systems using, and using
    data generated by, the Unity video game engine. There are two distinct domains
    in this book: *simulation* and *synthesis*. Simulation refers to, for all intents
    and purposes, building virtual robots (known as *agents*) that learn to do something
    inside a virtual world of your own creation. Synthesis refers to building virtual
    objects or worlds, outputting data about those objects and worlds, and using it
    to train machine learning systems outside of a game engine.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将带你探索使用Unity视频游戏引擎来构建和训练机器学习系统的方法和技术，以及使用和生成的数据。本书有两个明显的领域：*模拟* 和 *合成*。模拟指的是在你自己创建的虚拟世界中构建学习做某事的虚拟机器人（称为*代理*）。合成指的是构建虚拟对象或世界，输出有关这些对象和世界的数据，并将其用于在游戏引擎之外训练机器学习系统。
- en: Both simulation and synthesis are powerful techniques that enable new and exciting
    approaches to data-centric machine learning and AI.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟和合成都是强大的技术，能够为以数据为中心的机器学习和人工智能带来新的和令人兴奋的方法。
- en: A Whole New World of ML
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个全新的ML世界
- en: 'We’ll get to the structure of the book shortly, but first, here’s a synopsis
    of the remainder of this chapter, which is split into four sections:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 不久后我们将进入本书的结构，但首先，这里是本章剩余部分的概述，分为四个部分：
- en: 'In [“The Domains”](#ch1-domains), we’ll introduce the domains of machine learning
    that the book explores: simulation and synthesis.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [“领域”](#ch1-domains) 中，我们将介绍本书探索的机器学习领域：模拟与合成。
- en: In [“The Tools”](#ch1-tools), we’ll meet the tools we’ll be using—the *Unity
    engine*, the *Unity ML-Agents Toolkit*, *PyTorch*, and *Unity Perception*—and
    how they fit together.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [“工具”](#ch1-tools) 中，我们将会介绍我们将要使用的工具——*Unity引擎*，*Unity ML-Agents Toolkit*，*PyTorch*
    和 *Unity Perception*，以及它们如何结合在一起。
- en: 'In [“The Techniques”](#ch1-techniques), we’ll look at the techniques we’ll
    be using for machine learning: *proximal policy optimization* (PPO), *soft actor-critic*
    (SAC), *behavioral cloning* (BC), and *generative adversarial imitation learning*
    (GAIL).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [“技术”](#ch1-techniques) 中，我们将探讨我们将用于机器学习的技术：*近端策略优化*（PPO），*软演员-评论家*（SAC），*行为克隆*（BC）和
    *生成对抗性模仿学习*（GAIL）。
- en: And finally, in [“Projects”](#ch1-projects), we’ll summarize the projects that
    we’ll be building throughout this book, and how they relate to the domains and
    the tools.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，在 [“项目”](#ch1-projects) 中，我们将总结我们将在本书中构建的项目，以及它们与领域和工具的关系。
- en: By the end of this chapter, you’ll be ready to dive into the world of simulations
    and synthesis, you’ll know at a high level how a game engine works, and you’ll
    see why it’s a nearly perfect tool for machine learning. By the end of the book,
    you’ll be ready to tackle any problem you can think of that might benefit from
    game engine-driven simulation or synthesis.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将准备好深入探索模拟和合成的世界，你将了解游戏引擎的工作原理，并理解为什么它几乎是机器学习的完美工具。在本书结束时，你将准备好解决任何可能从游戏引擎驱动的模拟或合成中受益的问题。
- en: The Domains
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 领域
- en: The twin pillars of this book are *simulation* and *synthesis*. In this section,
    we’ll unpack exactly what we mean by each of these terms and how this book will
    explore the concepts.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的双大支柱是*模拟*和*合成*。在本节中，我们将详细解释这两个术语的确切含义，以及本书将如何探索这些概念。
- en: Simulation and synthesis are core parts of the future of artificial intelligence
    and machine learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟和合成是人工智能和机器学习未来的核心部分。
- en: 'Many applications immediately jump out at you: combine simulation with deep
    reinforcement learning to validate how a new robot will function before building
    a physical product; create the brain of your self-driving car without the car;
    build your warehouse and train your pick-and-place robots without the warehouse
    (or the robots).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用立即显而易见：将模拟与深度强化学习结合起来，验证新机器人在建造物理产品之前的功能；在没有汽车的情况下创建你自动驾驶汽车的大脑；在没有仓库（或机器人）的情况下建造你的仓库并训练你的拾取和放置机器人。
- en: 'Other uses are more subtle: synthesize data to create artificial data using
    simulations, instead of information recorded from the real world, and then train
    traditional machine learning models; take real user activity and, with behavioral
    cloning combined with simulations, use it to add a biological- or human-seeming
    element to an otherwise perfect, machine-learned task.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其他用途更微妙：通过模拟来合成数据，而不是使用从真实世界记录的信息，然后训练传统的机器学习模型；利用行为克隆与模拟相结合，以真实用户活动为基础，在本来是完美的机器学习任务中添加生物或人类化的元素。
- en: A video game engine, such as Unity, can simulate enough of the real world, with
    enough fidelity, to be useful for simulation-based machine learning and artificial
    intelligence. Not only can a game engine allow you to simulate enough of a city
    and a car to test, train, and validate a self-driving car deep learning model,
    but it can also simulate the hardware down to the level of engine temperatures,
    power remaining, LIDAR, sonar, x-ray, and beyond. Want to incorporate a fancy,
    expensive new sensor in your robot? Try it out and see if it might improve performance
    before you invest a single cent in new equipment. Save money, time, compute power,
    and engineering resources, and get a better view of your problem space.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个视频游戏引擎，比如Unity，可以模拟足够接近真实世界的环境，并具备足够的逼真度，从而对基于模拟的机器学习和人工智能非常有用。游戏引擎不仅可以让你模拟足够大的城市和汽车来测试、训练和验证自动驾驶汽车深度学习模型，还可以模拟到引擎温度、剩余功率、激光雷达、声纳、X射线等硬件的细节。想要在你的机器人中加入一个新的高端昂贵传感器？在你投资任何一分钱之前试一试，看看它是否能提高性能。节省金钱、时间、计算资源和工程资源，更好地了解你的问题空间。
- en: Is it literally impossible, or potentially unsafe, to acquire enough of your
    data? Create a simulation and test your theories. Cheap, unlimited training data
    is only a simulation away.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 是完全不可能的，还是可能不安全的，去获取足够多的你的数据？创建一个模拟并测试你的理论。便宜、无限的训练数据只隔着一个模拟。
- en: Simulation
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟
- en: 'There’s not one specific thing that we refer to when we say *simulation*. Simulation,
    in this context, can mean practically any use of a game engine to develop a scene
    or environment where machine learning is then applied. In this book, we use simulation
    as a term to broadly refer to the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说*模拟*时，我们并不指代一个具体的东西。在这个背景下，模拟可以广泛指使用游戏引擎开发场景或环境，然后应用机器学习。在本书中，我们将模拟作为一个术语，广泛涵盖以下内容：
- en: Using a game engine to create an environment with certain components that are
    the agent or agents
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用游戏引擎创建一个具有特定组件的环境，这些组件是代理或者代理们
- en: Giving the agent(s) the ability to move, or otherwise interact or work with,
    the environment and/or other agents
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赋予代理（们）移动的能力，或者与环境和/或其他代理进行互动或工作
- en: Connecting the environment to a machine learning framework to train a model
    that can operate the agent(s) within the environment
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将环境与机器学习框架连接起来，训练一个能够在环境中操作代理（们）的模型
- en: Using that trained model to operate with the environment in the future, or connecting
    the model to a similarly equipped agent elsewhere (e.g., in the real world, with
    an actual robot)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练好的模型来操作未来的环境，或将模型连接到其他同样配备的代理人（例如在真实世界中，与实际机器人）
- en: Synthesis
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合成
- en: 'Synthesis is a significantly easier thing to pin down: *synthesis*, in the
    context of this book, is the creation of ostensibly fake training data using a
    game engine. For example, if you were building some kind of image identification
    machine learning model for a supermarket, you might need to take photos of a box
    of a specific cereal brand from many different angles and with many different
    backgrounds and contexts.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的背景下，合成是一件相对容易明确的事情：*合成*是使用游戏引擎创建表面上看来是虚假的训练数据。例如，如果你为超市建立某种图像识别机器学习模型，你可能需要从多个角度和不同背景及环境中拍摄特定麦片品牌的盒子的照片。
- en: Using a game engine, you could create and load a 3D model of a box of cereal
    and then generate thousands of images of it—synthesizing them—in different angles,
    backgrounds, and skews, and save them out to a standard image format (JPG or PNG,
    for example). Then, with your enormous trove of training data, you could use a
    perfectly standard machine learning framework and toolkit (e.g., TensorFlow, PyTorch,
    Create ML, Turi Create, or one of the many web services-based training systems)
    and train a model that can recognize your cereal box.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用游戏引擎，你可以创建和加载一个麦片盒的3D模型，然后在不同角度、背景和倾斜角度下生成数千张图像，将它们综合起来，并保存为标准图像格式（例如 JPG
    或 PNG）。然后，利用你庞大的训练数据，你可以使用完全标准的机器学习框架和工具包（例如 TensorFlow、PyTorch、Create ML、Turi
    Create，或众多基于网络服务的训练系统之一），训练一个能识别你的麦片盒的模型。
- en: This mode could then be deployed to, for example, some sort of on-trolley AI
    system that helps people shop, guides them to the items on their shopping list,
    or helps store staff fill the shelves correctly and conduct inventory forecasting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以将这种模式部署到例如某种购物车上的AI系统中，帮助人们购物，引导他们找到购物清单上的物品，或帮助店员正确放置货架并进行库存预测。
- en: The synthesis is the creation of the training data by using the game engine,
    and the game engine often has nothing, or very little, to do with the training
    process itself.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 合成是使用游戏引擎创建训练数据，而游戏引擎本身通常与训练过程无关或关联很少。
- en: The Tools
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具
- en: This chapter provides you with an introduction to the tools that we’ll be using
    on our journey. If you’re not a game developer, the primary new tool you’ll encounter
    is Unity. Unity was traditionally a game engine but is now billed as a real-time
    3D engine.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您介绍了我们将在旅程中使用的工具。如果您不是游戏开发者，那么您将遇到的主要新工具是 Unity。Unity 传统上是一个游戏引擎，但现在被推广为实时3D引擎。
- en: Let’s go one by one through the tools you’ll encounter in this book.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一介绍本书中你将遇到的工具。
- en: Unity
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Unity
- en: First and foremost, Unity is a game and visual effects engine. Unity Technologies
    describes Unity as a *real-time 3D development platform*. We’re not going to repeat
    the marketing material from the Unity website for you, but if you’re curious about
    how the company positions itself, you can [check it out](https://oreil.ly/nnVUz).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要明确，Unity 是一款游戏和视觉效果引擎。Unity Technologies 将 Unity 描述为*实时3D开发平台*。我们不会为你重复 Unity
    网站上的营销材料，但如果你对公司如何定位感兴趣，你可以[查看它](https://oreil.ly/nnVUz)。
- en: Tip
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: This book isn’t here to teach you the fundamentals of Unity. Some of the authors
    of this book have already written several books on that—from a game development
    perspective—and you can find those at O’Reilly Media if you’re interested. You
    don’t need to learn Unity as a game developer to make use of it for simulation
    and synthesis with machine learning; in this book we’ll teach you *just enough
    Unity* to be effective at this.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不旨在教授你 Unity 的基础知识。本书的一些作者已经从游戏开发的角度撰写了几本书籍，如果你感兴趣，你可以在 O'Reilly Media 找到它们。你不需要像游戏开发者那样学习
    Unity，以便在模拟和机器学习合成中使用它；在本书中，我们将*只学习足够的 Unity*以在这方面取得效果。
- en: The Unity user interface looks like almost every other professional software
    package that has 3D features. We’ve included an example screenshot in [Figure 1-1](#fig:Unity).
    The interface has panes that can be manipulated, a 3D canvas for working with
    objects, and lots of settings. We’ll come back to the specifics of Unity’s user
    interface later. You can get a solid overview of its different elements in [the
    Unity documentation](https://oreil.ly/zN8xU).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Unity 用户界面看起来几乎与其他拥有 3D 功能的专业软件包相同。我们在[Figure 1-1](#fig:Unity)中包含了一个示例截图。该界面有可以操作的窗格，用于处理对象的
    3D 画布以及许多设置。我们稍后会回到 Unity 用户界面的具体内容。你可以在[Unity 文档](https://oreil.ly/zN8xU)中获取其不同元素的全面概述。
- en: You’ll be using Unity for both simulation and synthesis in this book.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中你将同时使用 Unity 进行仿真和合成。
- en: '![psml 0101](assets/psml_0101.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0101](assets/psml_0101.png)'
- en: Figure 1-1\. The Unity user interface
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 1-1\. Unity 用户界面
- en: The Unity engine comes with a robust set of tools that allow you to simulate
    gravity, forces, friction, movement, sensors of various kinds, and more. These
    tools are the exact set of tools needed to build a modern video game. It turns
    out that these are also the exact same set of tools needed to create simulations
    and to synthesize data for machine learning. But you probably already guessed
    that, given that you’re reading our book.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Unity 引擎配备了一套强大的工具，允许你模拟重力、力量、摩擦、运动、各种传感器等。这些工具正是构建现代视频游戏所需的完整工具集。事实证明，这些工具也是创建仿真和合成数据所需的完全相同的工具集。但是，考虑到你正在阅读我们的书籍，你可能已经猜到了这一点。
- en: Note
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This book was written for Unity 2021 and newer. If you’re reading this book
    in 2023 or beyond, Unity might look slightly different from our screenshots, but
    the concepts and overall flow shouldn’t have changed much. Game engines tend to,
    by and large, accumulate features rather than remove them, so the most common
    sorts of changes you’ll see are icons looking slightly different and things of
    that nature. For the latest notes on anything that might have changed, head to
    our [special website for the book](https://oreil.ly/1efRA).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本书适用于 Unity 2021 及更新版本。如果你在 2023 年或之后阅读本书，Unity 的界面可能与我们的截图略有不同，但是概念和整体流程应该没有太大变化。游戏引擎通常会不断添加功能，而不是移除它们，所以你可能会看到的最常见的变化是图标看起来略有不同之类的事情。关于任何可能发生变化的最新注释，请访问我们的[专用书籍网站](https://oreil.ly/1efRA)。
- en: PyTorch via Unity ML-Agents
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 Unity ML-Agents 的 PyTorch
- en: 'If you’re in the machine learning space, you’ve probably heard of the PyTorch
    open source project. As one of the most popular platforms and ecosystems for machine
    learning in both academia and industry, it’s nearly ubiquitous. In the simulation
    and synthesis space, it’s no different: PyTorch is one of the go-to frameworks.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你身处机器学习领域，你可能已经听说过 PyTorch 开源项目。作为学术界和工业界最受欢迎的机器学习平台和生态系统之一，它几乎无所不在。在模拟和综合空间中，情况也一样：PyTorch
    是首选框架之一。
- en: In this book, the underlying machine learning that we explore will mostly be
    done via PyTorch. We won’t be getting into the *weeds* of PyTorch, because much
    of the work we’ll be doing with PyTorch will be via the Unity ML-Agents Toolkit.
    We’ll be discussing the ML-Agents Toolkit momentarily, but essentially all you
    need to remember is that PyTorch is the *engine* that powers what the Unity ML-Agents
    Toolkit does. It’s there all the time, under the hood, and you can tinker with
    it if you need to, or if you know what you’re doing, but most of the time you
    don’t need to touch it at all.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们探索的基础机器学习大部分将通过 PyTorch 完成。我们不会深入探讨 PyTorch 的*细枝末节*，因为我们将大部分时间通过 Unity
    ML-Agents 工具包来使用 PyTorch。我们马上会讨论 ML-Agents 工具包，但你需要记住的是，PyTorch 是驱动 Unity ML-Agents
    工具包所做工作的*引擎*。它一直在那里，在幕后运行，如果需要或者你知道自己在做什么，你可以对其进行调整，但大部分时间你根本不需要碰它。
- en: Tip
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: We’re going to spend the rest of this section discussing the Unity ML-Agents
    Toolkit, so if you need a refresher on PyTorch, we highly recommend the [PyTorch
    website](https://pytorch.org), or one of the many excellent books that O’Reilly
    Media has published on the subject.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节的其余部分讨论 Unity ML-Agents 工具包，所以如果你需要回顾 PyTorch，我们强烈推荐访问[PyTorch 网站](https://pytorch.org)，或者
    O’Reilly Media 出版的关于该主题的众多优秀书籍之一。
- en: PyTorch is a library that provides support for performing computations using
    data flow graphs. It supports both training and inference using CPUs and GPUs
    (and other specialized machine learning hardware), and it runs on a huge variety
    of platforms ranging from serious ML-optimized servers to mobile devices.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是一个库，支持使用数据流图进行计算。它支持使用 CPU 和 GPU（以及其他专用的机器学习硬件）进行训练和推理，并且可以在从严肃的 ML
    优化服务器到移动设备的各种平台上运行。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Because most of the work you’ll be doing with PyTorch in this book is abstracted
    away, we will rarely be talking in terms of PyTorch itself. So, while it’s in
    the background of almost everything we’re going to explore, your primary interface
    to it will be via the Unity ML-Agents Toolkit and other tools.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在本书中您将使用 PyTorch 的大部分工作都是抽象化的，所以我们很少会直接谈论 PyTorch 本身。因此，虽然它几乎是我们要探索的一切的背景，但您与它的主要接口将是通过
    Unity ML-Agents 工具包和其他工具。
- en: We’ll be using PyTorch, via Unity ML-Agents, for all the simulation activities
    in the book.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 PyTorch，通过 Unity ML-Agents，进行本书中的所有模拟活动。
- en: Unity ML-Agents Toolkit
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Unity ML-Agents 工具包
- en: The Unity ML-Agents Toolkit (which, against Unity branding, we’ll abbreviate
    to *UnityML* or *ML-Agents* much of the time) is the backbone of the work you’ll
    be doing in this book. ML-Agents was initially released as a bare-bones experimental
    project and slowly grew to encompass a range of features that enable the Unity
    engine to serve as the simulation environment for training and exploring intelligent
    agents and other machine learning applications.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Unity ML-Agents 工具包（我们通常将其缩写为 *UnityML* 或 *ML-Agents*）是本书中您将进行的工作的支柱。ML-Agents
    最初是作为一个简陋的实验项目发布的，然后逐渐扩展到包含一系列功能，使 Unity 引擎能够作为训练和探索智能代理和其他机器学习应用的模拟环境。
- en: It’s an open source project that ships with many exciting and well-considered
    examples (as shown in [Figure 1-2](#fig:UnityMLBanner)), and it is freely available
    via [its GitHub project](https://oreil.ly/JPkQ8).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个开源项目，提供了许多令人兴奋和经过深思熟虑的示例（如[图 1-2](#fig:UnityMLBanner)所示），并且可以通过[其 GitHub
    项目](https://oreil.ly/JPkQ8)免费获取。
- en: '![psml 0102](assets/psml_0102.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0102](assets/psml_0102.png)'
- en: Figure 1-2\. The “hero image” of the Unity ML-Agents Toolkit, showing some of
    Unity’s example characters
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2\. Unity ML-Agents 工具包的“主题图像”，展示了一些 Unity 的示例角色
- en: If it wasn’t obvious, we’ll be using ML-Agents for all the simulation activities
    in the book. We’ll show you how to get ML-Agents up and running on your own system
    in [Chapter 2](ch02.html#chapter-creating-first-simulation). Don’t rush off to
    install it just yet!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不明显的话，我们将在本书中的所有模拟活动中使用 ML-Agents。我们将向您展示如何在您自己的系统上运行 ML-Agents，在[第 2 章](ch02.html#chapter-creating-first-simulation)中。不要急着安装它！
- en: Unity Perception
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Unity Perception
- en: The Unity Perception package (which we’ll abbreviate to *Perception* much of
    the time) is the tool we’ll be using to generate synthetic data. Unity Perception
    provides a collection of additional features to the Unity Editor that allow you
    to set scenes up appropriately to create *fake* data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Unity Perception 包（我们通常将其缩写为 *Perception*）是我们将使用来生成合成数据的工具。Unity Perception
    提供了一系列额外功能给 Unity 编辑器，允许您适当设置场景以创建 *伪造* 数据。
- en: Like ML-Agents, Perception is an open source project, and you can find it via
    [its GitHub project](https://oreil.ly/KbvHj).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 像 ML-Agents 一样，Perception 也是一个开源项目，您可以通过[其 GitHub 项目](https://oreil.ly/KbvHj)找到它。
- en: The Techniques
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术
- en: The ML-Agents Toolkit supports training using either, or a combination of, *reinforcement
    learning* and *imitation learning* techniques. Each of these will allow an agent
    to “learn” a desired behavior through repetitive trial and error—or “reinforcement”—and
    eventually converge on the ideal behavior for the provided success criteria. What
    differs between these techniques are the criteria, which are used to assess and
    optimize agent performance throughout.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents 工具包支持使用 *强化学习* 和 *模仿学习* 技术进行训练。这两种技术都允许代理通过反复的试验和错误（或“强化”）来“学习”所需的行为，并最终在提供的成功标准下收敛到理想的行为。这些技术的区别在于用于评估和优化代理性能的标准。
- en: Reinforcement Learning
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习
- en: '*Reinforcement learning* (RL) refers to learning processes that employ explicit
    rewards. It’s up to the implementation to award “points” for desirable behaviors
    and to deduct them for undesirable behaviors.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习*（RL）是指使用显式奖励进行学习过程。实施方负责为可取行为奖励“分数”，并为不良行为扣除分数。'
- en: At this point you may be thinking, *If I have to tell it what to do and what
    not to do, what’s the point of machine learning?* But let’s think, as an example,
    of teaching a bipedal agent to walk. Giving an explicit set of instructions for
    each state change required to walk—the exact degree of rotation each joint should
    take, in sequence—would be extensive and complex.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能会想，“如果我不得不告诉它该做什么和不该做什么，机器学习的意义何在？” 但让我们来想象一下，例如，教一个双足代理行走。为每个步态变化的每个状态变化提供明确的一套指令——每个关节应该以多少度的旋转顺序进行——将是广泛且复杂的。
- en: But by giving an agent a few points for moving toward a finish line, lots of
    points for reaching it, negative points when it falls over, and several hundred
    thousand attempts to get it right, it will be able to figure out the specifics
    on its own. So, RL’s great strength is in the ability to give goal-centric instructions
    that require complex behaviors to achieve.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 但是通过给代理几个点以向前移动，达到终点时给很多点，它跌倒时给负点，并尝试数十万次以获得正确结果，它将能够自行找出具体细节。因此，RL 的巨大优势在于能够给出以目标为中心的指令，需要复杂行为来实现。
- en: 'The ML-Agents framework ships with implementations for two different RL algorithms
    built in: *proximal policy optimization* (PPO) and *soft actor-critic* (SAC).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents 框架提供了两种不同的 RL 算法实现：*近端策略优化*（PPO）和 *软演员-评论家*（SAC）。
- en: Warning
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Take note of the acronyms for these techniques and algorithms: RL, PPO, and
    SAC. Memorize them. We’ll be using them often throughout the book.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这些技术和算法的缩写：RL、PPO 和 SAC。记住它们，我们将在整本书中经常使用它们。
- en: '*PPO* is a powerful, general-purpose RL algorithm that’s repeatedly been proven
    to be highly effective and generally stable across a range of applications. PPO
    is the default algorithm used in ML-Agents, and it will be used for most of this
    book. We’ll be exploring in more detail how PPO works a little later on.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*PPO* 是一个强大的通用 RL 算法，已经反复证明在各种应用中非常有效且通常稳定。PPO 是 ML-Agents 中使用的默认算法，并且将在本书的大部分内容中使用。稍后我们将更详细地探讨
    PPO 的工作原理。'
- en: Tip
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Proximal policy optimization was created by the team at OpenAI and debuted in
    2017\. You can read the [original paper on arXiv](https://oreil.ly/JHfhI), if
    you’re interested in diving into the details.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 近端策略优化是由 OpenAI 团队创建的，并于2017年首次亮相。如果您对细节感兴趣，可以阅读 [arXiv 上的原始论文](https://oreil.ly/JHfhI)。
- en: '*SAC* is an *off-policy* RL algorithm. We’ll get to what that means a little
    later, but for now, it generally offers a reduction in the number of training
    cycles needed in return for increased memory requirements. This makes it a better
    choice for slow training environments when compared to an *on-policy* approach
    like PPO. We’ll be using SAC once or twice in this book, and we’ll explore how
    it works in a little more detail when we get there.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*SAC* 是一个 *离线* RL 算法。稍后我们将详细讨论其含义，但目前来看，相对于像 PPO 这样的 *在线* 方法，它通常减少了所需的训练周期，但增加了内存需求。在本书中我们将会使用一两次
    SAC，并在到达时更详细地探讨其工作原理。'
- en: Tip
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Soft actor-critic was created by the Berkeley Artificial Intelligence Research
    (BAIR) group and debuted in December 2018\. You can read the [original release
    documentation](https://oreil.ly/7kNmg) for the details.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 软演员-评论家是由伯克利人工智能研究组（BAIR）创建的，并于2018年12月首次亮相。您可以阅读 [原始发布文档](https://oreil.ly/7kNmg)
    以获取详细信息。
- en: Imitation Learning
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模仿学习
- en: Similar to RL, *imitation learning* (IL) removes the need to define complex
    instructions in favor of simply setting objectives. However, IL also removes the
    need to define explicit objectives or rewards. Instead, a demonstration is given—usually
    a recording of the agent being manually controlled by a human—and rewards are
    defined *intrinsically* based on the agent *imitating* the behavior being demonstrated.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 RL，*模仿学习*（IL）消除了需要定义复杂指令的需求，而是简单地设定目标。然而，IL也消除了需要定义显式目标或奖励的必要性。相反，通常会提供一个演示——通常是一个由人手动控制的代理的录像——并根据代理模仿所展示行为来内在定义奖励。
- en: This is great for complex domains in which the desirable behaviors are highly
    specific or the vast majority of possible actions are undesirable. Training with
    IL is also highly effective for multistage objectives—where an agent needs to
    achieve intermediate objectives in a certain order to receive a reward.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于复杂领域尤其有用，其中期望的行为非常具体或者大多数可能的行为都是不可取的。对于多阶段目标（代理需要按特定顺序实现中间目标以获得奖励），使用 IL
    进行训练也非常有效。
- en: 'The ML-Agents framework ships with implementations for two different IL algorithms
    built in: *behavioral cloning* (BC) and *generative adversarial imitation learning*
    (GAIL).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents框架内置了两种不同的IL算法的实现：*行为克隆*（BC）和*生成对抗模仿学习*（GAIL）。
- en: '*BC* is an IL algorithm that trains an agent to precisely mimic the demonstrated
    behavior. Here, BC is only responsible for defining and allocating intrinsic rewards;
    an existing RL approach such as PPO or SAC is employed for the underlying training
    process.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*BC*是一种IL算法，用于训练代理精确模仿演示的行为。在这里，BC仅负责定义和分配内在奖励；而现有的RL方法，如PPO或SAC，则用于基础训练过程中。'
- en: '*GAIL* is a generative adversarial approach, applied to IL. In GAIL, two separate
    models are pitted against each other during training: one is the agent behavior
    model, which does its best to mimic the given demonstration; the other is a discriminator,
    which is repeatedly served either a snippet of human-driven demonstrator behavior
    or agent-driven model behavior and must guess which one it is.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*GAIL*是一种生成对抗方法，应用于IL。在GAIL中，两个独立的模型在训练过程中互相对抗：一个是代理行为模型，它尽最大努力模仿给定的演示；另一个是鉴别器，它会反复接收人类驱动的演示者行为或代理驱动的模型行为的片段，并且必须猜测是哪一个。'
- en: Tip
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: GAIL originated in Jonathan Ho and Stefano Ermon’s paper [“Generative Adversarial
    Imitation Learning”](https://oreil.ly/bokpR).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: GAIL源于Jonathan Ho和Stefano Ermon的论文[“生成对抗模仿学习”](https://oreil.ly/bokpR)。
- en: As the discriminator gets better at spotting the mimic, the agent model must
    improve to be able to fool it once again. Likewise, as the agent model improves,
    the discriminator must establish increasingly strict or nuanced internal criteria
    to spot the fake. In this back-and-forth, each is forced to iteratively improve.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随着鉴别器在识别模仿者方面变得更加熟练，代理模型必须改进以再次欺骗它。同样地，随着代理模型的改进，鉴别器必须建立越来越严格或更为微妙的内部标准来识别伪造的行为。在这种来回之间，每个人都被迫进行迭代改进。
- en: Tip
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Behavioral cloning is often the best approach for applications in which it is
    possible to demonstrate all, or almost all, of the conditions that the agent may
    find itself in. GAIL is instead able to extrapolate new behaviors, which allows
    imitation to be learned from limited demonstrations.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 行为克隆通常是应用中最佳的方法，因为可以演示出代理可能遇到的所有或几乎所有条件。相比之下，GAIL能够推广新的行为，这允许从有限的演示中学习模仿。
- en: BC and GAIL can also be used together, often by employing BC in early training
    and then allocating the partially trained behavior model to be the agent half
    of a GAIL model. Starting with BC will often make an agent improve quickly in
    early training, while switching to GAIL in late training will allow it to develop
    behaviors beyond those that were demonstrated.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期训练中通常通过使用BC，BC和GAIL也可以一起使用，然后将部分训练好的行为模型分配为GAIL模型的代理部分。从BC开始，通常会使代理在早期训练中迅速改进，而在后期训练中切换到GAIL将允许其开发超出演示的行为。
- en: Hybrid Learning
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合学习
- en: Though RL or IL alone will almost always do the trick, they can be combined.
    An agent can then be rewarded—and its behavior informed—by both explicitly defined
    rewards for achieving objectives *and* implicit rewards for effective imitation.
    The weights of each can even be tuned so that an agent can be trained to prioritize
    one as the primary objective or both as equal objectives.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然单独使用RL或IL几乎总是可以达到目的，它们可以结合使用。然后，代理可以通过明确定义的达成目标的奖励和有效模仿的隐式奖励来获得奖励并且其行为得到指导。甚至可以调整每种奖励的权重，以便训练代理将其中一个作为主要目标或两者作为平等目标。
- en: In hybrid training, the IL demonstration serves to put the agent on the right
    path early in training, while explicit RL rewards encourage specific behavior
    within or beyond that. This is necessary in domains where the ideal agent should
    outperform the human demonstrator. Because of that early hand-holding, training
    with RL and IL together can make it significantly faster to train an agent to
    solve complex problems or navigate a complex environment in a scenario with sparse
    rewards.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在混合训练中，IL演示旨在在训练初期将代理放在正确的路径上，而显式的RL奖励则鼓励在此之内或之外的特定行为。这在理想的代理应该优于人类演示者的领域中是必要的。由于这种早期的辅助作用，同时使用RL和IL训练可以显著加快训练速度，让代理在解决复杂问题或在稀疏奖励情境中导航复杂环境时更快地训练出解决方案。
- en: Tip
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: '*Sparse-reward environments* are those in which the agent is rewarded especially
    infrequently with explicit rewards. In such an environment, the time it takes
    for an agent to “accidentally” stumble upon a rewardable behavior—and thus receive
    its first indication of what it should be doing—can waste much of the available
    training time. But combined with IL, the demonstration can inform on desirable
    behaviors that work toward explicit rewards.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*稀疏奖励环境*是那些代理特别少接收显式奖励的环境。在这样的环境中，代理“偶然”发现一个可奖励行为并因此收到其首次指示应该做什么，可能会浪费大部分可用的训练时间。但与
    IL 结合，演示可以提供有关朝向显式奖励的理想行为的信息。'
- en: Together these produce a complex rewards scheme that can encourage highly specific
    behaviors from an agent, but applications that require this level of complexity
    for an agent to succeed are few.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些共同产生了一个复杂的奖励方案，可以鼓励代理从事高度特定的行为，但是需要代理达到成功的复杂程度的应用程序并不多。
- en: Summary of Techniques
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术总结
- en: This chapter is an introductory survey of concepts and techniques, and you’ll
    be exposed to and use each of the techniques we’ve looked at here over the course
    of this book. In doing so, you’ll become more familiar with how each of them works
    in a practical sense.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是概念和技术的入门调查，你将在本书的过程中接触和使用我们在这里看到的每一种技术。通过这样做，你将更加熟悉它们在实际操作中的工作方式。
- en: 'The gist of it is as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其要义如下：
- en: 'The Unity ML-Agents Toolkit currently provides a selection of training algorithms
    across two categories:'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unity ML-Agents Toolkit 目前提供了跨两个类别的一系列训练算法：
- en: 'For reinforcement learning (RL): proximal policy optimization (PPO) and soft
    actor-critic (SAC)'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于强化学习（RL）：近端策略优化（PPO）和软演员-评论家（SAC）
- en: 'For imitation learning (IL): behavioral cloning (BC) and generative adversarial
    imitation learning (GAIL)'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于模仿学习（IL）：行为克隆（BC）和生成对抗性模仿学习（GAIL）
- en: 'These methods can be used independently or together:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法可以独立使用，也可以一起使用：
- en: RL can be used with PPO or SAC alone, or in conjunction with an IL method such
    as BC.
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL 可以单独使用 PPO 或 SAC，也可以与 BC 等 IL 方法结合使用。
- en: BC can be used alone, as a step on the path to an approach using GAIL, or in
    conjunction with RL.
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: BC 可以单独使用，作为使用 GAIL 方法或与 RL 结合的一步。
- en: RL techniques require a set of defined rewards.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL 技术需要一组定义好的奖励。
- en: IL techniques require some sort of provided demonstration.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IL 技术需要提供一些演示。
- en: Both RL and IL *learn by doing*.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL 和 IL *通过实践学习*。
- en: We’ll be touching on or directly using all these techniques across the remainder
    of the book’s exploration of simulation topics.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书剩余的模拟主题探索过程中，我们将涉及或直接使用所有这些技术。
- en: Projects
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目
- en: This book is a practical, pragmatic work. We want you to get up and running
    using simulations and synthesis as quickly as possible, and we assume you’d prefer
    to focus on the implementation whenever possible.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是一本实际、务实的工作。我们希望你尽快使用模拟和合成开始工作，并且我们假设你在可能的时候更愿意专注于实施。
- en: So, while we do explore behind the scenes often, the meat of the book is in
    the projects we’ll be building together.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然我们经常探索幕后情况，但本书的实质内容在于我们将共同构建的项目。
- en: 'The practical, project-based side of the book is split between the two domains
    we discussed earlier: simulation and synthesis.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的实际基于项目的部分分为我们之前讨论的两个领域：模拟和合成。
- en: Simulation Projects
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟项目
- en: 'Our simulation projects will be varied: when you’re building a simulation environment
    in Unity, there’s a wide range of *ways* in which the agent that exists in the
    environment can *observe* and *sense* its world.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模拟项目将是多样的：当你在 Unity 中构建模拟环境时，代理可以通过多种*方式*观察和*感知*其世界。
- en: 'Some simulation projects will use an agent that observes the world using *vector
    observations*: that is, numbers. Whatever numbers you might want to send it. Literally
    anything you like. Realistically, though, vector observations are usually things
    like the agent’s distance from something, or other positional information. But
    really, any number can be an observation.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模拟项目将使用一个代理来观察世界，使用*向量观察*：也就是说，数字。任何你想发送的数字。从字面上讲，任何你喜欢的数字。实际上，向量观察通常是如代理距离某物的距离或其他位置信息之类的东西。但实际上，任何数字都可以作为一种观察。
- en: 'Some simulation projects will use an agent that observes the world using *visual
    observations*: that is, pictures! Because Unity is a game engine, and game engines,
    like film, have a concept of *cameras*, you can simply (virtually) mount cameras
    on your agent and just have it exist in the game world. The view from these cameras
    can then be fed into your machine learning system, allowing the agent to learn
    about its world based on the camera input.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一些仿真项目将使用一个观察世界的代理，使用*视觉观察*，也就是图片！因为 Unity 是一个游戏引擎，而游戏引擎和电影一样有*摄像机*的概念，所以你可以简单地（虚拟地）在代理身上安装摄像机，并让它存在于游戏世界中。这些摄像机的视角可以输入到你的机器学习系统中，让代理根据摄像机的输入学习它的世界。
- en: 'The simulation examples we’ll be looking at using Unity, ML-Agents, and PyTorch
    include:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Unity、ML-Agents 和 PyTorch 的仿真示例包括：
- en: A ball that can roll itself to a target, in [Chapter 2](ch02.html#chapter-creating-first-simulation)
    (we know, it sounds too amazing to be true, but it is!)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可以自行滚动到目标的球，在[第 2 章](ch02.html#chapter-creating-first-simulation)中（我们知道，听起来太神奇了，但这是真的！）
- en: A cube that can push a block into a goal area, in [Chapter 4](ch04.html#chapter-more-advanced-simulation)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可以推动方块进入目标区域的立方体，在[第 4 章](ch04.html#chapter-more-advanced-simulation)中。
- en: A simple self-driving car navigating a track, in [Chapter 5](ch05.html#chapter-self-driving-car)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一辆简单的自动驾驶汽车在[第 5 章](ch05.html#chapter-self-driving-car)中行驶。
- en: A ball that seeks a coin, trained by imitating human demonstrations, in [Chapter 6](ch06.html#chapter-introduction-to-imitation)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个通过模仿人类演示训练来寻找硬币的球，在[第 6 章](ch06.html#chapter-introduction-to-imitation)中。
- en: An ballistic launcher agent that can launch a ball at a target, using curriculum
    learning, in [Chapter 8](ch08.html#chapter-introduction-to-curriculum-learning)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个弹道发射代理人，可以使用课程学习将球发射到目标位置，在[第 8 章](ch08.html#chapter-introduction-to-curriculum-learning)中。
- en: A group of cubes that work together to push blocks to goals, in [Chapter 9](ch09.html#chapter-coop-intro)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组立方体共同工作，将方块推向目标，在[第 9 章](ch09.html#chapter-coop-intro)中。
- en: Training agents to balance a ball on top of itself, using visual inputs (i.e.,
    cameras) instead of precise measurements, in [Chapter 10](ch10.html#chapter-vision)
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练代理人使用视觉输入（即摄像机），而不是精确测量来平衡球顶部，在[第 10 章](ch10.html#chapter-vision)中。
- en: Connecting to and manipulating ML-Agents with, Python, in [Chapter 11](ch11.html#chapter-python)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python 连接并操作 ML-Agents，在[第 11 章](ch11.html#chapter-python)中。
- en: Synthesis Projects
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 综合项目
- en: Our synthesis projects will be fewer than our simulations because the domain
    is a little simpler. We focus on building on the material supplied by Unity to
    showcase the possibilities of simulation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的综合项目会比仿真项目少一些，因为领域要简单一些。我们专注于利用 Unity 提供的材料来展示仿真的可能性。
- en: 'The synthesis examples we’ll be looking at, using Unity and Perception, include:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Unity 和 Perception 的综合示例包括：
- en: A generator for images of randomly thrown and placed dice, in [Chapter 3](ch03.html#chapter-introducing-synthesis)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个随机投掷和放置骰子图像生成器，在[第 3 章](ch03.html#chapter-introducing-synthesis)中。
- en: Improving the dice image generator by changing the floor and colors of the dice,
    in [Chapter 13](ch13.html#chapter-advanced-synthesis)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过改变骰子图像生成器的地板和颜色来改进，在[第 13 章](ch13.html#chapter-advanced-synthesis)中。
- en: Generating images of supermarket products to allow for out-of-Unity training
    on images with complex backdrops and haphazard positioning, in [Chapter 14](ch14.html#chapter-shop)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成超市产品图像，以允许在具有复杂背景和随意位置的图像上进行Unity之外的训练，在[第 14 章](ch14.html#chapter-shop)中。
- en: We won’t focus on the actual training process once you’ve generated your synthesized
    data, as there are many, many good books and online posts on the subject and we
    only have so many pages in this book.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成了合成数据，我们不会专注于实际的训练过程，因为关于这个主题有许多很好的书籍和在线文章，而我们在这本书中只有有限的页面。
- en: Summary and Next Steps
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与下一步
- en: You’ve taken the first steps, and this chapter contained a bit of the required
    background material. From here onward, we’ll be teaching you by *doing*. This
    book has the word *practical* in the title for a reason, and we want you to get
    a feel for simulation and synthesis by building projects of your own.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经迈出了第一步，本章包含了一些必要的背景材料。从这里开始，我们将通过*实践*来教你。这本书的标题中有*实践*这个词是有原因的，我们希望你通过构建自己的项目来感受仿真和综合。
- en: Note
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can find the code for every example at our [special website for the book](https://oreil.ly/1efRA)—we
    recommend downloading the code only when you need it. We’ll also keep the website
    up-to-date with any changes you should be aware of, so do bookmark it!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我们的[书籍专用网站](https://oreil.ly/1efRA)找到每个示例的代码 — 我们建议你在需要时才下载这些代码。我们也会及时更新网站，以便您了解任何需要注意的更改，请务必收藏该网址！
- en: In the next chapter, we’ll look at how you can create your first simulation,
    implement an agent to *do something* in it, and train a machine learning system
    using reinforcement learning.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何创建您的第一个模拟，实现一个代理程序在其中*执行某些操作*，并使用强化学习训练机器学习系统。
