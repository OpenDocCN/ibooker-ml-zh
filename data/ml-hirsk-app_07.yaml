- en: Chapter 5\. Security for Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 机器学习的安全性
- en: If “the worst enemy of security is complexity,” as Bruce Schneier [claims](https://oreil.ly/jfFU3),
    unduly complex machine learning systems are innately insecure. Other researchers
    have also released numerous studies describing and confirming specific security
    vulnerabilities for ML systems. And we’re now beginning to see how real-world
    attacks occur, like [Islamic State operatives blurring their logos](https://oreil.ly/8mSPC)
    in online content to evade social media filters. Since organizations often take
    measures to secure valuable software and data assets, ML systems should be no
    different. Beyond specific incident response plans, several additional information
    security processes should be applied to ML systems. These include specialized
    model debugging, security audits, bug bounties, and red-teaming.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“安全的最大敌人是复杂性”，正如 Bruce Schneier 所[声称的](https://oreil.ly/jfFU3)，过于复杂的机器学习系统本质上是不安全的。其他研究人员还发布了许多关于
    ML 系统特定安全漏洞的研究，并确认了这些漏洞。我们现在开始看到现实世界中的攻击如何发生，比如 [伊斯兰国运营商在在线内容中模糊其标志](https://oreil.ly/8mSPC)
    以逃避社交媒体过滤器。由于组织通常采取措施保护宝贵的软件和数据资产，ML 系统也应不例外。除了特定的事件响应计划外，还应将几种额外的信息安全流程应用于 ML
    系统。这些包括专门的模型调试、安全审计、漏洞赏金和红队测试。
- en: 'Some of the primary security threats for today’s ML systems include the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当今 ML 系统的一些主要安全威胁包括以下内容：
- en: Insider manipulation of ML system training data or software to alter system
    outcomes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部人员操作 ML 系统训练数据或软件，以改变系统结果
- en: Manipulation of ML system functionality and outcomes by external adversaries
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部对手通过操作 ML 系统功能和结果
- en: Exfiltration of proprietary ML system logic or training data by external adversaries
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部对手窃取专有 ML 系统逻辑或训练数据
- en: Trojans or malware hidden in third-party ML software, models, data, or other
    artifacts
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 木马或恶意软件隐藏在第三方 ML 软件、模型、数据或其他工件中
- en: For mission-critical or otherwise high-stakes deployments of AI, systems should
    be tested and audited for at least these known vulnerabilities. Textbook ML model
    assessment will not detect them, but newer model debugging techniques can help,
    especially when fine-tuned to address specific security vulnerabilities. Audits
    can be conducted internally or by specialist teams in what’s known as “red-teaming,”
    as is done by [Meta](https://oreil.ly/nCqSa). [Bug bounties](https://oreil.ly/rnZ9o),
    or when organizations offer monetary rewards to the public for finding vulnerabilities,
    are another practice from general information security that should probably also
    be applied to ML systems. Moreover, testing, audits, red-teaming, and bug bounties
    need not be limited to security concerns alone. These types of processes can also
    be used to spot other ML system problems, such as those related to bias, instability,
    or a lack of robustness, reliability, or resilience, and spot them before they
    explode into AI incidents.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 AI 的关键或其他高风险的部署，系统应至少测试和审计这些已知的漏洞。传统的 ML 模型评估无法检测到它们，但是较新的模型调试技术可以帮助，特别是在针对特定安全漏洞进行调优时。审计可以由内部或由专业团队进行，这就是所谓的“红队测试”，就像
    [Meta](https://oreil.ly/nCqSa) 所做的那样。[漏洞赏金](https://oreil.ly/rnZ9o)，或者组织向公众提供货币奖励以发现漏洞，是通用信息安全实践的另一种做法，可能也应用于
    ML 系统。此外，测试、审计、红队测试和漏洞赏金不应仅限于安全问题。这些类型的流程也可以用于发现其他与 ML 系统相关的问题，例如偏见、不稳定性或缺乏鲁棒性、可靠性或弹性，并在它们演变为
    AI 事件之前发现它们。
- en: Note
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Audits, red-teaming, and bug bounties need not be limited to security concerns
    alone. Bug bounties can be used to find all manner of problems in public-facing
    ML systems, including bias, unauthorized decisions, and product safety or negligence
    issues, in addition to security and privacy issues.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 审计、红队测试和漏洞赏金不应仅限于安全问题。漏洞赏金可用于发现公共 ML 系统中的各种问题，包括偏见、未经授权的决策以及产品安全或疏忽问题，除了安全和隐私问题。
- en: This chapter explores security basics, like the CIA triad and best practices
    for data scientists, before delving into ML security. ML attacks are discussed
    in detail, including ML-specific attacks and general attacks that are also likely
    to affect ML systems. Countermeasures are then put forward, like specialized robust
    ML defenses and privacy-enhancing technologies (PETs), security-aware model debugging
    and monitoring approaches, and a few more general solutions. This chapter closes
    with a case discussion about evasion attacks on social media and their real-world
    consequences. After reading the chapter, readers should be able to conduct basic
    security audits (or “red-teaming”) on their ML systems, spot problems, and enact
    straightforward countermeasures where necessary. See [Chapter 11](ch11.html#unique_chapter_id_11)
    for ML security code examples.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章节探讨了安全基础知识，如CIA三要素和数据科学家的最佳实践，然后深入讨论了机器学习安全问题。详细讨论了机器学习攻击，包括专门针对机器学习的攻击和可能影响机器学习系统的一般性攻击。接着提出了一些对策，例如专门的强大机器学习防御和隐私增强技术（PETs），安全感知的模型调试和监控方法，以及一些更一般的解决方案。本章以社交媒体逃避攻击及其现实后果案例讨论结束。读者在阅读完本章后，应能够对其机器学习系统进行基本的安全审计（或“红队测试”），发现问题，并在必要时采取简单的对策。参见[第11章](ch11.html#unique_chapter_id_11)获取机器学习安全代码示例。
- en: Security Basics
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全基础知识
- en: There are lots of basic lessons to learn from the broader field of computer
    security that will help harden our ML systems. Before we get into ML hacks and
    countermeasures, we’ll need to go over the importance of an adversarial mindset,
    discuss the CIA triad for identifying security incidents, and highlight a few
    straightforward best practices for security that should be applied to any IT group
    or computer system, including data scientists and ML systems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从计算机安全的广泛领域中学到许多基本的教训，这些教训将有助于加强我们的机器学习系统。在深入讨论机器学习的黑客攻击和对策之前，我们需要重点介绍敌对思维的重要性，讨论用于识别安全事件的CIA三要素，并强调适用于任何IT团队或计算机系统的几项简单的最佳安全实践，包括数据科学家和机器学习系统。
- en: The Adversarial Mindset
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 敌对思维
- en: 'Like many practitioners in hyped technology fields, makers and users of ML
    systems tend to focus on the positives: automation, increased revenues, and the
    sleek coolness of new tech. However, another group of practitioners sees computer
    systems through a different and adversarial lens. Some of those practitioners
    likely work alongside us, helping to protect our organization’s IT systems from
    those that deliberately seek to abuse, attack, hack, and misuse ML systems to
    benefit themselves and do harm to others. A good first step toward learning ML
    security is to adopt such an adversarial mindset, or at least to block out overly
    positive ML hype and think about the intentional abuse and misuse of ML systems.
    And yes, even the one we’re working on right now.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多炒作技术领域的从业者一样，机器学习系统的制造者和用户往往关注积极的方面：自动化、增加的收入以及新技术的时髦性。然而，另一群从业者则通过不同的敌对视角看待计算机系统。其中一些从业者可能与我们并肩工作，帮助保护我们组织的IT系统，以防止那些故意滥用、攻击、入侵和误用机器学习系统以谋取私利并对他人造成伤害的人士。学习机器学习安全的一个良好的第一步是采纳这种敌对思维，或者至少阻止过度乐观的机器学习炒作，思考机器学习系统被故意滥用和误用的情况，包括我们正在开发的系统。
- en: Warning
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Don’t be naive about high-risk ML systems. They can hurt people. People will
    attack them and people will abuse them to harm others.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不要对高风险的机器学习系统天真。它们可能会伤害人们。人们会攻击它们，并会滥用它们来伤害他人。
- en: Maybe a disgruntled coworker poisoned our training data, maybe there is malware
    hidden in binaries associated with some third-party ML software we’re using, maybe
    our model or training data can be extracted through an unprotected endpoint, or
    maybe a botnet could hit our organization’s public-facing IT services with a distributed
    denial-of-service (DDOS) attack, taking down our ML system as collateral damage.
    Although such attacks won’t happen to us every day, they will happen to someone,
    somewhere, frequently. Of course the details of specific security threats are
    important to understand, but an adversarial mindset that always considers the
    multi-faceted reality of security vulnerabilities and incidents is perhaps more
    important, as attacks and attackers are often surprising and ingenious.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 也许一个不满的同事毒害了我们的训练数据，也许某些第三方 ML 软件关联的二进制文件中隐藏了恶意软件，也许我们的模型或训练数据可以通过未受保护的端点提取，或者可能是一个僵尸网络可以通过分布式拒绝服务（DDOS）攻击我们组织的公共面向
    IT 服务，导致我们的 ML 系统成为附带损害。虽然这样的攻击不会每天发生在我们身上，但它们会经常发生在某个地方的某人身上。当然，了解特定安全威胁的细节很重要，但总是考虑安全漏洞和事件多方面现实的对抗思维也许更为重要，因为攻击和攻击者往往是出人意料且巧妙的。
- en: CIA Triad
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CIA 三要素
- en: From a data security perspective, goals and failures are usually defined in
    terms of the confidentiality, integrity, and availability (CIA) triad ([Figure 5-1](#triad)).
    To briefly summarize the triad, data should only be available to authorized users
    (confidentiality), data should be correct and up-to-date (integrity), and data
    should be promptly available when needed (availability). If one of these tenets
    is broken, this is usually a security incident. The CIA triad applies directly
    to malicious access, alteration, or destruction of ML system training data. But
    it might be a bit more difficult to see how the CIA triad applies to an ML system
    issuing decisions or predictions, and ML attacks tend to blend traditional data
    privacy and computer security concerns in confusing ways. So, let’s go over an
    example of each.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据安全的角度来看，通常将目标和失败定义为机密性、完整性和可用性（CIA）三要素（[图 5-1](#triad)）。简要总结三要素，数据应仅对授权用户可用（机密性），数据应正确和及时更新（完整性），数据在需要时应及时可用（可用性）。如果这些原则中的任何一个被违反，通常会发生安全事件。CIA
    三要素直接适用于恶意访问、更改或销毁 ML 系统的训练数据。但可能更难看到 CIA 三要素如何适用于 ML 系统发出决策或预测，而 ML 攻击往往以混淆的方式融合传统数据隐私和计算机安全问题。因此，让我们逐个例子来看一下。
- en: '![mlha 0501](assets/mlha_0501.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0501](assets/mlha_0501.png)'
- en: Figure 5-1\. The CIA triad for information security
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 信息安全的 CIA 三要素
- en: The confidentiality of an ML system can be breached by an inversion attack (see
    [“Model extraction and inversion attacks”](#inversion_attack)) in which a bad
    actor interacts with an API in an appropriate manner, but uses explainable artificial
    intelligence techniques to extract information about our model and training data
    from their submitted input data and our system’s predictions. In a more dangerous
    and sophisticated membership inference attack (see [“Membership inference attacks”](#membership_inference_attack)),
    individual rows of training data, up to entire training datasets, can be extracted
    from ML system APIs or other endpoints. Note that these attacks can happen without
    unauthorized access to training files or databases, but result in the same security
    and privacy harms for our users or for our organization, potentially including
    serious legal liabilities.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ML 系统的机密性可以通过逆推攻击（见[“模型提取和逆推攻击”](#inversion_attack)）来突破，恶意行为者通过适当方式与 API 交互，但使用可解释的人工智能技术从其提交的输入数据和我们系统的预测中提取有关我们模型和训练数据的信息。在更危险和复杂的成员推断攻击中（见[“成员推断攻击”](#membership_inference_attack)），可以从
    ML 系统的 API 或其他端点提取单个训练数据行，直至整个训练数据集。请注意，这些攻击可能发生在未经授权访问训练文件或数据库的情况下，但对我们的用户或组织可能造成相同的安全和隐私危害，甚至可能涉及严重的法律责任。
- en: An ML system’s integrity can be compromised by several means, such as data poisoning
    attacks or adversarial example attacks. In a data poisoning attack (see [“Data
    poisoning attacks”](#data_poisoning_attack)), an organizational insider subtly
    changes system training data to alter system predictions in their favor. Only
    a small proportion of training data must be manipulated to change system outcomes,
    and specialized techniques from active learning and other fields can help attackers
    do so with greater efficiency. When ML systems apply millions of rules or parameters
    to thousands of interacting input features, it becomes nearly impossible to understand
    all the different predictions an ML system could make. In an adversarial example
    attack (see [“Adversarial example attacks”](#adversarial_example_attack)), an
    external attacker preys on such overly complex mechanisms by finding strange rows
    of data—adversarial examples—that evoke unexpected and improper outcomes from
    the ML system, and typically does so to benefit themselves at our expense.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ML系统的完整性可能会受到多种方式的损害，例如数据污染攻击或对抗性示例攻击。在数据污染攻击中（参见[“数据污染攻击”](#data_poisoning_attack)），组织内部人员会微妙地更改系统训练数据，以改变系统预测以符合他们的利益。只需操纵少量训练数据即可改变系统结果，而来自主动学习和其他领域的专业技术可以帮助攻击者更有效地这样做。当ML系统将数百万规则或参数应用于数千个交互输入特征时，几乎不可能理解ML系统可能做出的所有不同预测。在对抗性示例攻击中（参见[“对抗性示例攻击”](#adversarial_example_attack)），外部攻击者通过找到奇怪的数据行——对抗性示例——来利用这些过度复杂的机制，从ML系统中引发意外和不当的结果，通常是为了自己的利益而损害我们。
- en: The availability of an ML system is violated when users cannot get access to
    the services they expect. This can be a consequence of the aforementioned attacks
    bringing down the system, from more standard denial-of-service attacks, from *sponge
    example* attacks, or from bias. People depend on ML systems more and more in their
    daily lives, and when these models relate to high-impact decisions in government,
    finance, or employment, an ML system being down can deny users access to essential
    services. Recent [research](https://oreil.ly/D8KWt) has uncovered the threat of
    sponge examples, or a specially designed kind of input data that forces neural
    networks to slow down their predictions and consume inordinate amounts of energy.
    Sadly, many ML systems also perpetuate systemic biases in outcomes and accuracy
    for historically marginalized demographic groups. Minorities may be less likely
    to experience the same levels of availability from automated credit offers or
    resume scanners. More directly frighteningly, they may be more likely to experience
    faulty predictions by facial recognition systems, including those used in security
    or law enforcement contexts. (Chapters [4](ch04.html#unique_chapter_id_4) and
    [10](ch10.html#unique_chapter_id_10) treat bias and bias testing for ML systems
    in detail.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ML系统的可用性受到侵犯时，用户无法获得他们期望的服务。这可能是前述攻击将系统拖垮的结果，也可能是更标准的拒绝服务攻击、*海绵示例*攻击或偏见造成的后果。人们在日常生活中越来越依赖ML系统，当这些模型涉及到政府、金融或就业等高影响决策时，ML系统的崩溃会使用户无法访问重要服务。最近的[研究](https://oreil.ly/D8KWt)揭示了海绵示例的威胁，即一种特殊设计的输入数据类型，迫使神经网络减慢预测速度并消耗大量能源。可悲的是，许多ML系统还在成果和准确性方面延续了历史上边缘化人群的系统偏见。少数群体可能不太可能从自动信用提供或简历扫描等服务中获得相同水平的可用性。更直接令人担忧的是，他们可能更有可能因面部识别系统（包括在安全或执法背景下使用的系统）的错误预测而遭受影响。
    （第[4](ch04.html#unique_chapter_id_4)章和[10](ch10.html#unique_chapter_id_10)章详细讨论了ML系统中的偏见和偏见测试。）
- en: These are just a few ways that an ML system can experience security problems.
    There are many more. If readers are starting to feel worried, keep reading! We’ll
    discuss straightforward security concepts and best practices next. These tips
    can go a long way toward protecting any computer system.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是ML系统可能遇到安全问题的几种方式。还有很多其他方式。如果读者开始感到担忧，继续阅读！接下来我们将讨论简单直接的安全概念和最佳实践。这些提示可以在保护任何计算机系统方面发挥重要作用。
- en: Best Practices for Data Scientists
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学家的最佳实践
- en: 'Starting with the basics will go a long way toward securing more complex ML
    systems. The following list summarizes those basics in the context of a data science
    workflow:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从基础开始对于确保更复杂的ML系统非常重要。以下列表总结了在数据科学工作流程中这些基础知识的内容：
- en: Access control
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 访问控制
- en: The fewer people that access sensitive resources the better. There are many
    sensitive components in an ML system, but restricting training data, training
    code, and deployment code to only those who require access will mitigate security
    risks related to data exfiltration, data poisoning, backdoor attacks, and other
    attacks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 能够访问敏感资源的人员越少越好。机器学习系统中有许多敏感组件，但只允许那些需要访问的人访问训练数据、训练代码和部署代码将有助于减少与数据外泄、数据毒化、后门攻击及其他攻击相关的安全风险。
- en: Bug bounties
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Bug悬赏
- en: Bug bounties, or when organizations offer monetary rewards to the public for
    finding vulnerabilities, are another practice from general information security
    that should probably also be applied to ML systems. A key insight with bug bounties
    is they incentivize user participation. Users are busy. Sometimes we need to reward
    them for providing feedback.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Bug悬赏，即组织向公众提供找到漏洞的经济奖励，是信息安全领域的另一种实践，可能也应用于机器学习系统。关键的洞察力在于它们激励用户参与。用户很忙。有时我们需要奖励他们提供反馈。
- en: Incident response plans
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 事件响应计划
- en: It’s a common practice to have incident response plans in place for mission-critical
    IT infrastructure to quickly address any failures or attacks. Make sure those
    plans cover ML systems and have the necessary detail to be helpful if an ML system
    fails or suffers an attack. We’ll need to nail down who does what when an AI incident
    occurs, especially in terms of business authority, technical know-how, budget,
    and internal and external communications. There are excellent resources to help
    us get started with incident response from organizations like [NIST](https://oreil.ly/u967-)
    and [SANS Institute](https://oreil.ly/dS6oW). If readers would like to see an
    example incident response plan for ML systems, check out BNH.AI’s [GitHub](https://oreil.ly/xN4Cs).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为关键的IT基础设施制定事件响应计划是一种常见做法，以快速应对任何故障或攻击。确保这些计划覆盖了机器学习系统，并具备在机器学习系统发生故障或遭受攻击时有所帮助的必要细节。我们需要明确在AI事件发生时谁负责什么，特别是在业务权限、技术知识、预算以及内部和外部沟通方面。像[NIST](https://oreil.ly/u967-)和[SANS
    Institute](https://oreil.ly/dS6oW)这样的组织有出色的资源可以帮助我们开始事件响应。如果读者想查看一个机器学习系统的示例事件响应计划，请访问BNH.AI的[GitHub](https://oreil.ly/xN4Cs)。
- en: Routine backups
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 常规备份
- en: Ransomware attacks, where malicious hackers freeze access to an organization’s
    IT systems—and delete precious resources if ransom payments are not made—are not
    uncommon. Make sure to back up important files on a frequent and routine basis
    to protect against both accidental and malicious data loss. It’s also a best practice
    to keep physical backups unplugged (or “air-gapped”) from any networked machines.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 勒索软件攻击，即恶意黑客冻结组织的IT系统访问，并在不支付赎金时删除宝贵资源，这种情况并不少见。确保定期和常规地备份重要文件，以防意外和恶意数据丢失。此外，最佳实践是将物理备份与任何联网机器分离（或“空气隔离”）。
- en: Least privilege
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最小特权
- en: A strict application of the notion of least privilege, i.e., ensuring all personnel—even
    “rockstar” data scientists and ML engineers—receive the absolute minimum required
    IT system permissions, is one of the best ways to guard against insider ML attacks.
    Pay special attention to limiting the number of root, admin, or super users.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 严格应用最小特权的概念，即确保所有人员——甚至是“摇滚明星”数据科学家和机器学习工程师——只获得绝对必要的IT系统权限，是防范内部机器学习攻击的最佳途径之一。特别注意限制root、管理员或超级用户的数量。
- en: Passwords and authentication
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 密码与认证
- en: Use random and unique passwords, multifactor authentication, and other authentication
    methods to ensure access controls and permissions are preserved. It’s also not
    a bad idea to enforce a higher level of password hygiene, such as the use of password
    managers, for any personnel assigned to sensitive projects. Physical keys, such
    as [Yubikeys](https://oreil.ly/oGT49), are some of the strongest authentication
    measures available. Given how common password phishing has become, in addition
    to hacks like SIM-switching that circumvent phone-based authentication, use of
    physical keys should be considered for high-risk applications.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机和唯一的密码、多因素认证和其他认证方法，以确保保留访问控制和权限。对于分配给敏感项目的任何人员，实施更高水平的密码卫生（如使用密码管理器）也不失为一个好主意。物理钥匙，例如[Yubikeys](https://oreil.ly/oGT49)，是目前最强的认证措施之一。考虑到密码钓鱼的普遍性，以及像SIM换卡这样绕过基于电话的认证的黑客攻击，应该考虑在高风险应用中使用物理钥匙。
- en: Physical media
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 物理媒体
- en: Avoid the use of physical storage media for sensitive projects if at all possible,
    except when required for backups. Printed documents, thumb drives, backup media,
    and other portable data sources are often lost and misplaced by busy data scientists
    and engineers. Worse still, they can be stolen by motivated adversaries. For less
    sensitive work, consider enacting policies and education around physical media
    use.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，避免对敏感项目使用物理存储介质，除非需要进行备份。印刷文件、闪存驱动器、备份媒体和其他便携数据源经常会被忙碌的数据科学家和工程师遗失或误放。更糟糕的是，它们可能会被积极进取的对手盗取。对于不那么敏感的工作，考虑制定围绕物理介质使用的政策和教育。
- en: Product security
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 产品安全
- en: If our organization makes software, it’s likely that we apply any number of
    security features and tests to these products. There’s probably also no logical
    reason to not apply these same standards to public- or customer-facing ML systems.
    We should reach out to security professionals in our organization to discuss applying
    standard product security measures to our ML systems.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的组织制作软件，很可能会对这些产品应用任意数量的安全功能和测试。同样，没有逻辑理由不将这些标准应用于面向公众或客户的机器学习系统。我们应该联系我们组织中的安全专家，讨论将标准产品安全措施应用于我们的机器学习系统。
- en: Red teams
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 红队
- en: For mission-critical or otherwise high-stakes deployments of ML, systems should
    be tested under adversarial conditions. In what’s known as red-teaming, teams
    of skilled practitioners attempt to attack ML systems and report their findings
    back to product owners.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习的任务关键或其他高风险部署，系统应在敌对条件下进行测试。在所谓的红队测试中，技术娴熟的团队试图攻击机器学习系统，并向产品所有者报告他们的发现。
- en: Third parties
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第三方
- en: Building an ML system typically requires code, data, and personnel from outside
    our organization. Sadly, each new entrant to the build-out increases our risk.
    Watch out for data poisoning in third-party data or conducted by third-party personnel.
    Scan all third-party packages and models for malware, and control all deployment
    code to prevent the insertion of backdoors or other malicious payloads.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 构建机器学习系统通常需要来自我们组织以外的代码、数据和人员。遗憾的是，每一个新的参与者都会增加我们的风险。注意第三方数据中的数据污染或由第三方人员进行的数据污染。扫描所有第三方包和模型以检测恶意软件，并控制所有部署代码以防止插入后门或其他恶意有效载荷。
- en: Version and environment control
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 版本和环境控制
- en: 'To ensure basic security, we’ll need to know which changes were made to what
    files, when, and by whom. In addition to version control of source code, any number
    of commercial or open source environment managers can automate tracking for large
    data science projects. Check out some of these open resources to get started with
    ML environment management: [DVC](https://oreil.ly/O6_6l), [gigantum](https://oreil.ly/80VT7),
    [mlflow](https://oreil.ly/pDjDF), [ml-metadata](https://oreil.ly/p6EUA), and [modeldb](https://oreil.ly/KhM3o).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保基本安全性，我们需要知道对哪些文件进行了什么样的更改，何时进行的更改，以及由谁进行的更改。除了源代码的版本控制外，任何商业或开源环境管理器都可以自动跟踪大数据科学项目。查看一些这些开放资源，开始进行机器学习环境管理：[DVC](https://oreil.ly/O6_6l)，[gigantum](https://oreil.ly/80VT7)，[mlflow](https://oreil.ly/pDjDF)，[ml-metadata](https://oreil.ly/p6EUA)，和[modeldb](https://oreil.ly/KhM3o)。
- en: ML security, to be discussed in the next sections, will likely be more interesting
    for data scientists than the more general tactics described here. However, because
    the security measures considered here are so simple, not following them could
    potentially result in legal liabilities for our organization, in addition to embarrassing
    or costly breaches and hacks. While still debated and somewhat amorphous, violations
    of security standards, as enforced by the [US Federal Trade Commission (FTC)](https://oreil.ly/XfCYP)
    and other regulators, can bring with them unpleasant scrutiny and enforcement
    actions. Hardening the security of our ML systems is a lot of work, but failing
    to get the basics right can make big trouble when we’re building out more complex
    ML systems with lots of subsystems and dependencies.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习安全，将在接下来的章节中讨论，对于数据科学家可能比这里描述的更一般的战术更感兴趣。然而，由于这里考虑的安全措施如此简单，不遵守它们可能会导致我们组织的法律责任，以及令人尴尬或昂贵的违规和黑客攻击。虽然仍在辩论中且有些模糊，但违反由[美国联邦贸易委员会(FTC)](https://oreil.ly/XfCYP)和其他监管机构执行的安全标准，可能会带来令人不愉快的审查和执法行动。加固我们机器学习系统的安全性是一项艰巨的工作，但是在构建具有许多子系统和依赖关系的更复杂的机器学习系统时，如果基础工作没有做好，可能会带来大麻烦。
- en: Machine Learning Attacks
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习攻击
- en: Various ML software artifacts, ML prediction APIs, and other AI system endpoints
    are now vectors for cyber and insider attacks. Such ML attacks can negate all
    the other hard work a data science team puts into mitigating other risks—because
    once our ML system is attacked, it’s not our system anymore. And attackers typically
    have their own agendas regarding accuracy, bias, privacy, reliability, robustness,
    resilience, and unauthorized decisions. The first step in defending against these
    attacks is to understand them. We’ll go over an overview of the most well-known
    ML attacks in the sections that follow.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 各种ML软件工件，ML预测API以及其他AI系统端点现在成为网络和内部人员攻击的向量。这些ML攻击可以抵消数据科学团队为缓解其他风险所做的所有努力，因为一旦我们的ML系统受到攻击，它就不再是我们的系统。攻击者通常对准确性，偏见，隐私，可靠性，鲁棒性，弹性和未经授权的决策有他们自己的议程。防御这些攻击的第一步是了解它们。我们将在接下来的章节中概述最为人熟知的ML攻击。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Most attacks and vulnerabilities for ML systems are premised on the opaque and
    unduly complex nature of classical ML algorithms. If a system is so complex its
    operators don’t understand it, then attackers can manipulate it without the operators
    knowing what’s happened.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数针对ML系统的攻击和漏洞都基于经典ML算法的不透明和过度复杂的特性。如果一个系统过于复杂以至于其操作员无法理解它，那么攻击者可以在操作员不知情的情况下操纵它。
- en: 'Integrity Attacks: Manipulated Machine Learning Outputs'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整性攻击：操纵机器学习输出
- en: 'Our tour of ML attacks will begin with attacks on ML model integrity, i.e.,
    attacks that alter system outputs. Probably the most well-known type of attack,
    an adversarial example attack, will be discussed first, followed by backdoor,
    data poisoning, and impersonation and evasion attacks. When thinking through these
    attacks, remember that they can often be used in two primary ways: (1) to grant
    attackers the ML outcome they desire, or (2) to deny a third party their rightful
    outcome.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的ML攻击之旅将从攻击ML模型的完整性开始，即修改系统输出的攻击。可能是最为人熟知的攻击类型之一，对抗性示例攻击将首先讨论，随后是后门攻击，数据污染以及冒充和逃避攻击。在考虑这些攻击时，请记住它们通常可以用两种主要方式之一使用：（1）授予攻击者他们期望的ML结果，或（2）拒绝第三方其正当的结果。
- en: Adversarial example attacks
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗性示例攻击
- en: A motivated attacker can learn, by trial and error with a prediction API (i.e.,
    “exploration” or “sensitivity analysis”), via an inversion attack (see [“Model
    extraction and inversion attacks”](#inversion_attack)), or by social engineering,
    how to game our ML model to receive their desired prediction outcome or how to
    change someone else’s outcome. Carrying out an attack by specifically engineering
    a row of data for such purposes is referred to as an adversarial example attack.
    An attacker could use an adversarial example attack to grant themselves a loan,
    a lower than appropriate insurance premium, or to avoid pretrial detention based
    on a criminal risk score. See [Figure 5-2](#adversarial) for an illustration of
    a fictitious attacker executing an adversarial example attack on a credit lending
    model using strange rows of data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个积极进攻者可以通过与预测API的试错（即“探索”或“敏感性分析”），通过反演攻击（见[“模型提取和反演攻击”](#inversion_attack)），或通过社会工程学，学会如何操纵我们的ML模型以获得他们期望的预测结果，或如何改变他人的结果。专门设计数据行以进行这种目的的攻击被称为对抗性示例攻击。攻击者可以使用对抗性示例攻击来授予自己贷款，低于适当的保险费率，或根据犯罪风险评分避免预审拘留。查看[图5-2](#adversarial)以查看虚构攻击者利用奇怪的数据行对信用放贷模型执行对抗性示例攻击的插图。
- en: '![mlha 0502](assets/mlha_0502.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0502](assets/mlha_0502.png)'
- en: Figure 5-2\. An adversarial example attack ([digital, color version](https://oreil.ly/04ycs))
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. 一种对信用放贷模型执行对抗性示例攻击的虚构攻击者的插图（[数字，彩色版本](https://oreil.ly/04ycs)）
- en: Backdoor attacks
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后门攻击
- en: Consider a scenario where an employee, consultant, contractor, or malicious
    external actor has access to our model’s production code—code that makes real-time
    predictions. This individual could change that code to recognize a strange or
    unlikely combination of input variable values to trigger a desired prediction
    outcome. Like other outcome manipulation hacks, backdoor attacks can be used to
    trigger model outputs that an attacker wants, or outcomes a third party does not
    want. As depicted in [Figure 5-3](#backdoor), an attacker could insert malicious
    code into our model’s production scoring engine that recognizes the combination
    of a realistic age but negative years on a job (`yoj`) to trigger an inappropriate
    positive prediction outcome for themselves or their associates. To alter a third
    party’s outcome, an attacker could insert an artificial rule into our model’s
    scoring code that prevents our model from producing positive outcomes for a certain
    group of people.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个场景，一个员工、顾问、承包商或恶意的外部行为者有权访问我们模型的生产代码——用于实时预测的代码。这个人可以更改代码，以识别奇怪或不太可能的输入变量值组合，以触发期望的预测结果。与其他结果操纵黑客攻击一样，后门攻击可以用于触发攻击者想要的模型输出，或者第三方不希望的结果。如在[图
    5-3](#backdoor)所示，攻击者可以将恶意代码插入我们模型的生产评分引擎，以识别一个真实年龄但工作年限（`yoj`）为负数的组合，以触发他们自己或他们的同伴的不恰当的正面预测结果。为了改变第三方的结果，攻击者可以向我们模型的评分代码中插入一个人工规则，阻止我们模型为某一群体产生正面的结果。
- en: '![mlha 0503](assets/mlha_0503.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0503](assets/mlha_0503.png)'
- en: Figure 5-3\. A backdoor attack ([digital, color version](https://oreil.ly/04ycs))
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 后门攻击（[数字版，彩色](https://oreil.ly/04ycs)）
- en: Data poisoning attacks
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据污染攻击
- en: Data poisoning refers to someone systematically changing our training data to
    manipulate our model’s predictions. To poison data, an attacker must have access
    to some or all of our training data. And at many companies, many different employees,
    consultants, and contractors have just that—and with little oversight. It’s also
    possible a malicious external actor could acquire unauthorized access to some
    or all of our training data and poison it. A very direct kind of data poisoning
    attack might involve altering the labels of a training dataset. In [Figure 5-4](#data_poisoning),
    the attacker changes a small number of training data labels so that people with
    their kind of credit history will erroneously receive a credit product. It’s also
    possible that a malicious actor could use data poisoning to train our model to
    intentionally discriminate against a group of people, depriving them of the big
    loan, big discount, or low insurance premiums they rightfully deserve.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 数据污染是指有人系统性地改变我们的训练数据，以操纵我们模型的预测。为了进行数据污染，攻击者必须能够访问部分或全部我们的训练数据。在许多公司，许多不同的员工、顾问和承包商都有这样的访问权限，而监督却很少。也有可能，恶意的外部行为者可能未经授权地获取部分或全部我们的训练数据并进行污染。一个非常直接的数据污染攻击可能涉及改变训练数据集的标签。在[图
    5-4](#data_poisoning)中，攻击者改变了少量训练数据的标签，以便那些具有他们信用历史的人会错误地接收到一个信用产品。也有可能，恶意行为者可能利用数据污染来训练我们的模型有意歧视某个群体，剥夺他们应有的大额贷款、大折扣或低保险费率。
- en: '![mlha 0504](assets/mlha_0504.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0504](assets/mlha_0504.png)'
- en: Figure 5-4\. A data poisoning attack ([digital, color version](https://oreil.ly/04ycs))
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 数据污染攻击（[数字版，彩色](https://oreil.ly/04ycs)）
- en: While it’s simplest to think of data poisoning as changing the values in the
    existing rows of a dataset, data poisoning can also be conducted by adding seemingly
    harmless or superfluous columns onto a dataset and ML model. Altered values in
    these columns could then trigger altered model predictions. This is one of many
    reasons to avoid dumping massive numbers of columns into an unexplainable ML model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最简单地将数据污染看作是改变数据集现有行的值，但数据污染也可以通过向数据集和 ML 模型添加看似无害或多余的列来进行。这些列中的修改值可能会触发修改后的模型预测。这是避免向无法解释的
    ML 模型倾倒大量列的许多原因之一。
- en: Impersonation and evasion attacks
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 冒名顶替和逃避攻击
- en: Using trial and error, a model inversion attack (see [“Model extraction and
    inversion attacks”](#inversion_attack)), or social engineering, an attacker can
    learn the types of individuals that receive a desired prediction outcome from
    our ML system. The attacker can then impersonate this kind of input or individual
    to receive a desired prediction outcome, or to evade an undesired outcome. These
    kinds of impersonation and evasion attacks resemble identity theft from the ML
    model’s perspective. They’re also similar to adversarial example attacks (see
    [“Adversarial example attacks”](#adversarial_example_attack)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用试错法、模型逆推攻击（参见[“模型提取和逆推攻击”](#inversion_attack)）或社会工程学，攻击者可以了解到我们的机器学习系统从中得到预测结果的个体类型。攻击者随后可以冒充这种类型的输入或个体，以获取所需的预测结果，或逃避不希望的结果。这些冒充和逃避攻击类似于从机器学习模型角度看的身份盗窃。它们也类似于对抗性示例攻击（参见[“对抗性示例攻击”](#adversarial_example_attack)）。
- en: Like an adversarial example attack, an impersonation attack involves artificially
    changing the input data values to our model. Unlike an adversarial example attack,
    where a potentially random-looking combination of input data values could be used
    to trick our model, impersonation implies using the information associated with
    another modeled entity (i.e., customer, employee, financial transaction, patient,
    product, etc.) to receive the prediction our model associates with that type of
    entity. And evasion implies the converse—changing our own data to avoid an adverse
    prediction.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与对抗性示例攻击类似，冒充攻击涉及人为更改我们模型的输入数据值。与对抗性示例攻击不同的是，后者可能使用看似随机的输入数据值组合来欺骗我们的模型，而冒充意味着使用与另一个建模实体相关联的信息（即客户、员工、财务交易、患者、产品等），以获取我们的模型与该类型实体相关联的预测。而逃避则意味着相反的行为—更改我们自己的数据以避免不良预测。
- en: 'In [Figure 5-5](#impersonation), an attacker learns what characteristics our
    model associates with awarding a credit product, and then falsifies their own
    information to receive the credit product. They could share their strategy with
    others, potentially leading to large losses for our company. Sound like science
    fiction? It’s not. Closely related evasion attacks have worked for [facial-recognition
    payment and security systems](https://oreil.ly/69u8J), and [“Case Study: Real-World
    Evasion Attacks”](#case_evasion_attacks) will address several documented instances
    of evasions of ML security systems.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 5-5](#impersonation) 中，攻击者了解了我们的模型与授信产品关联的特征，然后伪造他们自己的信息来获取该授信产品。他们可以与他人分享他们的策略，这可能导致我们公司遭受巨大损失。听起来像科幻小说？不是。与此密切相关的逃避攻击已经在[面部识别支付和安全系统](https://oreil.ly/69u8J)中奏效，并且[“案例研究：真实世界的逃避攻击”](#case_evasion_attacks)将讨论几个文档记录的逃避机器学习安全系统实例。
- en: '![mlha 0505](assets/mlha_0505.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0505](assets/mlha_0505.png)'
- en: Figure 5-5\. An impersonation attack ([digital, color version](https://oreil.ly/04ycs))
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. 一种冒充攻击（[数字版，彩色版本](https://oreil.ly/04ycs)）
- en: Attacks on machine learning explanations
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对机器学习解释的攻击
- en: 'In what has been called a “scaffolding” attack—see [“Fooling LIME and SHAP:
    Adversarial Attacks on Post hoc Explanation Methods”](https://oreil.ly/xx9dH)—adversaries
    can poison post hoc explanations such as local interpretable model-agnostic explanations
    and Shapley additive explanations. Attacks on partial dependence, another common
    post hoc explanation technique, have also been published recently—see [“Fooling
    Partial Dependence via Data Poisoning”](https://oreil.ly/KMNmt). Attacks on explanations
    can be used to alter both operator and consumer perceptions of an ML system—for
    example, to make another hack in the pipeline harder to find, or to make a biased
    model appear fair—known as [fairwashing](https://oreil.ly/YD-QJ). These attacks
    make clear that as ML pipelines and AI systems become more complex, bad actors
    could look to many different parts of the system, from training data all the way
    to post hoc explanations, to alter system outputs.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在所谓的“脚手架”攻击中—参见[“愚弄LIME和SHAP：对后续解释方法的对抗攻击”](https://oreil.ly/xx9dH)—对手可以操纵后续解释方法，如局部可解释模型无关解释和Shapley加性解释。对部分依赖的攻击，另一种常见的后续解释技术，最近也已发布—参见[“通过数据毒化愚弄部分依赖”](https://oreil.ly/KMNmt)。对解释的攻击可以用来改变操作员和消费者对机器学习系统的认知—例如，使管道中的另一个黑客攻击更难发现，或使一个有偏见的模型看起来公平—被称为[美化]。这些攻击清楚地表明，随着机器学习管道和AI系统变得更加复杂，不良行为者可能会瞄准系统的许多不同部分，从训练数据一直到后续解释，以改变系统输出。
- en: 'Confidentiality Attacks: Extracted Information'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保密性攻击：提取的信息
- en: Without proper countermeasures, bad actors can access sensitive information
    about our model and data. Model extraction and inversion attacks refer to hackers
    rebuilding our model and extracting information from their copy of the model.
    Membership inference attacks allow bad actors to know what rows of data are in
    our training data, and even to reconstruct training data. Both attacks only require
    access to an unguarded ML system prediction API or other system endpoints.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有适当的对策，恶意行为者可以访问关于我们模型和数据的敏感信息。模型提取和反转攻击是指黑客重建我们的模型，并从其模型副本中提取信息。成员推断攻击允许恶意行为者知道我们训练数据中的哪些行，并甚至重构训练数据。这些攻击只需访问一个无防护的机器学习系统预测
    API 或其他系统端点即可。
- en: Note
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Model extraction, model inversion, membership inference, and some other ML attacks,
    can all be thought of as a new take on an older and more common intellectual property
    and security issue—reverse engineering. Confidentiality attacks, and other ML
    attacks, can be used to *reverse engineer* and reconstruct our potentially sensitive
    models and data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 模型提取、模型反转、成员推断以及其他一些机器学习攻击，都可以看作是对一个较旧和更常见的知识产权和安全问题——逆向工程的一种新的处理方式。保密性攻击以及其他机器学习攻击，可以被用来*逆向工程*和重构我们潜在敏感的模型和数据。
- en: Model extraction and inversion attacks
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型提取和反转攻击
- en: 'Inversion (see [Figure 5-6](#inversion)) basically means getting unauthorized
    information out of our model—as opposed to the normal usage pattern of putting
    information into our model. If an attacker can receive many predictions from our
    model API or other endpoint (website, app, etc.), they can train a surrogate model
    between their inputs and our system’s predictions. That extracted surrogate model
    is trained between the inputs the attacker used to generate the received predictions
    and the received predictions themselves. Depending on the number of predictions
    the attacker can receive, the surrogate model could become quite an accurate simulation
    of our model. Unfortunately, once the surrogate model is trained, we have several
    big problems:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 反转（参见 [图 5-6](#inversion)）基本上是指从我们的模型中获取未授权的信息——与正常的使用模式相反，即将信息输入到我们的模型中。如果攻击者可以从我们的模型
    API 或其他端点（网站、应用等）接收许多预测，他们可以在他们的输入和我们系统预测之间训练一个替代模型。那个提取出的替代模型是在攻击者用来生成接收到的预测的输入和接收到的预测之间训练的。根据攻击者可以接收到的预测数量，替代模型可以成为我们模型的相当准确的模拟。不幸的是，一旦替代模型训练完成，我们就会遇到几个重大问题：
- en: Models are really just compressed versions of training data. With the surrogate
    model, an attacker can start learning about our potentially sensitive training
    data.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型实际上只是训练数据的压缩版本。有了替代模型，攻击者可以开始了解我们潜在敏感的训练数据。
- en: Models are valuable intellectual property. Attackers can now sell access to
    their copy of our model and cut into our return on investment.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是有价值的知识产权。攻击者现在可以出售他们复制的我们模型的访问权限，并削减我们的投资回报。
- en: The attacker now has a sandbox from which to plan impersonation, adversarial
    example, membership inference, or other attacks against our model.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 攻击者现在拥有一个沙盒，可以在其中策划模仿、对抗性示例、成员推断或其他攻击来针对我们的模型。
- en: Such surrogate models can also be trained using external data sources that can
    be somehow matched to our predictions, as ProPublica [famously did](https://oreil.ly/FvMDm)
    with the proprietary COMPAS criminal risk assessment instrument.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的替代模型也可以使用外部数据源进行训练，这些数据源可以与我们的预测相匹配，就像 ProPublica 与专有的 COMPAS 犯罪风险评估工具 [（引用自此处）](https://oreil.ly/FvMDm)
    一样。
- en: '![mlha 0506](assets/mlha_0506.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0506](assets/mlha_0506.png)'
- en: Figure 5-6\. An inversion attack ([digital, color version](https://oreil.ly/04ycs))
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. 一种反转攻击（[数字版，彩色](https://oreil.ly/04ycs)）
- en: Membership inference attacks
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成员推断攻击
- en: In an attack that starts with model extraction and is also carried out by surrogate
    models, a malicious actor can determine whether a given person or product is in
    our model’s training data. Called a membership inference attack (see [Figure 5-7](#membership_inference)),
    this hack is executed with two layers of models. First, an attacker passes data
    into a public prediction API or other endpoint, receives predictions back, and
    trains a surrogate model or models between the passed data and the predictions.
    Once a surrogate model (or models) has been trained to replicate our model, the
    attacker then trains a second layer classifier that can differentiate between
    data that was used to train the first surrogate model and data that was not used
    to train that surrogate. When this second model is used to attack our model, it
    can give a solid indication as to whether any given row (or rows) of data is in
    our training data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在以模型提取开始并且也由替代模型执行的攻击中，恶意行为者可以确定我们的模型训练数据中是否存在特定的人或产品。称为成员推理攻击（见[图5-7](#membership_inference)），这种黑客攻击是由两层模型执行的。首先，攻击者将数据传入公共预测API或其他端点，接收预测结果，并训练替代模型或模型，以在传入数据和预测结果之间建立联系。一旦训练出替代模型（或多个模型）来复制我们的模型，攻击者接着训练第二层分类器，可以区分用于训练第一个替代模型和未用于训练该替代模型的数据。当这第二个模型用于攻击我们的模型时，它可以明确指出任何给定的数据行（或多行）是否存在于我们的训练数据中。
- en: Membership in a training dataset can be sensitive when the model and data are
    related to undesirable outcomes such as bankruptcy or disease, or desirable outcomes
    like high income or net worth. Moreover, if the relationship between a single
    row and the target of our model can be easily generalized by an attacker, such
    as an obvious relationship between race, gender, or age and some undesirable outcome,
    this attack can violate the privacy of an entire class of people. Frighteningly,
    when carried out to its fullest extent, a membership inference attack could also
    allow a malicious actor, with access only to an unprotected public prediction
    API or other model endpoint, to reverse engineer large portions of a sensitive
    or valuable training dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型和数据与破产或疾病等不良结果，或高收入或净值等良好结果相关时，训练数据集中的成员身份可能非常敏感。此外，如果攻击者可以轻易地通过单一行数据与我们模型的目标之间的关系进行泛化，比如种族、性别或年龄与某些不良结果之间的明显关系，这种攻击可能会侵犯整个人群的隐私。令人恐惧的是，如果达到最极端，成员推理攻击甚至可以让一个恶意行为者仅通过未受保护的公共预测API或其他模型端点的访问，逆向工程敏感或有价值的大部分训练数据集。
- en: '![mlha 0507](assets/mlha_0507.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![mlha 0507](assets/mlha_0507.png)'
- en: Figure 5-7\. A membership inference attack ([digital, color version](https://oreil.ly/04ycs))
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7\. 一个成员推理攻击（[数字，彩色版本](https://oreil.ly/04ycs)）
- en: While the discussed attacks are some of the most well-known varieties, keep
    in mind that these are not the only types of ML hacks, and that new attacks can
    emerge very quickly. Accordingly, we’ll also address a few general concerns to
    help us frame the broader threat environment, before moving on to countermeasures
    we can use to protect our ML system.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所讨论的攻击是一些最为知名的种类，但请记住这并不是机器学习黑客攻击的唯一类型，新的攻击可能非常快速地出现。因此，在进入到我们可以用来保护我们的机器学习系统的对策之前，我们还将讨论一些一般性的关注点，以帮助我们构建更广泛的威胁环境框架。
- en: General ML Security Concerns
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习安全的一般关注点
- en: 'One common theme of this book is that ML systems are fundamentally software
    systems, and applying commonsense software best practices to ML systems is usually
    a good idea. The same applies for security. As software systems and services,
    ML systems exhibit similar failure modes, and experience the same attacks, as
    general software systems. What are some other general concerns? Unpleasant things
    like intentional abuses of AI technology, availability attacks, trojans and malware,
    man-in-the-middle attacks, unnecessarily complex unexplainable systems, and the
    woes of distributed computing:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的一个普遍主题是，机器学习系统基本上是软件系统，将常识性的软件最佳实践应用于机器学习系统通常是一个好主意。安全方面也是如此。作为软件系统和服务，机器学习系统表现出类似于通用软件系统的失败模式，并经历与一般软件系统相同的攻击。还有哪些一般性的关注点？像是意图滥用AI技术、可用性攻击、木马和恶意软件、中间人攻击、不必要复杂的无法解释系统以及分布式计算的困难：
- en: Abuses of machine learning
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 滥用机器学习
- en: Nearly all tools can also be weapons, and ML models and AI systems can be abused
    in numerous ways. Let’s start by considering deepfakes. Deepfakes are an application
    of deep learning that can, when done carefully, seamlessly blend fragments of
    audio and video into convincing new media. While deepfakes can be used to bring
    movie actors back to life, as was done in some recent *Star Wars* films, deepfakes
    can be used to harm and extort people. Of course, nonconsensual pornography, in
    which the victim’s face is blended into an adult video, is one of the most popular
    uses of deepfakes, as documented by the [BBC](https://oreil.ly/05QCB) and other
    news outlets. Deepfakes have also been implicated in financial crimes, e.g., when
    an attacker [used a CEO’s voice](https://oreil.ly/A0a8_) to order money transferred
    into their own account. Algorithmic discrimination is another common application
    of abusive AI. In a “fairwashing” attack, post hoc explanations can be altered
    to hide discrimination in a biased model. And facial recognition can be used directly
    for [racial profiling](https://oreil.ly/KBvXa). We’re touching on just a few of
    the ways ML systems can be abused; for a broader treatment of this important topic,
    see [“AI-Enabled Future Crime”](https://oreil.ly/8L3ax).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有工具也可以被用作武器，机器学习模型和人工智能系统可以被滥用多种方式。让我们从深度伪造开始考虑。深度伪造是深度学习的一种应用，当仔细进行时，可以将音频和视频片段无缝地融合成令人信服的新媒体。虽然深度伪造可以用于使电影演员重生，就像一些最近的*星球大战*电影中所做的那样，但深度伪造也可以用于伤害和敲诈勒索人们。当然，非自愿色情，即将受害者的面孔混合到成人视频中，是深度伪造的最流行用途之一，正如[BBC](https://oreil.ly/05QCB)和其他新闻媒体所报道的那样。深度伪造也被牵涉到金融犯罪中，例如当攻击者使用CEO的声音来命令将钱转入他们自己的账户时。算法歧视是滥用人工智能的另一个常见应用。在“公平洗白”攻击中，事后解释可以被修改以隐藏偏向模型中的歧视。面部识别也可以直接用于[种族识别](https://oreil.ly/KBvXa)。我们只是触及了机器学习系统可以被滥用的一些方式；有关这一重要主题的更广泛处理，请参阅[“AI-Enabled
    Future Crime”](https://oreil.ly/8L3ax)。
- en: General availability attacks
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一般可用性攻击
- en: ML systems can fall victim to more general denial-of-service (DOS) attacks,
    just like other public-facing services. If a public-facing ML system is critical
    to our organization, we make sure it’s hardened with firewalls and filters, reverse
    domain name server system (DNS) lookup, and other countermeasures that increase
    availability during a DOS attack. Unfortunately, we also have to think through
    another kind of availability failure for ML systems—those caused by algorithmic
    discrimination. If algorithmic discrimination is severe enough, whether driven
    by internal failures or adversarial attacks, our ML system will likely not be
    usable by a large portion of its users. Be sure to test for bias during training
    and throughout a system’s deployed lifecycle.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统可能会成为像其他面向公众服务一样受到拒绝服务（DOS）攻击的受害者。如果面向公众的机器学习系统对我们的组织至关重要，我们会确保其通过防火墙和过滤器、反向域名服务器系统（DNS）查找以及其他增加在DOS攻击期间可用性的对策来加固。不幸的是，我们还必须考虑通过算法歧视引起的另一种可用性失败。如果算法歧视足够严重，无论是由内部故障还是敌对攻击驱动，我们的机器学习系统可能不会被其大部分用户使用。确保在训练期间和系统部署生命周期中进行偏见测试。
- en: Trojans and malware
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 木马和恶意软件
- en: ML in the research and development environment is dependent on a diverse ecosystem
    of open source software packages. Some of these packages have many contributors
    and users. Some are highly specific and only meaningful to a small number of researchers
    or practitioners. It’s well understood that many packages are maintained by brilliant
    statisticians and ML researchers whose primary focus is mathematics or algorithms,
    not software engineering or security. It’s not uncommon for an ML pipeline to
    be dependent on dozens or even hundreds of external packages, any one of which
    could be hacked to conceal an attack payload. Third-party packages with large
    binary data stores and pretrained ML models seem especially ripe for these kinds
    of problems. If possible, scan all software artifacts associated with an ML system
    for malware and trojans.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在研发环境中，机器学习依赖于一个多样化的开源软件包生态系统。其中一些软件包拥有许多贡献者和用户。有些则非常特定，只对少数研究人员或从业者有意义。众所周知，许多软件包由杰出的统计学家和机器学习研究人员维护，他们的主要关注点是数学或算法，而不是软件工程或安全性。一个机器学习流水线可能依赖于几十甚至上百个外部软件包，其中任何一个都可能被黑客利用来隐藏攻击载荷。具有大型二进制数据存储和预训练机器学习模型的第三方软件包似乎尤为容易受到这类问题的影响。如果可能，扫描与机器学习系统相关的所有软件工件，查找木马和恶意软件。
- en: Man-in-the-middle attacks
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 中间人攻击
- en: Because many ML system predictions and decisions are transmitted over the internet
    or an organization’s network, they can be manipulated by bad actors during that
    journey. Where possible, use encryption, certificates, mutual authentication,
    or other countermeasures to ensure the integrity of ML system results passed across
    networks.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因为许多机器学习系统的预测和决策都是通过互联网或组织的网络传输的，它们在传输过程中可以被恶意行为者操控。在可能的情况下，使用加密、证书、双向认证或其他对策来确保通过网络传输的机器学习系统结果的完整性。
- en: Unexplainable machine learning
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 不可解释的机器学习
- en: Although recent developments in interpretable models and model explanations
    have provided the opportunity to use accurate and also transparent models, many
    machine learning workflows are still centered around unexplainable models. Such
    models are a common type of often unnecessary complexity in a commercial ML workflow.
    A dedicated, motivated attacker can, over time, learn more about our overly complex
    unexplainable ML model than our own team knows about it. (Especially in today’s
    turnover-prone data science job market.) This knowledge imbalance can potentially
    be exploited to conduct the attacks we’ve described or for other yet unknown types
    of attacks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近对可解释模型和模型解释的发展提供了使用准确且透明模型的机会，但许多机器学习工作流程仍围绕不可解释模型展开。这种模型在商业机器学习工作流中是常见的不必要复杂性的一种类型。一个专注且有动力的攻击者可以随着时间的推移，比我们自己的团队更深入地了解我们过于复杂且不可解释的机器学习模型。
    （特别是在今天的频繁人员流动的数据科学职场。）这种知识不平衡可能被利用来进行我们已经描述的攻击，或者其他尚未知晓的攻击类型。
- en: Distributed computing
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式计算
- en: For better or worse, we live in the age of big data. Many organizations are
    now using distributed data processing and ML systems. Distributed computing can
    provide a broad attack surface for a malicious internal or external actor. Data
    could be poisoned on only one or a few worker nodes of a large distributed data
    storage or processing system. A backdoor could be coded into just one model of
    a large ensemble. Instead of debugging one simple dataset or model, now practitioners
    must sometimes examine data or models distributed across large computing clusters.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 不管是好是坏，我们生活在大数据时代。许多组织现在正在使用分布式数据处理和机器学习系统。分布式计算可以为恶意的内部或外部行为者提供广泛的攻击面。数据可能只在一个或几个大型分布式数据存储或处理系统的工作节点上被污染。一个后门可能只编码在大型集成模型的一个模型中。现在从一个简单的数据集或模型调试转向，从业者有时必须检查分布在大型计算集群中的数据或模型。
- en: Starting to get worried again? Hang in there—we’ll cover countermeasures for
    confidentiality, integrity, and availability attacks on ML systems next.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 又开始感到担忧了吗？挺住——接下来我们将讨论关于机器学习系统的保密性、完整性和可用性攻击的对策。
- en: Countermeasures
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对策
- en: There are many countermeasures we can use and, when paired with the governance
    processes proposed in [Chapter 1](ch01.html#unique_chapter_id_1), bug bounties,
    security audits, and red-teaming, such measures are more likely to be effective.
    Additionally, there are the newer subdisciplines of adversarial ML and robust
    ML, which are giving the full academic treatment to these subjects. This section
    will outline some of the defensive measures we can use to help make our ML systems
    more secure, including model debugging for security, model monitoring for security,
    privacy-enhancing technologies, robust ML, and a few general approaches.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用许多对策，并结合在[第1章](ch01.html#unique_chapter_id_1)中提议的治理过程、漏洞赏金、安全审计和红队行动，这些措施更有可能是有效的。此外，还有对抗性机器学习和强化机器学习等较新的子学科，这些学科正在对这些主题进行全面的学术处理。本节将概述我们可以使用的一些防御措施，以帮助使我们的机器学习系统更安全，包括安全的模型调试、安全的模型监控、增强隐私技术、强化机器学习以及一些一般方法。
- en: Model Debugging for Security
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全的模型调试
- en: ML models can and should be tested for security vulnerabilities before they
    are released. In these tests, the goal is basically to attack our own ML systems,
    to understand our level of security, and to patch up any discovered vulnerabilities.
    Some general techniques that work across different types of ML models for security
    debugging are adversarial example searches, sensitivity analysis, audits for insider
    attacks and model extraction attacks, and discrimination testing.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布之前，机器学习模型可以并且应该进行安全漏洞测试。在这些测试中，目标基本上是攻击我们自己的机器学习系统，了解我们的安全水平，并修补任何发现的漏洞。适用于安全调试的跨不同类型机器学习模型的一些通用技术包括对抗示例搜索、敏感性分析、针对内部攻击和模型提取攻击的审计，以及歧视性测试。
- en: Adversarial example searches and sensitivity analysis
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗示例搜索和敏感性分析
- en: Conducting sensitivity analysis with an adversarial mindset, or better yet,
    conducting our own adversarial example attacks, is a good way to determine if
    our system is vulnerable to perhaps the simplest and most common type of ML integrity
    attack. The idea of these ethical hacks is to understand what feature values (or
    combinations thereof) can cause large swings in our system’s output predictions.
    If we’re working in the deep learning space, packages like [cleverhans](https://oreil.ly/6LuBF)
    and [foolbox](https://oreil.ly/M4ayU) can help us get started with testing our
    ML system. For those working with structured data, good old sensitivity analysis
    can go a long way toward pointing out instabilities in our system. We can also
    use genetic learning to evolve our own adversarial examples or we can use [heuristic
    methods based on individual conditional expectation](https://oreil.ly/_qQdn) to
    find adversarial examples. Once we find instabilities in our ML system, triggered
    by these adversarial examples, we’ll want to use cross-validation or regularization
    to train a more stable model, apply techniques from robust machine learning (see
    [“Robust Machine Learning”](#robust_machine_learning)), or explicitly monitor
    for the discovered adversarial examples in real time. We should also link this
    information to the system’s incident response plan, in case it’s useful later.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以对抗态度进行敏感性分析，或者更好地说，进行我们自己的对抗性示例攻击，是确定我们的系统是否容易受到可能是最简单和最常见的ML完整性攻击类型之一的好方法。这些道德黑客的想法是了解什么特征值（或其组合）可以导致我们系统输出预测大幅波动。如果我们在深度学习领域工作，像[cleverhans](https://oreil.ly/6LuBF)和[foolbox](https://oreil.ly/M4ayU)这样的软件包可以帮助我们开始测试我们的ML系统。对于那些处理结构化数据的人来说，老牌的敏感性分析可以在指出系统不稳定性方面发挥很大作用。我们还可以使用遗传学习来演化我们自己的对抗性示例，或者使用[基于个体条件期望的启发式方法](https://oreil.ly/_qQdn)来找到对抗性示例。一旦我们发现由这些对抗性示例触发的ML系统的不稳定性，我们将希望使用交叉验证或正则化来训练一个更稳定的模型，应用来自健壮机器学习的技术（见[“健壮机器学习”](#robust_machine_learning)），或者在实时中明确监控发现的对抗性示例。我们还应将此信息链接到系统的事件响应计划中，以防以后有用。
- en: Auditing for insider data poisoning
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对内部数据污染进行审计
- en: If a data poisoning attack were to occur, system insiders—employees, contractors,
    and consultants—are not unlikely culprits. How can we track down insider data
    poisoning? First, score those individuals with our system. Any insider receiving
    a positive outcome could be the attacker or know the attacker. Because a smart
    attacker will likely perform the minimum changes to the training data that result
    in a positive outcome, we can also use residual analysis to look for beneficial
    outcomes with larger than expected residuals, indicating the ML model may have
    been inclined to issue a negative outcome for the individual had the training
    data not been altered. Data and environment management are strong countermeasures
    for insider data poisoning, as any changes to data are tracked with ample metadata
    (who, what, when, etc.). We can also try the reject on negative impact (RONI)
    technique, proposed in the seminal [“The Security of Machine Learning”](https://oreil.ly/exh6g),
    to remove potentially altered rows from system training data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发生数据污染攻击，系统内部人员—员工、承包商和顾问—可能是嫌疑人。我们如何追踪内部数据污染者？首先，对这些个人使用我们的系统进行评分。任何接收到积极结果的内部人员可能是攻击者或知道攻击者的人。因为聪明的攻击者可能会对训练数据进行最小的更改，以产生积极的结果，所以我们还可以使用残差分析来寻找具有大于预期残差的有益结果，这表明ML模型可能倾向于为这些个体发出负面结果，如果训练数据没有被改变的话。数据和环境管理是内部数据污染的强有力对策，因为所有数据变更都有充分的元数据记录（谁、什么时候等等）。我们还可以尝试拒绝对负面影响（RONI）技术，该技术在开创性的[“机器学习安全性”](https://oreil.ly/exh6g)中提出，以从系统训练数据中删除潜在已改变的行。
- en: Bias testing
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏倚测试
- en: DOS attacks, resulting from some kind of bias—intentional or not—are a plausible
    type of availability attack. In fact, it’s already happened. In 2016, Twitter
    users poisoned the [Tay chatbot](https://oreil.ly/uPqNx) to the point where only
    those users interested in neo-Nazi pornography would find the system’s service
    appealing. This type of attack could also happen in a more serious context, such
    as employment, lending, or medicine, where an attacker uses data poisoning, model
    backdoors, or other types of attacks to deny service to a certain group of customers.
    This is one of the many reasons to conduct bias testing, and remediate any discovered
    discrimination, both at training time and as part of regular model monitoring.
    There are several great open source tools for detecting discrimination and making
    attempts to remediate it, such as [aequitas](https://oreil.ly/e412j), [Themis](https://oreil.ly/yJiT6),
    and [AIF360](https://oreil.ly/HsKEg).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 源于某种偏见（无论是有意还是无意）的DOS攻击，是一种可信的可用性攻击类型。事实上，这种情况已经发生过。2016年，Twitter用户毒害了[Tay聊天机器人](https://oreil.ly/uPqNx)，导致只有对新纳粹色情内容感兴趣的用户才会觉得系统服务有吸引力。这种攻击也可能发生在更严重的背景下，比如就业、借贷或医疗领域，攻击者利用数据毒化、模型后门或其他类型的攻击，来拒绝某个群体的服务。这是进行偏见测试并在训练时以及作为常规模型监控的一部分进行补救的许多原因之一。有几个优秀的开源工具可用于检测歧视并尝试修复它，比如[aequitas](https://oreil.ly/e412j)、[Themis](https://oreil.ly/yJiT6)和[AIF360](https://oreil.ly/HsKEg)。
- en: 'Ethical hacking: model extraction attacks'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 道德黑客：模型提取攻击
- en: 'Model extraction attacks are harmful on their own, but they are also the first
    stage for a membership inference attack. We should conduct our own model extraction
    attacks to determine if our system is vulnerable to these confidentiality attacks.
    If we find some API or model endpoint that allows us to train a surrogate model
    between input data and system outputs, we lock it down with solid authentication
    and throttle any abnormal requests at this endpoint. Because a model extraction
    attack may have already happened via this endpoint, we need to analyze our extracted
    surrogate models as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 模型提取攻击本身是有害的，但它们也是成员推断攻击的第一阶段。我们应该进行自己的模型提取攻击，以确定我们的系统是否容易受到这些保密攻击的影响。如果我们发现某个API或模型端点允许我们在输入数据和系统输出之间训练一个替代模型，我们将通过坚实的认证措施来锁定它，并限制在这个端点的任何异常请求。由于可能已经通过这个端点发生了模型提取攻击，我们需要按以下方式分析我们提取的替代模型：
- en: What are the accuracy bounds of different types of surrogate models? We must
    try to understand the extent to which a surrogate model can really be used to
    gain knowledge about our ML system.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型替代模型的准确性界限是什么？我们必须尽力理解替代模型在多大程度上真正可用于获取关于我们ML系统的知识。
- en: What types of data trends can be learned from our surrogate model? What about
    linear trends represented by linear model coefficients? Or course summaries of
    population subgroups in a surrogate decision tree?
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的替代模型可以学到哪些数据趋势？线性模型系数表示的线性趋势又如何？或者说人口子群的课程总结在替代决策树中是如何表示的？
- en: What rules can be learned from a surrogate decision tree? For example, how to
    reliably impersonate an individual who would receive a beneficial prediction?
    Or how to construct effective adversarial examples?
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从替代决策树中可以学到哪些规则？例如，如何可靠地模仿一个会得到有利预测的个体？或者如何构建有效的对抗性示例？
- en: If we see that it is possible to train an accurate surrogate model from one
    of our system endpoints, and to answer some of these questions, then we’ll need
    to take some next steps. First, we’ll conduct a membership inference attack on
    ourselves to see if that two-stage attack would also be possible. We’ll also need
    to record all of the information related to this ethical hacking analysis and
    link it to the system’s incident response plan. Incident responders may find this
    information helpful at a later date, and it may unfortunately need to be reported
    as a breach if there is strong evidence that an attack has occurred.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看到可以从系统的一个端点训练一个准确的替代模型，并回答这些问题的可能性，那么我们将需要采取一些下一步措施。首先，我们将对自己进行成员推断攻击，以查看是否也可能发生这种两阶段攻击。我们还需要记录与这次道德黑客分析相关的所有信息，并将其与系统的事件响应计划联系起来。事后响应者可能会在以后的某个日期发现这些信息很有帮助，如果有强有力的证据表明发生了攻击，可能不幸需要报告为一次违规事件。
- en: Debugging security vulnerabilities in our ML system is important work that can
    save us future money, time, and heartache, but so is watching our system to ensure
    it stays secure. Next we’ll take up model monitoring for security.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 调试我们ML系统中的安全漏洞是重要的工作，可以为我们节省未来的金钱、时间和痛苦，但监视我们的系统以确保其保持安全同样重要。接下来，我们将讨论安全模型监控。
- en: Model Monitoring for Security
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全模型监控
- en: 'Once hackers can manipulate or extract our ML model, it’s really not our model
    anymore. To guard against attacks on our model, we’ll not only need to train and
    debug it with security in mind; we’ll also need to monitor it closely once it
    goes live. Monitoring for security should be geared toward algorithmic discrimination,
    anomalies in input data queues, anomalies in predictions, and high usage. Here
    are some tips on what and how to monitor:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦黑客能够操纵或提取我们的ML模型，它实际上就不再是我们的模型了。为了防范对模型的攻击，我们不仅需要在训练和调试时考虑安全性；一旦模型投入使用，我们还需要密切监控它。安全监控应重点关注算法歧视、输入数据队列中的异常、预测中的异常以及高使用率。以下是一些关于什么和如何监控的提示：
- en: Bias monitoring
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见监测
- en: Bias testing, as discussed in other chapters, must be applied during model training.
    But for many reasons, including unintended consequences and malicious hacking,
    discrimination testing must be performed during deployment too. If bias is found
    during deployment, it should be investigated and remediated. This helps to ensure
    a model that was fair during training remains fair in production.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如其他章节所述，偏见测试必须在模型训练期间应用。但由于许多原因，包括意外后果和恶意黑客攻击，部署期间也必须进行歧视测试。如果在部署期间发现偏见，应进行调查和补救。这有助于确保在训练期间公平的模型在生产中仍然是公平的。
- en: Input anomalies
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 输入异常
- en: Unrealistic combinations of data, which could be used to trigger backdoors in
    model mechanisms, should not be allowed into model scoring queues. Anomaly detection
    ML techniques, like autoencoders and isolation forests, may be generally helpful
    in tracking problematic input data. However, we can also use commonsense data
    integrity constraints to catch problematic data before it hits our model. An example
    of such unrealistic data is an age of 40 years and a job tenure of 50 years. If
    possible, we should also consider monitoring for random data, training data, or
    duplicate data. Because random data is often used in model extraction and inversion
    attacks, we build out alerts or controls that help our team understand if and
    when our model may be encountering batches of random data. Real-time scoring of
    rows that are extremely similar or identical to data used in training, validation,
    or testing should be recorded and investigated, as they could indicate a membership
    inference attack. Finally, be on the lookout for duplicate data in real-time scoring
    queues, as this could indicate an evasion or impersonation attack.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不允许将不现实的数据组合进入模型评分队列，因为这可能会触发模型机制中的后门。异常检测ML技术，如自编码器和孤立森林，通常有助于跟踪问题输入数据。然而，在数据到达模型之前，我们也可以使用常识数据完整性约束来捕捉问题数据。这种不现实的数据的一个例子是40岁的年龄和50年的工作年限。如果可能的话，我们还应考虑监控随机数据、训练数据或重复数据。因为随机数据经常被用于模型提取和反演攻击，所以我们建立警报或控制措施，帮助团队了解我们的模型何时可能遇到随机数据批次。应记录并调查实时评分中与训练、验证或测试数据极为相似或相同的行，因为这可能表明成员推理攻击。最后，在实时评分队列中查找重复数据，因为这可能表明逃避或冒名顶替攻击。
- en: Output anomalies
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 输出异常
- en: Output anomalies can be indicative of adversarial example attacks. When scoring
    new data, we compare our ML model prediction against a trusted, transparent benchmark
    model or a benchmark model trained on a trusted data source and pipeline. If the
    difference between our more complex and opaque ML model and our interpretable
    or trusted model is too great, we fall back to the predictions of the conservative
    model or send the row of data for manual processing. Statistical control limits,
    which are akin to moving confidence intervals, can also be used to monitor for
    anomalous outputs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 输出异常可能表明敌对样本攻击。在评分新数据时，我们将我们的ML模型预测与可信、透明的基准模型或基于可信数据源和流水线训练的基准模型进行比较。如果我们更复杂和不透明的ML模型与我们可解释或可信任的模型之间的差异太大，我们会退回到保守模型的预测，或者将数据行发送进行手动处理。统计控制限，类似于移动置信区间，也可以用于监控异常输出。
- en: Metamonitoring
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 元监控
- en: We monitor the basic operating statistics—the number of predictions in a certain
    time period, latency, CPU, memory, and disk load, or the number of concurrent
    users—to ensure our system is functioning normally. We can even train an autoencoder–based
    anomaly detection metamodel on our entire ML system’s operating statistics and
    then monitor this metamodel for anomalies. An anomaly in system operations could
    tip us off that something is generally amiss in our ML system.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们监控基本的操作统计数据——特定时间段内的预测次数、延迟、CPU、内存和磁盘负载，或者并发用户数量——以确保我们的系统正常运行。我们甚至可以对我们整个ML系统的操作统计数据进行基于自编码器的异常检测元模型训练，然后监控这个元模型以检测异常。系统操作中的异常可能会提示我们在ML系统中普遍存在问题。
- en: Monitoring for attacks is one the most proactive steps we can take to counter
    ML hacks. However, there are a still a few more countermeasures to discuss. We’ll
    look into privacy-enhancing technologies next.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 监控攻击是我们可以采取的最积极的措施之一来对抗ML黑客。然而，还有几个反制措施需要讨论。接下来我们将研究隐私增强技术。
- en: Privacy-Enhancing Technologies
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私增强技术（Privacy-Enhancing Technologies）
- en: Privacy-preserving ML is a research subdiscipline with direct ramifications
    for the confidentiality of our ML training data. While just beginning to gain
    steam in the ML and ML operations (MLOps) communities, PETs can give us an edge
    when it comes to protecting our data and models. Some of the most promising and
    practical techniques from this emergent field include federated learning and differential
    privacy.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护机器学习（Privacy-preserving ML）是一个研究子领域，直接影响到我们机器学习训练数据的保密性。尽管在机器学习（ML）和机器学习运营（MLOps）社区刚刚开始受到关注，PETs在保护我们的数据和模型方面可以给我们带来优势。这个新兴领域中一些最有前景和实用的技术包括联邦学习和差分隐私。
- en: Federated learning
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联邦学习
- en: Federated learning is an approach to training ML models across multiple decentralized
    devices or servers holding local data samples, without exchanging raw data between
    them. This approach is different from traditional centralized ML techniques where
    all datasets are uploaded to a single server. The main benefit of federated learning
    is that it enables the construction of ML models without sharing data among many
    parties. Federated learning avoids sharing data by training local models on local
    data samples and exchanging parameters between servers or edge devices to generate
    a global model, which is then shared by all servers or edge devices. Assuming
    a secure aggregation process is used, federated learning helps address fundamental
    data privacy and data security concerns. Among other open source resources, we
    should look into [PySyft](https://oreil.ly/8HpeR) or [FATE](https://oreil.ly/W3uYP)
    to start learning about implementing federated learning at our organization (or
    with partner organizations).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习是一种跨多个分散设备或服务器进行ML模型训练的方法，这些设备或服务器持有本地数据样本，它们之间不交换原始数据。这种方法不同于传统的集中式ML技术，后者要求将所有数据集上传到单个服务器。联邦学习的主要优势在于它能够在不同参与方之间构建ML模型而无需共享数据。联邦学习通过在本地数据样本上训练本地模型，并在服务器或边缘设备之间交换参数来生成全局模型，然后由所有服务器或边缘设备共享。假设使用了安全聚合过程，联邦学习有助于解决基本的数据隐私和数据安全问题。在其他开源资源中，我们应该了解一下[PySyft](https://oreil.ly/8HpeR)或[FATE](https://oreil.ly/W3uYP)，以便开始学习如何在我们的组织（或与合作伙伴组织）实施联邦学习。
- en: Differential privacy
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 差分隐私
- en: 'Differential privacy is a system for sharing information about a dataset by
    describing patterns about groups in the dataset without disclosing information
    about specific individuals. In ML tools, this is often accomplished using specialized
    types of differentially private learning algorithms. This makes it more difficult
    to extract sensitive information from training data or a trained ML model in model
    extraction, model inversion, or membership inference attacks. In fact, an ML model
    is said to be differentially private if an outside observer cannot tell if an
    individual’s information was used to train the model. There are lots of high-quality
    open source repositories to check out and try, including the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私是一种通过描述数据集中组的模式来共享数据集信息的系统，而不透露具体个体的信息。在ML工具中，通常使用特殊类型的差分隐私学习算法来实现这一点。这使得从训练数据或训练后的ML模型中提取敏感信息变得更加困难，例如模型提取、模型反演或成员推断攻击。实际上，如果外部观察者无法判断个体信息是否被用于训练模型，则称ML模型具有差分隐私性质。有许多高质量的开源存储库可供查阅和尝试，包括以下内容：
- en: Google’s [differential-privacy](https://oreil.ly/rjwKK)
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌的[differential-privacy](https://oreil.ly/rjwKK)
- en: IBM’s [diffprivlib](https://oreil.ly/QOFm-)
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM的[diffprivlib](https://oreil.ly/QOFm-)
- en: TensorFlow’s [privacy](https://oreil.ly/WyPD6)
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow的[隐私](https://oreil.ly/WyPD6)
- en: Many ML approaches that invoke differential privacy are based on [differentially
    private stochastic gradient descent (DP-SGD)](https://oreil.ly/raWeC). DP-SGD
    injects structured noise into gradients determined by SGD at each training iteration.
    In general, DP-SGD and related techniques ensure that ML models do not memorize
    too much specific information about training data. Because they prevent ML algorithms
    from focusing on particular individuals, they could also lead to increased generalization
    performance and fairness benefits.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 许多调用差分隐私的ML方法基于[差分私有随机梯度下降（DP-SGD）](https://oreil.ly/raWeC)。DP-SGD在每个训练迭代中向由SGD确定的梯度注入结构化噪声。总体而言，DP-SGD及其相关技术确保ML模型不会对训练数据的具体信息进行过多记忆。因为它们防止ML算法集中于特定个体，它们也可能导致增加的泛化性能和公平性好处。
- en: Readers may hear about confidential computing or homomorphic encryption under
    the PET topic heading as well. These are promising research and technology directions
    to watch as well. Another subdiscipline of ML research to watch is robust ML,
    which can help us counter adversarial example attacks, data poisoning, and other
    adversarial manipulation of our ML system.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 读者们可能在PET主题下听说过保密计算或同态加密。这些也是有前途的研究和技术方向。另一个值得关注的ML研究子领域是鲁棒ML，它可以帮助我们对抗对抗性示例攻击、数据毒化以及其他对ML系统的对抗性操纵。
- en: Robust Machine Learning
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鲁棒机器学习
- en: Robust ML includes many cutting-edge ML algorithms developed to counter adversarial
    example attacks, and to a certain extent data poisoning as well. The study of
    robust ML gained momentum after several researchers showed that silly, or even
    invisible, changes to input data can result in huge swings in output predictions
    for computer vision systems. Such swings in model outcomes are a troubling sign
    in any domain, but when considering medical imaging or semi-autonomous vehicles,
    they are downright dangerous. Robust ML models help enforce stability in model
    outcomes, and, importantly, fairness—that similar individuals be treated similarly.
    Similar individuals in ML training data or live data are individuals that are
    close to one another in the Euclidean space of the data. Robust ML techniques
    often try to establish a hypersphere around individual examples of data, and ensure
    that other similar data within the hypersphere receive similar predictions. Whether
    caused by bad actors, overfitting, underspecification, or other factors, robust
    ML approaches help protect our organization from risks arising from unexpected
    predictions. Interesting papers and code are hosted at the [Robust ML site](https://oreil.ly/H36uh),
    and the Madry Lab at the Massachusetts Institute of Technology has even published
    a full [Python package for robust ML](https://oreil.ly/k-qDZ).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒ML包括许多先进的ML算法，用于对抗对抗性示例攻击，以及在一定程度上对抗数据毒化。在几位研究人员展示出输入数据的微小甚至不可见的变化可能导致计算机视觉系统输出预测的巨大波动之后，对鲁棒ML的研究获得了动力。这种模型结果的波动在任何领域都是一个令人不安的迹象，但在考虑医学成像或半自主车辆时，它们简直是危险的。鲁棒ML模型有助于保持模型结果的稳定性，并且重要的是公平性——即类似的个体应被类似地对待。在ML训练数据或实时数据中，类似的个体是在数据的欧几里得空间中彼此接近的个体。鲁棒ML技术经常尝试在个体数据示例周围建立一个超球体，并确保超球体内的其他相似数据获得类似的预测。无论是由于恶意行为者、过度拟合、规范不足还是其他因素，鲁棒ML方法帮助我们的组织保护免受意外预测带来的风险。有趣的论文和代码托管在[鲁棒ML网站](https://oreil.ly/H36uh)，马萨诸塞理工学院的Madry实验室甚至发布了一个完整的[Python鲁棒ML包](https://oreil.ly/k-qDZ)。
- en: General Countermeasures
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般对策
- en: 'There are a number of catchall countermeasures that can defend against several
    different types of ML attacks, including authentication, throttling, and watermarking.
    Many of these same kinds of countermeasures are also best practices for ML systems
    in general, like interpretable models, model management, and model monitoring.
    The last topic we will address before the chapter’s case study is a brief description
    of important and general countermeasures against ML system attacks:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多综合性对策可以防御多种类型的ML攻击，包括认证、限流和水印。许多这类对策也是ML系统的一般最佳实践，例如可解释的模型、模型管理和模型监控。在本章案例研究之前，我们将讨论一个重要且通用的反对ML系统攻击的简要描述主题：
- en: Authentication
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 认证
- en: Whenever possible, disallow anonymous use for high-stakes ML systems. Login
    credentials, multifactor authentication, or other types of authentication that
    force users prove their identity, authorization, and permission to use a system
    put a blockade between our model API and anonymous bad actors.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，对于高风险的机器学习系统，应禁止匿名使用。登录凭据、多因素认证或其他需要用户证明其身份、授权和使用权限的认证方式，可以在我们的模型API与匿名的恶意行为者之间设置屏障。
- en: Interpretable, fair, or private models
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释、公平或隐私模型
- en: Modeling techniques now exist—e.g., monotonic GBMs (M-GBM), [scalable Bayesian
    rule lists (SBRL)](https://oreil.ly/Md375), [explainable neural networks (XNN)](https://oreil.ly/sd4XX)—that
    can allow for both accuracy and interpretability in ML models. These accurate
    and interpretable models are easier to document and debug than classic unexplainable
    ones. Newer types of fair and private modeling techniques—e.g., [LFR](https://oreil.ly/7ZHmw),
    [DP-SGD](https://oreil.ly/yuddM)—can also be trained to downplay outwardly visible
    demographic characteristics that can be observed, socially engineered into an
    adversarial example attack, or impersonated. These models, enhanced for interpretability,
    fairness, or privacy, should be more easily debugged, more robust to changes in
    an individual entity’s characteristics, and more secure than overused unexplainable
    models.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在存在多种建模技术，例如单调GBM（M-GBM）、[可扩展贝叶斯规则列表（SBRL）](https://oreil.ly/Md375)、[可解释神经网络（XNN）](https://oreil.ly/sd4XX)，这些技术能在机器学习模型中实现精度和可解释性的兼顾。这些准确且可解释的模型比经典的不可解释模型更易于文档化和调试。新型的公平和隐私建模技术，例如[LFR](https://oreil.ly/7ZHmw)、[DP-SGD](https://oreil.ly/yuddM)，还可以被训练用于减弱外观可见的人口特征，这些特征可能被观察到、社会工程化为对抗性示例攻击或被冒用。这些增强了可解释性、公平性或隐私性的模型应该比过度使用的不可解释模型更容易调试，更能应对个体特征变化的挑战，并且更安全。
- en: Model documentation
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 模型文档
- en: Model documentation is a risk-mitigation strategy that has been used for decades
    in banking. It allows knowledge about complex modeling systems to be preserved
    and transferred as teams of model owners change over time, and for knowledge to
    be standardized for efficient analysis by model validators and auditors. Model
    documentation should cover the who, when, where, what, and how of an ML system,
    including many details, from contact information for stakeholders to algorithmic
    specification. Model documentation is also a natural place to record any known
    vulnerabilities or security concerns for an ML system, enabling future maintainers
    or other operators that interact with the system to allocate oversight and security
    resources efficiently. Incident response plans should also be linked to model
    documentation. ([Chapter 2](ch02.html#unique_chapter_id_2) contains a sample documentation
    template.)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 模型文档是一种减少风险的策略，在银行业已经使用了几十年。它允许关于复杂建模系统的知识在模型所有者团队变更时得以保留和传递，并且使知识能够标准化，以便模型验证者和审计人员进行高效分析。模型文档应覆盖ML系统的“谁、何时、何地、什么以及如何”，包括从利益相关者的联系信息到算法规范的许多细节。模型文档还是记录ML系统已知漏洞或安全问题的自然位置，使未来的维护人员或与系统交互的其他操作者能够有效分配监督和安全资源。事故响应计划也应与模型文档相关联。([第2章](ch02.html#unique_chapter_id_2)
    包含一个样本文档模板。)
- en: Model management
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 模型管理
- en: Model management typically refers to a combination of process controls, like
    documentation, combined with technology controls, like model monitoring and model
    inventories. Organizations should have an exact count of deployed ML systems and
    a structured inventory of associated code, data, documentation, and incident response
    plans, and they should monitor all deployed models. These practices make it easier
    to understand when something goes wrong and to deal with problems quickly when
    they arise. ([Chapter 1](ch01.html#unique_chapter_id_1) discusses model risk management
    for ML in much more detail.)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 模型管理通常指一组过程控制，如文档化，与技术控制，如模型监控和模型清单的结合。组织应准确计算部署的ML系统数量，并结构化地列出相关的代码、数据、文档和事故响应计划，并监控所有部署的模型。这些实践使得当出现问题时更容易理解发生了什么，并能够迅速处理问题。([第1章](ch01.html#unique_chapter_id_1)
    更详细地讨论了ML模型风险管理。)
- en: Throttling
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 节流
- en: When high use or other anomalies, such as adversarial examples, or duplicate,
    random, or training data, are identified by model monitoring systems, consider
    throttling prediction APIs or other system endpoints. Throttling can refer to
    restricting high numbers of rapid predictions from single users, artificially
    increasing prediction latency for all users, or other methods that can slow down
    attackers conducting model or data extraction attacks and adversarial example
    attacks.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型监控系统识别出高频使用或其他异常情况，如对抗性示例，重复、随机或训练数据时，请考虑限制预测 API 或其他系统端点的流量。限流可以指限制单个用户的大量快速预测，人为增加所有用户的预测延迟，或其他能够减缓攻击者进行模型或数据提取攻击和对抗性示例攻击的方法。
- en: Watermarking
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 水印技术
- en: Watermarking refers to adding a subtle marker to our data or predictions, for
    the purpose of deterring theft of data or models. If data or predictions carry
    identifiable traits, such as actual watermarks on images or sentinel markers in
    structured data, it can make stolen assets harder to use and more identifiable
    to law enforcement or other investigators once a theft occurs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 水印技术指的是向数据或预测中添加微小标记，以防止数据或模型被盗。如果数据或预测带有可识别的特征，例如图像上的实际水印或结构化数据中的哨兵标记，一旦发生盗窃，这些特征可以使被盗资产更难以使用，并且更容易被执法部门或其他调查人员识别。
- en: Applying these general defenses and best practices, along with some of the more
    specific countermeasures discussed in previous sections, is a great way to achieve
    a high level of security for an ML system. And now that we’ve covered security
    basics, ML attacks, and many countermeasures for those attacks, readers are armed
    with the knowledge needed to start red-teaming your organization’s AI—especially
    if you can work with your organization’s IT security professionals. We’ll now
    examine some real-world AI security incidents to provide additional motivation
    for doing the hard work of red-teaming AI, and to gain insights into some of today’s
    most common ML security issues.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些通用防御和最佳实践，以及前面讨论过的一些具体对策，是实现 ML 系统高安全性的一个好方法。现在我们已经介绍了安全基础知识、ML 攻击以及许多对应的对策，读者们已经掌握了开始进行红队测试组织
    AI 所需的知识——尤其是如果能与组织的 IT 安全专业人员合作。接下来，我们将研究一些真实世界的 AI 安全事件，以提供额外动力来进行 AI 红队测试的艰苦工作，并深入了解当今最常见的
    ML 安全问题。
- en: 'Case Study: Real-World Evasion Attacks'
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：真实世界的规避攻击
- en: ML systems used for both physical and online security have suffered evasion
    attacks in recent years. This case discussion touches on evasion attacks used
    to avoid Facebook filters and perpetuate disinformation and terrorist propaganda,
    and evasion attacks against real-world payment and physical security systems.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，既用于物理安全又用于在线安全的 ML 系统遭受了规避攻击。本案例讨论了用于规避 Facebook 过滤器以及传播虚假信息和恐怖主义宣传的规避攻击，以及针对真实世界支付和物理安全系统的规避攻击。
- en: Evasion Attacks
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规避攻击
- en: As the COVID pandemic ground on and the 2020 US presidential campaign was in
    high gear, those proliferating disinformation related to both topics took advantage
    of weaknesses in Facebook’s manual and automated content filtering. As reported
    by NPR, [“Tiny Changes Let False Claims About COVID-19, Voting Evade Facebook
    Fact Checks”](https://oreil.ly/aYSTr). While Facebook uses news organizations
    such as Reuters and the Associated Press to fact-check claims made by its billions
    of users, it also uses AI-based content filtering, particularly to catch copies
    of human-identified misinformation posts. Unfortunately, minor changes, some as
    simple as different backgrounds or fonts, image cropping, or simply describing
    memes with words instead of images, allowed bad actors to circumvent Facebook’s
    ML-based content filters. In its defense, Facebook does carry out enforcement
    actions against many offenders, including limiting the distribution of posts,
    not recommending posts or groups, and demonetization. Yet, according to one advocacy
    group, Facebook fails to catch about [42% of disinformation posts containing information
    flagged by human fact checkers](https://oreil.ly/fzCrb). The same advocacy group,
    Avaaz, estimates that a sample of just 738 unlabeled disinformation posts led
    to an estimated 142 million views and 5.6 million user interactions.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 随着COVID大流行的持续和2020年美国总统竞选的高潮，那些与这两个主题相关的虚假信息滋生并利用了Facebook手动和自动内容过滤的弱点。根据NPR报道，[“微小的改变使关于COVID-19和选举的虚假声明逃避了Facebook的事实检查”](https://oreil.ly/aYSTr)。尽管Facebook使用路透社和美联社等新闻机构来对其数十亿用户所作声明进行事实检查，但它也使用基于AI的内容过滤器，特别是用于捕捉被人类识别的误导性帖子的副本。不幸的是，一些小改动，如不同的背景或字体，图像裁剪，或者仅仅是用文字描述表情包而不是图片，让恶意行为者能够绕过Facebook基于ML的内容过滤器。为了自卫，Facebook对许多违规者进行了执行行动，包括限制帖子的传播、不推荐帖子或群组以及取消广告收益。然而，根据一家倡导组织的说法，Facebook未能捕捉到大约[42%的被人类事实检查员标记的虚假信息帖子](https://oreil.ly/fzCrb)。同一倡导组织Avaaz估计，仅738个未标记的虚假信息帖子样本导致了大约1.42亿次浏览和560万次用户互动。
- en: Recent events have shown us that online disinformation and security threats
    can spill over into the real world. Disinformation about the 2020 US election
    and the COVID pandemic are thought to be primary drivers of the frightening US
    Capitol riots of January 6, 2021\. In perhaps even more disturbing evasion attacks,
    the BBC has reported that [ISIS operatives continue to evade Facebook content
    filters](https://oreil.ly/qV25u). By blurring logos, splicing their videos with
    mainstream news content, or just using strange punctuation, ISIS members or affiliates
    have been able to post propaganda, explosive-making tutorials, and even evasion
    attack tutorials to Facebook, garnering tens of thousands views for their violent,
    disturbing, and vitriolic content. While evasion attacks on AI-based filters are
    certainly a major culprit, there are also fewer human moderators for Arabic content
    on Facebook. Regardless of whether it’s humans or machines failing at the job,
    this type of content can be truly dangerous, contributing both to radicalization
    and real-world violence. Physical evasion attacks are also a concern for the near
    future. Researchers recently showed that [some AI-based physical security systems
    are easy targets for evasion attacks](https://oreil.ly/xVmDj). With the permission
    of system operators, researchers used lifelike three-dimensional masks to bypass
    facial recognition security checks on Alipay and WeChat payment systems. In one
    egregious case, researchers were even able to use a picture of another person
    on an IPhone screen to board a plane at Amsterdam’s Schiphol Airport.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的事件表明，网络虚假信息和安全威胁可能会波及现实世界。有关2020年美国大选和COVID大流行的虚假信息被认为是引发2021年1月6日令人震惊的美国国会骚乱的主要驱动力。在更为令人不安的逃避攻击中，BBC报道称[ISIS成员继续逃避Facebook内容过滤器](https://oreil.ly/qV25u)。通过模糊标志、将他们的视频与主流新闻内容拼接，或者仅仅使用奇怪的标点符号，ISIS成员或附属机构能够发布宣传、制造爆炸物的教程，甚至逃避攻击的教程到Facebook上，为他们的暴力、令人不安和恶毒内容赢得数以万计的观看。虽然针对基于AI的过滤器的逃避攻击显然是主要罪魁祸首，但Facebook上阿拉伯语内容的人类审核员较少。无论是人类还是机器在工作中失败，这类内容都可能真正危险，既促进激进化，又导致现实世界的暴力。物理逃避攻击也是不久的将来的一个担忧。研究人员最近表明，一些基于AI的物理安全系统易受逃避攻击的攻击目标，[特别是使用逼真的三维面具绕过支付宝和微信支付系统的面部识别安全检查](https://oreil.ly/xVmDj)。在一个严重的案例中，研究人员甚至能够在阿姆斯特丹斯希普霍尔机场使用另一个人的图片在iPhone屏幕上登机。
- en: Lessons Learned
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学到的教训
- en: 'Taken together, bad actors’ evasions of online safeguards to post dangerous
    content, and evasions of physical security systems to make monetary payments and
    to travel by plane paints a scary picture of a world where ML security is not
    taken seriously. What lessons learned from this chapter could be applied to prevent
    these evasion attacks? The first lesson is related to robust ML. ML systems used
    for high-stakes security applications, be they online or real world, must not
    be fooled by tiny changes to normal system inputs. Robust ML, and related technologies,
    must progress to the point where simplistic evasion techniques, like blurring
    of logos or changes to punctuation, are not effective evasion measures. Another
    lesson comes from the beginning of the chapter: the adversarial mindset. Anyone
    who thought seriously about security risks for these AI-based security systems
    should have realized that masks, or just other images, were an obvious evasion
    technique. Thankfully, it turns out that some organizations do employ countermeasures
    for adversarial scenarios. Better facial recognition security systems deploy techniques
    meant to ensure the liveness of the subjects they are identifying. The better
    facial recognition systems also employ discrimination testing to ensure availability
    is high, and error rates are as low as possible, for all their users.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 将在线保护措施逃避的不良行为者与逃避物理安全系统以进行货币支付和飞行联系起来，描绘了一个世界的可怕画面，即机器学习安全问题未被认真对待的世界。从本章中学到的教训如何应用以防止这些逃避攻击？第一课程与强大的机器学习相关。用于高风险安全应用的机器学习系统，无论是在线还是现实世界，都不能被对正常系统输入的微小更改所欺骗。强大的机器学习及相关技术必须进步到这样一个程度，即简单的逃避技术，如模糊化标志或更改标点符号，不再是有效的逃避手段。另一个教训来自本章开头：对抗性思维。任何认真思考过这些基于AI安全系统安全风险的人都应该意识到，面具或其他图像是一种明显的逃避技术。值得庆幸的是，一些组织确实对对抗场景采取了应对措施。更好的面部识别安全系统采用旨在确保所识别对象活跃性的技术。更好的面部识别系统还采用歧视性测试，以确保所有用户的可用性尽可能高，错误率尽可能低。
- en: Another major lesson to be learned from real-world evasion attacks pertains
    to the responsible use of technology in general, and ML in particular. Social
    media has proliferated beyond physical borders, and its complexity has grown past
    many countries’ current abilities to effectively regulate it. With a lack of government
    regulation, users are counting on social media companies to regulate themselves.
    Being tech companies, social networks often rely on more technology, like AI-based
    content filters, to retain control of their systems. But what if those controls
    don’t really work? As technology and ML play larger roles in human lives, lack
    of rigor and responsibility in their design, implementation, and deployment will
    have ever-increasing consequences. Those designing technologies for security or
    other high-stakes applications have an especially serious obligation to be realistic
    about today’s ML capabilities and apply process and technology controls to ensure
    adequate real-world performance.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从现实世界的逃避攻击中可以学到的另一个重要课程与技术的负责使用有关，尤其是机器学习。社交媒体已经蔓延到超越物理边界，其复杂性已经超过了许多国家目前有效监管的能力。在缺乏政府监管的情况下，用户寄望于社交媒体公司自我监管。作为技术公司，社交网络通常依赖更多技术，如基于AI的内容过滤器，来保持对其系统的控制。但如果这些控制实际上并不起作用呢？随着技术和机器学习在人类生活中扮演更大角色，设计、实施和部署中缺乏严谨和责任将会带来日益严重的后果。那些为安全或其他高风险应用设计技术的人有着特别严肃的责任，要对今天的机器学习能力保持现实，并应用过程和技术控制来确保足够的现实世界性能。
- en: Resources
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: Further Reading
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[“A Marauder’s Map of Security and Privacy in Machine Learning”](https://oreil.ly/0k7D3)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“安全和隐私的机器学习掠夺者地图”](https://oreil.ly/0k7D3)'
- en: '[“BIML Interactive Machine Learning Risk Framework”](https://oreil.ly/csQ22)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“BIML 交互式机器学习风险框架”](https://oreil.ly/csQ22)'
- en: '[FTC’s “Start with Security” guidelines](https://oreil.ly/jmeja)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FTC的“从安全开始”指南](https://oreil.ly/jmeja)'
- en: '[Adversarial Threat Landscape for Artificial-Intelligence Systems](https://oreil.ly/KxEbC)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能系统的对抗性威胁景观](https://oreil.ly/KxEbC)'
- en: '[NIST Computer Security Resource Center](https://oreil.ly/pncXb)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NIST 计算机安全资源中心](https://oreil.ly/pncXb)'
- en: '[NIST de-identification tools](https://oreil.ly/M8xhr)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NIST 数据去标识化工具](https://oreil.ly/M8xhr)'
