- en: Chapter 4\. Algorithms for Edge AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章\. 边缘 AI 算法
- en: 'There are two main categories of algorithms that are important in edge AI:
    feature engineering and artificial intelligence. Both types have numerous subcategories;
    in this chapter we’re going to explore a cross-section of them.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘 AI 中，有两类重要的算法：特征工程和人工智能。这两种类型都有许多子类别；在本章中，我们将探讨它们的横截面。
- en: The goal is to provide an overview for each algorithm type from an engineering
    perspective, highlighting their typical usage, strengths, weaknesses, and suitability
    for deployment on edge hardware. This should give you a place to start when planning
    real-world projects, which we’ll walk through in the coming chapters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是从工程角度提供每种算法类型的概述，突出它们的典型用途、优势、劣势以及在边缘硬件部署中的适用性。这将为您在规划实际项目时提供一个起点，我们将在接下来的章节中详细介绍。
- en: Feature Engineering
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: In data science, feature engineering is the process of turning raw data into
    inputs usable by the statistical tools we use to describe and model situations
    and processes. Feature engineering involves using your domain expertise to understand
    which parts of the raw data contain the relevant information, then extracting
    that signal from the surrounding noise.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，特征工程是将原始数据转化为统计工具可用的输入的过程，用于描述和建模情况和过程。特征工程涉及使用您的领域专业知识来理解原始数据的哪些部分包含相关信息，然后从周围的噪音中提取该信号。
- en: From an edge AI perspective, feature engineering is all about transforming raw
    sensor data into usable information. The better your feature engineering, the
    easier life is for the AI algorithms that are attempting to interpret it. When
    working with sensor data, feature engineering naturally makes use of digital signal
    processing algorithms. It can also involve chopping the data into manageable chunks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从边缘 AI 的角度来看，特征工程关乎将原始传感器数据转化为可用信息。特征工程做得越好，AI 算法在尝试解释它时就会越轻松。在处理传感器数据时，特征工程自然会利用数字信号处理算法。它还可以涉及将数据切分成可管理的块。
- en: Working with Data Streams
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理数据流
- en: As we’ve seen, the majority of sensors produce time series data. The goal of
    an edge AI application is to take these streams of time series data and make sense
    of them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，大多数传感器产生时间序列数据。边缘 AI 应用的目标是获取这些时间序列数据流并理解其含义。
- en: The most common way to manage streams is to chop a time series into chunks,
    often called windows, then analyze the chunks one at a time.^([1](ch04.html#idm45988807896720))
    This produces a time series of results that you can interpret in order to understand
    what is going on. [Figure 4-1](#windowing_diagram) shows how a window is taken
    from a stream of data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 管理流的最常见方法是将时间序列切分成块，通常称为窗口，然后逐个分析这些块。^([1](ch04.html#idm45988807896720)) 这会产生一系列结果的时间序列，您可以解释以了解正在发生的事情。[图
    4-1](#windowing_diagram) 显示了如何从数据流中获取窗口。
- en: '![A diagram of a window being taken from a time series and processed into a
    result.](assets/aiae_0401.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![从时间序列中提取窗口并处理成结果的图示。](assets/aiae_0401.png)'
- en: Figure 4-1\. A time series is often broken into chunks, called windows, which
    are analyzed one at a time
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 时间序列通常被分成称为窗口的块，逐个进行分析
- en: It takes a certain amount of time to process a single chunk of data—we can call
    this the *latency* of our system. This limits how often we can take and process
    a window of data. The rate at which we can capture and process data is known as
    the *frame rate* of a system, often expressed in the number of windows that can
    be processed per second. Frames may be sequential or they may overlap, as shown
    in [Figure 4-2](#windowing_diagram_2).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 处理单个数据块需要一定时间 — 我们可以称之为系统的*延迟*。这限制了我们可以多频繁地获取和处理数据窗口的次数。我们可以捕获和处理数据的速率称为系统的*帧率*，通常以每秒可处理的窗口数来表达。帧可以是顺序的，也可以是重叠的，如
    [图 4-2](#windowing_diagram_2) 所示。
- en: '![A diagram showing windows of a time series at two different frame rates,
    one sequential and one overlapping.](assets/aiae_0402.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![显示时间序列的窗口在两种不同帧率下的图示，一种是顺序的，另一种是重叠的。](assets/aiae_0402.png)'
- en: Figure 4-2\. Depending on the frame rate, windows can potentially overlap; overlapping
    is desirable for data that contains events because it increases the chance that
    an entire event will fall within a window, rather than being cut short
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 根据帧率不同，窗口可能会重叠；对包含事件的数据来说，重叠是有利的，因为这增加了整个事件落入窗口内的机会，而不会被截断。
- en: The lower the latency, the more windows of data can be analyzed in a given period
    of time. The more analysis you can do, the more reliable the results. For example,
    imagine we are using a machine learning model to recognize a command. If the windows
    are too far apart, we might miss critical parts of a spoken command and not be
    able to recognize it (see [Figure 4-3](#windowing_diagram_3)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟越低，每个时间段内可以分析的数据窗口就越多。你可以进行的分析越多，结果就越可靠。例如，想象我们正在使用机器学习模型来识别一个命令。如果窗口之间的间隔太大，我们可能会错过口头命令的关键部分，无法识别它（见[图 4-3](#windowing_diagram_3)）。
- en: '![A diagram showing windows of a time series at a low frame rate, with some
    events being missed.](assets/aiae_0403.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![显示时间序列窗口在低帧率下的图表，其中一些事件被错过。](assets/aiae_0403.png)'
- en: Figure 4-3\. If the frame rate is too low, some parts of the signal will not
    be processed; if you are trying to detect short-lived events, this might mean
    that some events are missed
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 如果帧率过低，信号的某些部分将不会被处理；如果你试图检测短暂事件，这可能意味着某些事件被忽略了。
- en: The choice of window size is very important. The larger the window, the longer
    it takes to process the data within it. However, larger windows contain more information
    about the signal—meaning they may make life easier for the signal processing and
    AI algorithms being used. The trade-off between window size and frame rate is
    an important thing to explore when you are developing a system.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口大小的选择非常重要。窗口越大，处理其中数据所需的时间越长。然而，较大的窗口包含了关于信号的更多信息——这意味着它们可能会使信号处理和AI算法的生活变得更轻松。在开发系统时，窗口大小和帧率之间的权衡是需要探索的重要内容。
- en: As we’ll see later, there are many different AI algorithms—and some of them
    are more sensitive to window size than others. Some algorithms (typically those
    that maintain an internal memory of what is occurring in a signal) are able to
    work well with very small window sizes, while others require large window sizes
    in order to properly parse a signal. Algorithm choice also impacts latency, which
    also constrains window size. It’s a complex system of trade-offs between window
    size, latency, and algorithm choice.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如后文所述，有许多不同的AI算法，其中一些对窗口大小的敏感性较大。有些算法（通常是那些在信号中维护内部记忆的算法）能够在非常小的窗口大小下工作良好，而其他一些则需要较大的窗口大小才能正确解析信号。算法的选择还会影响延迟，这也限制了窗口大小。这是窗口大小、延迟和算法选择之间复杂的权衡系统。
- en: 'Windowing also applies to video streams: in this case, each “window” of the
    video is a certain number of still images—typically a single one, but some AI
    algorithms can potentially analyze several images at the same time.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口处理也适用于视频流：在这种情况下，视频的每个“窗口”是一定数量的静态图像——通常是一个，但一些AI算法有可能同时分析多个图像。
- en: More sophisticated techniques for dealing with streams of data fall into the
    category of digital signal processing. These techniques can be combined with windowing
    in order to create data that feeds AI algorithms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据流的更复杂技术属于数字信号处理范畴。这些技术可以与窗口处理结合使用，以生成供AI算法使用的数据。
- en: Digital Signal Processing Algorithms
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数字信号处理算法
- en: There are hundreds of different signal processing algorithms that can help digest
    the signals produced by sensors. In this section, we’ll cover some of the DSP
    algorithms that are most important for edge AI.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有数百种不同的信号处理算法可以帮助消化传感器产生的信号。在本节中，我们将介绍一些对边缘AI非常重要的DSP算法。
- en: Resampling
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重采样
- en: All time series signals have a sample rate (also known as a frequency), often
    described in terms of the number of data samples per second (Hz). It’s often necessary
    to change the sample rate of a signal. For example, you might want to reduce the
    rate of a signal (known as *downsampling*) if it is producing data faster than
    you can process it. On the other hand, you may want to increase the rate of a
    signal (*upsampling*) so that it can be conveniently analyzed alongside another
    signal that has a higher frequency.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所有时间序列信号都有一个采样率（也称为频率），通常用每秒数据样本数（Hz）来描述。通常需要改变信号的采样率。例如，如果信号产生的数据比你处理它的速度快，可能需要减少信号的速率（称为*降采样*）。另一方面，如果要方便地与另一个频率较高的信号一起分析，可能需要增加信号的速率（*升采样*）。
- en: Downsampling works by “throwing away” some of the samples in order to achieve
    the target frequency. For example, if you threw away every other frame of a 10
    Hz (10 samples per second) signal it would become a 5 Hz signal. However, due
    to a phenomenon called aliasing, reducing the frequency in this way can lead to
    distortion in the output. To help combat this, signals must have some high-frequency
    information removed before they are downsampled. This is achieved using a low-pass
    filter, described in the next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**下采样**通过“丢弃”一些样本来实现目标频率。例如，如果你丢弃一个10 Hz（每秒10个样本）信号的每一帧，它将成为一个5 Hz信号。然而，由于一种称为混叠的现象，以这种方式减少频率可能会导致输出的失真。为了帮助应对这个问题，在进行下采样之前必须删除一些高频信息。这通过低通滤波器来实现，在下一节中描述。'
- en: Upsampling works in the opposite way—new samples are created and inserted to
    increase the frequency of a signal. For example, if an extra sample was inserted
    after every sample in a 10 Hz signal, it would become a 20 Hz signal. The difficult
    part is knowing what to insert! There’s no way to know what would actually have
    been happening during the time between two samples, but a technique known as *interpolation*
    can be used to fill in the blanks with an approximation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**上采样**是反向操作——新样本被创建并插入以增加信号的频率。例如，如果在一个10 Hz信号中的每个样本后插入一个额外的样本，它将变成一个20 Hz信号。困难的部分是知道要插入什么！在两个样本之间的时间内实际上发生了什么是无法知道的，但是可以使用称为*插值*的技术来用近似值填补空白。'
- en: In addition to time series, images can also be upsampled and downsampled. In
    this case, it’s the spatial resolution (pixels per image) that is being increased
    or decreased. Like time series resampling, the resizing of images also requires
    anti-aliasing or interpolation techniques.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了时间序列，图像也可以进行上采样和下采样。在这种情况下，增加或减少的是空间分辨率（每幅图像的像素）。像时间序列重采样一样，图像的调整也需要抗混叠或插值技术。
- en: Both upsampling and downsampling are important, but downsampling is more commonly
    encountered in edge AI. It’s typical for sensors to produce an output at a set
    frequency, leaving it to the developer to downsample and obtain the frequency
    that best suits the rest of their signal processing pipeline.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上采样和下采样都很重要，但在边缘人工智能中更常见的是下采样。传感器通常以固定频率产生输出，将其降采样并获得最适合其信号处理流程的频率，这是很典型的做法。
- en: For edge AI applications, upsampling is mostly useful if you wish to combine
    two signals with different frequencies into a single time series. However, this
    can also be achieved by downsampling the higher frequency signal, which might
    be computationally cheaper.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于边缘人工智能应用程序，如果您希望将两个具有不同频率的信号合并为单一的时间序列，则上采样大多数情况下是有用的。然而，通过对高频信号进行下采样也可以实现这一点，这可能在计算上更便宜。
- en: Filtering
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**滤波**'
- en: A digital filter is a function that, applied to a time series signal, transforms
    it in certain ways. Many different types of filters exist, and they can be very
    useful in preparing data for edge AI algorithms.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数字滤波器是应用于时间序列信号的函数，以特定的方式转换它。存在许多不同类型的滤波器，它们在为边缘人工智能算法准备数据方面非常有用。
- en: Low-pass filters are designed to allow low-frequency elements of a signal to
    pass through, while removing high-frequency elements. The *cutoff frequency* of
    the filter describes the frequency beyond which high-frequency signals will be
    affected, and the *frequency response* describes how much those signals will be
    affected.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**低通滤波器**旨在允许信号的低频成分通过，同时去除高频成分。滤波器的*截止频率*描述了高频信号将受到影响的频率，而*频率响应*描述了这些信号将受到多大影响。'
- en: High-pass filters are the same thing in reverse, allowing frequencies *above*
    a cutoff frequency to pass, and attenuating (reducing) those below. A band-pass
    filter combines the two, allowing frequencies within a certain *band* but attenuating
    those outside of it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**高通滤波器**则是相反的操作，允许**高于**截止频率的频率通过，并减弱（降低）低于截止频率的频率。**带通滤波器**结合了两者，允许一定**带宽**内的频率通过，但减弱带外的频率。'
- en: The purpose of filtering in edge AI is to isolate the useful parts of a signal,
    removing parts that do not contribute to solving the problem. For example, a speech
    recognition application could use a band-pass filter to allow frequencies in the
    normal range of human speech (125 Hz to 8 kHz) while rejecting information in
    other frequencies. This could make it easier for a machine learning model to interpret
    the speech without being distracted by other information in the signal.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘人工智能中，过滤的目的是隔离信号的有用部分，去除不有助于解决问题的部分。例如，语音识别应用可以使用带通滤波器，允许人类语音正常范围内的频率（125
    Hz至8 kHz），同时拒绝其他频率的信息。这可以使机器学习模型更容易解释语音，而不会被信号中的其他信息分散注意力。
- en: Filters can be applied to any type of data. For example, if a low-pass filter
    is applied to an image, it has a blurring or smoothing effect. If a high-pass
    filter is applied to the same image, it will “sharpen” details.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器可以应用于任何类型的数据。例如，如果将低通滤波器应用于图像，则会产生模糊或平滑效果。如果将高通滤波器应用于同一图像，则会“锐化”细节。
- en: One type of low-pass filter is a *moving average filter*. Given a time series,
    it calculates a moving average of values within a certain window. In addition
    to smoothing the data, it has the effect of making a single value represent information
    from a wide range of time.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一种低通滤波器是*移动平均滤波器*。给定一个时间序列，它计算在一定窗口内数值的移动平均。除了平滑数据外，它还能使单个数值代表一段时间内的广泛信息。
- en: If several moving averages are calculated and stacked together, each with differing
    window lengths, a momentary snapshot of the signal (containing several different
    moving averages) contains information about changes in the signal across a window
    of time and a number of different frequencies. This can be a helpful technique
    in feature engineering, since it means an AI algorithm can observe a broad window
    of time using relatively few data points.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计算并堆叠几个具有不同窗口长度的移动平均值，则信号的瞬时快照（包含几个不同移动平均值的信息）可以描述信号在时间窗口和多个不同频率上的变化。这对于特征工程是一种有用的技术，因为它意味着AI算法可以使用相对较少的数据点观察到广泛时间窗口内的信号变化。
- en: Filtering is an extremely common signal processing operation. Many embedded
    processors provide hardware support for some types of filtering, which reduces
    latency and energy usage.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤是一种非常常见的信号处理操作。许多嵌入式处理器提供对某些类型的过滤的硬件支持，这样可以减少延迟和能量消耗。
- en: Spectral analysis
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 频谱分析
- en: A time series signal can be said to be in the *time domain*, meaning it represents
    how a set of variables change over time. Using some common mathematical tools,
    it’s possible to transform a time series signal into the *frequency domain*. The
    values obtained through transformation describe how much of the signal lies in
    various frequency bands over a range of frequencies—a spectrum.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列信号可以称为*时间域*，表示它代表一组变量随时间变化的方式。使用一些常见的数学工具，可以将时间序列信号转换成*频率域*。通过转换获得的数值描述了信号在不同频段上的分布情况，形成一个频谱。
- en: By slicing a signal into multiple, thin windows and then transforming each window
    into the frequency domain, as shown in [Figure 4-5](#wave_spectrogram), it’s possible
    to create a map of how the signal’s frequencies change over time. This map, known
    as a spectrogram, serves as a very effective input to machine learning models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将信号切片成多个窄窗口，然后将每个窗口转换成频率域，如[图 4-5](#wave_spectrogram)所示，可以创建一个描述信号频率随时间变化的图谱。这个图谱被称为频谱图，是机器学习模型非常有效的输入。
- en: '![](assets/aiae_0405.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiae_0405.png)'
- en: Figure 4-5\. The same clip of audio represented as a waveform in the time domain
    (top) and a spectrogram in the frequency domain (bottom)
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 同一段音频以时间域（顶部）和频率域（底部）表示的波形
- en: Spectrograms are commonly used in real-world applications, especially around
    audio. Separating the data into windowed frequency bands allows relatively small
    and simple models to interpret it.^([2](ch04.html#idm45988808474368)) It’s also
    possible for humans to visually distinguish one word from another while looking
    at spectrograms—some people have even learned to read them.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 频谱图在现实世界的应用中非常常见，特别是在音频领域。将数据分隔成窗口化的频段使得相对简单的模型可以解释它。^([2](ch04.html#idm45988808474368))
    人们甚至可以通过观察频谱图区分单词之间的差异，有些人甚至已经学会了阅读它们。
- en: There are many algorithms that can transform a signal from the time to the frequency
    domain, but the most common is the Fourier transform. It’s a very common operation,
    and there’s often hardware support (or at least optimized implementations) available
    for performing Fourier transforms on embedded devices.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法可以将信号从时间域转换到频率域，但最常见的是傅立叶变换。这是一个非常常见的操作，并且通常有硬件支持（或至少优化的实现），可用于在嵌入式设备上执行傅立叶变换。
- en: 'There are a huge number of algorithms and techniques for digital signal processing
    and time series analysis; they’re major fields of engineering and study. Some
    great resources on the subjects are:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数字信号处理和时间序列分析有大量的算法和技术；它们是工程和研究的主要领域。关于这些主题的一些重要资源包括：
- en: '[*The Scientist and Engineer’s Guide to Digital Signal Processing*](https://oreil.ly/jo0UJ),
    by Steven W. Smith (California Technical, 1997)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*《科学家和工程师的数字信号处理指南》*](https://oreil.ly/jo0UJ)，作者Steven W. Smith（加利福尼亚技术出版社，1997）'
- en: '[*Practical Time Series Analysis*](https://oreil.ly/NoZrs), by Aileen Nielsen
    (O’Reilly, 2019)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*《实用时间序列分析》*](https://oreil.ly/NoZrs)，作者Aileen Nielsen（O’Reilly，2019）'
- en: Image feature detection
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像特征检测
- en: 'A whole subset of signal processing algorithms are concerned with the extraction
    of useful features^([3](ch04.html#idm45988808463232)) from images. These have
    traditionally been referred to as *computer vision* algorithms. Some common examples
    include:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一整套信号处理算法专注于从图像中提取有用的特征^([3](ch04.html#idm45988808463232))。这些传统上被称为*计算机视觉*算法。一些常见的例子包括：
- en: Edge detection
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘检测
- en: Used to identify boundaries in an image (see [Figure 4-6](#figure_edge_detection_algorithms_boundaries))
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 用于识别图像中的边界（见[图 4-6](#figure_edge_detection_algorithms_boundaries)）
- en: Corner detection
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 角点检测
- en: Used to find points in an image that have an interesting two-dimensional structure
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 用于发现图像中具有有趣的二维结构的点
- en: Blob detection
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 斑点检测
- en: Used to identify regions of an image that have something in common
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 用于识别图像中具有共同特征的区域
- en: Ridge detection
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 脊检测
- en: Used to identify curves within an image
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 用于识别图像中的曲线
- en: '![A visualization of edges detected within a photograph of a person.](assets/aiae_0406.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![一个展示图像中检测到的边缘的可视化效果。](assets/aiae_0406.png)'
- en: Figure 4-6\. Edge detection algorithms find boundaries between areas with different
    colors or intensities
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6\. 边缘检测算法可找出不同颜色或强度区域之间的边界。
- en: Image feature detection reduces a big, messy image into a more compact representation
    of the visual structures that are present within it. This can potentially make
    life easier for any AI algorithms that are operating downstream.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图像特征检测将一个复杂的大图像减少为其内部存在的视觉结构的更紧凑表示。这可能使下游操作的任何AI算法更容易处理。
- en: Feature detection is not always necessary when working with images. Typically,
    deep learning models are able to learn their own ways of extracting features,
    reducing the utility of preprocessing. However, it’s still common to perform feature
    detection when interpreting image data using other types of edge AI algorithm.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理图像时，不总是需要进行特征检测。通常情况下，深度学习模型能够学习提取特征的方法，减少了预处理的效用。然而，在使用其他类型的边缘AI算法解释图像数据时，执行特征检测仍然很常见。
- en: The [OpenCV project](https://opencv.org) provides a set of libraries for feature
    detection (and other image-processing tasks) that will run on most SoC devices.
    For microcontrollers, [OpenMV](https://openmv.io) provides an open source library
    of feature detection algorithm implementations along with hardware designed to
    run them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenCV项目](https://opencv.org)提供了一组用于特征检测（以及其他图像处理任务）的库，可在大多数SoC设备上运行。对于微控制器，[OpenMV](https://openmv.io)提供了一个开源库，其中包含特征检测算法实现以及专为运行它们而设计的硬件。'
- en: Combining Features and Sensors
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合特征和传感器
- en: There’s nothing stopping you from combining several different features and signals
    as the input to your AI algorithms. For example, you could calculate several moving
    averages of a time series over several different windows and pass them all into
    a machine learning model together. There are no hard-and-fast rules, so feel free
    to experiment and be creative with the way you slice and dice your data. The following
    chapters will provide a framework for experimentation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么能阻止您将几个不同的特征和信号组合作为AI算法的输入。例如，您可以计算时间序列的几个移动平均值，并将它们全部传递给机器学习模型。没有硬性规定，所以可以自由实验，并创意性地处理数据。接下来的章节将提供实验框架。
- en: Going beyond combining features from the same signal, *sensor fusion* is the
    concept of integrating data from multiple sensors together. For example, an edge
    AI fitness tracker could combine information from an accelerometer, gyroscope,
    and heart rate sensor to try to detect which sport a wearer is playing.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 超越从同一信号中组合特征，*传感器融合*是将来自多个传感器的数据集成的概念。例如，边缘AI健身追踪器可以结合来自加速度计、陀螺仪和心率传感器的信息，试图检测佩戴者正在进行的运动。
- en: In a more complex edge AI scenario, the sensors don’t even have to be integrated
    with the same device. Imagine a smart climate control system that makes use of
    temperature and occupancy sensors distributed throughout a building to optimize
    air conditioning usage.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在更复杂的边缘AI场景中，传感器甚至不需要集成在同一设备上。想象一下智能气候控制系统，利用分布在建筑物各处的温度和占用传感器来优化空调使用。
- en: 'There are three categories of sensor fusion:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器融合分为三类：
- en: Complementary
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 补充
- en: Where multiple sensors combine to deliver a more complete understanding of a
    situation than would be possible with a single sensor—for example, the various
    sensors on our hypothetical fitness tracker.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 多个传感器结合，以提供比单一传感器更全面的情况理解，例如我们假设的健身追踪器上的各种传感器。
- en: Competitive
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争
- en: Where multiple sensors measure the same exact thing in order to reduce the likelihood
    of bad measurements—for example, multiple redundant sensors monitoring the temperature
    of a critical piece of equipment.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 多个传感器测量相同的事物，以减少错误测量的可能性，例如，多个冗余传感器监测关键设备的温度。
- en: Cooperative
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 合作
- en: Where information from multiple sensors combines to create a signal that was
    not otherwise available—for example, two cameras producing a stereo image that
    provides depth information.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 多个传感器的信息结合，形成原本不可得的信号，例如，两个摄像头产生立体图像，提供深度信息。
- en: 'The challenge inherent in sensor fusion is how to combine multiple signals
    that may even occur at different rates. You should consider the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器融合面临的挑战在于如何将可能以不同速率发生的多个信号结合起来。您应考虑以下内容：
- en: Aligning the signals in time. For many algorithms, it’s important that all of
    the signals we intend to fuse are sampled at the same frequency, and that the
    values reflect simultaneous measurements. This can be achieved through resampling—for
    example, upsampling a low-frequency signal so that it has the same rate as the
    high-frequency signal it is being fused with.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 时间信号的对齐。对于许多算法来说，我们打算融合的所有信号以相同的频率采样，并且值反映出同时测量。这可以通过重新采样来实现，例如，将低频信号上采样，使其具有与要融合的高频信号相同的速率。
- en: Scaling the signals. It’s critical that the signals’ values are on the same
    scale, so that a signal with typically large values does not overwhelm a signal
    with typically smaller ones.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放信号。信号的值必须处于相同的尺度上，以防通常具有较大值的信号压倒通常具有较小值的信号。
- en: Numerically combining the signals. This can be done using simple mathematical
    operations (addition, multiplication, or averaging) or with more sophisticated
    algorithms such as the Kalman filter (covered later)—or simply by concatenating
    the data together and passing it into the algorithm as a single matrix.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数值上结合信号。可以使用简单的数学运算（加法、乘法或平均值）或更复杂的算法，如卡尔曼滤波器（稍后介绍）来完成此操作，或者简单地将数据连接在一起，并将其作为单个矩阵传递给算法。
- en: 'You can perform sensor fusion before or after other stages of feature engineering.
    For an arbitrary example: if you intended to fuse two time series, you might choose
    to run a low pass over one of them first, then scale them to the same scale, combine
    the two through averaging, and transform the combined values into the frequency
    domain. Don’t be afraid to experiment!'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在特征工程的其他阶段之前或之后执行传感器融合。举个任意的例子：如果您打算融合两个时间序列，您可以选择先对其中一个运行低通滤波器，然后将它们缩放到相同的尺度，通过平均值将两者组合，并将组合后的值转换为频域。不要害怕尝试！
- en: We now have some serious tools for processing data. In the next section, we’ll
    explore the AI algorithms that will help us understand it.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一些处理数据的重要工具。在下一节中，我们将探讨能帮助我们理解数据的AI算法。
- en: Artificial Intelligence Algorithms
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能算法
- en: 'There are two ways to think about AI algorithms. One is based on functionality:
    what are they designed to do? The other is based on implementation: how do they
    work? Both aspects are important. Functionality is critical to the application
    you are trying to build, and implementation is important when thinking about your
    constraints—which generally means your dataset and the device you will be deploying
    to.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种思考AI算法的方式。一种是基于功能：它们设计做什么？另一种是基于实施：它们如何工作？这两个方面都很重要。功能对于您正在构建的应用至关重要，而实施在考虑约束时很重要——这通常意味着您的数据集和将要部署到的设备。
- en: Algorithm Types by Functionality
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 功能类型的算法
- en: First up, let’s look at the most important types of algorithm from a functional
    perspective. Mapping the problem you are trying to solve to these algorithm types
    is known as *framing*, and we’ll be diving deep into framing in [Chapter 6](ch06.html#understanding_and_framing_problems).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从功能角度看最重要的算法类型。将您试图解决的问题映射到这些算法类型被称为*框架化*，我们将深入探讨第[6章](ch06.html#understanding_and_framing_problems)中的框架化。
- en: Classification
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类
- en: 'Classification algorithms try to solve the problem of distinguishing between
    various *types*, or *classes*, of things. This could mean:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 分类算法试图解决区分各种*类型*或*类别*事物的问题。这可能意味着：
- en: A fitness monitor with an accelerometer classifying walking versus running
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有加速度计的健身监视器分类步行与跑步
- en: A security system with an image sensor classifying an empty room versus a room
    with a person present
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个带有图像传感器的安全系统，分类空房间与有人房间
- en: A wildlife camera classifying four different species of animal
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个野生动物摄像机分类四种不同物种的动物
- en: '[Figure 4-7](#classifier_figure) shows a classifier being used to determine
    whether a forklift truck is idle or moving, based on data collected by an accelerometer.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-7](#classifier_figure) 显示一个分类器在基于加速度计收集的数据的基础上用于确定铲车是空闲还是移动。'
- en: '![Two graphs showing accelerometer data for a forklift, one classified as idle
    and the other as moving.](assets/aiae_0407.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![显示一个铲车的加速度计数据的两个图表，一个被分类为空闲，另一个被分类为移动。](assets/aiae_0407.png)'
- en: Figure 4-7\. A classifier typically outputs a probability distribution including
    each possible class
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 一个分类器通常输出包含每个可能类别的概率分布
- en: 'Classification can be categorized in a few different ways, depending on the
    task:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 分类可以根据任务的不同方式进行分类：
- en: Binary classification
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类
- en: The input belongs to one of two classes.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输入属于两个类别中的一个。
- en: Multiclass classification
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类
- en: The input belongs to one of more than two classes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输入属于超过两个类别中的一个。
- en: Multilabel classification
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类
- en: The input belongs to zero or more of any number of classes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输入属于零个或多个任意数量的类。
- en: The most common forms of classification are binary and multiclass. With these
    forms of classification, you always need at least two classes. Even if there’s
    only one thing you care about (for example, a person in the room), you also need
    a class that represents everything that you *don’t* care about (for instance,
    rooms that don’t have people in them). Multilabel classification is comparatively
    rare.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的分类形式是二元和多类。使用这些分类形式时，您总是需要至少两个类别。即使只有一件事情是您关心的（例如房间里的一个人），您也需要一个代表您*不*关心的一切的类别（例如没有人的房间）。多标签分类相对较少见。
- en: Regression
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归
- en: 'Regression algorithms try to come up with numbers. This could mean:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 回归算法试图提出数字。这可能意味着：
- en: A smart thermostat that predicts the temperature in an hour’s time
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个智能恒温器预测一小时后的温度
- en: A virtual scale that estimates the weight of a food product using a camera
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个使用摄像头估算食品重量的虚拟秤
- en: A virtual sensor that estimates a motor’s speed of rotation based on its sound
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个基于声音估算电机转速的虚拟传感器
- en: Virtual sensors, like the latter two examples, are a particularly interesting
    case of regression. They can use available sensor data to predict measurements
    from different types of sensor—without actually requiring those sensors to be
    present.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟传感器，如后两个示例，是回归的一个特别有趣的案例。它们可以使用可用的传感器数据来预测来自不同类型传感器的测量值——而无需实际要求这些传感器存在。
- en: Object detection and segmentation
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对象检测和分割
- en: Object detection algorithms take an image or video and identify the locations
    of specific objects within them, often by drawing *bounding boxes* around them.
    They combine classification and regression, identifying specific types of objects
    and predicting their numeric coordinates—as seen in [Figure 4-8](#object_detection_figure).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测算法获取图像或视频，并识别其中特定物体的位置，通常通过绘制*边界框*来实现。它们结合分类和回归，识别特定类型的物体并预测它们的数值坐标，正如在[图
    4-8](#object_detection_figure)中所见。
- en: '![A field of sheep, each identified by an object detection model and labelled
    with a bounding box and probability.](assets/aiae_0408.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![一群羊群，每只都被物体检测模型识别，并标有边界框和概率。](assets/aiae_0408.png)'
- en: Figure 4-8\. A common output for object detection models consists of bounding
    boxes drawn around detected objects, each with an individual confidence score
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 物体检测模型的常见输出包括绘制在检测到的物体周围的边界框，每个边界框带有个别的置信度分数。
- en: Specialized object detection algorithms exist for particular types of objects.
    For example, pose estimation models are designed to recognize human body parts
    and identify their locations within an image—as shown in [Figure 4-9](#pose_estimation).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为特定类型的物体存在专门的物体检测算法。例如，姿势估计模型专门设计用于识别人体部位并确定其在图像中的位置，正如在[图 4-9](#pose_estimation)中所示。
- en: '![A photograph of the author with key body parts labelled by a pose estimation
    model.](assets/aiae_0409.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![一张作者的照片，由姿势估计模型标记了关键身体部位。](assets/aiae_0409.png)'
- en: Figure 4-9\. Pose estimation identifies key points on a human body, the position
    of which can be used as input for other processes
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-9\. 姿势估计识别人体上的关键点，这些点的位置可以作为其他处理的输入。
- en: Segmentation algorithms are similar to object detection algorithms, but they
    classify images at a pixel level. This results in a *segmentation map*, as seen
    in [Figure 4-10](#segmentation_map), which attempts to label areas of the input
    with their content.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 分割算法类似于物体检测算法，但它们在像素级别上对图像进行分类。这导致了一个*分割地图*，正如在[图 4-10](#segmentation_map)中所见，它试图标记输入的区域及其内容。
- en: '![A photograph of a street scene with some of the objects painted over, indicating
    their segmentation.](assets/aiae_0410.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![一张街景照片，其中一些物体被涂抹，显示它们的分割结果。](assets/aiae_0410.png)'
- en: Figure 4-10\. This street scene has been labeled with a segmentation map. Different
    areas, such as people and the road surface, are shown in different shades. A segmentation
    algorithm aims to predict which pixels belong to which type of object.
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. 这幅街景已经用分割地图标记。不同区域，如人和道路表面，显示为不同的色调。分割算法旨在预测哪些像素属于哪种类型的物体。
- en: 'Here are some example use cases for object detection and segmentation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些物体检测和分割的示例用例：
- en: A farm monitor that uses cameras to count the number of animals in a field
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个农场监控器，使用摄像头统计田间动物的数量。
- en: A home fitness system that gives people feedback on their form during workouts
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个家庭健身系统，在锻炼期间为人们提供姿势反馈。
- en: An industrial camera that measures how much of a container is filled with product
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个工业摄像头，用于测量容器中填充产品的量。
- en: Anomaly detection
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异常检测
- en: 'Anomaly detection algorithms recognize when a signal has deviated from its
    normal behavior. They are useful in many applications:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测算法能识别信号偏离其正常行为的情况。它们在许多应用中都很有用：
- en: An industrial predictive maintenance system that can recognize when a motor
    has started to break down by its current draw
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个工业预测性维护系统，能够通过其电流绘制识别电动机开始故障的情况。
- en: A robot vacuum that can identify when it is driving on an unusual surface using
    an accelerometer
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个机器人吸尘器，可以通过加速度计识别其行驶在不寻常表面上的情况。
- en: A trail camera that knows when an unknown animal has walked past
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个追踪摄像头，知道何时有未知的动物经过。
- en: Anomaly detection algorithms are very useful for predictive maintenance. They’re
    also very helpful when paired with machine learning models. Many machine learning
    models will produce spurious, random results if they are presented with an input
    that isn’t in their training set.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测算法对预测性维护非常有用。当与机器学习模型配对时，它们也非常有帮助。如果机器学习模型面对不在其训练集中的输入，则许多模型将产生虚假的随机结果。
- en: To avoid this, an ML model can be paired with an anomaly detection algorithm
    that tells it when something is *out of distribution* so that its spurious results
    can be discarded. Some types of models can also be *calibrated* so that their
    output represents a true probability distribution that can be interpreted to recognize
    when the model is uncertain.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，可以将ML模型与异常检测算法配对，以便在发现“超出分布”的情况时丢弃其虚假结果。某些类型的模型也可以进行“校准”，使其输出表示真实的概率分布，可以用来识别模型不确定性的时候。
- en: Clustering
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类。
- en: 'Clustering algorithms try to group inputs by similarity and can recognize when
    an input is not similar to what it has seen before. They are often used when an
    edge AI device needs to learn from its environment, including for anomaly detection
    applications. For example, consider:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法尝试根据相似性对输入进行分组，并可以识别输入是否与以前看到的不同。它们经常用于边缘AI设备需要从环境中学习的情况，包括异常检测应用程序。例如，考虑以下情况：
- en: A voice assistant that learns which voice belongs to each of its users
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音助手能够学习其用户的声音特征。
- en: A predictive maintenance application that learns a “normal” state of operation
    and can detect deviations from it
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测性维护应用程序学习“正常”操作状态，并能检测其偏离情况。
- en: A vending machine that can recommend drinks based on a user’s previous choices
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动售货机可以根据用户以往的选择推荐饮料。
- en: A clustering algorithm can either learn its clusters on the fly (after deployment)
    or have them configured ahead of time.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '聚类算法可以在部署后实时学习其聚类，或者事先配置好。 '
- en: Dimensionality reduction
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降维。
- en: 'Dimensionality reduction algorithms take a signal and produce a representation
    of it that contains equivalent information but takes up a lot less space. The
    representations of two signals can then be compared to one another easily. Here
    are some example applications:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 降维算法接收一个信号并生成一个包含等效信息但占用更少空间的表示。两个信号的表示可以很容易地进行比较。以下是一些示例应用：
- en: Compression of audio, to make it cheaper to transmit sounds from a remote device
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩音频，使远程设备传输声音更便宜。
- en: Fingerprint recognition, ensuring a fingerprint matches the owner of a device
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指纹识别，确保指纹匹配设备的所有者。
- en: Facial recognition, recognizing individual faces in a video feed
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人脸识别，在视频中识别个别面孔。
- en: Dimensionality reduction tends to be used alongside other AI algorithms, as
    opposed to being used on its own. For example, it can be used in conjunction with
    a clustering algorithm to identify similar signals in complex data types, like
    audio and video.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 降维通常与其他AI算法一起使用，而不是单独使用。例如，它可以与聚类算法结合使用，以识别复杂数据类型（如音频和视频）中的相似信号。
- en: Transformation
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变换。
- en: 'Transformation algorithms take one signal and output another. Here are some
    examples:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 变换算法接收一个信号并输出另一个信号。以下是一些示例：
- en: Noise-canceling headphones that identify and remove specific noises in a signal
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可识别并消除信号中特定噪音的降噪耳机。
- en: A car reversing camera that enhances the image in dark or rainy conditions
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汽车倒车摄像头在黑暗或多雨条件下增强图像。
- en: A speech recognition device that takes an audio signal and outputs a transcription
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别设备接收音频信号并输出转录。
- en: The input and output of transformation algorithms can be extremely different.
    In the case of transcription, the input is a stream of audio data and the output
    is a sequence of words.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 变换算法的输入和输出可能极其不同。在转录的情况下，输入是音频数据流，输出是一系列单词。
- en: Combining Algorithms
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合算法。
- en: There’s no reason you can’t mix different types of algorithms in the same application.
    Later in this section we’ll explore techniques for combining algorithms (see [“Combining
    algorithms”](#combining_algorithms)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一个应用程序中混合不同类型的算法没有理由不可行。本节稍后我们将探讨组合算法的技术（见[“组合算法”](#combining_algorithms)）。
- en: Algorithm Types by Implementation
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按实施方式分类的算法类型。
- en: Exploring algorithms by functionality helps us understand what they are used
    for, but from an engineering perspective it’s important to get a sense for the
    different ways these functionalities can be implemented. There are hundreds of
    different ways to build a classification algorithm, for example, resulting from
    decades of computer science research. Each method has its own unique strengths
    and weaknesses that are amplified by the constraints posed by edge AI hardware.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通过功能来探索算法帮助我们理解它们的用途，但从工程角度来看，了解这些功能可以以不同的方式实现是很重要的。例如，有几百种不同的构建分类算法的方法，这是由几十年的计算机科学研究得出的。每种方法都有其独特的优势和弱点，这些弱点会因边缘AI硬件的限制而放大。
- en: In the following section, we’ll explore the most important ways that edge AI
    algorithms are implemented. Bear in mind that this isn’t an exhaustive list—we’re
    focused on edge AI, so we’re focused on technologies that work well on-device.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨边缘AI算法实施的最重要方式。请记住，这并不是详尽无遗的清单——我们关注的是边缘AI，因此我们专注于在设备上运行良好的技术。
- en: Conditionals and heuristics
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件和启发式
- en: 'The simplest type of AI algorithms are based on conditional logic: simple `if`
    statements that result in decisions. Let’s look back at the code snippet we explored
    in [“Artificial Intelligence”](ch01.html#artificial_intelligence):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单类型的AI算法基于条件逻辑：简单的`if`语句导致决策。让我们回顾一下我们在[“人工智能”](ch01.html#artificial_intelligence)中探讨过的代码片段：
- en: '[PRE0]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This simple algorithm does a basic calculation using some human-defined values
    (`seconds_to_stop`, etc.) and decides whether to apply a car’s brakes. Does this
    count as AI? It’s a question that might stimulate debate—but the answer is emphatically
    yes.^([4](ch04.html#idm45988814548256))
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单算法使用一些人为定义的值（`seconds_to_stop`等）进行基本计算，并决定是否刹车。这算得上是AI吗？这可能会引发辩论，但答案明确是肯定的。^([4](ch04.html#idm45988814548256))
- en: 'The common understanding of artificial intelligence is that it’s a quest to
    create machines that can think like human beings. The engineering definition is
    much more realistic: AI allows computers to do tasks that typically require human
    intelligence. In this case, controlling a car’s brakes to avoid a collision is
    definitely something that has typically required human intelligence. It would
    have been considered extremely impressive twenty years ago, but automatic braking
    is a common feature in modern vehicles.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的普遍理解是，它是一个创造可以像人类一样思考的机器的追求。工程定义更为现实：AI允许计算机执行通常需要人类智能的任务。在这种情况下，控制汽车刹车以避免碰撞显然是需要人类智能的任务。20年前，这被认为是极为令人印象深刻的，但现代车辆中的自动刹车已经成为常见功能。
- en: Note
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Before you laugh at the idea that `if` statements can be artificial intelligence,
    consider that *decision trees*—one of the most popular and effective categories
    of machine learning algorithms—are just `if` statements under the hood. These
    days, even deep learning models can be implemented as binary neural networks,
    which are essentially conditional logic. Intelligence comes from the application,
    not the implementation!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在你嘲笑`if`语句可以成为人工智能的想法之前，请考虑一下，*决策树*——最流行和有效的机器学习算法类别之一——本质上就是`if`语句。如今，甚至深度学习模型可以作为二元神经网络实现，这本质上也是条件逻辑。智能来自于应用，而不是实现！
- en: 'The conditional logic in our car braking algorithm is actually an implementation
    of classification. Given an input (the speed of the car and the distance from
    a wall), the algorithm classifies the situation into one of two types: safe driving
    or impending crash. Conditional logic is naturally used for classification since
    its output is categorical; an `if` statement gives us either one output or another.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们汽车刹车算法中的条件逻辑实际上是分类的一种实现。给定一个输入（汽车的速度和距离墙壁的距离），算法将情况分类为安全驾驶或即将碰撞。条件逻辑自然用于分类，因为其输出是分类的；一个`if`语句给出两种可能的输出之一。
- en: Conditional logic is connected to the idea of *heuristics*. A heuristic is a
    handcrafted rule that can be applied to a situation in order to help understand
    or react to it. For example, our car braking algorithm uses the heuristic that
    if we have less than four seconds before hitting a wall, we should apply the brakes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 条件逻辑与*启发式*的概念密切相关。启发式是一种手工制定的规则，可以应用于某种情况，以帮助理解或应对它。例如，我们的汽车刹车算法使用的启发式是，如果距离撞墙不足四秒钟，我们应该刹车。
- en: Heuristics are designed by human beings using domain knowledge. This domain
    knowledge can be built on data that has been collected about a real-world situation.
    In that respect, our seemingly simple car braking algorithm might actually represent
    some deep, well-researched understanding of the real world. Perhaps the value
    of `seconds_to_stop` was arrived at after millions of dollars’ worth of crash
    tests and represents the ideal value for the constant. With this in mind, it’s
    easy to see how even an `if` statement can represent a significant amount of human
    intelligence and knowledge, captured and distilled into a simple and elegant piece
    of code.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式是人类利用领域知识设计的。这种领域知识可以建立在对现实情况收集的数据上。在这方面，我们看似简单的汽车制动算法可能实际上代表了对现实世界的深入和深入的理解。也许`seconds_to_stop`的值是在进行了数百万美元的碰撞测试后得出的，并且代表了这个常数的理想值。有了这个想法，很容易看出，即使是一个`if`语句也可以代表大量的人类智慧和知识，被捕捉并蒸馏成简单而优雅的代码。
- en: Our car braking example is very simple—but when paired with signal processing,
    conditional logic can make some quite sophisticated decisions. For example, imagine
    you are building a predictive maintenance system that aims to alert workers of
    the health of an industrial machine based on the sounds it makes. Perhaps the
    machine makes a characteristic high-pitched whine when it is about to break down.
    If you capture audio and translate it into the frequency domain using a Fourier
    transform, you can use a simple `if` statement to determine when the whine is
    happening and let the workers know.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的汽车制动示例非常简单，但是当与信号处理配对时，条件逻辑可以做出一些相当复杂的决策。例如，想象一下，您正在构建一个预测维护系统，该系统旨在基于机器发出的声音警报工人机器的健康状态。也许机器在即将故障时会发出特征性的高音响。如果您捕捉音频并使用傅里叶变换将其转换为频域，您可以使用简单的`if`语句来确定何时发生这种尖叫声，并通知工人。
- en: Beyond `if` statements, you can use more complex logic to interpret situations
    based on known rules. For example, an industrial machine may use a handcoded algorithm
    to avoid damage by varying its speed based on measurements of internal temperature
    and pressure. The algorithm might take the temperature and pressure and directly
    calculate an RPM, using human insight that is captured in the code.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`if`语句，您还可以使用更复杂的逻辑根据已知规则解释情况。例如，工业机器可以使用一个手动编码的算法根据内部温度和压力测量来避免损坏，该算法可能会直接计算一个RPM，使用了被捕捉在代码中的人类洞察力。
- en: 'If it works for your situation, conditional logic and other handcoded algorithms
    can be amazing. It is easy to understand, easy to debug, and easy to test. There’s
    no risk of unspecified behavior: the code either branches one way or another,
    and all paths can be exercised with automated tests. It runs incredibly fast and
    will work on any imaginable device.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对您的情况有效，条件逻辑和其他手动编码的算法可以是非常出色的。它易于理解、易于调试和易于测试。不存在未指定行为的风险：代码要么分支一种方式，要么分支另一种方式，并且所有路径都可以通过自动化测试来执行。它运行非常快，并且可以在任何可想象的设备上运行。
- en: There are two major downsides of heuristics. First, developing them may require
    significant domain knowledge and programming expertise. Domain knowledge is not
    always available—for example, a small company might not have the resources to
    conduct the expensive research necessary to understand the fundamental mathematical
    rules of a system. In addition, even given domain knowledge, not everyone has
    the expertise required to design and implement a heuristic algorithm in efficient
    code.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 启发式方法有两个主要缺点。首先，开发它们可能需要显著的领域知识和编程专业知识。领域知识并非总是可得的——例如，一个小公司可能没有资源进行必要的昂贵研究，以理解系统的基本数学规则。此外，即使拥有领域知识，也不是每个人都具备设计和实现高效代码中的启发式算法所需的专业知识。
- en: 'The second big downside is the idea of *combinatorial explosion*. The more
    variables that are present in a situation, the more difficult it is to model with
    traditional computer algorithms. A good example of this is the game of chess:
    there are so many pieces, and so many possible moves, that deciding what to do
    next requires a vast amount of computation. Even the most advanced chess computers
    built using conditional logic can easily be beaten by expert human players.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个大缺点是*组合爆炸*的概念。在一个情况中存在的变量越多，传统计算机算法对其建模就越困难。国际象棋是一个很好的例子：棋子和可能的移动方式如此之多，以至于决定下一步该做什么需要大量的计算。即使是使用条件逻辑构建的最先进的国际象棋计算机也很容易被专业的人类玩家击败。
- en: Some edge AI problems are *far* more complex than games of chess. For example,
    imagine trying to handwrite conditional logic that can determine whether a camera
    image shows an orange or a banana. With some tricks (“yellow means banana, orange
    means orange”) you might succeed for some categories of images—but it would be
    impossible to make it generalize beyond the simplest of scenes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一些边缘AI问题比象棋游戏复杂得多。例如，想象一下尝试手写条件逻辑，以确定相机图像是否显示橙色或香蕉。通过一些技巧（“黄色代表香蕉，橙色代表橙子”），你可能会成功处理一些类别的图像，但是要使其普遍化超出最简单场景是不可能的。
- en: A good rule of thumb for handcoded logic is that the more data values you have
    to deal with, the more difficult it is going to be to get a satisfactory solution.
    Fortunately, there are plenty of algorithms that can step in when a handcoded
    approach fails.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 手写逻辑的一个好的经验法则是，你需要处理的数据值越多，要得到令人满意的解决方案就越困难。幸运的是，当手写方法失败时，有很多算法可以介入。
- en: Classical machine learning
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经典机器学习
- en: Machine learning is a special approach to creating algorithms. Where heuristic
    algorithms are created by handcoding logic based on known rules, machine learning
    algorithms discover their own rules—by exploring large amounts of data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是创建算法的一种特殊方法。启发式算法是通过手写逻辑基于已知规则创建的，而机器学习算法则通过探索大量数据来发现它们自己的规则。
- en: 'The following description, taken from the book *TinyML*, introduces the basic
    ideas behind machine learning:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下描述来自书籍*TinyML*，介绍了机器学习背后的基本思想：
- en: To create a machine learning program, a programmer feeds data into a special
    kind of algorithm and lets the algorithm discover the rules. This means that as
    programmers, we can create programs that make predictions based on complex data
    without having to understand all of the complexity ourselves. The machine learning
    algorithm builds a *model* of the system based on the data we provide, through
    a process we call *training*. The model is a type of computer program. We run
    data through this model to make predictions, in a process called *inference*.
  id: totrans-171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 要创建一个机器学习程序，程序员将数据输入到一种特殊的算法中，并让算法发现规则。这意味着作为程序员，我们可以创建根据复杂数据进行预测的程序，而不必完全理解所有复杂性。机器学习算法根据我们提供的数据建立系统的*模型*，通过我们称之为*训练*的过程。该模型是一种计算机程序。我们通过这个模型运行数据来进行预测，这个过程称为*推理*。
- en: ''
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*TinyML* (O’Reilly, 2019)'
  id: totrans-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*TinyML*（O’Reilly, 2019）'
- en: Machine learning algorithms can perform all of the functional tasks described
    earlier in this chapter, from classification to transformation. The key requirement
    for using machine learning is that you have a *dataset*. This is a large store
    of data, generally collected under real-world conditions, that is used to train
    the model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可以执行本章前述的所有功能任务，从分类到转换。使用机器学习的关键要求是你有一个*数据集*。这是一个大量数据的存储，通常在真实条件下收集，用于训练模型。
- en: Typically, the data needed to train a machine learning model is gathered during
    the development process, aggregated from as many sources as possible. As we’ll
    see in later chapters, a large and varied dataset is critical for working with
    edge AI—but especially machine learning.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在开发过程中需要训练机器学习模型的数据是从尽可能多的来源聚合而来。正如我们将在后面章节看到的那样，一个大而多样的数据集对于处理边缘AI尤其是机器学习至关重要。
- en: Since machine learning depends on large datasets, and because *training* a machine
    learning model is computationally expensive, the training part generally happens
    before deployment, with *inference* happening on the edge. It’s certainly possible
    to train machine learning models on-device, but the lack of data combined with
    the small amount of compute make it a challenge.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习依赖于大数据集，并且由于*训练*机器学习模型的计算成本高昂，训练部分通常在部署之前进行，*推理*则发生在边缘。在设备上进行机器学习模型的训练是可能的，但由于缺乏数据和有限的计算量，这是一个挑战。
- en: 'In edge AI, there are two main ways to work with machine learning datasets:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘AI中，有两种主要的方法来处理机器学习数据集：
- en: Supervised learning
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习
- en: Where the dataset has been *labeled* by an expert to assist the machine learning
    algorithm in understanding it
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已经被专家*标记*，以帮助机器学习算法理解它
- en: Unsupervised learning
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Where the algorithm identifies structures in the data without human help
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在没有人类帮助的情况下识别数据中的结构
- en: Machine learning has a major dataset-related drawback. ML algorithms depend
    entirely on their training data to know how to respond to inputs. As long as they
    are receiving inputs that are similar to their training data, they should work
    well. However, if they receive an input that is significantly dissimilar from
    their training dataset—known as an *out-of-distribution* input—they will produce
    an output that is completely useless.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习存在一个与数据集相关的主要缺点。ML算法完全依赖于它们的训练数据来知道如何响应输入。只要它们接收到与训练数据类似的输入，它们应该能够良好运作。但是，如果它们接收到一个与其训练数据显著不同的输入，即所谓的*分布外*输入，它们将产生完全无用的输出。
- en: The tricky part is that there is no obvious way of telling, from the output,
    that an input was out of distribution. This means that there’s always a risk that
    a model is providing useless predictions. Avoiding this problem is a core concern
    when working with machine learning.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 棘手的部分是从输出中没有明显的方法可以告诉一个输入是否是分布外的。这意味着模型总是存在提供无用预测的风险。在处理机器学习时，避免这个问题是一个核心关注点。
- en: There are many different types of machine learning algorithms. *Classical* machine
    learning encompasses the vast majority of them used in practice, with the major
    exception of *deep learning* (which we’ll explore in the next section).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同类型的机器学习算法。*经典*机器学习包括大多数实际应用中使用的算法，主要例外是*深度学习*（我们将在下一节中探讨）。
- en: 'Here are some of the most useful types of classical ML algorithms for edge
    AI. The title indicates whether they are supervised or unsupervised algorithms:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是边缘人工智能中最有用的几种经典机器学习算法。标题指示它们是监督还是无监督算法：
- en: Regression analysis (supervised)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 回归分析（监督）
- en: Learns the mathematical relationships between input and output to predict a
    continuous value. Easy to train, fast to run, low data requirements, and highly
    interpretable, but can only learn simple systems.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 学习输入和输出之间的数学关系以预测连续值。易于训练，运行速度快，数据需求低，高度可解释，但只能学习简单系统。
- en: Logistic regression (supervised)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归（监督）
- en: A classification-oriented type of regression analysis, logistic regression learns
    the relationship between input values and *categories* of output—for relatively
    simple systems.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种面向分类的回归分析类型，逻辑回归学习输入值与*类别*输出之间的关系——适用于相对简单的系统。
- en: Support vector machine (supervised)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（监督）
- en: Uses fancy mathematics to learn much more complex relationships than basic regression
    analysis. Low data requirements, fast to run, can learn complex systems, but difficult
    to train and low interpretability.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用复杂的数学方法学习比基本回归分析更复杂的关系。数据需求低，运行速度快，能够学习复杂系统，但训练困难且解释性较低。
- en: Decision trees and random forests (supervised)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树和随机森林（监督）
- en: Uses an iterative process to construct a series of `if` statements that predict
    an output category or value. Easy to train, fast to run, highly interpretable,
    can learn complex systems but may require a lot of training data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迭代过程构建一系列`if`语句以预测输出类别或值。易于训练，运行速度快，高度可解释，可以学习复杂系统，但可能需要大量训练数据。
- en: Kalman filter (supervised)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 卡尔曼滤波器（监督）
- en: Predicts the next datapoint given a history of measurements. Can factor in multiple
    variables to improve precision. Often trained on-device, low data requirements,
    fast to run, easy to interpret, but can only model relatively simple systems.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 预测给定测量历史的下一个数据点。可以考虑多个变量以提高精度。通常在设备上训练，数据需求低，运行速度快，易于解释，但只能模拟相对简单的系统。
- en: Nearest neighbors (unsupervised)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻（无监督）
- en: Classifies data by how similar it is to known data points. Often trained on-device,
    low data requirements, easy to interpret, but can only model relatively simple
    systems and can be slow with lots of data points.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其与已知数据点的相似性对数据进行分类。通常在设备上训练，数据需求低，易于解释，但只能模拟相对简单的系统，处理大量数据点可能会较慢。
- en: Clustering (unsupervised)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类（无监督）
- en: Learns to group inputs by similarity but does not require labels. Often trained
    on-device, low data requirements, fast to run, easy to interpret, but can only
    model relatively simple systems.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 学习通过相似性分组输入，但不需要标签。通常在设备上训练，数据需求低，运行速度快，易于解释，但只能模拟相对简单的系统。
- en: Classical ML algorithms are an incredible set of tools for interpreting the
    output of your feature engineering pipeline and making decisions with data. They
    cover the spectrum from highly efficient to highly flexible, and they can perform
    many functional tasks. Another major benefit is that they tend to be very explainable—it’s
    easy to understand how they are making their decisions. And depending on the algorithm,
    the data requirements can be quite low (deep learning typically requires very
    large datasets).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习算法是解释特征工程管道输出并基于数据做出决策的一组令人难以置信的工具。它们涵盖了从高效到高度灵活的全谱，并且可以执行许多功能性任务。另一个重要的好处是它们往往是可解释的——很容易理解它们如何做出决策。而且根据算法的不同，数据要求可以相当低（深度学习通常需要非常大的数据集）。
- en: The diverse pool of classical ML algorithms (there are literally hundreds) are
    both a blessing and a curse for edge AI. On the one hand, there are algorithms
    well suited to many different situations, which makes it possible to find one
    that is—theoretically—ideal for a particular use case. On the other hand, the
    large constellation of algorithms can be challenging to explore.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习算法的多样池（总共有数百种）对边缘人工智能来说既是一种祝福又是一种诅咒。一方面，有许多适合不同情况的算法，这使得理论上可以找到一个特定用例的理想算法。另一方面，庞大的算法集合可能会难以探索。
- en: While libraries like [scikit-learn](https://oreil.ly/EI2MV) make it easy to
    try out many different algorithms, there’s an art and a science to tuning each
    one to perform optimally, and to interpreting their results. In addition, if you’re
    hoping to deploy to a microcontroller, you may have to write your own efficient
    implementation of an algorithm—there are not many open source versions available
    yet.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像[scikit-learn](https://oreil.ly/EI2MV)这样的库使得尝试许多不同算法变得简单，但调整每个算法以实现最佳性能以及解释它们的结果既是一门艺术也是一门科学。此外，如果希望部署到微控制器，可能需要编写自己的高效算法实现——目前并没有多少开源版本可用。
- en: A major downside of classical ML algorithms is that they run into a relatively
    low ceiling in terms of complexity of the systems they can model. This means that
    to get the best results, they often have to be paired with heavy feature engineering—which
    can be complex to design and computationally costly. Even with feature engineering,
    there are some tasks—such as the classification of image data—where classical
    ML algorithms just don’t perform well.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习算法的一个主要缺点是它们在能够建模系统复杂性方面遇到了相对低的上限。这意味着为了获得最佳结果，它们通常必须与复杂的特征工程配对——这可能是设计复杂且计算成本高昂的。即使进行了特征工程，在某些任务（例如图像数据的分类）中，经典机器学习算法表现不佳。
- en: That said, classical ML algorithms are a fantastic set of tools for making on-device
    decisions. But if you hit their limitations, deep learning might help.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，经典机器学习算法是做出设备端决策的一套奇妙工具。但如果遇到它们的限制，深度学习可能会提供帮助。
- en: Deep learning
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习
- en: Deep learning is a type of machine learning that focuses on neural networks.
    These have proven such an effective tool that deep learning has grown into a gigantic
    field, with deep neural networks being applied to many types of application.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一种专注于神经网络的机器学习类型。这些网络已被证明是非常有效的工具，深度学习已发展成为一个庞大的领域，深度神经网络被应用于许多类型的应用。
- en: Note
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This book focuses on the important properties of deep learning algorithms from
    an engineering perspective. The underlying mechanics of deep learning are interesting,
    but they’re not required knowledge for building an edge AI product. Using modern
    tools, any engineer can deploy deep learning models without a formal background
    in machine learning. We’ll share some of the tools for doing that in the tutorial
    chapters later on.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 本书侧重于从工程视角讨论深度学习算法的重要特性。深度学习的基本机制很有趣，但并非构建边缘人工智能产品所必需的知识。使用现代工具，任何工程师都可以在没有机器学习正式背景的情况下部署深度学习模型。我们将在后续的教程章节中分享一些工具来实现这一点。
- en: Deep learning shares the same principles as classical ML. A dataset is used
    to train a model, which can be implemented on a device to perform inference. There
    isn’t anything magical about a model—it’s just a combination of an algorithm and
    a collection of numbers that are fed into it, along with the model’s input, in
    order to produce the desired output.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习与经典机器学习分享相同的原则。使用数据集训练模型，然后可以在设备上实现推理。模型并非神奇——它只是算法和一组数字的组合，这些数字与输入一起馈送到模型中，以产生期望的输出。
- en: The numbers in the model are called *weights*, or *parameters*, and they’re
    generated during the training process. The term *neural network* refers to the
    way that the model combines its input with its parameters, which was inspired
    by the way neurons in an animal brain connect to one another.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中的数字称为*权重*或*参数*，它们是在训练过程中生成的。术语*神经网络*指的是模型将其输入与参数结合的方式，这受到了动物大脑中神经元连接方式的启发。
- en: 'Many of the most mind-blowing feats of AI engineering that we’ve seen over
    the past decade have made use of deep learning models. Here are some popular highlights:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年中，我们见到的许多人工智能工程的最令人震惊的成就都利用了深度学习模型。以下是一些热门亮点：
- en: '[AlphaGo](https://oreil.ly/ynHNq), a computer program that used deep learning
    to beat the best players at [Go](https://oreil.ly/LZOWt), an ancient game once
    thought impossible for computers to master'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AlphaGo](https://oreil.ly/ynHNq)，一个计算机程序，使用深度学习击败了[围棋](https://oreil.ly/LZOWt)，这个曾被认为计算机无法掌握的古老游戏中的最佳选手。'
- en: '[GPT-3](https://oreil.ly/fADs3), a model that can generate written language
    that is indistinguishable from human writing'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-3](https://oreil.ly/fADs3)，一个能够生成与人类写作难以区分的书面语言的模型。'
- en: '[Fusion reactor control](https://oreil.ly/7r9_5), using deep learning to control
    the shape of plasma within a fusion reactor'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[聚变反应堆控制](https://oreil.ly/7r9_5)，使用深度学习控制聚变反应堆内等离子体的形状。'
- en: '[DALL•E](https://oreil.ly/Gw5gq), a model that can generate realistic images
    and abstract art based on text prompts'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DALL•E](https://oreil.ly/Gw5gq)，一个能够根据文本提示生成逼真图像和抽象艺术的模型。'
- en: '[GitHub Copilot](https://copilot.github.com), software that assists software
    engineers by automatically writing code'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub Copilot](https://copilot.github.com)，一种通过自动编写代码来帮助软件工程师的软件。'
- en: Beyond the fancy stuff, deep learning excels at all of the tasks in our subsections
    of algorithm types (see [“Algorithm Types by Functionality”](#algo_types_by_functionality)).
    It has proven to be flexible, adaptable, and an incredibly useful tool in allowing
    computers to understand and influence the world.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 除了花哨的东西，深度学习在我们算法类型子节中的所有任务中表现出色（见[“按功能划分的算法类型”](#algo_types_by_functionality)）。它已被证明是灵活、适应性强的，是让计算机理解和影响世界的极其有用的工具。
- en: Deep learning models are effective because they work as *universal function
    approximators*. It’s been mathematically proven that, as long as you can describe
    something as a continuous function, [a deep learning network can model it](https://oreil.ly/4xX1m).
    This basically means that for any dataset that shows various inputs and desired
    outputs, there’s a deep learning model out there that can convert one into the
    other.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型之所以有效，是因为它们作为*通用函数逼近器*。数学上已经证明，只要你能将某事描述为连续函数，[深度学习网络就可以对其建模](https://oreil.ly/4xX1m)。这基本上意味着，对于任何显示各种输入和期望输出的数据集，都有一个深度学习模型可以将一个转换为另一个。
- en: A really exciting result of this ability is that during training, deep learning
    models can figure out how to do their own feature engineering. If a special transformation
    is needed to help interpret the data, a deep learning model can potentially learn
    how to do it. This doesn’t make feature engineering obsolete, but it definitely
    reduces the burden on the developer to get things exactly right.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能力的一个非常令人兴奋的结果是，在训练过程中，深度学习模型可以找出如何进行自己的特征工程。如果需要特殊的转换来帮助解释数据，深度学习模型可以潜在地学会如何做到这一点。这并不是说特征工程变得过时了，但它确实减轻了开发人员确保一切完全正确的负担。
- en: The reason deep learning models are so good at approximating functions is that
    they can have very large numbers of parameters. With each parameter, the model
    gets a little bit more flexibility, allowing it to describe a slightly more complex
    function.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型之所以如此擅长逼近函数，是因为它们可以拥有非常大量的参数。每个参数使得模型具有更多的灵活性，从而可以描述稍微复杂一些的函数。
- en: This property leads to the two major drawbacks of deep learning models. First,
    finding the ideal values for all of these parameters is a difficult process. It
    involves training a model with lots of data. Data is often a rare and precious
    resource, difficult and expensive to obtain, so this can be a major obstacle.
    Fortunately, there are many techniques that can help make the most of limited
    data—we’ll cover them later in the book.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这种属性导致了深度学习模型的两个主要缺点。首先，找到所有这些参数的理想值是一个困难的过程。它涉及用大量数据训练模型。数据通常是一种稀有和宝贵的资源，难以获取且昂贵，因此这可能是一个主要障碍。幸运的是，有许多技术可以帮助充分利用有限的数据——我们将在本书的后面部分涵盖它们。
- en: The second major drawback is the risk of *overfitting*. Overfitting is when
    a machine learning model learns a dataset *too* well. Instead of modeling the
    general rules that lead from outputs to inputs in its dataset, it memorizes the
    dataset completely. This means that it won’t perform well on data that it hasn’t
    seen before.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个主要缺点是*过拟合*的风险。过拟合是指机器学习模型对数据集学习得*过于*彻底。它完全记忆了数据集，而不是对导致数据输出的一般规则进行建模。这意味着它在未曾见过的数据上表现不佳。
- en: Overfitting is a risk with all machine learning models, but it’s especially
    a challenge for deep learning models because they can have so many parameters.
    Each additional parameter provides the model with slightly more ability to memorize
    its dataset.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是所有机器学习模型面临的风险，但对于深度学习模型来说尤为严峻，因为它们可以拥有如此多的参数。每增加一个参数都会使模型稍微具备更多的记忆数据集的能力。
- en: 'There are a lot of different types of deep learning models. Here are some of
    the most important for edge AI:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同类型的深度学习模型。以下是一些在边缘AI中最重要的模型：
- en: Fully connected models
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接模型
- en: The simplest type of deep learning model, fully connected models consist of
    stacked *layers* of *neurons*. The input of a fully connected model is fed directly
    in as a long series of numbers. Fully connected models are capable of learning
    any function, but they are mostly blind to spatial relationships in their inputs
    (for example, which values in an input are next to one another).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的深度学习模型类型是全连接模型，由堆叠的*神经元*层组成。全连接模型的输入直接作为一长串数字进行馈送。全连接模型能够学习任何函数，但对其输入中的空间关系（例如，哪些输入值相邻）大多视而不见。
- en: In an embedded context, this means they work well for discrete values (for example,
    if the input features are a set of statistics about a time series) but they aren’t
    as great with raw time series or image data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入式环境中，这意味着它们对离散值非常有效（例如，如果输入特征是关于时间序列的一组统计数据），但对原始时间序列或图像数据的处理效果则不佳。
- en: Fully connected models are very well supported on embedded devices, with hardware
    and software optimizations commonly available.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接模型在嵌入式设备上得到了很好的支持，通常可以使用硬件和软件优化。
- en: Convolutional models
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积模型
- en: Convolutional models are designed to make use of the spatial information in
    their inputs. For example, they can learn to recognize shapes in images, or the
    structures of signals within time series sensor data. This makes them extremely
    useful in embedded applications since spatial information is important in so many
    of the signals we deal with.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积模型旨在利用其输入中的空间信息。例如，它们可以学习识别图像中的形状或时间序列传感器数据中的结构。这使得它们在嵌入式应用中非常有用，因为空间信息在我们处理的许多信号中都很重要。
- en: Like fully connected models, convolutional models are very well supported on
    embedded devices.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 像全连接模型一样，卷积模型在嵌入式设备上得到了很好的支持。
- en: Sequence models
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 序列模型
- en: Sequence models were designed originally for use on sequences of data, like
    time series signals or even written language. To help them recognize long-term
    patterns in time series, they often include some internal “memory.”
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 序列模型最初是为处理数据序列而设计的，比如时间序列信号或甚至书面语言。为了帮助它们识别时间序列中的长期模式，它们通常包含一些内部的“记忆”。
- en: It turns out that sequence models are very flexible, and there’s increasing
    evidence that they can be very effective on any signal where spatial information
    is important. Many people believe they will eventually take over from convolutional
    models.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，序列模型非常灵活，越来越多的证据表明它们在任何需要空间信息重要的信号上都可以非常有效。许多人认为它们最终会取代卷积模型。
- en: Sequence models are currently less well supported than convolutional and fully
    connected models on embedded devices; there are few open source libraries that
    provide optimized implementations for them. This is more due to inertia than technical
    limitations, so the situation is likely to change over the next couple of years.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，在嵌入式设备上，序列模型的支持程度比卷积模型和全连接模型要差一些；提供优化实现的开源库也比较少。这更多是由于惯性而非技术限制，因此这种情况可能会在接下来的几年内发生变化。
- en: Embedding models
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型
- en: 'An embedding model is a pretrained deep learning model that is designed for
    dimensionality reduction—it takes a big, messy input and represents it as a smaller
    set of numbers that describes it within a certain context. They are used in the
    same way a signal processing algorithm would be: they produce features that can
    be interpreted by another ML model.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型是预训练的深度学习模型，旨在进行维度降低——它将一个复杂的输入表示为一组更小的数字，以在特定上下文中描述它。它们的使用方式类似于信号处理算法：它们生成可以被另一个机器学习模型解释的特征。
- en: Embedding models are available for many tasks, from image processing (turning
    a big messy image into a numeric description of its contents) to speech recognition
    (turning raw audio into a numeric description of the vocal sounds within it).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多任务，如图像处理（将混乱的大图像转换为其内容的数值描述）或语音识别（将原始音频转换为其内部语音声音的数值描述），都可以使用嵌入模型。
- en: The most common use for embedding models is *transfer learning*, which is a
    way of reducing the amount of data required to train a model. We’ll learn more
    about that later.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型最常见的用途是*迁移学习*，这是一种减少训练模型所需数据量的方法。我们稍后会详细了解这个概念。
- en: Embedding models can be fully connected, convolutional, or sequence models,
    so their support on embedded devices varies—but convolutional embedding models
    are the most common.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型可以是完全连接的、卷积的或序列模型，因此它们在嵌入式设备上的支持有所不同，但卷积嵌入模型最为常见。
- en: It’s only in recent years that deep learning models have been brought to edge
    AI hardware. Since they are often large and involve significant computation to
    run, it’s been the advent of high-end MCUs and SoCs with relatively powerful processors
    and large amounts of ROM and RAM that have enabled them to make the leap.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习模型才被引入到边缘AI硬件中。由于它们通常体积大且需要大量计算资源才能运行，高端MCU和SoC的出现，具有相对强大的处理器和大量的ROM和RAM，使它们能够迈出这一步。
- en: It’s possible to run a small deep learning model using just a few kilobytes
    of memory, but for models that do more complex things—from audio classification
    to object detection—it is common for models to require dozens or hundreds of kilobytes
    as a minimum.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能只用几千字节的内存运行一个小型深度学习模型，但对于执行更复杂任务的模型——从音频分类到物体检测——通常需要数十到数百千字节作为最小要求。
- en: This is already impressive since traditional server-side machine learning models
    can be anywhere from tens of megabytes to several terabytes in size. Using clever
    optimization, and by limiting scope, embedded models can be made much smaller—we’ll
    introduce some of these techniques shortly.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经令人印象深刻了，因为传统的服务器端机器学习模型大小从几十兆字节到数百兆字节不等。通过巧妙的优化和限制范围，嵌入模型可以做得更小——我们很快将介绍其中的一些技术。
- en: 'There are various ways to run a deep learning model on an embedded device.
    Here’s a quick summary:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方式可以在嵌入设备上运行深度学习模型。以下是一个快速总结：
- en: Interpreters
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 解释器
- en: Deep learning interpreters, like [TensorFlow Lite for Microcontrollers](https://oreil.ly/4Q7xN),
    use an interpreter to execute a model that is stored as a file. They are flexible
    and easy to work with, but they come with some computational and memory overhead,
    and they don’t support every type of model.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习解释器，例如[TensorFlow Lite for Microcontrollers](https://oreil.ly/4Q7xN)，使用解释器执行存储为文件的模型。它们灵活且易于使用，但会带来一些计算和内存开销，并且不支持每一种模型类型。
- en: Code generation
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成
- en: Code generation tools, like [EON](https://oreil.ly/SmT-s), take a trained deep
    learning model and translate it into optimized embedded source code. This is more
    efficient than an interpreter-based approach, and the code is human-readable so
    it can still be debugged, but it still doesn’t support every possible model type.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成工具，例如[EON](https://oreil.ly/SmT-s)，将训练好的深度学习模型转换为优化的嵌入式源代码。这比基于解释器的方法更高效，且代码易于阅读，因此仍然可以进行调试，但仍不支持每种可能的模型类型。
- en: Compilers
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器
- en: Deep learning compilers, like [microTVM](https://oreil.ly/0JTaR), take a trained
    model and generate optimized bytecode that can be included into embedded applications.
    The implementation they generate can be highly efficient, but it’s not as easy
    to debug and maintain as actual source code. They can support model types not
    explicitly supported by interpreters and code generation. It’s common for embedded
    hardware vendors to provide custom interpreters or compilers to assist with running
    deep learning models on their hardware.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[microTVM](https://oreil.ly/0JTaR)的深度学习编译器接收训练好的模型，并生成优化的字节码，可以嵌入到嵌入式应用程序中。它们生成的实现可以非常高效，但不像实际的源代码那样易于调试和维护。它们可以支持解释器和代码生成未明确支持的模型类型。嵌入式硬件供应商通常会提供定制的解释器或编译器，以协助在其硬件上运行深度学习模型。
- en: Handcoding
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 手工编码
- en: It’s possible to implement a deep learning network by writing code by hand,
    incorporating the parameter values from a trained model. This is a difficult and
    time-consuming process, but it allows full control over optimization and allows
    you to support any model type.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过手工编写代码实现深度学习网络，将来自训练模型的参数值纳入其中。这是一个困难且耗时的过程，但可以完全控制优化，并支持任何模型类型。
- en: The environment for deploying deep learning models is very different between
    SoCs and microcontrollers. Since SoCs run full, modern operating systems, they
    also support most of the tools that are used to run deep learning models on servers.
    This means that pretty much any type of model will run on a Linux SoC. That said,
    the latency of the model will vary depending on the architecture of the model
    and the SoC’s processor.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在SoCs和微控制器之间，部署深度学习模型的环境差异很大。由于SoCs运行完整的现代操作系统，它们也支持大多数用于在服务器上运行深度学习模型的工具。这意味着几乎任何类型的模型都可以在Linux
    SoC上运行。尽管如此，模型的延迟会根据模型的架构和SoC的处理器而异。
- en: There are also interpreters designed specifically for SoC devices. For example,
    [TensorFlow Lite](https://oreil.ly/pNs5W) provides tools that allow deep learning
    models to be run more efficiently on SoCs—typically those that are used in smartphones.
    They include optimized implementations of deep learning operations that make use
    of features available in some SoCs, such as GPUs.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 还有专为SoC设备设计的解释器。例如，[TensorFlow Lite](https://oreil.ly/pNs5W)提供了工具，允许在SoCs上更高效地运行深度学习模型——通常是在智能手机上使用的SoCs。它们包括优化的深度学习操作实现，利用了某些SoCs可用的功能，如GPU。
- en: The SoCs that have integrated deep learning accelerators are a special case.
    Typically, the hardware vendor will provide a special compiler or interpreter
    that allows the model to make use of hardware acceleration. Accelerators typically
    only accelerate certain operations, so the amount of speedup depends on the architecture
    of the model.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 集成了深度学习加速器的SoC是一个特殊情况。通常，硬件供应商会提供特殊的编译器或解释器，允许模型利用硬件加速功能。加速器通常只加速特定操作，因此加速比取决于模型的架构。
- en: Since microcontrollers don’t run full operating systems, the standard tools
    for running deep learning models aren’t available. Instead, frameworks like TensorFlow
    Lite for Microcontrollers provide a baseline of model support. They tend to lag
    behind the standard tools a little in terms of operator support, meaning they
    will not run some model architectures.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 由于微控制器不运行完整的操作系统，无法使用标准工具来运行深度学习模型。相反，像TensorFlow Lite for Microcontrollers这样的框架提供了模型支持的基准。它们在操作符支持方面通常落后于标准工具，意味着它们无法运行某些模型架构。
- en: Operators and Kernels
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运算符和内核
- en: In edge machine learning, an *operator*, or *kernel*, is an implementation of
    a particular mathematical operation used to run a deep learning model. These are
    overloaded terms with different meanings in other fields, including elsewhere
    in deep learning.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘机器学习中，*运算符*或*内核*是用于运行深度学习模型的特定数学操作的实现。这些术语在其他领域，包括深度学习的其他地方，也有不同的含义。
- en: Typical high-end microcontrollers have hardware features such as SIMD instructions
    that will drastically improve the performance of deep learning models. TensorFlow
    Lite for Microcontrollers includes optimized implementations of operators, making
    use of these instructions, for several vendors. Like with SoCs, the vendors of
    microcontroller-based hardware accelerators often provide custom compilers or
    interpreters that allow models to run on their hardware.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的高端微控制器具有诸如 SIMD 指令之类的硬件特性，这将显著提高深度学习模型的性能。TensorFlow Lite for Microcontrollers
    包括多个供应商优化实现的操作符，利用这些指令。与 SoCs 一样，基于微控制器的硬件加速器的供应商通常提供自定义编译器或解释器，允许模型在其硬件上运行。
- en: The core advantages of deep learning are its flexibility, reduced requirements
    for feature engineering, and ability to make use of large amounts of data due
    to the high parameter counts of models. Deep learning is noteworthy for its ability
    to approximate complex systems, going beyond simple prediction to perform tasks
    such as generating art and accurately recognizing objects in images. Deep learning
    provides a lot of freedom, and researchers have only just begun to explore its
    potential.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的核心优势包括其灵活性、减少特征工程的要求以及利用大量数据的能力，因为模型的高参数数量。深度学习以其逼近复杂系统的能力而著称，不仅能进行简单预测，还能执行生成艺术和准确识别图像中物体等任务。深度学习提供了很大的自由度，研究人员仅仅开始探索其潜力。
- en: The core disadvantages are its high data requirements, its propensity toward
    overfitting, the relatively large size and computational complexity of deep learning
    models, and the complexity of the training process. Additionally, deep learning
    models can be hard to interpret—it can be challenging to explain why they make
    one prediction over another. That said, there are tools and techniques that help
    mitigate most of these drawbacks.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的核心缺点包括高数据需求、易于过拟合的倾向、深度学习模型的相对较大尺寸和计算复杂性，以及训练过程的复杂性。此外，深度学习模型往往难以解释——解释为何它们对一个预测优于另一个预测可能具有挑战性。尽管如此，有工具和技术可以帮助减轻大部分这些缺点。
- en: Combining algorithms
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合算法
- en: 'A single edge AI application can make use of multiple different types of algorithms.
    Here are some typical ways this is done:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 单个边缘 AI 应用可以利用多种不同类型的算法。以下是一些典型的使用方式：
- en: Ensembles
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 集成
- en: An *ensemble* is a collection of machine learning models that are fed the same
    input. Their outputs are combined mathematically in order to make a decision.
    Since every ML model has its own strengths and weaknesses, an ensemble of models
    is often more accurate together than its constituent parts. The downside of ensembles
    is the additional complexity, memory, and compute required to store and run multiple
    models.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*集成*是一组机器学习模型，它们接收相同的输入。它们的输出在数学上结合以做出决策。由于每个 ML 模型都有其自身的优势和劣势，模型的集成通常比其组成部分更精确。集成的缺点是额外的复杂性、内存和计算资源，以存储和运行多个模型。'
- en: Cascades
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 级联
- en: A *cascade* is a set of ML models that are run in sequence. For example, in
    a cellphone with a built-in digital assistant, a small, lightweight model is run
    constantly to detect any signs of human speech. Once speech is detected, a larger,
    more computationally expensive model is woken up in order to determine what was
    said.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '*级联*是一组依次运行的 ML 模型。例如，在装有内置数字助理的手机中，会不断运行一个小型、轻量级模型以检测任何人类语音的迹象。一旦检测到语音，会唤醒一个更大、计算代价更高的模型，以确定说了什么。'
- en: Cascades are a great way of saving energy since they allow you to avoid unnecessary
    computation. In a heterogeneous compute environment, where multiple types of processor
    are available, the individual components of a cascade can even be run on different
    processors.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 级联是节省能量的好方法，因为它们可以避免不必要的计算。在异构计算环境中，可以在不同处理器上运行级联的各个组成部分。
- en: Feature extractors
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取器
- en: As we learned earlier, embedding models take a high-dimensional input, like
    an image, and distill it down to a set of numbers that describe its content. The
    output of an embedding model can be fed into another model, designed to make predictions
    based on what the embedding model describes about the original input. In this
    case, the embedding model is being used as a *feature extractor*.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所学到的，嵌入模型接受高维输入（如图像），并将其精炼为一组描述其内容的数字。嵌入模型的输出可以馈送到另一个模型中，该模型设计用于根据嵌入模型对原始输入的描述进行预测。在这种情况下，嵌入模型被用作*特征提取器*。
- en: If a pretrained embedding model is used, this technique—known as *transfer learning*—can
    massively reduce the amount of data required to train a model. Instead of learning
    how to interpret the original high-dimensional input, the model only needs to
    learn how to interpret the simple output returned by the feature extractor.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用预训练的嵌入模型，这种技术——称为*迁移学习*——可以大幅减少训练模型所需的数据量。模型不需要学习如何解释原始的高维输入，而只需要学习如何解释特征提取器返回的简单输出。
- en: For example, imagine you wish to train a model to identify different species
    of birds from photographs. Rather than train an entire model from scratch, you
    could use the output of a pretrained feature extractor as the input to your model.
    This could reduce the amount of data and training time required in order to get
    good results.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你希望训练一个模型，从照片中识别不同物种的鸟类。与其从头开始训练一个完整的模型，你可以使用预训练的特征提取器的输出作为你的模型的输入。这可以减少所需的数据量和训练时间，以获得良好的结果。
- en: Many pretrained deep learning feature extractors are available under open source
    licenses. They are commonly used for image related tasks, since large public image
    datasets are available for pretraining.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 许多预训练的深度学习特征提取器都可以在开源许可证下获得。它们通常用于与图像相关的任务，因为可以预训练大型公共图像数据集。
- en: Multimodal models
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 多模型
- en: A *multimodal model* is a single model that takes inputs of multiple types of
    data simultaneously. For example, a multimodal model might accept both audio and
    accelerometer data together. This technique can be used as a mechanism for sensor
    fusion, using a single model to combine disparate data types.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '*多模型*是一个单一模型，同时接受多种类型的数据输入。例如，多模型可以同时接受音频和加速度计数据。这种技术可以用作传感器融合的机制，使用单一模型来结合不同的数据类型。'
- en: Postprocessing algorithms
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后处理算法
- en: On edge AI devices, we typically work with streams of data—for example, a continuous
    time series of audio data. When we run an edge AI algorithm on that stream of
    data, it will produce a second time series that represents the outputs of the
    algorithm over time.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘 AI 设备上，我们通常处理数据流——例如，连续的音频数据时间序列。当我们在这些数据流上运行边缘 AI 算法时，它将产生第二个时间序列，表示随时间变化的算法输出。
- en: This poses a problem. How do we interpret this second time series in order to
    decide? For example, imagine we are analyzing audio to detect when somebody says
    a keyword so that we can trigger some functionality on a product. What we *really*
    want to know is when did we hear the keyword?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这带来了一个问题。我们如何解释这第二个时间序列以做出决策呢？例如，想象我们正在分析音频以检测某人何时说出关键字，以便在产品上触发某些功能。我们*真正想知道的是我们何时听到了这个关键字。
- en: Unfortunately, the time series of inference results is not ideal for this purpose.
    First, it contains many events that do not represent a keyword being detected.
    To clean these up, we can ignore any whose confidence that a keyword was spotted
    is below a certain threshold.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，推理结果的时间序列对于此目的并不理想。首先，它包含许多并不代表检测到关键字的事件。为了清理这些数据，我们可以忽略那些其关键字被检测到的置信度低于某个阈值的事件。
- en: Second, the model may occasionally (and briefly) detect a keyword when a keyword
    was not actually spoken. We need to filter out these blips to clean up our output.
    This is equivalent to running a low-pass filter on the time series.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，模型偶尔（而且短暂地）可能会检测到一个并非实际被说出来的关键字。我们需要过滤掉这些突发事件，以清理我们的输出。这相当于在时间序列上运行低通滤波器。
- en: Finally, instead of telling us *each* time the keyword was spoken, the raw time
    series tells us at a set rate whether the keyword is *currently* being spoken.
    This means we need to do some output gating to get the information we really want.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，与其告诉我们*每一次*关键字何时被说出来，原始时间序列告诉我们的是以一定的频率告诉我们当前是否正在说出关键字。这意味着我们需要进行一些输出门控以获取我们真正想要的信息。
- en: After cleaning up the raw output, we now have a signal that tells us when a
    keyword was actually spotted. This is something we can use in our application
    logic to control our device.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 清理完原始输出后，我们现在有一个信号告诉我们关键字何时实际被检测到。这是我们可以在应用逻辑中使用的东西，用来控制我们的设备。
- en: This sort of postprocessing is extremely common in edge AI applications. The
    exact postprocessing algorithm used, and its particular parameters—for example,
    the threshold for considering something a match—can be determined on a case-by-case
    basis. Tools like Edge Impulse’s Performance Calibration (covered in [“Performance
    Calibration”](ch10.html#performance_calibration)) allow developers to automate
    discovery of the ideal postprocessing algorithm for their application.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘人工智能应用中，这种后处理非常常见。可以根据具体情况确定使用的后处理算法及其特定参数，例如用于考虑匹配的阈值。像Edge Impulse的性能校准（在[“性能校准”](ch10.html#performance_calibration)中有介绍）这样的工具，允许开发人员自动发现适合其应用程序的理想后处理算法。
- en: Fail-safe design
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 失效安全设计
- en: There are many things that can go wrong with an edge AI application, so it’s
    critical that there are always safeguards in place to protect against unexpected
    issues.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘人工智能应用中可能会出现许多问题，因此必须始终采取措施来防止意外问题的发生。
- en: For example, imagine a wildlife camera that uses a deep learning model to identify
    when an animal of interest has been photographed and uploads the animal’s image
    via a satellite connection. Under normal operation, it may send a few photographs
    a day—not costing very much in data fees.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一个野生动物摄像机，它使用深度学习模型来识别感兴趣动物的照片，并通过卫星连接上传动物的图像。在正常操作下，它可能每天发送几张照片，数据费用不高。
- en: But out in the field, a physical problem with the camera hardware—such as dirt
    or reflections on the lens—might result in images being taken that are very different
    from those in the original training dataset. These out-of-distribution images
    could lead to unspecified behavior from the deep learning model—which could mean
    that the model begins to constantly report that the animal of interest is present.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在现场，摄像机硬件可能存在物理问题，例如镜头上的污垢或反射，可能导致拍摄的图像与原始训练数据集中的图像非常不同。这些分布外图像可能会导致深度学习模型的未指定行为，这可能意味着模型开始持续报告感兴趣的动物存在。
- en: These false positives, caused by out-of-distribution inputs, might result in
    hundreds of images being uploaded via satellite connection. Not only would the
    camera be rendered useless, but it could potentially cost large amounts in data
    transfer fees.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这些由于分布外输入引起的误报可能导致通过卫星连接上传数百张图像。这不仅会使摄像机变得无用，而且可能导致大量的数据传输费用。
- en: In real-world applications, there’s no way to avoid things like damage to sensors
    or unexpected behavior from algorithms. Instead, it’s important that you design
    your application to be fail-safe. This means that if part of the system were to
    fail, the application would minimize harm.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的应用中，无法避免像传感器损坏或算法的意外行为等问题。因此，设计应用程序以具备失效安全性至关重要。这意味着如果系统的一部分出现故障，应用程序将最小化损害。
- en: The best way to do this varies between situations. In the case of a wildlife
    camera, it could be wise to build in a rate limit that kicks in if an unreasonable
    number of photographs are being uploaded. In another application, you might shut
    a system down entirely rather than risk harm being caused.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同情况下，最佳方法有所不同。例如，对于野生动物摄像机，建议设置一个速率限制，以防止上传过多照片。在其他应用中，你可能选择完全关闭系统，而不是冒着可能引起损害的风险。
- en: Building fail-safe applications is an important part of responsible AI—and good
    engineering in general. It is something to think about from the very beginning
    of any project.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 构建失效安全的应用程序是负责任的人工智能和良好工程的重要组成部分。这是任何项目开始时都需要考虑的事项。
- en: Optimization for Edge Devices
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 边缘设备的优化
- en: With machine learning models, and particularly deep learning models, there’s
    often a trade-off between how well a model performs its task and how much memory
    and compute the model requires.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习模型，特别是深度学习模型，通常存在着模型执行任务效果与内存和计算资源消耗之间的权衡。
- en: This trade-off is extremely important for edge AI. Edge devices are typically
    computationally constrained. They are designed to minimize cost and energy usage,
    not to maximize compute. At the same time, they are expected to deal with real-time
    sensor data, often at high frequencies, and potentially react in real time to
    events in the data stream.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这种权衡对于边缘人工智能至关重要。边缘设备通常具有计算能力的限制。它们设计用于最小化成本和能源使用，而不是最大化计算能力。同时，它们需要处理实时传感器数据，通常是高频率的，并可能根据数据流中的事件实时作出反应。
- en: Larger machine learning models tend to be better at complex tasks, since they
    have more capacity—which is helpful for learning complicated relationships between
    inputs and outputs. This extra capacity means they may require more ROM and RAM,
    and it also means they take longer to compute. The additional compute time results
    in higher power consumption, as we’ll learn in [“Duty cycle”](ch10.html#duty_cycle).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的机器学习模型往往在复杂任务上表现更好，因为它们有更大的容量——这对学习输入和输出之间复杂关系是有帮助的。这种额外的容量意味着它们可能需要更多的ROM和RAM，并且意味着计算时间更长。额外的计算时间导致更高的功耗，正如我们将在[“工作循环”](ch10.html#duty_cycle)中学到的那样。
- en: Finding the correct balance between *task performance* and *computational performance*
    is essential in any application. It’s a matter of juggling constraints. On the
    one hand, there’s a minimum standard for performance at a given task. On the other
    hand, hardware choices create hard limits on available memory, latency, and energy.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何应用程序中，找到正确的*任务性能*和*计算性能*之间的平衡是至关重要的。这是一个权衡约束条件的问题。一方面，在给定任务中有性能的最低标准。另一方面，硬件选择会对可用内存、延迟和能耗造成硬性限制。
- en: Managing this trade-off is one of the difficult—but fascinating—parts of edge
    AI development. It’s part of what makes the field uniquely interesting, and why
    tools for things like AutoML (which we’ll learn about in [“Automated machine learning
    (AutoML)”](ch05.html#automl_tools)) need to be redesigned for edge AI.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 管理这种权衡是边缘AI开发中最困难但也是最吸引人的部分之一。这是使这一领域独特有趣的一部分，也是为边缘AI重新设计自动机器学习工具（我们将在[“自动化机器学习（AutoML）”](ch05.html#automl_tools)中学习）的原因之一。
- en: Here are some of the factors that can help us minimize compute requirements
    while maximizing task performance.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可以帮助我们在尽可能减少计算要求的同时最大化任务性能的因素之一。
- en: Choice of algorithm
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法的选择
- en: Every edge AI algorithm has a slightly different profile of memory usage and
    computational complexity. The constraints of your target hardware should inform
    your choice of algorithm. Typically, classical ML algorithms are smaller and more
    efficient than deep learning algorithms.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 每个边缘AI算法的内存使用和计算复杂性都有稍微不同的特征。您目标硬件的约束条件应该指导您选择算法。通常，经典机器学习算法比深度学习算法更小更高效。
- en: However, it’s commonly the case that feature engineering algorithms use vastly
    more compute than either, making the choice between classical ML and deep learning
    less significant. The exception to this rule is the analysis of image data, which
    typically requires little feature engineering but relatively large deep learning
    models.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常情况下，特征工程算法的计算量远远超过二者，使得在经典机器学习和深度学习之间的选择变得不那么重要。这个规则的例外是图像数据的分析，这通常需要较少的特征工程，但相对较大的深度学习模型。
- en: 'Here are some common ways to reduce the latency and memory required by your
    choice of algorithms:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是减少算法选择所需的延迟和内存的常见方法：
- en: Reduce the complexity of feature engineering. More math means higher latency.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少特征工程的复杂性。更多的数学意味着更高的延迟。
- en: Reduce the amount of data that reaches the AI algorithm.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少到达AI算法的数据量。
- en: Use classical ML instead of deep learning.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用经典机器学习而不是深度学习。
- en: Trade complexity between feature engineering and machine learning model, depending
    on which runs more efficiently on your device.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的设备上，根据哪个运行效率更高，特征工程和机器学习模型之间的复杂性权衡。
- en: Reduce the size (the number of weights and layers) of deep learning models.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少深度学习模型的大小（权重和层数）。
- en: Choose model types that have accelerator support on your device of choice.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择在您选择的设备上具有加速器支持的模型类型。
- en: Compression and optimization
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 压缩和优化
- en: 'There are many optimization techniques designed to reduce the amount of data
    and computation required by a given algorithm. Here are some of the most important
    types:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多优化技术旨在减少给定算法所需的数据量和计算量。以下是一些最重要的类型：
- en: Quantization
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 量化
- en: One way to reduce the amount of memory and computation required by an algorithm
    or model is to decrease the precision of its numeric representations. As mentioned
    in [“How Are Values Represented?”](ch03.html#numeric_types), there are many different
    ways to represent numbers in computation—some that have more precision than others.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 减少算法或模型所需的内存和计算量的一种方法是降低其数值表示的精度。如在[“数值类型是如何表示的？”](ch03.html#numeric_types)中提到的，有许多不同的计算数值表示方式——有些比其他更精确。
- en: Quantization is the process of taking a set of values and reducing their precision
    while preserving the important information they contain. It can be done for both
    signal processing algorithms and ML models. It’s especially useful for deep learning
    models, which by default tend to have 32-bit floating-point weights. By reducing
    the weights to 8-bit integers you can reduce a model to 1/4th its size—typically
    without much reduction in accuracy.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是将一组值的精度降低而保留其重要信息的过程。它既可以用于信号处理算法，也可以用于机器学习模型。对于默认具有32位浮点权重的深度学习模型而言，这特别有用。通过将权重减少到8位整数，可以将模型减少到原来的四分之一大小，通常不会显著降低准确性。
- en: Another advantage of quantization is that the code to perform integer math is
    faster and more portable than the code for floating-point math. This means that
    quantization results in a significant speedup on many devices, and that quantized
    algorithms will run on devices that lack floating-point units.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 量化的另一个优势是，执行整数运算的代码比执行浮点运算的代码更快、更便携。这意味着量化在许多设备上可以显著加快速度，并且量化算法可以在缺乏浮点单元的设备上运行。
- en: Quantization is a lossy optimization, meaning that it typically reduces the
    task performance of the algorithm. In ML models, this can be mitigated by training
    at a lower precision so that the model learns to compensate.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种有损优化，通常会降低算法的任务性能。在机器学习模型中，可以通过以较低精度进行训练来减少这种影响，从而使模型学会进行补偿。
- en: Operator fusion
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 操作融合
- en: In operator fusion, a computation-aware algorithm is used to inspect the operators
    that are used when a deep learning model is run. When certain groups of operators
    are used together, it’s possible to replace them with a single *fused* implementation
    that has been written to maximize computational efficiency.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作融合中，使用计算感知算法检查深度学习模型运行时使用的运算符。当某些运算符组合在一起使用时，可以用单个*融合*实现替换它们，以最大化计算效率。
- en: 'Operator fusion is a lossless technique: it improves computational performance
    without causing any reduction in task performance. The downside is that fused
    implementations are only available for certain combinations of operators, so its
    impact depends greatly on the architecture of a model.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 操作融合是一种无损技术：它提高了计算性能，而不会降低任务性能。缺点是融合实现仅适用于某些运算符的特定组合，因此其影响在很大程度上取决于模型的架构。
- en: Pruning
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪
- en: Pruning is a lossy technique applied during the training of a deep learning
    model. It forces many of the model’s weights to have a value of zero, creating
    what is known as a *sparse* model. In theory, this should allow for faster computation
    since any multiplication involving a zero weight will invariably result in a zero.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪是在训练深度学习模型期间应用的一种有损技术。它强制模型的许多权重为零，从而创建所谓的*稀疏*模型。理论上，这应该能够加快计算速度，因为任何涉及零权重的乘法将不可避免地得到零。
- en: However, at this point in time there is very little edge AI hardware and software
    designed to take advantage of sparse weights. This will change over the next few
    years, but for now the main benefit of pruning is that sparse models are easier
    to compress, due to their large blocks of identical values. This is helpful when
    models need to be sent over the air.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前很少有边缘人工智能硬件和软件设计能够利用稀疏权重。未来几年这种情况可能会改变，但目前修剪的主要优势在于稀疏模型更容易压缩，因为它们包含大量相同值的块。这在需要通过空中传送模型时非常有用。
- en: Knowledge distillation
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: Knowledge distillation is another lossy deep learning training technique that
    enables a large “teacher” model to help train a smaller “student” model to reproduce
    its functionality. It takes advantage of the fact that there is typically a lot
    of redundancy in the weights of a deep learning model, meaning that it’s possible
    to find an equivalent model that is smaller but performs almost as well.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏是另一种有损深度学习训练技术，它使得一个大型的“教师”模型能够帮助训练一个更小的“学生”模型以复制其功能。它利用了深度学习模型的权重通常存在大量冗余的事实，这意味着可以找到一个更小但几乎同样有效的等效模型。
- en: Knowledge distillation is a bit fiddly, so it’s not yet a common technique—but
    it’s likely to become a best practice over the next few years.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏有些繁琐，因此还不是常见的技术，但在未来几年可能会成为最佳实践。
- en: Binary neural networks (BNNs)
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制神经网络（BNNs）
- en: BNNs are deep learning models where every weight is a single binary number.
    Since binary arithmetic is extremely fast on computers, binary neural networks
    can be very efficient to run. However, they are a relatively new technology and
    the tooling for training and running inference with them is not yet in broad use.
    Binarization is similar to quantization and is therefore a lossy technique.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: BNNs是深度学习模型，其中每个权重都是一个单一的二进制数。由于计算机上的二进制算术非常快速，二进制神经网络可以运行得非常高效。然而，它们是一种相对较新的技术，用于训练和推断的工具尚未广泛使用。二进制化类似于量化，因此是一种有损技术。
- en: Spiking neural networks (SNNs)
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 脉冲神经网络（SNNs）
- en: A spiking neural network is an artificial neural network where the signals transmitted
    through the network have a time component. As “neuromorphic” systems, they are
    designed to more closely resemble the way biological neurons work. They have different
    trade-offs compared to traditional deep learning models, offering improved performance
    and efficiency for some tasks. However, they require specialized hardware (in
    the form of an accelerator) in order to offer a benefit.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 脉冲神经网络是一种人工神经网络，其中网络传输的信号具有时间组件。作为“神经形态”系统，它们设计得更接近生物神经元的工作方式。它们与传统深度学习模型相比有不同的权衡，对某些任务提供了改进的性能和效率。然而，它们需要专门的硬件（加速器形式），以便提供优势。
- en: SNNs can either be trained directly or be created from a traditional deep learning
    model in a conversion process. This process may be lossy.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: SNNs可以直接训练，也可以通过转换过程从传统深度学习模型创建。这个过程可能是有损的。
- en: Model compression has two major downsides. The first is that running compressed
    models often requires specific software, hardware, or a combination of the two.
    This can limit the devices that a compressed model can be deployed to.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩有两个主要的缺点。第一个是运行压缩模型通常需要特定的软件、硬件或二者的组合。这可能限制可以部署压缩模型的设备范围。
- en: The second downside is more dangerous. The lossy nature of compression often
    results in a subtle degradation of a model’s predictive performance that can be
    difficult to spot. The reduction in precision can bias a model to perform well
    in common cases, but to lose performance in the “long tail” of less frequently
    encountered inputs.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个缺点更为危险。压缩的有损性质通常导致模型预测性能的微妙下降，很难察觉。精度的降低可能会导致模型在常见情况下表现良好，但在“长尾”少见输入的情况下失去性能。
- en: This problem can amplify the biases inherent in datasets and algorithms. For
    example, if a dataset collected for training an ML-powered health wearable contains
    fewer examples from people in minority groups, model compression may lead to degraded
    performance for people in these groups. Since they are a minority, the impact
    on the model’s overall accuracy might be hard to spot. This makes it extremely
    important to evaluate your system’s performance on every subgroup within your
    dataset (see [“Collecting Metadata”](ch07.html#collecting_metadata)).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可能放大数据集和算法中固有的偏见。例如，如果用于训练ML驱动健康可穿戴设备的数据集中，少数群体的示例较少，模型压缩可能导致这些群体的性能下降。由于他们是少数群体，对模型整体准确性的影响可能很难察觉。这使得对数据集中每个子组的系统性能进行评估变得极为重要（见[“收集元数据”](ch07.html#collecting_metadata)）。
- en: There are two excellent scientific papers on this topic from researcher Sara
    Hooker et al. One is [“What Do Compressed Deep Neural Networks Forget?”](https://oreil.ly/v3Bvl),
    and the other is [“Characterising Bias in Compressed Models”](https://oreil.ly/V_cTk).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这个主题有两篇优秀的科学论文，由研究员Sara Hooker等人撰写。一篇是[“压缩深度神经网络会忘记什么？”](https://oreil.ly/v3Bvl)，另一篇是[“压缩模型中的偏见特征”](https://oreil.ly/V_cTk)。
- en: On-Device Training
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设备上训练
- en: In the vast majority of cases, machine learning models used in edge AI are trained
    before being deployed to a device. Training requires large amounts of data, typically
    annotated with labels, and involves significant computation—the equivalent of
    hundreds or thousands of inferences per data point. This limits the utility of
    on-device training, since by nature edge AI applications are subject to severe
    constraints in memory, compute, energy, and connectivity.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在绝大多数情况下，用于边缘AI的机器学习模型在部署到设备之前进行训练。训练需要大量数据，通常用标签进行注释，并涉及大量计算——相当于每个数据点数百或数千次推断。这限制了设备上训练的实用性，因为边缘AI应用本质上受到内存、计算、能量和连接性的严格约束。
- en: 'That said, there are a few scenarios where on-device training makes sense.
    Here’s an overview:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，在一些场景中进行设备上训练是有意义的。以下是一个概述：
- en: Predictive maintenance
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 预测性维护
- en: A common example of on-device training happens in predictive maintenance when
    a machine is being monitored to determine whether it is functioning normally.
    A small on-device model can be trained with data that represents a “normal” state.
    If the machine’s signals start to deviate from that baseline, the application
    can notice and take action.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测性维护中，常见的设备上训练例子是当监测机器是否正常运行时。可以使用代表“正常”状态的数据来训练一个小型的设备上模型。如果机器的信号开始偏离基线，应用程序可以察觉并采取行动。
- en: This use case is only possible when it can be assumed that abnormal signals
    are rare, and that at any given moment the machine is likely to be operating normally.
    This allows the device to treat the data being collected as having an implicit
    “normal” label. If abnormal states were common, it would be impossible to make
    assumptions about the state at any given moment.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在可以假设异常信号很少，并且在任何给定时刻机器可能正常运行时，这种用例才可能发生。这使得设备能够将正在收集的数据视为具有隐含的“正常”标签。如果异常状态很常见，那么在任何给定时刻假设状态将变得不可能。
- en: Personalization
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化
- en: Another example where on-device training makes sense is when a user is asked
    to deliberately provide labels. For example, some smartphones use facial recognition
    as a security method. When the user sets up the device, they are asked to enroll
    images of their face. A numeric representation of these facial images is stored.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个适合设备上训练的例子是当用户被要求故意提供标签时。例如，一些智能手机使用面部识别作为安全方法。用户设置设备时，会被要求注册他们的面部图像。这些面部图像的数值表示被存储。
- en: 'These types of applications tend to use carefully designed embedding models
    that convert raw data into compact numeric representations of their content. The
    embeddings are designed in such a way that the Euclidean distance between two
    embeddings^([5](ch04.html#idm45988809109696)) corresponds to the similarity between
    them. In our face recognition example, this makes it easy to determine whether
    a new face matches the representations stored during setup: the distance between
    the new face and the enrolled face is calculated, and if it is sufficiently close
    then the faces are considered the same.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型的应用通常使用精心设计的嵌入模型，将原始数据转换为其内容的紧凑数值表示。这些嵌入被设计成欧几里得距离^[5](ch04.html#idm45988809109696)能够对应于它们之间的相似性。在我们的面部识别示例中，这使得很容易确定新面孔是否与设置期间存储的表示相匹配：计算新面孔与注册面孔之间的距离，如果足够接近则认为是同一人。
- en: This form of personalization works well because, typically, the algorithm used
    to determine embedding similarity can be very simple; either a distance calculation
    or a nearest neighbors algorithm. The embedding model has done all the hard work.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这种个性化形式有效的原因是，通常用于确定嵌入相似性的算法可以非常简单；可以是距离计算或最近邻算法。嵌入模型已经完成了所有的繁重工作。
- en: Implicit association
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 隐性关联
- en: A further example of on-device training is when labels are available by association.
    For example, battery management features such as Apple’s [Optimized Battery Charging](https://oreil.ly/OzgdM)
    train models on-device to predict what time a user is likely to be using their
    device. One way to do this would be to train a forecasting model to output a probability
    of usage at a specific time, given a log of the previous few hours’ usage.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个设备上训练的例子是当标签通过关联可用时。例如，电池管理功能如苹果的[优化电池充电](https://oreil.ly/OzgdM)通过设备上的模型训练来预测用户何时可能在使用他们的设备。一种方法是训练一个预测模型，在给定前几个小时使用记录的情况下，输出在特定时间的使用概率。
- en: In this case, it’s easy to collect and label training data on a single device.
    Usage logs are collected in the background, and labels are applied according to
    some metric (such as whether the screen was activated). The implicit association
    between time and log content allow the data to be labeled. A simple model can
    then be trained.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，收集和标记训练数据在单个设备上很容易。使用日志在后台收集，根据某些指标（例如屏幕是否被激活）应用标签。时间和日志内容之间的隐含关联允许数据被标记。然后可以训练一个简单的模型。
- en: Federated learning
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习
- en: One of the obstacles to training on-device is a lack of training data. In addition,
    on-device data is often private and users are not comfortable with it being transmitted.
    Federated learning is a way of training models in a distributed manner, across
    many devices, while preserving privacy. Instead of raw data being transmitted,
    partially trained models are passed around between devices (or between each device
    and a central server). The partially trained models can be combined and distributed
    back to devices once they are ready.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 设备上训练的一个障碍是缺乏训练数据。此外，设备上的数据通常是私密的，用户不希望其被传输。联合学习是一种分布式训练模型的方式，跨多个设备进行，同时保护隐私。部分训练好的模型在设备之间（或者在每个设备与中央服务器之间）传递，而不是传输原始数据。一旦准备就绪，这些部分训练好的模型可以合并并分发回设备。
- en: Federated learning often seems attractive since it appears to provide a way
    for models to learn and improve while in the field. However, it has some serious
    limitations. It is computationally expensive and requires large amounts of data
    transfer, which runs counter to the core benefits of edge AI. The training process
    is very complex and requires both on-device and server-side components, which
    increases project risk.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 联合学习看起来很有吸引力，因为它似乎为模型在实地学习和改进提供了一种方式。然而，它也有一些严重的局限性。它需要大量的计算资源和数据传输，这与边缘人工智能的核心优势相抵触。训练过程非常复杂，需要设备端和服务器端的组件，这增加了项目风险。
- en: Since data is not stored globally, there is no way to validate that the trained
    model is performing well across the entire deployment. The fact that models are
    uploaded from local devices presents a vector for security attacks. Finally, and
    most importantly, it does not solve the problem of labels. If labeled data is
    not available, federated learning is useless.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据未存储在全局，无法验证训练模型在整个部署中的表现良好。从本地设备上传模型存在安全攻击的风险。最重要的是，这并不能解决标签问题。如果没有标记的数据，联合学习就毫无意义。
- en: Over-the-air updates
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 空中更新
- en: Although not actually an on-device training technique, the most common way to
    update models in the field is via over-the-air updates. A new model can be trained
    in the lab, using data collected from the field, and distributed to devices via
    firmware updates.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实际上不是一种设备上的训练技术，但在现场更新模型的最常见方式是通过空中更新。可以在实验室中训练新模型，使用从现场收集的数据，并通过固件更新分发到设备上。
- en: This depends on network communication, and it doesn’t solve the problem of obtaining
    labeled data, but it’s the most common way to keep models up to date over time.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于网络通信，并不能解决获取标记数据的问题，但它是随时间保持模型最新的最常见方式。
- en: Summary
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We’ve now learned about the key AI algorithms that make edge AI possible, along
    with the hardware that runs them. The next chapter will walk through the tools—and
    the skills—that are needed to bring everything together.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了使边缘人工智能成为可能的关键 AI 算法，以及运行它们的硬件。下一章将介绍将所有内容整合在一起所需的工具和技能。
- en: ^([1](ch04.html#idm45988807896720-marker)) The chunks can be discrete or overlapping,
    or even have gaps between them.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#idm45988807896720-marker)) 块可以是离散的，重叠的，甚至它们之间有间隙。
- en: ^([2](ch04.html#idm45988808474368-marker)) One reason for this is that the raw
    audio shown in [Figure 4-5](#wave_spectrogram) consists of 44,100 samples, while
    the equivalent spectrogram only has 3,960 elements. The smaller input means a
    smaller model.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#idm45988808474368-marker)) 这是因为图 [4-5](#wave_spectrogram) 中显示的原始音频由
    44,100 个样本组成，而相应的频谱图只有 3,960 个元素。更小的输入意味着更小的模型。
- en: ^([3](ch04.html#idm45988808463232-marker)) In image processing, a feature is
    a particular piece of information about an image, such as the positions of certain
    visual structures. The Wikipedia page titled [“Feature (computer vision)”](https://oreil.ly/-EC-T)
    lists many common image features.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.html#idm45988808463232-marker)) 在图像处理中，特征是关于图像的特定信息，如某些视觉结构的位置。维基百科页面[“特征（计算机视觉）”](https://oreil.ly/-EC-T)列出了许多常见的图像特征。
- en: ^([4](ch04.html#idm45988814548256-marker)) There’s a well-documented phenomenon
    known as the [“AI effect”](https://oreil.ly/hcR8Q), where the moment AI researchers
    figure out how to make a computer do a task, critics no longer consider that task
    representative of intelligence.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.html#idm45988814548256-marker)) 有一个广为人知的现象被称为[“AI效应”](https://oreil.ly/hcR8Q)，即当AI研究人员找到让计算机完成某项任务的方法后，批评者不再认为该任务代表智能。
- en: ^([5](ch04.html#idm45988809109696-marker)) An embedding can be thought of as
    a coordinate in a multidimensional space. The Euclidean distance between two embeddings
    is the distance between the two coordinates.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.html#idm45988809109696-marker)) 嵌入可以被看作是多维空间中的一个坐标。两个嵌入之间的欧氏距离就是这两个坐标之间的距离。
