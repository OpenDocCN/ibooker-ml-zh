- en: 'Chapter 4\. Technical Interview: Model Training and Evaluation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 技术面试：模型训练和评估
- en: In this chapter, we will cover the ML model training process and related interview
    questions. To many practitioners, the model training is the most exciting part,
    and I agree―it’s very satisfying to see the model become more and more accurate
    throughout the process. However, to begin ML model training, hyperparameter tuning,
    and running experiments with various algorithms, you’ll need to have data. Machine
    learning at its core is letting algorithms find patterns in data and then making
    predictions and decisions based on those patterns. Having useful data is the foundation
    of ML, and as the industry adage says, “Garbage in, garbage out.” That is, if
    the ML models are training on useless data, then the resulting model and inferences
    will also be useless.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖机器学习模型训练过程及相关面试问题。对许多实践者来说，模型训练是最令人兴奋的部分，我也同意——在整个过程中看到模型变得越来越准确确实令人满足。然而，要开始机器学习模型训练、超参数调整并运行各种算法实验，您需要拥有数据。机器学习的核心是让算法在数据中找到模式，然后根据这些模式进行预测和决策。拥有有用的数据是机器学习的基础，正如行业格言所说：“垃圾进，垃圾出”。也就是说，如果机器学习模型训练在无用数据上，那么产生的模型和推断也将无用。
- en: I’ll start with an overview of data processing and cleaning, which transforms
    raw data into a format that is useful for (and compatible with) ML algorithms.
    Next, I’ll go through algorithm selection, such as trade-offs between ML algorithms
    in different scenarios, and how to generally select the best one for a given problem.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从数据处理和清洗的概述开始，这将把原始数据转换为对机器学习算法有用和兼容的格式。接下来，我将介绍算法选择，例如在不同场景下各种机器学习算法之间的权衡，以及如何一般性地选择最适合给定问题的算法。
- en: After that, I’ll cover model training and the process of optimizing the model’s
    performance. This can be an ambiguous and challenging process, and there are some
    best practices you’ll learn, such as hyperparameter tuning and experiment tracking,
    which can prevent the best results from being lost and ensure that they are reproducible.
    On that note, I’ll also go over how to know when an ML algorithm is *good*, in
    a practical sense. This involves model evaluation and comparisons against some
    baseline models or baseline heuristics. Model evaluation can also help you determine
    the efficacy of the model on new, unseen data and discover whether the model might
    overfit, underfit, or otherwise underperform in the real world.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我将涵盖模型训练和优化模型性能的过程。这可能是一个模糊而具有挑战性的过程，您将学习到一些最佳实践，如超参数调整和实验跟踪，这可以避免丢失最佳结果，并确保这些结果可以复现。在这一点上，我还将讨论如何判断一个机器学习算法在实际中是否*优秀*。这涉及模型评估以及与一些基准模型或基准启发式方法进行比较。模型评估还可以帮助您确定模型在新的、未见过的数据上的效果，并发现模型可能在现实世界中过拟合、欠拟合或表现不佳。
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: I try to mention as many common ML interview techniques as space allows, but
    there are many more under the sun. Be sure to check out the linked resources to
    extend your learning and interview preparation!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我尽量提及尽可能多的常见机器学习面试技巧，但阳光下还有更多。务必查看链接的资源，以扩展您的学习和面试准备！
- en: Throughout this chapter, I’ll give practical tips and examples to help you succeed
    in your ML interviews. By the end of this chapter, you should have a solid understanding
    of the data cleaning, preprocessing, model training, and evaluation process and
    be able to discuss them well in your own interviews.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将提供实用的技巧和示例，帮助您在机器学习面试中取得成功。到本章结束时，您应该对数据清洗、预处理、模型训练和评估过程有扎实的理解，并能够在自己的面试中进行深入讨论。
- en: Defining a Machine Learning Problem
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义机器学习问题
- en: In this section, I provide a high-level overview of defining an ML problem,
    including why and how this shows up in interview questions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将概述定义机器学习问题的高层次概述，包括为什么以及如何在面试问题中展示这一点。
- en: 'Consider the following scenario: you, the candidate, are walking through an
    ML project you’ve built. The goal is to predict if a user will click on a promotional
    email for a particular singer’s concerts.^([1](ch04.html#ch04fn1)) Your interviewer
    thinks for a few seconds after your overview, then says, “It sounds like you can
    use the time that a user listens to artist A to determine who gets sent promotional
    emails for that artist. For example, if they listen to artist A for more than
    five hours a week, then send an email if artist A has a concert in the listener’s
    area. Given that there are simpler approaches that don’t use machine learning
    and achieve the same thing as your model, *why did you choose ML*?”'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下场景：你，作为应聘者，正在走你建立的一个机器学习项目。目标是预测用户是否会点击特定歌手音乐会的促销邮件。[^1] 在你概述完后，面试官思考了几秒钟，然后说道：“听起来你可以利用用户听歌手
    A 的时间来决定谁会收到该歌手音乐会的促销邮件。例如，如果他们每周听歌手 A 超过五个小时，那么如果该歌手在听众所在地区有音乐会，就发送邮件。考虑到有一些简单的方法，它们不使用机器学习也能达到与你的模型相同的效果，*为什么选择了机器学习*？”
- en: You freeze because you hadn’t thought of this question. It seemed like a fun,
    self-directed project at the time, and you just wanted to learn. You can’t quite
    understand what the interviewer is probing at with their question. What do you
    do?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你陷入了冻结状态，因为你没有考虑到这个问题。当时，这似乎是一个有趣的自主项目，你只是想学习一下。你不能完全理解面试官用这个问题探讨什么。你该怎么办？
- en: 'It’s important to understand beforehand how you can answer these questions
    well. Here are some possible angles:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前了解如何回答这些问题很重要。以下是一些可能的角度：
- en: Did you think about using a heuristic-based (i.e., rules-based) baseline first?
    In applicable situations, you can also use an as-simple-as-possible model, such
    as a logistic regression model, as a baseline. Then, the goal of your ML model
    would be to perform better than the baseline.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有考虑过首先使用基于启发式（即基于规则的）基线吗？在适用的情况下，你还可以使用尽可能简单的模型，比如逻辑回归模型，作为一个基线。然后，你的机器学习模型的目标将是比基线表现更好。
- en: In real-world situations, new ML initiatives often aren’t launched or approved
    unless there’s clear business value to justify the engineering time and effort.
    For example, if the costs of implementing an ML system to recommend concerts from
    scratch don’t outweigh the expected earnings, then it’s easier to use heuristics.
    Projected savings on complexities, manual work, or time is also a reason to use
    ML over heuristics.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现实世界中，新的机器学习项目往往不会启动或批准，除非有明确的商业价值来证明工程时间和精力的投入是值得的。例如，如果实施一个从头开始推荐音乐会的机器学习系统的成本不足以抵消预期的收益，那么使用启发式方法会更容易。预计的节约复杂性、手动工作或时间也是选择机器学习而不是启发式方法的原因之一。
- en: Don’t worry though—the interviewer isn’t slighting your project but rather asking,
    “Why ML?” This is very common in the professional ML world. Asking “why ML?” *doesn’t*
    mean “you really shouldn’t have used ML.” It’s just the beginning of a discussion,
    one that ML professionals have often in their day-to-day lives. The way you respond
    to this question, especially for new grads, can be a good signal of whether you
    can transition well to working in ML in industry.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但不要担心——面试官并不是在贬低你的项目，而是在问：“为什么选择了机器学习？”这在专业的机器学习领域非常常见。问“为什么选择了机器学习？”*并不*意味着“你真的不应该使用机器学习。”这只是开始讨论的一部分，专业的机器学习人员在日常生活中经常遇到这种情况。你如何回应这个问题，特别是对于新毕业生来说，可以很好地表明你是否能顺利过渡到在工业界从事机器学习工作。
- en: 'Here’s what you could say that would work in this scenario:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你可以这样回答：
- en: 'Be honest: “To be honest, I just wanted to learn some new modeling techniques
    with a side project, and since I’m a heavy user of Spotify, I wanted to see how
    I could emulate its email feature with ML.”'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坦诚地回答：“老实说，我只是想通过一个副业项目学习一些新的建模技术，因为我是 Spotify 的重度用户，我想看看如何用机器学习模拟它的邮件功能。”
- en: 'If you’re talking about a work project: “In reality, I found that heuristics
    worked, but only for the most average users. For example, heavy users require
    a longer listening time to determine their favorite artists. Additionally, once
    we included other data like *likes* and *add to playlists* to the heuristics,
    we noticed a higher response to the promotional email. Thus, the heuristics became
    too complicated and hard to scale. That’s why we started using ML instead, so
    it could find patterns in a larger number of features.”'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你在谈论一个工作项目：“事实上，我发现启发式方法确实有效，但仅适用于最普通的用户。例如，重度用户需要更长的听歌时间来确定他们喜欢的艺术家。此外，一旦我们在启发式方法中包含了其他数据，比如*喜欢*和*添加到播放列表*，我们注意到对促销电子邮件的反应更高。因此，启发式方法变得过于复杂和难以扩展。这就是为什么我们开始使用ML，因此它可以在更多特征中找到模式。”
- en: Tip
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Tip
- en: Being honest is fine. As a new grad, I once prefaced one of my side project
    walkthroughs with, “This is a classifier for Ariana Grande images. I just wanted
    to do this project for fun, and there’s no real reason it had to be Ariana Grande.
    Here’s how I did it…” But I still managed to be taken seriously by interviewers,
    by justifying the project as an opportunity to use convolutional neural networks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 诚实是好的。作为一名新毕业生，我曾经在一次副业项目展示中开头说：“这是一个用于Ariana Grande图像的分类器。我只是想做这个项目来娱乐，没有真正的原因必须选择Ariana
    Grande。这里是我是如何做的…” 但我仍然成功地被面试官认真对待，因为我解释这个项目是使用卷积神经网络的机会。
- en: If you’re doing your own side project and expect to use it for answering interview
    questions, consider what heuristic methods could achieve the goal you want. Later,
    you can use them as a simple baseline to see if the ML method is better. This
    will help you stand out from other candidates. I’ll cover model selection and
    model evaluation later in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在做自己的副业项目，并期望用它来回答面试问题，考虑哪些启发式方法可以达到你想要的目标。稍后，你可以将它们用作简单的基准线，看看ML方法是否更好。这将帮助你脱颖而出。我稍后会讨论模型选择和模型评估。
- en: Data Preprocessing and Feature Engineering
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理和特征工程
- en: In this section, I’ll summarize common data preprocessing and feature engineering
    techniques and scenarios as well as common ML interview questions that cover this
    step in the ML lifecycle. For simplicity, I’ll assume that data is available for
    the ML interview questions, even if that is a common challenge in real-life scenarios.
    I’ll start with an introduction to data acquisition,^([2](ch04.html#ch04fn2))
    exploratory data analysis (EDA), and feature engineering.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将总结常见的数据预处理和特征工程技术和场景，以及涵盖ML生命周期中这一步的常见ML面试问题。为简单起见，我假设ML面试问题的数据是可用的，即使这在现实场景中是一个常见的挑战。我将从数据采集的介绍开始，^([2](ch04.html#ch04fn2))
    探索性数据分析（EDA）和特征工程。
- en: Tip
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Tip
- en: All data and ML roles will use data preprocessing and EDA. Some of the techniques
    in this chapter are specifically for ML but are still useful for data analysts
    or data engineers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据和ML角色都将使用数据预处理和EDA。本章的一些技术特别适用于ML，但对数据分析师或数据工程师也同样有用。
- en: Introduction to Data Acquisition
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据采集入门
- en: 'Acquiring data, commonly referred to as *data acquisition* in the context of
    ML, can involve the following options:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 获得数据，在ML的背景下通常称为*数据采集*，可以涉及以下选项：
- en: Work access, usually proprietary data
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作访问，通常是专有数据
- en: Public datasets, such as from Kaggle, [census bureaus](https://oreil.ly/_BFu5),
    and the like
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公共数据集，例如来自Kaggle，[人口普查局](https://oreil.ly/_BFu5)等
- en: Web scraping (beware of some sites’ terms and conditions)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络爬虫（注意某些网站的条款和条件）
- en: Academic access, such as being part of a research lab at your university
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学术访问，比如参与你大学的研究实验室
- en: 'Purchasing data from vendors:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从供应商购买数据：
- en: Some vendors also help annotate and label data, such as [Figure Eight](https://oreil.ly/LAH7w)
    and [Scale AI](https://scale.com).
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些供应商还可以帮助注释和标记数据，例如[Figure Eight](https://oreil.ly/LAH7w)和[Scale AI](https://scale.com)。
- en: Your workplace or academic institution will usually help cover the costs, as
    the prices are typically too high to be worth it for individual side projects.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的工作场所或学术机构通常会帮助支付成本，因为这些价格通常对个人副业项目来说太高了。
- en: Creating synthetic data through simulations
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过模拟创建合成数据
- en: Creating your own raw data, such as taking your own photos, crowdsourcing data,
    or using art/designs that you create yourself
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建你自己的原始数据，比如拍摄自己的照片，众包数据或使用你自己创建的艺术/设计
- en: Introduction to Exploratory Data Analysis
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索性数据分析入门
- en: Now that you have acquired the data, it’s time to analyze it. Your primary aim
    with EDA is to see if the data is sufficient as a starting point or if you need
    more. Thus, aim to get a high-level overview of the distribution of the data and
    find any flaws and quirks. Flaws and quirks may include too many missing values,
    skewed data distributions, or duplicates. EDA also covers general traits of each
    feature, looking at the means, distributions, and so on. If you find flaws, there
    are ways you can resolve the issues later during data cleaning and feature engineering;
    what is important during EDA is simply to be *aware* of potential issues.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经获取了数据，是时候开始分析了。你在进行探索性数据分析（EDA）时的主要目标是查看数据是否足够作为起点，或者你是否需要更多数据。因此，你需要对数据的分布进行高级概述，并找出任何缺陷和怪异之处。缺陷和怪异可能包括过多的缺失值、数据分布倾斜或重复值。EDA
    还涵盖每个特征的一般特征，查看均值、分布等。如果发现缺陷，你可以在数据清洗和特征工程过程中稍后解决这些问题；而在进行EDA 时，重要的是*意识到*潜在问题。
- en: Tip
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: For ML and data practitioners, it is important to have *some* domain knowledge.
    In my side projects on video-game pricing, I was well aware of industry dynamics
    and customer behaviors, being an avid gamer myself. At work, I need to learn about
    each domain to build useful ML models; for example, customers in telecom have
    different behaviors than those in fintech.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习和数据从业者来说，拥有*一些*领域知识非常重要。在我关于视频游戏定价的个人项目中，作为一名狂热的游戏玩家，我对行业动态和客户行为非常了解。在工作中，我需要学习每个领域，以构建有用的机器学习模型；例如，电信客户的行为与金融科技客户不同。
- en: My common approach is to run [ydata-profiling](https://oreil.ly/S3XXt), formerly
    known as pandas-profiling, and start drilling down from the generated report (an
    example report is shown in [Figure 4-1](#screenshot_of_ydatahyphenprofilingsemico)).
    Note that this is merely a starting point, and using domain knowledge to suss
    out patterns or abnormalities will be important. What might be an issue for some
    industries and models might be expected in others. For example, in the case of
    a RecSys problem, it is more common to have sparser data than in a time-series
    dataset. Simply looking at generated stats isn’t enough. Also, some domains have
    algorithms that take care of common issues for that domain, and those issues are
    thus less cause for alarm.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我的常规做法是运行 [ydata-profiling](https://oreil.ly/S3XXt)，之前称为 pandas-profiling，并从生成的报告开始深入分析（示例报告见[图 4-1](#screenshot_of_ydatahyphenprofilingsemico)）。请注意，这只是一个起点，使用领域知识来发现模式或异常将非常重要。对一些行业和模型来说可能是问题的东西，在其他行业中可能是预期的。例如，在
    RecSys 问题中，数据稀疏性比在时间序列数据集中更常见。仅仅查看生成的统计数据是不够的。此外，一些领域可能有算法来处理该领域的常见问题，因此这些问题可能不会引起太大关注。
- en: '![Screenshot of ydata-profiling; source: ydata-profiling documentation](assets/mlin_0401.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![ydata-profiling 屏幕截图；来源：ydata-profiling 文档](assets/mlin_0401.png)'
- en: 'Figure 4-1\. Screenshot of ydata-profiling; source: [ydata-profiling documentation](https://oreil.ly/jOE08).'
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. ydata-profiling 屏幕截图；来源：[ydata-profiling 文档](https://oreil.ly/jOE08)。
- en: More details on EDA are out of scope for this book, but I recommend reading
    [*Making Sense of Data*](https://oreil.ly/zFoDd) by Glenn J. Myatt and Wayne P.
    Johnson (Wiley) for more information.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 关于EDA 的更多细节超出了本书的范围，但我建议阅读 [*Making Sense of Data*](https://oreil.ly/zFoDd)，作者是
    Glenn J. Myatt 和 Wayne P. Johnson（Wiley），以获取更多信息。
- en: 'After some iterations, let’s say that you’ve completed the EDA, coming to a
    decision point: the data seems sound enough (for now) to continue, or you might
    need to acquire more data or another dataset first; rinse and repeat.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一些迭代，假设你已完成了探索性数据分析（EDA），来到一个决策点：数据现在似乎足够可靠（暂时可以继续），或者你可能需要先获取更多数据或另一个数据集；反复进行。
- en: Tip
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When interviewers ask what you’d do when you begin with an ML problem, they
    are expecting to hear you mention EDA at some point early in the process, after
    you’ve acquired some data source(s). It’s important to show that you’re able to
    look critically at the data and even find flaws, not just take in a precleaned
    dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当面试官询问你在解决机器学习问题时从何处开始时，他们希望听到你在获取某些数据源后的早期阶段提到探索性数据分析（EDA）。展示你能够批判性地查看数据甚至发现缺陷是非常重要的，而不仅仅是使用预先清理过的数据集。
- en: Introduction to Feature Engineering
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程介绍
- en: After exploration of the data, iterating until there is a good starting point
    for model training, it’s time for feature engineering. In ML, features refer to
    inputs to ML models. The goal is to make modifications to the dataset to ensure
    compatibility with the ML models, but also to handle any observed flaws or incompleteness
    in the data, such as missing values. The topics I discuss here include handling
    missing data, handling duplicate data, standardizing data, and preprocessing data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索数据后，迭代直到找到模型训练的良好起点，现在是进行特征工程的时候了。在机器学习中，特征指的是ML模型的输入。目标是修改数据集，以确保与ML模型兼容，同时处理数据中观察到的任何缺陷或不完整性，如缺失值。我在这里讨论的主题包括处理缺失数据、处理重复数据、标准化数据和数据预处理。
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Some of the techniques overlap with what is commonly referred to as “data cleaning,”
    which can happen in more stages of the ML lifecycle than feature engineering,
    but is useful to introduce here.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一些技术与通常称为“数据清洗”的内容重叠，这可能比特征工程更频繁发生，但在此介绍是很有用的。
- en: Handling missing data with imputation
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用插补处理缺失数据
- en: There are common imputation techniques for handling missing data that you should
    be able to mention in an interview, along with their pros and cons. These include
    filling in with the mean or median value and using a tree-based model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据的常见插补技术应该能够在面试中提到，并分析其优缺点。这些包括用均值或中位数填充以及使用基于树的模型。
- en: '[Table 4-1](#pros_and_cons_of_common_imputation_techn) lists some things to
    keep in mind when filling in missing values.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 4-1](#pros_and_cons_of_common_imputation_techn) 列出了在填补缺失值时需注意的一些事项。'
- en: Table 4-1\. Pros and cons of common imputation techniques
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4-1\. 常见插补技术的优缺点
- en: '| Technique | Pros | Cons |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 优点 | 缺点 |'
- en: '| --- | --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Mean/median/mode | Simple to implement | Might not account for outliers compared
    to tree-based methods Not as suitable for categorical variables |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 均值/中位数/众数 | 实现简单 | 与基于树的方法相比可能无法考虑异常值，对分类变量不太适合 |'
- en: '| Tree-based models | Can capture more underlying patterns Suitable for both
    numerical and categorical variables | Adds a level of complexity during data preprocessing
    Model needs to be retrained if the underlying distribution of data changes |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 基于树的模型 | 可捕获更多潜在模式 | 适用于数值和分类变量 添加了数据预处理的复杂度 如果数据的基础分布发生变化，需要重新训练模型 |'
- en: Handling duplicate data
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理重复数据
- en: 'There are just about an infinite number of ways that observations can be duplicated
    by accident, so this is one of the issues to discover when conducting EDA:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎有无数种观察结果会被意外复制的方式，所以这是在进行探索性数据分析时需要发现的问题之一：
- en: Data ingestion jobs might run twice due to error.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取作业可能因错误而运行两次。
- en: While doing some complicated joins, some rows could have been unintentionally
    duplicated and then not discovered.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进行一些复杂的连接操作时，有些行可能会无意中重复，然后没有发现。
- en: Some edge cases can cause the data source to provide duplicated data.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些边缘情况可能导致数据源提供重复数据。
- en: … and so on.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: … 等等。
- en: If you encounter duplicated data, you can use SQL or Python to deduplicate the
    data, and make sure the records are represented in a format that’s easier for
    you to access and use later down the road.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到重复数据，可以使用SQL或Python对数据进行去重，并确保记录以更便于您后续访问和使用的格式表示。
- en: Standardizing data
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化数据
- en: 'After you handle missing and duplicate data, the data should be standardized.
    This includes handling outliers, scaling features, and ensuring that data types
    and formats are consistent:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失和重复数据后，数据应该进行标准化。这包括处理异常值、缩放特征，确保数据类型和格式一致：
- en: Handle outliers
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 处理异常值
- en: Techniques for handling outliers include removing extreme outliers from the
    dataset, replacing them with less extreme values (known as *winsorizing*) and
    logarithmic scale transforms. I’d caution against removing outliers since doing
    so really depends on domain knowledge; in some domains, there are more severe
    consequences; for example, removing horse-carriage image data from a self-driving
    car training dataset just because they aren’t a common type of vehicle might cause
    the model to not recognize horse-carriages in the real world. Hence, carefully
    evaluate the impacts before deciding on a particular technique.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 处理异常值的技术包括从数据集中删除极端异常值，用较不极端的值替换它们（称为*Winsorizing*），以及对数尺度转换。我建议不要轻易删除异常值，因为这取决于领域知识；在某些领域，后果可能更严重。例如，从自动驾驶汽车训练数据集中删除马车图像数据，仅因为它们不是常见类型的车辆，可能导致模型在现实世界中无法识别马车。因此，在决定具体技术之前，务必仔细评估其影响。
- en: Scale features
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放特征
- en: For datasets with multiple features with numerical values, larger values might
    be misconstrued by ML algorithms to have more impact. For example, one column
    is price, which ranges from $50 to $5,000, while another feature is the amount
    of time an ad shows up, which ranges from 0 to 10 times. The two features are
    in different units, but both are numerical, so it is possible that the price column
    will be represented as having a higher magnitude of impact. Some models, such
    as gradient-descent-based models, are more sensitive to the scale of features.
    Hence, it’s better to scale the features so that they range from [-1, 1] or [0,
    1].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有多个数值特征的数据集，较大的值可能会被ML算法误解为具有更大的影响。例如，一个列是价格，范围从$50到$5,000，而另一个特征是广告显示时间，范围从0到10次。这两个特征使用不同的单位，但都是数值的，因此价格列可能被表示为具有更高数量级的影响。一些模型，如基于梯度下降的模型，对特征的规模更为敏感。因此，最好将特征缩放到[-1,
    1]或[0, 1]的范围内。
- en: Warning
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Be careful when scaling features. It is useful to combine different techniques
    or to use what you found while conducting EDA. For example, a feature might have
    extreme outliers, such as most of them being in the range [0, 100], except one
    observation of 1000\. Without checking, you might scale the feature values based
    on the min of 0 and max of 1000\. This may cause the information contained in
    the features to be compressed.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放特征时要小心。结合不同的技术或使用探索性数据分析（EDA）中发现的信息是有用的。例如，一个特征可能有极端的异常值，如大多数值在[0, 100]范围内，但有一个观测值为1000。如果不检查，您可能会基于最小值0和最大值1000来缩放特征值。这可能会导致特征中包含的信息被压缩。
- en: Data type consistency
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型一致性
- en: 'I was once working on an ML model and got results that weren’t what I expected,
    and it took me a while to debug. Finally, I identified the issue: a numerical
    column was formatted as a string! Surveying your final data types to ensure that
    they will make sense once fed into your ML model will be useful before you go
    through the rest of the process; consider it a part of quality assurance (QA).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经在处理一个ML模型时得到了意外的结果，花了一些时间调试。最后，我找到了问题所在：一个数值列被格式化为字符串！在继续流程之前，检查最终数据类型以确保它们在输入ML模型时是有意义的，这将对质量保证（QA）很有帮助。
- en: Tip
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: An interviewer may ask follow-up questions about how exactly you have handled
    outliers, feature scale, or datatype consistency, so be sure to brush up on the
    rationale and trade-offs of each approach.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 面试官可能会询问您如何处理异常值、特征规模或数据类型一致性的后续问题，因此请确保了解每种方法的原理和权衡。
- en: Data preprocessing
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Preprocessing data will allow for features to make sense to the ML model in
    the context of the type of algorithm you’re using. Preprocessing for structured
    data can include one-hot encoding, label encoding, binning, feature selection,
    and so on.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用的算法类型的背景下，预处理数据将使特征对ML模型有意义。结构化数据的预处理可以包括独热编码、标签编码、分箱、特征选择等。
- en: Tip
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: '*Unstructured data* is “information that is not arranged according to a preset
    data model or schema, and therefore cannot be stored in a traditional relational
    database or RDBMS (relational database management system). Text and multimedia
    are two common types of unstructured content.”^([3](ch04.html#ch04fn3)) When you
    encounter unstructured data, the preprocessing may be different (potentially even
    transforming the data to become structured). For illustration purposes, I focus
    on examples of preprocessing structured data in this chapter.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*非结构化数据*是“未按预设数据模型或架构排列的信息，因此无法存储在传统的关系数据库或关系数据库管理系统（RDBMS）中。文本和多媒体是两种常见的非结构化内容类型。”^([3](ch04.html#ch04fn3))
    当遇到非结构化数据时，预处理可能会有所不同（甚至可能将数据转换为结构化）。出于说明目的，本章节专注于预处理结构化数据的示例。'
- en: One-hot encoding of categorical data
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分类数据的独热编码
- en: 'You may want to represent categorical data as numerical data. Each category
    becomes a feature, with 0 or 1 representing the state of that feature in each
    observation. For example, imagine a simple weather dataset where it’s only possible
    to have sunny or cloudy weather. You would have the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望将分类数据表示为数值数据。每个类别成为一个特征，每个观测中的状态由0或1表示。例如，想象一个简单的天气数据集，只可能有晴天或多云的天气。您将得到以下结果：
- en: March 1
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 3月1日
- en: 'Weather: Sunny'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气：晴天
- en: 'Temperature (Celsius): 27'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温度（摄氏度）：27
- en: March 2
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 3月2日
- en: 'Weather: Sunny'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气：晴天
- en: 'Temperature (Celsius): 25'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温度（摄氏度）：25
- en: March 3
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 3月3日
- en: 'Weather: Cloudy'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气：多云
- en: 'Temperature (Celsius): 20'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温度（摄氏度）：20
- en: 'But the “Weather” feature can be one-hot encoded to have features with all
    the possible weather states:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，“天气”特征可以进行独热编码，使其拥有所有可能的天气状态的特征：
- en: March 1
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 3月1日
- en: 'Sunny: 1'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 晴天：1
- en: 'Cloudy: 0'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多云：0
- en: March 2
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 3月2日
- en: 'Sunny: 1'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 晴天：1
- en: 'Cloudy: 0'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多云：0
- en: March 3
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 3月3日
- en: 'Sunny: 0'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 晴天：0
- en: 'Cloudy: 1'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多云：1
- en: One-hot encoding is often used because numbers are easier for ML algorithms
    to understand; some algorithms don’t take in categorical values, but this has
    improved over the years, where some implementations can take categorical values
    into account and transform them behind the scenes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码通常被使用，因为数字对机器学习算法更容易理解；某些算法不接受分类值，但这在多年来有所改进，一些实现可以在幕后考虑分类值并进行转换。
- en: One downside of one-hot encoding is that for features originally with high cardinality
    (there are lots of unique values in that feature), one-hot encoding can cause
    the feature count to increase drastically, which can be computationally more expensive.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一位缺点是独热编码对于原本具有高基数的特征（该特征中有许多唯一值）可能导致特征数量急剧增加，这可能在计算上更昂贵。
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Sometimes, lack of domain knowledge or understanding of business logic might
    cause issues in data preprocessing. An example is defining churned users as those
    who canceled a product within the last seven days, but the product or business
    logic actually counts churned users as those who left within the last 60 days.
    (If for some reason, the business logic doesn’t work well for ML, we can then
    discuss a middle ground.)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，缺乏领域知识或对业务逻辑的理解可能会在数据预处理中引起问题。例如，将退订用户定义为在过去七天内取消产品的用户，但实际上产品或业务逻辑将退订用户定义为在过去60天内离开的用户。（如果由于某种原因，业务逻辑不适合机器学习，我们可以讨论一个折中方案。）
- en: Label encoding
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标签编码
- en: Label encoding maps the categories to numbers but keeps them in the same feature.
    For example, types of weather can be mapped to unique numbers, as illustrated
    in [Figure 4-2](#label_encoding_illustration).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码将类别映射到数字，但保持在同一个特征中。例如，天气类型可以映射到唯一数字，如图[4-2](#label_encoding_illustration)所示。
- en: '![Label encoding illustration](assets/mlin_0402.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![标签编码示例](assets/mlin_0402.png)'
- en: Figure 4-2\. Label encoding illustration.
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. 标签编码示例。
- en: 'One of the downsides of label encoding is that some ML algorithms can conflate
    the scale and values to mean a higher magnitude of impact. To use our previous
    example, Weather can be label encoded: Sunny becomes 0, and Cloudy becomes 1\.
    But to ML, this could conflate cloudy as a higher magnitude since 1 is greater
    than 0.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码的一个缺点是一些机器学习算法可能会混淆比例和值，将它们视为影响的更高级别。举个例子，天气可以进行标签编码：晴天变为0，多云变为1。但对于机器学习来说，这可能会导致将多云误认为比较重要，因为1比0大。
- en: Thankfully, in many ML algorithms you can use built-in classes (e.g., scikit-learn’s
    [`LabelEncoder`](https://oreil.ly/Wm_7r) class) so that the algorithm will know
    behind the scenes that this is just a categorization and not necessarily indicative
    of magnitude.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在许多机器学习算法中，您可以使用内置类（例如scikit-learn的[`LabelEncoder`](https://oreil.ly/Wm_7r)类），使算法在幕后知道这只是一种分类，而不一定是大小的指示。
- en: Tip
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Of course, if you forget to let the algorithm know that label-encoded features
    are, in fact, label encoded, then the ML algorithm will likely treat that feature
    like a normal numerical feature. You can see how this can cause issues if you
    didn’t address this during interview questions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果您忘记让算法知道标签编码的特征实际上是标签编码，那么机器学习算法可能会像处理普通数值特征一样处理该特征。如果在面试问题中没有解决这个问题，您可以看到这可能会导致问题。
- en: Binning for numerical values
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数值分箱
- en: Binning can reduce the amount of cardinality and help models generalize more.
    For example, if the dataset has a price of $100, it might not generalize the first
    time it sees $95, even if in the particular application, $95 is similar to $100\.
    As an illustration, you can define the bin edges as [15, 25, 35, 45, 55, 65, 75,
    85, 99], which will create similar price ranges like “$15–$25,” “$25–$35,” “$35–$45,”
    and so on.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 分箱可以减少基数的数量，并帮助模型更好地概括。例如，如果数据集中的价格为$100，即使在特定应用中，$95与$100相似，也可能不会一开始就概括$95。作为例证，您可以定义分箱边界为[15,
    25, 35, 45, 55, 65, 75, 85, 99]，这将创建类似的价格范围，如“$15-$25”，“$25-$35”，“$35-$45”等。
- en: A downside of binning is that it introduces hard edges into the meanings of
    the bins, such that an observation of $46 would be seen as completely different
    from the bin “$35–$45,” even though it might still be similar.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 箱式分组法的一个缺点是它将分箱的含义引入硬边界，因此，一个观察结果为$46可能会被视为与“$35-$45”分箱完全不同，即使它可能仍然相似。
- en: Feature selection
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征选择
- en: Sometimes, your dataset will have features that are highly correlated—that is,
    there is *collinearity* between features. As an extreme example, you might have
    height in centimeters but also height in meters, which essentially capture the
    same information. There may be other features that capture a high proportion of
    the same information as well, and removing them might reduce accidentally overfitting
    or improve model speed because the model doesn’t need to handle as many features.
    Dimensionality reduction is a common technique for feature selection; it reduces
    the dimensionality of the data while retaining the most important information.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，您的数据集将具有高度相关的特征——即特征之间存在*共线性*。举一个极端的例子，您可能有以厘米为单位的身高和以米为单位的身高，这本质上捕捉了相同的信息。可能还有其他捕捉相同信息高比例的特征，删除它们可能会减少意外过度拟合或改善模型速度，因为模型不需要处理那么多特征。降维是特征选择的常见技术；它在保留最重要信息的同时减少数据的维度。
- en: You might also use feature importance tables, such as those provided in XGBoost
    or CatBoost, and prune the features with the lowest importance—that is, the lowest
    contribution to the model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用特征重要性表，例如XGBoost或CatBoost中提供的表格，并剪枝具有最低重要性的特征——即对模型贡献最低的特征。
- en: Sample Interview Questions on Data Preprocessing and Feature Engineering
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理和特征工程示例面试问题
- en: Now that I’ve covered some basics of data preprocessing and feature engineering,
    let’s go through some example interview questions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我已经介绍了一些数据预处理和特征工程的基础知识，让我们通过一些示例面试问题来进一步讨论。
- en: 'Interview question 4-1: What’s the difference between feature engineering and
    feature selection?'
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面试问题4-1：特征工程和特征选择有什么区别？
- en: Example answer
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 示例答案
- en: Feature engineering is about creating or transforming features from raw data.
    This is done to better represent the data and make the data more suitable for
    ML compared to its raw format. Common techniques include handling missing data,
    standardizing data formats, and so on.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是关于从原始数据中创建或转换特征的过程。这样做是为了更好地表示数据，并使数据与其原始格式相比更适合机器学习。常见的技术包括处理缺失数据，标准化数据格式等。
- en: Feature selection is about narrowing down relevant ML features to simplify the
    model and prevent overfitting. Common techniques include PCA (principal component
    analysis) or using tree-based models’ feature importance to see which features
    contribute more useful signals.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择是关于缩小相关的机器学习特征，以简化模型并防止过拟合的过程。常见的技术包括PCA（主成分分析）或使用基于树的模型的特征重要性来查看哪些特征提供更有用的信号。
- en: 'Interview question 4-2: How do you prevent data leakage issues while conducting
    data preprocessing?'
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '面试问题 4-2: 在进行数据预处理时，如何防止数据泄露问题？'
- en: Example answer
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 示例答案
- en: Being cautious with training, validation, and test data splits is one of the
    most common ways to prevent data leakage. However, things aren’t always so simple.
    For example, in the case when data imputation is done with the mean value of all
    observations in the feature, that means the mean value contains information about
    all observations, not just the training split. In that case, make sure to conduct
    data imputation with only information about the training split, on the training
    split. Other examples of data leakage could include time-series splits; we should
    be careful that we don’t accidentally shuffle and split the time series incorrectly
    (e.g., using tomorrow to predict today instead of the other way around).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 谨慎处理训练、验证和测试数据分割是防止数据泄露的最常见方式之一。然而，事情并不总是那么简单。例如，在数据插补时使用所有观测的平均值，这意味着平均值包含了所有观测的信息，而不仅仅是训练集的。在这种情况下，请务必只使用关于训练集的信息来进行数据插补。数据泄露的其他例子可能包括时间序列分割；我们应该小心，不要错误地对时间序列进行洗牌和分割（例如，使用明天来预测今天，而不是反之）。
- en: 'Interview question 4-3: How do you handle a skewed data distribution during
    feature engineering, assuming that the minority data class is required for the
    machine learning problem?'
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '面试问题 4-3: 在特征工程中如何处理偏斜的数据分布，假设机器学习问题需要少数数据类？'
- en: Example answer
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 示例答案
- en: Sampling techniques,^([4](ch04.html#ch04fn4)) such as oversampling the minority
    data classes, could help during preprocessing and feature engineering (for example,
    using techniques like SMOTE). It’s important to note that for oversampling, any
    duplicate or synthetic instances should be generated only from the training data
    to avoid data leakage with the validation or test set.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 采样技术，^([4](ch04.html#ch04fn4))，例如过采样少数数据类，在预处理和特征工程中可能有所帮助（例如，使用SMOTE等技术）。重要的是要注意，对于过采样，任何重复或合成的实例应仅从训练数据生成，以避免与验证或测试集的数据泄露。
- en: The Model Training Process
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练过程
- en: 'Now that you have data ready for ML, it’s time to move on to the next step:
    model training. This process includes the steps of defining the ML task, selecting
    the most suitable ML algorithms for the task, and actually training the model.
    In this section, I will also provide common interview questions and tips that
    will help you succeed.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好进行机器学习的数据，是时候进入下一步了：模型训练。这个过程包括定义机器学习任务、选择最适合任务的机器学习算法以及实际训练模型的步骤。在本节中，我还将提供一些常见的面试问题和技巧，这将帮助您取得成功。
- en: The Iteration Process in Model Training
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练中的迭代过程
- en: At the outset of an ML project, you likely thought about what you wanted the
    general result to be, such as “getting the highest accuracy possible on a Kaggle
    competition” or “using this data to predict video game sales prices.” You might
    have also started researching some algorithms that are good at the task, such
    as time-series predictions. Determining what that final ML task will be is (often)
    an iterative process in which you may go back and forth between steps before you
    land on something, as illustrated in [Figure 4-3](#example_iteration_process_during_ml_trai).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目的开始阶段，您可能已经考虑过您希望的一般结果，例如“在Kaggle竞赛中获得尽可能高的准确率”或“使用这些数据预测视频游戏销售价格”。您可能还开始研究一些在任务中表现良好的算法，例如时间序列预测。确定最终的机器学习任务通常是一个迭代的过程，在这个过程中，您可能会在各个步骤之间来回反复，直到最终确定某些内容，就像在[图
    4-3](#example_iteration_process_during_ml_trai)中所示。
- en: '![Example iteration process during ML training](assets/mlin_0403.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习训练中的示例迭代过程](assets/mlin_0403.png)'
- en: Figure 4-3\. Example iteration process during ML training.
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 机器学习训练中的示例迭代过程。
- en: 'For example, let’s take a look at all the steps in a project to predict video
    game sales:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看一下预测视频游戏销售项目中的所有步骤：
- en: 'Define the ML task, model selection. You might start with an idea: use time-series
    data with ARIMA (AutoRegressive Integrated Moving Average), because the problem
    seems simple—price prediction often uses time-series data.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义机器学习任务，选择模型。您可能从一个想法开始：使用时间序列数据和ARIMA（自回归综合移动平均），因为问题看起来很简单——价格预测通常使用时间序列数据。
- en: Data acquisition. You might acquire a dataset with time-series data—that is,
    it only has the time, such as a date or timestamp, and the price. The future price
    is the output of the model predictions, and the historical prices are the inputs.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据采集。您可能会获取一个具有时间序列数据的数据集，即只有时间，如日期或时间戳，以及价格。未来价格是模型预测的输出，而历史价格是输入。
- en: However, you may run into a situation where using ARIMA doesn’t seem to be working
    out, and you troubleshoot by analyzing the source data more closely. It turns
    out that you’re combining data of the games from large companies (also known as
    “AAA” games) with smaller games from independent studios (also known as “indie”
    games). AAA games often have large budgets for marketing and promotion, so on
    average, they sell more than indie games.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，您可能会遇到这样一种情况：使用ARIMA似乎行不通，于是您通过更仔细地分析源数据来排除故障。结果发现，您正在将大公司（也称为“AAA”游戏）的数据与独立工作室（也称为“独立”游戏）的较小游戏数据混合在一起。AAA游戏通常拥有较大的市场营销和推广预算，因此平均销量高于独立游戏。
- en: Define the ML task (again). The next step is to reevaluate the ML task. After
    some thought, you decide to still predict the time series, thus keeping the ML
    task the same.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义ML任务（再次）。下一步是重新评估ML任务。经过一番思考，您决定仍然预测时间序列，因此保持ML任务不变。
- en: 'Data acquisition (again). This time, though, you already know what you may
    need to do differently so that the results will be better. Thus, you acquire more
    data: whether a game is AAA or indie. You might even end up hand labeling it.'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据采集（再次）。不过，这一次，您已经知道可能需要采取不同的措施以获得更好的结果。因此，您获取了更多数据：游戏是AAA还是独立。您甚至可能最终手动标记它。
- en: Model selection (again). Now you realize the model needs to change since ARIMA
    doesn’t take categorical variables such as the labels “Indie” and “AAA.” Thus,
    you look online and find other [algorithms](https://oreil.ly/ApYUa) that can mix
    categorical variables with numerical variables, and you try one of those.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型选择（再次）。现在您意识到模型需要更改，因为ARIMA不能处理类别变量，比如“独立”和“AAA”标签。因此，您上网查找其他可以混合类别变量和数值变量的[算法](https://oreil.ly/ApYUa)，并尝试其中之一。
- en: 'Continue to iterate the previous steps until good enough. You might rinse and
    repeat if that still doesn’t work well, acquiring more types of features, trying
    different models, or doing feature engineering like one-hot encoding. The ML task
    could change along the way as well: instead of predicting the exact sales numbers,
    you might opt to predict the bins, such as (high, mid, low) sales, with high sales
    being above 50,000 units or something you’ve defined through EDA.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续迭代之前的步骤，直到达到足够好的效果。如果仍然不起作用，您可以重复进行，获取更多类型的特征，尝试不同的模型，或者进行如独热编码之类的特征工程。ML任务可能也会随之改变：而不是预测精确的销售数字，您可以选择预测类别，例如（高、中、低）销量，其中高销量定义为超过50,000单位或通过探索性数据分析定义的某些值。
- en: If you’ve done a project from end to end, you know the iterative nature of the
    steps described in this section. You may notice that in this example, you can
    clearly see what led you to go back to data acquisition and what then led you
    to go back to defining the ML task. There’s always a reason, even if the reason
    is just to see if the new approach works better than your current approach. This
    gives you a lot of interesting information to provide in response to your interviewer’s
    questions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您从头到尾完成了一个项目，您会了解本节描述的步骤的迭代性质。您可能会注意到，在这个例子中，您可以清楚地看到是什么导致您回到数据采集，然后是什么导致您回到定义ML任务。即使理由只是为了看看新方法是否比当前方法更好，也总是有一个理由。这为您在面试官提问时提供了很多有趣的信息。
- en: 'Interviewers will want to make sure of the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 面试官们希望确认以下几点：
- en: You are knowledgeable about common ML tasks in their field.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您了解其领域中常见的ML任务。
- en: You are knowledgeable about common algorithms related to said tasks.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您了解与所述任务相关的常见算法。
- en: You know how to evaluate those models.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您知道如何评估这些模型。
- en: Defining the ML Task
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义ML任务
- en: In the previous section, you saw how the steps from data acquisition to model
    training are often iterative and that explaining the rationale for each of your
    iterations will help in your interview answers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，您看到从数据采集到模型训练的步骤通常是迭代的，解释每次迭代的基本原理将有助于您的面试答案。
- en: To select the ML model, you need to define the ML task. To figure this out,
    you can ask yourself what algorithm to use and what task(s) is associated with
    the algorithm. For example, is it classification or regression?
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择机器学习模型，你需要定义机器学习任务。为了弄清楚这一点，你可以问自己要使用什么算法以及与该算法相关的任务是什么。例如，它是分类还是回归？
- en: 'There is no prescriptive method to tell you the correct algorithms, but generally
    you’d want to know:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 目前没有固定的方法能告诉你正确的算法，但通常你需要知道：
- en: Do you have enough data?
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否有足够的数据？
- en: Are you predicting a quantity/numerical value or a category/categorical value?
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是在预测数量/数值还是类别/分类值？
- en: Do you have labeled data (i.e., you know the ground truth labels)? This could
    determine if supervised learning or unsupervised learning is better for the task.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否有标记数据（即你知道地面真实标签）？这可能决定监督学习还是无监督学习对于任务更好。
- en: Tasks could include regression, classification, anomaly detection, recommender
    systems, reinforcement learning, natural language processing, generative AI, and
    so on, all of which you read about in [Chapter 3](ch03.html#technical_interviewcolon_machine_learnin).
    A simplified overview of selecting the ML task is illustrated in [Figure 4-4](#simplified_flowchart_of_ml_task_selectio).
    Knowing the goal and the data that you have available (or plan to acquire) can
    help you initially select tasks. For example, different types of ML tasks are
    better suited depending on the labeled data that is available or if the target
    variable is continuous or categorical.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 任务可能包括回归、分类、异常检测、推荐系统、强化学习、自然语言处理、生成式人工智能等，所有这些内容你都可以在[第三章](ch03.html#technical_interviewcolon_machine_learnin)中阅读到。选择机器学习任务的简化概述如图[图 4-4](#simplified_flowchart_of_ml_task_selectio)所示。了解目标和可用数据（或计划获取的数据）可以帮助你最初选择任务。例如，根据可用的标记数据或目标变量是连续的还是分类的，不同类型的机器学习任务更适合。
- en: '![Simplified flowchart of ML task selection](assets/mlin_0404.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习任务选择简化流程图](assets/mlin_0404.png)'
- en: Figure 4-4\. Simplified flowchart of ML task selection.
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 机器学习任务选择简化流程图。
- en: Overview of Model Selection
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型选择概述
- en: Now that you have an idea of the ML task, let’s move on to model selection.
    Remember that this is an iterative process, so you might not decide this in one
    go. However, you do need to select a model (or a few) as a starting point. In
    interviews, you will be asked about why you selected such and such algorithm(s)
    or model(s), and just going off gut feeling won’t be enough for a successful answer.
    As you saw in [Figure 4-4](#simplified_flowchart_of_ml_task_selectio), you already
    have a place to start from based on the ML task(s) that you defined. So let’s
    dig deeper into some common algorithms and libraries (mostly in Python) that you
    can use to implement the task.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对机器学习任务有了一个概念，让我们继续进行模型选择。请记住，这是一个迭代过程，所以你可能不会一次性做出决定。然而，你确实需要选择一个模型（或几个模型）作为起点。在面试中，你会被问及为什么选择这样或那样的算法或模型，仅凭直觉不足以得到成功的答案。正如你在[图 4-4](#simplified_flowchart_of_ml_task_selectio)中看到的那样，你已经有了一个基于你定义的机器学习任务的起点。因此，让我们深入了解一些常见的算法和库（主要是Python），你可以用来实现这些任务。
- en: 'I want to make a quick clarification on terminology: when you are selecting
    an algorithm initially, that isn’t technically *model selection* until you test
    it out and compare the performance of the resulting model. This term is often
    used interchangeably since you inevitably want to make the final decision based
    on actual model performance. As Jason Brownlee puts it in *Machine Learning Mastery*:
    “Model selection is a process that can be applied both across different types
    of models (e.g., logistic regression, SVM, KNN, etc.) and across models of the
    same type configured with different model hyperparameters (e.g., different kernels
    in an SVM).”^([5](ch04.html#ch04fn5))'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我想快速澄清一下术语：当你最初选择算法时，那并不在技术上是*模型选择*，直到你测试并比较出结果模型的性能。虽然这两个术语常常被交替使用，因为你最终希望根据实际的模型性能做出最终决定。正如杰森·布朗利在《机器学习精要》中所说：“模型选择是一个可以应用于不同类型的模型（例如逻辑回归、SVM、KNN等）以及在同一类型的模型配置不同模型超参数（例如在SVM中不同的核函数）的过程。”^([5](ch04.html#ch04fn5))
- en: 'Here are some algorithms and libraries that can be used as simple starting
    points for each task. Note that many libraries are versatile and can be used for
    multiple purposes (e.g., decision trees can be used for both classification and
    regression), but I list some simplified examples for understanding:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些可以作为每个任务简单起点的算法和库。请注意，许多库都是多功能的，可以用于多个目的（例如，决策树既可以用于分类也可以用于回归），但我列出了一些简化的示例以便理解：
- en: Classification
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 分类
- en: Algorithms include [decision trees](https://oreil.ly/EKZWI), [random forest](https://oreil.ly/IkQXJ),
    and the like. Example Python libraries to start with include [scikit-learn](https://oreil.ly/f2Frn),
    [CatBoost](https://catboost.ai), and [LightGBM](https://oreil.ly/_cFT3).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 算法包括[决策树](https://oreil.ly/EKZWI)，[随机森林](https://oreil.ly/IkQXJ)等。开始使用的示例Python库包括[scikit-learn](https://oreil.ly/f2Frn)，[CatBoost](https://catboost.ai)和[LightGBM](https://oreil.ly/_cFT3)。
- en: Regression
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 回归
- en: Algorithms include [logistic regression](https://oreil.ly/EaQdP), decision trees,
    and the like. Example Python libraries to start with are scikit-learn and [statsmodels](https://oreil.ly/ASFkP).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 算法包括[逻辑回归](https://oreil.ly/EaQdP)，决策树等。开始使用的示例Python库包括scikit-learn和[statsmodels](https://oreil.ly/ASFkP)。
- en: Clustering (unsupervised learning)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类（无监督学习）
- en: Algorithms include [k-means clustering](https://oreil.ly/VSTOe), [DBSCAN](https://oreil.ly/Dd1i0),
    and the like. An example Python library to start with is scikit-learn.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 算法包括[k-means聚类](https://oreil.ly/VSTOe)，[DBSCAN](https://oreil.ly/Dd1i0)等。一个开始使用的示例Python库是scikit-learn。
- en: Time-series prediction
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列预测
- en: Algorithms include [ARIMA](https://oreil.ly/EmD-0), [LSTM](https://oreil.ly/Ym7mh),
    and the like. Example Python libraries to start with include statsmodels, [Prophet](https://oreil.ly/xOtUh),
    Keras/[TensorFlow](https://oreil.ly/_4vBj), and so on.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 算法包括[ARIMA](https://oreil.ly/EmD-0)，[LSTM](https://oreil.ly/Ym7mh)等。开始使用的示例Python库包括statsmodels、[Prophet](https://oreil.ly/xOtUh)、Keras/[TensorFlow](https://oreil.ly/_4vBj)等。
- en: Recommender systems
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统
- en: Algorithms include matrix factorization techniques such as collaborative filtering.
    Example libraries and tools to start with include Spark’s [MLlib](https://oreil.ly/tOH7V)
    or [Amazon Personalize](https://oreil.ly/jmzwo) on AWS.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 算法包括矩阵分解技术，如协同过滤。开始使用的示例库和工具包括Spark的[MLlib](https://oreil.ly/tOH7V)或AWS上的[Amazon
    Personalize](https://oreil.ly/jmzwo)。
- en: Reinforcement learning
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习
- en: Algorithms include multiarmed bandit, Q-learning, and policy gradient. Example
    libraries to start with include [Vowpal Wabbit](https://oreil.ly/QgSWp), [TorchRL](https://oreil.ly/O7V_d)
    (PyTorch), and TensorFlow-RL.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 算法包括多臂赌博机、Q学习和策略梯度。开始使用的示例库包括[Vowpal Wabbit](https://oreil.ly/QgSWp)、[TorchRL](https://oreil.ly/O7V_d)（PyTorch）和TensorFlow-RL。
- en: Computer vision
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: Deep learning techniques are common starting points for computer vision tasks.
    [OpenCV](https://opencv.org) is an important computer vision library that also
    supports some ML models. Popular deep learning frameworks include TensorFlow,
    Keras, PyTorch, and Caffe.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术是计算机视觉任务的常见起点。[OpenCV](https://opencv.org)是一个重要的计算机视觉库，也支持一些机器学习模型。流行的深度学习框架包括TensorFlow、Keras、PyTorch和Caffe。
- en: Natural language processing
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: All the deep learning frameworks mentioned before can also be used for NLP.
    In addition, it’s common to try out transformer-based methods or find something
    on Hugging Face. Nowadays, using the OpenAI API and GPT models is also common.
    [LangChain](https://oreil.ly/t-AJ4) is a fast-growing library for NLP workflows.
    There is also Google’s recently launched [Bard](https://oreil.ly/1OjhJ).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 所有前述的深度学习框架也可以用于自然语言处理。此外，尝试基于transformer的方法或在Hugging Face找到相关内容是很常见的。如今，使用OpenAI
    API和GPT模型也很普遍。[LangChain](https://oreil.ly/t-AJ4)是一个快速增长的用于NLP工作流的库。还有谷歌最近推出的[Bard](https://oreil.ly/1OjhJ)。
- en: Tip
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If the task is from one of the well-known ML families, then there are also well-known
    algorithms specific to that task. As always, the heuristics I provide are only
    a common starting point, and you might end up trying other versatile techniques,
    such as tree-based models or ensembling.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任务来自于著名的机器学习家族之一，那么也有专门针对该任务的著名算法。我提供的启发法只是一个常见的起点，你可能会尝试其他多功能技术，如基于树的模型或集成方法。
- en: Overview of Model Training
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练概述
- en: Now that you’ve gone through the steps of defining the ML task and selecting
    an algorithm, you will start the process of model training, which includes hyperparameter
    tuning and optimizer or loss function tuning, if applicable. The goal of this
    step is to see the model get better and better by changing the parameters of the
    model itself. Sometimes, this won’t work out, and you’ll need to go back to the
    earlier stages to improve the model via the input data. This section focuses on
    tuning the model itself but not the data.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经完成了定义机器学习任务和选择算法的步骤，接下来将开始模型训练的过程，其中包括超参数调优以及优化器或损失函数的调优（如果适用）。这一步骤的目标是通过改变模型本身的参数使模型变得越来越好。有时候这并不奏效，您需要回到较早的阶段通过输入数据来改进模型。本节重点在于调优模型本身而不是数据。
- en: In interviews, it is more interesting to the employer to hear how you increased
    your model performance, rather than just that you got a high-performing model.
    In some cases, even having a low-performing model in the end can still demonstrate
    your suitability for the role if you were very thoughtful about your ML training
    process even when other factors were out of your control, such as data acquisition.
    In other cases, having a high-accuracy model doesn’t matter to the interviewer
    so much if you haven’t deployed it; it’s common to see models perform well in
    the training phase and offline evaluation but then do poorly in production or
    live scenarios.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在面试中，雇主更感兴趣的是听到您如何提高模型性能，而不仅仅是您获得了一个高性能的模型。在某些情况下，即使最终的模型表现不佳，只要您在机器学习训练过程中非常深思熟虑，即使其他因素不在您的控制范围内（如数据获取），也可以展示您适合这个角色。在其他情况下，即使您拥有高准确度的模型，如果您没有部署它，面试官可能不会太在意；通常情况下，我们会看到模型在训练阶段和离线评估中表现良好，但在生产或实时场景中表现不佳。
- en: Hyperparameter tuning
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数调优
- en: '*Hyperparameter tuning* is where you select the optimal hyperparameters for
    the model via manual tweaks, grid search, or even AutoML. Hyperparameters include
    traits or architecture of the model itself, such as learning rate, batch size,
    the number of hidden layers in a neural network, and so on. Each specific model
    might have its own parameters, such as [changepoint and seasonality prior scale](https://oreil.ly/6ydRg)
    in Prophet. The goal of hyperparameter tuning is, for example, to see if the learning
    rate is higher or if the model will converge faster and perform better.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*超参数调优*是通过手动调整、网格搜索或甚至自动机器学习来选择模型的最佳超参数。超参数包括模型本身的特性或架构，例如学习率、批量大小、神经网络中隐藏层的数量等。每个具体的模型可能会有其自己的参数，例如Prophet中的[changepoint和季节性先验尺度](https://oreil.ly/6ydRg)。超参数调优的目标是例如看看如果学习率更高或模型会更快地收敛并表现更好。'
- en: It is important to have a good system to keep track of hyperparameter-tuning
    experiments so that the experiments can be reproducible. Imagine the pain if you
    saw a model run that yielded great results, but because the edits were made directly
    to the script, you lost the exact changes and weren’t able to reproduce the good
    results! Tracking will be discussed more in [“Experiment tracking”](#experiment_tracking).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个良好的系统来跟踪超参数调优实验非常重要，这样可以保证实验的可重复性。想象一下，如果您看到一个运行良好的模型，但由于对脚本直接进行了修改，您丢失了确切的更改，无法再现好的结果会有多么痛苦！跟踪将在[“实验跟踪”](#experiment_tracking)中进一步讨论。
- en: ML loss functions
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习损失函数
- en: '*Loss functions* in ML measure the difference between the model’s predicted
    outputs and the ground truth. A goal of the model is to minimize the loss function
    since by doing so, the model is making the most accurate predictions based on
    your definition of accuracy in the model. Examples of ML loss functions include
    mean squared error (MSE) and mean absolute error (MAE).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习中的损失函数*测量模型预测输出与真实值之间的差异。模型的目标是最小化损失函数，因为这样可以根据您对模型精度的定义进行最准确的预测。机器学习损失函数的例子包括均方误差（MSE）和平均绝对误差（MAE）。'
- en: ML optimizers
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习优化器
- en: '*Optimizers* are how the ML model’s parameters are adjusted to minimize the
    loss function. Sometimes, there are options to change the optimizer; for example,
    PyTorch has [13 common optimizers](https://oreil.ly/b9o1l) to select from. Adam
    and Adagrad are popular optimizers, and it’s likely the model’s hyperparameters
    themselves are tuned to improve performance. This could be an additional lever
    to pull, depending on the structure of your model and any hypothesized reasons
    why your current optimizer isn’t working out.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*优化器* 是调整机器学习模型参数以最小化损失函数的方式。有时候，可以选择不同的优化器；例如，PyTorch 提供了[13种常见的优化器](https://oreil.ly/b9o1l)供选择。Adam
    和 Adagrad 是流行的优化器，很可能模型的超参数也被调整以提升性能。根据模型结构和当前优化器不理想的原因，这可能是额外的优化点。'
- en: Experiment tracking
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验跟踪
- en: While conducting hyperparameter tuning, you’ll need to keep track of the performance
    of each iteration of the model. You won’t be able to figure out which set of parameters
    performs better if you don’t have the records of past parameters to compare to.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行超参数调整时，你需要记录每个模型迭代的性能。如果没有过去参数记录来进行比较，你将无法找出哪组参数表现更好。
- en: A company you interview with might have tools for ML experiment tracking. Generally,
    it doesn’t matter if you have experience with the specific tool the company is
    using as long as you are aware of experiment tracking. I’ve tracked experiments
    with Microsoft Excel before, and so have many other practitioners. It is becoming
    more common, though, to use a centralized experiment-tracking platform. Examples
    include [MLflow](https://oreil.ly/RNpng), [TensorBoard](https://oreil.ly/tt-ur),
    [Weights & Biases](https://oreil.ly/gIW5j), and [Keras Tuner](https://oreil.ly/Xt1k-).
    There are many more, such as [Kubeflow](https://oreil.ly/tTNa4), [DVC](https://oreil.ly/OPFQ_),
    [Comet ML](https://oreil.ly/cig1c), and so on. For the interview, it is highly
    unlikely that it matters which exact ones you have experience in, as long as you
    are aware that you should be tracking the results somehow in a centralized location.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你面试的公司可能有机器学习实验跟踪工具。通常情况下，你是否有使用过公司具体工具的经验并不重要，只要你知道实验跟踪的重要性。我以前用 Microsoft
    Excel 进行过实验跟踪，很多其他从业者也有类似经历。然而，现在更普遍的是使用集中式的实验跟踪平台。例如，[MLflow](https://oreil.ly/RNpng)，[TensorBoard](https://oreil.ly/tt-ur)，[Weights
    & Biases](https://oreil.ly/gIW5j)，以及[Keras Tuner](https://oreil.ly/Xt1k-)。还有很多其他工具，比如[Kubeflow](https://oreil.ly/tTNa4)，[DVC](https://oreil.ly/OPFQ_)，[Comet
    ML](https://oreil.ly/cig1c)，等等。在面试中，你具体使用过哪些工具并不重要，关键是你知道应该在集中位置跟踪实验结果。
- en: Additional resource for model training
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型训练的额外资源
- en: Google has a [Google Machine Learning Education site](https://oreil.ly/BthDc)
    (free at the time of writing) for those who are interested in a more detailed
    overview; start with the [Machine Learning Crash Course](https://oreil.ly/5rJ1q)
    (focused on ML and TensorFlow and runnable on Google Colab).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Google 拥有一个[Google 机器学习教育网站](https://oreil.ly/BthDc)（在撰写时免费提供），适合对机器学习有更详细了解的人士；可以从[机器学习入门课程](https://oreil.ly/5rJ1q)开始（专注于机器学习和
    TensorFlow，并且可以在 Google Colab 上运行）。
- en: Sample Interview Questions on Model Selection and Training
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型选择和训练的样例面试问题
- en: Now that we’ve reviewed common considerations during model training, let’s look
    at some example interview questions.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经复习了模型训练过程中的常见考虑因素，让我们来看一些示例面试问题。
- en: 'Interview question 4-4: In what scenario would you use a reinforcement learning
    algorithm rather than, say, a tree-based method?'
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面试问题 4-4：在什么场景下你会选择强化学习算法而不是基于树的方法？
- en: Example answer
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 示例答案
- en: RL algorithms are useful when it’s important to learn from trial and error and
    the sequence of actions is important. RL is also useful when the outcome can be
    delayed but we want the RL agent to be continuously improving. Examples include
    game playing, robotics, recommender systems, and so on.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法在重视通过试错学习和行动顺序的场景下非常有用。当结果可能延迟出现但我们希望强化学习代理不断改进时，强化学习也很有用。例如游戏玩法、机器人技术、推荐系统等。
- en: In contrast, tree-based methods, such as decision trees or random forests, are
    useful when the problem is static and nonsequential. In other words, it’s not
    as useful to account for delayed rewards or sequential decision making, and a
    static dataset (at the time of training) is sufficient.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于树的方法（如决策树或随机森林）在问题是静态且非顺序的情况下很有用。换句话说，考虑延迟奖励或顺序决策不如对训练时的静态数据集足够重要。
- en: 'Interview question 4-5: What are some common mistakes made during model training,
    and how would you avoid them?'
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面试问题 4-5：在模型训练过程中常见的一些错误是什么，以及你如何避免它们？
- en: Example answer
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 示例答案
- en: Overfitting is a common problem, when the resulting model captures overly complex
    information in the training data and doesn’t generalize well to new observations.
    Regularization techniques^([6](ch04.html#ch04fn6)) can be used to prevent overfitting.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是一个常见问题，当生成的模型捕捉了训练数据中过于复杂的信息，并且不能很好地推广到新的观察数据时。正则化技术^([6](ch04.html#ch04fn6))可以用于防止过拟合。
- en: Not tuning common hyperparameters could cause models to not perform well since
    the default hyperparameters might (often) not work directly out of the box to
    be the best solution.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不调整常见的超参数，模型可能表现不佳，因为默认的超参数通常不会直接出箱成为最佳解决方案。
- en: Overengineering the problem could also cause issues during model training; sometimes
    it’s best to try out a simple baseline model before jumping right into very complex
    models or combinations of models.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 过度设计问题可能导致模型训练过程中出现问题；有时候，在尝试非常复杂的模型或模型组合之前，尝试简单的基准模型可能更好。
- en: 'Interview question 4-6: In what scenario might ensemble models be useful?'
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面试问题 4-6：在什么情况下集成模型可能会有用？
- en: Example answer
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 示例答案
- en: When working with imbalanced datasets, where one class significantly outnumbers
    the others, ensemble methods can help improve the accuracy of results on minority
    data classes. By using ensemble models and combining multiple models, we can avoid
    and reduce model bias toward the majority data class.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理不平衡数据集时，其中一个类别显著多于其他类别时，集成方法可以帮助提高少数类别数据的结果准确性。通过使用集成模型和结合多个模型，我们可以避免并减少模型对多数数据类别的偏向。
- en: Model Evaluation
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: Now that you’re training your model, it’s time to evaluate it and determine
    if you should continue iterating on it or conclude that it’s good enough. As an
    aside, often the business metric will and should be decided before starting the
    ML modeling. *Business metrics* include increasing click-through rate, improving
    the conversion rate of customers, or achieving higher satisfaction as measured
    by customer surveys. These metrics are not the same as the ML model metrics mentioned
    in this section; rather, they are used to see if the model performs well on the
    test dataset after being trained on the training dataset and evaluated with the
    evaluation dataset.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你正在训练你的模型，是时候评估它并确定是否应该继续迭代或者认为它已经足够好了。顺便说一句，通常应该在开始机器学习建模之前决定业务度量标准。*业务度量标准*包括提高点击率、提高客户转化率或通过客户调查来衡量更高的满意度。这些指标与本节中提到的机器学习模型指标不同，而是用于查看模型在训练数据集上训练并在评估数据集上评估后是否表现良好。
- en: Interviewers are looking for knowledge on common ways to evaluate models in
    the field. For example, time-series interview questions might expect you to know
    about mean absolute error (MAE), root mean square error (RMSE), and similar evaluation
    metrics, which were part of one of my interviews for a role in fintech. You’ll
    likely also discuss trade-offs between false positives and false negatives, a
    big part of what I encountered when I interviewed for my job in security machine
    learning. Other common expectations are knowing the variance bias trade-off and
    how to measure it, or accuracy versus precision and recall.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 面试官们希望了解在领域中评估模型的常见方法。例如，时间序列面试问题可能期望你了解平均绝对误差（MAE）、均方根误差（RMSE）以及类似的评估指标，这些是我在金融科技角色面试中遇到的一部分。你还可能讨论假阳性和假阴性之间的权衡，这是我在安全机器学习工作面试中遇到的重要部分。其他常见的期望包括了解方差偏差权衡及其测量方法，或者准确率与精确度和召回率的区别。
- en: Summary of Common ML Evaluation Metrics
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见机器学习评估指标总结
- en: Here are some common metrics used for evaluating ML models. The metrics you’ll
    choose depend on the ML task.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是用于评估机器学习模型的一些常见指标。你选择的指标将取决于机器学习任务。
- en: Note that I won’t define all the terms in this book at the risk of it turning
    into a statistics textbook, but I will define and illustrate the most common ones.
    Additional resources are included if you want to understand the rest of the metrics
    in depth.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我不会在这本书中定义所有术语，以免它变成一本统计学教科书，但我会定义和说明最常见的术语。如果你想深入了解其余的指标，还包括了额外的资源。
- en: Classification metrics
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类指标
- en: '*Classification metrics* are used to measure the performance of classification
    models. As a shorthand, note that TP = true positive, TN = true negative, FP =
    false positive, and FN = false negative, as illustrated in [Figure 4-5](#illustration_of_true_positivescomma_fals).
    Here are some other terms and values to know:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*分类指标*用于衡量分类模型的性能。简而言之，注意到TP = 真正例，TN = 真负例，FP = 假正例，FN = 假负例，如图[4-5](#illustration_of_true_positivescomma_fals)所示。以下是其他需要了解的术语和数值：'
- en: Precision = TP / (TP + FP) (as illustrated in [Figure 4-6](#illustration_of_precision_versus_recall))
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精度 = TP / (TP + FP)（如图[4-6](#illustration_of_precision_versus_recall)所示）
- en: Recall = TP / (TP + FN) (as illustrated in [Figure 4-6](#illustration_of_precision_versus_recall))
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 召回率 = TP / (TP + FN)（如图[4-6](#illustration_of_precision_versus_recall)所示）
- en: Accuracy = (TP + TN) / (TP + TN + FP + FN)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率 = (TP + TN) / (TP + TN + FP + FN)
- en: '![Illustration of true positives, false positives, false negatives, and true
    negatives](assets/mlin_0405.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![真正例、假正例、假负例和真负例的示意图](assets/mlin_0405.png)'
- en: 'Figure 4-5\. Illustration of true positives, false positives, false negatives,
    and true negatives; source: [Walber](https://oreil.ly/1oyCp), CC BY-SA 4.0, [Wikimedia
    Commons](https://oreil.ly/UJafx).'
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-5\. 真正例、假正例、假负例和真负例的示例；来源：[Walber](https://oreil.ly/1oyCp)，CC BY-SA 4.0，[维基媒体共享资源](https://oreil.ly/UJafx)。
- en: '![Illustration of precision versus recall](assets/mlin_0406.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![精度与召回率的示意图](assets/mlin_0406.png)'
- en: Figure 4-6\. Illustration of precision versus recall.
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-6\. 精度与召回率的示意图。
- en: 'With these terms, we can then construct various evaluations:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些术语，我们可以构建各种评估：
- en: Confusion matrix
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: A summary of the TP/TN/FP/FN values in matrix form (as illustrated in [Figure 4-7](#confusion_matrix_example)).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: TP/TN/FP/FN值的矩阵形式摘要（如图[4-7](#confusion_matrix_example)所示）
- en: F1 score
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数
- en: Harmonic mean of *precision* and *recall.*
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*精度*和*召回率*的调和平均数。'
- en: AUC (area under the ROC curve) and ROC (receiver operating characteristic)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: AUC（ROC曲线下的面积）和ROC（接收者操作特征曲线）
- en: The curve plots the true positive rate against the false positive rate at various
    thresholds.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线在不同阈值下绘制真正例率与假正例率。
- en: '![Confusion matrix example](assets/mlin_0407.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![混淆矩阵示例](assets/mlin_0407.png)'
- en: Figure 4-7\. Confusion matrix example.
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7\. 混淆矩阵示例。
- en: Regression metrics
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归指标
- en: '*Regression metrics* are used to measure the performance of regression models.
    Here are some terms and values to know:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*回归指标*用于衡量回归模型的性能。以下是一些需要了解的术语和数值：'
- en: 'MAE: mean absolute error (<math display="inline"><mi>M</mi><mi>A</mi><mi>E</mi><mo>(</mo><mi>y</mi><mo>,</mo><mi>ŷ</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle
    displaystyle="false"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mo>|</mo><msub><mi>y</mi><mi>i</mi></msub><mo>–</mo><msub><mi>ŷ</mi><mi>i</mi></msub><mo>|</mo></math>)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAE：平均绝对误差（<math display="inline"><mi>M</mi><mi>A</mi><mi>E</mi><mo>(</mo><mi>y</mi><mo>,</mo><mi>ŷ</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle
    displaystyle="false"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mo>|</mo><msub><mi>y</mi><mi>i</mi></msub><mo>–</mo><msub><mi>ŷ</mi><mi>i</mi></msub><mo>|</mo></math>）
- en: 'MSE: mean squared error'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MSE：均方误差
- en: 'RMSE: root mean squared error'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSE：均方根误差
- en: 'R²: R-squared'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R²：R平方
- en: Clustering metrics
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类指标
- en: '*Clustering metrics* are used to measure the performance of clustering models.
    Using clustering metrics may depend on whether you have ground truth labels or
    not. Here I assume you do not, but if you do, then classification metrics can
    also be used. Here is a list of terms to be aware of:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*聚类指标*用于衡量聚类模型的性能。使用聚类指标可能取决于是否有地面实况标签。这里假设你没有，但如果有，分类指标也可以使用。以下是需要了解的术语列表：'
- en: Silhouette coefficient
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数
- en: Measures the cohesion of an item to other items in its cluster and separation
    with items in other clusters; ranges from -1 to 1
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量物品与其所在集群中其他物品的凝聚力以及与其他集群中物品的分离度；范围从-1到1
- en: Calinski-Harabasz Index
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Calinski-Harabasz指数
- en: A score meant to determine the quality of clusters; when the score is higher,
    it means clusters are dense and well separated
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 评分用于确定集群的质量；得分越高，意味着集群更密集且分离良好
- en: Ranking metrics
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排名指标
- en: '*Ranking metrics* are used for recommender or ranking systems. Here are some
    terms to be aware of:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '*排名指标*用于推荐或排名系统。以下是需要了解的一些术语：'
- en: Mean reciprocal rank (MRR)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 平均倒数排名（MRR）
- en: Measures the accuracy of a ranking system by how high or low the first relevant
    document appears
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量排名系统的准确性，即第一个相关文档出现的高低
- en: Precision at K
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 精度在K处
- en: Calculates the proportion of recommended items at the top that are relevant
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 计算推荐项目中顶部相关的比例
- en: Normalized discounted cumulative gain (NDCG)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化折现累计增益（NDCG）
- en: Compares the importance/rank that the ML model predicted to the actual relevance
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 比较ML模型预测的重要性/排名与实际相关性
- en: 'Now that you’ve decided on the metrics (and sometimes you’ll want to use a
    few), you’ll need to implement them with code. In common ML libraries in Python,
    there are already implementations of most of the metrics mentioned, so you don’t
    have to implement them from scratch yourself. The following metrics implementations
    are good starting points:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经决定了指标（有时你会想使用几个），你需要用代码来实现它们。在Python的常见ML库中，大多数提到的指标都已经有了实现，因此你不必自己从头开始实现它们。以下指标实现是一个很好的起点：
- en: '[TensorFlow and Keras metrics](https://oreil.ly/z6UD_)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow 和 Keras 评估指标](https://oreil.ly/z6UD_)'
- en: '[Scikit-learn metrics](https://oreil.ly/CyyXE)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-learn 指标](https://oreil.ly/CyyXE)'
- en: '[MLlib metrics](https://oreil.ly/-4ZdG)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLlib 指标](https://oreil.ly/-4ZdG)'
- en: This list is not exhaustive, so take a look in the documentation of the library
    you are using. If the built-in implementations don’t fit your specific needs for
    some reason, then you can write something custom. If this comes up in the interview,
    it’s best to explain why. For example, if you wanted to mix and match a few different
    metrics from different libraries, you might have had to write some code to connect
    them all and aggregate them.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表并非详尽无遗，请查看您使用的库的文档。如果内置实现由于某些原因不符合您的特定需求，则可以编写自定义代码。如果这在面试中提到，最好解释原因。例如，如果您想要混合匹配几种不同库的不同指标，您可能需要编写一些代码来连接它们并进行聚合。
- en: Trade-offs in Evaluation Metrics
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估指标中的权衡
- en: For interviewers, it is important for you to demonstrate that you can think
    critically about ML evaluation metrics and various trade-offs. For example, using
    accuracy alone can hide a model’s flaws with its predictions on a minority class
    (a category that has very few data points compared to the majority class) if the
    model is simply very good with prediction on the majority class. In that case,
    it would be good to supplement with more metrics, such as F1 score. However, at
    times you need to explicitly make a trade-off.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于面试官来说，你展示出能够对ML评估指标和各种权衡进行批判性思考非常重要。例如，仅使用准确度可能会隐藏模型在少数类（相比于多数类数据点非常少的类别）上预测的缺陷。在这种情况下，最好补充使用更多指标，如F1分数。但有时你需要明确地做出权衡。
- en: For example, in a medical model that predicts lung cancer from X-ray scan images,
    false negatives will have a very high impact. Hence, reducing false negatives
    is desirable. When false negatives are reduced, the recall metric is increased
    (see the previous section for a definition). But in some situations, on the way
    to reducing false negatives, the model may have accidentally learned to classify
    more patients as positive even if they do not have lung cancer. In other words,
    false positives have increased as an indirect result and decreased the model’s
    precision.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在医疗模型中，从X光扫描图像预测肺癌时，假阴性将会产生非常高的影响。因此，减少假阴性是可取的。当减少假阴性时，召回率指标会增加（请参阅前一节的定义）。但在某些情况下，在减少假阴性的过程中，模型可能会意外地学会将更多的患者分类为阳性，即使他们并没有肺癌。换句话说，假阳性作为间接结果增加了，降低了模型的精确度。
- en: Thus, it is important to decide on trade-offs between false positives and false
    negatives; in some cases, the juice could be worth the squeeze, and sometimes,
    it might not be. It will be helpful if you can discuss trade-offs like this when
    you answer interview questions.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在做出假阳性和假阴性之间的权衡时非常重要；在某些情况下，得失比可能是值得的，而有时则可能不值得。当你回答面试问题时，如果能够讨论这类权衡将会很有帮助。
- en: Tip
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The interviewer can tell from your thoughtful answers that you can think critically
    about biases in the models and can select appropriate models and metrics, which
    makes you a more effective ML practitioner.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 面试官可以从你思考周到的答案中看出，你能够对模型中的偏差进行批判性思考，并能够选择适当的模型和指标，这使你成为更有效的ML从业者。
- en: Additional Methods for Offline Evaluation
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离线评估的附加方法
- en: Using the model metrics I previously outlined, you can measure how effective
    the model has become at predicting previously unseen labels as compared to ground
    truth labels that were hidden from the model. Hopefully, you’ve experimented with
    a few tweaks to get here; even if your first model ended up being the best-performing
    one as measured by metrics, it’s worth it to see what’s not working. Your interviewer
    might ask about it, too!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我之前概述的模型指标，您可以衡量模型在预测之前未见标签方面的有效性，与模型隐藏的地面真实标签相比。希望您已经进行了一些调整实验；即使您的第一个模型以度量标准衡量的效果最佳，也值得查看哪些方面不起作用。您的面试官可能也会询问这方面的内容！
- en: 'Before the model is deployed, though, it’s difficult to confirm that the model
    will indeed perform well live, in production. In this case, “live” means it’s
    out in the world, similar to being “live on air.” *Production* refers to software
    systems running with real inputs and outputs. There are many reasons why the model
    might perform poorly in production despite doing well on model metrics: data distribution
    in the real world is sometimes not captured by the training data, and there are
    edge cases and outliers, and so on.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在模型部署之前，确认模型在实际生产中表现良好是很困难的。在这种情况下，“实时”意味着它已经在世界上运行，类似于“现场直播”。*生产*是指具有实际输入和输出的软件系统。尽管在模型指标上表现良好，但在生产环境中可能表现不佳的原因有很多：真实世界中的数据分布有时无法由训练数据捕捉，存在边界情况和异常值等。
- en: These days, many employers look for experience with understanding how models
    might behave in production. This is different from a school or academic perspective
    because with real inputs, models behaving poorly will cause real losses to a business.
    For example, a poor fraud-detection model could cost a bank millions. A recommender
    system that keeps surfacing irrelevant or inappropriate content could cause customers
    to lose trust in a company. In some cases, the company could be sued. Your interviewers
    will be keen to see if this is something you are aware of and if you have given
    thought to how to prevent scenarios like these.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，许多雇主寻求具备了解模型在生产环境中可能表现的经验。这与学校或学术角度不同，因为在实际输入中，表现不佳的模型将给企业带来真正的损失。例如，一种糟糕的欺诈检测模型可能会给银行造成数百万的损失。一个推荐系统如果一直推荐无关或不适当的内容，可能会导致客户对公司失去信任。在某些情况下，公司可能会被起诉。面试官将会很关注你是否意识到这一点，并且是否考虑过如何预防这些情况的发生。
- en: On the other hand, it’s quite fulfilling to work in ML knowing that if the model
    is successful, it could be part of preventing losses of millions from fraud or
    could be working behind the scenes of your favorite music streaming app!
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在机器学习中工作是非常令人满足的，因为如果模型成功，它可以成为防止数百万欺诈损失的一部分，或者可以在你最喜爱的音乐流媒体应用程序的背后运行！
- en: 'You can further evaluate the models before they go live in production and gauge
    if the models are indeed robust and can generalize to new data. Methods to do
    this include:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型投入生产之前，您可以进一步评估模型，并评估模型是否确实强大且能够推广到新数据。可以使用以下方法来实现这一点：
- en: Perturbation tests^([7](ch04.html#ch04fn7))
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 扰动测试^([7](ch04.html#ch04fn7))
- en: Introduce some noise or transform the test data. For example, for images, see
    if randomly adding some pixels will cause the model to be unable to predict the
    correct result.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 引入一些噪声或转换测试数据。例如，对于图像，看看随机添加一些像素是否会导致模型无法预测正确的结果。
- en: Invariance tests
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 不变性测试
- en: Test if an ML model performs consistently in different conditions. For example,
    removing or changing certain inputs shouldn’t lead to drastic changes in the output.
    If you remove one feature completely and the model makes different predictions,
    then you should consider investigating that feature. This is especially important
    if that feature is, or is related to, sensitive information, such as race or demographics.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 测试机器学习模型是否在不同条件下表现一致。例如，删除或更改某些输入不应导致输出发生 drastica 更改。如果完全删除一个特征后，模型做出不同的预测，那么您应考虑调查该特征。如果该特征是或与敏感信息（如种族或人口统计信息）相关，则尤为重要。
- en: Slice-based evaluation
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 基于切片的评估
- en: Test your model performance on various slices, or subgroups, of the test split.
    For example, your model might be performing well overall on metrics like accuracy
    and F1, but when you investigate, it is performing poorly on people over the age
    of 35 and people under the age of 15\. This will be important to investigate and
    iterate on, especially if you’ve overlooked some groups while training.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试切分的各种片段或子组上测试您的模型性能。例如，您的模型在总体精度和F1等指标上表现良好，但当您进行调查时，发现在35岁以上和15岁以下的人群中表现不佳。这将是重要的调查和迭代对象，特别是在训练过程中可能忽略了某些群体时。
- en: For more on these evaluation techniques, please see [*Designing Machine Learning
    Systems*](https://oreil.ly/JVYBI) by Chip Huyen (O’Reilly).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多有关这些评估技术的信息，请参阅Chip Huyen的《*设计机器学习系统*》（https://oreil.ly/JVYBI）（O'Reilly）。
- en: Model Versioning
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型版本管理
- en: The goal of model evaluation is to see whether a model is performing well enough,
    or if it’s performing better than the baseline or another ML model. After each
    model training, you will have various model artifacts, such as the model definition,
    model parameters, data snapshot, and so on. When you want to pick out the model
    that has been performing well, it’s more convenient if the output model artifacts
    can be easily retrieved. Having model versioning is more convenient than running
    the entire model training pipeline to regenerate the model artifacts, even if
    you know the specific hyperparameters that led to said model.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估的目标是查看模型是否表现足够好，或者是否比基线或其他ML模型表现更好。每次模型训练后，您将拥有各种模型工件，例如模型定义、模型参数、数据快照等。当您希望挑选出表现良好的模型时，如果输出的模型工件可以轻松检索，将更为便利。与重新运行整个模型训练管道以重新生成模型工件相比，具有模型版本管理功能更为便利，即使您知道导致该模型的特定超参数。
- en: The tools used for experiment tracking (listed in [“Experiment tracking”](#experiment_tracking)
    earlier in this chapter) often support model versioning as well.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本章前面已列出的实验跟踪工具（见[“实验跟踪”](#experiment_tracking)）通常也支持模型版本管理。
- en: Sample Interview Questions on Model Evaluation
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估中的样本面试问题
- en: Now that we’ve gone through common techniques and considerations for model evaluation,
    let’s look at some example interview questions.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了模型评估的常见技术和注意事项，让我们来看一些示例面试问题的内容。
- en: 'Interview question 4-7: What is the ROC metric, and when is it useful?'
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面试问题4-7：什么是ROC指标，何时使用它？
- en: Example answer
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 示例回答
- en: The ROC (receiver operating characteristic) curve can be used to evaluate a
    binary classification model. The curve plots the true positive rate against the
    false positive rate at various thresholds—the threshold being the probability
    between 0 and 1, above which the prediction is considered to be that class. For
    example, if the threshold is set to 0.6, then the probability predictions of the
    model that are above 0.6 probability of being class 1 will be labeled as class
    1.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ROC（接收者操作特征）曲线可用于评估二元分类模型。该曲线在不同阈值下绘制真正率与假正率之间的关系——阈值是介于0和1之间的概率，超过该概率的模型预测将被标记为该类。例如，如果将阈值设定为0.6，则模型的概率预测大于0.6的类1概率将被标记为类1。
- en: Using ROC can help us determine the trade-off in the true positive rate and
    the false positive rate at various thresholds, and we can then decide what is
    the optimal threshold to use.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ROC可以帮助我们确定在各种阈值下真正率和假正率的权衡，并且我们可以决定使用什么是最佳阈值。
- en: 'Interview question 4-8: What is the difference between precision and recall;
    when would you use one over the other in a classification task?'
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面试问题4-8：精确度和召回率有什么区别；在分类任务中何时使用其中之一？
- en: Example answer
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 示例回答
- en: '*Precision* measures the accuracy of the model at making correct predictions
    (quality), and *recall* measures the model’s accuracy in terms of how many relevant
    items are predicted correctly (quantity). Mathematically, precision is TP / (TP
    + FP) while recall is TP / (TP + FN).'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确度*（*Precision*）衡量模型在进行正确预测时的准确性（质量），*召回率*（*Recall*）衡量模型在正确预测相关项的数量方面的准确性。在数学上，精确度为
    TP / (TP + FP)，而召回率为 TP / (TP + FN)。'
- en: Precision can be more important than recall when it is more critical to reduce
    FPs and keep them low. One example is malware detection or email spam detection,
    where too many false positives can lead to user distrust. FPs in email spam detection
    can move legitimate business emails to the spam folder, causing delays and loss
    of business.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 当关键是减少FP并将其保持低时，精度比召回更重要。一个例子是恶意软件检测或电子邮件垃圾邮件检测，在那里太多的假阳性可能会导致用户不信任。电子邮件垃圾邮件检测中的假阳性可以将合法的业务邮件移至垃圾邮件文件夹，导致延误和业务损失。
- en: On the other hand, recall can be more important than precision in high-stakes
    predictions such as medical diagnostics. Increased recall means that there are
    fewer false negatives, even if that potentially causes some accidental FPs. In
    this situation, it’s a higher priority to minimize the chances of missing true
    cases.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在高风险预测（如医疗诊断）中，召回率比精确率更重要。增加召回率意味着较少的假阴性，即使这可能导致一些意外的假阳性。在这种情况下，最重要的是尽量减少错过真实案例的机会。
- en: 'Interview question 4-9: What is the NDCG (normalized discounted cumulative
    gain), explained on a high level? What type of ML task is it used for?'
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面试问题 4-9：高层次解释NDCG（归一化折扣累积增益）是什么？它适用于哪种类型的机器学习任务？
- en: Example answer
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 示例答案
- en: NDCG is used to measure the quality of ranking tasks, such as recommender systems,
    information retrieval, and search engines/applications. It compares the importance/rank
    that the ML model predicted to the actual relevance. If the model’s predictions
    differ a lot from the actual (or ideal) relevance, such as showing products at
    the top of a shopping website that the customer isn’t interested in, then the
    score will be lower. NDCG is calculated via the sum of the predicted relevance
    scores (DCG, or discounted cumulative gain) divided by the IDCG (ideal discounted
    cumulative gain). This is then normalized so that the result is between 0 and
    1.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: NDCG用于衡量排名任务的质量，如推荐系统、信息检索和搜索引擎/应用程序。它比较了机器学习模型预测的重要性/排名与实际相关性之间的关系。如果模型的预测与实际（或理想）相关性差异很大，例如在购物网站的顶部显示客户不感兴趣的产品，则得分会较低。NDCG通过预测相关性得分（DCG，折扣累积增益）之和除以IDCG（理想折扣累积增益）来计算。然后对结果进行归一化，使其在0到1之间。
- en: Summary
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, I walked through an overview of the ML modeling and training
    process and how each step relates to the ML interview. First, you defined the
    ML task and acquired suitable data. Next, you selected the model based on which
    algorithms were suitable for the task as a starting point. You also selected a
    baseline model, starting with something simple to compare any further ML models
    against, such as a heuristic-based method or an as-simple-as-possible model like
    logistic regression.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我概述了机器学习建模和训练过程的概述，以及每个步骤如何与机器学习面试相关联。首先，您定义了机器学习任务并获取了合适的数据。接下来，您根据任务的适用算法选择了模型作为起点。您还选择了一个基准模型，从简单的东西开始，以便比较任何进一步的机器学习模型，比如基于启发式方法或尽可能简单的模型，比如逻辑回归。
- en: In all these steps, it’s important to note in an interview how you *iterated*
    on the process to make the model better, which could even involve going back to
    a previous step, such as data acquisition. While answering interview questions
    about your experience with ML model training on your own projects, whether that’s
    a school, personal, or work project, it’s essential to talk about the trade-offs
    you faced and the reasons why you thought a certain technique would help.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些步骤中，在面试中注意如何通过迭代过程来改进模型非常重要，这甚至可能涉及返回到以前的步骤，比如数据获取。在回答有关您在自己的项目上进行机器学习模型训练经验的面试问题时，无论是学校、个人还是工作项目，都需要谈论您面临的权衡以及为什么认为某种技术会有所帮助。
- en: Simply having a highly accurate model (as measured on the test set) isn’t enough,
    as it matters a lot these days to employers that the ML candidate has exposure
    to how models might behave in production. If you are applying for an ML role that
    builds the production pipelines and infrastructure, then it’s even more important.
    Finally, you reviewed how to evaluate the ML models and select the best one.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅拥有高度准确的模型（如测试集上的测量）是不够的，因为对于雇主来说，重要的是了解应聘者如何在生产中使用模型。如果您申请的是构建生产管道和基础设施的机器学习角色，那么这一点就更加重要。最后，您审查了如何评估机器学习模型并选择最佳模型。
- en: 'In the next chapter, I’ll discuss the next major component of ML technical
    interviews: coding.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我将讨论机器学习技术面试的下一个主要组成部分：编码。
- en: ^([1](ch04.html#ch04fn1-marker)) Let’s assume that for this project, there exists
    some public dataset that is well suited for this problem.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#ch04fn1-marker)) 假设在这个项目中，存在适合这个问题的一些公共数据集。
- en: ^([2](ch04.html#ch04fn2-marker)) Keep in mind any licensing, copyright, and
    privacy issues.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#ch04fn2-marker)) 请记住任何许可、版权和隐私问题。
- en: ^([3](ch04.html#ch04fn3-marker)) “Unstructured Data,” MongoDB, accessed October
    24, 2023, [*https://oreil.ly/3DqzA*](https://oreil.ly/3DqzA).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch04.html#ch04fn3-marker)) “非结构化数据”，MongoDB，访问时间2023年10月24日，[*https://oreil.ly/3DqzA*](https://oreil.ly/3DqzA)。
- en: ^([4](ch04.html#ch04fn4-marker)) Sampling techniques are discussed in [Chapter 3](ch03.html#technical_interviewcolon_machine_learnin).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch04.html#ch04fn4-marker)) 在[第3章](ch03.html#technical_interviewcolon_machine_learnin)中讨论了抽样技术。
- en: ^([5](ch04.html#ch04fn5-marker)) Jason Brownlee, “A Gentle Introduction to Model
    Selection for Machine Learning,” *Machine Learning Mastery* (blog), September
    26, 2019, [*https://oreil.ly/2ylZa*](https://oreil.ly/2ylZa).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch04.html#ch04fn5-marker)) Jason Brownlee，《机器学习模型选择的简介》，*Machine Learning
    Mastery*（博客），2019年9月26日，[*https://oreil.ly/2ylZa*](https://oreil.ly/2ylZa)。
- en: ^([6](ch04.html#ch04fn6-marker)) Mentioned in [Chapter 3](ch03.html#technical_interviewcolon_machine_learnin).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch04.html#ch04fn6-marker)) 在[第3章](ch03.html#technical_interviewcolon_machine_learnin)提到。
- en: ^([7](ch04.html#ch04fn7-marker)) This terminology is used in Chip Huyen’s book
    *Designing Machine Learning Systems* (O’Reilly), and I use the same terms for
    this section for convenience since there doesn’t seem to be a unified terminology
    but more of a high-level grouping.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch04.html#ch04fn7-marker)) 这个术语在 Chip Huyen 的书籍《设计机器学习系统》（O'Reilly）中使用，出于方便起见，在本节中我使用相同的术语，因为似乎没有统一的术语，而更多是高级别的分组。
- en: '[*OceanofPDF.com*](https://oceanofpdf.com)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[*OceanofPDF.com*](https://oceanofpdf.com)'
