- en: Chapter 5\. Using AutoML to Detect Fraudulent Transactions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。使用 AutoML 检测欺诈交易
- en: In this chapter, you build a Vertex AI AutoML model to predict whether a financial
    transaction is fraudulent or not. You will clean and explore the dataset in a
    Google Colab notebook environment before creating a managed dataset on Vertex
    AI as you did in [Chapter 3](ch03.html#machine_learning_libraries_and_framewor).
    Once you have created a managed dataset, you will use AutoML to create a classification
    model to predict if a transaction is fraudulent or not. Along the way, the chapter
    discusses classification models in general and the corresponding metrics that
    are commonly used to evaluate them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将构建一个 Vertex AI AutoML 模型来预测金融交易是否存在欺诈行为。在创建 Vertex AI 上的托管数据集之前，您将在 Google
    Colab 笔记本环境中清理和探索数据集，就像您在[第三章](ch03.html#machine_learning_libraries_and_framewor)中所做的那样。一旦创建了托管数据集，您将使用
    AutoML 创建一个分类模型来预测交易是否存在欺诈行为。在本章中，还将讨论一般的分类模型以及通常用于评估它们的相应指标。
- en: The overall workflow of this chapter is very similar to what you worked through
    in [Chapter 4](ch04.html#use_automl_to_predict_advertising_media) for the problem
    of predicting advertising media channel sales. For this reason, in many places
    in this chapter you will see more concise details where the conversations would
    be very similar. If you get stuck in these sections, please refer back to [Chapter 4](ch04.html#use_automl_to_predict_advertising_media)
    for more details.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的整体工作流程与您在[第四章](ch04.html#use_automl_to_predict_advertising_media)中解决预测广告媒体渠道销售问题时非常相似。因此，在本章的许多地方，您将看到更简明的细节，对话内容可能会非常相似。如果您在这些部分遇到困难，请参考[第四章](ch04.html#use_automl_to_predict_advertising_media)获取更多详细信息。
- en: 'The Business Use Case: Fraud Detection for Financial Transactions'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务用例：金融交易欺诈检测
- en: Your task in this chapter, as mentioned, is to build a model to predict whether
    a financial transaction is fraudulent or legitimate. Your new company is a mobile
    payment service that serves hundreds of thousands of users. Fraudulent transactions
    are fairly rare and are usually caught by other protections. However, the unfortunate
    truth is that some of these are slipping through the cracks and negatively impacting
    your users. Your company can rectify these after the fact, but there is a fear
    of losing customers due to them having to report these transactions. The goal
    is to improve the fraud-detection software that your company is using by leveraging
    machine learning (ML) to build a bespoke model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，本章中您的任务是构建一个模型，以预测金融交易是否存在欺诈或合法。您的新公司是一家为数十万用户提供移动支付服务的公司。欺诈交易相对较少，并且通常会被其他保护机制捕捉到。然而，不幸的是，一些此类交易却会溜过漏网，对您的用户造成负面影响。您的公司可以事后纠正这些问题，但存在客户因不得不报告这些交易而流失的风险。目标是通过利用机器学习（ML）构建定制模型来改进您公司正在使用的欺诈检测软件。
- en: One complicating factor will be that the corresponding dataset will be highly
    unbalanced. The vast majority of transactions are going to be legitimate transactions,
    so a simple model that predicts that all transactions are legitimate would be
    equally accurate and useless. You will need to leverage other metrics to better
    understand your model’s performance along the way.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一个复杂的因素是相应的数据集将极度不平衡。绝大多数交易将是合法交易，因此简单地预测所有交易为合法交易将同样准确且无用。您需要利用其他指标来更好地理解模型的性能。
- en: This task might normally be one that is passed to a data scientist to create
    some sort of advanced model (such as an autoencoder), but you have been tasked
    to quickly get together a benchmark model that could be used to prototype other
    parts of the fraud-detection system. That may seem like a hopeless task, but remember
    in the previous project you were able to quickly create such a prototype using
    AutoML for media channel sales prediction. So, you should feel confident that
    you are up to the challenge!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这项任务通常可能会被交给数据科学家来创建某种高级模型（例如自编码器），但您已被委托快速组建一个用于原型化欺诈检测系统其他部分的基准模型。这似乎是一个无望的任务，但请记住，在以前的项目中，您已能够快速使用
    AutoML 创建这样一个用于预测媒体渠道销售的原型。因此，您应该对自己能够应对这一挑战感到自信！
- en: Project Workflow
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目工作流程
- en: The project workflow in this chapter, illustrated in [Figure 5-1](#overall_workflow_for_the_fraud_detectio),
    will be similar to that in the previous chapter. For this reason, some details
    of the process will be omitted to avoid repetition, but feel free to refer back
    to the previous chapter as needed.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的项目工作流程，如[图5-1](#overall_workflow_for_the_fraud_detectio)，与上一章类似。因此，为避免重复，流程的某些细节将被省略，但如有需要，请随时参考前一章。
- en: '![Overall workflow for the fraud-detection project](assets/lcai_0501.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![欺诈检测项目的整体工作流程](assets/lcai_0501.png)'
- en: Figure 5-1\. Overall workflow for the fraud-detection project.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 欺诈检测项目的整体工作流程。
- en: Now that you understand the business use case and objective, you can proceed
    to data extraction and analysis as in your previous project. After the data extraction
    and analysis steps, you will upload the dataset into the AutoML platform. The
    various features (soon to be introduced) will be fed into the model. You’ll evaluate
    the AutoML results and then deploy the model to make predictions. After this activity
    is completed, you will have the benchmark model ready for the engineering team
    to start developing a better fraud-detection pipeline. And who knows, this model
    may actually be the one that goes into production.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您了解了业务用例和目标，可以像以前的项目一样进行数据提取和分析。完成数据提取和分析后，您将上传数据集到AutoML平台。各种特征（即将介绍）将被输入模型中。您将评估AutoML的结果，然后部署模型进行预测。完成这些活动后，您将为工程团队准备好基准模型，以便开始开发更好的欺诈检测管道。也许这个模型实际上会投入生产。
- en: Project Dataset
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目数据集
- en: The project dataset consists of transaction data that has been simulated to
    replicate user behavior and fraudulent transactions. This has been done using
    [PaySim](https://oreil.ly/VtIDP). PaySim is an open source tool developed by a
    group of researchers who at the time were studying scalable resource-efficient
    systems for big data analytics.^([1](ch05.html#ch01fn1))
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目数据集包含模拟的交易数据，以复制用户行为和欺诈交易。这是使用[PaySim](https://oreil.ly/VtIDP)完成的。PaySim是一组研究人员开发的开源工具，当时他们正在研究大数据分析的可伸缩资源有效系统。^([1](ch05.html#ch01fn1))
- en: Since financial transaction data can be difficult to use without exposing user
    information, your company decided to use this simulated data. Data analysts at
    your company have confirmed that the dataset that has been shared is really similar
    in distribution to the actual data that your company sees in its application,
    so you can move forward assuming that the data is representative of the real-world
    data that your company wants to leverage at prediction time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于金融交易数据可能会暴露用户信息，所以您的公司决定使用这些模拟数据。贵公司的数据分析师已确认，分享的数据集在分布上与贵公司应用程序中看到的实际数据非常相似，因此您可以继续假设该数据代表了贵公司在预测时要利用的真实世界数据。
- en: The dataset has been provided as a CSV file in Google Cloud Storage (download
    at *https://oreil.ly/n1y1X*). In your version of the dataset, there are 10 columns.
    [Table 5-1](#schema_and_field_value_information_for) gives the column names, data
    types, and some information about the possible values for these columns.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已作为CSV文件存储在Google Cloud Storage中（可在*https://oreil.ly/n1y1X*下载）。在您的数据集版本中，共有10列。[表5-1](#schema_and_field_value_information_for)提供了这些列的列名、数据类型以及有关这些列可能值的一些信息。
- en: Table 5-1\. Schema and field value information for the customer churn dataset
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-1\. 客户流失数据集的模式和字段值信息
- en: '| Column name | Column type | Notes about field values |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 列类型 | 关于字段值的备注 |'
- en: '| --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `step` | Integer | Represents number of hours since simulated data was generated
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `step` | Integer | 自生成模拟数据以来的小时数 |'
- en: '| `type` | String | Type of transaction |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `type` | String | 交易类型 |'
- en: '| `amount` | Float | Amount of transaction |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `amount` | Float | 交易金额 |'
- en: '| `nameOrig` | String | Anonymized name of customer who originated transaction
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `nameOrig` | String | 发起交易的客户的匿名化姓名 |'
- en: '| `oldbalanceOrg` | Float | Initial balance before the transaction for the
    originator |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| `oldbalanceOrg` | Float | 发起方交易前的初始余额 |'
- en: '| `newbalanceOrig` | Float | New balance after the transaction for the originator
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `newbalanceOrig` | Float | 发起方交易后的新余额 |'
- en: '| `nameDest` | String | Anonymized name of the customer who owns the destination
    account |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `nameDest` | String | 目标账户所有者的匿名化姓名 |'
- en: '| `oldbalanceDest` | Float | Initial balance before the transaction for the
    destination account |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `oldbalanceDest` | Float | 目标账户交易前的初始余额 |'
- en: '| `newbalanceDest` | Float | New balance after the transaction for the destination
    account |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| `newbalanceDest` | 浮点数 | 交易后目标账户的新余额 |'
- en: '| `isFraud` | Integer | 1 if the transaction was fraudulent and 0 otherwise
    |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| `isFraud` | 整数 | 如果交易是欺诈，则为1，否则为0 |'
- en: Exploring the Dataset Using Pandas, Matplotlib, and Seaborn
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Pandas、Matplotlib 和 Seaborn 探索数据集
- en: All of the code in this section, including some additional examples, is included
    in a Jupyter notebook in the [low-code-ai repository on GitHub](https://oreil.ly/supp-lcai).
    You will walk through how to create this notebook from the very beginning, but
    the notebook in GitHub is great to use as a resource for later independent work
    or in case you get stuck.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此节中的所有代码，包括一些额外的示例，都包含在 [GitHub 上的 low-code-ai 仓库](https://oreil.ly/supp-lcai)
    的 Jupyter 笔记本中。你将从头开始创建这个笔记本，但 GitHub 上的笔记本非常适合在独立工作或遇到困难时使用。
- en: As in [Chapter 4](ch04.html#use_automl_to_predict_advertising_media), you will
    be loading a CSV file into a Jupyter Notebook environment and analyzing and exploring
    your data using Pandas, Matplotlib, and Seaborn to get a better understanding
    of your features. You will use the information you gain from this process to select
    the best set of features for your model in AutoML.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在 [第 4 章](ch04.html#use_automl_to_predict_advertising_media) 中那样，你将在 Jupyter
    Notebook 环境中加载 CSV 文件并使用 Pandas、Matplotlib 和 Seaborn 分析和探索数据，以更好地理解你的特征。你将利用这个过程中获得的信息来选择最佳的特征集用于你的
    AutoML 模型。
- en: Loading Data into a Pandas DataFrame in a Google Colab Notebook
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Google Colab 笔记本中将数据加载到 Pandas DataFrame
- en: First, go to [*https://colab.research.google.com*](https://colab.research.google.com)
    and open a new notebook, following the process discussed in Chapters [2](ch02.html#data_is_the_first_step)
    and [4](ch04.html#use_automl_to_predict_advertising_media) for notebook creation
    in Google Colab. You may rename this notebook to a more meaningful name by clicking
    on the name and replacing the current name with a new one, say, *Fraud_Detection_Model.ipynb*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，前往 [*https://colab.research.google.com*](https://colab.research.google.com)，打开一个新的笔记本，按照第
    [2](ch02.html#data_is_the_first_step) 章和第 [4](ch04.html#use_automl_to_predict_advertising_media)
    章中讨论的过程进行操作创建 Google Colab 笔记本。你可以通过点击名称并替换当前名称为一个更有意义的名称，比如 *Fraud_Detection_Model.ipynb*，对此笔记本进行重命名。
- en: 'Next, type the following code into the first code block to import the packages
    needed to analyze and visualize the financial transactions dataset and execute
    the cell:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在第一个代码块中输入以下代码以导入分析和可视化金融交易数据集所需的包，并执行该单元格：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that the packages are imported, the next step is to load the data into
    a Pandas DataFrame:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在导入了所需的包，下一步是将数据加载到 Pandas DataFrame 中：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Your data is now loaded into the DataFrame. It is always a good idea to take
    a peek at a few rows of data before moving forward. To look at the first few rows
    of data, add the following code to a new cell and execute that cell:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的数据已经加载到 DataFrame 中了。在继续之前，查看几行数据总是一个好主意。要查看前几行数据，请将以下代码添加到新单元格中并执行该单元格：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Take a look now at the first five rows of data in the DataFrame. This will give
    you some insight into what the different columns look like. What do you notice
    about the data? Tables [5-2](#the_first_six_columns_of_output_from_th) and [5-3](#the_last_five_columns_of_output_from_th)
    show the output from this line of code.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看看 DataFrame 中的前五行数据。这将帮助你了解不同列的样子。你注意到数据有什么特点？表格 [5-2](#the_first_six_columns_of_output_from_th)
    和 [5-3](#the_last_five_columns_of_output_from_th) 显示了此行代码的输出。
- en: Table 5-2\. The first six columns of output from the `transaction_df.head()`
    statement
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 5-2\. `transaction_df.head()` 语句输出的前六列
- en: '| `step` | `type` | `amount` | `nameOrig` | `oldbalanceOrg` | `newbalanceOrig`
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `step` | `type` | `amount` | `nameOrig` | `oldbalanceOrg` | `newbalanceOrig`
    |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| `1` | `PAYMENT` | `9839.64` | `C1231006815` | `170136.0` | `160296.36` |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `PAYMENT` | `9839.64` | `C1231006815` | `170136.0` | `160296.36` |'
- en: '| `1` | `PAYMENT` | `1864.28` | `C1666544295` | `21249.0` | `19384.72` |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `PAYMENT` | `1864.28` | `C1666544295` | `21249.0` | `19384.72` |'
- en: '| `1` | `TRANSFER` | `181.00` | `C1305486145` | `181.0` | `0.00` |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `TRANSFER` | `181.00` | `C1305486145` | `181.0` | `0.00` |'
- en: '| `1` | `CASH_OUT` | `181.00` | `C84003671` | `181.0` | `0.00` |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `CASH_OUT` | `181.00` | `C84003671` | `181.0` | `0.00` |'
- en: '| `1` | `PAYMENT` | `11668.14` | `C2048537720` | `41554.0` | `29855.86` |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| `1` | `PAYMENT` | `11668.14` | `C2048537720` | `41554.0` | `29855.86` |'
- en: Table 5-3\. The last five columns of output from the `transaction_df.head()`
    statement
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 5-3\. `transaction_df.head()` 语句输出的后五列
- en: '| `nameDest` | `oldbalanceDest` | `newbalanceDest` | `isFraud` | `isFlaggedFraud`
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| `nameDest` | `oldbalanceDest` | `newbalanceDest` | `isFraud` | `isFlaggedFraud`
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| `M1979787155` | `0.0` | `0.0` | `0` | `0` |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| `M1979787155` | `0.0` | `0.0` | `0` | `0` |'
- en: '| `M204428225` | `0.0` | `0.0` | `0` | `0` |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| `M204428225` | `0.0` | `0.0` | `0` | `0` |'
- en: '| `C553264065` | `0.0` | `0.0` | `1` | `0` |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `C553264065` | `0.0` | `0.0` | `1` | `0` |'
- en: '| `C38997010` | `21182.0` | `0.0` | `1` | `0` |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `C38997010` | `21182.0` | `0.0` | `1` | `0` |'
- en: '| `M1230701703` | `0.0` | `0.0` | `0` | `0` |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `M1230701703` | `0.0` | `0.0` | `0` | `0` |'
- en: Note that so far the `step` column has a value of `1` for all of the rows you’re
    looking at. There are many reasons this could be the case. For example, all rows
    could have the same value, the data could be grouped up in some way by step, or
    it could just be a coincidence since we are only looking at five rows. In this
    case, it is actually that the data is sorted by `step` in ascending order, though
    this is not clear at all from only five rows of data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，注意`step`列在您正在查看的所有行中的值为`1`。这可能有很多原因。例如，所有行可能具有相同的值，数据可能按`step`分组，或者仅仅是一个巧合，因为我们只看了五行。在这种情况下，实际上数据是按`step`按升序排序的，尽管从仅五行数据中完全不清楚。
- en: You may also notice a pattern with the `type` column with the `oldbalanceDest`
    and `newbalanceDest` columns. In these rows, whenever there is a `PAYMENT` or
    `TRANSFER` transaction type, the `oldbalanceDest` and `newbalanceDest` columns
    are both zero. This could be a coincidence, but it is something you should explore
    in the dataset later on.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还注意到`type`列与`oldbalanceDest`和`newbalanceDest`列的模式。在这些行中，每当出现`PAYMENT`或`TRANSFER`交易类型时，`oldbalanceDest`和`newbalanceDest`列都为零。这可能是一个巧合，但您应该在稍后的数据集中探索这一点。
- en: In the last column in the DataFrame, you should notice something odd. There
    is a column, `isFlaggedFraud`, that you did not see in the original schema. There
    are a lot of reasons this could be the case, but this is another reason why basic
    data exploration can be useful. If you run across this sort of scenario in practice,
    you may have to reach back out to the person or team who shared the data so that
    you can validate whether the column should be there, and if so what it represents.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在DataFrame的最后一列中，您应该注意到一些奇怪的事情。有一列`isFlaggedFraud`，这在原始模式中是看不到的。这可能有很多原因，但这也是基本数据探索可以派上用场的另一个原因。如果在实践中遇到这种情况，您可能需要重新联系共享数据的人员或团队，以验证该列是否应该存在，以及它代表什么。
- en: In this case, you learned that this was a column that gave the output of the
    previous model’s prediction if the transaction was legitimate or fraudulent. Why
    would this be a bad feature for your model to use? Because you may not have `isFlaggedFraud`
    at prediction time. Ideally, this is coming from a model that you will deprecate
    once a new and better model has been deployed. However, this discovery is still
    useful. You can use the previous model as a benchmark to compare against to be
    sure that your new model shows improvement.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您了解到这是一个列，它显示了上一个模型预测的输出，指示交易是否合法或欺诈。为什么这会成为您的模型不应使用的不良特征？因为在预测时可能没有`isFlaggedFraud`。理想情况下，这是来自一个您将废弃的模型，一旦部署了一个新的更好的模型。然而，这一发现仍然很有用。您可以使用以前的模型作为基准进行比较，确保您的新模型显示出改进。
- en: Exploring the Dataset
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索数据集
- en: As you saw in [Chapter 4](ch04.html#use_automl_to_predict_advertising_media),
    AutoML will do a lot of the work of describing your features and performing feature
    engineering before training your model, but it is important that you still take
    the time to understand the data you are using for training your model. ML is very
    sensitive to the quality of your data. If there are problems with your data, they
    will not magically go away in the process of using AutoML.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第四章](ch04.html#use_automl_to_predict_advertising_media)中所看到的，AutoML将在训练模型之前做大量工作来描述您的特征并执行特征工程，但重要的是您仍然要花时间了解用于训练模型的数据。机器学习对数据质量非常敏感。如果您的数据存在问题，它们在使用AutoML过程中不会神奇地消失。
- en: Descriptive analysis
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述性分析
- en: 'First, use the `.info()` method on your DataFrame to get a quick understanding
    of the amount of data you are working with and the data types of the columns.
    Add the following code into a cell and execute it to do so:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在您的DataFrame上使用`.info()`方法，快速了解您正在处理的数据量和列的数据类型。将以下代码添加到一个单元格中并执行以执行此操作：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You should see that there are 6,362,620 rows of transaction data with the 11
    columns that you expect. The data types all match the expected types (with the
    `object` type corresponding to the String data type in the Pandas DataFrame).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到有 6,362,620 行交易数据和您预期的 11 列。数据类型都符合预期的类型（`object` 类型对应于 Pandas DataFrame
    中的字符串数据类型）。
- en: 'As before, the easiest way to get a quick look at the descriptive statistics
    of your dataset in Pandas is to use the `.describe()` method. Create a new cell
    in your notebook, add the following code, and execute the cell to see the descriptive
    statistics (as seen in [Figure 5-2](#rendered_output_of_transaction_dfdotdes)):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如以往一样，在 Pandas 中快速查看数据集的描述统计信息的最简单方法是使用`.describe()`方法。在您的笔记本中创建一个新单元格，添加以下代码，并执行该单元格以查看描述统计信息（如[图 5-2](#rendered_output_of_transaction_dfdotdes)所示）：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Rendered output of transaction_df.describe()](assets/lcai_0502.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![transaction_df.describe()的渲染输出](assets/lcai_0502.png)'
- en: Figure 5-2\. Rendered output of `transaction_df.describe()`.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. `transaction_df.describe()`的渲染输出。
- en: It seems like there are no null values by looking at the count for each row.
    The `step` field varies between 1 and 743 fairly uniformly. The `amount`, `oldbalanceOrg`,
    `newbalanceOrig`, `oldbalanceDest`, and `newbalanceDest` vary over a large range,
    but this is reasonably to be expected of financial transactions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来每行的计数显示没有空值。`step`字段在 1 到 743 之间变化相对均匀。`amount`、`oldbalanceOrg`、`newbalanceOrig`、`oldbalanceDest`
    和 `newbalanceDest` 在一个较大范围内变化，但这在金融交易中是可以预期的。
- en: The `isFraud` column shows an interesting feature of the dataset. The `mean`
    of a field with values of only 0 and 1 corresponds to the proportion of values
    that are equal to 1\. In this case, about 0.129% of transactions were indeed fraudulent,
    or about 1 in every 800 transactions. This represents a fairly unbalanced dataset
    that can cause issues when attempting to build a classification model. You will
    explore these issues and possible solutions in later sections, but for now continue
    to explore the data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`isFraud`列展示了数据集的一个有趣特征。一个字段只有值 0 和 1 的均值对应于值等于 1 的比例。在这种情况下，大约有 0.129% 的交易确实是欺诈交易，或者大约每
    800 笔交易中就有一笔是欺诈交易。这代表了一个相当不平衡的数据集，在尝试构建分类模型时可能会导致问题。您将在后续部分探索这些问题和可能的解决方案，但现在继续探索数据。'
- en: The `isFlaggedFraud` column has about double the number of rows flagged as fraudulent
    transactions compared with the actual amount of fraudulent transactions. Is this
    a bad thing? Not necessarily. You want to be sure you are catching the fraudulent
    transactions, and if you flag a few extra transactions and recognize them as legitimate
    transactions upon further consideration, then it is a small price to pay for capturing
    fraudulent transactions. However, if you flag too many additional transactions,
    then you will be wasting a significant amount of time and resources exploring
    legitimate transactions. These are both things you will need to consider when
    evaluating models in the near future.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`isFlaggedFraud`列中被标记为欺诈交易的行数大约是实际欺诈交易数量的两倍。这是一件坏事吗？未必。您希望能够捕捉到欺诈交易，如果在进一步考虑后将一些额外标记的交易识别为合法交易，那么这就是为了捕捉欺诈交易而付出的小代价。然而，如果标记了太多额外的交易，那么您将浪费大量时间和资源来探索合法交易。这些都是您在近期评估模型时需要考虑的因素。'
- en: 'There are a few columns missing from your descriptive analysis, though. Notice
    that you only have statistics for the numeric features (the columns of type integer
    and float), but you’re missing data from the `type`, `nameOrig`, and `nameDest`
    columns. The `describe()` method will only return the statistics for numerical
    features if there is a mix of numerical and categorical features. To see descriptive
    statistics for categorical features, limit the scope to just those columns. To
    do that, add the following code to a new cell and execute that cell:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，您的描述性分析中还缺少一些列。请注意，您仅对数值特征（整数和浮点数类型的列）进行了统计，但缺少了`type`、`nameOrig`和`nameDest`列的数据。如果数值和分类特征混合，`describe()`方法只会返回数值特征的统计信息。要查看分类特征的描述统计信息，请将范围限制为这些列。要执行此操作，请将以下代码添加到新单元格，并执行该单元格：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You are defining a list of columns you want to see the descriptive statistics
    for, and then using the notation `transaction_df[cols]`, only considering those
    columns for the `.describe()` method ([Figure 5-3](#code_and_output_of_the_describeleft_par)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在定义一个列的列表，你希望查看描述性统计信息，并使用符号`transaction_df[cols]`，仅考虑`.describe()`方法中的这些列（[图 5-3](#code_and_output_of_the_describeleft_par)）。
- en: '![Code and output of the describe() method for categorical columns](assets/lcai_0503.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![分类列描述方法的代码和输出](assets/lcai_0503.png)'
- en: Figure 5-3\. Code and output of the `describe()` method for categorical columns.
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 分类列的`describe()`方法的代码和输出。
- en: Once again, we see that there is a value for every row for each of these columns
    by comparing the `count` with the number of rows in the DataFrame. As expected,
    there are five unique values for the `type` column, but notice that the `nameOrig`
    column is almost in a one-to-one relationship with the transactions. In particular,
    there are 6,353,307 values for `nameOrig` and 6,362,620 total transactions. This
    is not a good sign for this being a useful feature for building a good model.
    Why? Because if there is a feature that identifies a row, or nearly identifies
    a row, then you risk the model only looking at that value and not learning more
    complex relationships between the features. This seems to be less of an issue
    for the `nameDest` column, but it’s still a risk given the large number of unique
    values.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 再次看到，通过将`count`与DataFrame中的行数进行比较，可以看到每行这些列都有一个值。预期地，`type`列有五个唯一值，但请注意`nameOrig`列几乎与交易的一对一关系。特别地，`nameOrig`列有6,353,307个值，而总交易数为6,362,620。这不是一个好的迹象，用于构建良好模型的特征。为什么呢？因为如果有一个特征可以识别一行或几乎识别一行，那么你的模型可能只看这个值而不会学习更复杂的特征关系。对于`nameDest`列来说，这似乎不是那么大的问题，但考虑到唯一值的数量很大，仍然存在风险。
- en: Tip
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: AutoML will perform feature engineering and selection for you, but if you can
    give it a better set of features to start with, you will still see better performance.
    For example, you could re-create the `nameOrig` and `nameDest` features as a Boolean-valued
    column where the value is `True` if the value is repeated more than once and `False`
    otherwise. Is this the right approach? Maybe. To verify this, you should reach
    out to domain experts and experiment with different feature transformations to
    see what works best. You will learn more about feature engineering as a whole
    when working through upcoming chapters.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML将为您执行特征工程和选择，但如果您可以提供一个更好的特征集开始，您仍然会看到更好的性能。例如，您可以重新创建`nameOrig`和`nameDest`特征作为布尔值列，其中如果该值重复超过一次，则该值为`True`，否则为`False`。这是正确的方法吗？也许是。要验证这一点，您应该与领域专家联系，并尝试不同的特征转换，看看哪种方法最有效。在处理接下来的章节时，您将更多地了解整体特征工程。
- en: Exploratory analysis
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索性分析
- en: 'Before visualizing certain features of the data, it is worth exploring how
    well the previous model is performing. One easy way to do this is by creating
    a new column, which you will call `isCorrect`, which is a simple 1 if `isFraud`
    and `isFlaggedFraud` are the same and 0 otherwise. To compute this new column
    and count the number of times that the old model correctly predicted fraudulent
    transactions, execute the following code in a new cell:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在可视化数据的某些特征之前，值得探索前一个模型的表现如何。一个简单的方法是创建一个新列，称为`isCorrect`，如果`isFraud`和`isFlaggedFraud`相同，则为1，否则为0。要计算这个新列并计算旧模型正确预测欺诈交易的次数，请在新单元格中执行以下代码：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should see the result of `6354423`, but what does that actually mean? In
    Python, the Boolean data type has two possible values: `True` and `False`. When
    you use the `sum` function, it treats the `True` value as 1 and the `False` value
    as 0\. The result that you see then is the number of `True` values from the statement
    `transaction_df[''isFraud''] == transaction_df[''isFlaggedFraud'']`. This statement
    returns `True` when the columns have the same value and `False` otherwise. Recall
    that there were `6362620` rows of data total, which means that 99.87% of transactions
    were correctly flagged.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到`6354423`的结果，但这实际上意味着什么呢？在Python中，布尔数据类型有两个可能的值：`True`和`False`。当你使用`sum`函数时，它将`True`值视为1，`False`值视为0。因此，你看到的结果是从语句`transaction_df['isFraud']
    == transaction_df['isFlaggedFraud']`中`True`值的数量。当这些列具有相同值时返回`True`，否则返回`False`。回想一下总共有`6362620`行数据，这意味着99.87%的交易被正确标记。
- en: 'So, the old model was a really good model, right? Most of the transactions
    are not fraudulent, and those are the ones that you are most worried about. But
    consider another related question: How many fraudulent transactions did it correctly
    predict? You can figure that out by asking where the `isFraud` column has a value
    of `1` and the `isCorrect` column also has a value of `1`. Execute the following
    code in a new cell to see the results:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，旧模型是一个真正好的模型，对吧？大多数交易都不是欺诈交易，而这些是您最担心的交易。但请考虑另一个相关的问题：它正确预测了多少笔欺诈交易？您可以通过询问`isFraud`列的值为`1`并且`isCorrect`列也为`1`来找出答案。在新单元格中执行以下代码以查看结果：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Why does that line of code tell you the number of fraudulent transactions that
    were successfully flagged? The `isFraud` column has a value of 1 only if the transaction
    was fraudulent, and the `isCorrect` column is `True` (which is treated as `1`
    for arithmetic) if the prediction was correct. So, multiplying the values of these
    columns will only give 1 if both columns have a value of 1—otherwise that product
    will be 0\. Adding up the number of 1s gives the total number of correctly flagged
    fraudulent transactions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这行代码告诉您成功标记的欺诈交易数量？如果交易是欺诈的，`isFraud`列的值只有1，而且如果预测是正确的，则`isCorrect`列为`True`（在算术中视为`1`）。因此，将这些列的值相乘只有在两列的值都为1时才会得到1，否则乘积将为0。将1的数量相加即可得到正确标记的欺诈交易总数。
- en: The number of correctly marked fraudulent transactions was 16\. By using the
    code `transaction_df['isFraud'].sum()`, you see that the total number of fraudulent
    transactions was 8,213\. That means only 0.19% of fraudulent transactions were
    successfully flagged. This model, which looked great when we saw the overall accuracy,
    did very poorly when we asked a slightly different, maybe more relevant question.
    This is an example of a metric known as *recall*. When selecting the model objective
    and considering evaluation metrics later in the chapter, you will explore how
    to interpret the different available metrics in AutoML.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 16个正确标记的欺诈交易。通过使用代码`transaction_df['isFraud'].sum()`，您可以看到总共有8,213笔欺诈交易。这意味着只有0.19%的欺诈交易被成功标记。当我们询问一个略有不同、可能更相关的问题时，这个模型表现非常糟糕。这是一个被称为*召回率*的指标的例子。在选择模型目标并在本章后面考虑评估指标时，您将探讨如何解释AutoML中可用的不同指标。
- en: 'The next way that you can visualize the data is to create a bar chart for categorical
    features to understand the percentage of fraudulent transactions by value. For
    example, for the `type` feature, there are five values. To create such a bar chart,
    use the following code in a new cell to create the visualization:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过创建条形图来可视化数据的另一种方式，以了解每个值的欺诈交易百分比。例如，对于`type`特征，有五个值。要创建这样的条形图，请在新单元格中使用以下代码来创建可视化效果：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This line of code groups the rows by the value of `type` and takes the mean
    of the `isFraud` column. Recall that since `isFraud` has a value of 1 or 0, this
    is equivalent to the percentage of values that are 1\. You then use the `plot.bar()`
    method to plot the results as a bar graph, as shown in [Figure 5-4](#bar_graph_of_percentage_of_fraudulent_t).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码根据`type`的值对行进行分组，并取`isFraud`列的平均值。请记住，由于`isFraud`的值为1或0，这相当于值为1的百分比。然后，使用`plot.bar()`方法将结果绘制成条形图，如[图 5-4](#bar_graph_of_percentage_of_fraudulent_t)所示。
- en: '![Bar graph of percentage of fraudulent transactions versus type of transaction](assets/lcai_0504.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![欺诈交易百分比的条形图与交易类型](assets/lcai_0504.png)'
- en: Figure 5-4\. Bar graph of percentage of fraudulent transactions versus type
    of transaction.
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 欺诈交易百分比的条形图与交易类型。
- en: 'You should notice something interesting immediately. There are only two transaction
    types in your dataset that have fraudulent transactions: `CASH_OUT` and `TRANSFER`.
    You can confirm this with the following line of code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该立即注意到一些有趣的事情。您的数据集中只有两种交易类型有欺诈交易：`CASH_OUT`和`TRANSFER`。您可以使用以下代码确认这一点：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That outputs the number of times each value combination appears. The output
    you should see will be similar to [Table 5-4](#output_from_the_value_countsleft_parent).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出每个值组合出现的次数。您应该看到的输出将类似于[表 5-4](#output_from_the_value_countsleft_parent)。
- en: Table 5-4\. Output from the `value_counts()` function
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-4\. `value_counts()`函数的输出
- en: '| `type` | `isFraud` |   |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `type` | `isFraud` |   |'
- en: '| --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `CASH_IN` | `0` | `1399284` |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `CASH_IN` | `0` | `1399284` |'
- en: '| `CASH_OUT` | `0` | `2233384` |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `CASH_OUT` | `0` | `2233384` |'
- en: '|   | `1` | `4116` |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|   | `1` | `4116` |'
- en: '| `DEBIT` | `0` | `41432` |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `DEBIT` | `0` | `41432` |'
- en: '| `PAYMENT` | `0` | `2151495` |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `PAYMENT` | `0` | `2151495` |'
- en: '| `TRANSFER` | `0` | `528812` |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `TRANSFER` | `0` | `528812` |'
- en: '|   | `1` | `4097` |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|   | `1` | `4097` |'
- en: In this case you can confirm that fraudulent transactions only appear for the
    transaction types mentioned. That means—as long as this dataset is really representative
    of the transactions your company sees—this could be a very useful feature. However,
    if fraudulent transactions of new types start appearing, your model will almost
    certainly miss them because it never saw such an example.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您可以确认欺诈交易仅出现在提到的交易类型中。这意味着——只要这个数据集真正代表了贵公司看到的交易——这可能是一个非常有用的特征。然而，如果新类型的欺诈交易开始出现，您的模型几乎肯定会错过它们，因为它从未见过这样的例子。
- en: Note
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: It will be important to monitor the transaction types for fraudulent transactions
    over time and consider retraining the model as needed. This conversation around
    model monitoring and continuous training is beyond the scope of this book, but
    it’s certainly a concept worth being aware of.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移监控欺诈交易的交易类型，并根据需要重新训练模型将是非常重要的。关于模型监控和持续训练的讨论超出了本书的范围，但这绝对是一个值得关注的概念。
- en: Another useful way to visualize data with categorical labels is to bucketize
    the numeric features and see for each bucket what percentage of rows is fraudulent
    versus legitimate. *Bucketization*, or *discretization*, is the process of splitting
    a numeric variable into value ranges or “buckets” and assigning each element to
    one of the buckets. You did this in [Chapter 4](ch04.html#use_automl_to_predict_advertising_media)
    when you created histograms as well. You can do this for exploring the amount
    of the transaction and the new and old balance features for the originator and
    destination accounts.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可视化具有分类标签数据的有用方法是将数值特征分桶，并查看每个桶中欺诈行为与合法行为的百分比。*桶化*，或称为*离散化*，是将数值变量分割成值范围或“桶”的过程，并将每个元素分配到一个桶中。您在[第4章](ch04.html#use_automl_to_predict_advertising_media)中创建直方图时也做过这样的操作。您可以用这种方法来探索交易金额以及原始账户和目标账户的新旧余额特征。
- en: 'For example, suppose you want to visualize the percentage of different ranges
    of the `amount` feature:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您想可视化`amount`特征不同范围的百分比：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Before looking at the output, take a moment to parse that first line of code.
    You are creating a new column, `amountBkts`, in your DataFrame that will contain
    the bucket information. The `pd.qcut()` function assigns buckets based on quantiles.
    You are bucketizing the `amount` column into 10 buckets. The first bucket contains
    the data from the 0th to the 10th percentile, the second contains the data from
    the 10th to the 20th percentile, and so on (see [Figure 5-5](#percentage_of_fraudulent_transaction_v)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看输出之前，先花点时间解析代码的第一行。您正在为DataFrame创建一个名为`amountBkts`的新列，该列将包含桶信息。`pd.qcut()`函数基于分位数分配桶。您正在将`amount`列分成10个桶。第一个桶包含从0到10分位数的数据，第二个包含从10到20分位数的数据，依此类推（参见[图5-5](#percentage_of_fraudulent_transaction_v)）。
- en: '![Percentage of fraudulent transaction versus decile ranges for the amount
    column](assets/lcai_0505.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![交易金额列的欺诈交易百分比与十分位数范围](assets/lcai_0505.png)'
- en: Figure 5-5\. Percentage of fraudulent transactions versus decile ranges for
    the `amount` column.
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5\. 交易金额列的欺诈交易百分比与十分位数范围。
- en: You can see a general trend here that hints at something you may have intuitively
    considered. The larger the transaction amount, the more likely it is that the
    transaction is fraudulent. In particular, you see that there is a sevenfold jump
    in the fraudulent transaction rate for transactions with an amount above the 90th
    percentile.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里看到一个普遍的趋势，这暗示了您可能直觉上考虑过的一些内容。交易金额越大，交易很可能是欺诈的可能性就越大。特别是，您会看到在金额超过90分位数的交易中，欺诈交易率增加了七倍。
- en: 'To look at the same visualization for the `oldbalanceOrg` column, add the following
    code into a new cell and execute that cell:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看`oldbalanceOrg`列的同样可视化效果，请将以下代码添加到一个新的单元格并执行该单元格：
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There is a new argument, `duplicates='drop'`, in the `pd.qcut()` function. This
    argument is here to merge duplicate buckets into a single bucket. In this case,
    the 30th percentile of the `oldbalanceOrg` column is 0, so the first three buckets
    would be identical. These buckets are merged into the 30th to 40th percentile
    bucket instead.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.qcut()`函数中有一个新的参数`duplicates=''drop''`。该参数用于将重复的桶合并为一个单独的桶。在这种情况下，`oldbalanceOrg`列的第30百分位数为0，因此前三个桶将相同。这些桶被合并到第30到40百分位的桶中。'
- en: As you can see in [Figure 5-6](#percentage_of_fraudulent_transaction), there
    is definitely some sort of relationship between `oldbalanceOrg` and `isFraud`.
    The overall trend is that the higher the old balance in the originator account,
    the higher the chance of fraud—but only up to a certain point. This chance of
    fraud decreases once the value is in the 90th to 100th percentile bucket.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在[图5-6](#percentage_of_fraudulent_transaction)中所见，`oldbalanceOrg`和`isFraud`之间显然存在某种关系。总体趋势是，发起账户的旧余额越高，欺诈的可能性也越高，但仅限于某个点。这种欺诈可能性在数值处于第90到100百分位区间时会降低。
- en: '![Percentage of fraudulent transaction versus decile ranges for the oldbalanceOrg
    column](assets/lcai_0506.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![旧发起余额列的欺诈交易百分比与十分位范围](assets/lcai_0506.png)'
- en: Figure 5-6\. Percentage of fraudulent transactions versus decile ranges for
    the `oldbalanceOrg` column.
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6\. 旧发起余额列的欺诈交易百分比与十分位范围。
- en: As an exercise, explore the relationship between the other numeric features
    and the label in a similar fashion. The code for these examples is available in
    the GitHub notebook, and the corresponding visualizations can be found in Figures
    [5-7](#percentage_of_fraudulent_transactio), [5-8](#percentage_of_fraudulent_transacti),
    and [5-9](#percentage_of_fraudulent_transaction_ve).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个练习，以类似的方式探索其他数值特征与标签之间的关系。这些示例的代码可在GitHub笔记本中找到，并且对应的可视化可以在图[5-7](#percentage_of_fraudulent_transactio)、[5-8](#percentage_of_fraudulent_transacti)和[5-9](#percentage_of_fraudulent_transaction_ve)中找到。
- en: '![Percentage of fraudulent transaction versus decile ranges for the newbalanceDest
    column](assets/lcai_0507.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![旧目标余额列的欺诈交易百分比与十分位范围](assets/lcai_0507.png)'
- en: Figure 5-7\. Percentage of fraudulent transactions versus decile ranges for
    the `newbalanceDest` column.
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7\. 新目标余额列的欺诈交易百分比与十分位范围。
- en: '![Percentage of fraudulent transaction versus decile ranges for the newbalanceOrig
    column](assets/lcai_0508.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![新起源余额列的欺诈交易百分比与十分位范围](assets/lcai_0508.png)'
- en: Figure 5-8\. Percentage of fraudulent transactions versus decile ranges for
    the `newbalanceOrig` column.
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-8\. 新起源余额列的欺诈交易百分比与十分位范围。
- en: '![Percentage of fraudulent transaction versus decile ranges for the oldbalanceDest
    column](assets/lcai_0509.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![旧目标余额列的欺诈交易百分比与十分位范围](assets/lcai_0509.png)'
- en: Figure 5-9\. Percentage of fraudulent transactions versus decile ranges for
    the `oldbalanceDest` column.
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-9\. 旧目标余额列的欺诈交易百分比与十分位范围。
- en: 'There are other ways to explore transactions. For example, you know that only
    two types of transactions historically have been fraudulent. You can create a
    scatterplot for one of these types, say `CASH_OUT`, where you plot the `newbalanceDest`
    versus the `oldbalanceDest` and then color the points depending on whether they
    were fraudulent or not. Use the following code to create this visualization:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 探索交易的其他方法有很多。例如，你知道历史上只有两种类型的交易是欺诈性的。你可以为其中一种类型，比如`CASH_OUT`，创建一个散点图，其中你将`newbalanceDest`与`oldbalanceDest`进行绘制，然后根据它们是否欺诈性着色。使用以下代码创建此可视化：
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The first line applies two filters. The first filter pulls rows with the `type`
    of `CASH_OUT` and all transactions where the `newbalanceDest` is less than `1e8`
    or 100,000,000\. The second line is about building the scatterplot. We set the
    `x` and `y` columns, the column for coloring the points (`isFraud`), and a color
    map `YlOrRd` to make it easier to see the points. This specific color map assigns
    a darker color (red) to `isFraud=1` and a lighter color (yellow) to `isFraud=0`.
    For other color maps, see the [documentation for Matplotlib](https://oreil.ly/oDW76).
    Finally, since you are plotting many points, you set `alpha=0.1` to add a little
    bit of transparency to the points. This makes the visualization easier to parse
    (see [Figure 5-10](#a_plot_of_newbalancedest_versus_oldbala)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行应用了两个过滤器。第一个过滤器提取了 `type` 为 `CASH_OUT` 且 `newbalanceDest` 小于 `1e8` 或 100,000,000
    的所有交易。第二行是关于构建散点图的。我们设置了 `x` 和 `y` 轴列，用于着色点的列 (`isFraud`)，以及一个颜色映射 `YlOrRd`，使点更易于分辨。这种特定的颜色映射将
    `isFraud=1` 分配为较深的颜色（红色），将 `isFraud=0` 分配为较浅的颜色（黄色）。有关其他颜色映射，请参阅[Matplotlib 文档](https://oreil.ly/oDW76)。最后，由于要绘制许多点，我们设置
    `alpha=0.1` 以在点上添加少量透明度。这使得可视化更易于解析（见[图 5-10](#a_plot_of_newbalancedest_versus_oldbala)）。
- en: '![A plot of newbalanceDest versus oldbalanceDest for CASH_OUT transaction type
    colored by the value of isFraud](assets/lcai_0510.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图：以 CASH_OUT 交易类型为基础，绘制 newbalanceDest 相对于 oldbalanceDest 的图，根据 isFraud 的值进行着色](assets/lcai_0510.png)'
- en: Figure 5-10\. A plot of `newbalanceDest` versus `oldbalanceDest` for `CASH_OUT`
    transaction type colored by the value of `isFraud`.
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-10\. 以 `CASH_OUT` 交易类型为基础，绘制 `newbalanceDest` 相对于 `oldbalanceDest` 的图，根据
    `isFraud` 的值进行着色。
- en: There seems to be a region where the points are closer to red than yellow, or
    where there are more fraudulent transactions than legitimate transactions. This
    is a good sign that there is some sort of cross-correlation between these two
    columns. This relationship can be captured in a few different ways, such as *feature
    crossing*, which AutoML will use as part of the feature selection and engineering
    process. Feature crossing is the process of creating synthetic features by concatenating,
    or crossing, two or more preexisting categorical features. You will explore manually
    performing this process in [Chapter 8](ch08.html#improving_custom_model_performance)
    when working with custom code models.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来有一些区域的点比黄色更接近红色，或者存在更多的欺诈交易而非合法交易。这表明这两列之间存在某种交叉相关性。这种关系可以通过一些不同的方式捕捉，如*特征交叉*，AutoML
    将在特征选择和工程过程中使用它。特征交叉是通过连接两个或更多预先存在的分类特征来创建合成特征的过程。在处理自定义代码模型时，您将在[第 8 章](ch08.html#improving_custom_model_performance)手动执行此过程。
- en: As an exercise, continue to visually explore the dataset using the tools you
    have learned about in this chapter and the previous chapter. See if you can find
    any interesting new insights that you may have not expected.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，继续使用本章和前一章介绍的工具直观地探索数据集。看看是否能发现一些意想不到的新见解。
- en: Exporting the Dataset
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导出数据集
- en: After exploring the dataset, you are ready to export it into a form that will
    ultimately be usable by Vertex AI AutoML. You will see a reminder here of how
    to do this, but note that the data is already stored in a CSV file in Google Cloud
    Storage and stored in a table on BigQuery, so for the next section you don’t need
    to load the exported dataset. If you are interested in learning more about loading
    data into BigQuery, this will be one of the steps in the project you work through
    in [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索数据集之后，您可以将其导出为 Vertex AI AutoML 最终可用的形式。您将在这里看到如何执行此操作的提醒，但请注意，数据已经存储在 Google
    Cloud Storage 中的 CSV 文件和存储在 BigQuery 表中，因此在下一节中，您无需加载导出的数据集。如果您有兴趣了解如何将数据加载到 BigQuery
    中，这将是您在项目中完成的步骤之一，详见[第 6 章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)。
- en: 'To export data from a Pandas DataFrame to a CSV file, you can use the following
    code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 Pandas DataFrame 中的数据导出到 CSV 文件，您可以使用以下代码：
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It is always a good practice to check the contents of the CSV file after it
    has been written, and you can use the `!head` command to do this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 每次在将 CSV 文件写入后，检查其内容都是一个好习惯，可以使用 `!head` 命令执行此操作：
- en: '[PRE14]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To download the file, follow the process from [Chapter 4](ch04.html#use_automl_to_predict_advertising_media).
    Additionally, Google Colab has a function in the `google.colab` package that can
    also download the file for you. To use this function, import the function from
    the `google.colab` package and download the file to your local machine:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载文件，请按照[第4章](ch04.html#use_automl_to_predict_advertising_media)中的过程进行操作。此外，Google
    Colab 在 `google.colab` 包中还有一个功能可以为您下载文件。要使用此功能，请从 `google.colab` 包中导入该函数，并将文件下载到您的本地机器：
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Warning
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Though downloading files programmatically is a very convenient capability of
    Google Colab, this is also why it is extremely important to go through any notebook
    that you are running and be sure that you understand what code is being run in
    the notebook. You do not want to have unexpected files downloaded to your machine
    that could be malicious in nature, and the easiest way to avoid this is to carefully
    read through the code being executed.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通过编程方式下载文件是 Google Colab 的一项非常方便的功能，但这也是为什么非常重要要仔细查看您运行的任何笔记本，并确保您理解在笔记本中运行的代码。您不希望将可能具有恶意性质的意外文件下载到您的计算机上，避免这种情况的最简单方法是仔细阅读执行的代码。
- en: Classification Models and Metrics
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类模型和度量
- en: Before you begin to train the model, let’s ask what exactly classification models
    are and how to properly evaluate them. More mathematical details will be found
    later in [Chapter 7](ch07.html#training_custom_ml_models_in_python), but with
    at least a basic understanding of classification models, you can comprehend results
    and detect troublesome behavior from your model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练模型之前，让我们先了解一下分类模型是什么，以及如何正确评估它们。更多数学细节将在[第7章](ch07.html#training_custom_ml_models_in_python)中找到，但是只要对分类模型有基本的理解，您就可以理解结果并检测模型的问题行为。
- en: A *classification model* is a model that returns a categorical output, like
    *cat, dog, fish*, or *fraudulent*. In a classification model, you know up front
    which classes you are trying to distinguish. These classes can be numbers as well;
    for example, a rating or a problem where numeric labels have been used in place
    of categorical labels. In the case of your problem in this chapter, those classes
    are `0` and `1`, representing *legitimate* and *fraudulent* transactions respectively.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*分类模型* 是一种返回分类输出的模型，比如 *猫、狗、鱼* 或 *欺诈* 。在分类模型中，您事先知道要区分的类别。这些类别也可以是数字；例如，评级或使用数值标签替代分类标签的问题。在本章中，您的问题是
    `0` 和 `1`，分别代表 *合法* 和 *欺诈* 交易。'
- en: In practice, classification models do not return the predicted class per se,
    but rather return a probability for all of the possible classes. Usually, the
    predicted class is simply the most likely class. In the case of binary classification,
    where there are two possible classes, that may not be the case. For example, suppose
    you only classify a transaction as *fraudulent* if there is at least a 50% chance
    of the transaction being fraudulent from the output of the model. You may be missing
    cases of fraud. On the other hand, if you set that *threshold* too low, say 5%,
    then you may end up flagging way too many transactions as fraudulent and thus
    spending too many resources exploring those transactions further. As you may guess,
    figuring out where exactly to place the threshold is both a modeling problem and
    a business problem.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，分类模型并不直接返回预测的类别，而是返回所有可能类别的概率。通常情况下，预测的类别只是最可能的类别。在二元分类的情况下，可能不是这样。例如，假设只有当模型输出显示交易是
    *欺诈* 的概率至少为50%时，才将其分类为 *欺诈* 。这样可能会漏掉一些欺诈案例。另一方面，如果将 *阈值* 设置得太低，例如5%，则可能会将太多的交易标记为欺诈，从而浪费大量资源进一步探索这些交易。正如您所想象的，确定阈值的确切位置既是建模问题也是业务问题。
- en: To help determine where to place this threshold, you can leverage different
    evaluation metrics to make the decision based on your business needs. Accuracy
    is a simple and easy-to-understand metric that is commonly used for classification
    models. However, you have already seen the danger in using accuracy as your main
    evaluation metric. After all, the original model was highly accurate, over 99%,
    but barely caught any of the fraudulent transactions. Accuracy is an important
    metric nonetheless, but it does not always give the complete picture.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定阈值的设置位置，您可以利用不同的评估指标根据业务需求做出决策。准确率是一种简单易懂的分类模型常用指标。然而，正如您已经看到的，单独使用准确率作为主要评估指标存在危险。毕竟，原始模型的准确率很高，超过99%，但几乎没有捕捉到任何欺诈交易。准确率仍然是一个重要的指标，但它并不能总是提供完整的画面。
- en: These sorts of issues tend to arise when you have unbalanced classes, that is,
    when one class is much more common than other classes. In your case here, recall
    that only 0.129% of transactions were fraudulent. The ratio of fraudulent to legitimate
    is nearly 1:800\. This scenario means that certain issues can easily be masked
    by metrics like accuracy. In scenarios such as fraud detection, where the *positive*
    class is very rare, accuracy quickly becomes unreliable on its own.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在类别不平衡时，即一个类别比其他类别常见得多时，往往会出现这类问题。在这里，只有0.129%的交易是欺诈的。欺诈与合法的比例接近1:800。这种情况意味着某些问题很容易被像准确率这样的指标掩盖。在诸如欺诈检测之类的场景中，*阳性*
    类别非常罕见，单靠准确率很快就变得不可靠。
- en: You can use metrics like recall and precision together with accuracy to get
    a clearer picture of your model’s performance. *Recall* can be thought of as the
    true positive rate. In this example of fraudulent transactions, the recall represents
    the percentage of fraudulent transactions that the model successfully predicts.
    *Precision* can be thought of as the probability the model is right when it predicts
    a positive case. In the case of the fraudulent transactions problem, precision
    represents the percentage of the predicted fraudulent transactions that were indeed
    fraudulent. If your goal is to proactively flag fraudulent transactions for review,
    recall would be a very important metric to consider. You would likely be willing
    to sacrifice a little bit of accuracy or precision to improve recall. Of course,
    this is still something you want to balance, as there is a resource cost to reviewing
    a large number of transactions.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以结合准确率、召回率和精确率来更清楚地了解模型的性能。*召回率* 可以视为真阳性率。在这个欺诈交易的例子中，召回率表示模型成功预测的欺诈交易的百分比。*精确率*
    可以视为模型在预测阳性案例时正确的概率。在欺诈交易问题中，精确率表示被预测为欺诈交易的交易中确实是欺诈的百分比。如果您的目标是主动标记欺诈交易进行审核，那么召回率将是一个非常重要的指标。您可能愿意在提高召回率的同时牺牲一些准确率或精确率。当然，这仍然是您需要权衡的事情，因为审核大量交易会有资源成本。
- en: Often, recall and precision are computed in terms of the confusion matrix. The
    *confusion matrix* breaks down the predictions into a table based on the predicted
    class and the actual class. An example of this is shown in [Table 5-5](#confusion_matrix_for_a_general_proble).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，召回率和精确率是基于混淆矩阵计算的。*混淆矩阵* 将预测结果按预测类别和实际类别分解为表格。如 [表 5-5](#confusion_matrix_for_a_general_proble)
    所示。
- en: Table 5-5\. Confusion matrix for a general problem
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-5\. 一般问题的混淆矩阵
- en: '|   | Predicted positives | Predicted negatives |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|   | 预测阳性 | 预测阴性 |'
- en: '| --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Actual positives | True positives (TP) | False negatives (FN) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 实际阳性 | 真阳性 (TP) | 假阴性 (FN) |'
- en: '| Actual negatives | False positives (FP) | True negatives (TN) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 实际阴性 | 假阳性 (FP) | 真阴性 (TN) |'
- en: Recall is defined as <math><mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>F</mi><mi>N</mi><mo>+</mo><mi>T</mi><mi>P</mi><mo>)</mo></mrow></mfrac></math>
    , or the percentage of actual positives that was predicted as positive. Precision
    is defined as <math><mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>P</mi><mo>)</mo></mrow></mfrac></math>
    , or the percentage of predicted positives that was actually positive. As you
    may expect, there is a balancing act between precision and recall in practice.
    Having a lower threshold for the positive class (say, *fraudulent transaction*)
    will lead to more false positives and fewer false negatives, thus lower precision
    and higher recall. On the other hand, having a higher threshold for the positive
    class will lead to more false negatives and fewer false positives, thus higher
    precision and lower recall.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率被定义为 <math><mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>F</mi><mi>N</mi><mo>+</mo><mi>T</mi><mi>P</mi><mo>)</mo></mrow></mfrac></math>
    ，即被预测为正的实际正例的百分比。精确度被定义为 <math><mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>P</mi><mo>)</mo></mrow></mfrac></math>
    ，即被预测为正的实际正例的百分比。正如你所预期的那样，在实践中，精确度和召回率之间存在一种平衡。对于正类别的阈值较低（例如*欺诈交易*），会导致更多的假阳性和更少的假阴性，因此精确度较低而召回率较高。另一方面，对于正类别的阈值较高会导致更多的假阴性和更少的假阳性，因此精确度较高而召回率较低。
- en: Tip
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Which is more important: precision or recall? Many times, the answer to this
    question comes down to a business decision. What are your priorities? For example,
    if your company wants to capture fraudulent transactions as much as possible but
    is willing to check some legitimate transactions as well, you will likely want
    to favor recall as your evaluation metric.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个更重要：精确度还是召回率？很多时候，这个问题的答案取决于业务决策。你的优先事项是什么？例如，如果你的公司希望尽可能捕获欺诈交易，但也愿意检查一些合法交易，那么你可能会倾向于将召回率作为你的评估指标。
- en: 'How do you know you are finding the right balance between precision and recall?
    One metric is known as the *F1-score*, which is the *harmonic mean* of precision
    and recall. In general, harmonic means are often considered the “best average”
    for ratios. For two numbers it is easy to write down. Let <math><mi>P</mi></math>
    be the precision and <math><mi>R</mi></math> be the recall. Then the F1-score
    can be computed using the following formula:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如何确保你在精确度和召回率之间找到了正确的平衡？一个度量标准被称为*F1分数*，它是精确度和召回率的*调和平均数*。一般来说，调和平均数通常被认为是比率的“最佳平均数”。对于两个数字，可以很容易地写下来。假设
    <math><mi>P</mi></math> 是精确度，<math><mi>R</mi></math> 是召回率。那么F1分数可以用以下公式计算：
- en: <math><mrow><mtext>F1-score</mtext> <mo>=</mo> <mfrac><mrow><mn>2</mn><mo>(</mo><mi>P</mi><mo>×</mo><mi>R</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mrow></math>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mtext>F1分数</mtext> <mo>=</mo> <mfrac><mrow><mn>2</mn><mo>(</mo><mi>P</mi><mo>×</mo><mi>R</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mrow></math>
- en: The F1-score tends to be skewed more toward the smaller of precision and recall.
    For that reason, this can be a good metric for ensuring that you don’t sacrifice
    too much of one metric to optimize another.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数倾向于更偏向于精确度和召回率中较小的那个。因此，这可以是一个很好的度量标准，以确保你不会为了优化一个而牺牲另一个。
- en: Precision and recall heavily depend on the set threshold, but also on the underlying
    model as well. How do you decide on the underlying model when your metrics (including
    accuracy) depend so heavily on the threshold? There are metrics that depend only
    on the underlying model, such as the area under the receiver operator characteristic
    curve (ROC AUC) and the area under the precision-recall curve (PR AUC). Both of
    these metrics vary between 0 and 1\. The closer in value to 1 your metric is,
    the better the model. A deep discussion on these metrics is beyond the scope of
    this chapter, but an intuitive understanding will be helpful in practice.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度和召回率在很大程度上取决于设定的阈值，但也取决于底层模型。在你的度量标准（包括准确度）如此依赖于阈值时，如何决定底层模型？有些度量标准仅依赖于底层模型，例如接收器操作特性曲线下面积（ROC
    AUC）和精确率-召回率曲线下面积（PR AUC）。这两个度量标准都在0到1之间变化。你的度量标准越接近1，模型就越好。对这些度量标准的深入讨论超出了本章的范围，但直观理解对实践很有帮助。
- en: The ROC curve plots the recall (or the true positive rate) versus the false
    positive rate, which is defined as the percentage of negative examples that were
    classified as positive. The different points on this curve correspond to different
    thresholds, thus the curve as a whole sees all of the different possible thresholds.
    The area under this curve, the ROC AUC, can be thought of as the chance that the
    model (independent of threshold) gives a higher probability that a positive example
    is indeed positive, versus a negative example. This is a really useful metric
    for classification problems, but often it is not as useful for situations where
    the dataset is unbalanced or there is a bigger cost to a false negative than a
    false positive. One could reasonably argue that this is the situation that you
    are in here, where missing a fraudulent transaction will likely be more costly
    than having to check a legitimate one.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线绘制了召回率（或真正例率）与假正例率之间的关系，假正例率定义为被错误分类为正例的负例百分比。该曲线上的不同点对应于不同的阈值，因此整体曲线涵盖了所有可能的阈值。曲线下面积，即ROC
    AUC，可以看作是模型（独立于阈值）在给出一个正例确实是正例的更高概率时的机会，与一个负例相比。这对于分类问题是一个非常有用的度量标准，但在数据集不平衡或误判一个负例的成本比误判一个正例的成本更高的情况下，通常不那么实用。在这里，可以合理地认为您所处的情况正是如此，即错过一个欺诈交易的成本可能比查看一个合法交易更高。
- en: Another metric is the PR curve. The idea is the same as the ROC curve, but here
    you plot precision versus recall instead. The PR curve tends to be a stronger
    metric in the case of unbalanced datasets. Why is this? Well, both the precision
    and recall are focused on the positive class, and if you set the positive class
    to be the rarer class, then your metrics are based on the success of predicting
    this class rather than the dataset as a whole.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个度量标准是PR曲线。其概念与ROC曲线相同，但这里是将精确率与召回率相对比。在不平衡数据集的情况下，PR曲线往往是一个更强的度量标准。为什么呢？好吧，精确率和召回率都集中在正类上，如果将正类设置为较稀少的类别，那么你的度量标准就是基于成功预测该类别而不是整个数据集。
- en: In the following section, when evaluating the model that you train in AutoML,
    you will see examples of both the ROC curve and the PR curve.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，当评估您在AutoML中训练的模型时，您将看到ROC曲线和PR曲线的示例。
- en: Using AutoML to Train a Classification Model
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AutoML训练分类模型
- en: Now that you have explored your data, you are ready to train a model using AutoML.
    Similarly to [Chapter 4](ch04.html#use_automl_to_predict_advertising_media), you
    will create a managed dataset on Vertex AI and then explore the dataset statistics.
    Then you will train the model and check model performance. Since classification
    metrics in general may be newer to you, you will spend some time studying those
    before moving on to understanding feature importances and serving predictions
    with your model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经探索了数据，可以使用AutoML训练模型了。类似于[第4章](ch04.html#use_automl_to_predict_advertising_media)，您将在Vertex
    AI上创建一个受管数据集，然后探索数据集统计信息。然后您将训练模型并检查模型性能。由于分类度量标准可能对您来说较新，您将花一些时间研究这些内容，然后再深入了解特征重要性并使用模型进行预测。
- en: Many of these steps were covered in detail in [Chapter 4](ch04.html#use_automl_to_predict_advertising_media)
    for a regression problem. For that reason you will see a lighter treatment of
    some topics in this chapter and be referred back to [Chapter 4](ch04.html#use_automl_to_predict_advertising_media)
    for more details.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤中的许多细节在[第4章](ch04.html#use_automl_to_predict_advertising_media)中详细介绍了回归问题。因此，本章节中的某些主题将以较简略的方式处理，并建议您回顾[第4章](ch04.html#use_automl_to_predict_advertising_media)以获取更多详细信息。
- en: Creating a Managed Dataset and Selecting the Model Objective
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建受管数据集并选择模型目标
- en: Go to [*console.cloud.google.com*](https://console.cloud.google.com) and then
    go to Vertex AI (either using the side menu or the search bar). From there, select
    Datasets, and once on the Datasets page click the Create button. Replace the autogenerated
    name with `fraud_detection`. Similarly to the project from [Chapter 4](ch04.html#use_automl_to_predict_advertising_media),
    you first need to choose a data type and objective. Since you are working with
    tabular data and want to solve a classification problem, you should choose Tabular
    and then “Regression/classification” as shown in [Figure 5-11](#the_required_options_for_your_new_fraud).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 前往[*console.cloud.google.com*](https://console.cloud.google.com)，然后进入 Vertex
    AI（可以使用侧边菜单或搜索栏）。从那里，选择数据集，然后在数据集页面点击创建按钮。用`fraud_detection`替换自动生成的名称。类似于[第4章](ch04.html#use_automl_to_predict_advertising_media)的项目，首先需要选择数据类型和目标。由于您处理的是表格数据并希望解决分类问题，您应该选择Tabular，然后选择“回归/分类”，如[图5-11](#the_required_options_for_your_new_fraud)所示。
- en: '![The required options for your new fraud_detection dataset](assets/lcai_0511.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![您的新fraud_detection数据集所需的选项](assets/lcai_0511.png)'
- en: Figure 5-11\. The required options for your new `fraud_detection` dataset.
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-11\. 您的新`fraud_detection`数据集所需的选项。
- en: 'Click the Create button and you will be taken to the next page, where you can
    specify the source data. Here you have two choices: the CSV file in Google Cloud
    Storage, which you used in your notebook exploration, or a table in BigQuery that
    contains the exact same data that was prepared for you in advance. The data stored
    in BigQuery will come with additional schema information—for example, the data
    types of each column—that the CSV does not contain, so you will use this approach.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 点击创建按钮，您将进入下一页，在此页面中，您可以指定源数据。这里您有两个选择：Google Cloud Storage 中的CSV文件，这是您在笔记本探索中使用的文件，或者事先为您准备好的BigQuery表中的表格。存储在BigQuery中的数据将附带额外的架构信息，例如每个列的数据类型，这是CSV文件中不包含的，因此您将使用这种方法。
- en: 'Under “Select a data source,” click the radio button for “Select a table or
    view from BigQuery” and then type in the following BigQuery path and hit the Continue
    button:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在“选择数据源”下，点击“选择BigQuery中的表或视图”的单选按钮，然后输入以下BigQuery路径，点击继续按钮：
- en: '[PRE16]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It will take around 15–20 minutes to load and preprocess the data. AutoML does
    a lot of additional preprocessing to analyze and prepare the data for the upcoming
    training job. Afterward you will be ready to explore the data in the Vertex AI
    Datasets UI.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载和预处理大约需要15-20分钟。AutoML会执行大量额外的预处理工作，以分析和准备数据以进行即将进行的训练作业。之后，您将可以在Vertex
    AI数据集 UI 中探索数据。
- en: Exploring Dataset Statistics
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索数据集统计信息
- en: Once the data has been loaded and prepared, you can move on to analyzing the
    dataset. Click the Generate Statistics button. It will take a few minutes to generate
    the statistics. Once they are generated, you can click on the various columns
    and explore the statistics. Based on your analysis from before, there should not
    be any surprises, but it is always worth taking an additional look before moving
    forward. For example, if you click the `type` column you will see the statistics
    shown in [Figure 5-12](#column_statistics_for_the_type_column_g). You will see
    that there are no missing values; there are the five distinct values that you
    saw before and the breakdown of the number of times each value appears. The new
    part here should be the nice visualization to be able to quickly visually parse
    the portion of the total number of values each value comprises.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据加载并准备就绪，您可以开始分析数据集。点击生成统计信息按钮。生成统计信息可能需要几分钟时间。生成完成后，您可以点击各个列并查看统计信息。根据之前的分析，不应该有任何意外，但在继续之前值得再看一眼。例如，如果点击`type`列，您将看到[图5-12](#column_statistics_for_the_type_column_g)中显示的统计信息。您将看到没有缺失值；有五个不同的值，这些值的出现次数的详细分解。新的部分应该是漂亮的可视化图表，可以快速地视觉解析每个值在总数中的占比。
- en: '![Column statistics for the type column generated by Vertex AI](assets/lcai_0512.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![由Vertex AI生成的type列的列统计](assets/lcai_0512.png)'
- en: Figure 5-12\. Column statistics for the `type` column generated by Vertex AI.
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-12\. 由Vertex AI生成的`type`列的列统计。
- en: Before moving on to the next section, go through each column and ensure that
    everything looks how you expect based on the analysis you did before. Pay attention
    to the data types and the statistics and ensure that there are no surprises. You
    will not find any such surprises this time around, but developing this habit is
    good, and it is a best practice to be sure before you begin any training using
    AutoML.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，请逐列检查确保所有内容与您之前分析的预期一致。特别关注数据类型和统计信息，并确保没有任何意外情况。这次您不会发现任何意外，但养成这种习惯是很好的，确保在使用AutoML进行任何训练之前进行充分的检查是最佳实践。
- en: Training the Model
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: To  train your model, click the Train New Model button and select the Other
    option (as shown in [Figure 5-13](#options_for_training_a_new_model_in_ver)).
    Be sure that Classification is chosen as the objective and AutoML is selected
    as the model training method, and hit the Continue button.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练您的模型，请点击“训练新模型”按钮，并选择“其他”选项（如[图 5-13](#options_for_training_a_new_model_in_ver)所示）。确保选择分类作为目标，并选择AutoML作为模型训练方法，然后点击“继续”按钮。
- en: '![Options for training a new model in Vertex AI AutoML](assets/lcai_0513.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![在Vertex AI AutoML中训练新模型的选项](assets/lcai_0513.png)'
- en: Figure 5-13\. Options for training a new model in Vertex AI AutoML.
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-13\. 在Vertex AI AutoML中训练新模型的选项。
- en: Next, on the “Model details” page, select “Train new model” and set the name
    of the model as `fraud_detection`. Also, choose `isFraud` as the target column.
    Your inputs should look like those shown in [Figure 5-14](#model_details_page_after_model_name_and).
    Hit the Continue button to go to the next page.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在“模型详细信息”页面上，选择“训练新模型”，将模型名称设为`fraud_detection`。同时，选择`isFraud`作为目标列。您的输入应该与[图 5-14](#model_details_page_after_model_name_and)中显示的相同。点击“继续”按钮进入下一页。
- en: '![Model details page after model name and target column are selected](assets/lcai_0514.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![模型名称和目标列选定后的模型详细信息页面](assets/lcai_0514.png)'
- en: Figure 5-14\. Model details page after model name and target column are selected.
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-14\. 模型名称和目标列选定后的模型详细信息页面。
- en: On the “Training options” page, choose which features to use to train and ensure
    that the features are treated correctly as either categorical or numeric. On the
    right side of the screen, click the minus signs beside the `isFlaggedFraud`, `nameDest`,
    and `nameOrig` features. This will remove these features from the training process.
    Ensure that the `step` feature is treated as categorical as well. This will be
    important because this represents a time period and not a numeric value. In particular,
    `step` being 10 should not be counted as twice as influential on the model as
    `step` being 5\. See [Figure 5-15](#training_options_page_after_options_are) to
    see what your page should look like before hitting the Continue button.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在“训练选项”页面上，选择要用于训练的特征，并确保这些特征被正确地视为分类或数值。在屏幕右侧，点击`isFlaggedFraud`、`nameDest`和`nameOrig`特征旁边的减号。这将从训练过程中删除这些特征。确保`step`特征也被视为分类特征。这一点非常重要，因为它代表一个时间段而不是数值。特别是，`step`为10不应被认为比`step`为5对模型具有两倍影响力。参见[图 5-15](#training_options_page_after_options_are)查看在点击“继续”按钮之前您的页面应该是什么样子。
- en: '![Training options page after options are selected](assets/lcai_0515.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![选定选项后的训练选项页面](assets/lcai_0515.png)'
- en: Figure 5-15\. Training options page after options are selected.
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-15\. 选定选项后的训练选项页面。
- en: On the final “Compute and pricing” page, you will set the training budget. As
    a note, the pricing is computed per hour of training performed. For up-to-date
    pricing, please see the [Vertex AI documentation](https://oreil.ly/x8C3f). With
    this in mind, set the budget to one hour. Be sure Early Stopping is selected since
    it will terminate the model training process once improvement is no longer being
    seen. This is very useful if you set a higher budget but ultimately end up not
    needing that time. Once you are done, click Start Training.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在最终的“计算和定价”页面上，您将设置训练预算。请注意，定价是按每小时训练时间计算的。有关最新定价，请参阅[Vertex AI文档](https://oreil.ly/x8C3f)。考虑到这一点，将预算设定为一小时。确保选择了早停功能，因为它会在不再看到改进时终止模型训练过程。如果您设置了较高的预算但最终并不需要那么多时间，这将非常有用。完成后，点击“开始训练”。
- en: After model training is completed, you will receive an email notification, and
    the model is registered in the model registry.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，您将收到一封电子邮件通知，并且该模型将注册到模型注册表中。
- en: Evaluating Model Performance
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: Once your model has completed the training process, you are ready to view the
    evaluation metrics. Navigate, either via the side menu or the search bar, to Vertex
    AI Model Registry. Click on your model, `fraud_detection`, and then click the
    corresponding version you want to view. If you are following this chapter, that
    will be version `1`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的模型完成训练过程，你就可以查看评估指标了。通过侧边菜单或搜索栏导航到 Vertex AI 模型注册表。点击你的模型 `fraud_detection`，然后点击你想查看的相应版本。如果你在跟随本章，那将是版本
    `1`。
- en: Since you care most about trying to predict fraudulent transactions, click the
    1 class (highlighted in [Figure 5-16](#model_evaluation_metrics_for_the_fraud))
    to see how your model performed on the test dataset set aside by AutoML. Note
    that the evaluation metrics for your model may differ from the one shown in this
    chapter due to randomness in the model training process.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你最关心尝试预测欺诈交易，点击图中突出显示的 1 类（在图 5-16 中）来查看你的模型在由 AutoML 分离的测试数据集上的表现。请注意，由于模型训练过程中的随机性，你的模型评估指标可能与本章中显示的有所不同。
- en: '![Model evaluation metrics for the fraud_detection model](assets/lcai_0516.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![fraud_detection 模型的模型评估指标](assets/lcai_0516.png)'
- en: Figure 5-16\. Model evaluation metrics for the `fraud_detection` model.
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-16\. `fraud_detection` 模型的模型评估指标。
- en: Note the slider for Confidence threshold. You can move this around to see how
    it affects the precision and recall. Suppose, for example, your company is willing
    to have up to 50% of flagged transactions be legitimate so that they can capture
    as much fraud as possible while still being able to have the resources to check
    the flagged transactions. For the model represented in [Figure 5-16](#model_evaluation_metrics_for_the_fraud),
    the threshold that corresponds to 50% precision is 0.06\. That is, if the model
    predicts that there is more than a 6% chance of fraud, you would want to flag
    it as fraud and check it out. Note that the recall in this scenario is 95.2%—in
    other words, only 1 in 20 fraudulent transactions would be expected to be missed
    in this scenario.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意置信阈值的滑块。你可以移动它来查看它对精确度和召回率的影响。例如，假设你的公司愿意有多达 50% 的标记交易是合法的，以便尽可能多地捕捉欺诈，同时仍然有资源检查标记的交易。对于图
    5-16 所代表的模型，对应于 50% 精确度的阈值为 0.06。也就是说，如果模型预测欺诈的概率超过 6%，你会希望将其标记为欺诈并进行检查。请注意，这种情况下的召回率为
    95.2% — 换句话说，在这种情况下，预期有 20 笔欺诈交易中只会漏掉 1 笔。
- en: The best confidence threshold will depend on your business needs and goals,
    so there is no single correct answer here.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳置信阈值将取决于你的业务需求和目标，因此在这里没有单一正确答案。
- en: Model Feature Importances
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型特征重要性
- en: If you click “All labels” on the side of the UI and then scroll down to the
    bottom of the page, you can see the feature importances for your model. Recall
    from [Chapter 4](ch04.html#use_automl_to_predict_advertising_media) that feature
    importances measure features’ impact on model predictions. Note that, as with
    the evaluation metrics, the exact importances you see for your model may differ
    from the importances that are shown in this chapter.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 UI 侧边栏点击“所有标签”，然后滚动到页面底部，你可以看到模型的特征重要性。从[第四章](ch04.html#use_automl_to_predict_advertising_media)回顾，特征重要性衡量了特征对模型预测的影响。请注意，与评估指标一样，你在模型中看到的确切重要性可能与本章显示的重要性有所不同。
- en: The feature importances for this model are shown in [Figure 5-17](#feature_importances_for_the_fraud_detec).
    The `oldbalanceOrg` and `newbalanceOrig` features were the most important features,
    and `amount` and `type` were the third and fourth, respectively. Recall, when
    you were exploring your data in the notebook environment, these features seemed
    to have the clearest patterns between the feature value and the percentage of
    fraudulent transactions. In general, these are good to check to be sure that nothing
    is possibly odd with what your model learned. It is not uncommon for a model to
    notice patterns that you may not have noticed, but it should be a red flag if
    your model seems to have learned something nonsensical. For example, if a model
    to predict taxi fare learned that the distance of the ride was the least important
    feature, in some circumstances this could be a reason to explore your data more
    closely. Feature importances are very useful both to stakeholders, to understand
    how a model makes predictions, and to data scientists and ML engineers, to understand
    how to debug models in the case that surprising or nonsensical predictions are
    being made.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的特征重要性显示在[图 5-17](#feature_importances_for_the_fraud_detec)中。`oldbalanceOrg`
    和 `newbalanceOrig` 特征是最重要的特征，而 `amount` 和 `type` 分别排在第三和第四位。回想一下，在笔记本环境中探索数据时，这些特征似乎在特征值与欺诈交易百分比之间有最清晰的模式。总的来说，检查这些特征是很重要的，以确保你的模型学到的内容没有什么异常之处。模型可能会注意到你可能没有注意到的模式，这并不罕见，但如果你的模型似乎学到了一些荒谬的东西，这应该是一个警告信号。例如，如果一个预测出租车费用的模型学到了乘车距离是最不重要的特征，在某些情况下，这可能是一个需要更仔细探索数据的原因。特征重要性对于利益相关者来说非常有用，以了解模型如何进行预测；对于数据科学家和机器学习工程师来说，它们在调试模型时理解意外或荒谬预测的方式也非常有帮助。
- en: '![Feature importances for the fraud_detection model](assets/lcai_0517.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![fraud_detection 模型的特征重要性](assets/lcai_0517.png)'
- en: Figure 5-17\. Feature importances for the `fraud_detection` model.
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-17\. `fraud_detection` 模型的特征重要性。
- en: Getting Predictions from Your Model
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从您的模型获取预测结果
- en: Now that you have seen the evaluation metrics for your model, you are ready
    to deploy the model for prediction. If you get stuck in the process for deploying
    your model, refer back to [Chapter 4](ch04.html#use_automl_to_predict_advertising_media),
    as the process in this chapter will be identical.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经看到了模型的评估指标，可以准备部署模型进行预测了。如果在部署模型的过程中遇到困难，请参考 [第 4 章](ch04.html#use_automl_to_predict_advertising_media)，因为本章的过程与该章相同。
- en: 'Complete the following steps to deploy your model, leaving all unmentioned
    options as the default value:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 完成以下步骤来部署您的模型，将所有未提及的选项保留为默认值：
- en: On the “Define your endpoint” page, name the endpoint `fraud_endpoint`.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“定义您的端点”页面上，将端点命名为 `fraud_endpoint`。
- en: On the “Model settings” page, select `n1-standard-2, 2vCPUs, 7.5GiB memory`
    as the Machine type. Using a smaller machine type will lower the cost of the deployed
    model and meet your needs in this example.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“模型设置”页面上，选择 `n1-standard-2, 2vCPUs, 7.5GiB memory` 作为机器类型。使用较小的机器类型将降低部署模型的成本，并满足本示例中的需求。
- en: On the “Model settings” page, under Explainability options, click the checkbox
    for “Enable feature attributions for this model.”
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“模型设置”页面上，在可解释性选项下，选中“为该模型启用特征归因”复选框。
- en: On the “Model monitoring” page, disable model monitoring, as you will not need
    it here.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“模型监控”页面上，禁用模型监控，因为在这里你不需要它。
- en: After completing these steps, click Deploy. The model will take a few minutes
    to deploy. Once the model has finished deploying, you can test the model on the
    bottom of the page.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，点击部署。模型将需要几分钟来部署。一旦模型完成部署，您可以在页面底部测试模型。
- en: 'Use the following values to test your model, noting that once again, your exact
    results will vary from the model being shown in [Figure 5-18](#model_testing_with_predictions_and_loca):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下值来测试您的模型，注意，再次强调，您的确切结果将与 [图 5-18](#model_testing_with_predictions_and_loca)
    中显示的模型有所不同：
- en: '| `step` | `14` |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| `step` | `14` |'
- en: '| `type` | `CASH_OUT` |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| `type` | `CASH_OUT` |'
- en: '| `amount` | `1000000` |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| `amount` | `1000000` |'
- en: '| `oldbalanceOrg` | `1000000` |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| `oldbalanceOrg` | `1000000` |'
- en: '| `newbalanceOrig` | `0` |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| `newbalanceOrig` | `0` |'
- en: '| `oldbalanceDest` | `0` |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| `oldbalanceDest` | `0` |'
- en: '| `newbalanceDest` | `0` |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| `newbalanceDest` | `0` |'
- en: '![Model testing with predictions and local feature importances](assets/lcai_0518.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![使用预测和本地特征重要性进行模型测试](assets/lcai_0518.png)'
- en: Figure 5-18\. Model testing with predictions and local feature importances.
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-18\. 使用预测和本地特征重要性进行模型测试。
- en: Once you are done, click the Predict button. To see the predicted probability
    of a fraudulent transaction from your model, change the “Selected label” to 1.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，点击“预测”按钮。要查看您模型预测的欺诈交易的概率，将“选择的标签”改为1。
- en: For the model shown in [Figure 5-18](#model_testing_with_predictions_and_loca),
    the “Confidence score” is the predicted probability. In this case, it gives a
    91% chance of the transaction being fraudulent. Based on the conversation from
    before around evaluation metrics and thresholds, this transaction would be marked
    as fraudulent. You can also see the local feature importances. The highest feature
    importance value corresponds to the `oldbalanceOrg` feature. In this case, there
    is a `CASH_OUT` transaction for the exact balance of the account with 1,000,000
    dollars in it. Based on your model, there is a high chance that this transaction
    is fraudulent.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[图 5-18](#model_testing_with_predictions_and_loca)中所示的模型，“置信度分数”是预测概率。在这种情况下，它给出了交易欺诈的可能性为91%。基于之前关于评估指标和阈值的讨论，该交易将被标记为欺诈。您还可以看到局部特征重要性。最高的特征重要性值对应于`oldbalanceOrg`特征。在这种情况下，存在一个精确余额为100万美元的账户的`CASH_OUT`交易。根据您的模型，这笔交易有很高的欺诈可能性。
- en: As an exercise, explore different combinations of feature values and see what
    the predicted probabilities and corresponding feature attributions are.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，探索不同特征值的组合，并查看预测的概率和相应的特征归因。
- en: Warning
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Do not forget to *undeploy* your model once you are finished with this chapter.
    Deployed models incur cost even when they are not being used so that they are
    always available to return quick predictions. To undeploy the model, go to Vertex
    AI Endpoints, click the endpoint name `fraud_endpoint`, click the “More actions”
    three-dot menu (as shown in [Figure 5-19](#the_location_of_the_quotation_markundep)),
    and finally click “Undeploy model from endpoint.”
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，不要忘记*取消部署*您的模型。已部署的模型即使未被使用也会产生费用，以便随时返回快速预测。取消部署模型时，请转到Vertex AI端点，点击端点名称为`fraud_endpoint`，点击“更多操作”三点菜单（如[图 5-19](#the_location_of_the_quotation_markundep)所示），最后点击“从端点中取消部署模型”。
- en: '![The location of the “Undeploy model from endpoint” option](assets/lcai_0519.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![“取消模型部署到端点”的选项位置](assets/lcai_0519.png)'
- en: Figure 5-19\. The location of the “Undeploy model from endpoint” option.
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-19\. “取消模型部署到端点”的选项位置。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: You have now trained both a regression model and a classification model over
    the past two chapters using AutoML! In this chapter you explored your data in
    a Google Colab environment, then uploaded that data to a Vertex AI managed dataset
    before training the classification model using AutoML. The recall of the original
    model was significantly lower than the recall of the new model, meaning that this
    new model will ultimately lead to better flagging of transactions and saving your
    customers’ accounts.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两章中，您已经使用AutoML训练了回归模型和分类模型！在这一章中，您在Google Colab环境中探索了您的数据，然后将数据上传到Vertex
    AI的受管理数据集，然后使用AutoML训练分类模型。原始模型的召回率明显低于新模型的召回率，这意味着这个新模型最终将更好地标记交易并保护您的客户账户。
- en: This is just the beginning of your ML journey, though. In many cases, you will
    want the ability to have more control over your model training process. In practice,
    this means moving toward more low-code and custom code solutions. The next chapter
    covers how to explore data using SQL in BigQuery and train ML models using SQL
    in BigQuery ML.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这只是您的机器学习之旅的开始。在许多情况下，您将希望能够更多地控制模型训练过程。实际上，这意味着向更低代码和自定义代码解决方案迈进。下一章将介绍如何使用BigQuery中的SQL探索数据，并使用BigQuery
    ML在SQL中训练ML模型。
- en: '^([1](ch05.html#ch01fn1-marker)) For more details, see “PaySim: A Financial
    Mobile Money Simulator for Fraud Detection” by E. A. Lopez-Rojas, A. Elmir, and
    S. Axelsson (in The 28th European Modeling and Simulation Symposium-EMSS, Larnaca,
    Cyprus, 2016).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch05.html#ch01fn1-marker))更多详情请见“PaySim: A Financial Mobile Money Simulator
    for Fraud Detection” by E. A. Lopez-Rojas, A. Elmir, and S. Axelsson (in The 28th
    European Modeling and Simulation Symposium-EMSS, Larnaca, Cyprus, 2016)。'
