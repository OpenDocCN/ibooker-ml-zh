- en: Chapter 3\. Model Explainability and Interpretability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ç«  æ¨¡å‹å¯è§£é‡Šæ€§ä¸å¯è§£é‡Šæ€§
- en: 'Making sense of a machine learning model can seem as hard as making sense of
    intelligence itself. Computer scientist Marvin Minsky famously [described â€œintelligenceâ€
    as a suitcase word](https://oreil.ly/SgFAp): â€œa word that means nothing by itself,
    but holds a bunch of things inside that you have to unpack.â€ This becomes even
    more confusing when you see models with superhuman performance in some tasks (for
    example, playing Go or chess), but that fail epically in others (for example,
    mistaking a picture of a person on a bus for a pedestrian). Machine learning is
    great at creating functions that map to complex decision spaces. Problems arise
    when you want to understand why the model made a particular decision.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£æœºå™¨å­¦ä¹ æ¨¡å‹å¯èƒ½çœ‹èµ·æ¥åƒæ˜¯ç†è§£æ™ºèƒ½æœ¬èº«ä¸€æ ·å›°éš¾ã€‚è®¡ç®—æœºç§‘å­¦å®¶é©¬æ–‡Â·æ˜æ–¯åŸºæ›¾è‘—ååœ°å°†â€œæ™ºèƒ½â€æè¿°ä¸º[â€œä¸€ä¸ªæ‰‹æç®±è¯â€](https://oreil.ly/SgFAp)ï¼šâ€œä¸€ä¸ªæœ¬èº«å¹¶ä¸ä»£è¡¨ä»»ä½•ä¸œè¥¿ï¼Œä½†å†…å«ä¸€å †ä½ å¿…é¡»è§£å¼€çš„ä¸œè¥¿ã€‚â€
    å½“ä½ çœ‹åˆ°åœ¨æŸäº›ä»»åŠ¡ä¸­å…·æœ‰è¶…äººè¡¨ç°ï¼ˆä¾‹å¦‚ï¼Œä¸‹å›´æ£‹æˆ–å›½é™…è±¡æ£‹ï¼‰ï¼Œä½†åœ¨å…¶ä»–ä»»åŠ¡ä¸­å´æƒ¨è´¥ï¼ˆä¾‹å¦‚ï¼Œå°†å…¬äº¤è½¦ä¸Šçš„äººç‰©è¯¯è®¤ä¸ºè¡Œäººï¼‰ï¼Œè¿™ä¸€åˆ‡å˜å¾—æ›´åŠ æ··ä¹±ã€‚æœºå™¨å­¦ä¹ æ“…é•¿åˆ›å»ºæ˜ å°„åˆ°å¤æ‚å†³ç­–ç©ºé—´çš„å‡½æ•°ã€‚é—®é¢˜åœ¨äºï¼Œå½“ä½ æƒ³è¦ç†è§£æ¨¡å‹ä¸ºä½•åšå‡ºç‰¹å®šå†³ç­–æ—¶ã€‚
- en: Even worse, â€œinterpretabilityâ€â€”the tool you want to use to pick apart a modelâ€”may
    count as a suitcase word itself.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ç³Ÿç³•çš„æ˜¯ï¼Œâ€œå¯è§£é‡Šæ€§â€â€”â€”ä½ æƒ³è¦ç”¨æ¥è§£ææ¨¡å‹çš„å·¥å…·â€”â€”æœ¬èº«å¯èƒ½ä¹Ÿç®—æ˜¯ä¸€ä¸ªæ‰‹æç®±è¯ã€‚
- en: Explainability Versus Interpretability
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§£é‡Šæ€§ä¸å¯è§£é‡Šæ€§çš„åŒºåˆ«
- en: '*Explainability* and *interpretability* are often used interchangeably when
    it comes to making sense of ML models and their outputs. For interpretability,
    there are at least a few non-math-heavy definitions you could use. AI researcher
    Tim Miller described it as â€œthe degree to which human beings can understand the
    cause of a decision,â€^([1](ch03.html#idm45621854762112)) while Kim et al. described
    it as â€œthe degree to which a machineâ€™s output can be consistently predicted.â€^([2](ch03.html#idm45621854760192))'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¯è§£é‡Šæ€§*å’Œ*å¯è§£é‡Šæ€§*åœ¨è§£é‡ŠMLæ¨¡å‹åŠå…¶è¾“å‡ºæ—¶é€šå¸¸å¯ä»¥äº’æ¢ä½¿ç”¨ã€‚å¯¹äºå¯è§£é‡Šæ€§ï¼Œè‡³å°‘æœ‰å‡ ä¸ªéæ•°å­¦é‡çš„å®šä¹‰å¯ä¾›é€‰æ‹©ã€‚AIç ”ç©¶å‘˜è’‚å§†Â·ç±³å‹’å°†å…¶æè¿°ä¸ºâ€œäººç±»èƒ½å¤Ÿç†è§£å†³ç­–åŸå› çš„ç¨‹åº¦â€ï¼Œ^([1](ch03.html#idm45621854762112))
    è€Œé‡‘å°šå®‡ç­‰äººå°†å…¶æè¿°ä¸ºâ€œèƒ½å¤Ÿä¸€è‡´é¢„æµ‹æœºå™¨è¾“å‡ºçš„ç¨‹åº¦â€ã€‚^([2](ch03.html#idm45621854760192))'
- en: What these definitions have in common is that they focus on the decisions that
    a model makes. Contrast this with *explainability* (sometimes referred to as Xplainable
    AI or XAI.^([3](ch03.html#idm45621854755760))) While itâ€™s often used in similar
    contexts, the term usually emphasizes the learned model internals, such as the
    weights of a neural network or the node splits of a tree.^([4](ch03.html#idm45621854753488))
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å®šä¹‰çš„å…±åŒç‚¹åœ¨äºå®ƒä»¬ä¾§é‡äºæ¨¡å‹æ‰€åšçš„å†³ç­–ã€‚ä¸*å¯è§£é‡Šæ€§*ï¼ˆæœ‰æ—¶ç§°ä¸ºXplainable AIæˆ–XAIã€‚^([3](ch03.html#idm45621854755760))ï¼‰ç›¸åã€‚å°½ç®¡å®ƒé€šå¸¸åœ¨ç±»ä¼¼çš„è¯­å¢ƒä¸­ä½¿ç”¨ï¼Œä½†è¯¥æœ¯è¯­é€šå¸¸å¼ºè°ƒå­¦ä¹ æ¨¡å‹å†…éƒ¨ï¼Œå¦‚ç¥ç»ç½‘ç»œçš„æƒé‡æˆ–æ ‘çš„èŠ‚ç‚¹åˆ†è£‚ã€‚^([4](ch03.html#idm45621854753488))
- en: Even though this distinction hasnâ€™t been formalized among researchers, weâ€™ll
    use *interpretable* as referring to model outputs and *explainable* as referring
    to model internals throughout the rest of the book.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç ”ç©¶äººå‘˜å°šæœªæ­£å¼åŒºåˆ†è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬ä¹¦çš„å…¶ä½™éƒ¨åˆ†ä¸­å°†*å¯è§£é‡Šæ€§*æŒ‡æ¶‰æ¨¡å‹è¾“å‡ºï¼Œ*å¯è§£é‡Šæ€§*æŒ‡æ¶‰æ¨¡å‹å†…éƒ¨ã€‚
- en: The Need for Interpretable and Explainable Models
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¹äºéœ€è¦å¯è§£é‡Šæ€§å’Œå¯è§£é‡Šæ€§çš„æ¨¡å‹çš„éœ€æ±‚
- en: If you have a model that can make decisions on the test data with high enough
    accuracy, surely thatâ€™s enough to deploy it, right?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æœ‰ä¸€ä¸ªèƒ½å¤Ÿåœ¨æµ‹è¯•æ•°æ®ä¸Šä»¥è¶³å¤Ÿé«˜å‡†ç¡®ç‡åšå‡ºå†³ç­–çš„æ¨¡å‹ï¼Œé‚£ä¹ˆéƒ¨ç½²å®ƒè‚¯å®šè¶³å¤Ÿäº†ï¼Œå¯¹å§ï¼Ÿ
- en: As Doshi-Velez and Kim point out,^([5](ch03.html#idm45621854740496)) getting
    an output decision from the ML model is not always the end. Consider the hypothetical
    case of using a neural network in oncology. The model can make decisions that
    could be life changing for patients. Said patients would be well within their
    legal rights to ask for more details from the doctor, and they probably wonâ€™t
    be satisfied with a response like â€œTrust me on this, we have a really good neural
    network.â€ How good that neural network is might be in doubt as well. After all,
    you would have to make sure that the computer vision model that looks at X-rays
    is actually looking at the body part in question, not looking in the corners for
    a text label mistakenly left in by a human radiologist.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚å¤šå¸Œ-éŸ¦å‹’å…¹å’Œé‡‘æ‰€æŒ‡å‡ºçš„é‚£æ ·ï¼Œ^([5](ch03.html#idm45621854740496)) ä»MLæ¨¡å‹è·å¾—è¾“å‡ºå†³ç­–å¹¶ä¸æ€»æ˜¯ç»ˆç‚¹ã€‚è€ƒè™‘ä½¿ç”¨ç¥ç»ç½‘ç»œåœ¨è‚¿ç˜¤å­¦ä¸­çš„å‡è®¾æ¡ˆä¾‹ã€‚è¯¥æ¨¡å‹å¯ä»¥åšå‡ºå¯¹æ‚£è€…å¯èƒ½å…·æœ‰ç”Ÿå‘½å˜åŒ–æ„ä¹‰çš„å†³ç­–ã€‚è¿™äº›æ‚£è€…åœ¨æ³•å¾‹ä¸Šæœ‰æƒè¦æ±‚åŒ»ç”Ÿæä¾›æ›´å¤šç»†èŠ‚ï¼Œè€Œä»–ä»¬å¯èƒ½ä¸ä¼šå¯¹â€œè¯·ç›¸ä¿¡æˆ‘ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªéå¸¸å¥½çš„ç¥ç»ç½‘ç»œâ€è¿™æ ·çš„å›ç­”æ»¡æ„ã€‚è¿™ä¸ªç¥ç»ç½‘ç»œåˆ°åº•æœ‰å¤šå¥½å¯èƒ½ä¹Ÿå€¼å¾—æ€€ç–‘ã€‚æ¯•ç«Ÿï¼Œä½ éœ€è¦ç¡®ä¿æ£€æŸ¥Xå…‰çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ç¡®å®åœ¨è§‚å¯Ÿæ‰€ç ”ç©¶çš„èº«ä½“éƒ¨ä½ï¼Œè€Œä¸æ˜¯åœ¨è§’è½é‡Œå¯»æ‰¾è¢«äººç±»æ”¾é”™çš„æ–‡æœ¬æ ‡ç­¾ã€‚
- en: Interpretability and explainability are important safeguards against this kind
    of ignorance.^([6](ch03.html#idm45621854738064)) This is especially important
    when a model is being used in a context that it hasnâ€™t encountered before, which
    is extremely important when considering a modelâ€™s fairness, privacy, and robustness.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¯è§£é‡Šæ€§å’Œå¯è§£é‡Šæ€§æ˜¯é˜²æ­¢è¿™ç§æ— çŸ¥çš„é‡è¦ä¿éšœã€‚^([6](ch03.html#idm45621854738064)) ç‰¹åˆ«æ˜¯åœ¨æ¨¡å‹è¢«ç”¨äºå…¶ä»¥å‰æœªæ›¾é‡åˆ°çš„æƒ…å¢ƒæ—¶ï¼Œè€ƒè™‘æ¨¡å‹çš„å…¬å¹³æ€§ã€éšç§æ€§å’Œé²æ£’æ€§å°¤ä¸ºé‡è¦ã€‚
- en: '[Nick Bostrom has famously postulated](https://oreil.ly/H61nH) that interpretability
    is a safeguard against creating a super-intelligent AI with goals contrary to
    those of its human creators. If you can interpret an advanced AI model and reliably
    explain its decisions, you can also reverse engineer it to make sure it does what
    you want and does not try to harm you. All the more reason to recognize the importance
    of model interpretability and explainability.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[å°¼å…‹Â·åšæ–¯ç‰¹ç½—å§†æ›¾è‘—ååœ°æå‡ºè¿‡](https://oreil.ly/H61nH)ï¼Œè§£é‡Šæ€§æ˜¯é˜²æ­¢åˆ›é€ å‡ºå…·æœ‰ä¸å…¶äººç±»åˆ›é€ è€…ç›¸æ‚–ç›®æ ‡çš„è¶…æ™ºèƒ½äººå·¥æ™ºèƒ½çš„ä¿éšœã€‚å¦‚æœæ‚¨èƒ½è§£é‡Šä¸€ä¸ªå…ˆè¿›çš„AIæ¨¡å‹å¹¶å¯é åœ°è§£é‡Šå…¶å†³ç­–ï¼Œé‚£ä¹ˆæ‚¨ä¹Ÿå¯ä»¥åå‘å·¥ç¨‹å®ƒï¼Œç¡®ä¿å®ƒæŒ‰ç…§æ‚¨çš„æ„æ„¿è¡Œäº‹ï¼Œè€Œä¸ä¼šè¯•å›¾ä¼¤å®³æ‚¨ã€‚æ›´åŠ ç†ç”±å»è®¤è¯†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå¯è§£é‡Šæ€§çš„é‡è¦æ€§ã€‚'
- en: If your project absolutely needs interpretability or explainability as a feature,
    you will need to weigh drops in interpretability against the importance of performance
    gains. Decision trees are much more intuitively explainable and interpretable
    than deep neural networks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„é¡¹ç›®ç»å¯¹éœ€è¦å¯è§£é‡Šæ€§æˆ–å¯è§£é‡Šæ€§ä½œä¸ºç‰¹å¾ï¼Œæ‚¨å°†éœ€è¦æƒè¡¡å¯è§£é‡Šæ€§çš„é™ä½ä¸æ€§èƒ½æå‡çš„é‡è¦æ€§ã€‚å†³ç­–æ ‘æ¯”æ·±åº¦ç¥ç»ç½‘ç»œæ›´ç›´è§‚è§£é‡Šå’Œå¯è§£é‡Šã€‚
- en: That being said, the performance gains granted by deep neural networks are why
    theyâ€™ve become so much more popular than decision trees.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¯è™½å¦‚æ­¤ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œå¸¦æ¥çš„æ€§èƒ½æå‡æ­£æ˜¯å®ƒä»¬æ¯”å†³ç­–æ ‘æ›´å—æ¬¢è¿çš„åŸå› ã€‚
- en: A Possible Trade-off Between Explainability and Privacy
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯èƒ½å­˜åœ¨è§£é‡Šæ€§å’Œéšç§ä¹‹é—´çš„æƒè¡¡
- en: In [ChapterÂ 1](ch01.html#chapter1), we detailed a variety of ways in which the
    internal rules or even the training dataset of a model could be stolen. Most of
    these ways involve closely inspecting the modelâ€™s decision outputs for all logits.
    The goal was to train a model to imitate the target as closely as possible. These
    attacks assume the attacker has access to more detailed information about the
    modelâ€™s decisions beyond just the final output value.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[ç¬¬1ç« ](ch01.html#chapter1)ä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†è®¨è®ºäº†æ¨¡å‹çš„å†…éƒ¨è§„åˆ™ç”šè‡³è®­ç»ƒæ•°æ®é›†å¯èƒ½è¢«çªƒå–çš„å„ç§æ–¹å¼ã€‚è¿™äº›æ–¹å¼å¤§å¤šæ¶‰åŠç´§å¯†æ£€æŸ¥æ¨¡å‹å†³ç­–è¾“å‡ºçš„æ‰€æœ‰å¯¹æ•°å‡½ã€‚ç›®æ ‡æ˜¯å°½å¯èƒ½è®­ç»ƒä¸€ä¸ªæ¨¡å‹ä»¥æ¨¡ä»¿ç›®æ ‡ã€‚è¿™äº›æ”»å‡»å‡è®¾æ”»å‡»è€…å¯ä»¥è®¿é—®æœ‰å…³æ¨¡å‹å†³ç­–æ›´è¯¦ç»†ä¿¡æ¯ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€ç»ˆè¾“å‡ºå€¼ã€‚
- en: These interpretability methods go far beyond just listing all the logits. They
    provide more insight into the internals of a model than logits ever could. Itâ€™s
    theoretically possible to create an attack mechanism based on them. For example,
    instead of training a model on a classification dataset, you could train the model
    on saliency maps taken from a model that already performed well. A determined
    attacker could create a loss function based on the KL divergence between the saliency
    maps of the two models for given inputs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è§£é‡Šæ€§æ–¹æ³•è¿œä¸æ­¢äºç®€å•åˆ—å‡ºæ‰€æœ‰çš„é€»è¾‘å›å½’å€¼ã€‚å®ƒä»¬æä¾›çš„è§è§£æ¯”é€»è¾‘å›å½’å€¼æ›´èƒ½æ·±å…¥äº†è§£æ¨¡å‹çš„å†…éƒ¨ã€‚ç†è®ºä¸Šå¯ä»¥åŸºäºå®ƒä»¬åˆ›å»ºä¸€ä¸ªæ”»å‡»æœºåˆ¶ã€‚ä¾‹å¦‚ï¼Œä¸æ˜¯åœ¨åˆ†ç±»æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè€Œæ˜¯åœ¨å·²ç»è¡¨ç°è‰¯å¥½çš„æ¨¡å‹çš„æ˜¾è‘—æ€§å›¾ä¸Šè®­ç»ƒæ¨¡å‹ã€‚ä¸€ä¸ªå†³å¿ƒåšå®šçš„æ”»å‡»è€…å¯ä»¥åŸºäºä¸¤ä¸ªæ¨¡å‹çš„ç»™å®šè¾“å…¥çš„æ˜¾è‘—æ€§å›¾ä¹‹é—´çš„KLæ•£åº¦åˆ›å»ºä¸€ä¸ªæŸå¤±å‡½æ•°ã€‚
- en: As we noted in [ChapterÂ 1](ch01.html#chapter1), the best hope for defending
    against such attacks is to limit the output rate of predictions. Itâ€™s also important
    to limit who can see the full extent of the output predictions at all. For example,
    think twice when it comes to exposing the logits to anyone other than your team.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨[ç¬¬1ç« ](ch01.html#chapter1)ä¸­æ‰€æŒ‡å‡ºçš„ï¼ŒæŠµå¾¡è¿™ç±»æ”»å‡»çš„æœ€ä½³å¸Œæœ›æ˜¯é™åˆ¶é¢„æµ‹çš„è¾“å‡ºé€Ÿç‡ã€‚åŒæ ·é‡è¦çš„æ˜¯è¦é™åˆ¶é™¤äº†ä½ çš„å›¢é˜Ÿä¹‹å¤–çš„ä»»ä½•äººçœ‹åˆ°å…¨éƒ¨é¢„æµ‹è¾“å‡ºçš„ç¨‹åº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨å‘ä»»ä½•äººå…¬å¼€é€»è¾‘å›å½’å€¼æ—¶è¦ä¸‰æ€è€Œåè¡Œã€‚
- en: While no privacy mechanism is truly perfect, limiting the audience to only those
    necessary can go a long way.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ²¡æœ‰çœŸæ­£å®Œç¾çš„éšç§æœºåˆ¶ï¼Œä½†å°†è§‚ä¼—é™åˆ¶åœ¨å¿…è¦çš„äººç¾¤ä¹‹å†…å¯ä»¥èµ°å¾—æ›´è¿œã€‚
- en: Evaluating the Usefulness of Interpretation or Explanation Methods
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯„ä¼°è§£é‡Šæˆ–è¯´æ˜æ–¹æ³•çš„å®ç”¨æ€§
- en: 'It might be overwhelming to choose which method to use. As the field has matured,
    more guidelines have emerged on how to evaluate an interpretability method. Doshi-Velez
    and Kimâ€™s three-level framework is a good example of this.^([7](ch03.html#idm45621854718720))
    The three levels they outline are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä½¿ç”¨å“ªç§æ–¹æ³•å¯èƒ½ä¼šè®©äººä¸çŸ¥æ‰€æªã€‚éšç€è¯¥é¢†åŸŸçš„æˆç†Ÿï¼Œå…³äºå¦‚ä½•è¯„ä¼°è§£é‡Šæ€§æ–¹æ³•çš„æŒ‡å¯¼æ–¹é’ˆè¶Šæ¥è¶Šå¤šã€‚Doshi-Velez å’Œ Kim çš„ä¸‰çº§æ¡†æ¶å°±æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ã€‚^([7](ch03.html#idm45621854718720))
    ä»–ä»¬æ¦‚è¿°çš„ä¸‰ä¸ªçº§åˆ«åŒ…æ‹¬ï¼š
- en: '*Application-level evaluation (real task)*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*åº”ç”¨çº§è¯„ä¼°ï¼ˆå®é™…ä»»åŠ¡ï¼‰*'
- en: If you put your model explanation into your product, will the user understand
    what itâ€™s saying? A good example of this would be a product that detects worn-down
    joints in veterinary X-rays of animals. An AI can be trained on previous radiology
    images to predict whether or not an animal is sick. An interpretable model, by
    contrast, will be able to not only communicate to the radiologist what itâ€™s predicting
    but highlight the parts of the X-ray that caused it to draw that conclusion. Itâ€™s
    worth comparing these kinds of model explanations to a human radiologist explaining
    a similar decision.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå°†ä½ çš„æ¨¡å‹è§£é‡Šæ”¾å…¥äº§å“ä¸­ï¼Œç”¨æˆ·èƒ½ç†è§£å…¶å«ä¹‰å—ï¼Ÿä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯ï¼Œäº§å“å¯ä»¥æ£€æµ‹åŠ¨ç‰©çš„å…½åŒ»Xå…‰ä¸­ç£¨æŸçš„å…³èŠ‚ã€‚é€šè¿‡å…ˆå‰çš„æ”¾å°„å­¦å›¾åƒå¯¹AIè¿›è¡Œè®­ç»ƒï¼Œä»¥é¢„æµ‹åŠ¨ç‰©æ˜¯å¦ç”Ÿç—…ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸€ä¸ªå¯è§£é‡Šçš„æ¨¡å‹ä¸ä»…èƒ½å¤Ÿå‘æ”¾å°„ç§‘åŒ»ç”Ÿè§£é‡Šå…¶é¢„æµ‹å†…å®¹ï¼Œè¿˜èƒ½çªå‡ºæ˜¾ç¤ºå¯¼è‡´å…¶å¾—å‡ºç»“è®ºçš„Xå…‰éƒ¨ä½ã€‚å€¼å¾—æ¯”è¾ƒçš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹è§£é‡Šä¸äººç±»æ”¾å°„ç§‘åŒ»ç”Ÿè§£é‡Šç±»ä¼¼å†³ç­–çš„æ–¹å¼ã€‚
- en: '*Human-level evaluation (simple task)*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*äººç±»çº§è¯„ä¼°ï¼ˆç®€å•ä»»åŠ¡ï¼‰*'
- en: This is similar to the application-level evaluation but without a specific end
    user in mind. With this kind of evaluation, you should ask whether a random person
    (not necessarily a user or domain expert) would be able to understand the modelâ€™s
    decision. If domain experts are rare and/or expensive (like the veterinarians
    in the previous example), using the judgment of an average person as a baseline
    is a possible alternative. Ideally, one should ask which of the modelâ€™s explanations
    are easiest to understand.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç±»ä¼¼äºåº”ç”¨çº§è¯„ä¼°ï¼Œä½†æ²¡æœ‰ç‰¹å®šçš„æœ€ç»ˆç”¨æˆ·è€ƒè™‘ã€‚é€šè¿‡è¿™ç§è¯„ä¼°ï¼Œä½ åº”è¯¥è¯¢é—®ä¸€ä¸ªéšæœºçš„äººï¼ˆä¸ä¸€å®šæ˜¯ç”¨æˆ·æˆ–é¢†åŸŸä¸“å®¶ï¼‰æ˜¯å¦èƒ½å¤Ÿç†è§£æ¨¡å‹çš„å†³ç­–ã€‚å¦‚æœé¢†åŸŸä¸“å®¶ç¨€ç¼ºå’Œ/æˆ–æ˜‚è´µï¼ˆå¦‚å‰é¢ä¾‹å­ä¸­çš„å…½åŒ»ï¼‰ï¼Œä½¿ç”¨æ™®é€šäººçš„åˆ¤æ–­ä½œä¸ºåŸºå‡†æ˜¯ä¸€ä¸ªå¯èƒ½çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåº”è¯¥è¯¢é—®æ¨¡å‹è§£é‡Šä¸­å“ªäº›æ˜¯æœ€å®¹æ˜“ç†è§£çš„ã€‚
- en: '*Function-level evaluation (proxy task)*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*åŠŸèƒ½çº§è¯„ä¼°ï¼ˆä»£ç†ä»»åŠ¡ï¼‰*'
- en: A function-level evaluation doesnâ€™t rely on human criticism as much as the previous
    evaluations. Instead, it relies on the properties of the model type in question.
    In fact, this is the kind of evaluation you would turn to after youâ€™ve demonstrated
    that you can obtain human-understandable explanations. Some of these explanations
    may be better than others, possibly due to a single metric. For example, if you
    are relying on a decision tree or decision rules, deeper trees or deeper rule
    sets may be more complex and harder to interpret (even if theyâ€™re technically
    feasible). You might create a function that selects shallower trees or rule sets,
    on the condition that they retain a certain level of predictive power.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°çº§è¯„ä¼°ä¸åƒå‰å‡ ä¸ªè¯„ä¼°é‚£æ ·ä¾èµ–äºäººç±»æ‰¹è¯„ï¼Œè€Œæ˜¯ä¾èµ–äºæ‰€è®¨è®ºçš„æ¨¡å‹ç±»å‹çš„å±æ€§ã€‚å®é™…ä¸Šï¼Œåœ¨æ‚¨å·²ç»å±•ç¤ºå‡ºæ‚¨å¯ä»¥è·å¾—äººç±»ç†è§£çš„è§£é‡Šä¹‹åï¼Œè¿™æ˜¯æ‚¨ä¼šè½¬å‘çš„è¯„ä¼°ç±»å‹ã€‚å…¶ä¸­ä¸€äº›è§£é‡Šå¯èƒ½æ¯”å…¶ä»–è§£é‡Šæ›´å¥½ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºå•ä¸€åº¦é‡æ ‡å‡†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä¾èµ–äºå†³ç­–æ ‘æˆ–å†³ç­–è§„åˆ™ï¼Œæ›´æ·±çš„æ ‘æˆ–æ›´æ·±çš„è§„åˆ™é›†å¯èƒ½æ›´å¤æ‚ä¸”æ›´éš¾è§£é‡Šï¼ˆå³ä½¿å®ƒä»¬åœ¨æŠ€æœ¯ä¸Šæ˜¯å¯è¡Œçš„ï¼‰ã€‚æ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œé€‰æ‹©è¾ƒæµ…çš„æ ‘æˆ–è§„åˆ™é›†ï¼Œæ¡ä»¶æ˜¯å®ƒä»¬ä¿ç•™ä¸€å®šæ°´å¹³çš„é¢„æµ‹èƒ½åŠ›ã€‚
- en: With these approaches in mind, letâ€™s consider what an explanation of a large
    language model might look like according to these definitions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°è¿™äº›æ–¹æ³•ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹æ ¹æ®è¿™äº›å®šä¹‰ï¼Œä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„è§£é‡Šå¯èƒ½æ˜¯ä»€ä¹ˆæ ·å­ã€‚
- en: Definitions and Categories
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸åˆ†ç±»
- en: Interpretability and explainability in machine learning are complex and nuanced
    topics. Solutions that work in one domain might not work in another. As such,
    weâ€™re going to cover a few important terms that are often used by interpretability
    and explainability practitioners and researchers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå¯è§£é‡Šæ€§å’Œå¯è§£é‡Šæ€§æ˜¯å¤æ‚ä¸”å¾®å¦™çš„ä¸»é¢˜ã€‚åœ¨ä¸€ä¸ªé¢†åŸŸæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆå¯èƒ½åœ¨å¦ä¸€ä¸ªé¢†åŸŸæ— æ•ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€äº›ç”±å¯è§£é‡Šæ€§å’Œè§£é‡Šæ€§ä»ä¸šè€…å’Œç ”ç©¶äººå‘˜ç»å¸¸ä½¿ç”¨çš„é‡è¦æœ¯è¯­ã€‚
- en: â€œBlack Boxâ€
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: â€œé»‘ç›’å­â€
- en: 'Machine learning models are often referred to as a *black box* (we touched
    upon this in [ChapterÂ 1](ch01.html#chapter1)). This can be for one of two reasons:
    the details of the model might be proprietary and might be intentionally hidden,
    or the function behind the model is available for inspection, but itâ€™s so complicated
    that no human could comprehend it. Usually when talking about black box models,
    we are referring to the second reason, although many of the techniques discussed
    in this chapter could easily apply to the first reason.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ æ¨¡å‹é€šå¸¸è¢«ç§°ä¸º*é»‘ç›’å­*ï¼ˆæˆ‘ä»¬åœ¨[ç¬¬ä¸€ç« ](ch01.html#chapter1)å·²ç»æ¶‰åŠè¿‡ï¼‰ã€‚è¿™å¯èƒ½æœ‰ä¸¤ä¸ªåŸå› ï¼šæ¨¡å‹çš„ç»†èŠ‚å¯èƒ½æ˜¯ä¸“æœ‰çš„ï¼Œå¯èƒ½æ˜¯æ•…æ„éšè—çš„ï¼›æˆ–è€…æ¨¡å‹èƒŒåçš„åŠŸèƒ½æ˜¯å¯ä»¥æ£€æŸ¥çš„ï¼Œä½†æ˜¯å®ƒéå¸¸å¤æ‚ï¼Œæ²¡æœ‰äººç±»å¯ä»¥ç†è§£ã€‚é€šå¸¸åœ¨è®¨è®ºé»‘ç›’å­æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬æŒ‡çš„æ˜¯ç¬¬äºŒä¸ªåŸå› ï¼Œå°½ç®¡æœ¬ç« è®¨è®ºçš„è®¸å¤šæŠ€æœ¯å¾ˆå®¹æ˜“é€‚ç”¨äºç¬¬ä¸€ä¸ªåŸå› ã€‚
- en: Global Versus Local Interpretability
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¨å±€ä¸å±€éƒ¨çš„å¯è§£é‡Šæ€§
- en: '*Global* interpretations of a model decision can be generalized to the entire
    model behavior. *Local* interpretations are restricted to the input-output pair
    in question. The scope of interpretability could even fall somewhere between these.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…¨å±€*è§£é‡Šæ¨¡å‹å†³ç­–å¯ä»¥æ¨å¹¿åˆ°æ•´ä¸ªæ¨¡å‹è¡Œä¸ºã€‚*å±€éƒ¨*è§£é‡Šè¢«é™åˆ¶åœ¨æ‰€è®¨è®ºçš„è¾“å…¥-è¾“å‡ºå¯¹ä¸Šã€‚å¯è§£é‡Šæ€§çš„èŒƒå›´ç”šè‡³å¯ä»¥ä»‹äºè¿™ä¸¤è€…ä¹‹é—´ã€‚'
- en: Model-Agnostic Versus Model-Specific Methods
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ— å…³ä¸æ¨¡å‹ç‰¹å®šæ–¹æ³•
- en: '*Model-agnostic* interpretability methods do not depend on what type of model
    youâ€™re using: tree based, neural network based, or something else entirely. By
    their nature, these methods do not have access to the modelâ€™s internal information,
    such as architecture or weights. Model-agnostic methods are applied after training
    and typically function by looking at data input and output pairs. These agnostic
    methods usually work by analyzing feature input and output pairs. By definition,
    these methods cannot have access to model internals such as weights or structural
    information.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ¨¡å‹æ— å…³*çš„è§£é‡Šæ–¹æ³•ä¸ä¾èµ–äºæ‚¨ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ï¼šåŸºäºæ ‘çš„ã€åŸºäºç¥ç»ç½‘ç»œçš„ï¼Œæˆ–è€…å®Œå…¨ä¸åŒçš„å…¶ä»–ç±»å‹ã€‚è¿™äº›æ–¹æ³•æœ¬è´¨ä¸Šä¸è®¿é—®æ¨¡å‹çš„å†…éƒ¨ä¿¡æ¯ï¼Œæ¯”å¦‚ç»“æ„æˆ–æƒé‡ã€‚æ¨¡å‹æ— å…³æ–¹æ³•é€šå¸¸æ˜¯åœ¨è®­ç»ƒååº”ç”¨ï¼Œå¹¶ä¸”é€šå¸¸é€šè¿‡è§‚å¯Ÿæ•°æ®çš„è¾“å…¥å’Œè¾“å‡ºå¯¹æ¥è¿ä½œã€‚è¿™äº›æ— å…³æ–¹æ³•é€šå¸¸é€šè¿‡åˆ†æç‰¹å¾çš„è¾“å…¥å’Œè¾“å‡ºå¯¹æ¥å·¥ä½œã€‚æ ¹æ®å®šä¹‰ï¼Œè¿™äº›æ–¹æ³•æ— æ³•è®¿é—®æ¨¡å‹çš„å†…éƒ¨ä¿¡æ¯ï¼Œæ¯”å¦‚æƒé‡æˆ–ç»“æ„ä¿¡æ¯ã€‚'
- en: '*Model-specific* interpretation tools are limited to specific model classes.
    The interpretation of regression weights in a linear model is a model-specific
    interpretation, since by definition the interpretation of intrinsically interpretable
    models is always model-specific. Tools that only work for the interpretation of
    neural networks, for example, are model-specific. Model-agnostic tools, however,
    can be used on any machine learning model.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç‰¹å®šæ¨¡å‹*çš„è§£é‡Šå·¥å…·ä»…é™äºç‰¹å®šçš„æ¨¡å‹ç±»åˆ«ã€‚çº¿æ€§æ¨¡å‹ä¸­å›å½’æƒé‡çš„è§£é‡Šæ˜¯ä¸€ç§ç‰¹å®šäºæ¨¡å‹çš„è§£é‡Šï¼Œå› ä¸ºæŒ‰å®šä¹‰ï¼Œå†…åœ¨å¯è§£é‡Šæ¨¡å‹çš„è§£é‡Šå§‹ç»ˆæ˜¯ç‰¹å®šäºæ¨¡å‹çš„ã€‚ä¾‹å¦‚ï¼Œä»…é€‚ç”¨äºç¥ç»ç½‘ç»œè§£é‡Šçš„å·¥å…·å±äºç‰¹å®šäºæ¨¡å‹çš„å·¥å…·ã€‚ç„¶è€Œï¼Œæ¨¡å‹æ— å…³çš„å·¥å…·å¯ä»¥ç”¨äºä»»ä½•æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚'
- en: Interpreting GPT-2
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£è¯» GPT-2
- en: 'The vast majority of large language models are ML models pre-trained on a large
    corpus of text and fine-tunable for others. These are made of large numbers of
    transformer layers. They can be classified according to the differences in their
    initial modeling task as well as how many encoder and decoder layers theyâ€™re made
    of. *Autoregressive models* are pre-trained on the classic language-modeling task:
    guess the next token having read all the previous ones. They correspond to the
    decoder of the original transformer model, and a mask is used on top of the full
    sentence so that the attention heads can only see what was before in the text,
    and not whatâ€™s after. While they can be fine-tuned to do many tasks, the most
    common one is text generation.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç»å¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹éƒ½æ˜¯åœ¨å¤§é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒçš„ ML æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥è¿›è¡Œå¾®è°ƒä»¥ç”¨äºå…¶ä»–ä»»åŠ¡ã€‚è¿™äº›æ¨¡å‹ç”±å¤§é‡çš„ Transformer å±‚ç»„æˆã€‚å®ƒä»¬å¯ä»¥æ ¹æ®åˆå§‹å»ºæ¨¡ä»»åŠ¡çš„ä¸åŒä»¥åŠç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°çš„å¤šå°‘è¿›è¡Œåˆ†ç±»ã€‚*è‡ªå›å½’æ¨¡å‹*æ˜¯åœ¨ç»å…¸çš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼šåœ¨é˜…è¯»æ‰€æœ‰å…ˆå‰çš„æ ‡è®°åçŒœæµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚å®ƒä»¬å¯¹åº”äºåŸå§‹
    Transformer æ¨¡å‹çš„è§£ç å™¨ï¼Œè¿˜ä½¿ç”¨äº†ä¸€ä¸ªé®ç½©å±‚æ¥è¦†ç›–æ•´ä¸ªå¥å­ï¼Œä»¥ä¾¿æ³¨æ„åŠ›å¤´åªèƒ½çœ‹åˆ°æ–‡æœ¬ä¸­ä¹‹å‰çš„å†…å®¹ï¼Œè€Œä¸æ˜¯ä¹‹åçš„å†…å®¹ã€‚è™½ç„¶å®ƒä»¬å¯ä»¥è¿›è¡Œå¤šç§ä»»åŠ¡çš„å¾®è°ƒï¼Œä½†æœ€å¸¸è§çš„æ˜¯æ–‡æœ¬ç”Ÿæˆã€‚
- en: The large GPT (Generative Pre-trained Transformer) class of models from OpenAI
    are some of the most famous examples of *autoregressive language models*. [GPT-2](https://oreil.ly/62C81),
    the successor to the first-generation GPT, scaled up the size of the network and
    the data it was trained on. What made GPT-2 unique was that it was able to generalize
    to a much larger set of tasks than its predecessor, to perform much better on
    them than linear scaling laws would have predicted. OpenAI did not release GPT-2
    to the public for some time, for fear that it could generate human-like text that
    could be used for nefarious purposes. After [learning more about its capabilities](https://oreil.ly/v2iZ3),
    OpenAI slowly rolled out GPT-2 to research partners, companies, beta testers,
    and eventually the general public.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI çš„å¤§å‹ GPTï¼ˆç”Ÿæˆé¢„è®­ç»ƒ Transformerï¼‰æ¨¡å‹ç±»æ˜¯ä¸€äº›æœ€è‘—åçš„*è‡ªå›å½’è¯­è¨€æ¨¡å‹*ç¤ºä¾‹ä¹‹ä¸€ã€‚[GPT-2](https://oreil.ly/62C81)ï¼Œä½œä¸ºç¬¬ä¸€ä»£
    GPT çš„ç»§ä»»è€…ï¼Œæ‰©å¤§äº†ç½‘ç»œçš„è§„æ¨¡å’Œè®­ç»ƒæ•°æ®çš„èŒƒå›´ã€‚ä½¿ GPT-2 ç‹¬ç‰¹çš„æ˜¯ï¼Œå®ƒèƒ½å¤Ÿæ¨å¹¿åˆ°æ¯”å…¶å‰ä»»é¢„æµ‹çš„æ›´å¤§ä»»åŠ¡é›†åˆï¼Œå¹¶ä¸”åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°æ¯”çº¿æ€§ç¼©æ”¾æ³•é¢„æµ‹çš„è¦å¥½å¾—å¤šã€‚OpenAI
    æ›¾æœ‰ä¸€æ®µæ—¶é—´æ²¡æœ‰å°† GPT-2 å…¬å¼€å‘å¸ƒï¼Œå› ä¸ºæ‹…å¿ƒå®ƒå¯èƒ½ç”Ÿæˆç±»ä¼¼äººç±»æ–‡æœ¬çš„å†…å®¹ï¼Œè¿™å¯èƒ½è¢«ç”¨äºä¸è‰¯ç›®çš„ã€‚åœ¨[æ›´å¤šäº†è§£å…¶èƒ½åŠ›ä¹‹å](https://oreil.ly/v2iZ3)ï¼ŒOpenAI
    é€æ¸å‘ç ”ç©¶ä¼™ä¼´ã€å…¬å¸ã€æµ‹è¯•ç”¨æˆ·ä»¥åŠæœ€ç»ˆå‘å…¬ä¼—å‘å¸ƒäº† GPT-2ã€‚
- en: Note
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: You can find all the code associated with this tutorial in the notebook [*Chapter_3_Interpreting_GPT.ipynb*](https://oreil.ly/GjIDm).
    This was heavily inspired by the LessWrong post [Interpreting GPT, the Logit Lens](https://oreil.ly/w6fiB).
    Much of the code has been refactored and now uses PyTorch and HuggingFace instead
    of TensorFlow. See the original blog post for the TensorFlow version.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨ç¬”è®°æœ¬ [*Chapter_3_Interpreting_GPT.ipynb*](https://oreil.ly/GjIDm) ä¸­æ‰¾åˆ°ä¸æœ¬æ•™ç¨‹ç›¸å…³çš„æ‰€æœ‰ä»£ç ã€‚è¿™ç¯‡ç¬”è®°å—åˆ°äº†
    LessWrong æ–‡ç«  [Interpreting GPT, the Logit Lens](https://oreil.ly/w6fiB) çš„å¯å‘ã€‚å¤§éƒ¨åˆ†ä»£ç å·²è¿›è¡Œäº†é‡æ„ï¼Œç°åœ¨ä½¿ç”¨çš„æ˜¯
    PyTorch å’Œ HuggingFaceï¼Œè€Œä¸æ˜¯ TensorFlowã€‚æœ‰å…³ TensorFlow ç‰ˆæœ¬ï¼Œè¯·å‚é˜…åŸå§‹åšå®¢æ–‡ç« ã€‚
- en: OpenAI released [GPT-3 in 2020](https://arxiv.org/abs/2005.14165), an even larger
    model than GPT-2, trained on more data and with even higher output quality. Itâ€™s
    even more difficult to tell whether or not GPT-3â€™s output was written by a human.^([8](ch03.html#idm45621854672480))
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI åœ¨ [2020 å¹´å‘å¸ƒäº† GPT-3](https://arxiv.org/abs/2005.14165)ï¼Œè¿™æ˜¯æ¯” GPT-2 æ›´å¤§çš„æ¨¡å‹ï¼Œä½¿ç”¨æ›´å¤šçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¾“å‡ºè´¨é‡æ›´é«˜ã€‚å¾ˆéš¾åˆ¤æ–­
    GPT-3 çš„è¾“å‡ºæ˜¯å¦æ˜¯äººç±»å†™çš„ã€‚^([8](ch03.html#idm45621854672480))
- en: 'At the time of writing, GPT-3 is only available through an API. This is due
    not only to safety concerns but also to the size of the model: it is so big that
    just downloading, storing, and running it is a complex, time-consuming, and potentially
    expensive process.^([9](ch03.html#idm45621854668624)) However, itâ€™s safe to assume
    that many of the techniques and principles we use to understand GPT-2 can also
    apply to a larger model like GPT-3. With that in mind, letâ€™s explore how you can
    get more context on GPT-2â€™s decisions.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼ŒGPT-3ä»…é€šè¿‡APIå¯ç”¨ã€‚è¿™ä¸ä»…æ˜¯å‡ºäºå®‰å…¨è€ƒè™‘ï¼Œè¿˜å› ä¸ºæ¨¡å‹ä½“ç§¯è¿‡å¤§ï¼šä»…ä¸‹è½½ã€å­˜å‚¨å’Œè¿è¡Œå®ƒå°±æ˜¯ä¸€ä¸ªå¤æ‚ã€è€—æ—¶ä¸”å¯èƒ½æ˜‚è´µçš„è¿‡ç¨‹ã€‚^([9](ch03.html#idm45621854668624))
    ä¸è¿‡ï¼Œå¯ä»¥æ”¾å¿ƒåœ°å‡è®¾æˆ‘ä»¬ç”¨æ¥ç†è§£GPT-2çš„è®¸å¤šæŠ€æœ¯å’ŒåŸåˆ™ä¹Ÿå¯ä»¥é€‚ç”¨äºåƒGPT-3è¿™æ ·æ›´å¤§çš„æ¨¡å‹ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æ¢è®¨å¦‚ä½•è·å–æ›´å¤šå…³äºGPT-2å†³ç­–èƒŒæ™¯çš„ä¸Šä¸‹æ–‡ã€‚
- en: Tip
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æç¤º
- en: If you are specifically working with HuggingFace Transformer models, the [exBERT
    tool](https://oreil.ly/zYlcl) serves a similar function as the logit lens. This
    open source tool enables users to explore the learned attention weights and contextual
    representations of HuggingFace Transformer models. Input a sentence, and exBERT
    will pass the tokenized input through the specified model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ç‰¹åˆ«ä½¿ç”¨HuggingFace Transformeræ¨¡å‹å·¥ä½œï¼Œ[exBERTå·¥å…·](https://oreil.ly/zYlcl)æä¾›ä¸é€»è¾‘é•œå¤´ç±»ä¼¼çš„åŠŸèƒ½ã€‚è¿™ä¸ªå¼€æºå·¥å…·ä½¿ç”¨æˆ·èƒ½å¤Ÿæ¢ç´¢HuggingFace
    Transformeræ¨¡å‹çš„å­¦ä¹ æ³¨æ„æƒé‡å’Œä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚è¾“å…¥ä¸€ä¸ªå¥å­ï¼ŒexBERTå°†é€šè¿‡æŒ‡å®šçš„æ¨¡å‹ä¼ é€’æ ‡è®°åŒ–çš„è¾“å…¥ã€‚
- en: Weâ€™re going to be using GPT-2 as itâ€™s available from HuggingFace. For the interpretability,
    weâ€™re going to use the utilities package [transformer-utils](https://oreil.ly/xXHAF),
    written by [nostalgebraist](https://oreil.ly/YL6uc).^([10](ch03.html#idm45621854662512))
    Letâ€™s look at the commented excerpt.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨GPT-2ï¼Œå› ä¸ºå®ƒå¯ä»¥ä»HuggingFaceè·å¾—ã€‚ä¸ºäº†æé«˜å¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç”±[nostalgebraist](https://oreil.ly/YL6uc)ç¼–å†™çš„å®ç”¨ç¨‹åºåŒ…[transformer-utils](https://oreil.ly/xXHAF)ã€‚^([10](ch03.html#idm45621854662512))
    è®©æˆ‘ä»¬çœ‹çœ‹è¯„è®ºæ‘˜å½•ã€‚
- en: Warning
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è­¦å‘Š
- en: Running this code has a high RAM requirement. If you are running in Google Colab,
    use the largest GPU available in Colab Pro and set the RAM to the highest setting.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œæ­¤ä»£ç éœ€è¦å¤§é‡çš„RAMã€‚å¦‚æœæ‚¨åœ¨Google Colabä¸­è¿è¡Œï¼Œè¯·ä½¿ç”¨Colab Proæä¾›çš„æœ€å¤§GPUï¼Œå¹¶å°†RAMè®¾ç½®ä¸ºæœ€é«˜è®¾ç½®ã€‚
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Youâ€™re mainly interested in the `plot_logit_lens` function, which wraps the
    various metrics you can use to look at the model. This function is geared toward
    decoders and *autoregressive models* that rely on the decoder part of the original
    transformer and use an attention mask so that at each position, the model can
    only look at the tokens before the attention heads. The space of autoregressive
    models includes [Original GPT](https://oreil.ly/pE1Dq), [GPT-2](https://oreil.ly/wsppp),
    [CTRL](https://oreil.ly/fNSRE), [Transformer-XL](https://oreil.ly/DFEeb), [Reformer](https://oreil.ly/L8uyH),
    and [XLNet](https://oreil.ly/ipV8V) (though weâ€™ll mainly focus on GPT and its
    successors). This class of models is distinct from encoders or autoencoding models,
    or seq-to-seq transformer models, which are in turn distinct from retrieval-based
    models and multi-modal models (for example, like CLIP, described in [â€œDeep Dive:
    Saliency Mapping with CLIPâ€](#deep-dive-saliency-mapping)). To feed data into
    the autoregressive model, you need to first tokenize the input text using GPT-2â€™s
    tokenizer.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¸»è¦å…³æ³¨çš„æ˜¯`plot_logit_lens`å‡½æ•°ï¼Œå®ƒåŒ…è£…äº†æ‚¨å¯ä»¥ç”¨æ¥æŸ¥çœ‹æ¨¡å‹çš„å„ç§åº¦é‡æ ‡å‡†ã€‚è¿™ä¸ªå‡½æ•°ä¸“ä¸ºè§£ç å™¨å’Œä¾èµ–åŸå§‹å˜å‹å™¨çš„è§£ç å™¨éƒ¨åˆ†ä»¥åŠä½¿ç”¨æ³¨æ„åŠ›æ©ç çš„è‡ªå›å½’æ¨¡å‹è€Œè®¾è®¡ï¼Œå› æ­¤åœ¨æ¯ä¸ªä½ç½®ï¼Œæ¨¡å‹åªèƒ½æŸ¥çœ‹æ³¨æ„åŠ›å¤´ä¹‹å‰çš„æ ‡è®°ã€‚è‡ªå›å½’æ¨¡å‹çš„ç©ºé—´åŒ…æ‹¬[åŸå§‹GPT](https://oreil.ly/pE1Dq)ï¼Œ[GPT-2](https://oreil.ly/wsppp)ï¼Œ[CTRL](https://oreil.ly/fNSRE)ï¼Œ[Transformer-XL](https://oreil.ly/DFEeb)ï¼Œ[Reformer](https://oreil.ly/L8uyH)å’Œ[XLNet](https://oreil.ly/ipV8V)ï¼ˆå°½ç®¡æˆ‘ä»¬ä¸»è¦å…³æ³¨GPTåŠå…¶åç»§è€…ï¼‰ã€‚è¿™ä¸€ç±»æ¨¡å‹ä¸ç¼–ç å™¨æˆ–è‡ªç¼–ç æ¨¡å‹ã€åºåˆ—åˆ°åºåˆ—å˜å‹å™¨æ¨¡å‹ä»¥åŠæ£€ç´¢æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼ˆä¾‹å¦‚å¦‚CLIPä¸­æè¿°çš„ï¼‰æœ‰æ‰€ä¸åŒã€‚è¦å°†æ•°æ®è¾“å…¥è‡ªå›å½’æ¨¡å‹ï¼Œæ‚¨éœ€è¦é¦–å…ˆä½¿ç”¨GPT-2çš„æ ‡è®°å™¨å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ã€‚
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this section, you create a selection of texts for the model to read from,
    then use the `plot_logit_lens` function to look at the modelâ€™s ability to predict
    the next word. Our main text comes from the now-famous 2015 paper [â€œHuman-level
    control through deep reinforcement learningâ€](https://oreil.ly/n8uxk). Here is
    the abstract for that paper, along with a few strings about dogs:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæ‚¨å¯ä»¥åˆ›å»ºæ¨¡å‹é˜…è¯»çš„æ–‡æœ¬é€‰é›†ï¼Œç„¶åä½¿ç”¨`plot_logit_lens`å‡½æ•°æŸ¥çœ‹æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä¸»è¦æ–‡æœ¬æ¥è‡ªäºç°åœ¨è‘—åçš„2015å¹´è®ºæ–‡[â€œé€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ å®ç°äººç±»æ°´å¹³æ§åˆ¶â€](https://oreil.ly/n8uxk)ã€‚ä»¥ä¸‹æ˜¯è¯¥è®ºæ–‡çš„æ‘˜è¦ï¼Œä»¥åŠå…³äºç‹—çš„å‡ ä¸ªå­—ç¬¦ä¸²ï¼š
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This package generates plots of the activity at each layer of the decoder, as
    shown in [FigureÂ 3-1](#gpt-logit-lens-0). The decoder model, for each token it
    takes in at position *n* (displayed on the bottom of the plots, representing the
    inputs), tries to predict the token at position *n* + 1 (displayed at the top
    of the plots, representing the outputs). Various hidden layers are labeled on
    the side *y*-axis. Exactly what weâ€™re measuring at each of these layers of the
    decoder model for each of the tokens depends on the arguments you pass to the
    `plot_logit_lens` function.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è½¯ä»¶åŒ…ç”Ÿæˆè§£ç å™¨æ¯ä¸€å±‚æ´»åŠ¨çš„å›¾è¡¨ï¼Œå¦‚[å›¾Â 3-1](#gpt-logit-lens-0)æ‰€ç¤ºã€‚è§£ç å™¨æ¨¡å‹é’ˆå¯¹æ¯ä¸ªä½ç½®*n*çš„è¾“å…¥ä»¤ç‰Œï¼ˆæ˜¾ç¤ºåœ¨å›¾è¡¨åº•éƒ¨ï¼Œè¡¨ç¤ºè¾“å…¥ï¼‰å°è¯•é¢„æµ‹ä½ç½®*n*
    + 1çš„ä»¤ç‰Œï¼ˆæ˜¾ç¤ºåœ¨å›¾è¡¨é¡¶éƒ¨ï¼Œè¡¨ç¤ºè¾“å‡ºï¼‰ã€‚å„éšè—å±‚åœ¨*Y*è½´ä¾§é¢æ ‡è®°ã€‚å¯¹äºè§£ç å™¨æ¨¡å‹æ¯ä¸ªä»¤ç‰Œçš„æ¯ä¸ªå±‚ï¼Œæˆ‘ä»¬æµ‹é‡çš„å†…å®¹å®Œå…¨å–å†³äºæ‚¨ä¼ é€’ç»™`plot_logit_lens`å‡½æ•°çš„å‚æ•°ã€‚
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output of this `plot_logit_lens` function is a table of the most likely
    seeming tokens at each layer for each output position ([FigureÂ 3-1](#gpt-logit-lens-0)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot_logit_lens`å‡½æ•°çš„è¾“å‡ºæ˜¯æ¯ä¸ªè¾“å‡ºä½ç½®ä¸Šæ¯ä¸ªå±‚æœ€å¯èƒ½çš„ä»¤ç‰Œçš„è¡¨æ ¼ï¼ˆè§[å›¾Â 3-1](#gpt-logit-lens-0)ï¼‰ã€‚'
- en: '![ptml 0301](assets/ptml_0301.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0301](assets/ptml_0301.png)'
- en: Figure 3-1\. Looking at the logits of each layer leading into positions 75 through
    100
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 3-1\. æŸ¥çœ‹è¿›å…¥ç¬¬75åˆ°100ä½ç½®çš„æ¯ä¸ªå±‚çš„logits
- en: Itâ€™s nice to see the specific words, but youâ€™d also want to know how close the
    model is to suggesting the correct token at each step.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹åˆ°å…·ä½“çš„è¯è¯­å¾ˆå¥½ï¼Œä½†æ‚¨è¿˜æƒ³çŸ¥é“æ¨¡å‹åœ¨æ¯ä¸ªæ­¥éª¤ä¸­å»ºè®®æ­£ç¡®ä»¤ç‰Œçš„æ¥è¿‘ç¨‹åº¦ã€‚
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And you get a table of ranks at each layer ([FigureÂ 3-2](#gpt-logit-lens-1)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†è·å¾—æ¯ä¸ªå±‚çš„æ’åè¡¨æ ¼ï¼ˆè§[å›¾Â 3-2](#gpt-logit-lens-1)ï¼‰ã€‚
- en: '![ptml 0302](assets/ptml_0302.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0302](assets/ptml_0302.png)'
- en: Figure 3-2\. Rank of the probabilities of the correct token of each layer leading
    into positions 75 through 100
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 3-2\. æ¯ä¸ªå±‚çš„æ­£ç¡®ä»¤ç‰Œæ¦‚ç‡çš„æ’åï¼Œé¢†å…ˆäºç¬¬75åˆ°100ä½ç½®
- en: KL divergence is useful for determining how much the full distribution of probabilities
    at each layer diverges from the eventual final output.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: KLæ•£åº¦æœ‰åŠ©äºç¡®å®šæ¯å±‚æ¦‚ç‡å®Œå…¨åˆ†å¸ƒä¸æœ€ç»ˆè¾“å‡ºä¹‹é—´çš„å·®å¼‚ç¨‹åº¦ã€‚
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This divergence starts out high early on, but then drops closer to 0 as the
    inputs propagate through the network ([FigureÂ 3-3](#gpt-logit-lens-2)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åˆ†æ­§åœ¨æ—©æœŸå¾ˆé«˜ï¼Œä½†éšåéšç€è¾“å…¥é€šè¿‡ç½‘ç»œä¼ æ’­ï¼Œé€æ¸æ¥è¿‘0ï¼ˆè§[å›¾Â 3-3](#gpt-logit-lens-2)ï¼‰ã€‚
- en: '![ptml 0303](assets/ptml_0303.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0303](assets/ptml_0303.png)'
- en: Figure 3-3\. KL divergence of probabilities
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 3-3\. æ¦‚ç‡çš„KLæ•£åº¦
- en: If the preceding isnâ€™t informative enough for you, you can also specify the
    inclusion of sub-blocks of the network.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå‰é¢çš„å†…å®¹å¯¹æ‚¨ä¸å¤Ÿè¯¦ç»†ï¼Œæ‚¨è¿˜å¯ä»¥æŒ‡å®šåŒ…å«ç½‘ç»œå­å—ã€‚
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using this, you can see just how far the rank of the correct output token has
    to climb to get past all the competing choices ([FigureÂ 3-4](#gpt-logit-lens-3)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ­£ç¡®è¾“å‡ºä»¤ç‰Œçš„æ’åéœ€è¦å¤šå¤§åŠªåŠ›æ‰èƒ½è¶…è¿‡æ‰€æœ‰ç«äº‰é€‰æ‹©ï¼ˆè§[å›¾Â 3-4](#gpt-logit-lens-3)ï¼‰ã€‚
- en: '![ptml 0304](assets/ptml_0304.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0304](assets/ptml_0304.png)'
- en: Figure 3-4\. Ranks of a rare yet correct output token throughout sub-blocks
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 3-4\. ç¨€æœ‰ä½†æ­£ç¡®è¾“å‡ºä»¤ç‰Œçš„æ’åï¼Œæ¶µç›–å­å—
- en: The `plot_logit_lens` utility has a lot of options for different levels of granularity.
    What does all of this look like if we switch to a more repetitive input?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`   `plot_logit_lens`å®ç”¨ç¨‹åºå…·æœ‰è®¸å¤šä¸åŒç²’åº¦çº§åˆ«çš„é€‰é¡¹ã€‚å¦‚æœæˆ‘ä»¬åˆ‡æ¢åˆ°æ›´åŠ é‡å¤çš„è¾“å…¥ï¼Œæ‰€æœ‰è¿™äº›ä¼šæ˜¯ä»€ä¹ˆæ ·å­ï¼Ÿ'
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This analysis of the repetitive inputs produces [FigureÂ 3-5](#gpt-logit-lens-4).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹é‡å¤è¾“å…¥çš„è¿™ç§åˆ†æç”Ÿæˆäº†[å›¾Â 3-5](#gpt-logit-lens-4)ã€‚
- en: '![ptml 0305](assets/ptml_0305.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0305](assets/ptml_0305.png)'
- en: Figure 3-5\. Analysis on a more repetitive set of inputs
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 3-5\. å¯¹æ›´é‡å¤çš„è¾“å…¥è¿›è¡Œåˆ†æ
- en: So how does this all relate to the three-part framework?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆè¿™ä¸€åˆ‡å¦‚ä½•ä¸ä¸‰éƒ¨åˆ†æ¡†æ¶ç›¸å…³è”ï¼Ÿ
- en: If you wanted to do an application-level evaluation, could a machine learning
    engineer easily understand whatâ€™s going on? You would want to compare the explanation
    produced by the application-level evaluation to other ways of explaining the internals
    of the large language models. Most importantly, you would want to compare this
    to how the ML engineerâ€™s colleagues might explain whatâ€™s going on within the model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¿›è¡Œåº”ç”¨çº§è¯„ä¼°ï¼Œæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆèƒ½å¦è½»æ¾ç†è§£æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Ÿæ‚¨å¸Œæœ›å°†åº”ç”¨çº§è¯„ä¼°äº§ç”Ÿçš„è§£é‡Šä¸å…¶ä»–è§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹å†…éƒ¨æœºåˆ¶çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæ‚¨å¸Œæœ›å°†å…¶ä¸MLå·¥ç¨‹å¸ˆçš„åŒäº‹å¦‚ä½•è§£é‡Šæ¨¡å‹å†…éƒ¨æœºåˆ¶è¿›è¡Œæ¯”è¾ƒã€‚
- en: If you wanted to evaluate this explanation in terms of a â€œhuman-level evaluation,â€
    youâ€™d expand your focus beyond just machine learning developers to ask whether
    a non-ML software engineer (or, better yet, a non-engineer) could understand what
    was going on in the model. This approach might involve making the explanation
    of the input-to-output framing much more explicit in the plot.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³ä»â€œäººç±»æ°´å¹³è¯„ä¼°â€çš„è§’åº¦æ¥è¯„ä»·è¿™ä¸ªè§£é‡Šï¼Œä½ ä¼šæŠŠç„¦ç‚¹ä»ä»…é™äºæœºå™¨å­¦ä¹ å¼€å‘è€…æ‰©å±•åˆ°è¯¢é—®éæœºå™¨å­¦ä¹ è½¯ä»¶å·¥ç¨‹å¸ˆï¼ˆæˆ–è€…æ›´å¥½çš„æ˜¯éå·¥ç¨‹å¸ˆï¼‰æ˜¯å¦èƒ½ç†è§£æ¨¡å‹ä¸­æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ã€‚è¿™ç§æ–¹æ³•å¯èƒ½æ¶‰åŠåœ¨å›¾ä¸­æ›´æ˜ç¡®åœ°è§£é‡Šä»è¾“å…¥åˆ°è¾“å‡ºçš„æ¡†æ¶ã€‚
- en: You might look at a functional-level evaluation after youâ€™ve gotten human feedback.
    Once you understand what kinds of explanations a human can start to understand,
    you want some kind of proxy metric with which to evaluate the explanations. For
    example, how early does the correct token appear in the diagram? We could compare
    this timing across other transformer models. You might also time how long it takes
    for your interpretability method to run. The various approaches to analyzing the
    layers of a GPT model were all much faster than a technique like SHapley Additive
    exPlanations (or SHAP, defined in [â€œShapley and SHAPâ€](#shapley-shap-sect)) would
    have been for such a large set of inputs and outputs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨å¾—åˆ°äººç±»åé¦ˆåè¿›è¡ŒåŠŸèƒ½çº§è¯„ä¼°ã€‚ä¸€æ—¦ä½ äº†è§£äº†äººç±»èƒ½å¤Ÿå¼€å§‹ç†è§£çš„è§£é‡Šç±»å‹ï¼Œä½ å°±å¸Œæœ›æœ‰æŸç§ä»£ç†æŒ‡æ ‡æ¥è¯„ä¼°è¿™äº›è§£é‡Šã€‚ä¾‹å¦‚ï¼Œæ­£ç¡®ä»¤ç‰Œåœ¨å›¾è¡¨ä¸­å‡ºç°çš„æ—¶é—´æœ‰å¤šæ—©ï¼Ÿæˆ‘ä»¬å¯ä»¥æ¯”è¾ƒè¿™ç§æ—¶é—´è·¨å…¶ä»–å˜å‹å™¨æ¨¡å‹ã€‚ä½ è¿˜å¯ä»¥è®¡æ—¶è§£é‡Šæ€§æ–¹æ³•çš„è¿è¡Œæ—¶é—´ã€‚åˆ†æGPTæ¨¡å‹å±‚æ¬¡çš„å„ç§æ–¹æ³•æ¯”åƒSHapley
    Additive exPlanationsï¼ˆæˆ–ç®€ç§°SHAPï¼Œè§[â€œShapley and SHAPâ€](#shapley-shap-sect)ï¼‰è¿™æ ·çš„æŠ€æœ¯å¯¹å¦‚æ­¤å¤§é‡çš„è¾“å…¥å’Œè¾“å‡ºæ¥è¯´è¦å¿«å¾—å¤šã€‚
- en: Methods for Explaining Models and Interpreting Outputs
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§£é‡Šæ¨¡å‹å’Œè§£é‡Šè¾“å‡ºçš„æ–¹æ³•
- en: The field of model explainability and interpretability changes quickly. However,
    some methods have stood the test of time, even after decades of use.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¯è§£é‡Šæ€§å’Œå¯è§£é‡Šæ€§é¢†åŸŸå‘å±•è¿…é€Ÿã€‚ç„¶è€Œï¼Œä¸€äº›æ–¹æ³•å³ä½¿ç»è¿‡æ•°åå¹´çš„ä½¿ç”¨ä»ç»å—ä½äº†æ—¶é—´çš„è€ƒéªŒã€‚
- en: Inherently Explainable Models
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šå¯è§£é‡Šçš„æ¨¡å‹
- en: Some models are easy to explain because their individual parameters correspond
    to decision points that humans can easily understand, such as linear and logistic
    regression models, symbolic regression models, support vector machines, and decision
    trees.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›æ¨¡å‹æ˜“äºè§£é‡Šï¼Œå› ä¸ºå®ƒä»¬çš„å„ä¸ªå‚æ•°å¯¹åº”äºäººç±»å¯ä»¥è½»æ¾ç†è§£çš„å†³ç­–ç‚¹ï¼Œä¾‹å¦‚çº¿æ€§å’Œé€»è¾‘å›å½’æ¨¡å‹ã€ç¬¦å·å›å½’æ¨¡å‹ã€æ”¯æŒå‘é‡æœºå’Œå†³ç­–æ ‘ã€‚
- en: Linear regression
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: çº¿æ€§å›å½’
- en: '*Linear regression* (and by extension multilinear regression) is perhaps the
    simplest type of inherently explainable model. A linear regression model simply
    takes in a dataset of a dependent and independent variable. Given a dataset <math
    alttext="StartSet y Subscript i Baseline comma x Subscript i Baseline 1 Baseline
    comma ellipsis comma x Subscript i p Baseline EndSet Subscript i equals 1 Superscript
    n"><msubsup><mrow><mo>{</mo><msub><mi>y</mi> <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi>
    <mrow><mi>i</mi><mn>1</mn></mrow></msub> <mo>,</mo><mo>...</mo><mo>,</mo><msub><mi>x</mi>
    <mrow><mi>i</mi><mi>p</mi></mrow></msub> <mo>}</mo></mrow> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup></math> , with <math alttext="y"><mi>y</mi></math> representing
    the independent variable and <math alttext="x"><mi>x</mi></math> representing
    the dependent variable, the model is <math alttext="y Subscript i Baseline equals
    beta 0 plus beta 1 x Subscript i Baseline 1 Baseline plus ellipsis plus beta Subscript
    p Baseline x Subscript i p Baseline plus epsilon Subscript i Baseline equals bold
    x Subscript i Superscript upper T Baseline beta plus epsilon Subscript i"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>Î²</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>Î²</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow></msub> <mo>+</mo>
    <mo>...</mo> <mo>+</mo> <msub><mi>Î²</mi> <mi>p</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi><mi>p</mi></mrow></msub>
    <mo>+</mo> <msub><mi>Ïµ</mi> <mi>i</mi></msub> <mo>=</mo> <msubsup><mi>ğ±</mi> <mrow><mi>i</mi></mrow>
    <mi>T</mi></msubsup> <mi>Î²</mi> <mo>+</mo> <msub><mi>Ïµ</mi> <mi>i</mi></msub></mrow></math>
    , where <math alttext="i equals 1 comma ellipsis comma n"><mrow><mi>i</mi> <mo>=</mo>
    <mn>1</mn> <mo>,</mo> <mo>...</mo> <mo>,</mo> <mi>n</mi></mrow></math> .'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*çº¿æ€§å›å½’*ï¼ˆä»¥åŠå¤šå…ƒçº¿æ€§å›å½’ï¼‰æˆ–è®¸æ˜¯æœ€ç®€å•çš„å›ºæœ‰å¯è§£é‡Šæ¨¡å‹ç±»å‹ã€‚çº¿æ€§å›å½’æ¨¡å‹åªéœ€è¾“å…¥ä¸€ä¸ªä¾èµ–å˜é‡å’Œä¸€ä¸ªç‹¬ç«‹å˜é‡çš„æ•°æ®é›†ã€‚ç»™å®šæ•°æ®é›† <math alttext="StartSet
    y Subscript i Baseline comma x Subscript i Baseline 1 Baseline comma ellipsis
    comma x Subscript i p Baseline EndSet Subscript i equals 1 Superscript n"><msubsup><mrow><mo>{</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>,</mo><msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow></msub>
    <mo>,</mo><mo>...</mo><mo>,</mo><msub><mi>x</mi> <mrow><mi>i</mi><mi>p</mi></mrow></msub>
    <mo>}</mo></mrow> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup></math>
    ï¼Œå…¶ä¸­ <math alttext="y"><mi>y</mi></math> è¡¨ç¤ºç‹¬ç«‹å˜é‡ï¼Œ<math alttext="x"><mi>x</mi></math>
    è¡¨ç¤ºä¾èµ–å˜é‡ï¼Œæ¨¡å‹ä¸º <math alttext="y Subscript i Baseline equals beta 0 plus beta 1 x Subscript
    i Baseline 1 Baseline plus ellipsis plus beta Subscript p Baseline x Subscript
    i p Baseline plus epsilon Subscript i Baseline equals bold x Subscript i Superscript
    upper T Baseline beta plus epsilon Subscript i"><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <msub><mi>Î²</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>Î²</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow></msub> <mo>+</mo> <mo>...</mo>
    <mo>+</mo> <msub><mi>Î²</mi> <mi>p</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi><mi>p</mi></mrow></msub>
    <mo>+</mo> <msub><mi>Ïµ</mi> <mi>i</mi></msub> <mo>=</mo> <msubsup><mi>ğ±</mi> <mrow><mi>i</mi></mrow>
    <mi>T</mi></msubsup> <mi>Î²</mi> <mo>+</mo> <msub><mi>Ïµ</mi> <mi>i</mi></msub></mrow></math>
    ï¼Œå…¶ä¸­ <math alttext="i equals 1 comma ellipsis comma n"><mrow><mi>i</mi> <mo>=</mo>
    <mn>1</mn> <mo>,</mo> <mo>...</mo> <mo>,</mo> <mi>n</mi></mrow></math> ã€‚'
- en: 'Here the various beta terms describe the linear relationship, and epsilon represents
    a random error term. To see this in action, letâ€™s look at a very simple linear
    regression example:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œå„ä¸ª beta é¡¹æè¿°äº†çº¿æ€§å…³ç³»ï¼Œè€Œ epsilon è¡¨ç¤ºéšæœºè¯¯å·®é¡¹ã€‚è¦çœ‹åˆ°è¿™ä¸€ç‚¹çš„å®é™…æ•ˆæœï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸ªéå¸¸ç®€å•çš„çº¿æ€§å›å½’ç¤ºä¾‹ï¼š
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you have a regression problem, itâ€™s a best practice to first make sure the
    problem is adequately solvable with linear regression before moving onto more
    complex models. You might be surprised to see just how many problems are adequately
    solved by a linear regression model.^([11](ch03.html#idm45621848233456))
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æœ‰ä¸€ä¸ªå›å½’é—®é¢˜ï¼Œåœ¨è½¬å‘æ›´å¤æ‚çš„æ¨¡å‹ä¹‹å‰ï¼Œé¦–å…ˆç¡®ä¿çº¿æ€§å›å½’èƒ½å¤Ÿå……åˆ†è§£å†³è¿™ä¸ªé—®é¢˜æ˜¯æœ€ä½³å®è·µã€‚ä½ å¯èƒ½ä¼šæƒŠè®¶åœ°çœ‹åˆ°æœ‰å¤šå°‘é—®é¢˜å¯ä»¥é€šè¿‡çº¿æ€§å›å½’æ¨¡å‹å……åˆ†è§£å†³ã€‚^([11](ch03.html#idm45621848233456))
- en: Tip
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æç¤º
- en: 'Do you want to speed up scikit-learn on (Intel) CPUs? With [scikit-learn-intelex](https://oreil.ly/YEwz1),
    you can get from 1.4 up to about 4,800Ã— speedups by adding one line of code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è¦åœ¨ (Intel) CPU ä¸ŠåŠ é€Ÿ scikit-learn å—ï¼Ÿé€šè¿‡ [scikit-learn-intelex](https://oreil.ly/YEwz1)ï¼Œåªéœ€æ·»åŠ ä¸€è¡Œä»£ç ï¼Œé€Ÿåº¦æé«˜äº†å¤§çº¦
    1.4 è‡³ 4,800 å€ï¼š
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Speaking of code, these intrinsically interpretable models can be explored more
    in the notebook [*Chapter_3_Intrinsically_Interpretable_Models.ipynb*](https://oreil.ly/IxnwJ).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è°ˆåˆ°ä»£ç ï¼Œè¿™äº›å›ºæœ‰å¯è§£é‡Šæ¨¡å‹å¯ä»¥åœ¨ç¬”è®°æœ¬ [*Chapter_3_Intrinsically_Interpretable_Models.ipynb*](https://oreil.ly/IxnwJ)
    ä¸­è¿›ä¸€æ­¥æ¢ç´¢ã€‚
- en: Logistic regression
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’
- en: '*Logistic regression* is a type of linear model that is used to predict the
    probability of a categorical variable. In other words, it is a binary classifier.
    The reason itâ€™s referred to as â€œregressionâ€ is that it is a regression model for
    the *probability* of an event or variable. The value of this event or variable
    depends on whether or not a certain probability threshold has been met.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*é€»è¾‘å›å½’*æ˜¯ä¸€ç§çº¿æ€§æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹åˆ†ç±»å˜é‡çš„æ¦‚ç‡ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæ˜¯ä¸€ä¸ªäºŒå…ƒåˆ†ç±»å™¨ã€‚ä¹‹æ‰€ä»¥ç§°ä¸ºâ€œå›å½’â€ï¼Œæ˜¯å› ä¸ºå®ƒæ˜¯ä¸€ä¸ªäº‹ä»¶æˆ–å˜é‡æ¦‚ç‡çš„å›å½’æ¨¡å‹ã€‚è¯¥äº‹ä»¶æˆ–å˜é‡çš„å€¼å–å†³äºæ˜¯å¦è¾¾åˆ°äº†æŸä¸ªæ¦‚ç‡é˜ˆå€¼ã€‚'
- en: Logistic regression is also known in the literature as *logit regression*, *maximum-entropy
    classification (MaxEnt)*, or the *log-linear classifier*. In this model, the probabilities
    describing the possible outcomes of a single trial are modeled using a [logistic
    function](https://oreil.ly/yvTrp) <math alttext="f left-parenthesis x right-parenthesis
    equals StartFraction upper L Over 1 plus e Superscript minus k left-parenthesis
    x minus x 0 right-parenthesis Baseline EndFraction"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mi>L</mi> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>k</mi><mo>(</mo><mi>x</mi><mo>-</mo><msub><mi>x</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow></msup></mrow></mfrac></mrow></math> , where <math alttext="x
    0"><msub><mi>x</mi> <mn>0</mn></msub></math> is the <math alttext="x"><mi>x</mi></math>
    value of the sigmoidâ€™s midpoint, <math alttext="upper L"><mi>L</mi></math> is
    the curveâ€™s maximum value, and <math alttext="k"><mi>k</mi></math> is the logistic
    growth rate or steepness of the curve.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ–‡çŒ®ä¸­ï¼Œé€»è¾‘å›å½’ä¹Ÿè¢«ç§°ä¸º*logitå›å½’*ã€*æœ€å¤§ç†µåˆ†ç±»ï¼ˆMaxEntï¼‰*æˆ–*å¯¹æ•°çº¿æ€§åˆ†ç±»å™¨*ã€‚åœ¨è¯¥æ¨¡å‹ä¸­ï¼Œé€šè¿‡ä¸€ä¸ª[é€»è¾‘å‡½æ•°](https://oreil.ly/yvTrp)ï¼Œæ¥å»ºæ¨¡æè¿°å•æ¬¡è¯•éªŒå¯èƒ½ç»“æœçš„æ¦‚ç‡ï¼Œå…¶ä¸­<math
    alttext="f left-parenthesis x right-parenthesis equals StartFraction upper L Over
    1 plus e Superscript minus k left-parenthesis x minus x 0 right-parenthesis Baseline
    EndFraction"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mi>L</mi> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>k</mi><mo>(</mo><mi>x</mi><mo>-</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>ï¼Œå…¶ä¸­<math
    alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math>æ˜¯Så‹å‡½æ•°ä¸­ç‚¹çš„<math alttext="x"><mi>x</mi></math>å€¼ï¼Œ<math
    alttext="upper L"><mi>L</mi></math>æ˜¯æ›²çº¿çš„æœ€å¤§å€¼ï¼Œ<math alttext="k"><mi>k</mi></math>æ˜¯é€»è¾‘å¢é•¿ç‡æˆ–æ›²çº¿çš„é™¡å³­åº¦ã€‚
- en: In this code snippet, you can create a basic logistic regression model and then
    view the decision boundaries.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤ä»£ç ç‰‡æ®µä¸­ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„é€»è¾‘å›å½’æ¨¡å‹ï¼Œç„¶åæŸ¥çœ‹å†³ç­–è¾¹ç•Œã€‚
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Much like how linear regression is the simple first option for regression problems,
    logistic regression is the simple first option for classification problems. As
    you can see in the preceding code snippet, the logistic regression model is so
    simple to define that most of the code is used for the plotting.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒçº¿æ€§å›å½’æ˜¯å›å½’é—®é¢˜çš„ç®€å•é¦–é€‰ä¸€æ ·ï¼Œé€»è¾‘å›å½’æ˜¯åˆ†ç±»é—®é¢˜çš„ç®€å•é¦–é€‰ã€‚æ­£å¦‚æ‚¨åœ¨å‰é¢çš„ä»£ç ç‰‡æ®µä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œé€»è¾‘å›å½’æ¨¡å‹å®šä¹‰èµ·æ¥éå¸¸ç®€å•ï¼Œå¤§éƒ¨åˆ†ä»£ç ç”¨äºç»˜å›¾ã€‚
- en: Linear models that describe their outputs as a weighted sum of the input variables
    are easy to implement and understand. The problem is that they depend on certain
    assumptions that often do not hold in the real world. For example, linear regression
    often assumes that the error epsilon follows a Gaussian distribution, but real-world
    phenomena can follow distributions that look nothing like a Gaussian. Some variables
    might interact while others might not. Among those that interact, some might have
    linear relationships, and some might have nonlinear relationships. Fortunately,
    there are a wide variety of nonlinear models that better fit the data and still
    provide interpretability.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å®ƒä»¬çš„è¾“å‡ºæè¿°ä¸ºè¾“å…¥å˜é‡åŠ æƒå’Œçš„çº¿æ€§æ¨¡å‹æ˜“äºå®æ–½å’Œç†è§£ã€‚é—®é¢˜åœ¨äºå®ƒä»¬ä¾èµ–äºé€šå¸¸åœ¨ç°å®ä¸–ç•Œä¸­ä¸æˆç«‹çš„æŸäº›å‡è®¾ã€‚ä¾‹å¦‚ï¼Œçº¿æ€§å›å½’é€šå¸¸å‡è®¾è¯¯å·®Îµéµå¾ªé«˜æ–¯åˆ†å¸ƒï¼Œä½†ç°å®ä¸–ç•Œçš„ç°è±¡å¯èƒ½éµå¾ªçœ‹èµ·æ¥ä¸é«˜æ–¯åˆ†å¸ƒå®Œå…¨ä¸åŒçš„åˆ†å¸ƒã€‚æœ‰äº›å˜é‡å¯èƒ½ä¼šäº’ç›¸ä½œç”¨ï¼Œè€Œå¦ä¸€äº›åˆ™å¯èƒ½ä¸ä¼šã€‚åœ¨ç›¸äº’ä½œç”¨çš„å˜é‡ä¸­ï¼Œæœ‰äº›å¯èƒ½å…·æœ‰çº¿æ€§å…³ç³»ï¼Œè€Œå¦ä¸€äº›å¯èƒ½å…·æœ‰éçº¿æ€§å…³ç³»ã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰å„ç§å„æ ·çš„éçº¿æ€§æ¨¡å‹å¯ä»¥æ›´å¥½åœ°æ‹Ÿåˆæ•°æ®ï¼ŒåŒæ—¶ä»ç„¶æä¾›è§£é‡Šèƒ½åŠ›ã€‚
- en: Generalized linear model
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¹¿ä¹‰çº¿æ€§æ¨¡å‹
- en: If the target outcome *y*, given the features, does not follow a Gaussian distribution,
    then a *generalized linear model* (GLM) is a good choice. The main approach of
    GLMs is to keep the weighted sum of features, but allow non-Gaussian outcome distributions
    and connect the expected mean of this distribution to the weighted sum.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç»™å®šç‰¹å¾æ—¶ç›®æ ‡ç»“æœ*y*ä¸ç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œåˆ™å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMï¼‰æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚GLMçš„ä¸»è¦æ–¹æ³•æ˜¯ä¿æŒç‰¹å¾çš„åŠ æƒå’Œï¼Œä½†å…è®¸éé«˜æ–¯ç»“æœåˆ†å¸ƒï¼Œå¹¶å°†æ­¤åˆ†å¸ƒçš„æœŸæœ›å‡å€¼ä¸åŠ æƒå’Œç›¸è¿æ¥ã€‚
- en: <math alttext="g left-parenthesis upper E Subscript upper Y Baseline left-parenthesis
    y vertical-bar x right-parenthesis right-parenthesis equals beta 0 plus beta 1
    x 1 plus ellipsis beta Subscript p Baseline x Subscript p" display="block"><mrow><mi>g</mi>
    <mrow><mo>(</mo> <msub><mi>E</mi> <mi>Y</mi></msub> <mrow><mo>(</mo> <mi>y</mi>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>Î²</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>Î²</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mo>...</mo> <msub><mi>Î²</mi> <mi>p</mi></msub> <msub><mi>x</mi>
    <mi>p</mi></msub></mrow></math>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="g left-parenthesis upper E Subscript upper Y Baseline left-parenthesis
    y vertical-bar x right-parenthesis right-parenthesis equals beta 0 plus beta 1
    x 1 plus ellipsis beta Subscript p Baseline x Subscript p" display="block"><mrow><mi>g</mi>
    <mrow><mo>(</mo> <msub><mi>E</mi> <mi>Y</mi></msub> <mrow><mo>(</mo> <mi>y</mi>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>Î²</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>Î²</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mo>...</mo> <msub><mi>Î²</mi> <mi>p</mi></msub> <msub><mi>x</mi>
    <mi>p</mi></msub></mrow></math>
- en: While GLMs can be used for Gaussian distributions, this approach can also be
    applied to Poisson, gamma, and inverse gamma distributions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶GLMså¯ç”¨äºé«˜æ–¯åˆ†å¸ƒï¼Œä½†è¿™ç§æ–¹æ³•ä¹Ÿå¯åº”ç”¨äºæ³Šæ¾ã€Gammaå’ŒåGammaåˆ†å¸ƒã€‚
- en: For GLMs, you can turn to scikit-learnâ€™s [generalized linear models](https://oreil.ly/lxOV8).
    Upon importing the TweedieRegressor, you can toggle the `power`, `alpha`, and
    `link` settings to adjust the complexity of your linear model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºGLMsï¼Œä½ å¯ä»¥ä½¿ç”¨scikit-learnçš„[å¹¿ä¹‰çº¿æ€§æ¨¡å‹](https://oreil.ly/lxOV8)ã€‚åœ¨å¯¼å…¥TweedieRegressoråï¼Œä½ å¯ä»¥è°ƒæ•´`power`ã€`alpha`å’Œ`link`è®¾ç½®æ¥è°ƒæ•´çº¿æ€§æ¨¡å‹çš„å¤æ‚æ€§ã€‚
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The output is a series of coefficients (all the beta values) and an intercept
    (corresponding to the first beta).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºæ˜¯ä¸€ç³»åˆ—ç³»æ•°ï¼ˆæ‰€æœ‰betaå€¼ï¼‰å’Œä¸€ä¸ªæˆªè·ï¼ˆå¯¹åº”äºç¬¬ä¸€ä¸ªbetaï¼‰ã€‚
- en: Generalized additive models
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¹¿ä¹‰å¯åŠ æ¨¡å‹
- en: 'If the true relationship between the features and *y* is not linear, then a
    *generalized additive model* (GAM) is a good choice. GAMs are basically GLMs that
    allow nonlinear relationships. The formula is very similar:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç‰¹å¾ä¸*y*ä¹‹é—´çš„çœŸå®å…³ç³»ä¸æ˜¯çº¿æ€§çš„ï¼Œé‚£ä¹ˆ**å¹¿ä¹‰å¯åŠ æ¨¡å‹**ï¼ˆGAMï¼‰æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚GAMsåŸºæœ¬ä¸Šæ˜¯å…è®¸éçº¿æ€§å…³ç³»çš„å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMsï¼‰ã€‚å…¶å…¬å¼éå¸¸ç›¸ä¼¼ï¼š
- en: <math alttext="g left-parenthesis upper E Subscript upper Y Baseline left-parenthesis
    y vertical-bar x right-parenthesis right-parenthesis equals beta 0 plus f 1 left-parenthesis
    x 1 right-parenthesis plus f 2 left-parenthesis x 2 right-parenthesis plus ellipsis
    plus f Subscript p Baseline left-parenthesis x Subscript p Baseline right-parenthesis"
    display="block"><mrow><mi>g</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mi>Y</mi></msub>
    <mrow><mo>(</mo> <mi>y</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>Î²</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>f</mi> <mn>1</mn></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo>
    <msub><mi>f</mi> <mn>2</mn></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>f</mi> <mi>p</mi></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>p</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="g left-parenthesis upper E Subscript upper Y Baseline left-parenthesis
    y vertical-bar x right-parenthesis right-parenthesis equals beta 0 plus f 1 left-parenthesis
    x 1 right-parenthesis plus f 2 left-parenthesis x 2 right-parenthesis plus ellipsis
    plus f Subscript p Baseline left-parenthesis x Subscript p Baseline right-parenthesis"
    display="block"><mrow><mi>g</mi> <mrow><mo>(</mo> <msub><mi>E</mi> <mi>Y</mi></msub>
    <mrow><mo>(</mo> <mi>y</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>Î²</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>f</mi> <mn>1</mn></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo>
    <msub><mi>f</mi> <mn>2</mn></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>f</mi> <mi>p</mi></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>p</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: The only difference is that the linear terms <math alttext="beta Subscript p
    Baseline x Subscript p"><mrow><msub><mi>Î²</mi> <mi>p</mi></msub> <msub><mi>x</mi>
    <mi>p</mi></msub></mrow></math> have been replaced with more flexible <math alttext="f
    Subscript p Baseline left-parenthesis x Subscript p Baseline right-parenthesis"><mrow><msub><mi>f</mi>
    <mi>p</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>p</mi></msub> <mo>)</mo></mrow></mrow></math>
    functions (usually representing splines). Itâ€™s still a sum of features, but optional
    nonlinearity is now represented by the functions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å”¯ä¸€çš„åŒºåˆ«æ˜¯çº¿æ€§é¡¹<math alttext="beta Subscript p Baseline x Subscript p"><mrow><msub><mi>Î²</mi>
    <mi>p</mi></msub> <msub><mi>x</mi> <mi>p</mi></msub></mrow></math> è¢«æ›´åŠ çµæ´»çš„<math
    alttext="f Subscript p Baseline left-parenthesis x Subscript p Baseline right-parenthesis"><mrow><msub><mi>f</mi>
    <mi>p</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>p</mi></msub> <mo>)</mo></mrow></mrow></math>
    å‡½æ•°ï¼ˆé€šå¸¸ä»£è¡¨æ ·æ¡å‡½æ•°ï¼‰å–ä»£ã€‚å®ƒä»ç„¶æ˜¯ç‰¹å¾çš„æ€»å’Œï¼Œä½†ç°åœ¨å¯é€‰çš„éçº¿æ€§ç”±è¿™äº›å‡½æ•°è¡¨ç¤ºã€‚
- en: To use GAMs in Python, you can use [pyGAM](https://oreil.ly/eQwEd).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨Pythonä¸­ä½¿ç”¨GAMsï¼Œä½ å¯ä»¥ä½¿ç”¨[pyGAM](https://oreil.ly/eQwEd)ã€‚
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Generalized additive models plus interactions
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¹¿ä¹‰å¯åŠ æ¨¡å‹åŠ äº¤äº’é¡¹
- en: If features interact, then you can either add up interactions manually or turn
    to *generalized additive models plus interactions* (GA2Ms).^([12](ch03.html#idm45621842768384))
    These capture much more complex interactions than regular GAMs do. Applying GA2Ms
    to a dataset is not that different from applying the GAMs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç‰¹å¾ä¹‹é—´å­˜åœ¨äº¤äº’ä½œç”¨ï¼Œé‚£ä¹ˆä½ å¯ä»¥æ‰‹åŠ¨æ·»åŠ äº¤äº’é¡¹ï¼Œæˆ–è€…è½¬å‘**å¹¿ä¹‰å¯åŠ æ¨¡å‹åŠ äº¤äº’é¡¹**ï¼ˆGA2Msï¼‰ã€‚^([12](ch03.html#idm45621842768384))
    è¿™äº›æ¨¡å‹æ•æ‰çš„äº¤äº’ä½œç”¨æ¯”æ™®é€šçš„GAMså¤æ‚å¾—å¤šã€‚å°†GA2Msåº”ç”¨åˆ°æ•°æ®é›†ä¸Šä¸åº”ç”¨GAMså¹¶æ²¡æœ‰å¤ªå¤§åŒºåˆ«ã€‚
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Beyond exploring the data, you can also train the GA2M model, which manifests
    itself as the `ExplainableBoostingRegressor` class. If youâ€™re working on a classification
    problem, you use the `ExplainableBoostingClassifier` class instead.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ¢ç´¢æ•°æ®å¤–ï¼Œä½ è¿˜å¯ä»¥è®­ç»ƒGA2Mæ¨¡å‹ï¼Œå®ƒè¡¨ç°ä¸º`ExplainableBoostingRegressor`ç±»ã€‚å¦‚æœä½ åœ¨è§£å†³åˆ†ç±»é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨`ExplainableBoostingClassifier`ç±»ã€‚
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Whatâ€™s the downside of using this approach? Although the pairwise interaction
    terms in GA2M increase accuracy greatly, the model is *extremely* time-consuming
    and CPU-hungry.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ç§æ–¹æ³•çš„ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿè™½ç„¶GA2Mä¸­çš„æˆå¯¹äº¤äº’é¡¹æå¤§åœ°æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†è¯¥æ¨¡å‹çš„è®¡ç®—éå¸¸è€—æ—¶å’ŒCPUå¯†é›†ã€‚
- en: Symbolic regression
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¬¦å·å›å½’
- en: Many of the previously described methods can be thought of as ways to create
    large equations to serve as models. *Symbolic regression* (SR) takes this to the
    extreme, by iteratively changing components of a formula to better fit the data.
    SR seeks an accurate model of the data in the form of a (hopefully elegant) mathematical
    expression. SR is generally considered hard and is usually attempted using evolutionary
    algorithms. If you have tabular data or data that could theoretically be described
    using an equation, then symbolic regression is a good choice.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å…ˆå‰æè¿°çš„è®¸å¤šæ–¹æ³•éƒ½å¯ä»¥è§†ä¸ºåˆ›å»ºå¤§å‹æ–¹ç¨‹ä½œä¸ºæ¨¡å‹çš„æ–¹å¼ã€‚*ç¬¦å·å›å½’*ï¼ˆSRï¼‰å°†è¿™ä¸€è¿‡ç¨‹æ¨å‘æè‡´ï¼Œé€šè¿‡è¿­ä»£åœ°æ”¹å˜å…¬å¼çš„ç»„æˆéƒ¨åˆ†ä»¥æ›´å¥½åœ°æ‹Ÿåˆæ•°æ®ã€‚SRå¯»æ±‚ä»¥ï¼ˆå¸Œæœ›æ˜¯ä¼˜é›…çš„ï¼‰æ•°å­¦è¡¨è¾¾å¼å½¢å¼çš„æ•°æ®å‡†ç¡®æ¨¡å‹ã€‚SRé€šå¸¸è¢«è®¤ä¸ºæ˜¯å›°éš¾çš„ï¼Œå¹¶ä¸”é€šå¸¸ä½¿ç”¨è¿›åŒ–ç®—æ³•å°è¯•ã€‚å¦‚æœä½ æœ‰è¡¨æ ¼æ•°æ®æˆ–è€…å¯ä»¥ç”¨æ–¹ç¨‹æè¿°çš„æ•°æ®ï¼Œé‚£ä¹ˆç¬¦å·å›å½’æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚
- en: Suppose you have a two-dimensional dataset like the following.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ æœ‰ä¸€ä¸ªäºŒç»´æ•°æ®é›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This has created a dataset with 100 data points, with 5 features each. The relation
    to model is 2.5382 <math alttext="cosine left-parenthesis x 3 right-parenthesis
    plus x 0 squared minus 0.5"><mrow><mo form="prefix">cos</mo> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi>x</mi>
    <mn>0</mn> <mn>2</mn></msubsup> <mo>-</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math>
    . Now, letâ€™s create a [PySR](https://oreil.ly/GeV6M) model and train it. PySRâ€™s
    main interface is in the style of scikit-learn.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ•°æ®é›†åŒ…å« 100 ä¸ªæ•°æ®ç‚¹ï¼Œæ¯ä¸ªæ•°æ®ç‚¹æœ‰ 5 ä¸ªç‰¹å¾ã€‚ä¸æ¨¡å‹çš„å…³ç³»æ˜¯ 2.5382 <math alttext="cosine left-parenthesis
    x 3 right-parenthesis plus x 0 squared minus 0.5"><mrow><mo form="prefix">cos</mo>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>)</mo></mrow> <mo>+</mo>
    <msubsup><mi>x</mi> <mn>0</mn> <mn>2</mn></msubsup> <mo>-</mo> <mn>0</mn> <mo>.</mo>
    <mn>5</mn></mrow></math> ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª [PySR](https://oreil.ly/GeV6M) æ¨¡å‹å¹¶è¿›è¡Œè®­ç»ƒã€‚PySR
    çš„ä¸»è¦æ¥å£é‡‡ç”¨äº†ç±»ä¼¼ scikit-learn çš„é£æ ¼ã€‚
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This will set up the model for 40 iterations of the search code, which contains
    hundreds of thousands of mutations and equation evaluations. You can then fit
    the model to the data by running `model.fit(X, y)`. Internally, this launches
    a Julia process, which will do a multithreaded search for equations to fit the
    dataset.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è®¾ç½®æ¨¡å‹ä¸ºæœç´¢ä»£ç çš„ 40 æ¬¡è¿­ä»£ï¼Œå…¶ä¸­åŒ…å«æ•°åä¸‡æ¬¡çš„å˜å¼‚å’Œæ–¹ç¨‹æ±‚è§£ã€‚ç„¶åï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œ `model.fit(X, y)` å°†æ¨¡å‹æ‹Ÿåˆåˆ°æ•°æ®ä¸Šã€‚åœ¨å†…éƒ¨ï¼Œè¿™å°†å¯åŠ¨ä¸€ä¸ª
    Julia è¿›ç¨‹ï¼Œè¯¥è¿›ç¨‹å°†è¿›è¡Œå¤šçº¿ç¨‹æœç´¢ä»¥é€‚åº”æ•°æ®é›†çš„æ–¹ç¨‹å¼ã€‚
- en: Note
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: If youâ€™re not familiar with the [Julia language](https://julialang.org), itâ€™s
    incredibly useful for machine learning. Julia is a dynamic, general-purpose programming
    language capable of high-performance scientific computing with high-level code.
    It is known for being able to handle any kind of UTF-8 encoding like math symbols
    and emojis.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ [Julia è¯­è¨€](https://julialang.org)ï¼Œå®ƒå¯¹æœºå™¨å­¦ä¹ éå¸¸æœ‰ç”¨ã€‚Julia æ˜¯ä¸€ç§åŠ¨æ€çš„ã€é€šç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼Œèƒ½å¤Ÿè¿›è¡Œé«˜æ€§èƒ½ç§‘å­¦è®¡ç®—ï¼Œæ”¯æŒé«˜çº§ä»£ç ä¸­çš„
    UTF-8 ç¼–ç ï¼Œå¦‚æ•°å­¦ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·ã€‚
- en: If you want to learn more, Oâ€™Reilly has some great resources, such as [â€œLearning
    Juliaâ€](https://oreil.ly/E58zY).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šï¼ŒOâ€™Reilly æœ‰ä¸€äº›å¾ˆæ£’çš„èµ„æºï¼Œæ¯”å¦‚ [â€œå­¦ä¹  Juliaâ€](https://oreil.ly/E58zY)ã€‚
- en: Equations will be printed during training, and once you are satisfied, you may
    quit early by hitting `'q'` and then `\<enter\>`. After the model has been fit,
    you can run `model.predict(X)` to see the predictions on a given dataset. Run
    `print(model)` to print the learned equations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼å°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰“å°å‡ºæ¥ï¼Œä¸€æ—¦æ‚¨æ»¡æ„ï¼Œå¯ä»¥é€šè¿‡æŒ‰ `'q'` ç„¶å `\<enter\>` æ¥æå‰é€€å‡ºã€‚æ¨¡å‹æ‹Ÿåˆå®Œæˆåï¼Œæ‚¨å¯ä»¥è¿è¡Œ `model.predict(X)`
    æ¥æŸ¥çœ‹åœ¨ç»™å®šæ•°æ®é›†ä¸Šçš„é¢„æµ‹ç»“æœã€‚è¿è¡Œ `print(model)` å¯ä»¥æ‰“å°å‡ºå­¦ä¹ åˆ°çš„æ–¹ç¨‹å¼ã€‚
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This arrow in the `pick` column indicates which equation is currently selected
    by your `model_selection` strategy for prediction (you may change `model_selection`
    after `.fit(X, y)` as well). `model.equations_` is a Pandas DataFrame containing
    all equations, including callable format (`lambda_format`), SymPy format (`sympy_format`,
    which you can also get with `model.sympy()`), and even JAX and PyTorch format
    (both of which are differentiable and that you can get with `model.jax()` and
    `model.pytorch()`).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç®­å¤´åœ¨ `pick` åˆ—ä¸­è¡¨ç¤ºæ‚¨çš„ `model_selection` ç­–ç•¥å½“å‰é€‰æ‹©çš„æ–¹ç¨‹å¼ç”¨äºé¢„æµ‹ï¼ˆæ‚¨ä¹Ÿå¯ä»¥åœ¨ `.fit(X, y)` åæ›´æ”¹
    `model_selection`ï¼‰ã€‚`model.equations_` æ˜¯ä¸€ä¸ª Pandas DataFrameï¼ŒåŒ…å«æ‰€æœ‰æ–¹ç¨‹å¼ï¼ŒåŒ…æ‹¬å¯è°ƒç”¨æ ¼å¼ï¼ˆ`lambda_format`ï¼‰ã€SymPy
    æ ¼å¼ï¼ˆ`sympy_format`ï¼Œæ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ `model.sympy()` è·å¾—ï¼‰ã€ä»¥åŠ JAX å’Œ PyTorch æ ¼å¼ï¼ˆè¿™ä¸¤è€…éƒ½æ˜¯å¯å¾®åˆ†çš„ï¼Œå¯ä»¥é€šè¿‡
    `model.jax()` å’Œ `model.pytorch()` è·å¾—ï¼‰ã€‚
- en: Support vector machines
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ”¯æŒå‘é‡æœº
- en: A precursor to neural network methods, *support vector machines* (SVMs) are
    a set of supervised learning methods used for classification, regression, and
    outlier detection. SVMs are great for when you have high-dimensional data, with
    possibly more dimensions than samples in your dataset. SVMs are memory-efficient
    and can be customized with their kernel functions (though packages like sklearn
    already have some great ones). The main downside of SVMs is that regularization
    is crucial if you want to avoid overfitting. Unlike methods like logistic regression,
    SVMs do not provide probability estimates. You need to turn to methods like fivefold
    cross-validation to get those estimates, and doing so will likely undo any computing
    efficiency advantages from using SVMs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¥ç»ç½‘ç»œæ–¹æ³•çš„å‰èº«ï¼Œ*æ”¯æŒå‘é‡æœº*ï¼ˆSVMï¼‰æ˜¯ä¸€ç»„ç”¨äºåˆ†ç±»ã€å›å½’å’Œå¼‚å¸¸æ£€æµ‹çš„ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚SVM åœ¨å¤„ç†é«˜ç»´æ•°æ®æ—¶æ•ˆæœæ˜¾è‘—ï¼Œå¯èƒ½æ¯”æ•°æ®é›†ä¸­çš„æ ·æœ¬æ›´å¤šçš„ç»´åº¦è¿˜è¦å¤šã€‚SVM
    å†…å­˜æ•ˆç‡é«˜ï¼Œå¯ä»¥é€šè¿‡æ ¸å‡½æ•°è¿›è¡Œå®šåˆ¶åŒ–ï¼ˆè™½ç„¶åƒ sklearn è¿™æ ·çš„åŒ…å·²ç»å…·æœ‰äº†ä¸€äº›å‡ºè‰²çš„æ ¸å‡½æ•°ï¼‰ã€‚SVM çš„ä¸»è¦ç¼ºç‚¹æ˜¯å¦‚æœè¦é¿å…è¿‡æ‹Ÿåˆï¼Œæ­£åˆ™åŒ–è‡³å…³é‡è¦ã€‚ä¸é€»è¾‘å›å½’ç­‰æ–¹æ³•ä¸åŒï¼ŒSVM
    ä¸æä¾›æ¦‚ç‡ä¼°è®¡ã€‚æ‚¨éœ€è¦ä½¿ç”¨ç±»ä¼¼äº”æŠ˜äº¤å‰éªŒè¯çš„æ–¹æ³•æ¥è·å–è¿™äº›ä¼°è®¡ï¼Œè¿™æ ·åšå¯èƒ½ä¼šæ¶ˆé™¤ä½¿ç”¨ SVM çš„è®¡ç®—æ•ˆç‡ä¼˜åŠ¿ã€‚
- en: To use SVMs, there are many methods, but the most popular is scikit-learnâ€™s
    [support vector machine implementation](https://oreil.ly/5NYAF).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨SVMï¼Œæœ‰è®¸å¤šæ–¹æ³•ï¼Œä½†æœ€æµè¡Œçš„æ˜¯scikit-learnçš„[æ”¯æŒå‘é‡æœºå®ç°](https://oreil.ly/5NYAF)ã€‚
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Decision tree
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘
- en: Like SVMs, *decision trees* excel at fitting to nonlinear relationships (though
    they can struggle with linear relationships). Where decision trees excel is in
    sorting data into distinct groups and providing intuitive visualizations.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºSVMï¼Œ*å†³ç­–æ ‘*åœ¨æ‹Ÿåˆéçº¿æ€§å…³ç³»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ˆå°½ç®¡å®ƒä»¬åœ¨å¤„ç†çº¿æ€§å…³ç³»æ—¶å¯èƒ½ä¼šé‡åˆ°å›°éš¾ï¼‰ã€‚å†³ç­–æ ‘æ“…é•¿çš„æ˜¯å°†æ•°æ®åˆ†ç±»ä¸ºä¸åŒç»„ï¼Œå¹¶æä¾›ç›´è§‚çš„å¯è§†åŒ–ã€‚
- en: 'Like many other machine learning methods, scikit-learn has a variety of [decision
    tree variants available](https://oreil.ly/u81jy). Itâ€™s also worth talking about
    one of the more popular interpretable decision tree algorithms: [XGBoost](https://oreil.ly/EkTuq)
    (which also has a [scikit-learn-like API](https://oreil.ly/pz1qL)).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è®¸å¤šå…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•ç±»ä¼¼ï¼Œscikit-learnæä¾›äº†å¤šç§[å†³ç­–æ ‘å˜ä½“](https://oreil.ly/u81jy)ã€‚è¿˜å€¼å¾—ä¸€æçš„æ˜¯å…¶ä¸­ä¸€ç§æ›´å—æ¬¢è¿çš„å¯è§£é‡Šå†³ç­–æ ‘ç®—æ³•ï¼š[XGBoost](https://oreil.ly/EkTuq)ï¼ˆå®ƒä¹Ÿå…·æœ‰ç±»ä¼¼äº[scikit-learnçš„API](https://oreil.ly/pz1qL)ï¼‰ã€‚
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Decision rules
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å†³ç­–è§„åˆ™
- en: A *decision rule* is a set of if-then statements that can be used to make a
    decision. If the conditions in the if-then statement are met, then the decision
    rule will be followed. Decision rules are often used in decision-making processes
    because they are easy to understand and can be applied quickly. When most people
    start out programming in languages like Python, itâ€™s common for them to use if-then
    statements extensively. As such, this can be a very intuitive way of understanding
    the logic behind a decision.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*å†³ç­–è§„åˆ™*æ˜¯ä¸€ç»„if-thenè¯­å¥ï¼Œå¯ç”¨äºåšå‡ºå†³ç­–ã€‚å¦‚æœif-thenè¯­å¥ä¸­çš„æ¡ä»¶å¾—åˆ°æ»¡è¶³ï¼Œåˆ™ä¼šæ‰§è¡Œå†³ç­–è§„åˆ™ã€‚å†³ç­–è§„åˆ™é€šå¸¸ç”¨äºå†³ç­–è¿‡ç¨‹ä¸­ï¼Œå› ä¸ºå®ƒä»¬æ˜“äºç†è§£å¹¶ä¸”å¯ä»¥å¿«é€Ÿåº”ç”¨ã€‚å½“å¤§å¤šæ•°äººå¼€å§‹ä½¿ç”¨åƒPythonè¿™æ ·çš„ç¼–ç¨‹è¯­è¨€æ—¶ï¼Œä»–ä»¬é€šå¸¸ä¼šå¹¿æ³›ä½¿ç”¨if-thenè¯­å¥ã€‚å› æ­¤ï¼Œè¿™å¯ä»¥æˆä¸ºç†è§£å†³ç­–é€»è¾‘çš„ä¸€ç§éå¸¸ç›´è§‚çš„æ–¹å¼ã€‚'
- en: 'Creating all these if-then statements for a dataset with a large number of
    features can be very time consuming. There are many algorithms for coming up with
    these rules. Here are three of the most popular:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªåŒ…å«å¤§é‡ç‰¹å¾çš„æ•°æ®é›†çš„if-thenè¯­å¥å¯èƒ½éå¸¸è€—æ—¶ã€‚æœ‰è®¸å¤šç®—æ³•å¯ä»¥ç”Ÿæˆè¿™äº›è§„åˆ™ã€‚ä»¥ä¸‹æ˜¯å…¶ä¸­ä¸‰ç§æœ€å—æ¬¢è¿çš„ï¼š
- en: '*OneR*'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*OneR*'
- en: OneR learns rules based on a single feature. Itâ€™s one of the simplest and easiest-to-understand
    approaches. While other algorithms may produce more accurate rules, OneR is fast
    and easy enough to serve as a benchmark to compare other algorithms against. To
    leverage OneR in Python, you can use the `OneRClassifier` implementation in the
    [MLxtend library](https://oreil.ly/BDIoD).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: OneRåŸºäºå•ä¸ªç‰¹å¾å­¦ä¹ è§„åˆ™ã€‚è¿™æ˜¯ä¸€ç§æœ€ç®€å•ä¸”æ˜“äºç†è§£çš„æ–¹æ³•ä¹‹ä¸€ã€‚è™½ç„¶å…¶ä»–ç®—æ³•å¯èƒ½ç”Ÿæˆæ›´å‡†ç¡®çš„è§„åˆ™ï¼Œä½†OneRå¿«é€Ÿè€Œç®€å•ï¼Œè¶³ä»¥ä½œä¸ºä¸å…¶ä»–ç®—æ³•è¿›è¡Œæ¯”è¾ƒçš„åŸºå‡†ã€‚è¦åœ¨Pythonä¸­ä½¿ç”¨OneRï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[MLxtendåº“](https://oreil.ly/BDIoD)ä¸­çš„`OneRClassifier`å®ç°ã€‚
- en: Note
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: OneR is a very simple algorithm that assumes the data is categorical. It will
    not work as well with continuous data. You probably donâ€™t want to rely on it for
    complex NLP or computer vision tasks.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: OneRæ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ç®—æ³•ï¼Œå‡è®¾æ•°æ®æ˜¯åˆ†ç±»çš„ã€‚å®ƒåœ¨å¤„ç†è¿ç»­æ•°æ®æ—¶æ•ˆæœä¸ä½³ã€‚æ‚¨å¯èƒ½ä¸å¸Œæœ›ä¾èµ–å®ƒæ¥å¤„ç†å¤æ‚çš„è‡ªç„¶è¯­è¨€å¤„ç†æˆ–è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚
- en: '*Sequential covering*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*é¡ºåºè¦†ç›–*'
- en: Sequential covering is an iterative method that adds new if-then rules, removes
    the data points that are explained by the new rules, and repeats the process for
    the remaining data points until all the data points are explained. For using decision
    rules generated via sequential covering, [Oracleâ€™s Skater library](https://oreil.ly/LkiWk)
    has good implementations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºåºè¦†ç›–æ˜¯ä¸€ç§è¿­ä»£æ–¹æ³•ï¼Œé€šè¿‡æ·»åŠ æ–°çš„if-thenè§„åˆ™ï¼Œç§»é™¤è¢«æ–°è§„åˆ™è§£é‡Šçš„æ•°æ®ç‚¹ï¼Œå¹¶é‡å¤è¯¥è¿‡ç¨‹ï¼Œç›´åˆ°æ‰€æœ‰æ•°æ®ç‚¹éƒ½å¾—åˆ°è§£é‡Šã€‚ä½¿ç”¨é¡ºåºè¦†ç›–ç”Ÿæˆçš„å†³ç­–è§„åˆ™æ—¶ï¼Œ[Oracleçš„Skateråº“](https://oreil.ly/LkiWk)æœ‰è‰¯å¥½çš„å®ç°ã€‚
- en: '*Bayesian rule lists*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*è´å¶æ–¯è§„åˆ™åˆ—è¡¨*'
- en: This approach involves bringing in various frequentist statistics about the
    data as a starting point. This prior knowledge about the patterns can then be
    used to create a decision list based on Bayesian statistics. Depending on the
    implementation, this may also have some overlap with sequential covering. For
    implementing decision rules via Bayesian rule lists, a tool like the [iModels](https://oreil.ly/yDK4M)
    package is a great choice; it has an interface similar to that of sklearn. It
    also contains implementation of specific decision rules algorithms like Friedman
    and Popescuâ€™s RuleFit.^([13](ch03.html#idm45621841570432))
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•æ¶‰åŠå¼•å…¥å…³äºæ•°æ®çš„å„ç§é¢‘ç‡ç»Ÿè®¡ä½œä¸ºèµ·ç‚¹çš„å…ˆéªŒçŸ¥è¯†ã€‚è¿™äº›å…³äºæ¨¡å¼çš„å…ˆéªŒçŸ¥è¯†å¯ä»¥ç”¨æ¥åŸºäºè´å¶æ–¯ç»Ÿè®¡åˆ›å»ºå†³ç­–åˆ—è¡¨ã€‚æ ¹æ®å®ç°æ–¹å¼ï¼Œè¿™å¯èƒ½è¿˜ä¸é¡ºåºè¦†ç›–æœ‰ä¸€äº›é‡å ã€‚å¯¹äºé€šè¿‡è´å¶æ–¯è§„åˆ™åˆ—è¡¨å®ç°å†³ç­–è§„åˆ™ï¼Œåƒ[iModels](https://oreil.ly/yDK4M)åŒ…è¿™æ ·çš„å·¥å…·æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ï¼›å®ƒå…·æœ‰ç±»ä¼¼äºsklearnçš„æ¥å£ã€‚å®ƒè¿˜åŒ…å«ç‰¹å®šå†³ç­–è§„åˆ™ç®—æ³•çš„å®ç°ï¼Œå¦‚Friedmanå’ŒPopescuçš„RuleFitã€‚^([13](ch03.html#idm45621841570432))
- en: Beyond intrinsically interpretable models
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¶…è¶Šå†…åœ¨å¯è§£é‡Šæ¨¡å‹
- en: All of the models described thus far have some easy way to transform their parameters
    into human-understandable guides to their underlying decision making. However,
    for a lot of domains, you may want a model that predicts the patterns in the data
    well regardless of how easy to understand its parameters are. Since 2012, neural
    networkâ€“based methods have replaced a lot of the methods weâ€™ve described here
    in many domains. Given how much neural networks can vary, there should be interpretability
    methods that arenâ€™t specific to any one model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‰€æœ‰æè¿°çš„æ¨¡å‹éƒ½æœ‰ä¸€äº›ç®€å•çš„æ–¹æ³•æ¥å°†å®ƒä»¬çš„å‚æ•°è½¬åŒ–ä¸ºäººç±»å¯ç†è§£çš„æŒ‡å¯¼ï¼Œä»¥è§£é‡Šå®ƒä»¬çš„åŸºç¡€å†³ç­–ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šé¢†åŸŸï¼Œæ‚¨å¯èƒ½å¸Œæœ›æœ‰ä¸€ä¸ªé¢„æµ‹æ•°æ®æ¨¡å¼çš„æ¨¡å‹ï¼Œè€Œä¸ç®¡å…¶å‚æ•°çš„æ˜“ç†è§£ç¨‹åº¦å¦‚ä½•ã€‚è‡ª2012å¹´ä»¥æ¥ï¼ŒåŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•å·²ç»åœ¨è®¸å¤šé¢†åŸŸå–ä»£äº†æˆ‘ä»¬åœ¨è¿™é‡Œæè¿°çš„è®¸å¤šæ–¹æ³•ã€‚è€ƒè™‘åˆ°ç¥ç»ç½‘ç»œçš„å·¨å¤§å˜åŒ–ï¼Œåº”è¯¥æœ‰ä¸€äº›ä¸ç‰¹å®šäºä»»ä½•ä¸€ä¸ªæ¨¡å‹çš„è§£é‡Šæ–¹æ³•ã€‚
- en: Local Model-Agnostic Interpretability Methods
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ¬åœ°æ¨¡å‹æ— å…³è§£é‡Šæ–¹æ³•
- en: As weâ€™ve mentioned, local interpretability focuses on making sense of individual
    predictions. Many of the previously discussed models had built-in methods for
    interpreting local predictions, such as the terms in a decision tree or multiple
    linear regression. However, if weâ€™re comparing multiple model types that include
    many different architectures of neural networks, using these intrinsic interpretability
    methods will be like comparing apples and oranges. This is why weâ€™d ideally like
    to have a way to combine local interpretability with model-agnostic interpretability.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œæœ¬åœ°å¯è§£é‡Šæ€§ä¾§é‡äºç†è§£ä¸ªä½“é¢„æµ‹çš„æ„ä¹‰ã€‚è®¸å¤šå…ˆå‰è®¨è®ºçš„æ¨¡å‹éƒ½æœ‰å†…ç½®çš„æ–¹æ³•æ¥è§£é‡Šå±€éƒ¨é¢„æµ‹ï¼Œä¾‹å¦‚å†³ç­–æ ‘ä¸­çš„é¡¹æˆ–å¤šé‡çº¿æ€§å›å½’ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬æ¯”è¾ƒåŒ…æ‹¬è®¸å¤šä¸åŒç¥ç»ç½‘ç»œæ¶æ„çš„å¤šä¸ªæ¨¡å‹ç±»å‹ï¼Œä½¿ç”¨è¿™äº›å†…åœ¨å¯è§£é‡Šæ€§æ–¹æ³•å°†åƒæ˜¯åœ¨æ¯”è¾ƒè‹¹æœå’Œæ©™å­ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ç†æƒ³åœ°å¸Œæœ›æœ‰ä¸€ç§æ–¹æ³•æ¥ç»“åˆå±€éƒ¨å¯è§£é‡Šæ€§å’Œæ¨¡å‹æ— å…³çš„å¯è§£é‡Šæ€§ã€‚
- en: Local interpretable model-agnostic explanation
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ¬åœ°å¯è§£é‡Šæ¨¡å‹æ— å…³è§£é‡Š
- en: 'A *local interpretable model-agnostic explanation* (LIME) explains a prediction
    by replacing the complex model with a locally interpretable surrogate model. You
    can apply this technique to image, text, and even tabular data. The general steps
    of this technique are as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª*æœ¬åœ°å¯è§£é‡Šæ¨¡å‹æ— å…³è§£é‡Š*ï¼ˆLIMEï¼‰é€šè¿‡ç”¨æœ¬åœ°å¯è§£é‡Šçš„æ›¿ä»£æ¨¡å‹æ›¿æ¢å¤æ‚æ¨¡å‹æ¥è§£é‡Šé¢„æµ‹ã€‚æ‚¨å¯ä»¥å°†æ­¤æŠ€æœ¯åº”ç”¨äºå›¾åƒã€æ–‡æœ¬ï¼Œç”šè‡³è¡¨æ ¼æ•°æ®ã€‚æ­¤æŠ€æœ¯çš„ä¸€èˆ¬æ­¥éª¤å¦‚ä¸‹ï¼š
- en: Select a bunch of instances of outputs from the model you want to interpret.
    (This is *local*, because weâ€™re only interpreting this limited set rather than
    the *global* set of all possible model outputs.)
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©æ¨¡å‹è¾“å‡ºçš„ä¸€å †å®ä¾‹æ¥è§£é‡Šæ‚¨æƒ³è¦è§£é‡Šçš„æ¨¡å‹ã€‚ï¼ˆè¿™æ˜¯*å±€éƒ¨*çš„ï¼Œå› ä¸ºæˆ‘ä»¬åªè§£é‡Šè¿™ä¸ªæœ‰é™é›†è€Œä¸æ˜¯æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹è¾“å‡ºçš„*å…¨å±€*é›†åˆã€‚ï¼‰
- en: Create a surrogate model that reproduces the behavior of the model you want
    to interpret on these instances. You will know nothing about the model internals,
    only what the outputs look like.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªæ›¿ä»£æ¨¡å‹ï¼Œå®ƒå¤åˆ¶æ‚¨æƒ³è¦è§£é‡Šçš„æ¨¡å‹åœ¨è¿™äº›å®ä¾‹ä¸Šçš„è¡Œä¸ºã€‚æ‚¨å°†ä¸äº†è§£æ¨¡å‹å†…éƒ¨ï¼ŒåªçŸ¥é“è¾“å‡ºçš„å¤–è§‚ã€‚
- en: Create random perturbations of the input data and see how the surrogate model
    classifies them.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºè¾“å…¥æ•°æ®çš„éšæœºæ‰°åŠ¨ï¼Œå¹¶æŸ¥çœ‹æ›¿ä»£æ¨¡å‹å¦‚ä½•å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚
- en: Use these classification boundaries to create a decision boundary that can be
    used to explain the modelâ€™s predictions.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™äº›åˆ†ç±»è¾¹ç•Œåˆ›å»ºä¸€ä¸ªå†³ç­–è¾¹ç•Œï¼Œå¯ç”¨äºè§£é‡Šæ¨¡å‹çš„é¢„æµ‹ã€‚
- en: 'If you want a more formal mathematical version of this, assume the input data
    is <math alttext="x"><mi>x</mi></math> . The complex model to be interpreted is
    <math alttext="f"><mi>f</mi></math> , the simple interpretable model is <math
    alttext="g"><mi>g</mi></math> (with <math alttext="g element-of upper G"><mrow><mi>g</mi>
    <mo>âˆˆ</mo> <mi>G</mi></mrow></math> indicating that it is in the set of sparse
    linear models, like the kind discussed previously), and <math alttext="pi Subscript
    x"><msub><mi>Ï€</mi> <mi>x</mi></msub></math> is a proximity measure indicating
    the size of the local neighborhood of your data points <math alttext="x"><mi>x</mi></math>
    . From this, you would create a loss function <math alttext="script upper L"><mi>â„’</mi></math>
    that minimizes the difference between the outputs of <math alttext="f"><mi>f</mi></math>
    and <math alttext="g"><mi>g</mi></math> to be within <math alttext="pi Subscript
    x"><msub><mi>Ï€</mi> <mi>x</mi></msub></math> . Without any modification, this
    process would just make a complicated <math alttext="g"><mi>g</mi></math> nearly
    identical to <math alttext="f"><mi>f</mi></math> . This is why you add <math alttext="normal
    upper Omega left-parenthesis g right-parenthesis"><mrow><mi>Î©</mi> <mo>(</mo>
    <mi>g</mi> <mo>)</mo></mrow></math> , which is a regularizer that limits the complexity
    of your interpretable model <math alttext="g"><mi>g</mi></math> . This brings
    you to your general equation for training LIME:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦è¿™ä¸ªæ›´æ­£å¼çš„æ•°å­¦ç‰ˆæœ¬ï¼Œå‡è®¾è¾“å…¥æ•°æ®ä¸º<math alttext="x"><mi>x</mi></math>ã€‚è¦è§£é‡Šçš„å¤æ‚æ¨¡å‹æ˜¯<math alttext="f"><mi>f</mi></math>ï¼Œç®€å•å¯è§£é‡Šæ¨¡å‹æ˜¯<math
    alttext="g"><mi>g</mi></math>ï¼ˆå…¶ä¸­<math alttext="g element-of upper G"><mrow><mi>g</mi>
    <mo>âˆˆ</mo> <mi>G</mi></mrow></math>è¡¨æ˜å®ƒå±äºç¨€ç–çº¿æ€§æ¨¡å‹çš„é›†åˆï¼Œå¦‚å…ˆå‰è®¨è®ºçš„é‚£ç§ï¼‰ï¼Œè€Œ<math alttext="pi
    Subscript x"><msub><mi>Ï€</mi> <mi>x</mi></msub></math>æ˜¯ä¸€ä¸ªæŒ‡ç¤ºæ‚¨çš„æ•°æ®ç‚¹<math alttext="x"><mi>x</mi></math>å±€éƒ¨é‚»åŸŸå¤§å°çš„æ¥è¿‘åº¦é‡ã€‚ä»è¿™é‡Œï¼Œæ‚¨å°†åˆ›å»ºä¸€ä¸ªæŸå¤±å‡½æ•°<math
    alttext="script upper L"><mi>â„’</mi></math>ï¼Œå®ƒå°†æœ€å°åŒ–<math alttext="f"><mi>f</mi></math>å’Œ<math
    alttext="g"><mi>g</mi></math>çš„è¾“å‡ºä¹‹é—´çš„å·®å¼‚ï¼Œä½¿å…¶åœ¨<math alttext="pi Subscript x"><msub><mi>Ï€</mi>
    <mi>x</mi></msub></math>å†…ã€‚åœ¨ä¸è¿›è¡Œä»»ä½•ä¿®æ”¹çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªè¿‡ç¨‹å°†ä½¿å¤æ‚çš„<math alttext="g"><mi>g</mi></math>å‡ ä¹ä¸<math
    alttext="f"><mi>f</mi></math>ç›¸åŒã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ‚¨è¦æ·»åŠ <math alttext="normal upper Omega left-parenthesis
    g right-parenthesis"><mrow><mi>Î©</mi> <mo>(</mo> <mi>g</mi> <mo>)</mo></mrow></math>ï¼Œè¿™æ˜¯ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œé™åˆ¶æ‚¨çš„å¯è§£é‡Šæ¨¡å‹<math
    alttext="g"><mi>g</mi></math>çš„å¤æ‚æ€§ã€‚è¿™å°†å¸¦æ‚¨åˆ°è®­ç»ƒLIMEçš„ä¸€èˆ¬æ–¹ç¨‹å¼ã€‚
- en: <math alttext="xi left-parenthesis x right-parenthesis equals arg min Underscript
    g element-of upper G Endscripts script upper L left-parenthesis f comma g comma
    pi Subscript x Baseline right-parenthesis plus normal upper Omega left-parenthesis
    g right-parenthesis" display="block"><mrow><mi>Î¾</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mrow><mo form="prefix">arg</mo><mo form="prefix"
    movablelimits="true">min</mo></mrow> <mrow><mi>g</mi><mo>âˆˆ</mo><mi>G</mi></mrow></munder>
    <mi>â„’</mi> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>g</mi> <mo>,</mo> <msub><mi>Ï€</mi>
    <mi>x</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mi>Î©</mi> <mrow><mo>(</mo> <mi>g</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="xi left-parenthesis x right-parenthesis equals arg min Underscript
    g element-of upper G Endscripts script upper L left-parenthesis f comma g comma
    pi Subscript x Baseline right-parenthesis plus normal upper Omega left-parenthesis
    g right-parenthesis" display="block"><mrow><mi>Î¾</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <munder><mrow><mo form="prefix">arg</mo><mo form="prefix"
    movablelimits="true">min</mo></mrow> <mrow><mi>g</mi><mo>âˆˆ</mo><mi>G</mi></mrow></munder>
    <mi>â„’</mi> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>g</mi> <mo>,</mo> <msub><mi>Ï€</mi>
    <mi>x</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mi>Î©</mi> <mrow><mo>(</mo> <mi>g</mi>
    <mo>)</mo></mrow></mrow></math>
- en: 'The loss function is more specifically described as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°æ›´å…·ä½“åœ°æè¿°å¦‚ä¸‹ï¼š
- en: "<math alttext=\"script upper L left-parenthesis f comma g comma pi Subscript\
    \ x Baseline right-parenthesis equals sigma-summation Underscript z comma z prime\
    \ element-of script upper Z Endscripts pi Subscript x Baseline left-parenthesis\
    \ z right-parenthesis left-parenthesis f left-parenthesis z right-parenthesis\
    \ minus g left-parenthesis z prime right-parenthesis right-parenthesis squared\"\
    \ display=\"block\"><mrow><mi>â„’</mi> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>g</mi>\
    \ <mo>,</mo> <msub><mi>Ï€</mi> <mi>x</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>âˆ‘</mo>\
    \ <mrow><mi>z</mi><mo>,</mo><mi>z</mi><mi>Ã¢</mi><mi>\x80</mi><mi>\x99</mi><mo>âˆˆ</mo><mi>\U0001D4B5\
    </mi></mrow></munder> <msub><mi>Ï€</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi>\
    \ <mo>)</mo></mrow> <msup><mrow><mo>(</mo><mi>f</mi><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>-</mo><mi>g</mi><mrow><mo>(</mo><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mo>)</mo></mrow><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>"
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: "<math alttext=\"script upper L left-parenthesis f comma g comma pi Subscript\
    \ x Baseline right-parenthesis equals sigma-summation Underscript z comma z prime\
    \ element-of script upper Z Endscripts pi Subscript x Baseline left-parenthesis\
    \ z right-parenthesis left-parenthesis f left-parenthesis z right-parenthesis\
    \ minus g left-parenthesis z prime right-parenthesis right-parenthesis squared\"\
    \ display=\"block\"><mrow><mi>â„’</mi> <mrow><mo>(</mo> <mi>f</mi> <mo>,</mo> <mi>g</mi>\
    \ <mo>,</mo> <msub><mi>Ï€</mi> <mi>x</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>âˆ‘</mo>\
    \ <mrow><mi>z</mi><mo>,</mo><mi>z</mi><mi>Ã¢</mi><mi>\x80</mi><mi>\x99</mi><mo>âˆˆ</mo><mi>\U0001D4B5\
    </mi></mrow></munder> <msub><mi>Ï€</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi>\
    \ <mo>)</mo></mrow> <msup><mrow><mo>(</mo><mi>f</mi><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow><mo>-</mo><mi>g</mi><mrow><mo>(</mo><msup><mi>z</mi>\
    \ <mo>'</mo></msup> <mo>)</mo></mrow><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>"
- en: Intuitively, an *explanation* is a local linear approximation of the modelâ€™s
    behavior. While the model may be very complex globally, it is easier to approximate
    it around the vicinity of a particular instance. While treating the model as a
    black box, you perturb the instance you want to explain and learn a sparse linear
    model around it, as an explanation.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚åœ°è¯´ï¼Œ*è§£é‡Š*æ˜¯æ¨¡å‹è¡Œä¸ºçš„å±€éƒ¨çº¿æ€§é€¼è¿‘ã€‚è™½ç„¶æ¨¡å‹åœ¨å…¨å±€ä¸Šå¯èƒ½éå¸¸å¤æ‚ï¼Œä½†åœ¨ç‰¹å®šå®ä¾‹çš„å‘¨å›´è¿‘ä¼¼å®ƒä¼šæ›´å®¹æ˜“ã€‚å½“å°†æ¨¡å‹è§†ä¸ºé»‘ç›’æ—¶ï¼Œæ‚¨æ‰°åŠ¨è¦è§£é‡Šçš„å®ä¾‹ï¼Œå¹¶åœ¨å…¶å‘¨å›´å­¦ä¹ ä¸€ä¸ªç¨€ç–çº¿æ€§æ¨¡å‹ï¼Œä½œä¸ºè§£é‡Šã€‚
- en: The the modelâ€™s decision function is nonlinear. The bright red cross is the
    instance being explained (letâ€™s call it *X*). You sample instances around *X*,
    and weight them according to their proximity to *X* (weight here is indicated
    by size). You then learn a linear model (dashed line) that approximates the model
    well in the vicinity of *X*, but not necessarily globally.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„å†³ç­–å‡½æ•°æ˜¯éçº¿æ€§çš„ã€‚é²œçº¢è‰²çš„åå­—æ˜¯è¢«è§£é‡Šçš„å®ä¾‹ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º*X*ï¼‰ã€‚æ‚¨å¯¹*X*å‘¨å›´çš„å®ä¾‹è¿›è¡Œé‡‡æ ·ï¼Œå¹¶æ ¹æ®å®ƒä»¬ä¸*X*çš„æ¥è¿‘ç¨‹åº¦è¿›è¡ŒåŠ æƒï¼ˆè¿™é‡Œçš„æƒé‡ç”±å¤§å°è¡¨ç¤ºï¼‰ã€‚ç„¶åï¼Œæ‚¨å­¦ä¹ ä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼ˆè™šçº¿ï¼‰ï¼Œåœ¨*X*é™„è¿‘å¾ˆå¥½åœ°è¿‘ä¼¼æ¨¡å‹ï¼Œä½†ä¸ä¸€å®šåœ¨å…¨å±€èŒƒå›´å†…ã€‚
- en: 'Deep dive example: LIME on Vision Transformer models'
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ·±å…¥ç¤ºä¾‹ï¼šLIMEåœ¨è§†è§‰Transformeræ¨¡å‹ä¸Šçš„åº”ç”¨
- en: Numerous examples of LIME on CNN-based image classifiers exist. Since these
    are model-agnostic methods, itâ€™s worth demonstrating this by running LIME on [Vision
    Transformer (ViT) models](https://oreil.ly/yfZ2E) for image classification.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜åœ¨è®¸å¤šå…³äºLIMEåœ¨åŸºäºCNNçš„å›¾åƒåˆ†ç±»å™¨ä¸Šçš„ç¤ºä¾‹ã€‚ç”±äºè¿™äº›æ˜¯ä¸æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œå€¼å¾—é€šè¿‡åœ¨[è§†è§‰Transformer (ViT)æ¨¡å‹](https://oreil.ly/yfZ2E)ä¸Šè¿è¡ŒLIMEæ¥è¿›è¡Œæ¼”ç¤ºï¼Œç”¨äºå›¾åƒåˆ†ç±»ã€‚
- en: Note
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: You can find all the code associated with this tutorial in the notebook [*Chapter_3_LIME_for_Transformers.ipynb*](https://oreil.ly/lwDoi).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨ç¬”è®°æœ¬[*Chapter_3_LIME_for_Transformers.ipynb*](https://oreil.ly/lwDoi)ä¸­æ‰¾åˆ°ä¸æœ¬æ•™ç¨‹ç›¸å…³çš„æ‰€æœ‰ä»£ç ã€‚
- en: '[PRE21]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: For your NLP example, you can take a version of BERT fine-tuned for finance
    called finBERT. This is a BERT model that can do sentiment analysis on text data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‚¨çš„NLPç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä¸“é—¨é’ˆå¯¹é‡‘èé¢†åŸŸè¿›è¡Œäº†å¾®è°ƒçš„BERTç‰ˆæœ¬ï¼Œç§°ä¸ºfinBERTã€‚è¿™æ˜¯ä¸€ä¸ªBERTæ¨¡å‹ï¼Œå¯ä»¥å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚
- en: '[PRE22]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: For the `LimeTextExplainer` class, you need to specify a predictor function
    that will take the input and feed it through the tokenizer and model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº`LimeTextExplainer`ç±»ï¼Œæ‚¨éœ€è¦æŒ‡å®šä¸€ä¸ªé¢„æµ‹å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†æ¥å—è¾“å…¥å¹¶é€šè¿‡åˆ†è¯å™¨å’Œæ¨¡å‹è¿›è¡Œå¤„ç†ã€‚
- en: '[PRE23]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: For the actual text explainer, you feed in your sample sentence and the predictor
    function. For this demonstration, youâ€™ll set LIME to take two thousand samples.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå®é™…çš„æ–‡æœ¬è§£é‡Šå™¨ï¼Œæ‚¨éœ€è¦æä¾›æ ·æœ¬å¥å­å’Œé¢„æµ‹å‡½æ•°ã€‚åœ¨è¿™ä¸ªæ¼”ç¤ºä¸­ï¼Œæ‚¨å°†è®¾ç½® LIME æ¥é‡‡é›†ä¸¤åƒä¸ªæ ·æœ¬ã€‚
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In [FigureÂ 3-6](#lime-text), next to the output logits, you can see a breakdown
    of which features tilted the balance in favor of one output category or another.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ [å›¾Â 3-6](#lime-text) ä¸­ï¼Œé™¤äº†è¾“å‡ºçš„å¯¹æ•°ä¹‹å¤–ï¼Œæ‚¨è¿˜å¯ä»¥çœ‹åˆ°å“ªäº›ç‰¹å¾å€¾å‘äºæ”¯æŒæŸä¸ªè¾“å‡ºç±»åˆ«æˆ–å¦ä¸€ä¸ªã€‚
- en: '![ptml 0306](assets/ptml_0306.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0306](assets/ptml_0306.png)'
- en: Figure 3-6\. LIME running on text inputs
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-6\. LIME åœ¨æ–‡æœ¬è¾“å…¥ä¸Šè¿è¡Œ
- en: LIME isnâ€™t just for text classification; it can work for image models as well.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: LIME ä¸ä»…é€‚ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Œä¹Ÿé€‚ç”¨äºå›¾åƒæ¨¡å‹ã€‚
- en: '[PRE25]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Weâ€™re working with a PIL image at the start ([FigureÂ 3-7](#lime-image-input)).
    As with any torchvision model, weâ€™ll want to do some pre-processing. However,
    due to a quirk of the original LIME library, you need to add a workaround: the
    `LimeImageExplainer`.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»å¼€å§‹å¤„ç† PIL å›¾åƒï¼ˆ[å›¾Â 3-7](#lime-image-input)ï¼‰ã€‚ä¸ä»»ä½• torchvision æ¨¡å‹ä¸€æ ·ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œä¸€äº›é¢„å¤„ç†ã€‚ç„¶è€Œï¼Œç”±äºåŸå§‹
    LIME åº“çš„ä¸€ä¸ªæ€ªå¼‚ä¹‹å¤„ï¼Œæ‚¨éœ€è¦æ·»åŠ ä¸€ä¸ªè§£å†³æ–¹æ³•ï¼š`LimeImageExplainer`ã€‚
- en: '[PRE26]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![ptml 0307](assets/ptml_0307.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0307](assets/ptml_0307.png)'
- en: Figure 3-7\. LIME image input
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-7\. LIME å›¾åƒè¾“å…¥
- en: As with the text explainer, weâ€™ll create a prediction function that takes a
    batch of images and outputs the predictions. You just need to make sure the function
    properly makes use of the numpy-PIL conversion function in addition to the encoding
    and model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ–‡æœ¬è§£é‡Šå™¨ä¸€æ ·ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªé¢„æµ‹å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—ä¸€æ‰¹å›¾åƒå¹¶è¾“å‡ºé¢„æµ‹ã€‚æ‚¨åªéœ€ç¡®ä¿è¯¥å‡½æ•°æ­£ç¡®åœ°åˆ©ç”¨äº† numpy-PIL è½¬æ¢å‡½æ•°ä»¥åŠç¼–ç å’Œæ¨¡å‹ã€‚
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: From here, we can use the explainer to examine which parts of the image correspond
    to the top predicted class ([FigureÂ 3-8](#lime-image-pos)).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è§£é‡Šå™¨æ¥æ£€æŸ¥å›¾åƒä¸­ä¸é¡¶éƒ¨é¢„æµ‹ç±»åˆ«å¯¹åº”çš„éƒ¨åˆ†ï¼ˆ[å›¾Â 3-8](#lime-image-pos)ï¼‰ã€‚
- en: '[PRE28]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![ptml 0308](assets/ptml_0308.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0308](assets/ptml_0308.png)'
- en: Figure 3-8\. Positive contributions highlighted by LIME
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-8\. LIME é«˜äº®çš„æ­£é¢è´¡çŒ®
- en: Or if weâ€™re focusing on just the top class prediction, we can examine the explanation
    further to figure out which parts were in favor of the decision, and which parts
    counted against it ([FigureÂ 3-9](#lime-image-neg)).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå¦‚æœæˆ‘ä»¬åªå…³æ³¨é¡¶éƒ¨ç±»åˆ«çš„é¢„æµ‹ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æ£€æŸ¥è§£é‡Šï¼Œä»¥æ‰¾å‡ºå“ªäº›éƒ¨åˆ†æ”¯æŒå†³å®šï¼Œå“ªäº›éƒ¨åˆ†åå¯¹å®ƒï¼ˆ[å›¾Â 3-9](#lime-image-neg)ï¼‰ã€‚
- en: '[PRE29]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![ptml 0309](assets/ptml_0309.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0309](assets/ptml_0309.png)'
- en: Figure 3-9\. Negative contributions highlighted by LIME
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-9\. LIME é«˜äº®çš„è´Ÿé¢è´¡çŒ®
- en: These approaches arenâ€™t the only way to do LIME on transformer models. An alternative
    to this approach to using LIME is described in the [Captum package](https://oreil.ly/p78Oh).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ–¹æ³•ä¸æ˜¯åœ¨å˜æ¢å™¨æ¨¡å‹ä¸Šä½¿ç”¨ LIME çš„å”¯ä¸€æ–¹å¼ã€‚åœ¨ Captum è½¯ä»¶åŒ…ä¸­æè¿°äº†ä½¿ç”¨ LIME çš„å¦ä¸€ç§æ–¹æ³•çš„æ›¿ä»£æ–¹æ³•ã€‚
- en: Shapley and SHAP
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¤æ™®åˆ©ä¸ SHAP
- en: '[*SHapley Additive exPlanations* (SHAP)](https://oreil.ly/zbGHN) is an attribution
    method that fairly assigns the prediction to individual features. This is based
    on an idea called *Shapley value* from the domain of cooperative game theory.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[*SHapley Additive exPlanations* (SHAP)](https://oreil.ly/zbGHN) æ˜¯ä¸€ç§å½’å› æ–¹æ³•ï¼Œå…¬å¹³åœ°å°†é¢„æµ‹åˆ†é…ç»™å„ä¸ªç‰¹å¾ã€‚è¿™åŸºäºåˆä½œåšå¼ˆè®ºé¢†åŸŸçš„
    *å¤æ™®åˆ©å€¼* çš„æ¦‚å¿µã€‚'
- en: Suppose we have a group of four people who cooperate in a game together (also
    known as a â€œcoalitionâ€). The game could be a machine learning competition. After
    the game, they get a certain payout for their result, such as getting $10,000
    for winning first place. The central question is how that prize should be fairly
    distributed. In a machine learning competition, each coalition member likely contributed
    different parts, so splitting it perfectly evenly among all the members wouldnâ€™t
    make sense.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸€ç»„å››ä¸ªäººä¸€èµ·åˆä½œè¿›è¡Œæ¸¸æˆï¼ˆä¹Ÿç§°ä¸ºâ€œè”ç›Ÿâ€ï¼‰ã€‚è¿™ä¸ªæ¸¸æˆå¯èƒ½æ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ ç«èµ›ã€‚æ¯”èµ›ç»“æŸåï¼Œä»–ä»¬æ ¹æ®ç»“æœå¾—åˆ°ä¸€å®šçš„å¥–åŠ±ï¼Œæ¯”å¦‚èµ¢å¾—ç¬¬ä¸€åå¯è·å¾— 10,000
    ç¾å…ƒã€‚ä¸­å¿ƒé—®é¢˜æ˜¯å¦‚ä½•å…¬å¹³åˆ†é…å¥–é‡‘ã€‚åœ¨æœºå™¨å­¦ä¹ ç«èµ›ä¸­ï¼Œæ¯ä¸ªè”ç›Ÿæˆå‘˜å¯èƒ½è´¡çŒ®äº†ä¸åŒçš„éƒ¨åˆ†ï¼Œå› æ­¤å®Œå…¨å¹³å‡åˆ†é…ç»™æ‰€æœ‰æˆå‘˜æ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚
- en: Lloyd Shapley came up with Shapley values in 1951. These tell us the average
    contribution of the player to the payout. The explainable AI value SHAP makes
    use of Shapley values to determine which features of an input instance contributed
    to a model decision (instead of players in a game).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ³åŸƒå¾·Â·å¤æ™®åˆ©äº 1951 å¹´æå‡ºäº†å¤æ™®åˆ©å€¼ã€‚è¿™äº›å€¼å‘Šè¯‰æˆ‘ä»¬ç©å®¶å¯¹å¥–åŠ±çš„å¹³å‡è´¡çŒ®ã€‚å¯è§£é‡Šçš„ AI å€¼ SHAP åˆ©ç”¨å¤æ™®åˆ©å€¼æ¥ç¡®å®šè¾“å…¥å®ä¾‹çš„å“ªäº›ç‰¹å¾å¯¼è‡´äº†æ¨¡å‹å†³ç­–ï¼ˆè€Œä¸æ˜¯æ¸¸æˆä¸­çš„ç©å®¶ï¼‰ã€‚
- en: The main intuition behind Shapley values is that they measure how the coalition
    would have played with or without a certain player. Suppose, in your machine learning
    competition, we remove player Alice, who happens to be a domain expert. Rather
    than coming in first, the team places second and gets a payout of only $3,000.
    You could stop here and assume Alice contributed to 70% of the payout, but itâ€™s
    not that simple.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley å€¼çš„ä¸»è¦ç›´è§‰æ˜¯ï¼Œå®ƒä»¬è¡¡é‡äº†å¦‚æœæ²¡æœ‰æŸä¸ªç©å®¶ï¼Œè”ç›Ÿä¼šå¦‚ä½•å‘æŒ¥ä½œç”¨ã€‚å‡è®¾åœ¨æ‚¨çš„æœºå™¨å­¦ä¹ ç«èµ›ä¸­ï¼Œæˆ‘ä»¬ç§»é™¤äº†åŸŸä¸“å®¶ Alice ç©å®¶ã€‚å›¢é˜ŸåŸæœ¬å¯ä»¥æ’åç¬¬ä¸€ï¼Œä½†æœ€ç»ˆæ’åç¬¬äºŒï¼Œä»…è·å¾—
    $3,000 çš„å¥–é‡‘ã€‚æ‚¨å¯èƒ½ä¼šåœ¨è¿™é‡Œåœä¸‹æ¥ï¼Œå‡è®¾ Alice è´¡çŒ®äº†å¥–é‡‘çš„ 70%ï¼Œä½†äº‹å®å¹¶éå¦‚æ­¤ç®€å•ã€‚
- en: Players interact with each other, so we also need to take into account how the
    players perform when working together. Suppose the team also has Bob, who is a
    machine learning expert. Alice only achieves great results when working with Bob,
    so the contribution should be split between them. But weâ€™re not finished, because
    we also need to consider subsets, such as a three-person subset that excludes
    Bob and only contains Alice and her teammates Carol and Dylan. The Shapley value
    is used for calculating the contribution of each player for each possible subset
    of the coalition and averaging over all these contributions (known as the playerâ€™s
    *marginal value*).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ç©å®¶å½¼æ­¤ä¹‹é—´å­˜åœ¨äº’åŠ¨ï¼Œå› æ­¤æˆ‘ä»¬è¿˜éœ€è¦è€ƒè™‘ç©å®¶åœ¨å…±åŒå·¥ä½œæ—¶çš„è¡¨ç°ã€‚å‡è®¾å›¢é˜Ÿè¿˜æœ‰æœºå™¨å­¦ä¹ ä¸“å®¶ Bobã€‚Alice åªæœ‰åœ¨ä¸ Bob åˆä½œæ—¶æ‰èƒ½å–å¾—å‡ºè‰²çš„ç»“æœï¼Œå› æ­¤è´¡çŒ®åº”è¯¥åœ¨ä»–ä»¬ä¹‹é—´åˆ†é…ã€‚ä½†æˆ‘ä»¬è¿˜æ²¡æœ‰ç»“æŸï¼Œå› ä¸ºæˆ‘ä»¬è¿˜éœ€è¦è€ƒè™‘å­é›†ï¼Œä¾‹å¦‚ä¸€ä¸ªä¸‰äººå­é›†ï¼Œæ’é™¤
    Bobï¼Œä»…åŒ…å« Alice å’Œå¥¹çš„é˜Ÿå‹ Carol å’Œ Dylanã€‚Shapley å€¼ç”¨äºè®¡ç®—è”ç›Ÿçš„æ¯ä¸ªå¯èƒ½å­é›†çš„æ¯ä¸ªç©å®¶çš„è´¡çŒ®ï¼Œå¹¶å¯¹æ‰€æœ‰è¿™äº›è´¡çŒ®è¿›è¡Œå¹³å‡ï¼ˆè¢«ç§°ä¸ºç©å®¶çš„
    *è¾¹é™…ä»·å€¼*ï¼‰ã€‚
- en: Letâ€™s go back to the machine learning context. As mentioned earlier, we can
    frame the features in a data instance as players and the model output prediction
    as the payout. Shapley values tell us how this value is distributed among all
    the inputs. SHAP uses this method to create local explanations for individual
    predictions, but it can also be used for global interpretations by averaging values
    across an entire dataset passed into a model.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›åˆ°æœºå™¨å­¦ä¹ çš„èƒŒæ™¯ã€‚æ­£å¦‚å‰é¢æåˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ•°æ®å®ä¾‹ä¸­çš„ç‰¹å¾è§†ä¸ºç©å®¶ï¼Œæ¨¡å‹è¾“å‡ºé¢„æµ‹ä¸ºå¥–é‡‘ã€‚Shapley å€¼å‘Šè¯‰æˆ‘ä»¬è¿™ä¸ªå€¼å¦‚ä½•åœ¨æ‰€æœ‰è¾“å…¥ä¹‹é—´åˆ†é…ã€‚SHAP
    ä½¿ç”¨è¿™ç§æ–¹æ³•ä¸ºä¸ªåˆ«é¢„æµ‹åˆ›å»ºå±€éƒ¨è§£é‡Šï¼Œä½†ä¹Ÿå¯ä»¥é€šè¿‡å¯¹ä¼ å…¥æ¨¡å‹çš„æ•´ä¸ªæ•°æ®é›†çš„å¹³å‡å€¼è¿›è¡Œå…¨å±€è§£é‡Šã€‚
- en: "How do we actually calculate Shapley values? Consider the equation for getting\
    \ the Shapley value for a feature value <math alttext=\"i\"><mi>i</mi></math>\
    \ for your black-box model <math alttext=\"f\"><mi>f</mi></math> and your input\
    \ data instance <math alttext=\"x\"><mi>x</mi></math> (this would be a single\
    \ row in a tabular dataset). You iterate over all possible subsets of features\
    \ ( <math alttext=\"z prime\"><mrow><mi>z</mi> <mi>Ã¢</mi> <mi>\x80</mi> <mi>\x99\
    </mi></mrow></math> ) to make sure we account for all interactions between feature\
    \ values. Our sampling space is denoted with a â€² because for larger instances\
    \ like images, we donâ€™t treat each pixel as a feature; instead we find a way to\
    \ summarize the image into larger features. You get the black box model output\
    \ both with the feature weâ€™re interested in ( <math alttext=\"f Subscript x Baseline\
    \ left-parenthesis z prime right-parenthesis\"><mrow><msub><mi>f</mi> <mi>x</mi></msub>\
    \ <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>\
    \ ) and without it ( <math alttext=\"f Subscript x Baseline left-parenthesis z\
    \ prime minus i right-parenthesis\"><mrow><msub><mi>f</mi> <mi>x</mi></msub> <mrow><mo>(</mo>\
    \ <mi>z</mi> <mi>Ã¢</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>âˆ–</mo> <mi>i</mi> <mo>)</mo></mrow></mrow></math>\
    \ ). Seeing both tells us how the feature contributed to the model output in this\
    \ subset."
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: "æˆ‘ä»¬å¦‚ä½•å®é™…è®¡ç®— Shapley å€¼å‘¢ï¼Ÿè€ƒè™‘ä»¥ä¸‹æ–¹ç¨‹ï¼Œç”¨äºè·å–æ‚¨çš„é»‘ç›’æ¨¡å‹ <math alttext=\"f\"><mi>f</mi></math>\
    \ å’Œè¾“å…¥æ•°æ®å®ä¾‹ <math alttext=\"x\"><mi>x</mi></math> çš„ç‰¹å¾å€¼ <math alttext=\"i\"><mi>i</mi></math>\
    \ çš„ Shapley å€¼ï¼ˆè¿™å°†æ˜¯è¡¨æ ¼æ•°æ®é›†ä¸­çš„å•è¡Œï¼‰ã€‚æ‚¨éœ€è¦è¿­ä»£æ‰€æœ‰å¯èƒ½çš„ç‰¹å¾å­é›†ï¼ˆ<math alttext=\"z prime\"><mrow><mi>z</mi>\
    \ <mi>Ã¢</mi> <mi>\x80</mi> <mi>\x99</mi></mrow></math>ï¼‰ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬è€ƒè™‘äº†ç‰¹å¾å€¼ä¹‹é—´çš„æ‰€æœ‰äº¤äº’ã€‚æˆ‘ä»¬å°†é‡‡æ ·ç©ºé—´æ ‡è®°ä¸º\
    \ â€²ï¼Œå› ä¸ºå¯¹äºåƒå›¾åƒè¿™æ ·çš„æ›´å¤§å®ä¾‹ï¼Œæˆ‘ä»¬ä¸ä¼šå°†æ¯ä¸ªåƒç´ è§†ä¸ºç‰¹å¾ï¼›ç›¸åï¼Œæˆ‘ä»¬æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥æ€»ç»“å›¾åƒä¸ºæ›´å¤§çš„ç‰¹å¾ã€‚æ‚¨ä¼šè·å¾—åŒ…æ‹¬æˆ‘ä»¬æ„Ÿå…´è¶£çš„ç‰¹å¾çš„é»‘ç›’æ¨¡å‹è¾“å‡ºï¼ˆ<math\
    \ alttext=\"f Subscript x Baseline left-parenthesis z prime right-parenthesis\"\
    ><mrow><msub><mi>f</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup>\
    \ <mo>)</mo></mrow></mrow></math>ï¼‰ï¼Œä»¥åŠæ²¡æœ‰è¿™ä¸ªç‰¹å¾çš„è¾“å‡ºï¼ˆ<math alttext=\"f Subscript x Baseline\
    \ left-parenthesis z prime minus i right-parenthesis\"><mrow><msub><mi>f</mi>\
    \ <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mi>Ã¢</mi> <mi>\x80</mi> <mi>\x99\
    </mi> <mo>âˆ–</mo> <mi>i</mi> <mo>)</mo></mrow></mrow></math>ï¼‰ã€‚è§‚å¯Ÿè¿™ä¸¤è€…å‘Šè¯‰æˆ‘ä»¬ç‰¹å¾åœ¨è¯¥å­é›†ä¸­å¦‚ä½•å½±å“æ¨¡å‹è¾“å‡ºã€‚"
- en: You then do this for each possible permutation of subsets of features, each
    of which is additionally weighted by how many players are in the coalition, or
    how many features weâ€™re looking at in total for the data instance <math alttext="upper
    M"><mi>M</mi></math> . This allows us to tell if a feature adds a large change
    to the modelâ€™s decision even if weâ€™re already taking into account a lot of other
    features. This also lets us more directly observe the effects of features in isolation
    in smaller coalitions.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä½ è¦å¯¹æ¯ä¸ªç‰¹å¾å­é›†çš„å¯èƒ½æ’åˆ—è¿›è¡Œè¿™æ ·çš„æ“ä½œï¼Œè€Œæ¯ä¸ªæ’åˆ—çš„æƒé‡åˆå–å†³äºè”åˆä½“ä¸­çš„ç©å®¶æ•°é‡ï¼Œæˆ–è€…æˆ‘ä»¬é’ˆå¯¹æ•°æ®å®ä¾‹<math alttext="upper
    M"><mi>M</mi></math>æ€»å…±æŸ¥çœ‹çš„ç‰¹å¾æ•°é‡ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ¤æ–­ä¸€ä¸ªç‰¹å¾æ˜¯å¦å¯¹æ¨¡å‹çš„å†³ç­–äº§ç”Ÿäº†å¾ˆå¤§çš„æ”¹å˜ï¼Œå³ä½¿æˆ‘ä»¬å·²ç»è€ƒè™‘äº†å¾ˆå¤šå…¶ä»–ç‰¹å¾ã€‚è¿™ä¹Ÿä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´ç›´æ¥åœ°è§‚å¯Ÿåœ¨è¾ƒå°è”åˆä½“ä¸­å­¤ç«‹ç‰¹å¾çš„å½±å“ã€‚
- en: "<math alttext=\"phi Subscript i Baseline left-parenthesis f comma x right-parenthesis\
    \ equals sigma-summation Underscript z prime subset-of-or-equal-to x Superscript\
    \ prime Baseline Endscripts StartFraction StartAbsoluteValue z Superscript prime\
    \ Baseline EndAbsoluteValue factorial left-parenthesis upper M minus StartAbsoluteValue\
    \ z Superscript prime Baseline EndAbsoluteValue minus 1 right-parenthesis factorial\
    \ Over upper M factorial EndFraction left-parenthesis right-parenthesis f Subscript\
    \ x Baseline left-parenthesis z Superscript prime Baseline right-parenthesis minus\
    \ f Subscript x Baseline left-parenthesis z prime minus i right-parenthesis right-parenthesis\"\
    \ display=\"block\"><mrow><msub><mi>Ï†</mi> <mi>i</mi></msub> <mrow><mo>(</mo>\
    \ <mi>f</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>âˆ‘</mo>\
    \ <mrow><mi>z</mi><mi>Ã¢</mi><mi>\x80</mi><mi>\x99</mi><mo>âŠ†</mo><msup><mi>x</mi>\
    \ <mo>'</mo></msup></mrow></munder> <mfrac><mrow><mfenced close=\"|\" open=\"\
    |\" separators=\"\"><mi>z</mi><mi>Ã¢</mi><mi>\x80</mi><mi>\x99</mi></mfenced><mo>!</mo><mrow><mo>(</mo><mi>M</mi><mo>-</mo><mfenced\
    \ close=\"|\" open=\"|\" separators=\"\"><mi>z</mi><mi>Ã¢</mi><mi>\x80</mi><mi>\x99\
    </mi></mfenced><mo>-</mo><mn>1</mn><mo>)</mo></mrow><mo>!</mo></mrow> <mrow><mi>M</mi><mo>!</mo></mrow></mfrac>\
    \ <mfenced close=\")\" open=\"(\" separators=\"\"><mrow><mo>)</mo></mrow> <msub><mi>f</mi>\
    \ <mi>x</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow>\
    \ <mo>-</mo> <msub><mi>f</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mi>Ã¢</mi>\
    \ <mi>\x80</mi> <mi>\x99</mi> <mo>âˆ–</mo> <mi>i</mi> <mo>)</mo></mrow></mfenced></mrow></math>"
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: "<math alttext=\"phi Subscript i Baseline left-parenthesis f comma x right-parenthesis\
    \ equals sigma-summation Underscript z prime subset-of-or-equal-to x Superscript\
    \ prime Baseline Endscripts StartFraction StartAbsoluteValue z Superscript prime\
    \ Baseline EndAbsoluteValue factorial left-parenthesis upper M minus StartAbsoluteValue\
    \ z Superscript prime Baseline EndAbsoluteValue minus 1 right-parenthesis factorial\
    \ Over upper M factorial EndFraction left-parenthesis right-parenthesis f Subscript\
    \ x Baseline left-parenthesis z Superscript prime Baseline right-parenthesis minus\
    \ f Subscript x Baseline left-parenthesis z prime minus i right-parenthesis right-parenthesis\"\
    \ display=\"block\"><mrow><msub><mi>Ï†</mi> <mi>i</mi></msub> <mrow><mo>(</mo>\
    \ <mi>f</mi> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <munder><mo>âˆ‘</mo>\
    \ <mrow><mi>z</mi><mi>Ã¢</mi><mi>\x80</mi><mi>\x99</mi><mo>âŠ†</mo><msup><mi>x</mi>\
    \ <mo>'</mo></msup></mrow></munder> <mfrac><mrow><mfenced close=\"|\" open=\"\
    |\" separators=\"\"><mi>z</mi><mi>Ã¢</mi><mi>\x80</mi><mi>\x99</mi></mfenced><mo>!</mo><mrow><mo>(</mo><mi>M</mi><mo>-</mo><mfenced\
    \ close=\"|\" open=\"|\" separators=\"\"><mi>z</mi><mi>Ã¢</mi><mi>\x80</mi><mi>\x99\
    </mi></mfenced><mo>-</mo><mn>1</mn><mo>)</mo></mrow><mo>!</mo></mrow> <mrow><mi>M</mi><mo>!</mo></mrow></mfrac>\
    \ <mfenced close=\")\" open=\"(\" separators=\"\"><mrow><mo>)</mo></mrow> <msub><mi>f</mi>\
    \ <mi>x</mi></msub> <mrow><mo>(</mo> <msup><mi>z</mi> <mo>'</mo></msup> <mo>)</mo></mrow>\
    \ <mo>-</mo> <msub><mi>f</mi> <mi>x</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mi>Ã¢</mi>\
    \ <mi>\x80</mi> <mi>\x99</mi> <mo>âˆ–</mo> <mi>i</mi> <mo>)</mo></mrow></mfenced></mrow></math>"
- en: 'This still leaves one question: how do we remove features from a model input
    if our model typically takes in a fixed input size? This is solved in SHAP by
    replacing the removed feature value with a random replacement from somewhere else
    in the training data. If we do this for all subsets, the relevance of the feature
    is basically sampled out. You completely shuffle the features until theyâ€™re random,
    and a completely random feature offers no predictive power.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç„¶æœ‰ä¸€ä¸ªé—®é¢˜ï¼šå¦‚æœæˆ‘ä»¬çš„æ¨¡å‹é€šå¸¸æ¥å—å›ºå®šå¤§å°çš„è¾“å…¥ï¼Œæˆ‘ä»¬å¦‚ä½•ä»æ¨¡å‹è¾“å…¥ä¸­ç§»é™¤ç‰¹å¾ï¼Ÿåœ¨SHAPä¸­ï¼Œé€šè¿‡ç”¨è®­ç»ƒæ•°æ®ä¸­å…¶ä»–ä½ç½®çš„éšæœºæ›¿æ¢å€¼æ›¿æ¢å·²ç§»é™¤çš„ç‰¹å¾å€¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¦‚æœæˆ‘ä»¬å¯¹æ‰€æœ‰å­é›†éƒ½è¿™æ ·åšï¼Œé‚£ä¹ˆç‰¹å¾çš„ç›¸å…³æ€§åŸºæœ¬ä¸Šä¼šè¢«é‡‡æ ·æ‰ã€‚ä½ å®Œå…¨æ‰“ä¹±ç‰¹å¾ç›´åˆ°å®ƒä»¬æˆä¸ºéšæœºçš„ï¼Œè€Œå®Œå…¨éšæœºçš„ç‰¹å¾åˆ™ä¸å…·æœ‰é¢„æµ‹èƒ½åŠ›ã€‚
- en: 'However thereâ€™s still one barrier to using SHAP: computational complexity.
    Calculating all those subsets is expensive. For an instance with <math alttext="n"><mi>n</mi></math>
    features, we have <math alttext="2 Superscript n"><msup><mn>2</mn> <mi>n</mi></msup></math>
    subsets. For 10 features, we have <math alttext="2 Superscript 10 Baseline equals
    1 comma 024"><mrow><msup><mn>2</mn> <mn>10</mn></msup> <mo>=</mo> <mn>1</mn> <mo>,</mo>
    <mn>024</mn></mrow></math> subsets, and for 20 features, we have <math alttext="2
    Superscript 20 Baseline equals 1 comma 048 comma 576"><mrow><msup><mn>2</mn> <mn>20</mn></msup>
    <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>048</mn> <mo>,</mo> <mn>576</mn></mrow></math>
    subsets, and so on. One possible workaround is to calculate SHAP approximately
    instead of exactly. Kernel SHAP samples feature subsets and fits a linear regression
    model based on the samples. The variables are simply whether a feature is absent
    or present, with the output value being the prediction. The coefficients of this
    linear model can be interpreted as approximate Shapley values. This is similar
    to LIME, except we donâ€™t care how close instances are to each other, only how
    much information they contain.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ä½¿ç”¨SHAPä»ç„¶å­˜åœ¨ä¸€ä¸ªéšœç¢ï¼šè®¡ç®—å¤æ‚åº¦å¾ˆé«˜ã€‚è®¡ç®—æ‰€æœ‰è¿™äº›å­é›†æ˜¯æ˜‚è´µçš„ã€‚å¯¹äºå…·æœ‰<math alttext="n"><mi>n</mi></math>ä¸ªç‰¹å¾çš„å®ä¾‹ï¼Œæˆ‘ä»¬æœ‰<math
    alttext="2 Superscript n"><msup><mn>2</mn> <mi>n</mi></msup></math>ä¸ªå­é›†ã€‚å¯¹äº10ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬æœ‰<math
    alttext="2 Superscript 10 Baseline equals 1 comma 024"><mrow><msup><mn>2</mn>
    <mn>10</mn></msup> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>024</mn></mrow></math>ä¸ªå­é›†ï¼Œå¯¹äº20ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬æœ‰<math
    alttext="2 Superscript 20 Baseline equals 1 comma 048 comma 576"><mrow><msup><mn>2</mn>
    <mn>20</mn></msup> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>048</mn> <mo>,</mo> <mn>576</mn></mrow></math>ä¸ªå­é›†ï¼Œä»¥æ­¤ç±»æ¨ã€‚ä¸€ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ¡ˆæ˜¯è¿‘ä¼¼è®¡ç®—SHAPï¼Œè€Œä¸æ˜¯ç²¾ç¡®è®¡ç®—ã€‚Kernel
    SHAPå¯¹ç‰¹å¾å­é›†è¿›è¡Œé‡‡æ ·ï¼Œå¹¶æ ¹æ®è¿™äº›æ ·æœ¬æ‹Ÿåˆçº¿æ€§å›å½’æ¨¡å‹ã€‚å˜é‡åªæ˜¯ä¸€ä¸ªç‰¹å¾æ˜¯å¦å­˜åœ¨æˆ–ç¼ºå¤±ï¼Œè¾“å‡ºå€¼æ˜¯é¢„æµ‹ç»“æœã€‚è¿™ä¸ªçº¿æ€§æ¨¡å‹çš„ç³»æ•°å¯ä»¥è§£é‡Šä¸ºè¿‘ä¼¼çš„Shapleyå€¼ã€‚è¿™ç±»ä¼¼äºLIMEï¼Œä½†æˆ‘ä»¬ä¸å…³å¿ƒå®ä¾‹ä¹‹é—´çš„æ¥è¿‘ç¨‹åº¦ï¼Œåªå…³å¿ƒå®ƒä»¬åŒ…å«çš„ä¿¡æ¯é‡ã€‚
- en: There are other approximations for SHAP, such as Tree SHAP and Deep SHAP for
    tree-based models and deep neural networks respectively. These techniques are
    not really model-agnostic anymore, but on the plus side, they can at least take
    advantage of the model internals to speed up calculation.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: SHAPçš„å…¶ä»–è¿‘ä¼¼æ–¹æ³•åŒ…æ‹¬Tree SHAPå’ŒDeep SHAPï¼Œåˆ†åˆ«ç”¨äºåŸºäºæ ‘çš„æ¨¡å‹å’Œæ·±åº¦ç¥ç»ç½‘ç»œã€‚è¿™äº›æŠ€æœ¯ä¸å†æ˜¯çœŸæ­£çš„æ¨¡å‹æ— å…³çš„ï¼Œä½†å¥½å¤„åœ¨äºå®ƒä»¬è‡³å°‘å¯ä»¥åˆ©ç”¨æ¨¡å‹å†…éƒ¨åŠ é€Ÿè®¡ç®—ã€‚
- en: 'Deep dive example: SHAP on Vision Transformer models'
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ·±åº¦åˆ†æç¤ºä¾‹ï¼šSHAP å¯¹è§†è§‰Transformeræ¨¡å‹çš„åº”ç”¨
- en: SHAP can be used to explain predictions on many different data types, from tabular
    to image to language data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: SHAPå¯ç”¨äºè§£é‡Šå¤šç§ä¸åŒç±»å‹æ•°æ®çš„é¢„æµ‹ï¼Œä»è¡¨æ ¼æ•°æ®åˆ°å›¾åƒå’Œè¯­è¨€æ•°æ®ã€‚
- en: Note
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: You can find all the code associated with this tutorial in the notebook [*Chapter_3_SHAP_for_Transformers.ipynb*](https://oreil.ly/D8D5E).
    Interactive versions of Figures [3-10](#shap-example-0) through [3-15](#zeroshot-shap-1)
    are available in the *Chapter_3_SHAP_for_Transformers.ipynb* notebook.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨ç¬”è®°æœ¬[*Chapter_3_SHAP_for_Transformers.ipynb*](https://oreil.ly/D8D5E)ä¸­æ‰¾åˆ°ä¸æœ¬æ•™ç¨‹ç›¸å…³çš„æ‰€æœ‰ä»£ç ã€‚å›¾[3-10](#shap-example-0)åˆ°å›¾[3-15](#zeroshot-shap-1)çš„äº¤äº’ç‰ˆæœ¬ä¹Ÿå¯åœ¨*Chapter_3_SHAP_for_Transformers.ipynb*ç¬”è®°æœ¬ä¸­æ‰¾åˆ°ã€‚
- en: Consider the example of using SHAP for a large language model.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä½¿ç”¨SHAPåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹çš„ç¤ºä¾‹ã€‚
- en: '[PRE30]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: For your classification task, weâ€™ll use the HuggingFace Transformers library.
    Weâ€™ll create a standard `TextClassificationPipeline` using your model (DistilBERT)
    and the associated tokenizer.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‚¨çš„åˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨HuggingFace Transformersåº“ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼ˆDistilBERTï¼‰å’Œç›¸å…³çš„tokenizeråˆ›å»ºä¸€ä¸ªæ ‡å‡†çš„`TextClassificationPipeline`ã€‚
- en: This model is a fine-tuned version of `distilbert-base-uncased` on the sst-2-english
    dataset. Switching out this model name, even for other distilbert models fine-tuned
    on sst-2-english, can result in changes to the visualization and output labels.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹æ˜¯`distilbert-base-uncased`åœ¨sst-2-englishæ•°æ®é›†ä¸Šå¾®è°ƒçš„ç‰ˆæœ¬ã€‚å³ä½¿æ˜¯å¯¹å…¶ä»–åœ¨sst-2-englishä¸Šå¾®è°ƒçš„distilbertæ¨¡å‹æ¥è¯´ï¼Œæ›¿æ¢è¿™ä¸ªæ¨¡å‹åç§°ä¹Ÿå¯èƒ½ä¼šå¯¼è‡´å¯è§†åŒ–å’Œè¾“å‡ºæ ‡ç­¾çš„å˜åŒ–ã€‚
- en: '[PRE31]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With your DistilBERT model and its tokenizers imported, we can see how SHAP
    processes text classifications. Here is an example working with clearly positive
    text (see [FigureÂ 3-10](#shap-example-0)).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨çš„DistilBERTæ¨¡å‹åŠå…¶å¯¼å…¥çš„tokenizerï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°SHAPå¦‚ä½•å¤„ç†æ–‡æœ¬åˆ†ç±»ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¤„ç†æ˜¾ç„¶æ˜¯ç§¯ææ–‡æœ¬çš„ç¤ºä¾‹ï¼ˆå‚è§[å›¾Â 3-10](#shap-example-0)ï¼‰ã€‚
- en: '[PRE32]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![ptml 0310](assets/ptml_0310.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0310](assets/ptml_0310.png)'
- en: Figure 3-10\. Using SHAP on an obviously positive example sentence
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 3-10\. åœ¨ä¸€ä¸ªæ˜æ˜¾ç§¯æçš„ç¤ºä¾‹å¥å­ä¸Šä½¿ç”¨SHAPã€‚
- en: And here is an example working with neutral text with a negative bent ([FigureÂ 3-11](#shap-example-1)).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”è¿™æ˜¯ä¸€ä¸ªå¤„ç†æœ‰æ¶ˆæå€¾å‘çš„ä¸­æ€§æ–‡æœ¬çš„ç¤ºä¾‹ï¼ˆå‚è§[å›¾Â 3-11](#shap-example-1)ï¼‰ã€‚
- en: '[PRE34]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![ptml 0311](assets/ptml_0311.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0311](assets/ptml_0311.png)'
- en: Figure 3-11\. Using SHAP on an intentionally neutral review (thatâ€™s perceived
    as negative)
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 3-11\. åœ¨ä¸€ä¸ªæœ‰æ„ä¹‰ä¸Šæ˜¯ä¸­ç«‹è¯„ä»·ï¼ˆè¢«è®¤ä¸ºæ˜¯æ¶ˆæçš„ï¼‰ä¸Šä½¿ç”¨SHAPã€‚
- en: In this example, we again analyze neutral text but this time with a positive
    slant (see [FigureÂ 3-12](#shap-example-2)).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å†æ¬¡åˆ†æä¸­æ€§æ–‡æœ¬ï¼Œä½†è¿™æ¬¡æ˜¯å¸¦æœ‰ç§¯æå€¾å‘çš„ï¼ˆå‚è§[å›¾Â 3-12](#shap-example-2)ï¼‰ã€‚
- en: '[PRE36]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![ptml 0312](assets/ptml_0312.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0312](assets/ptml_0312.png)'
- en: Figure 3-12\. Using SHAP on another intentionally neutral review (thatâ€™s perceived
    as positive)
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 3-12\. åœ¨å¦ä¸€ä¸ªæœ‰æ„ä¹‰ä¸Šæ˜¯ä¸­ç«‹è¯„ä»·ï¼ˆè¢«è®¤ä¸ºæ˜¯ç§¯æçš„ï¼‰çš„æ–‡æœ¬ä¸Šä½¿ç”¨SHAPã€‚
- en: Finally, here we use SHAP to analyze a longer and intentionally ambiguous text
    ([FigureÂ 3-13](#shap-example-3)).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨SHAPæ¥åˆ†æä¸€ä¸ªæ›´é•¿ä¸”æ•…æ„æ¨¡ç³Šçš„æ–‡æœ¬ï¼ˆå‚è§[å›¾Â 3-13](#shap-example-3)ï¼‰ã€‚
- en: '[PRE38]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![ptml 0313](assets/ptml_0313.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0313](assets/ptml_0313.png)'
- en: Figure 3-13\. Using SHAP on a much longer review
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 3-13\. åœ¨ä¸€ä¸ªæ›´é•¿çš„è¯„è®ºä¸Šä½¿ç”¨SHAPã€‚
- en: You can even extend SHAP to interpreting zero-shot classification tasks ([FigureÂ 3-14](#zeroshot-shap-0)).^([14](ch03.html#idm45621840086784))
    The main difference between the previous approach (aside from the change in imports)
    is that weâ€™ll create a custom class `MyZeroShotClassificationâ€‹Pipeline` from the
    imported `ZeroShotClassificationPipeline` class.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ç”šè‡³å¯ä»¥å°†SHAPæ‰©å±•åˆ°è§£é‡Šé›¶æ ·æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆå‚è§[å›¾Â 3-14](#zeroshot-shap-0)ï¼‰ã€‚^([14](ch03.html#idm45621840086784))
    ä¸ä¹‹å‰çš„æ–¹æ³•ï¼ˆé™¤äº†å¯¼å…¥æ›´æ”¹å¤–ï¼‰çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œæˆ‘ä»¬å°†ä»å¯¼å…¥çš„`ZeroShotClassificationPipeline`ç±»åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰ç±»`MyZeroShotClassificationâ€‹Pipeline`ã€‚
- en: '[PRE40]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![ptml 0314](assets/ptml_0314.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0314](assets/ptml_0314.png)'
- en: Figure 3-14\. Using SHAP to interpret zero-shot text classification
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 3-14\. ä½¿ç”¨SHAPè§£é‡Šé›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»ã€‚
- en: This example shows the application of SHAP to a completely neutral text (see
    [FigureÂ 3-15](#zeroshot-shap-1)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç¤ºä¾‹å±•ç¤ºäº†SHAPåº”ç”¨äºå®Œå…¨ä¸­æ€§æ–‡æœ¬çš„æƒ…å†µï¼ˆå‚è§[å›¾Â 3-15](#zeroshot-shap-1)ï¼‰ã€‚
- en: '[PRE42]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now, like LIME, SHAP can be extended to image data.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼ŒåƒLIMEä¸€æ ·ï¼ŒSHAPå¯ä»¥æ‰©å±•åˆ°å›¾åƒæ•°æ®ã€‚
- en: There are numerous examples of [SHAP on CNN-based image classifiers](https://oreil.ly/eV5ib).
    You could use the same approach in a vision example, but thereâ€™s a reason so many
    of the SHAP examples out there are demonstrated on simple datasets like MNIST.
    SHAP operates on all the features of the input data. For text, thatâ€™s every token.
    For images, thatâ€™s every pixel. Even if youâ€™re just running SHAP on a simple image
    of a handwritten digit, itâ€™s computationally expensive.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è®¸å¤šå…³äº[åŸºäºCNNçš„å›¾åƒåˆ†ç±»å™¨ä¸Šçš„SHAPç¤ºä¾‹](https://oreil.ly/eV5ib)ã€‚æ‚¨å¯ä»¥åœ¨è§†è§‰ç¤ºä¾‹ä¸­ä½¿ç”¨ç›¸åŒçš„æ–¹æ³•ï¼Œä½†SHAPç¤ºä¾‹å¤§å¤šæ•°éƒ½æ˜¯åœ¨ç®€å•æ•°æ®é›†ï¼ˆå¦‚MNISTï¼‰ä¸Šæ¼”ç¤ºçš„åŸå› æ˜¯æœ‰é“ç†çš„ã€‚SHAPæ“ä½œè¾“å…¥æ•°æ®çš„æ‰€æœ‰ç‰¹å¾ã€‚å¯¹äºæ–‡æœ¬æ¥è¯´ï¼Œè¿™æ˜¯æ¯ä¸ªtokenã€‚å¯¹äºå›¾åƒæ¥è¯´ï¼Œè¿™æ˜¯æ¯ä¸ªåƒç´ ã€‚å³ä½¿æ‚¨åªæ˜¯åœ¨æ‰‹å†™æ•°å­—çš„ç®€å•å›¾åƒä¸Šè¿è¡ŒSHAPï¼Œè®¡ç®—æˆæœ¬ä¹Ÿå¾ˆé«˜ã€‚
- en: If you want an interpretability method for a neural network processing image
    data, there are much better options. For example, we havenâ€™t gone into *global*
    model-agnostic interpretability methods yet.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦ä¸€ä¸ªç”¨äºç¥ç»ç½‘ç»œå¤„ç†å›¾åƒæ•°æ®çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œè¿˜æœ‰æ›´å¥½çš„é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰ä»‹ç»*å…¨å±€*æ¨¡å‹æ— å…³çš„å¯è§£é‡Šæ€§æ–¹æ³•ã€‚
- en: '![ptml 0315](assets/ptml_0315.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0315](assets/ptml_0315.png)'
- en: Figure 3-15\. Using SHAP zero-shot text classification on an intentionally neutral
    input
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 3-15\. åœ¨ä¸€ä¸ªæœ‰æ„ä¹‰ä¸Šæ˜¯ä¸­ç«‹è¾“å…¥ä¸Šä½¿ç”¨SHAPè¿›è¡Œé›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»ã€‚
- en: Global Model-Agnostic Interpretability Methods
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¨å±€æ¨¡å‹æ— å…³çš„å¯è§£é‡Šæ€§æ–¹æ³•
- en: As discussed before, local interpretability focuses on making sense of individual
    decisions. By contrast, global methods seek to make sense of the behavior of the
    whole model. The inherently interpretable methods weâ€™ve discussed earlier offer
    global interpretations. However, those interpretation methods were all specific
    to the model type. Here, we want to examine the global behavior of a model in
    a way that is independent of the model type (model-agnostic).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œå±€éƒ¨å¯è§£é‡Šæ€§ä¸“æ³¨äºç†è§£ä¸ªåˆ«å†³ç­–ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¨å±€æ–¹æ³•æ—¨åœ¨ç†è§£æ•´ä¸ªæ¨¡å‹çš„è¡Œä¸ºã€‚æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„å›ºæœ‰å¯è§£é‡Šæ–¹æ³•æä¾›äº†å…¨å±€è§£é‡Šã€‚ç„¶è€Œï¼Œè¿™äº›è§£é‡Šæ–¹æ³•éƒ½æ˜¯é’ˆå¯¹ç‰¹å®šæ¨¡å‹ç±»å‹çš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¸Œæœ›ä»¥ä¸æ¨¡å‹ç±»å‹æ— å…³çš„æ–¹å¼æ£€éªŒæ¨¡å‹çš„å…¨å±€è¡Œä¸ºï¼ˆæ¨¡å‹æ— å…³ï¼‰ã€‚
- en: Permutation feature importance
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ’åˆ—ç‰¹å¾é‡è¦æ€§
- en: Permutation feature importance refers to permuting parts of the input features
    to see which ones cause the biggest change to the output predictions when modified.
    This can be applied to images, text, and tabular data
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: æ’åˆ—ç‰¹å¾é‡è¦æ€§æŒ‡çš„æ˜¯æ’åˆ—è¾“å…¥ç‰¹å¾çš„éƒ¨åˆ†ï¼Œä»¥æŸ¥çœ‹ä¿®æ”¹æ—¶å¯¹è¾“å‡ºé¢„æµ‹é€ æˆæœ€å¤§å˜åŒ–çš„ç‰¹å¾ã€‚è¿™å¯ä»¥åº”ç”¨äºå›¾åƒã€æ–‡æœ¬å’Œè¡¨æ ¼æ•°æ®ã€‚
- en: One way permutation feature importance can be applied to vision is by testing
    occlusion sensitivity. This is how much the decision output changes when certain
    sections of an image are occluded by a square of arbitrary size.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: æ’åˆ—ç‰¹å¾é‡è¦æ€§å¯ä»¥åº”ç”¨äºè§†è§‰çš„ä¸€ç§æ–¹å¼æ˜¯æµ‹è¯•é®æŒ¡æ•æ„Ÿæ€§ã€‚è¿™æ˜¯æŒ‡å½“å›¾åƒçš„æŸäº›éƒ¨åˆ†è¢«ä»»æ„å¤§å°çš„æ­£æ–¹å½¢é®æŒ¡æ—¶ï¼Œå†³ç­–è¾“å‡ºçš„å˜åŒ–ç¨‹åº¦ã€‚
- en: Global surrogate models
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å…¨å±€æ›¿ä»£æ¨¡å‹
- en: This technique involves taking a model and creating another that behaves extremely
    similarly. The idea is that you can take a model thatâ€™s otherwise a black box,
    and create an intrinsically interpretable model that behaves almost exactly like
    it (this is the â€œsurrogateâ€ in this case).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æŠ€æœ¯æ¶‰åŠä½¿ç”¨ä¸€ä¸ªæ¨¡å‹åˆ›å»ºå¦ä¸€ä¸ªè¡Œä¸ºæå…¶ç›¸ä¼¼çš„æ¨¡å‹ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œä½ å¯ä»¥æ‹¿ä¸€ä¸ªåŸæœ¬æ˜¯é»‘ç›’çš„æ¨¡å‹ï¼Œåˆ›å»ºä¸€ä¸ªæœ¬è´¨ä¸Šå¯è§£é‡Šçš„æ¨¡å‹ï¼Œå…¶è¡Œä¸ºå‡ ä¹ä¸åŸæ¨¡å‹å®Œå…¨ç›¸åŒï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹å°±æ˜¯â€œæ›¿ä»£æ¨¡å‹â€ï¼‰ã€‚
- en: The advantage of this approach is that one can make sense of the high-level
    behaviors of otherwise inscrutable models. The downside is that all the interpretations
    are of the surrogate, not the original model itself. While the decisions of the
    surrogate may be a close approximation, they are not the original model.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹åœ¨äºå¯ä»¥ç†è§£åŸæœ¬æ™¦æ¶©éš¾æ‡‚æ¨¡å‹çš„é«˜çº§è¡Œä¸ºã€‚ç¼ºç‚¹æ˜¯æ‰€æœ‰è§£é‡Šéƒ½æ˜¯é’ˆå¯¹æ›¿ä»£æ¨¡å‹è€ŒéåŸå§‹æ¨¡å‹æœ¬èº«ã€‚è™½ç„¶æ›¿ä»£æ¨¡å‹çš„å†³ç­–å¯èƒ½æ˜¯ä¸€ä¸ªæ¥è¿‘çš„è¿‘ä¼¼ï¼Œä½†å®ƒä»¬ä¸æ˜¯åŸå§‹æ¨¡å‹ã€‚
- en: Prototypes and criticisms
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åŸå‹å’Œæ‰¹è¯„
- en: Prototypes and criticisms are both example-based interpretability approaches.
    A *prototype* is a synthetic data point designed to be representative of all the
    data points that result in a certain decision. Criticisms do the opposite, in
    that they create a synthetic data point representing instances that result in
    incorrect decisions.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå‹å’Œæ‰¹è¯„éƒ½æ˜¯åŸºäºç¤ºä¾‹çš„å¯è§£é‡Šæ€§æ–¹æ³•ã€‚*åŸå‹* æ˜¯è®¾è®¡æˆä»£è¡¨å¯¼è‡´æŸä¸ªå†³ç­–çš„æ‰€æœ‰æ•°æ®ç‚¹çš„åˆæˆæ•°æ®ç‚¹ã€‚æ‰¹è¯„åˆ™ç›¸åï¼Œå®ƒä»¬åˆ›å»ºä¸€ä¸ªä»£è¡¨å¯¼è‡´é”™è¯¯å†³ç­–çš„å®ä¾‹çš„åˆæˆæ•°æ®ç‚¹ã€‚
- en: '*MMD-critic* is an example-based interpretability approach developed by Kim
    et al. that combines prototypes and criticisms in a single framework.^([15](ch03.html#idm45621839628720))
    At a high level, it can be summarized as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '*MMD-critic* æ˜¯ç”±Kimç­‰äººå¼€å‘çš„åŸºäºç¤ºä¾‹çš„å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œå°†åŸå‹å’Œæ‰¹è¯„ç»“åˆåœ¨ä¸€ä¸ªæ¡†æ¶ä¸­ã€‚^([15](ch03.html#idm45621839628720))
    åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œå¯ä»¥æ€»ç»“å¦‚ä¸‹ï¼š'
- en: Select the number of prototypes and criticisms you want to find.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©è¦æŸ¥æ‰¾çš„åŸå‹å’Œæ‰¹è¯„çš„æ•°é‡ã€‚
- en: Find prototypes with greedy search.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è´ªå©ªæœç´¢æ‰¾åˆ°åŸå‹ã€‚
- en: Prototypes are selected so that the distribution of the prototypes is close
    to the data distribution.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©åŸå‹ä»¥ä½¿åŸå‹åˆ†å¸ƒæ¥è¿‘æ•°æ®åˆ†å¸ƒã€‚
- en: Find criticisms with greedy search.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è´ªå©ªæœç´¢æ‰¾åˆ°æ‰¹è¯„ã€‚
- en: Points are selected as criticisms where the distribution of prototypes differs
    from the distribution of the data.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½œä¸ºæ‰¹è¯„é€‰å®šçš„ç‚¹æ˜¯åŸå‹åˆ†å¸ƒä¸æ•°æ®åˆ†å¸ƒä¸åŒçš„åœ°æ–¹ã€‚
- en: Explaining Neural Networks
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£é‡Šç¥ç»ç½‘ç»œ
- en: There are a lot of challenges when it comes to interpreting neural networks.
    Put simply, neural networks are universal function approximators. The idea is
    that with enough parameters in an equation, you can do pretty much anything. For
    example, as the famous physicist von Neumann is quoted as saying, â€œWith four parameters
    I can fit an elephant, and with five I can make him wiggle his trunk.â€
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: è§£é‡Šç¥ç»ç½‘ç»œå­˜åœ¨è®¸å¤šæŒ‘æˆ˜ã€‚ç®€å•åœ°è¯´ï¼Œç¥ç»ç½‘ç»œæ˜¯é€šç”¨å‡½æ•°é€¼è¿‘å™¨ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œé€šè¿‡åœ¨ä¸€ä¸ªæ–¹ç¨‹ä¸­ä½¿ç”¨è¶³å¤Ÿå¤šçš„å‚æ•°ï¼Œä½ å¯ä»¥åšå‡ ä¹ä»»ä½•äº‹æƒ…ã€‚ä¾‹å¦‚ï¼Œæ­£å¦‚è‘—åç‰©ç†å­¦å®¶å†¯Â·è¯ºä¼Šæ›¼æ‰€è¯´ï¼Œâ€œç”¨å››ä¸ªå‚æ•°æˆ‘å¯ä»¥æ‹Ÿåˆä¸€åªå¤§è±¡ï¼Œç”¨äº”ä¸ªå‚æ•°æˆ‘å¯ä»¥è®©å®ƒæ‘‡åŠ¨é¼»å­ã€‚â€
- en: With a neural network, each neuron is essentially a parameter in a gargantuan
    equation. Some parameters might be useless in the end, but as long as the model
    predicts a pattern in the data well enough, we usually donâ€™t care.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç¥ç»ç½‘ç»œè€Œè¨€ï¼Œæ¯ä¸ªç¥ç»å…ƒæœ¬è´¨ä¸Šéƒ½æ˜¯ä¸€ä¸ªå·¨å¤§æ–¹ç¨‹ä¸­çš„å‚æ•°ã€‚ä¸€äº›å‚æ•°æœ€ç»ˆå¯èƒ½æ˜¯æ— ç”¨çš„ï¼Œä½†åªè¦æ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°é¢„æµ‹æ•°æ®æ¨¡å¼ï¼Œæˆ‘ä»¬é€šå¸¸ä¸ä¼šå¤ªåœ¨æ„ã€‚
- en: This is obviously a problem when it comes to interpreting exactly what the model
    is doing. True, depending on which framework you use, you can add a bunch of abstractions
    to the defining of the mode. In fact, [PyTorch supports named tensors](https://oreil.ly/jWzUD),
    which are a way to add semantic meaning to the dimensions of a tensor.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ¶‰åŠå‡†ç¡®è§£é‡Šæ¨¡å‹æ­£åœ¨åšä»€ä¹ˆæ—¶ï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚ç¡®å®ï¼Œæ ¹æ®æ‚¨ä½¿ç”¨çš„æ¡†æ¶ï¼Œæ‚¨å¯ä»¥ä¸ºæ¨¡å¼çš„å®šä¹‰æ·»åŠ å¤§é‡æŠ½è±¡ã€‚äº‹å®ä¸Šï¼Œ[PyTorch æ”¯æŒå‘½åå¼ é‡](https://oreil.ly/jWzUD)ï¼Œè¿™æ˜¯å‘å¼ é‡çš„ç»´åº¦æ·»åŠ è¯­ä¹‰å«ä¹‰çš„ä¸€ç§æ–¹å¼ã€‚
- en: If you create a map of the modelâ€™s behavior without mapping out every single
    neuron, you might feel like youâ€™re missing some information. However, if you make
    your map of the modelâ€™s weights too granular, you can no longer understand it.
    This problem is often solved by looking at larger patterns in a neural networkâ€™s
    activations and behaviors, but even this might not tell the whole story. For example,
    a few researchers demonstrated some popular interpretability methods on copies
    of a neural network. For these copies they randomized a subset of the neural network
    weights. Despite this, they found that these interpretability methods could not
    tell the networks apart as long as the output accuracies were the same.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åˆ›å»ºæ¨¡å‹è¡Œä¸ºçš„åœ°å›¾è€Œæ²¡æœ‰æ˜ å°„å‡ºæ¯ä¸€ä¸ªç¥ç»å…ƒï¼Œæ‚¨å¯èƒ½ä¼šè§‰å¾—è‡ªå·±é”™è¿‡äº†ä¸€äº›ä¿¡æ¯ã€‚ç„¶è€Œï¼Œå¦‚æœæ‚¨å°†æ¨¡å‹æƒé‡çš„åœ°å›¾åˆ¶ä½œå¾—å¤ªç»†ç²’åº¦ï¼Œé‚£ä¹ˆæ‚¨å°±æ— æ³•ç†è§£å®ƒäº†ã€‚è¿™ä¸ªé—®é¢˜é€šå¸¸é€šè¿‡æŸ¥çœ‹ç¥ç»ç½‘ç»œæ¿€æ´»å’Œè¡Œä¸ºä¸­çš„æ›´å¤§æ¨¡å¼æ¥è§£å†³ï¼Œä½†å³ä½¿è¿™æ ·ä¹Ÿå¯èƒ½æ— æ³•è®²è¿°æ•´ä¸ªæ•…äº‹ã€‚ä¾‹å¦‚ï¼Œä¸€äº›ç ”ç©¶äººå‘˜å¯¹ç¥ç»ç½‘ç»œçš„å‰¯æœ¬ä¸Šæ¼”ç¤ºäº†ä¸€äº›æµè¡Œçš„å¯è§£é‡Šæ€§æ–¹æ³•ã€‚å¯¹äºè¿™äº›å‰¯æœ¬ï¼Œä»–ä»¬éšæœºåŒ–äº†ç¥ç»ç½‘ç»œæƒé‡çš„å­é›†ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä»–ä»¬å‘ç°è¿™äº›å¯è§£é‡Šæ€§æ–¹æ³•åœ¨è¾“å‡ºå‡†ç¡®æ€§ç›¸åŒçš„æƒ…å†µä¸‹æ— æ³•åŒºåˆ†ç½‘ç»œã€‚
- en: While these interpretability methods are still being improved, they are often
    still better than not having any interpretability methods at all. Whatâ€™s important
    is recognizing what these interpretations should and should not be used for.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™äº›å¯è§£é‡Šæ€§æ–¹æ³•ä»åœ¨ä¸æ–­æ”¹è¿›ä¸­ï¼Œå®ƒä»¬é€šå¸¸ä»ç„¶æ¯”å®Œå…¨æ²¡æœ‰ä»»ä½•å¯è§£é‡Šæ€§æ–¹æ³•è¦å¥½ã€‚é‡è¦çš„æ˜¯è¦è®¤è¯†åˆ°è¿™äº›è§£é‡Šæ–¹æ³•åº”è¯¥ç”¨äºä»€ä¹ˆï¼Œä¸åº”è¯¥ç”¨äºä»€ä¹ˆã€‚
- en: Saliency Mapping
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ˜¾è‘—æ€§æ˜ å°„
- en: A *saliency map* is an image that highlights the region on which a networkâ€™s
    activations or attention focuses first. The goal of a saliency map is to reflect
    the degree of importance of a pixel to the model. *Saliency mapping* is useful
    in that it can point to specific parts of the input (for example, pixels in an
    input image or tokens in input text) that it attributes to a decision.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ˜¾è‘—æ€§åœ°å›¾*æ˜¯ä¸€ä¸ªçªå‡ºæ˜¾ç¤ºç½‘ç»œæ¿€æ´»æˆ–å…³æ³¨çš„åŒºåŸŸçš„å›¾åƒã€‚æ˜¾è‘—æ€§åœ°å›¾çš„ç›®æ ‡æ˜¯åæ˜ åƒç´ å¯¹æ¨¡å‹çš„é‡è¦æ€§ç¨‹åº¦ã€‚*æ˜¾è‘—æ€§æ˜ å°„*çš„ç”¨å¤„åœ¨äºå®ƒå¯ä»¥æŒ‡å‡ºè¾“å…¥çš„ç‰¹å®šéƒ¨åˆ†ï¼ˆä¾‹å¦‚è¾“å…¥å›¾åƒä¸­çš„åƒç´ æˆ–è¾“å…¥æ–‡æœ¬ä¸­çš„æ ‡è®°ï¼‰ï¼Œè¿™äº›éƒ¨åˆ†è¢«å½’å› äºæŸä¸ªå†³ç­–ã€‚'
- en: Since saliency mapping allows you to look at what would contribute to different
    decisions, this can serve as a way to provide counterfactual evidence. For example,
    in a binary text sentiment classification task, one could look at embeddings that
    would contribute to either a positive or negative sentiment.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ˜¾è‘—æ€§æ˜ å°„å…è®¸æ‚¨æŸ¥çœ‹å¯¹ä¸åŒå†³ç­–æœ‰è´¡çŒ®çš„å†…å®¹ï¼Œè¿™å¯ä»¥ä½œä¸ºæä¾›åäº‹å®è¯æ®çš„ä¸€ç§æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œåœ¨äºŒå…ƒæ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¯ä»¥æŸ¥çœ‹ä¼šå¯¹ç§¯ææˆ–æ¶ˆææƒ…æ„Ÿæœ‰æ‰€è´¡çŒ®çš„åµŒå…¥ã€‚
- en: Gradient-based approaches are much faster to compute than methods like LIME
    or SHAP.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºæ¢¯åº¦çš„æ–¹æ³•æ¯”åƒ LIME æˆ– SHAP è¿™æ ·çš„æ–¹æ³•è®¡ç®—é€Ÿåº¦å¿«å¾—å¤šã€‚
- en: 'Deep Dive: Saliency Mapping with CLIP'
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·±å…¥æ¢è®¨ï¼šCLIP ä¸­çš„æ˜¾è‘—æ€§æ˜ å°„
- en: '[CLIP (Contrastive Language-Image Pre-training)](https://oreil.ly/PQ4my) is
    a model from OpenAI created as a bridge between how text is represented as embeddings
    and how images are represented as embeddings. In practical terms, this means that
    it can be used to compare the concept represented by an input string such as `"An
    adorable kitten sitting in a basket"` and the concept represented by an actual
    photo of a kitten in a basket, providing a numerical score of how close the two
    are. The OpenAI implementation was also trained to act as a zero-shot image classifier.
    For example, it is capable of not just recognizing an ImageNet photograph of a
    banana as a banana but also recognizing bananas in corrupted and low-quality photographs,
    as well as drawings and artistic depictions of bananas.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPï¼ˆå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒï¼‰](https://oreil.ly/PQ4my) æ˜¯ OpenAI åˆ›å»ºçš„ä¸€ä¸ªæ¨¡å‹ï¼Œç”¨ä½œæ–‡æœ¬è¡¨ç¤ºå’Œå›¾åƒè¡¨ç¤ºä¹‹é—´çš„æ¡¥æ¢ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥ç”¨æ¥æ¯”è¾ƒè¾“å…¥å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚`"ä¸€ä¸ªå¯çˆ±çš„å°çŒ«ååœ¨ç¯®å­é‡Œ"`ï¼‰æ‰€ä»£è¡¨çš„æ¦‚å¿µä¸å®é™…çŒ«å’ªç…§ç‰‡æ‰€ä»£è¡¨çš„æ¦‚å¿µä¹‹é—´çš„ç›¸ä¼¼æ€§å¾—åˆ†ã€‚OpenAI
    çš„å®ç°è¿˜ç»è¿‡è®­ç»ƒï¼Œå¯ä»¥ä½œä¸ºé›¶æ ·æœ¬å›¾åƒåˆ†ç±»å™¨ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œå®ƒä¸ä»…èƒ½å¤Ÿè¯†åˆ« ImageNet ä¸­é¦™è•‰çš„ç…§ç‰‡ï¼Œè¿˜èƒ½å¤Ÿè¯†åˆ«æŸåå’Œä½è´¨é‡ç…§ç‰‡ä¸­ä»¥åŠç”»ä½œä¸­çš„é¦™è•‰ã€‚'
- en: Training a network to associate text embeddings with images allows the model
    to describe the content of an image in human-understandable terms, not just as
    one-hot encoded vectors representing a predetermined number of output categories.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒç½‘ç»œä»¥å°†æ–‡æœ¬åµŒå…¥ä¸å›¾åƒå…³è”å…è®¸æ¨¡å‹ä»¥äººç±»å¯ç†è§£çš„æ–¹å¼æè¿°å›¾åƒå†…å®¹ï¼Œè€Œä¸ä»…ä»…æ˜¯ä½œä¸ºè¡¨ç¤ºé¢„å®šè¾“å‡ºç±»åˆ«æ•°é‡çš„ç‹¬çƒ­ç¼–ç å‘é‡ã€‚
- en: However, all these capabilities rely on human users trusting CLIP to *correctly*
    associate text embeddings with image embeddings. Given the enormous variety of
    possible image and text pairings, this is no simple task. Still, the concepts
    weâ€™ve covered so far can offer some guidance.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæ‰€æœ‰è¿™äº›èƒ½åŠ›éƒ½ä¾èµ–äºäººç±»ç”¨æˆ·ä¿¡ä»» CLIP èƒ½å¤Ÿ*æ­£ç¡®*å…³è”æ–‡æœ¬åµŒå…¥å’Œå›¾åƒåµŒå…¥ã€‚è€ƒè™‘åˆ°å¯èƒ½çš„å›¾åƒå’Œæ–‡æœ¬é…å¯¹çš„å·¨å¤§å¤šæ ·æ€§ï¼Œè¿™å¹¶ä¸æ˜¯ä¸€é¡¹ç®€å•çš„ä»»åŠ¡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬è¿„ä»Šæ‰€æ¶µç›–çš„æ¦‚å¿µå¯ä»¥æä¾›ä¸€äº›æŒ‡å¯¼ã€‚
- en: Note
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: You can find the code associated with this tutorial in the notebooks [*Chapter_3_CLIP_Saliency_mapping_Part1.ipynb*](https://oreil.ly/6pmRx)
    and [*Chapter_3_CLIP_Saliency_mapping_Part2.ipynb*](https://oreil.ly/GBP7Q). This
    was heavily inspired by [`hila-chefer`â€™s `Transformer-MM-Explainability` project](https://oreil.ly/9iKQ7),
    and makes use of the Captum library.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨ç¬”è®°æœ¬ [*Chapter_3_CLIP_Saliency_mapping_Part1.ipynb*](https://oreil.ly/6pmRx)
    å’Œ [*Chapter_3_CLIP_Saliency_mapping_Part2.ipynb*](https://oreil.ly/GBP7Q) ä¸­æ‰¾åˆ°ä¸æœ¬æ•™ç¨‹ç›¸å…³çš„ä»£ç ã€‚è¿™äº›ç¬”è®°æœ¬å—åˆ°
    [`hila-chefer` çš„ `Transformer-MM-Explainability` é¡¹ç›®](https://oreil.ly/9iKQ7) çš„å¯å‘ï¼Œå¹¶åˆ©ç”¨äº†
    Captum åº“ã€‚
- en: Running this code has a high RAM requirement. If you are running this in Google
    Colab, you should use the largest GPU available in Colab Pro and set the RAM to
    the highest setting.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œæ­¤ä»£ç éœ€è¦å¤§é‡ RAMã€‚å¦‚æœæ‚¨åœ¨ Google Colab ä¸­è¿è¡Œæ­¤ä»£ç ï¼Œè¯·ä½¿ç”¨ Colab Pro ä¸­æœ€å¤§çš„ GPUï¼Œå¹¶å°† RAM è®¾ç½®ä¸ºæœ€é«˜ã€‚
- en: For working with CLIP, you can download the code from the projectâ€™s [public
    repository](https://oreil.ly/Q1xgq). We will place the model in a subdirectory
    from which you can import the model.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: è‹¥è¦ä½¿ç”¨ CLIPï¼Œæ‚¨å¯ä»¥ä»é¡¹ç›®çš„ [å…¬å…±å­˜å‚¨åº“](https://oreil.ly/Q1xgq) ä¸‹è½½ä»£ç ã€‚æˆ‘ä»¬å°†æ¨¡å‹æ”¾åœ¨ä¸€ä¸ªå­ç›®å½•ä¸­ï¼Œæ‚¨å¯ä»¥ä»ä¸­å¯¼å…¥æ¨¡å‹ã€‚
- en: '[PRE44]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Once weâ€™ve set up your development environment, you can download the weights
    for the CLIP models. These weights are available directly from OpenAI.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬è®¾ç½®å¥½æ‚¨çš„å¼€å‘ç¯å¢ƒï¼Œæ‚¨å¯ä»¥ä¸‹è½½ CLIP æ¨¡å‹çš„æƒé‡ã€‚è¿™äº›æƒé‡å¯ä»¥ç›´æ¥ä» OpenAI è·å–ã€‚
- en: '[PRE45]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Before you test out CLIP with any inputs, you need to set up your pre-processing
    steps. While CLIP is a technically impressive model, itâ€™s still important to not
    forget proper pre-processing. Since CLIP works with both text and images, you
    need to be able to pre-process both data types.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨ä½¿ç”¨ä»»ä½•è¾“å…¥æµ‹è¯• CLIP ä¹‹å‰ï¼Œæ‚¨éœ€è¦è®¾ç½®é¢„å¤„ç†æ­¥éª¤ã€‚è™½ç„¶ CLIP æ˜¯ä¸€ä¸ªæŠ€æœ¯ä¸Šä»¤äººå°è±¡æ·±åˆ»çš„æ¨¡å‹ï¼Œä½†ä¸è¦å¿˜è®°æ­£ç¡®çš„é¢„å¤„ç†æ­¥éª¤ã€‚ç”±äº CLIP
    åŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼Œæ‚¨éœ€è¦èƒ½å¤Ÿé¢„å¤„ç†è¿™ä¸¤ç§æ•°æ®ç±»å‹ã€‚
- en: '[PRE47]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: For the text pre-processing, weâ€™ll use a case-insensitive tokenizer.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ–‡æœ¬é¢„å¤„ç†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸åŒºåˆ†å¤§å°å†™çš„åˆ†è¯å™¨ã€‚
- en: For the image pre-processing, weâ€™ll go through a standard pixel intensity normalization,^([16](ch03.html#idm45621839311008))
    image resizing, and center-cropping procedure.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå›¾åƒé¢„å¤„ç†ï¼Œæˆ‘ä»¬å°†è¿›è¡Œæ ‡å‡†çš„åƒç´ å¼ºåº¦å½’ä¸€åŒ–ï¼Œ^([16](ch03.html#idm45621839311008)) å›¾åƒè°ƒæ•´å¤§å°å’Œä¸­å¿ƒè£å‰ªçš„è¿‡ç¨‹ã€‚
- en: We could create this pre-processing stage ourselves, but we donâ€™t need to. As
    you saw earlier, we can load CLIPâ€™s pre-processing module for the particular model
    weâ€™re using. We can just inspect that pre-processing step to make sure it has
    all the correct stages.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è‡ªå·±åˆ›å»ºè¿™ä¸ªé¢„å¤„ç†é˜¶æ®µï¼Œä½†æˆ‘ä»¬æ²¡æœ‰å¿…è¦ã€‚æ­£å¦‚æ‚¨æ—©äº›æ—¶å€™çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½ç‰¹å®šæ¨¡å‹çš„ CLIP é¢„å¤„ç†æ¨¡å—ã€‚æˆ‘ä»¬åªéœ€æ£€æŸ¥è¯¥é¢„å¤„ç†æ­¥éª¤ï¼Œä»¥ç¡®ä¿å®ƒå…·æœ‰æ‰€æœ‰æ­£ç¡®çš„é˜¶æ®µã€‚
- en: '[PRE49]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Once this is done, you will prepare the model to take in a set of example images
    and their text descriptions. We can test the model by measuring the cosine similarity
    between the features generated for the text and the features generated for the
    image.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®Œæˆï¼Œæ‚¨å°†å‡†å¤‡æ¨¡å‹æ¥æ¥æ”¶ä¸€ç»„ç¤ºä¾‹å›¾åƒåŠå…¶æ–‡æœ¬æè¿°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æµ‹é‡ç”Ÿæˆçš„æ–‡æœ¬ç‰¹å¾å’Œå›¾åƒç‰¹å¾ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ¥æµ‹è¯•æ¨¡å‹ã€‚
- en: '[PRE51]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As shown in [FigureÂ 3-16](#clip-saliency-0), we have a wide variety of images,
    from the realistic to the abstract, from the clearly defined to the blurred and
    unclear.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[å›¾ 3-16](#clip-saliency-0)æ‰€ç¤ºï¼Œæˆ‘ä»¬æ‹¥æœ‰å„ç§å„æ ·çš„å›¾åƒï¼Œä»é€¼çœŸçš„åˆ°æŠ½è±¡çš„ï¼Œä»æ¸…æ™°å®šä¹‰çš„åˆ°æ¨¡ç³Šä¸æ¸…çš„ã€‚
- en: '![ptml 0316](assets/ptml_0316.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0316](assets/ptml_0316.png)'
- en: Figure 3-16\. CLIPâ€™s matching of words to images
  id: totrans-307
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-16\. CLIP å°†å•è¯ä¸å›¾åƒåŒ¹é…
- en: '[PRE52]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Despite some of these images being potentially difficult to classify by a human,
    CLIP does a pretty good job of pairing them with their textual descriptions, as
    shown in [FigureÂ 3-17](#clip-saliency-1).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™äº›å›¾åƒæœ‰äº›å¯èƒ½å¯¹äººç±»æ¥è¯´éš¾ä»¥åˆ†ç±»ï¼Œä½†æ˜¯ CLIP åœ¨å°†å®ƒä»¬ä¸å…¶æ–‡æœ¬æè¿°é…å¯¹æ–¹é¢åšå¾—ç›¸å½“å¥½ï¼Œæ­£å¦‚[å›¾ 3-17](#clip-saliency-1)æ‰€ç¤ºã€‚
- en: '![ptml 0317](assets/ptml_0317.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0317](assets/ptml_0317.png)'
- en: Figure 3-17\. Using CLIP to pair images with text descriptions
  id: totrans-311
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-17\. ä½¿ç”¨ CLIP å°†å›¾åƒä¸æ–‡æœ¬æè¿°é…å¯¹
- en: Weâ€™ll then run these pairs through our text and image pre-processing steps,
    followed by the CLIP model itself.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™äº›å¯¹é€šè¿‡æˆ‘ä»¬çš„æ–‡æœ¬å’Œå›¾åƒé¢„å¤„ç†æ­¥éª¤ï¼Œç„¶åé€šè¿‡ CLIP æ¨¡å‹æœ¬èº«ã€‚
- en: '[PRE53]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: To compare these image features with the text features, you normalize them both
    and calculate the dot product of each pair of features.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ¯”è¾ƒè¿™äº›å›¾åƒç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ï¼Œæ‚¨éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶è®¡ç®—æ¯å¯¹ç‰¹å¾çš„ç‚¹ç§¯ã€‚
- en: '[PRE54]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: As you can see in [FigureÂ 3-18](#clip-saliency-2), CLIP is very good at identifying
    not just when text and an image are similar but, just as importantly, when they
    are very dissimilar.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ‚¨åœ¨[å›¾ 3-18](#clip-saliency-2)ä¸­æ‰€è§ï¼ŒCLIP ä¸ä»…åœ¨æ–‡æœ¬å’Œå›¾åƒç›¸ä¼¼æ—¶è¡¨ç°éå¸¸å¥½ï¼Œè€Œä¸”åœ¨å®ƒä»¬éå¸¸ä¸ç›¸ä¼¼æ—¶åŒæ ·é‡è¦ã€‚
- en: Now that weâ€™ve validated that CLIP works on preselected images and text pairs,
    we can use it to generate classifications for images from completely different
    datasets. To make CLIPâ€™s output behave like the logits to the softmax operation,
    you just take the cosine similarity multiplied by 100.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»éªŒè¯äº† CLIP åœ¨é¢„é€‰å›¾åƒå’Œæ–‡æœ¬å¯¹ä¸Šçš„å·¥ä½œæ•ˆæœï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥ä¸ºæ¥è‡ªå®Œå…¨ä¸åŒæ•°æ®é›†çš„å›¾åƒç”Ÿæˆåˆ†ç±»ã€‚ä¸ºäº†ä½¿ CLIP çš„è¾“å‡ºè¡Œä¸ºç±»ä¼¼äº softmax
    æ“ä½œçš„å¯¹æ•°å‡ ç‡ï¼Œåªéœ€å°†ä½™å¼¦ç›¸ä¼¼åº¦ä¹˜ä»¥ 100ã€‚
- en: '![ptml 0318](assets/ptml_0318.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0318](assets/ptml_0318.png)'
- en: Figure 3-18\. Cosine similarity between text and image features
  id: totrans-319
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-18\. æ–‡æœ¬ä¸å›¾åƒç‰¹å¾ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
- en: '[PRE55]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Many of the interpretability techniques weâ€™ve covered so far in this chapter
    have been demonstrated many times on models with a limited number of possible
    outputs. The problem is that models can sometimes be presented with inputs that
    are like nothing theyâ€™ve seen before. This is why combining interpretability methods
    like saliency mapping with CLIPâ€™s zero-shot capabilities can be very powerful.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œåœ¨æœ¬ç« ä¸­æˆ‘ä»¬ä»‹ç»çš„è®¸å¤šå¯è§£é‡Šæ€§æŠ€æœ¯å·²ç»å¤šæ¬¡åœ¨å…·æœ‰æœ‰é™å¯èƒ½è¾“å‡ºçš„æ¨¡å‹ä¸Šè¿›è¡Œäº†æ¼”ç¤ºã€‚é—®é¢˜åœ¨äºï¼Œæ¨¡å‹æœ‰æ—¶ä¼šè¢«å‘ˆç°å‡ºå®ƒä»¬ä»æœªè§è¿‡çš„è¾“å…¥ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå°†æ˜¾è‘—æ€§æ˜ å°„ç­‰å¯è§£é‡Šæ€§æ–¹æ³•ä¸
    CLIP çš„é›¶æ ·æœ¬èƒ½åŠ›ç»“åˆåœ¨ä¸€èµ·å¯èƒ½éå¸¸å¼ºå¤§çš„åŸå› ã€‚
- en: To do saliency mapping with CLIP, make sure CLIP has been set up using the previously
    described steps. We also want to download [Captum](https://captum.ai), a model
    interpretability tool kit for PyTorch.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨ CLIP è¿›è¡Œæ˜¾è‘—æ€§æ˜ å°„ï¼Œè¯·ç¡®ä¿å·²æŒ‰å…ˆå‰æè¿°çš„æ­¥éª¤è®¾ç½®å¥½ CLIPã€‚æˆ‘ä»¬è¿˜éœ€è¦ä¸‹è½½[Captum](https://captum.ai)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº
    PyTorch çš„æ¨¡å‹å¯è§£é‡Šæ€§å·¥å…·åŒ…ã€‚
- en: For the saliency mapping, weâ€™ll also want to select the layers from which weâ€™ll
    take the activations. The final layers of the model are the final probabilities
    fed into the output layers. To get a sense for the logic happening between the
    input and output, you need to pick an intermediate layer.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ˜¾è‘—æ€§æ˜ å°„ï¼Œæˆ‘ä»¬è¿˜éœ€è¦é€‰æ‹©æˆ‘ä»¬å°†ä»ä¸­è·å–æ¿€æ´»çš„å±‚ã€‚æ¨¡å‹çš„æœ€ç»ˆå±‚æ˜¯è¾“å…¥åˆ°è¾“å‡ºå±‚çš„æœ€ç»ˆæ¦‚ç‡ã€‚ä¸ºäº†äº†è§£åœ¨è¾“å…¥å’Œè¾“å‡ºä¹‹é—´å‘ç”Ÿçš„é€»è¾‘ï¼Œæ‚¨éœ€è¦é€‰æ‹©ä¸€ä¸ªä¸­é—´å±‚ã€‚
- en: '[PRE56]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: CLIP should be okay to inspect, but there are a few key changes that you need
    to make for it all to work. The existing implementation of CLIP doesnâ€™t record
    attention in a way thatâ€™s easy to log, so weâ€™re going to monkey-patch the model.
    *Monkey-patching* refers to the process of dynamically modifying a class or function
    after itâ€™s been defined. Itâ€™s a quick way of patching an existing third-party
    codebase or library as a workaround for a bug or missing feature.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP åº”è¯¥å¯ä»¥æ£€æŸ¥ï¼Œä½†æ˜¯æ‚¨éœ€è¦è¿›è¡Œä¸€äº›å…³é”®æ›´æ”¹ï¼Œä»¥ä½¿å…¶å…¨éƒ¨æ­£å¸¸å·¥ä½œã€‚ç°æœ‰çš„ CLIP å®ç°ä¸ä¼šä»¥æ˜“äºè®°å½•çš„æ–¹å¼è®°å½•æ³¨æ„åŠ›ï¼Œå› æ­¤æˆ‘ä»¬å°†å¯¹æ¨¡å‹è¿›è¡Œè¡¥ä¸ã€‚*Monkey-patching*
    æŒ‡çš„æ˜¯åœ¨å®šä¹‰ååŠ¨æ€ä¿®æ”¹ç±»æˆ–å‡½æ•°çš„è¿‡ç¨‹ã€‚è¿™æ˜¯ä¿®è¡¥ç°æœ‰ç¬¬ä¸‰æ–¹ä»£ç åº“æˆ–åº“çš„å¿«é€Ÿæ–¹æ³•ï¼Œç”¨äºè§£å†³é”™è¯¯æˆ–ç¼ºå°‘åŠŸèƒ½ã€‚
- en: Note
  id: totrans-326
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The full extent of the monkey-patching of OpenAIâ€™s code can be found in the
    [accompanying notebook for this section](https://oreil.ly/oYXTi). These changes
    are geared toward the `'ViT-B/32'` and `'ViT-B/16'` models. Due to differences
    in architecture, these changes would not be compatible with the `'RN50'`, `'RN101'`,
    `'RN50x4'`, `'RN50x16'`, `'RN50x64'`, `'ViT-L/14'`, and `'ViT-L/14@336px'` CLIP
    models. OpenAI may change CLIP in the future to include these changes. For now,
    weâ€™re going to accommodate the branch weâ€™re working with.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI çš„ä»£ç çš„æ•´ä½“è¡¥ä¸çš„è¯¦ç»†å†…å®¹å¯ä»¥åœ¨[æœ¬èŠ‚çš„é™„å±ç¬”è®°æœ¬](https://oreil.ly/oYXTi)æ‰¾åˆ°ã€‚è¿™äº›æ›´æ”¹é’ˆå¯¹ 'ViT-B/32'
    å’Œ 'ViT-B/16' æ¨¡å‹ã€‚ç”±äºæ¶æ„çš„å·®å¼‚ï¼Œè¿™äº›æ›´æ”¹ä¸å…¼å®¹ 'RN50'ã€'RN101'ã€'RN50x4'ã€'RN50x16'ã€'RN50x64'ã€'ViT-L/14'
    å’Œ 'ViT-L/14@336px' CLIP æ¨¡å‹ã€‚OpenAI å¯èƒ½ä¼šåœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­åŒ…å«è¿™äº›æ›´æ”¹ã€‚ç›®å‰ï¼Œæˆ‘ä»¬å°†é€‚é…æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„åˆ†æ”¯ã€‚
- en: If you are unfamiliar with monkey-patching in Python, [hereâ€™s a tutorial](https://oreil.ly/ZblMe)
    on doing this in machine learning contexts.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¯¹ Python ä¸­çš„ monkey-patching ä¸ç†Ÿæ‚‰ï¼Œ[è¿™é‡Œæœ‰ä¸€ä¸ªæ•™ç¨‹](https://oreil.ly/ZblMe)ï¼Œé€‚ç”¨äºæœºå™¨å­¦ä¹ ç¯å¢ƒã€‚
- en: From here, weâ€™ll create a helper function that examines the attention blocks
    in both the image and text portions of CLIP.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå¸®åŠ©å‡½æ•°ï¼Œæ£€æŸ¥ CLIP å›¾åƒå’Œæ–‡æœ¬éƒ¨åˆ†çš„æ³¨æ„åŠ›å—ã€‚
- en: '[PRE57]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: There are two things weâ€™re interested in. The first is what parts of the input
    text the model is focusing on. For this reason, weâ€™ll define a function that overlays
    a heatmap over the characters in the text.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ„Ÿå…´è¶£çš„æœ‰ä¸¤ä»¶äº‹ã€‚ç¬¬ä¸€ä»¶æ˜¯æ¨¡å‹ä¸“æ³¨äºè¾“å…¥æ–‡æœ¬çš„å“ªäº›éƒ¨åˆ†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨æ–‡æœ¬ä¸­çš„å­—ç¬¦ä¸Šå åŠ çƒ­å›¾ã€‚
- en: '[PRE58]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The second thing weâ€™re interested in is what parts of the input image the model
    is focusing on. For this reason weâ€™ll define a function that overlays a heatmap
    over the pixels of the input image.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ„Ÿå…´è¶£çš„ç¬¬äºŒä»¶äº‹æ˜¯æ¨¡å‹ä¸“æ³¨äºè¾“å…¥å›¾åƒçš„å“ªäº›éƒ¨åˆ†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨è¾“å…¥å›¾åƒçš„åƒç´ ä¸Šå åŠ çƒ­å›¾ã€‚
- en: '[PRE59]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: With these helper functions, you can see where CLIPâ€™s attention is focusing
    on in both the text and image portions of the input. See Figures [3-19](#clip-saliency-3)
    to [3-36](#clip-saliency-20) for examples of CLIP saliency; the odd-numbered figures
    show saliency on input text, and the even-numbered figures show saliency on an
    image with similar content.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™äº›è¾…åŠ©å‡½æ•°ï¼Œæ‚¨å¯ä»¥çœ‹åˆ° CLIP åœ¨è¾“å…¥çš„æ–‡æœ¬å’Œå›¾åƒéƒ¨åˆ†çš„æ³¨æ„åŠ›é›†ä¸­åœ¨å“ªé‡Œã€‚è¯·å‚é˜…å›¾ [3-19](#clip-saliency-3) åˆ° [3-36](#clip-saliency-20)
    ï¼Œäº†è§£ CLIP æ˜¾è‘—æ€§çš„ç¤ºä¾‹ï¼›å¥‡æ•°ç¼–å·çš„å›¾æ˜¾ç¤ºè¾“å…¥æ–‡æœ¬çš„æ˜¾è‘—æ€§ï¼Œå¶æ•°ç¼–å·çš„å›¾æ˜¾ç¤ºç±»ä¼¼å†…å®¹çš„å›¾åƒä¸Šçš„æ˜¾è‘—æ€§ã€‚
- en: '[PRE60]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![ptml 0319](assets/ptml_0319.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0319](assets/ptml_0319.png)'
- en: Figure 3-19\. CLIP saliency on the input text, highlighting the *glasses* part
  id: totrans-338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-19\. åœ¨è¾“å…¥æ–‡æœ¬ä¸Šçš„ CLIP æ˜¾è‘—æ€§ï¼Œçªå‡ºæ˜¾ç¤º *çœ¼é•œ* éƒ¨åˆ†
- en: '![ptml 0320](assets/ptml_0320.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0320](assets/ptml_0320.png)'
- en: Figure 3-20\. CLIP image saliency on the input image, highlighting the glasses
  id: totrans-340
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-20\. åœ¨è¾“å…¥å›¾åƒä¸Šçš„ CLIP å›¾åƒæ˜¾è‘—æ€§ï¼Œçªå‡ºæ˜¾ç¤ºçœ¼é•œ
- en: '![ptml 0321](assets/ptml_0321.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0321](assets/ptml_0321.png)'
- en: Figure 3-21\. CLIP saliency on the input text, highlighting the *lipstick* part
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-21\. åœ¨è¾“å…¥æ–‡æœ¬ä¸Šçš„ CLIP æ˜¾è‘—æ€§ï¼Œçªå‡ºæ˜¾ç¤º *å£çº¢* éƒ¨åˆ†
- en: '![ptml 0322](assets/ptml_0322.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0322](assets/ptml_0322.png)'
- en: Figure 3-22\. CLIP image saliency on the input image, highlighting the lips
  id: totrans-344
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-22\. åœ¨è¾“å…¥å›¾åƒä¸Šçš„ CLIP å›¾åƒæ˜¾è‘—æ€§ï¼Œçªå‡ºæ˜¾ç¤ºå˜´å”‡
- en: '![ptml 0323](assets/ptml_0323.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0323](assets/ptml_0323.png)'
- en: Figure 3-23\. CLIP text saliency on text describing a rocket on a launch pad
  id: totrans-346
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-23\. åœ¨ç«ç®­å‘å°„å°ä¸Šæè¿°ç«ç®­çš„æ–‡å­—ä¸Šçš„ CLIP æ–‡æœ¬æ˜¾è‘—æ€§
- en: '![ptml 0324](assets/ptml_0324.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0324](assets/ptml_0324.png)'
- en: Figure 3-24\. CLIP image and text saliency on the input image, containing the
    Artemis rocket
  id: totrans-348
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-24\. åœ¨åŒ…å«é˜¿å°”å¿’å¼¥æ–¯ç«ç®­çš„è¾“å…¥å›¾åƒä¸Šçš„ CLIP å›¾åƒå’Œæ–‡æœ¬æ˜¾è‘—æ€§
- en: '![ptml 0325](assets/ptml_0325.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0325](assets/ptml_0325.png)'
- en: Figure 3-25\. CLIP text saliency on text describing a zebra
  id: totrans-350
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 3-25\. æè¿°æ–‘é©¬çš„æ–‡æœ¬ä¸Šçš„ CLIP æ–‡æœ¬æ˜¾è‘—æ€§
- en: '![ptml 0326](assets/ptml_0326.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0326](assets/ptml_0326.png)'
- en: Figure 3-26\. CLIP image and text saliency on the input image, containing both
    an elephant in the center but also two zebras toward the bottom near a watering
    hole
  id: totrans-352
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-26\. CLIP å›¾åƒå’Œæ–‡æœ¬æ˜¾è‘—æ€§åˆ†æç»“æœæ˜¾ç¤ºäº†è¾“å…¥å›¾åƒä¸­çš„ä¸€åªå¤§è±¡ä½äºä¸­å¿ƒï¼Œä½†ä¹Ÿæœ‰ä¸¤åªæ–‘é©¬åœ¨æ°´å‘é™„è¿‘çš„åº•éƒ¨ã€‚
- en: '![ptml 0327](assets/ptml_0327.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0327](assets/ptml_0327.png)'
- en: Figure 3-27\. CLIP text saliency on text describing a zebra
  id: totrans-354
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-27\. CLIP å¯¹æè¿°æ–‘é©¬çš„æ–‡æœ¬çš„æ˜¾è‘—æ€§åˆ†æã€‚
- en: '![ptml 0328](assets/ptml_0328.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0328](assets/ptml_0328.png)'
- en: Figure 3-28\. CLIP image and text saliency on the input image, containing both
    an elephant in the center but also two zebras toward the bottom near a watering
    hole
  id: totrans-356
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-28\. CLIP å›¾åƒå’Œæ–‡æœ¬æ˜¾è‘—æ€§åˆ†æç»“æœæ˜¾ç¤ºäº†è¾“å…¥å›¾åƒä¸­çš„ä¸€åªå¤§è±¡ä½äºä¸­å¿ƒï¼Œä½†ä¹Ÿæœ‰ä¸¤åªæ–‘é©¬åœ¨æ°´å‘é™„è¿‘çš„åº•éƒ¨ã€‚
- en: '![ptml 0329](assets/ptml_0329.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0329](assets/ptml_0329.png)'
- en: Figure 3-29\. CLIP text saliency on text describing an elephant
  id: totrans-358
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-29\. CLIP å¯¹æè¿°å¤§è±¡çš„æ–‡æœ¬çš„æ˜¾è‘—æ€§åˆ†æã€‚
- en: '![ptml 0330](assets/ptml_0330.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0330](assets/ptml_0330.png)'
- en: Figure 3-30\. CLIP image and text saliency on the input image, containing both
    an elephant in the center but also a zebra in the corner
  id: totrans-360
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-30\. CLIP å›¾åƒå’Œæ–‡æœ¬æ˜¾è‘—æ€§åˆ†æç»“æœæ˜¾ç¤ºäº†è¾“å…¥å›¾åƒä¸­çš„ä¸€åªå¤§è±¡ä½äºä¸­å¿ƒï¼ŒåŒæ—¶è§’è½é‡Œè¿˜æœ‰ä¸€åªæ–‘é©¬ã€‚
- en: '![ptml 0331](assets/ptml_0331.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0331](assets/ptml_0331.png)'
- en: Figure 3-31\. CLIP saliency on text describing a dog breed
  id: totrans-362
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-31\. CLIP å¯¹æè¿°ç‹—å“ç§çš„æ–‡æœ¬çš„æ˜¾è‘—æ€§åˆ†æã€‚
- en: '![ptml 0332](assets/ptml_0332.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0332](assets/ptml_0332.png)'
- en: Figure 3-32\. CLIP saliency example on image containing both a dog and cat,
    depending on whether the input text specifically mentioned the dog or the cat
  id: totrans-364
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-32\. CLIP å¯¹åŒ…å«ç‹—å’ŒçŒ«çš„å›¾åƒçš„æ˜¾è‘—æ€§åˆ†æç¤ºä¾‹ï¼Œå…·ä½“å–å†³äºè¾“å…¥æ–‡æœ¬æ˜¯å¦æ˜ç¡®æåˆ°äº†ç‹—æˆ–çŒ«ã€‚
- en: '![ptml 0333](assets/ptml_0333.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0333](assets/ptml_0333.png)'
- en: Figure 3-33\. CLIP saliency on text describing a dog breed
  id: totrans-366
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-33\. CLIP å¯¹æè¿°ç‹—å“ç§çš„æ–‡æœ¬çš„æ˜¾è‘—æ€§åˆ†æã€‚
- en: '![ptml 0334](assets/ptml_0334.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0334](assets/ptml_0334.png)'
- en: Figure 3-34\. CLIP saliency example on image containing both a dog and cat,
    depending on whether the input text described the breed of the dog or the breed
    of the cat
  id: totrans-368
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-34\. CLIP å¯¹åŒ…å«ç‹—å’ŒçŒ«çš„å›¾åƒçš„æ˜¾è‘—æ€§åˆ†æç¤ºä¾‹ï¼Œå…·ä½“å–å†³äºè¾“å…¥æ–‡æœ¬æè¿°çš„æ˜¯ç‹—çš„å“ç§è¿˜æ˜¯çŒ«çš„å“ç§ã€‚
- en: '![ptml 0335](assets/ptml_0335.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0335](assets/ptml_0335.png)'
- en: Figure 3-35\. CLIP saliency on the text, showing higher attribution on the word
    astronaut
  id: totrans-370
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-35\. CLIP å¯¹æ–‡æœ¬çš„æ˜¾è‘—æ€§åˆ†æï¼Œæ˜¾ç¤ºäº†å¯¹å•è¯â€œå®‡èˆªå‘˜â€æ›´é«˜çš„å½’å› ã€‚
- en: '![ptml 0336](assets/ptml_0336.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0336](assets/ptml_0336.png)'
- en: Figure 3-36\. CLIP saliency on the input image emphasizing the astronautâ€™s face,
    followed by similar pairs of text/image saliency pairs that emphasize the flag
    in the picture and then the astronaut suit
  id: totrans-372
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-36\. CLIP å¯¹è¾“å…¥å›¾åƒçš„æ˜¾è‘—æ€§åˆ†æå¼ºè°ƒäº†å®‡èˆªå‘˜çš„é¢éƒ¨ï¼Œæ¥ç€æ˜¯ä¸€ç³»åˆ—ç±»ä¼¼çš„æ–‡æœ¬/å›¾åƒæ˜¾è‘—æ€§åˆ†æå¯¹ï¼Œå¼ºè°ƒäº†å›¾ç‰‡ä¸­çš„å›½æ——ï¼Œç„¶åæ˜¯å®‡èˆªæœã€‚
- en: The saliency maps make a convincing case that CLIP can correctly identify which
    part of the picture the text describes. This is even the case when the target
    class is in just a small portion of the image.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾è‘—æ€§åœ°å›¾æœ‰åŠ›åœ°è¯æ˜äº†CLIPå¯ä»¥æ­£ç¡®è¯†åˆ«æ–‡æœ¬æè¿°çš„å›¾ç‰‡éƒ¨åˆ†ã€‚å³ä½¿ç›®æ ‡ç±»åˆ«åªå å›¾åƒçš„ä¸€å°éƒ¨åˆ†ï¼Œä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: It may be tempting to look at these results and assume weâ€™ve created an ideal
    zero-shot object detection model. Still, CLIP is not perfect, and itâ€™s important
    to consider the limitations and edge cases.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½ä¼šè®©äººæƒŠè®¶çš„æ˜¯ï¼ŒCLIPä¸ä»…ä»æ–‡æœ¬ä¸­è¿›è¡Œäº†ç§¯æçš„å½’å› ï¼Œè€Œä¸”è¿˜ç»™è¿™ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹åˆ†é…äº†å¾ˆé«˜çš„ç›¸ä¼¼æ€§åˆ†æ•°ï¼ˆè§å›¾3-37å’Œ3-38ï¼‰ã€‚
- en: Consider the following example of an image of random static paired with a text
    description.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä»¥ä¸‹ä¾‹å­ï¼Œä¸€ä¸ªéšæœºé™æ€å›¾åƒä¸æ–‡æœ¬æè¿°é…å¯¹ã€‚
- en: '[PRE61]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: It might be surprising not only how much CLIP is making positive attributions
    from the text, but also how high of a similarity score it assigns to this image-text
    pair (see Figures [3-37](#clip-saliency-23) and [3-38](#clip-saliency-24)).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä»…CLIPä»æ–‡æœ¬ä¸­è¿›è¡Œäº†ç§¯æçš„å½’å› ï¼Œè€Œä¸”è¿˜ç»™è¿™ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹åˆ†é…äº†å¾ˆé«˜çš„ç›¸ä¼¼æ€§åˆ†æ•°ï¼ˆè§å›¾3-37å’Œ3-38ï¼‰ã€‚
- en: '![ptml 0337](assets/ptml_0337.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0337](assets/ptml_0337.png)'
- en: Figure 3-37\. CLIP similarity score between text seemingly describing a dog
    and a random noise image
  id: totrans-379
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-37\. CLIP å¯¹çœ‹ä¼¼æè¿°ç‹—çš„æ–‡æœ¬å’Œéšæœºå™ªå£°å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§åˆ†æ•°ã€‚
- en: '![ptml 0338](assets/ptml_0338.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0338](assets/ptml_0338.png)'
- en: Figure 3-38\. CLIP saliency map on noise that looks nothing like a dog to a
    human
  id: totrans-381
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 3-38\. CLIP å¯¹çœ‹èµ·æ¥ä¸ç‹—æ¯«ä¸ç›¸åƒçš„å™ªéŸ³çš„æ˜¾è‘—æ€§åˆ†æå›¾ã€‚
- en: For comparison, consider an input image of a dog thatâ€™s much clearer ([FigureÂ 3-40](#clip-saliency-22)).
    This input image gets a very high CLIP similarity score (`27.890625`) because
    of that similarity ([FigureÂ 3-39](#clip-saliency-21)). Still, thatâ€™s a lower CLIP
    similarity score than the random noise we previously fed into CLIP.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¯”è¾ƒï¼Œè€ƒè™‘ä¸€å¼ æ›´æ¸…æ™°çš„ç‹—çš„è¾“å…¥å›¾åƒï¼ˆè§å›¾3-40ï¼‰ã€‚ç”±äºè¿™ç§ç›¸ä¼¼æ€§ï¼Œè¿™ä¸ªè¾“å…¥å›¾åƒå¾—åˆ°äº†éå¸¸é«˜çš„CLIPç›¸ä¼¼æ€§åˆ†æ•°ï¼ˆ`27.890625`ï¼‰ï¼ˆè§å›¾3-39ï¼‰ã€‚ä½†æ˜¯ï¼Œè¿™ä¸ªCLIPç›¸ä¼¼æ€§åˆ†æ•°æ¯”æˆ‘ä»¬ä¹‹å‰è¾“å…¥çš„éšæœºå™ªå£°è¿˜è¦ä½ã€‚
- en: '![ptml 0339](assets/ptml_0339.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0339](assets/ptml_0339.png)'
- en: Figure 3-39\. CLIP similarity score on human-understandable image of a husky
  id: totrans-384
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾3-39\. CLIPåœ¨äººç±»å¯ç†è§£çš„å“ˆå£«å¥‡å›¾åƒä¸Šçš„ç›¸ä¼¼æ€§åˆ†æ•°
- en: Anyone whoâ€™s able to see the image in [FigureÂ 3-40](#clip-saliency-22) would
    agree that itâ€™s a dog, and the saliency map shows that CLIP seems to focus on
    the dogâ€™s face and mouth.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿçœ‹åˆ°[å›¾Â 3-40](#clip-saliency-22)çš„ä»»ä½•äººéƒ½ä¼šåŒæ„è¿™æ˜¯ä¸€åªç‹—ï¼Œæ˜¾è‘—æ€§å›¾æ˜¾ç¤ºCLIPä¼¼ä¹é›†ä¸­åœ¨ç‹—çš„é¢éƒ¨å’Œå˜´å·´ä¸Šã€‚
- en: '![ptml 0340](assets/ptml_0340.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0340](assets/ptml_0340.png)'
- en: Figure 3-40\. CLIP saliency map on human-understandable image of a husky
  id: totrans-387
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾3-40\. CLIPåœ¨äººç±»å¯ç†è§£çš„å“ˆå£«å¥‡å›¾åƒä¸Šçš„æ˜¾è‘—æ€§å›¾
- en: The focus of CLIP might be confusing, but if you look closely at [FigureÂ 3-37](#clip-saliency-23),
    CLIP focuses much more on the `image of` part than on the dog in the human-legible
    part. Some part of this image is associated with CLIPâ€™s understanding of `image
    of` enough to give it a higher CLIP similarity score than the seemingly perfect
    text-to-image match with the dog.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å…³æ³¨CLIPå¯èƒ½ä¼šè®©äººæ„Ÿåˆ°å›°æƒ‘ï¼Œä½†å¦‚æœä½ ä»”ç»†çœ‹[å›¾Â 3-37](#clip-saliency-23)ï¼ŒCLIPæ›´åŠ å…³æ³¨`image of`éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯äººç±»å¯è¯»éƒ¨åˆ†çš„ç‹—ã€‚è¿™å¼ å›¾ç‰‡çš„æŸäº›éƒ¨åˆ†ä¸CLIPå¯¹`image
    of`çš„ç†è§£è¶³å¤Ÿç›¸å…³ï¼Œä»è€Œä½¿å…¶è·å¾—æ¯”ä¸ç‹—çœ‹ä¼¼å®Œç¾çš„æ–‡æœ¬åˆ°å›¾åƒåŒ¹é…æ›´é«˜çš„CLIPç›¸ä¼¼æ€§åˆ†æ•°ã€‚
- en: Itâ€™s important to remember that while associating text with images is a task
    humans can do, CLIP does not see the world in the same way that biological brains
    do.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œè™½ç„¶å°†æ–‡æœ¬ä¸å›¾åƒå…³è”æ˜¯äººç±»èƒ½å¤Ÿå®Œæˆçš„ä»»åŠ¡ï¼Œä½†CLIPå¹¶ä¸ä»¥ä¸ç”Ÿç‰©å¤§è„‘ç›¸åŒçš„æ–¹å¼çœ‹å¾…ä¸–ç•Œã€‚
- en: Adversarial Counterfactual Examples
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹æŠ—åäº‹å®ä¾‹
- en: We recommend looking at [ChapterÂ 5](ch05.html#chapter5) for more information
    on adversarial examples, but we will briefly cover a specific example here.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å»ºè®®æŸ¥çœ‹[ç¬¬5ç« ](ch05.html#chapter5)ä»¥è·å–æœ‰å…³å¯¹æŠ—æ ·æœ¬çš„æ›´å¤šä¿¡æ¯ï¼Œä½†æˆ‘ä»¬å°†åœ¨è¿™é‡Œç®€è¦ä»‹ç»ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ã€‚
- en: '*Counterfactual explanations* (also known as *contrastive explanations*) are
    a powerful approach. The idea behind a counterfactual is to present a modified
    version of a data instance that leads to a different prediction. Usually, the
    counterfactual is the smallest of the input features that changes the model output
    to another (predefined) output.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '*åäº‹å®è§£é‡Š*ï¼ˆä¹Ÿç§°ä¸º*å¯¹æ¯”è§£é‡Š*ï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„æ–¹æ³•ã€‚åäº‹å®èƒŒåçš„æƒ³æ³•æ˜¯æå‡ºæ•°æ®å®ä¾‹çš„ä¿®æ”¹ç‰ˆæœ¬ï¼Œå¯¼è‡´ä¸åŒçš„é¢„æµ‹ç»“æœã€‚é€šå¸¸ï¼Œåäº‹å®æ˜¯æ”¹å˜æ¨¡å‹è¾“å‡ºåˆ°å¦ä¸€ä¸ªï¼ˆé¢„å®šä¹‰ï¼‰è¾“å‡ºçš„æœ€å°è¾“å…¥ç‰¹å¾ã€‚'
- en: Counterfactuals first emerged in the field of psychology in the 1970s.^([17](ch03.html#idm45621836221104))
    The paper â€œCounterfactual Explanations Without Opening the Black Boxâ€ introduced
    the idea of using them in machine learning.^([18](ch03.html#idm45621836219968))
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: åäº‹å®é¦–æ¬¡å‡ºç°åœ¨20ä¸–çºª70å¹´ä»£çš„å¿ƒç†å­¦é¢†åŸŸã€‚^([17](ch03.html#idm45621836221104)) â€œä¸æ‰“å¼€é»‘ç›’çš„åäº‹å®è§£é‡Šâ€ä¸€æ–‡ä»‹ç»äº†åœ¨æœºå™¨å­¦ä¹ ä¸­ä½¿ç”¨å®ƒä»¬çš„æ¦‚å¿µã€‚^([18](ch03.html#idm45621836219968))
- en: An adversarial example is an instance with small, intentional feature perturbations
    that cause a machine learning model to make a false prediction. When it comes
    to explainability and interpretability, adversarial examples can serve a similar
    role as counterfactual explanations. In fact, the processes for generating both
    are very similar to one another.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æŠ—æ€§ç¤ºä¾‹æ˜¯æŒ‡å…·æœ‰å°çš„ã€æœ‰æ„çš„ç‰¹å¾æ‰°åŠ¨çš„å®ä¾‹ï¼Œè¿™äº›æ‰°åŠ¨ä¼šå¯¼è‡´æœºå™¨å­¦ä¹ æ¨¡å‹åšå‡ºé”™è¯¯çš„é¢„æµ‹ã€‚åœ¨å¯è§£é‡Šæ€§å’Œè§£é‡Šæ€§æ–¹é¢ï¼Œå¯¹æŠ—æ€§ç¤ºä¾‹å¯ä»¥èµ·åˆ°ä¸åäº‹å®è§£é‡Šç±»ä¼¼çš„ä½œç”¨ã€‚äº‹å®ä¸Šï¼Œç”Ÿæˆä¸¤è€…çš„è¿‡ç¨‹éå¸¸ç›¸ä¼¼ã€‚
- en: "In both cases, you want to figure out <math alttext=\"x prime\"><mrow><mi>x</mi>\
    \ <mi>Ã¢</mi> <mi>\x80</mi> <mi>\x99</mi></mrow></math> , an adversarial example,\
    \ in an optimization problem:"
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæ‚¨éƒ½å¸Œæœ›æ‰¾åˆ°ä¸€ä¸ªå¯¹æŠ—æ€§ä¾‹å­ï¼Œå³ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ä¸­çš„å°å‹å¯¹æŠ—æ€§ç‰¹å¾æ‰°åŠ¨ï¼š
- en: <math alttext="argmin Underscript x prime Endscripts d left-parenthesis x comma
    x prime right-parenthesis" display="block"><mrow><munder><mrow><mtext>argmin</mtext></mrow>
    <msup><mi>x</mi> <mo>'</mo></msup></munder> <mi>d</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="argmin Underscript x prime Endscripts d left-parenthesis x comma
    x prime right-parenthesis" display="block"><mrow><munder><mrow><mtext>argmin</mtext></mrow>
    <msup><mi>x</mi> <mo>'</mo></msup></munder> <mi>d</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>
- en: "Here, feeding <math alttext=\"x prime\"><mrow><mi>x</mi> <mi>Ã¢</mi> <mi>\x80\
    </mi> <mi>\x99</mi></mrow></math> into your model ( <math alttext=\"f left-parenthesis\
    \ x prime right-parenthesis\"><mrow><mi>f</mi> <mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup>\
    \ <mo>)</mo></mrow></math> ) will lead to a predefined output <math alttext=\"\
    c\"><mi>c</mi></math> . In both cases, when youâ€™re explaining a model (the counterfactual)\
    \ or attacking a model (the adversarial sample), you want to minimize the changes\
    \ to the input data."
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: "åœ¨è¿™é‡Œï¼Œå°†<math alttext=\"x prime\"><mrow><mi>x</mi> <mi>Ã¢</mi> <mi>\x80</mi> <mi>\x99\
    </mi></mrow></math>è¾“å…¥åˆ°æ‚¨çš„æ¨¡å‹ï¼ˆ<math alttext=\"f left-parenthesis x prime right-parenthesis\"\
    ><mrow><mi>f</mi> <mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></math>ï¼‰å°†å¯¼è‡´é¢„å®šä¹‰çš„è¾“å‡º<math\
    \ alttext=\"c\"><mi>c</mi></math>ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œå½“æ‚¨è§£é‡Šä¸€ä¸ªæ¨¡å‹ï¼ˆåäº‹å®ï¼‰æˆ–æ”»å‡»ä¸€ä¸ªæ¨¡å‹ï¼ˆå¯¹æŠ—æ€§æ ·æœ¬ï¼‰æ—¶ï¼Œæ‚¨éƒ½å¸Œæœ›æœ€å°åŒ–å¯¹è¾“å…¥æ•°æ®çš„æ›´æ”¹ã€‚"
- en: But beyond this general mathematical form, how do you calculate the adversarial
    example practically? The method depends on whether you have access to the model
    internals (the white-box approaches) or not (the black-box approaches).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é™¤äº†è¿™ç§ä¸€èˆ¬çš„æ•°å­¦å½¢å¼ä¹‹å¤–ï¼Œä½ å¦‚ä½•å®é™…è®¡ç®—å¯¹æŠ—æ ·æœ¬å‘¢ï¼Ÿè¿™ç§æ–¹æ³•å–å†³äºä½ æ˜¯å¦èƒ½è®¿é—®æ¨¡å‹çš„å†…éƒ¨ï¼ˆç™½ç›’æ–¹æ³•ï¼‰æˆ–ä¸èƒ½ï¼ˆé»‘ç›’æ–¹æ³•ï¼‰ã€‚
- en: Overcome the Limitations of Interpretability with a Security Mindset
  id: totrans-399
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…‹æœå¯è§£é‡Šæ€§çš„å±€é™æ€§éœ€è¦å®‰å…¨æ„è¯†ã€‚
- en: Weâ€™ve discussed a lot of packages and pointed out the limitations of interpretability
    as a field. So what is one supposed to do if these tools apparently tell us so
    little? Ultimately, you may never be able to understand every single aspect of
    a sufficiently complex model. The next best thing is to have a â€œsecurity mindsetâ€
    to overcome the limitations of interpretability.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»è®¨è®ºäº†è®¸å¤šåŒ…å¹¶æŒ‡å‡ºäº†è§£é‡Šæ€§ä½œä¸ºä¸€ä¸ªé¢†åŸŸçš„å±€é™æ€§ã€‚é‚£ä¹ˆï¼Œå¦‚æœè¿™äº›å·¥å…·æ˜¾ç„¶å‘Šè¯‰æˆ‘ä»¬å¦‚æ­¤å°‘ï¼Œæˆ‘ä»¬åº”è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿæœ€ç»ˆï¼Œä½ å¯èƒ½æ°¸è¿œæ— æ³•ç†è§£è¶³å¤Ÿå¤æ‚æ¨¡å‹çš„æ¯ä¸€ä¸ªæ–¹é¢ã€‚ä¸‹ä¸€æ­¥æœ€å¥½çš„æ–¹æ³•æ˜¯å…·å¤‡â€œå®‰å…¨æ„è¯†â€ï¼Œä»¥å…‹æœå¯è§£é‡Šæ€§çš„å±€é™æ€§ã€‚
- en: What is a security mindset? Itâ€™s the ability to spot potential or real flaws
    in the integrity of a program, system of programs, organization, or even person
    or group of people. An attacker might adopt a security mindset to exploit weaknesses,
    or a security practitioner might adopt it to better defend the system and patch
    those weaknesses. An individual can learn to have a security mindset. A security
    mindset can exist as intuition. It can even emerge as a culture resulting from
    the guidelines and procedures of a security-conscious organization. In a machine
    learning context, itâ€™s an ability to challenge assumptions about the behaviors
    and/or safety of your model.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯å®‰å…¨æ„è¯†ï¼Ÿè¿™æ˜¯å‘ç°ç¨‹åºã€ç³»ç»Ÿé›†æˆã€ç»„ç»‡ç”šè‡³ä¸ªäººæˆ–ä¸€ç¾¤äººçš„å®Œæ•´æ€§ä¸­æ½œåœ¨æˆ–å®é™…ç¼ºé™·çš„èƒ½åŠ›ã€‚æ”»å‡»è€…å¯èƒ½ä¼šé‡‡ç”¨å®‰å…¨æ„è¯†æ¥åˆ©ç”¨å¼±ç‚¹ï¼Œè€Œå®‰å…¨ä»ä¸šè€…å¯èƒ½ä¼šé‡‡ç”¨å®ƒæ¥æ›´å¥½åœ°ä¿å«ç³»ç»Ÿå¹¶ä¿®è¡¥è¿™äº›å¼±ç‚¹ã€‚ä¸ªäººå¯ä»¥å­¦ä¼šå…·å¤‡å®‰å…¨æ„è¯†ã€‚å®‰å…¨æ„è¯†å¯ä»¥å­˜åœ¨äºç›´è§‰ä¸­ã€‚å®ƒç”šè‡³å¯ä»¥ä½œä¸ºå®‰å…¨æ„è¯†çš„æ–‡åŒ–å‡ºç°ï¼Œç”±å®‰å…¨æ„è¯†ç»„ç»‡çš„æŒ‡å¯¼æ–¹é’ˆå’Œç¨‹åºå¼•å‘ã€‚åœ¨æœºå™¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œå®ƒæ˜¯æŒ‘æˆ˜æ¨¡å‹è¡Œä¸ºå’Œ/æˆ–å®‰å…¨æ€§å‡è®¾çš„èƒ½åŠ›ã€‚
- en: For example, in the previous tutorials, weâ€™ve given examples of using large
    language models for tasks like classification. This seems like a straightforward
    task, until you start questioning the assumptions behind the setup. For example,
    what if we used abstract labels like `A` and `B` instead of concrete labels like
    `POSITIVE` and `NEGATIVE`? If weâ€™re using full words, does the spelling or capitalization
    matter? [In at least one case](https://oreil.ly/ItNoi), the performance of a large
    language model on the SST evaluation benchmark jumped from 53% to 69% after the
    researchers just changed the output label â€œpositiveâ€ to â€œPositive.â€ In the case
    of [sentiment analysis and COVID testing](https://oreil.ly/CtCYu) (where testing
    negative for COVID should be seen as a good thing), the underlying meaning behind
    the labels â€œpositiveâ€ and â€œnegativeâ€ changed.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨ä¹‹å‰çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸¾äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œåˆ†ç±»ç­‰ä»»åŠ¡çš„ä¾‹å­ã€‚è¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªç›´æ¥çš„ä»»åŠ¡ï¼Œç›´åˆ°ä½ å¼€å§‹è´¨ç–‘è®¾ç½®èƒŒåçš„å‡è®¾ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨æŠ½è±¡æ ‡ç­¾åƒ`A`å’Œ`B`è€Œä¸æ˜¯åƒ`POSITIVE`å’Œ`NEGATIVE`è¿™æ ·çš„å…·ä½“æ ‡ç­¾ä¼šæ€ä¹ˆæ ·ï¼Ÿå¦‚æœæˆ‘ä»¬ä½¿ç”¨å®Œæ•´çš„å•è¯ï¼Œæ‹¼å†™æˆ–å¤§å†™å­—æ¯æ˜¯å¦é‡è¦ï¼Ÿ[è‡³å°‘åœ¨ä¸€ä¸ªæ¡ˆä¾‹ä¸­](https://oreil.ly/ItNoi)ï¼Œç ”ç©¶äººå‘˜ä»…ä»…å°†è¾“å‡ºæ ‡ç­¾â€œpositiveâ€æ”¹ä¸ºâ€œPositiveâ€ï¼Œä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹åœ¨SSTè¯„ä¼°åŸºå‡†ä¸Šçš„æ€§èƒ½ä»53%æå‡åˆ°äº†69%ã€‚åœ¨[æƒ…æ„Ÿåˆ†æå’ŒCOVIDæµ‹è¯•](https://oreil.ly/CtCYu)çš„æƒ…å†µä¸‹ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒCOVIDæµ‹è¯•ä¸ºé˜´æ€§åº”è¢«è§†ä¸ºä¸€ä»¶å¥½äº‹ï¼‰ï¼Œæ ‡ç­¾â€œpositiveâ€å’Œâ€œnegativeâ€èƒŒåçš„å«ä¹‰å‘ç”Ÿäº†å˜åŒ–ã€‚
- en: 'Part of the â€œsecurity mindsetâ€ also means recognizing that anthropomorphizing
    your AI system is incredibly dangerous. Consider the case of a Google engineer
    who [struck up a conversation with Googleâ€™s LaMDA conversational model, concluded
    it was sentient, kicked up a storm in the company, and got suspended from their
    job](https://oreil.ly/CtCYu). If one reads the publicly available snippets of
    the conversation, one could conclude it was a conversation between two people.
    However, there are two things that seemed absent from the conversation:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: â€œå®‰å…¨æ„è¯†â€çš„ä¸€éƒ¨åˆ†ä¹Ÿæ„å‘³ç€è¦è®¤è¯†åˆ°å°†ä½ çš„AIç³»ç»Ÿæ‹ŸäººåŒ–æ˜¯æå…¶å±é™©çš„ã€‚è€ƒè™‘ä¸€ä¸ªä¸è°·æ­Œçš„LaMDAå¯¹è¯æ¨¡å‹å¼€å§‹å¯¹è¯ã€å¾—å‡ºå…¶æœ‰æ„è¯†çš„ç»“è®ºçš„è°·æ­Œå·¥ç¨‹å¸ˆçš„æ¡ˆä¾‹ï¼Œå¹¶å› æ­¤åœ¨å…¬å¸å†…æ€èµ·è½©ç„¶å¤§æ³¢å¹¶è¢«åœèŒçš„æƒ…å†µã€‚å¦‚æœæœ‰äººé˜…è¯»è¿™æ®µå…¬å¼€å¯ç”¨çš„å¯¹è¯ç‰‡æ®µï¼Œå¯èƒ½ä¼šå¾—å‡ºè¿™æ˜¯ä¸¤ä¸ªäººä¹‹é—´çš„å¯¹è¯çš„ç»“è®ºã€‚ç„¶è€Œï¼Œä»å¯¹è¯ä¸­ç¼ºå°‘ä¸¤ä»¶äº‹æƒ…ï¼š
- en: The engineer in question aggressively doubting the sentience of the chatbot
    and seeing the result
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›¸å…³å·¥ç¨‹å¸ˆç§¯ææ€€ç–‘èŠå¤©æœºå™¨äººçš„æ™ºèƒ½æ€§å¹¶çœ‹åˆ°ç»“æœ
- en: All the other conversations that would have been less coherent
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰€æœ‰å…¶ä»–å¯èƒ½ä¸é‚£ä¹ˆè¿è´¯çš„å¯¹è¯
- en: The latter implies thereâ€™s a selection bias behind these claims of sentience.
    As for the former, considering counterfactuals is crucial when evaluating language
    models. After all, the chatbot is a Transformer model that was likely trained
    on datasets of human queries and responses in a chat room. The main goal of this
    training was to predict the most likely appropriate output response given a query.
    As such, this evidence of the chatbotâ€™s â€œsentienceâ€ is indistinguishable from
    a chatbot guessing the next likely sentences in a science fiction story about
    a sentient AI. In fact, further research has demonstrated that what most people
    think of as â€œhuman-likeâ€ in the responses from a chatbot is often simply the use
    of the first person.^([19](ch03.html#idm45621836184512))
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: åè€…æ„å‘³ç€è¿™äº›å£°ç§°æœ‰çŸ¥è§‰èƒ½åŠ›èƒŒåå­˜åœ¨é€‰æ‹©åå·®ã€‚è‡³äºå‰è€…ï¼Œåœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹æ—¶è€ƒè™‘å¯¹äº‹å®çš„åäº‹å®æ˜¯è‡³å…³é‡è¦çš„ã€‚æ¯•ç«Ÿï¼ŒèŠå¤©æœºå™¨äººæ˜¯ä¸€ä¸ªTransformeræ¨¡å‹ï¼Œå¾ˆå¯èƒ½æ˜¯åœ¨èŠå¤©å®¤ä¸­çš„äººç±»æŸ¥è¯¢å’Œå“åº”æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚è¿™ç§è®­ç»ƒçš„ä¸»è¦ç›®çš„æ˜¯æ ¹æ®æŸ¥è¯¢é¢„æµ‹æœ€å¯èƒ½çš„é€‚å½“è¾“å‡ºå“åº”ã€‚å› æ­¤ï¼ŒèŠå¤©æœºå™¨äººâ€œæœ‰çŸ¥è§‰â€çš„è¯æ®ä¸ä¸€ä¸ªé¢„æµ‹ä¸‹ä¸€ä¸ªå¯èƒ½å¥å­çš„èŠå¤©å®¤ç§‘å¹»æ•…äº‹ä¸­çš„èªæ˜AIå‡ ä¹æ˜¯ä¸€æ ·çš„ã€‚äº‹å®ä¸Šï¼Œè¿›ä¸€æ­¥çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å¤šæ•°äººè®¤ä¸ºèŠå¤©æœºå™¨äººå“åº”ä¸­çš„â€œç±»äººâ€ç‰¹å¾é€šå¸¸ä»…ä»…æ˜¯ç¬¬ä¸€äººç§°çš„ä½¿ç”¨ã€‚^([19](ch03.html#idm45621836184512))
- en: Limitations and Pitfalls of Explainable and Interpretable Methods
  id: totrans-407
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯è§£é‡Šæ€§å’Œè§£é‡Šæ€§æ–¹æ³•çš„å±€é™æ€§å’Œç¼ºé™·
- en: Before diving into the exact methods for interpreting and explaining models,
    letâ€™s take a look at some of the pitfalls of these methods.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥ç ”ç©¶è§£é‡Šå’Œè¯´æ˜æ¨¡å‹çš„ç¡®åˆ‡æ–¹æ³•ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹è¿™äº›æ–¹æ³•çš„ä¸€äº›ç¼ºé™·ã€‚
- en: First off, if you need to make high-stakes decisions, make sure to use inherently
    interpretable models. These are models such as decision trees that are more readily
    converted to output explanations (see [â€œDecision treeâ€](#decision-tree-ch3-sect)
    for more details).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå¦‚æœæ‚¨éœ€è¦åšå‡ºé«˜é£é™©çš„å†³ç­–ï¼Œè¯·ç¡®ä¿ä½¿ç”¨æœ¬è´¨ä¸Šå¯è§£é‡Šçš„æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹ä¾‹å¦‚å†³ç­–æ ‘æ›´å®¹æ˜“è½¬æ¢ä¸ºè¾“å‡ºè§£é‡Šï¼ˆè¯¦è§[â€œå†³ç­–æ ‘â€](#decision-tree-ch3-sect)ï¼‰ã€‚
- en: Before choosing a method, you need to be absolutely clear about what you want
    out of it. Are you trying to understand the nature of the data procurement process?
    How a decision was made? How the model works on a fundamental level? Some tools
    might be appropriate for some of these goals but not others.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é€‰æ‹©æ–¹æ³•ä¹‹å‰ï¼Œæ‚¨éœ€è¦éå¸¸æ¸…æ¥šæ‚¨å¸Œæœ›ä»ä¸­å¾—åˆ°ä»€ä¹ˆã€‚æ‚¨æ˜¯æƒ³ç†è§£æ•°æ®é‡‡è´­è¿‡ç¨‹çš„æœ¬è´¨å—ï¼Ÿå†³ç­–æ˜¯å¦‚ä½•åšå‡ºçš„ï¼Ÿæ¨¡å‹åœ¨åŸºæœ¬å±‚é¢ä¸Šæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿæœ‰äº›å·¥å…·å¯èƒ½å¯¹å…¶ä¸­æŸäº›ç›®æ ‡é€‚åˆï¼Œä½†å¯¹å…¶ä»–ç›®æ ‡åˆ™ä¸é€‚åˆã€‚
- en: If your goal is to make sense of the data generation process, this is only possible
    if you know that your model already generalizes well to unseen data.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„ç›®æ ‡æ˜¯ç†è§£æ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼Œé‚£ä¹ˆåªæœ‰åœ¨æ‚¨çŸ¥é“æ‚¨çš„æ¨¡å‹å·²ç»å¾ˆå¥½åœ°æ³›åŒ–åˆ°æœªè§æ•°æ®æ—¶æ‰å¯èƒ½å®ç°ã€‚
- en: Decision interpretability can be misleading. It highlights things like correlations,
    but doesnâ€™t go into the level of causal detail that causal inference does (see
    [ChapterÂ 7](ch07.html#chapter7)). Remember that correlation does not (always)
    imply causation.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–çš„å¯è§£é‡Šæ€§å¯èƒ½ä¼šè¯¯å¯¼ã€‚å®ƒçªå‡ºæ˜¾ç¤ºè¯¸å¦‚ç›¸å…³æ€§ä¹‹ç±»çš„äº‹ç‰©ï¼Œä½†å¹¶ä¸æ·±å…¥åˆ°å› æœæ¨æ–­æ‰€æ¶‰åŠçš„è¯¦ç»†å±‚æ¬¡ï¼ˆè§[ç¬¬7ç« ](ch07.html#chapter7)ï¼‰ã€‚è¯·è®°ä½ï¼Œç›¸å…³æ€§å¹¶ä¸æ€»æ˜¯æ„å‘³ç€å› æœå…³ç³»ã€‚
- en: Warning
  id: totrans-413
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: è­¦å‘Š
- en: Spurious correlations can result in inaccurate interpretations even with advanced
    interpretability methods like saliency methods and attention-based methods.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: è™šå‡çš„ç›¸å…³æ€§å¯èƒ½ä¼šå¯¼è‡´å³ä½¿ä½¿ç”¨å…ˆè¿›çš„å¯è§£é‡Šæ€§æ–¹æ³•å¦‚æ˜¾è‘—æ€§æ–¹æ³•å’ŒåŸºäºæ³¨æ„åŠ›çš„æ–¹æ³•ä¹Ÿä¼šå‡ºç°ä¸å‡†ç¡®çš„è§£é‡Šã€‚
- en: Tools such as feature importance usually estimate mean values, but one should
    beware the error bars on those means and take stock of the confidence intervals.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¸å¦‚ç‰¹å¾é‡è¦æ€§ä¹‹ç±»çš„å·¥å…·é€šå¸¸ä¼šä¼°è®¡å¹³å‡å€¼ï¼Œä½†æ˜¯åº”æ³¨æ„è¿™äº›å¹³å‡å€¼çš„è¯¯å·®èŒƒå›´ï¼Œå¹¶è€ƒè™‘ç½®ä¿¡åŒºé—´ã€‚
- en: 'A lot of machine learning involves working with extremely high-dimensional
    spaces. Thereâ€™s no way around it: high-dimensional data and feature spaces are
    hard to make sense of without grouping the data or features together first.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¤šæœºå™¨å­¦ä¹ æ¶‰åŠå¤„ç†æé«˜ç»´åº¦çš„ç©ºé—´ã€‚é«˜ç»´åº¦çš„æ•°æ®å’Œç‰¹å¾ç©ºé—´è¦æƒ³åœ¨æœªè¿›è¡Œæ•°æ®æˆ–ç‰¹å¾åˆ†ç»„çš„æƒ…å†µä¸‹ç†è§£æ˜¯å›°éš¾çš„ã€‚
- en: Even if you do find important features in those matrices, remember that this
    does not imply causality (weâ€™ve said this before and weâ€™ll say it again).
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æ‚¨åœ¨è¿™äº›çŸ©é˜µä¸­æ‰¾åˆ°äº†é‡è¦çš„ç‰¹å¾ï¼Œä¹Ÿè¯·è®°ä½è¿™å¹¶ä¸æ„å‘³ç€å› æœå…³ç³»ï¼ˆæˆ‘ä»¬ä¹‹å‰å·²ç»è¯´è¿‡è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†å†æ¬¡è¯´ä¸€éï¼‰ã€‚
- en: Risks of Deceptive Interpretability
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[è¯¯å¯¼æ€§å¯è§£é‡Šæ€§çš„é£é™©](https://wiki.example.org/deceptive_interpretability_risks)'
- en: Even if youâ€™re not anthropomorphizing your model or ML pipeline, you should
    always be wary of 100% believing your interpretability or explainability method.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æ‚¨ä¸å°†æ¨¡å‹æˆ–MLæµæ°´çº¿æ‹ŸäººåŒ–ï¼Œæ‚¨ä¹Ÿåº”å§‹ç»ˆè­¦æƒ•å¯¹å¯è§£é‡Šæ€§æˆ–è§£é‡Šæ€§æ–¹æ³•çš„100%ä¿¡ä»»ã€‚
- en: One of the big concerns in the AI safety field, which was a hypothetical scenario
    until it was recently realized, is a â€œdeceptively misaligned mesa-optimizer.â€^([20](ch03.html#idm45621836170272))
    In short, a machine learning model is trained in an environment in the hopes that
    it will behave similarly in the real world. To make sure its alignment in the
    test environment is the same as its alignment in the outside world, its creators
    resort to interpretability methods. However, it turns out the interpretability
    method itself shows one pattern to the human engineer, while corresponding to
    an unwanted behavior in the real world. This is one of those scenarios that was
    often discussed in the same breath as far-future AGI takeovers, until it was demonstrated
    in real life.^([21](ch03.html#idm45621836168112))
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨AIå®‰å…¨é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§å…³æ³¨ç‚¹æ˜¯â€œæ¬ºéª—æ€§ä¸å¯¹é½çš„Mesa-ä¼˜åŒ–å™¨â€ï¼Œç›´åˆ°æœ€è¿‘æ‰æˆä¸ºä¸€ä¸ªå‡è®¾æƒ…æ™¯ã€‚ç®€è€Œè¨€ä¹‹ï¼Œä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ä¸€ä¸ªç¯å¢ƒä¸­è®­ç»ƒï¼Œå¸Œæœ›å®ƒåœ¨ç°å®ä¸–ç•Œä¸­è¡¨ç°ç›¸ä¼¼ã€‚ä¸ºäº†ç¡®ä¿å…¶åœ¨æµ‹è¯•ç¯å¢ƒä¸­çš„å¯¹é½æ€§ä¸å…¶åœ¨å¤–éƒ¨ä¸–ç•Œä¸­çš„å¯¹é½æ€§ç›¸åŒï¼Œå…¶åˆ›å»ºè€…ä»¬é‡‡ç”¨äº†è§£é‡Šæ€§æ–¹æ³•ã€‚ç„¶è€Œï¼Œäº‹å®è¯æ˜ï¼Œè§£é‡Šæ€§æ–¹æ³•æœ¬èº«å‘äººç±»å·¥ç¨‹å¸ˆå±•ç¤ºäº†ä¸€ç§æ¨¡å¼ï¼Œè€Œåœ¨ç°å®ä¸–ç•Œä¸­å´å¯¹åº”ç€ä¸€ç§ä¸å¸Œæœ›çš„è¡Œä¸ºã€‚è¿™æ˜¯è¿‡å»ç»å¸¸ä¸è¿œæœŸAGIæ¥ç®¡è®¨è®ºåœ¨ä¸€èµ·çš„æƒ…æ™¯ä¹‹ä¸€ï¼Œç›´åˆ°å®ƒåœ¨ç°å®ç”Ÿæ´»ä¸­å¾—åˆ°è¯æ˜ã€‚^([21](ch03.html#idm45621836168112))
- en: While weâ€™ve mainly avoided the topic of reinforcement learning in this chapter,
    a lot of the computer vision interpretability tools weâ€™ve described previously
    apply here. In this case, the authors of [â€œGoal Misgeneralization in Deep Reinforcement
    Learningâ€](https://arxiv.org/abs/2105.14111v6) had a very simple reinforcement
    learning environment called CoinRun. In short, they demonstrated an RL agent that
    appeared to have very clear goals (namely, reaching the coin at the end of the
    level). However, when put in different environments, it instead was just going
    to the end of the level. This is obviously a much lower-stakes application of
    an AI than putting that model in a self-driving car, but it should still be a
    reminder to check all of your assumptions about an interpretability method.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶åœ¨æœ¬ç« ä¸­æˆ‘ä»¬ä¸»è¦é¿å…äº†å¼ºåŒ–å­¦ä¹ çš„è¯é¢˜ï¼Œä½†æˆ‘ä»¬ä¹‹å‰æè¿°çš„è®¸å¤šè®¡ç®—æœºè§†è§‰è§£é‡Šå·¥å…·åœ¨è¿™é‡ŒåŒæ ·é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œã€Šæ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„ç›®æ ‡è¯¯æ¦‚åŒ–ã€‹çš„ä½œè€…ä»¬æ‹¥æœ‰ä¸€ä¸ªéå¸¸ç®€å•çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒç§°ä¸ºCoinRunã€‚ç®€è€Œè¨€ä¹‹ï¼Œä»–ä»¬å±•ç¤ºäº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ä»£ç†äººï¼Œçœ‹ä¼¼æœ‰éå¸¸æ˜ç¡®çš„ç›®æ ‡ï¼ˆå³åˆ°è¾¾å…³å¡æœ«å°¾çš„ç¡¬å¸ï¼‰ã€‚ç„¶è€Œï¼Œå½“ç½®äºä¸åŒçš„ç¯å¢ƒä¸­æ—¶ï¼Œå®ƒå®é™…ä¸Šåªæ˜¯åˆ°è¾¾äº†å…³å¡çš„æœ«å°¾ã€‚æ˜¾ç„¶ï¼Œè¿™æ¯”å°†è¯¥æ¨¡å‹æ”¾å…¥è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­è¦ä½é£é™©å¾—å¤šï¼Œä½†è¿™ä»ç„¶åº”è¯¥æé†’æ‚¨æ£€æŸ¥æœ‰å…³è§£é‡Šæ€§æ–¹æ³•çš„æ‰€æœ‰å‡è®¾ã€‚
- en: If you really want a framework for how to think about the ML model youâ€™re evaluating,
    at best itâ€™s a LARPer acting out a role specified by humans without any true experience
    of the real world, and at worst itâ€™s a sociopath focusing on achieving its specified
    objective function regardless of how much that goal clashes with the wants and
    needs of the humans around it.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çœŸçš„æƒ³è¦ä¸€ä¸ªæ€è€ƒè¯„ä¼°ä¸­çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¡†æ¶ï¼Œæœ€å¥½å°†å…¶è§†ä¸ºä¸€ç§è§’è‰²æ‰®æ¼”è€…ï¼ŒæŒ‰ç…§äººç±»æŒ‡å®šçš„è§’è‰²è¡Œäº‹ï¼Œæ²¡æœ‰çœŸæ­£ä½“éªŒåˆ°ç°å®ä¸–ç•Œçš„ç»å†ï¼›æœ€ç³Ÿç³•çš„æƒ…å†µæ˜¯ï¼Œå®ƒæ˜¯ä¸€ä¸ªå…³æ³¨å®ç°å…¶æŒ‡å®šç›®æ ‡å‡½æ•°çš„ç¤¾ä¼šç—…æ€ï¼Œè€Œä¸ç®¡è¿™ä¸ªç›®æ ‡ä¸å…¶å‘¨å›´çš„äººç±»çš„æƒ³æ³•å’Œéœ€æ±‚æœ‰å¤šå¤§å†²çªã€‚
- en: Of course, even if youâ€™re incredibly mindful of every single parameter of your
    model, looking at the model alone is not enough. In the next chapter, we explore
    the various pitfalls involved in acquiring the training data that informs your
    machine learning model or pipelineâ€™s representation of the world.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå³ä½¿æ‚¨éå¸¸æ³¨æ„æ¨¡å‹çš„æ¯ä¸€ä¸ªå‚æ•°ï¼Œä»…ä»…çœ‹æ¨¡å‹æœ¬èº«æ˜¯ä¸å¤Ÿçš„ã€‚åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨è·å–è®­ç»ƒæ•°æ®çš„å„ç§é™·é˜±ï¼Œè¿™äº›æ•°æ®å†³å®šäº†æ‚¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹æˆ–æµç¨‹å¯¹ä¸–ç•Œçš„è¡¨ç¤ºã€‚
- en: Conclusion
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this chapter, you learned about the tools and techniques that help explain
    the predictions of an ML model. To that end, you need to choose a proper explainability
    technique (e.g., global or local; inherently explainable model or post hoc explanations),
    consider possible interactions with other aspects of trust (such as privacy),
    and be mindful of limitations of such methods.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæ‚¨äº†è§£äº†å¸®åŠ©è§£é‡Šæœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹çš„å·¥å…·å’ŒæŠ€æœ¯ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦é€‰æ‹©é€‚å½“çš„è§£é‡ŠæŠ€æœ¯ï¼ˆä¾‹å¦‚å…¨å±€æˆ–å±€éƒ¨ï¼›å›ºæœ‰å¯è§£é‡Šæ¨¡å‹æˆ–äº‹åè§£é‡Šï¼‰ï¼Œè€ƒè™‘ä¸ä¿¡ä»»çš„å…¶ä»–æ–¹é¢ï¼ˆå¦‚éšç§ï¼‰çš„å¯èƒ½äº¤äº’ä½œç”¨ï¼Œå¹¶æ³¨æ„è¿™äº›æ–¹æ³•çš„é™åˆ¶ã€‚
- en: '^([1](ch03.html#idm45621854762112-marker)) Tim Miller, [â€œExplanation in Artificial
    Intelligence: Insights from the Social Sciencesâ€](https://oreil.ly/teIWN), *Artificial
    Intelligence* 267 (2019): 1â€“38.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#idm45621854762112-marker)) Tim Millerï¼Œã€Šäººå·¥æ™ºèƒ½ä¸­çš„è§£é‡Šï¼šç¤¾ä¼šç§‘å­¦çš„å¯ç¤ºã€‹ï¼Œ*äººå·¥æ™ºèƒ½*
    267ï¼ˆ2019ï¼‰ï¼š1â€“38ã€‚
- en: ^([2](ch03.html#idm45621854760192-marker)) Been Kim et al., [â€œExamples Are Not
    Enough, Learn To Criticize! Criticism for Interpretabilityâ€](https://oreil.ly/deDPo),
    *Advances in Neural Information Processing Systems* 29 (2016).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.html#idm45621854760192-marker)) Been Kim ç­‰äººçš„æ–‡ç« [â€œä¾‹å­ä¸è¶³ä»¥ï¼Œå­¦ä¼šæ‰¹åˆ¤ï¼å¯è§£é‡Šæ€§çš„æ‰¹è¯„â€](https://oreil.ly/deDPo)ï¼Œã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ç¬¬29å·ï¼ˆ2016å¹´ï¼‰ã€‚
- en: ^([3](ch03.html#idm45621854755760-marker)) See this survey paper on [evaluation
    of XAI](https://arxiv.org/abs/2201.08164) and the [corresponding website](https://oreil.ly/FQSZe)
    with a curated categorization of XAI papers.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.html#idm45621854755760-marker)) è¯¦è§è¿™ç¯‡å…³äº[XAIè¯„ä¼°](https://arxiv.org/abs/2201.08164)çš„ç»¼è¿°è®ºæ–‡å’Œ[ç›¸å…³ç½‘ç«™](https://oreil.ly/FQSZe)ï¼Œæä¾›äº†XAIè®ºæ–‡çš„ç²¾é€‰åˆ†ç±»ã€‚
- en: ^([4](ch03.html#idm45621854753488-marker)) Dr. Matt Turek, [â€œExplainable Artificial
    Intelligence (XAI)â€](https://oreil.ly/6wkHq), *Defense Advanced Research Projects
    Agency (DARPA)*, 2016.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch03.html#idm45621854753488-marker)) Dr. Matt Turek çš„æ–‡ç« [â€œå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰â€](https://oreil.ly/6wkHq)ï¼Œå‘è¡¨äºå›½é˜²é«˜çº§ç ”ç©¶è®¡åˆ’å±€ï¼ˆDARPAï¼‰ï¼Œ2016å¹´ã€‚
- en: ^([5](ch03.html#idm45621854740496-marker)) Finale Doshi-Velez and Been Kim,
    [â€œTowards a Rigorous Science of Interpretable Machine Learningâ€](https://arxiv.org/abs/1702.08608),
    *arXiv preprint* (2017).
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch03.html#idm45621854740496-marker)) Finale Doshi-Velez å’Œ Been Kim çš„æ–‡ç« [â€œèµ°å‘å¯è§£é‡Šæœºå™¨å­¦ä¹ çš„ä¸¥æ ¼ç§‘å­¦â€](https://arxiv.org/abs/1702.08608)ï¼ŒarXivé¢„å°æœ¬ï¼ˆ2017å¹´ï¼‰ã€‚
- en: ^([6](ch03.html#idm45621854738064-marker)) Nirmal Sobha Kartha et al., [â€œWhy
    Are You Weird? Infusing Interpretability in Isolation Forest for Anomaly Detectionâ€](https://arxiv.org/abs/2112.06858),
    *arXiv preprint* (2021).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch03.html#idm45621854738064-marker)) Nirmal Sobha Kartha ç­‰äººçš„æ–‡ç« [â€œä¸ºä»€ä¹ˆä½ å¦‚æ­¤å¥‡æ€ªï¼Ÿæ³¨å…¥å­¤ç«‹æ£®æ—çš„å¯è§£é‡Šæ€§ç”¨äºå¼‚å¸¸æ£€æµ‹â€](https://arxiv.org/abs/2112.06858)ï¼ŒarXivé¢„å°æœ¬ï¼ˆ2021å¹´ï¼‰ã€‚
- en: ^([7](ch03.html#idm45621854718720-marker)) Doshi-Velez and Kim, â€œTowards a Rigorous
    Science of Interpretable Machine Learning.â€
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch03.html#idm45621854718720-marker)) Doshi-Velez å’Œ Kim çš„æ–‡ç« [â€œèµ°å‘å¯è§£é‡Šæœºå™¨å­¦ä¹ çš„ä¸¥æ ¼ç§‘å­¦â€]ã€‚
- en: ^([8](ch03.html#idm45621854672480-marker)) The *New York Times* judged [GPT-3â€™s
    ability to write original prose](https://oreil.ly/l5Xdm) as having fluency comparable
    to human levels.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch03.html#idm45621854672480-marker)) *çº½çº¦æ—¶æŠ¥* è¯„ä»·[GPT-3çš„å†™ä½œåŸåˆ›æ€§](https://oreil.ly/l5Xdm)è¾¾åˆ°äº†ä¸äººç±»æ°´å¹³ç›¸åª²ç¾çš„æµç•…ç¨‹åº¦ã€‚
- en: ^([9](ch03.html#idm45621854668624-marker)) Also, [Microsoft announced on September
    22, 2020](https://oreil.ly/gjCEN), that it had licensed â€œexclusiveâ€ use of GPT-3â€™s
    underlying model.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch03.html#idm45621854668624-marker)) æ­¤å¤–ï¼Œ[Microsoftåœ¨2020å¹´9æœˆ22æ—¥å®£å¸ƒ](https://oreil.ly/gjCEN)ï¼Œå·²ç»è·å¾—äº†GPT-3åŸºç¡€æ¨¡å‹çš„â€œç‹¬å®¶â€ä½¿ç”¨è®¸å¯ã€‚
- en: '^([10](ch03.html#idm45621854662512-marker)) nostalgebraist, [â€œInterpreting
    GPT: the Logit Lensâ€](https://oreil.ly/rZtju), *LessWrong* (blog), August 30,
    2020.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch03.html#idm45621854662512-marker)) nostalgebraist çš„æ–‡ç« [â€œè§£è¯»GPTï¼šå¯¹æ•°å‡ ç‡é€é•œâ€](https://oreil.ly/rZtju)ï¼Œ*LessWrong*ï¼ˆåšå®¢ï¼‰ï¼Œ2020å¹´8æœˆ30æ—¥ã€‚
- en: ^([11](ch03.html#idm45621848233456-marker)) If you want a more in-depth, intuitive
    explanation of Linear regression, check out [MLU Explainâ€™s article](https://oreil.ly/dEfCL).
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch03.html#idm45621848233456-marker)) å¦‚æœä½ å¸Œæœ›æ›´æ·±å…¥ã€æ›´ç›´è§‚åœ°äº†è§£çº¿æ€§å›å½’ï¼Œè¯·æŸ¥çœ‹[MLU Explainçš„æ–‡ç« ](https://oreil.ly/dEfCL)ã€‚
- en: '^([12](ch03.html#idm45621842768384-marker)) Yin Lou et al., [â€œAccurate Intelligible
    Models with Pairwise Interactionsâ€](https://dl.acm.org/doi/abs/10.1145/2487575.2487579),
    *Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining*, (August 2013): 623â€“31.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch03.html#idm45621842768384-marker)) Yin Lou ç­‰äººçš„æ–‡ç« [â€œå…·æœ‰æˆå¯¹äº¤äº’çš„å‡†ç¡®å¯ç†è§£æ¨¡å‹â€](https://dl.acm.org/doi/abs/10.1145/2487575.2487579)ï¼Œå‘è¡¨äºç¬¬19å±ŠACM
    SIGKDDå›½é™…æ•°æ®æŒ–æ˜ä¸çŸ¥è¯†å‘ç°ä¼šè®®ï¼ˆ2013å¹´ï¼‰ï¼š623â€“31ã€‚
- en: '^([13](ch03.html#idm45621841570432-marker)) Jerome H. Friedman and Bogdan E.
    Popescu, [â€œPredictive Learning via Rule Ensemblesâ€](https://arxiv.org/abs/0811.1679),
    *The Annals of Applied Statistics*, 2, no. 3 (2008): 916â€“54.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch03.html#idm45621841570432-marker)) Jerome H. Friedman å’Œ Bogdan E. Popescu
    çš„æ–‡ç« [â€œé€šè¿‡è§„åˆ™é›†çš„é¢„æµ‹å­¦ä¹ â€](https://arxiv.org/abs/0811.1679)ï¼Œå‘è¡¨äºã€Šåº”ç”¨ç»Ÿè®¡å¹´åˆŠã€‹ç¬¬2å·ç¬¬3æœŸï¼ˆ2008å¹´ï¼‰ï¼š916â€“54ã€‚
- en: ^([14](ch03.html#idm45621840086784-marker)) â€œZero-shotâ€ in machine learning
    refers to a model being able to perform tasks it hasnâ€™t previously been trained
    to do.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch03.html#idm45621840086784-marker)) åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œâ€œé›¶æ ·æœ¬å­¦ä¹ â€æŒ‡çš„æ˜¯æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œå…¶ä»¥å‰æœªç»è®­ç»ƒçš„ä»»åŠ¡ã€‚
- en: ^([15](ch03.html#idm45621839628720-marker)) Kim et al., â€œExamples Are Not Enough,
    Learn to Criticize! Criticism for Interpretability.â€
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch03.html#idm45621839628720-marker)) Kim ç­‰äººçš„æ–‡ç« [â€œä¾‹å­ä¸è¶³ä»¥ï¼Œå­¦ä¼šæ‰¹åˆ¤ï¼å¯è§£é‡Šæ€§çš„æ‰¹è¯„â€]ã€‚
- en: ^([16](ch03.html#idm45621839311008-marker)) In this case, weâ€™re using the pixel
    mean and standard deviation for ImageNet. This is pretty common for many tasks
    working with photographic input. Still, in many domains, itâ€™s worth directly calculating
    the mean and standard deviation for your particular dataset.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch03.html#idm45621839311008-marker)) åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ImageNetçš„åƒç´ å‡å€¼å’Œæ ‡å‡†å·®ã€‚è¿™å¯¹è®¸å¤šå¤„ç†æ‘„å½±è¾“å…¥çš„ä»»åŠ¡æ¥è¯´å¾ˆæ™®éã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šé¢†åŸŸï¼Œç›´æ¥è®¡ç®—ä½ ç‰¹å®šæ•°æ®é›†çš„å‡å€¼å’Œæ ‡å‡†å·®ä¹Ÿæ˜¯å€¼å¾—çš„ã€‚
- en: '^([17](ch03.html#idm45621836221104-marker)) David K. Lewis, *Counterfactuals*,
    (Cambridge: Harvard University Press, 1973).'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch03.html#idm45621836221104-marker)) David K. Lewisï¼Œã€Šåäº‹å®ã€‹ï¼Œï¼ˆå‰‘æ¡¥ï¼šå“ˆä½›å¤§å­¦å‡ºç‰ˆç¤¾ï¼Œ1973å¹´ï¼‰ã€‚
- en: ^([18](ch03.html#idm45621836219968-marker)) Sandra Wachter et al., [â€œCounterfactual
    Explanations Without Opening the Black Boxâ€](https://oreil.ly/kD6D9), *Harvard
    Journal of Law & Technology* 31, no. 2 (Spring 2017).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch03.html#idm45621836219968-marker)) Sandra Wachter ç­‰ï¼Œã€Šä¸æ‰“å¼€é»‘åŒ£å­çš„åäº‹å®è§£é‡Šã€‹ï¼Œ*å“ˆä½›æ³•å¾‹ä¸æŠ€æœ¯æ‚å¿—*
    31å·ï¼Œç¬¬2æœŸï¼ˆ2017å¹´æ˜¥å­£ï¼‰ã€‚
- en: ^([19](ch03.html#idm45621836184512-marker)) Maurice Jakesch et al., [â€œHuman
    Heuristics for AI-Generated Language Are Flawedâ€](https://arxiv.org/abs/2206.07271),
    *arXiv preprint* (2022).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch03.html#idm45621836184512-marker)) Maurice Jakesch ç­‰ï¼Œã€ŠAIç”Ÿæˆè¯­è¨€çš„äººç±»å¯å‘å¼å­˜åœ¨ç¼ºé™·ã€‹ï¼Œ*arXivé¢„å°æœ¬*ï¼ˆ2022å¹´ï¼‰ã€‚
- en: '^([20](ch03.html#idm45621836170272-marker)) Robert Miles, [â€œThe OTHER AI Alignment
    Problem: Mesa-Optimizers and Inner Alignmentâ€](https://youtu.be/bJLcIBixGj8),
    video, February 16, 2021.; Robert Miles, [â€œDeceptive Misaligned Mesa-Optimisers?
    Itâ€™s More Likely Than You Thinkâ€¦â€‹â€](https://youtu.be/IeWljQw3UgQ), video, May
    23, 2021.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch03.html#idm45621836170272-marker)) Robert Milesï¼Œã€Šå¦ä¸€ä¸ªAIå¯¹é½é—®é¢˜ï¼šMesaä¼˜åŒ–å™¨å’Œå†…éƒ¨å¯¹é½ã€‹ï¼Œè§†é¢‘ï¼Œ2021å¹´2æœˆ16æ—¥ï¼›Robert
    Milesï¼Œã€Šæ¬ºéª—æ€§ä¸å¯¹é½Mesaä¼˜åŒ–å™¨ï¼Ÿæ¯”ä½ æƒ³è±¡çš„æ›´æœ‰å¯èƒ½â€¦â€¦ã€‹ï¼Œè§†é¢‘ï¼Œ2021å¹´5æœˆ23æ—¥ã€‚
- en: ^([21](ch03.html#idm45621836168112-marker)) Robert Miles, [â€œWe Were Right! Real
    Inner Misalignmentâ€](https://youtu.be/zkbPdEHEyEI), video, October 10, 2021.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch03.html#idm45621836168112-marker)) Robert Milesï¼Œã€Šæˆ‘ä»¬æ˜¯å¯¹çš„ï¼çœŸæ­£çš„å†…éƒ¨ä¸å¯¹é½ã€‹ï¼Œè§†é¢‘ï¼Œ2021å¹´10æœˆ10æ—¥ã€‚
