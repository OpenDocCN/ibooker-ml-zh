- en: Chapter 2\. Data Representation Design Patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章 数据表示设计模式
- en: At the heart of any machine learning model is a mathematical function that is
    defined to operate on specific types of data only. At the same time, real-world
    machine learning models need to operate on data that may not be directly pluggable
    into the mathematical function. The mathematical core of a decision tree, for
    example, operates on boolean variables. Note that we are talking here about the
    mathematical core of a decision tree—decision tree machine learning software will
    typically also include functions to learn an optimal tree from data and ways to
    read in and process different types of numeric and categorical data. The mathematical
    function (see [Figure 2-1](#the_heart_of_a_decision_tree_machine_le)) that underpins
    a decision tree, however, operates on boolean variables and uses operations such
    as AND (&& in [Figure 2-1](#the_heart_of_a_decision_tree_machine_le)) and OR (+
    in [Figure 2-1](#the_heart_of_a_decision_tree_machine_le)).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习模型的核心是一个数学函数，该函数定义为仅在特定类型的数据上操作。与此同时，现实世界中的机器学习模型需要操作可能无法直接插入数学函数中的数据。例如，决策树的数学核心操作于布尔变量上。请注意，我们这里讨论的是决策树的数学核心——决策树机器学习软件通常还包括从数据中学习最优树的函数以及读取和处理不同类型的数值和分类数据的方法。然而，支撑决策树的数学函数（见[图2-1](#the_heart_of_a_decision_tree_machine_le)）实际上操作布尔变量，并使用AND（在[图2-1](#the_heart_of_a_decision_tree_machine_le)中为&&）和OR（在[图2-1](#the_heart_of_a_decision_tree_machine_le)中为+）等操作。
- en: '![The heart of a decision tree machine learning model to predict whether or
    not a baby requires intensive care is a mathematical model that operates on boolean
    variables.](Images/mldp_0201.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![用于预测婴儿是否需要重症监护的决策树机器学习模型的核心是操作布尔变量的数学模型。](Images/mldp_0201.png)'
- en: Figure 2-1\. The heart of a decision tree machine learning model to predict
    whether or not a baby requires intensive care is a mathematical model that operates
    on boolean variables.
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1 决策树机器学习模型的核心，用于预测婴儿是否需要重症监护。
- en: Suppose we have a decision tree to predict whether a baby will require intensive
    care (IC) or can be normally discharged (ND), and suppose that the decision tree
    takes as inputs two variables, *x1* and *x2*. The trained model might look something
    like [Figure 2-1](#the_heart_of_a_decision_tree_machine_le).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个决策树来预测婴儿是否需要重症监护（IC）或可以正常出院（ND），并且假设决策树的输入是两个变量，*x1* 和 *x2*。训练模型可能看起来像[图2-1](#the_heart_of_a_decision_tree_machine_le)所示。
- en: 'It is pretty clear that *x1* and *x2* need to be boolean variables in order
    for *f(x1, x2)* to work. Suppose that two of the pieces of information we’d like
    the model to consider when classifying a baby as requiring intensive care or not
    is the hospital that the baby is born in and the baby’s weight. Can we use the
    hospital that a baby is born in as an input to the decision tree? No, because
    the hospital takes neither the value True nor the value False and cannot be fed
    into the && (AND) operator. It’s mathematically not compatible. Of course, we
    can “make” the hospital value boolean by performing an operation such as:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，*x1* 和 *x2* 必须是布尔变量，才能使 *f(x1, x2)* 正常工作。假设我们希望模型在分类婴儿是否需要重症监护时考虑两个信息：婴儿出生的医院和婴儿的体重。我们能够将婴儿出生的医院作为决策树的输入吗？不行，因为医院既不是True也不是False的值，不能被传入&&（AND）运算符。从数学上讲是不兼容的。当然，我们可以通过进行操作如下：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'so that *x1* is True when the hospital is in France, and False if not. Similarly,
    a baby’s weight cannot be fed directly into the model, but by performing an operation
    such as:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当医院位于法国时，*x1* 为True，否则为False。同样，婴儿的体重不能直接输入模型，但可以通过进行操作如下：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: we can use the hospital or the baby weight as an input to the model. This is
    an example of how input data (hospital, a complex object or baby weight, a floating
    point number) can be represented in the form (boolean) expected by the model.
    This is what we mean by *data representation*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将医院或者婴儿体重作为模型的输入。这是输入数据（医院，一个复杂对象或婴儿体重，一个浮点数）如何以模型期望的形式（布尔型）表示的示例。这就是我们所说的*数据表示*。
- en: In this book, we will use the term *input* to represent the real-world data
    fed to the model (for example, the baby weight) and the term *feature* to represent
    the transformed data that the model actually operates on (for example, whether
    the baby weight is less than 3 kilograms). The process of creating features to
    represent the input data is called *feature engineering*, and so we can think
    of feature engineering as a way of selecting the data representation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用术语*输入*来表示输入到模型的真实世界数据（例如，婴儿体重），并使用术语*特征*来表示模型实际操作的转换后的数据（例如，婴儿体重是否小于3公斤）。创建用于表示输入数据的特征的过程称为*特征工程*，因此我们可以将特征工程视为一种选择数据表示的方式。
- en: Of course, rather than hardcoding parameters such as the threshold value of
    3 kilograms, we’d prefer the machine learning model to learn how to create each
    node by selecting the input variable and the threshold. Decision trees are an
    example of machine learning models that are capable of learning the data representation.^([1](ch02.xhtml#ch01fn1))
    Many of the patterns that we look at in this chapter will involve similarly *learnable*
    *data representations*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们更希望机器学习模型能够学习如何通过选择输入变量和阈值来创建每个节点，而不是硬编码参数，比如3公斤的阈值。决策树就是能够学习数据表示的机器学习模型的一个例子^([1](ch02.xhtml#ch01fn1))。本章我们将看到的许多模式将涉及类似的*可学习的数据表示*。
- en: The *Embeddings* design pattern is the canonical example of a data representation
    that deep neural networks are capable of learning on their own. In an embedding,
    the learned representation is dense and lower-dimensional than the input, which
    could be sparse. The learning algorithm needs to extract the most salient information
    from the input and represent it in a more concise way in the feature. The process
    of learning features to represent the input data is called *feature extraction*,
    and we can think of learnable data representations (like embeddings) as automatically
    engineered features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌入*设计模式是深度神经网络能够自行学习的数据表示的典型示例。在嵌入中，学习得到的表示是密集的，并且比输入（可能是稀疏的）更低维度。学习算法需要从输入中提取最显著的信息，并在特征中以更简洁的方式表示它。学习用于表示输入数据的特征的过程称为*特征提取*，我们可以将可学习的数据表示（如嵌入）视为自动化生成的特征。'
- en: The data representation doesn’t even need to be of a single input variable—an
    oblique decision tree, for example, creates a boolean feature by thresholding
    a linear combination of two or more input variables. A decision tree where each
    node can represent only one input variable reduces to a stepwise linear function,
    whereas an oblique decision tree where each node can represent a linear combination
    of input variables reduces to a piecewise linear function (see [Figure 2-2](#_a_decision_tree_classifier_where_each)).
    Considering how many steps will have to be learned to adequately represent the
    line, the piecewise linear model is simpler and faster to learn. An extension
    of this idea is the *Feature Cross* design pattern, which simplifies the learning
    of AND relationships between multivalued categorical variables.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据表示甚至可以不仅仅是单个输入变量的表示 —— 例如，斜决策树通过设定两个或更多输入变量的线性组合的阈值来创建一个布尔特征。每个节点只能表示一个输入变量的决策树会简化为分段线性函数，而每个节点可以表示输入变量线性组合的斜决策树会简化为分段线性函数（见[图2-2](#_a_decision_tree_classifier_where_each)）。考虑到需要学习来充分表示线性的步骤数量，分段线性模型更为简单且更快速。这个想法的扩展是*特征交叉*设计模式，简化了多值分类变量之间AND关系的学习。
- en: '![A decision tree classifier where each node can threshold only one input value
    (x1 or x2) will result in a stepwise linear boundary function, whereas an oblique
    tree classifier where a node can threshold a linear combination of input variables
    will result in a piecewise linear boundary function. The piecewise linear function
    requires fewer nodes and can achieve greater accuracy.](Images/mldp_0202.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![决策树分类器，每个节点只能设置一个输入值（x1或x2），将导致一个分段线性边界函数；而斜决策树分类器，每个节点可以设置输入变量的线性组合的阈值，将导致一个分段线性边界函数。分段线性函数需要更少的节点，并能达到更高的精度。](Images/mldp_0202.png)'
- en: Figure 2-2\. A decision tree classifier where each node can threshold only one
    input value (x1 or x2) will result in a stepwise linear boundary function, whereas
    an oblique tree classifier where a node can threshold a linear combination of
    input variables will result in a piecewise linear boundary function. The piecewise
    linear function requires fewer nodes and can achieve greater accuracy.
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. 决策树分类器，每个节点只能阈值化一个输入值（x1 或 x2），将导致一个逐步线性边界函数，而一个斜树分类器，其中一个节点可以阈值化输入变量的线性组合，将导致一个分段线性边界函数。分段线性函数需要更少的节点，并且可以达到更高的准确度。
- en: The data representation doesn’t need to be learned or fixed—a hybrid is also
    possible. The*Hashed Feature*design pattern is deterministic, but doesn’t require
    a model to know all the potential values that a particular input can take.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据表示不需要学习或固定，还可以使用混合方法。*哈希特征*设计模式是确定性的，但不需要模型知道特定输入可能采用的所有潜在值。
- en: The data representations we have looked at so far are all one-to-one. Although
    we could represent input data of different types separately or represent each
    piece of data as just one feature, it can be more advantageous to use *Multimodal
    Input*. That is the fourth design pattern we will explore in this chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们查看的数据表示都是一对一的。虽然我们可以分别表示不同类型的输入数据或将每个数据片段表示为单个特征，但使用*多模态输入*可能更有利。这是本章将要探讨的第四种设计模式。
- en: Simple Data Representations
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的数据表示
- en: Before we delve into learnable data representations, feature crosses, and more,
    let’s look at simpler data representations. We can think of these simple data
    representations as common *idioms* in machine learning—not quite patterns, but
    commonly employed solutions nevertheless.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入学习数据表示、特征交叉等之前，让我们先看看更简单的数据表示。我们可以将这些简单的数据表示视为机器学习中常见的*惯用法*，虽然不完全是模式，但通常被广泛采用。
- en: Numerical Inputs
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值输入
- en: Most modern, large-scale machine learning models (random forests, support vector
    machines, neural networks) operate on numerical values, and so if our input is
    numeric, we can pass it through to the model unchanged.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代大规模机器学习模型（随机森林、支持向量机、神经网络）都是基于数值的操作，因此如果我们的输入是数值型的，我们可以将其直接传递给模型。
- en: Why scaling is desirable
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么缩放是可取的
- en: Often, because the ML framework uses an optimizer that is tuned to work well
    with numbers in the [–1, 1] range, scaling the numeric values to lie in that range
    can be beneficial.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，由于机器学习框架使用的优化器经过调优，能够很好地处理[–1, 1]范围内的数字，因此将数值缩放到该范围内可能会有益处。
- en: 'A quick test with one of scikit-learn’s built-in datasets can prove the point
    (this is an excerpt from this book’s code [repository](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/simple_data_representation.ipynb)):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 scikit-learn 内置数据集的快速测试可以证明这一点（这是来自本书代码[仓库](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/simple_data_representation.ipynb)的摘录）：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When we ran this, we got a nearly 9% improvement on this model which uses just
    one input feature. Considering the number of features in a typical machine learning
    model, the savings can add up.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这个模型时，我们得到了几乎 9% 的改进，这个模型只使用了一个输入特征。考虑到典型机器学习模型中的特征数量，这些节省可能会积累起来。
- en: Another important reason for scaling is that some machine learning algorithms
    and techniques are very sensitive to the relative magnitudes of the different
    features. For example, a k-means clustering algorithm that uses the Euclidean
    distance as its proximity measure will end up relying heavily on features with
    larger magnitudes. Lack of scaling also affects the efficacy of L1 or L2 regularization
    since the magnitude of weights for a feature depends on the magnitude of values
    of that feature, and so different features will be affected differently by regularization.
    By scaling all features to lie between [–1, 1], we ensure that there is not much
    of a difference in the relative magnitudes of different features.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放的另一个重要原因是，一些机器学习算法和技术对不同特征的相对大小非常敏感。例如，使用欧氏距离作为其接近度测量的 k-means 聚类算法将主要依赖具有较大幅度特征。缺乏缩放还会影响
    L1 或 L2 正则化的效果，因为特征的权重大小取决于该特征的值的大小，因此不同特征会受到正则化的影响不同。通过将所有特征缩放到[–1, 1]之间，我们确保不同特征的相对大小差异不大。
- en: Linear scaling
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性缩放
- en: 'Four forms of scaling are commonly employed:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的四种缩放形式：
- en: Min-max scaling
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大缩放
- en: 'The numeric value is linearly scaled so that the minimum value that the input
    can take is scaled to –1 and the maximum possible value to 1:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数值线性缩放，使得输入可以取的最小值缩放到–1，最大可能值缩放到1：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The problem with min-max scaling is that the maximum and minimum value (`max_x1`
    and `min_x1`) have to be estimated from the training dataset, and they are often
    outlier values. The real data often gets shrunk to a very narrow range in the
    [–1, 1] band.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大缩放的问题在于必须从训练数据集中估计最大和最小值（`max_x1`和`min_x1`），它们通常是异常值。真实数据经常被缩小到[–1, 1]范围内的一个非常窄的区域。
- en: Clipping (in conjunction with min-max scaling)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪（与最小-最大缩放结合使用）
- en: Helps address the problem of outliers by using “reasonable” values instead of
    estimating the minimum and maximum from the training dataset. The numeric value
    is linearly scaled between these two reasonable bounds, then clipped to lie in
    the range [–1, 1]. This has the effect of treating outliers as –1 or 1.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用“合理”的值而不是从训练数据集估计最小和最大值来解决异常值问题。数值线性缩放在这两个合理范围内，然后裁剪到范围[–1, 1]。这样做的效果是将异常值视为–1或1。
- en: Z-score normalization
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Z-score 标准化
- en: 'Addresses the problem of outliers without requiring prior knowledge of what
    the reasonable range is by linearly scaling the input using the mean and standard
    deviation estimated over the training dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用训练数据集上估计的均值和标准差，线性缩放输入来解决异常值问题，而无需事先知道合理范围是什么：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The name of the method reflects the fact that the scaled value has zero mean
    and is normalized by the standard deviation so that it has unit variance over
    the training dataset. The scaled value is unbounded, but does lie between [–1,
    1] the majority of the time (67%, if the underlying distribution is normal). Values
    outside this range get rarer the larger their absolute value gets, but are still
    present.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法的名称反映了缩放值具有零均值，并且通过标准差进行归一化，从而使其在训练数据集上具有单位方差的事实。缩放值是无界的，但大多数时间（如果基础分布是正态分布，为67%）位于[–1,
    1]之间。绝对值越大的值超出此范围的概率较低，但仍然存在。
- en: Winsorizing
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 温索化
- en: Uses the empirical distribution in the training dataset to clip the dataset
    to bounds given by the 10th and 90th percentile of the data values (or 5th and
    95th percentile, and so forth). The winsorized value is min-max scaled.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据集中的经验分布将数据集裁剪到由数据值的第10和90百分位给出的边界（或第5和95百分位，依此类推）。修剪后的值是最小-最大缩放的。
- en: All the methods discussed so far scale the data linearly (in the case of clipping
    and winsorizing, linear within the typical range). Min-max and clipping tend to
    work best for uniformly distributed data, and Z-score tends to work best for normally
    distributed data. The impact of different scaling functions on the `mother_age`
    column in the baby weight prediction example is shown in [Figure 2-3](#the_histogram_of_mother_age_in_the_baby)
    (see the [full code](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/simple_data_representation.ipynb)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的所有方法都对数据进行线性缩放（在剪裁和温索化的情况下，典型范围内是线性的）。最小-最大缩放和剪裁倾向于最适合均匀分布的数据，而 Z-score
    倾向于最适合正态分布的数据。在婴儿体重预测示例中，不同缩放函数对 `mother_age` 列的影响显示在[图2-3](#the_histogram_of_mother_age_in_the_baby)中（查看[完整代码](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/simple_data_representation.ipynb)）。
- en: In [Figure 2-3](#the_histogram_of_mother_age_in_the_baby), note that minmax_scaled
    gets the x values into the desired range of [–1, 1] but continues to retain values
    at the extreme ends of the distribution where there are not enough examples. Clipping
    rolls up many of the problematic values, but requires getting the clipping thresholds
    exactly correct—here, the slow decline in the number of babies with mothers’ ages
    above 40 poses problems in setting a hard threshold. Winsorizing, similar to clipping,
    requires getting the percentile thresholds exactly correct. Z-score normalization
    improves the range (but does not constrain values to be between [–1, 1]) and pushes
    the problematic values further out. Of these three methods, zero-norming works
    best for `mother_age` because the raw age values were somewhat of a bell curve.
    For other problems, min-max scaling, clipping, or winsorizing might be better.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 2-3](#the_histogram_of_mother_age_in_the_baby)中，请注意minmax_scaled将x值放入所需范围[-1,
    1]，但继续保留分布极端端点的值，这些端点的例子不足。Clipping将许多问题值折叠起来，但需要准确设置截断阈值——在此处，40岁以上母亲的婴儿数量的缓慢下降造成了设置硬阈值的问题。Winsorizing与Clipping类似，需要准确设置百分位阈值。Z-score
    标准化改善了范围（但不限制值为[-1, 1]），并将问题值推迟。对于这三种方法中，零规范化对`mother_age`效果最好，因为原始年龄值有点钟形曲线。对于其他问题，min-max
    缩放、Clipping或Winsorizing可能更好。
- en: '![The histogram of mother_age in the baby weight prediction example is shown
    in the top-left panel, and different scaling functions (see the x-axis label)
    are shown in the remaining panels.](Images/mldp_0203.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![母亲年龄在婴儿体重预测示例中的直方图显示在左上方面板，不同的缩放函数（见x轴标签）显示在其余面板中。](Images/mldp_0203.png)'
- en: Figure 2-3\. The histogram of mother_age in the baby weight prediction example
    is shown in the top-left panel, and different scaling functions (see the x-axis
    label) are shown in the remaining panels.
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. 母亲年龄在婴儿体重预测示例中的直方图显示在左上方面板，不同的缩放函数（见x轴标签）显示在其余面板中。
- en: Nonlinear transformations
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非线性变换
- en: What if our data is skewed and neither uniformly distributed nor distributed
    like a bell curve? In that case, it is better to apply a *nonlinear transform*
    to the input before scaling it. One common trick is to take the logarithm of the
    input value before scaling it. Other common transformations include the sigmoid
    and polynomial expansions (square, square root, cube, cube root, and so on). We’ll
    know that we have a good transformation function if the distribution of the transformed
    value becomes uniform or normally distributed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据出现偏斜，既不均匀分布也不像钟形曲线分布怎么办？在这种情况下，在缩放之前应用非线性变换会更好。一个常见的技巧是在缩放之前取输入值的对数。其他常见的转换包括sigmoid函数和多项式扩展（平方、平方根、立方、立方根等）。我们会知道我们有一个好的转换函数，如果转换后的值的分布变得均匀或正态分布。
- en: 'Assume that we are building a model to predict the sales of a nonfiction book.
    One of the inputs to the model is the popularity of the Wikipedia page corresponding
    to the topic. The number of views of pages in Wikipedia is, however, highly skewed
    and occupies a large dynamic range (see the left panel of [Figure 2-4](#left_panel_the_distribution_of_the_numb):
    the distribution is highly skewed toward rarely viewed pages, but the most common
    pages are viewed tens of millions of times). By taking the logarithm of the views,
    then taking the fourth root of this log value and scaling the result linearly,
    we obtain something that is in the desired range and somewhat bell-shaped. For
    details of the code to query the Wikipedia data, apply these transformations,
    and generate this plot, refer to the [GitHub repository](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/simple_data_representation.ipynb)
    for this book.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在建立一个模型来预测一本非虚构书籍的销售情况。模型的一个输入是与主题对应的维基百科页面的流行程度。然而，维基百科页面的访问量存在严重的偏斜，并且占据了很大的动态范围（见[图 2-4](#left_panel_the_distribution_of_the_numb)的左侧面板：分布向极少被访问的页面倾斜，但最常见的页面被访问了数千万次）。通过取对数，然后取该对数值的四次方根，并线性缩放结果，我们得到了一个在所需范围内且略呈钟形的分布。有关查询维基百科数据、应用这些转换和生成此图的代码详细信息，请参阅本书的[GitHub
    代码库](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/simple_data_representation.ipynb)。
- en: '![Left panel: the distribution of the number of views of Wikipedia pages is
    highly skewed and occupies a large dynamic range. The second panel demonstrates
    that problems can be addressed by transforming the number of views using the logarithm,
    a power function, and linear scaling in succession. The third panel shows the
    effect of histogram equalization, and the fourth panel shows the effect of the
    Box-Cox transform.](Images/mldp_0204.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![左面板：维基百科页面浏览数分布高度偏斜，动态范围大。第二面板展示了通过连续使用对数、幂函数和线性缩放来转换浏览数的方法。第三面板展示了直方图均衡化的效果，第四面板展示了Box-Cox变换的效果。](Images/mldp_0204.png)'
- en: 'Figure 2-4\. Left panel: the distribution of the number of views of Wikipedia
    pages is highly skewed and occupies a large dynamic range. The second panel demonstrates
    that problems can be addressed by transforming the number of views using the logarithm,
    a power function, and linear scaling in succession. The third panel shows the
    effect of histogram equalization and the fourth panel shows the effect of the
    Box-Cox transform.'
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 左面板：维基百科页面浏览数分布高度偏斜，动态范围大。第二面板展示了通过连续使用对数、幂函数和线性缩放来转换浏览数的方法。第三面板展示了直方图均衡化的效果，第四面板展示了Box-Cox变换的效果。
- en: It can be difficult to devise a linearizing function that makes the distribution
    look like a bell curve. An easier approach is to bucketize the number of views,
    choosing the bucket boundaries to fit the desired output distribution. A principled
    approach to choosing these buckets is to do *histogram equalization*, where the
    bins of the histogram are chosen based on quantiles of the raw distribution, (see
    the third panel of [Figure 2-4](#left_panel_the_distribution_of_the_numb)). In
    the ideal situation, histogram equalization results in a uniform distribution
    (although not in this case, because of repeated values in the quantiles).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个线性化函数，使分布看起来像钟形曲线可能会很困难。更简单的方法是将浏览次数分桶化，选择桶的边界以适应所需的输出分布。选择这些桶的一个有原则的方法是进行*直方图均衡化*，直方图的箱子根据原始分布的分位数选择（见[图 2-4](#left_panel_the_distribution_of_the_numb)的第三面板）。在理想情况下，直方图均衡化会导致均匀分布（尽管在本例中不会，因为分位数中存在重复值）。
- en: 'To carry out histogram equalization in BigQuery, we can do:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要在BigQuery中执行直方图均衡化，我们可以执行以下操作：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'where the bins are obtained from:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中箱子是从以下位置获取的：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: See the [notebook](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/simple_data_representation.ipynb)
    in the code repository of this book for full details.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 查看本书代码库中的[notebook](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/simple_data_representation.ipynb)获取完整详情。
- en: 'Another method to handle skewed distributions is to use a parametric transformation
    technique like the *Box-Cox transform*. Box-Cox chooses its single parameter,
    lambda, to control the “heteroscedasticity” so that the variance no longer depends
    on the magnitude. Here, the variance among rarely viewed Wikipedia pages will
    be much smaller than the variance among frequently viewed pages, and Box-Cox tries
    to equalize the variance across all ranges of the number of views. This can be
    done using Python’s SciPy package:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 处理偏斜分布的另一种方法是使用像*Box-Cox变换*这样的参数化转换技术。Box-Cox选择其单一参数lambda来控制“异方差性”，使得方差不再取决于大小。在这里，很少被查看的维基百科页面之间的方差要比经常被查看的页面之间的方差小得多，而Box-Cox试图在所有浏览数的范围内均衡化方差。可以使用Python的SciPy包来完成这个过程：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The parameter estimated over the training dataset `(est_lambda)` is then used
    to transform other values:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据集上估计的参数`(est_lambda)`然后用于转换其他值：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Array of numbers
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数字数组
- en: 'Sometimes, the input data is an array of numbers. If the array is of fixed
    length, data representation can be rather simple: flatten the array and treat
    each position as a separate feature. But often, the array will be of variable
    length. For example, one of the inputs to the model to predict the sales of a
    nonfiction book might be the sales of all previous books on the topic. An example
    input might be:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，输入数据是一个数字数组。如果数组长度固定，数据表示可能会很简单：展平数组，并将每个位置视为单独的特征。但通常，数组长度会变化。例如，用于预测非小说书籍销量模型的输入之一可能是该主题所有先前书籍的销售量。例如输入可能是：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Obviously, the length of this array will vary in each row because there are
    different numbers of books published on different topics.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这个数组的长度在每一行中会有所不同，因为不同主题的书籍数量各不相同。
- en: 'Common idioms to handle arrays of numbers include the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数字数组的常见习语包括以下内容：
- en: Representing the input array in terms of its bulk statistics. For example, we
    might use the length (that is, count of previous books on topic), average, median,
    minimum, maximum, and so forth.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以其总体统计数据表示输入数组。例如，我们可能会使用长度（即以前有关该主题的书籍的数量）、平均值、中位数、最小值、最大值等。
- en: Representing the input array in terms of its empirical distribution—i.e., by
    the 10th/20th/... percentile, and so on.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以其经验分布表示输入数组——例如第10/20/...百分位数等。
- en: If the array is ordered in a specific way (for example, in order of time or
    by size), representing the input array by the last three or some other fixed number
    of items. For arrays of length less than three, the feature is padded to a length
    of three with missing values.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数组以特定方式排序（例如按时间顺序或按大小排序），则通过最后三个或其他固定数量的项目表示输入数组。对于长度小于三的数组，该特征将使用缺失值填充到长度为三。
- en: All these end up representing the variable-length array of data as a fixed-length
    feature. We could have also formulated this problem as a time-series forecasting
    problem, as the problem of forecasting the sales of the next book on the topic
    based on the time history of sales of previous books. By treating the sales of
    previous books as an array input, we are assuming that the most important factors
    in predicting a book’s sales are characteristics of the book itself (author, publisher,
    reviews, and so on) and not the temporal continuity of the sales amounts.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些最终都将变量长度数组表示为固定长度特征。我们也可以将这个问题表述为时间序列预测问题，即基于以前书籍销售的时间历史来预测下一本书的销售。通过将以前书籍的销售视为数组输入，我们假设预测书籍销售最重要的因素是书籍本身的特征（作者、出版商、评论等），而不是销售金额的时间连续性。
- en: Categorical Inputs
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类输入
- en: Because most modern, large-scale machine learning models (random forests, support
    vector machines, neural networks) operate on numerical values, categorical inputs
    have to be represented as numbers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因为大多数现代大规模机器学习模型（随机森林、支持向量机、神经网络）都是基于数值值运行的，所以分类输入必须表示为数字。
- en: 'Simply enumerating the possible values and mapping them to an ordinal scale
    will work poorly. Suppose that one of the inputs to the model that predicts the
    sales of a nonfiction book is the language that the book is written in. We can’t
    simply create a mapping table like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 枚举可能的值并将它们映射到一个有序标度将会效果不佳。假设模型的一个输入是预测非小说书籍销售的语言。我们不能简单地创建这样的映射表：
- en: '| Categorical input | Numeric feature |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 分类输入 | 数值特征 |'
- en: '| --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| English | 1.0 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 英文 | 1.0 |'
- en: '| Chinese | 2.0 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 中文 | 2.0 |'
- en: '| German | 3.0 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 德语 | 3.0 |'
- en: This is because the machine learning model will then attempt to interpolate
    between the popularity of German and English books to get the popularity of the
    book in Chinese! Because there is no ordinal relationship between languages, we
    need to use a categorical to numeric mapping that allows the model to learn the
    market for books written in these languages independently.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为机器学习模型将尝试在德语和英语书籍的流行度之间进行插值，以获取中文书籍的流行度！由于语言之间没有顺序关系，我们需要使用分类到数值的映射，使模型能够独立学习这些语言书籍市场。
- en: One-hot encoding
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独热编码
- en: 'The simplest method of mapping categorical variables while ensuring that the
    variables are independent is *one-hot encoding*. In our example, the categorical
    input variable would be converted into a three-element feature vector using the
    following mapping:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将分类变量映射为保证变量独立的最简单方法是*独热编码*。在我们的例子中，分类输入变量将通过以下映射转换为三元素特征向量：
- en: '| Categorical input | Numeric feature |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 分类输入 | 数值特征 |'
- en: '| --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| English | [1.0, 0.0, 0.0] |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 英语 | [1.0, 0.0, 0.0] |'
- en: '| Chinese | [0.0, 1.0, 0.0] |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 中文 | [0.0, 1.0, 0.0] |'
- en: '| German | [0.0, 0.0, 1.0] |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 德语 | [0.0, 0.0, 1.0] |'
- en: One-hot encoding requires us to know the *vocabulary* of the categorical input
    beforehand. Here, the vocabulary consists of three tokens (English, Chinese, and
    German), and the length of the resulting feature is the size of this vocabulary.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码要求我们事先知道分类输入的*词汇*。在这里，词汇包括三个标记（英语、中文和德语），生成的特征长度是这个词汇表的大小。
- en: 'In some circumstances, it can be helpful to treat a numeric input as categorical
    and map it to a one-hot encoded column:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，将数字输入视为分类变量，并将其映射到一个独热编码列可能会有所帮助：
- en: When the numeric input isan index
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当数字输入是一个索引
- en: For example, if we are trying to predict traffic levels and one of our inputs
    is the day of the week, we could treat the day of the week as numeric (1, 2, 3,
    …, 7), but it is helpful to recognize that the day of the week here is not a continuous
    scale but really just an index. It is better to treat it as categorical (Sunday,
    Monday, …, Saturday) because the indexing is arbitrary. Should the week start
    on Sunday (as in the USA), Monday (as in France), or Saturday (as in Egypt)?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们试图预测交通水平，而我们的输入之一是星期几，我们可以将星期几视为数值（1、2、3，…，7），但认识到这里的星期几并不是连续的刻度，而只是一个索引。将其视为分类（星期日、星期一，…，星期六）更有帮助，因为索引是任意的。周的起始日应该是星期天（美国）、星期一（法国）还是星期六（埃及）？
- en: When the relationship between input and label isnot continuous
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入和标签之间的关系不是连续的时候
- en: What should tip the scale toward treating day of the week as a categorical feature
    is that traffic levels on Friday are not affected by those on Thursday and Saturday.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 应该将一周中的某一天作为分类特征的理由在于，星期五的交通水平不受星期四和星期六的影响。
- en: When it is advantageous to bucket the numeric variable
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当将数字变量分桶时
- en: In most cities, traffic levels depend on whether it is the weekend, and this
    can vary by location (Saturday and Sunday in most of the world, Thursday and Friday
    in some Islamic countries). It would be helpful to then treat day of the week
    as a boolean feature (weekend or weekday). Such a mapping where the number of
    distinct inputs (here, seven) is greater than the number of distinct feature values
    (here, two) is called bucketing. Commonly, bucketing is done in terms of ranges—for
    example, we might bucket `mother_age` into ranges that break at 20, 25, 30, etc.
    and treat each of these bins as categorical, but it should be realized that this
    loses the ordinal nature of `mother_age`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数城市中，交通水平取决于是否是周末，并且这可能因地点而异（大部分世界在周六和周日，某些伊斯兰国家在星期四和星期五）。因此，将一周中的某一天视为布尔特征（周末或工作日）会很有帮助。这种映射中，独立输入的数量（这里是七个）大于独立特征值的数量（这里是两个），被称为分桶。通常，分桶是根据范围进行的——例如，我们可以将`mother_age`分桶为在20、25、30等处断开的范围，并将每个桶视为分类变量，但应意识到这会丢失`mother_age`的序数性质。
- en: When we want to treat different values of the numeric input as being independent
    when it comes to their effect on the label
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望处理数值输入的不同值时，视其对标签的影响为独立
- en: For example, the weight of a baby depends on the plurality^([2](ch02.xhtml#ch01fn2))
    of the delivery since twins and triplets tend to weigh less than single births.
    So, a lower-weight baby, if part of a triplet, might be healthier than a twin
    baby with the same weight. In this case, we might map the plurality to a categorical
    variable, since a categorical variable allows the model to learn independent tunable
    parameters for the different values of plurality. Of course, we can do this only
    if we have enough examples of twins and triplets in our dataset.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，婴儿的体重取决于分娩的多胎情况^([2](ch02.xhtml#ch01fn2))，因为双胞胎和三胞胎的体重通常比单胎轻。因此，如果三胞胎中有一个体重较轻的婴儿，可能比体重相同的双胞胎更健康。在这种情况下，我们可以将多胎数映射为分类变量，因为分类变量允许模型为不同的多胎值学习独立可调参数。当然，只有在我们的数据集中有足够的双胞胎和三胞胎的例子时才能这样做。
- en: Array of categorical variables
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数组的分类变量
- en: 'Sometimes, the input data is an array of categories. If the array is of fixed
    length, we can treat each array position as a separate feature. But often, the
    array will be of variable length. For example, one of the inputs to the natality
    model might be the type of previous births to this mother:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，输入数据是一个类别数组。如果数组长度固定，我们可以将每个数组位置视为单独的特征。但通常，数组的长度是可变的。例如，新生模型的一个输入可能是母亲之前的分娩类型：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Obviously, the length of this array will vary in each row because there are
    different numbers of older siblings for each baby.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，该数组的长度在每一行中会有所不同，因为每个婴儿的哥哥姐姐的数量也不同。
- en: 'Common idioms to handle arrays of categorical variables include the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数组分类变量的常见习语包括以下内容：
- en: '*Counting* the number of occurrences of each vocabulary item. So, the representation
    for our example would be `[2, 1, 1]` assuming that the vocabulary is `Induced,
    Natural`, and `Cesarean` (in that order). This is now a fixed-length array of
    numbers that can be flattened and used in positional order. If we have an array
    where an item can occur only once (for example, of languages a person speaks),
    or if the feature just indicates presence and not count (such as whether the mother
    has ever had a Cesarean operation), then the count at each position is 0 or 1,
    and this is called *multi-hot encoding*.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计数*每个词汇项的出现次数。因此，我们示例的表示将是`[2, 1, 1]`，假设词汇表是`Induced, Natural`和`Cesarean`（按顺序）。现在这是一个固定长度的数字数组，可以展平并按位置顺序使用。如果我们有一个数组，其中一个项目只能出现一次（例如一个人说的语言），或者如果该特征只表示存在而不是计数（例如母亲是否曾经接受过剖腹产手术），那么每个位置的计数为0或1，这被称为*多热编码*。'
- en: To avoid large numbers, the*relative frequency* can be used instead of the count.
    The representation for our example would be `[0.5, 0.25, 0.25]` instead of `[2,
    1, 1]`. Empty arrays (first-born babies with no previous siblings) are represented
    as `[0, 0, 0]`. In natural language processing, the relative frequency of a word
    overall is normalized by the relative frequency of documents that contain the
    word to yield [TF-IDF](https://oreil.ly/kNYHr) (short for term frequency–inverse
    document frequency). TF-IDF reflects how unique a word is to a document.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为避免大数字，可以使用*相对频率*代替计数。我们示例的表示方法将是`[0.5, 0.25, 0.25]`而不是`[2, 1, 1]`。空数组（没有兄弟姐妹的第一个孩子）表示为`[0,
    0, 0]`。在自然语言处理中，词的整体相对频率通过包含该词的文档的相对频率来归一化，从而产生[TF-IDF](https://oreil.ly/kNYHr)（词频-逆文档频率）。
- en: If the array is ordered in a specific way (e.g., in order of time), representing
    the input array by the last three items. Arrays shorter than three are padded
    with missing values.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数组以特定方式排序（例如，按时间顺序），则用最后三个项目表示输入数组。长度小于三的数组用缺失值填充。
- en: Representing the array by bulk statistics, e.g., the length of the array, the
    mode (most common entry), the median, the 10th/20th/… percentile, etc.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过批量统计来表示数组，例如数组的长度，众数（最常见的条目），中位数，第10/20/…百分位等。
- en: Of these, the counting/relative-frequency idiom is the most common. Note that
    both of these are a generalization of one-hot encoding—if the baby had no older
    siblings, the representation would be `[0, 0, 0]`, and if the baby had one older
    sibling who was born in a natural birth, the representation would be `[0, 1, 0]`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些表示方法中，计数/相对频率的习惯用法最为常见。请注意，这两者都是独热编码的一般化——如果婴儿没有兄弟姐妹，其表示将为`[0, 0, 0]`，如果婴儿有一个自然分娩的兄弟姐妹，则表示将为`[0,
    1, 0]`。
- en: Having seen simple data representations, let’s discuss design patterns that
    help with data representation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 看过简单的数据表示后，让我们讨论有助于数据表示的设计模式。
- en: 'Design Pattern 1: Hashed Feature'
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 1：散列特征
- en: 'The Hashed Feature design pattern addresses three possible problems associated
    with categorical features: incomplete vocabulary, model size due to cardinality,
    and cold start. It does so by grouping the categorical features and accepting
    the trade-off of collisions in the data representation.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 散列特征设计模式解决了与分类特征相关的三个可能的问题：不完整的词汇表、模型大小由于基数、以及冷启动。它通过分组分类特征来做到这一点，并接受数据表示中碰撞的权衡。
- en: Problem
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: One-hot encoding a categorical input variable requires knowing the vocabulary
    beforehand. This is not a problem if the input variable is something like the
    language a book is written in or the day of the week that traffic level is being
    predicted.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对分类输入变量进行独热编码需要预先知道词汇表。如果输入变量类似于书写语言或预测交通量的星期几，这不是问题。
- en: 'What if the categorical variable in question is something like the `hospital_id`
    of where the baby is born or the `physician_id` of the person delivering the baby?
    Categorical variables like these pose a few problems:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所讨论的分类变量是类似于`hospital_id`（婴儿出生地的医院编号）或`physician_id`（接生的医生编号）这样的内容，这些分类变量会带来一些问题：
- en: Knowing the vocabulary requires extracting it from the training data. Due to
    random sampling, it is possible that the training data does not contain all the
    possible hospitals or physicians. The vocabulary might be *incomplete*.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学会词汇需要从训练数据中提取。由于随机抽样，训练数据可能不包含所有可能的医院或医生。词汇可能是*不完整*的。
- en: The categorical variables have *high cardinality*. Instead of having feature
    vectors with three languages or seven days, we have feature vectors whose length
    is in the thousands to millions. Such feature vectors pose several problems in
    practice. They involve so many weights that the training data may be insufficient.
    Even if we can train the model, the trained model will require a lot of space
    to store because the entire vocabulary is needed at serving time. Thus, we may
    not be able to deploy the model on smaller devices.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类变量具有*高基数*。与具有三种语言或七天的特征向量不同，我们的特征向量长度可能达到数千到数百万。这种特征向量在实践中存在几个问题。它们涉及如此多的权重，以至于训练数据可能不足。即使我们可以训练模型，训练好的模型在服务时也需要大量空间来存储，因为整个词汇表在服务时都是必需的。因此，我们可能无法在较小的设备上部署模型。
- en: After the model is placed into production, new hospitals might be built and
    new physicians hired. The model will be unable to make predictions for these,
    and so a separate serving infrastructure will be required to handle such *cold-start*
    problems.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型投入生产后，可能会建造新的医院并雇佣新的医生。模型将无法预测这些情况，因此需要一个单独的服务基础设施来处理这些*冷启动*问题。
- en: Tip
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Even with simple representations like one-hot encoding, it is worth anticipating
    the cold-start problem and explicitly reserving all zeros for out-of-vocabulary
    inputs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 即使像单热编码这样简单的表示法，也值得预见冷启动问题，并明确保留所有零值以用于词汇外输入。
- en: 'As a concrete example, let’s take the problem of predicting the arrival delay
    of a flight. One of the inputs to the model is the departure airport. There were,
    at the time the dataset was collected, 347 airports in the United States:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 作为具体例子，让我们来看一下预测航班到达延误的问题。模型的输入之一是出发机场。在收集数据集时，美国有347个机场：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Some airports had as few as one to three flights over the entire time period,
    and so we expect that the training data vocabulary will be incomplete. 347 is
    large enough that the feature will be quite sparse, and it is certainly the case
    that new airports will get built. All three problems (incomplete vocabulary, high
    cardinality, cold start) will exist if we one-hot encode the departure airport.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机场在整个时间段内只有一到三次航班，因此我们预计训练数据词汇表将不完整。347足够大，特征将非常稀疏，新机场肯定会建造。如果我们对出发机场进行单热编码，这三个问题（不完整的词汇表，高基数，冷启动）都会存在。
- en: The airline dataset, like the natality dataset and nearly all the other datasets
    that we use in this book for illustration, is a [public dataset in BigQuery](https://oreil.ly/lgcKA),
    so you can try the query out. At the time we are writing this, 1 TB/month of querying
    is free, and there is a sandbox available so that you can use BigQuery up to this
    limit without putting down a credit card. We encourage you to bookmark our GitHub
    repository. For example, see the [notebook](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/hashed_feature.ipynb)
    in GitHub for the full code.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 航空数据集，如出生数据集和我们在本书中用于说明的几乎所有其他数据集，都是[BigQuery中的公共数据集](https://oreil.ly/lgcKA)，因此您可以尝试查询。在我们编写本文时，每月1
    TB的查询免费，还有一个沙盒可供使用，因此您可以在不用信用卡的情况下使用BigQuery达到此限制。我们建议您收藏我们的GitHub存储库。例如，查看GitHub中的[notebook](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/hashed_feature.ipynb)以获取完整的代码。
- en: Solution
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'The Hashed Feature design pattern represents a categorical input variable by
    doing the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希特征设计模式通过以下方式表示分类输入变量：
- en: Converting the categorical input into a unique string. For the departure airport,
    we can use [the three-letter IATA code](https://oreil.ly/B8nLw) for the airport.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分类输入转换为唯一字符串。对于出发机场，我们可以使用[三字母IATA代码](https://oreil.ly/B8nLw)。
- en: Invoking a deterministic (no random seeds or salt) and portable (so that the
    same algorithm can be used in both training and serving) hashing algorithm on
    the string.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对字符串应用确定性（无随机种子或盐）和可移植（以便在训练和服务中都可以使用相同算法）的哈希算法。
- en: Taking the remainder when the hash result is divided by the desired number of
    buckets. Typically, the hashing algorithm returns an integer that can be negative
    and the modulo of a negative integer is negative. So, the absolute value of the
    result is taken.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取哈希结果除以所需的桶数得到余数。通常，哈希算法返回一个整数，可能为负数，而负数的模仍为负数。因此，需要取结果的绝对值。
- en: 'In BigQuery SQL, these steps are achieved like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在BigQuery SQL中，可以通过以下方式实现这些步骤：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `FARM_FINGERPRINT` function uses FarmHash, a family of hashing algorithms
    that is deterministic, [well-distributed](https://github.com/google/farmhash/blob/master/Understanding_Hash_Functions),
    and for which implementations are [available](https://github.com/google/farmhash)
    in a number of programming languages.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`FARM_FINGERPRINT` 函数使用 FarmHash，一系列确定性、[分布良好](https://github.com/google/farmhash/blob/master/Understanding_Hash_Functions)的哈希算法，并且这些算法的实现在多种编程语言中都有[提供](https://github.com/google/farmhash)。'
- en: 'In TensorFlow, these steps are implemented by the `feature_column` function:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，这些步骤由 `feature_column` 函数实现：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For example, [Table 2-1](#the_farmhash_of_some_iata_airport_codes) shows the
    FarmHash of some IATA airport codes when hashed into 3, 10, and 1,000 buckets.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[表格 2-1](#the_farmhash_of_some_iata_airport_codes) 显示了一些 IATA 机场代码在散列到 3、10
    和 1,000 个桶时的 FarmHash。
- en: Table 2-1\. The FarmHash of some IATA airport codes when hashed into different
    numbers of buckets
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2-1\. 将一些 IATA 机场代码散列到不同数量的桶时的 FarmHash
- en: '| Row | departure_airport | hash3 | hash10 | hash1000 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 行 | 出发机场 | hash3 | hash10 | hash1000 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | DTW | 1 | 3 | 543 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 1 | DTW | 1 | 3 | 543 |'
- en: '| 2 | LBB | 2 | 9 | 709 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 2 | LBB | 2 | 9 | 709 |'
- en: '| 3 | SNA | 2 | 7 | 587 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 3 | SNA | 2 | 7 | 587 |'
- en: '| 4 | MSO | 2 | 7 | 737 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 4 | MSO | 2 | 7 | 737 |'
- en: '| 5 | ANC | 0 | 8 | 508 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 5 | ANC | 0 | 8 | 508 |'
- en: '| 6 | PIT | 1 | 7 | 267 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 6 | PIT | 1 | 7 | 267 |'
- en: '| 7 | PWM | 1 | 9 | 309 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 7 | PWM | 1 | 9 | 309 |'
- en: '| 8 | BNA | 1 | 4 | 744 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 8 | BNA | 1 | 4 | 744 |'
- en: '| 9 | SAF | 1 | 2 | 892 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 9 | SAF | 1 | 2 | 892 |'
- en: '| 10 | IPL | 2 | 1 | 591 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 10 | IPL | 2 | 1 | 591 |'
- en: Why It Works
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么它有效
- en: Assume that we have chosen to hash the airport code using 10 buckets (hash10
    in [Table 2-1](#the_farmhash_of_some_iata_airport_codes)). How does this address
    the problems we identified?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们选择使用 10 个桶对机场代码进行散列（hash10 在 [表格 2-1](#the_farmhash_of_some_iata_airport_codes)
    中）。这如何解决我们所识别的问题？
- en: Out-of-vocabulary input
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超出词汇表的输入
- en: Even if an airport with a handful of flights is not part of the training dataset,
    its hashed feature value will be in the range [0–9]. Therefore, there is no resilience
    problem during serving—the unknown airport will get the predictions corresponding
    with other airports in the hash bucket. The model will not error out.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 即使一个只有少数航班的机场不在训练数据集中，其散列特征值将在 [0–9] 范围内。因此，在服务期间不存在韧性问题——未知机场将获得与哈希桶中其他机场对应的预测。模型不会出错。
- en: If we have 347 airports, an average of 35 airports will get the same hash bucket
    code if we hash it into 10 buckets. An airport that is missing from the training
    dataset will “borrow” its characteristics from the other similar ~35 airports
    in the hash bucket. Of course, the prediction for a missing airport won’t be accurate
    (it is unreasonable to expect accurate predictions for unknown inputs), but it
    will be in the right range.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有 347 个机场，如果将其散列到 10 个桶中，平均每个桶将有相同哈希桶代码的大约 35 个机场。训练数据集中缺失的机场将从哈希桶中的其他相似
    ~35 个机场中“借用”其特征。当然，对于未知输入的预测不会准确（期望未知输入的准确预测是不合理的），但它将在正确范围内。
- en: Choose the number of hash buckets by balancing the need to handle out-of-vocabulary
    inputs reasonably and the need to have the model accurately reflect the categorical
    input. With 10 hash buckets, ~35 airports get commingled. A good rule of thumb
    is to choose the number of hash buckets such that each bucket gets about five
    entries. In this case, that would mean that 70 hash buckets is a good compromise.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过平衡处理合理的超出词汇表输入的需求和准确反映分类输入的需求来选择哈希桶的数量。使用 10 个哈希桶，大约有 ~35 个机场混合在一起。一个经验法则是选择哈希桶的数量，使每个桶大约有五个条目。在这种情况下，选择
    70 个哈希桶是一个很好的折衷方案。
- en: High cardinality
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高基数
- en: It’s easy to see that the high cardinality problem is addressed as long as we
    choose a small enough number of hash buckets. Even if we have millions of airports
    or hospitals or physicians, we can hash them into a few hundred buckets, thus
    keeping the system’s memory and model size requirements practical.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们选择足够少的哈希桶数量，就能解决高基数问题。即使我们有数百万个机场、医院或医生，我们也可以将它们哈希到几百个桶中，从而保持系统的内存和模型大小要求实用。
- en: We don’t need to store the vocabulary because the transformation code is independent
    of the actual data value and the core of the model only deals with `num_buckets`
    inputs, not the full vocabulary.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要存储词汇表，因为转换代码与实际数据值无关，模型的核心只处理 `num_buckets` 输入，而不是整个词汇表。
- en: It is true that hashing is lossy—since we have 347 airports, an average of 35
    airports will get the same hash bucket code if we hash it into 10 buckets. When
    the alternative is to discard the variable because it is too wide, though, a lossy
    encoding is an acceptable compromise.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，哈希是损失的——因为我们有347个机场，如果我们将其哈希到10个桶中，平均每个桶将有35个机场共享相同的哈希桶代码。然而，当选择是舍弃这个变量因为它太宽时，损失编码是一个可以接受的折衷方案。
- en: Cold start
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 冷启动
- en: 'The cold-start situation is similar to the out-of-vocabulary situation. If
    a new airport gets added to the system, it will initially get the predictions
    corresponding to other airports in the hash bucket. As an airport gets popular,
    there will be more flights from that airport. As long as we periodically retrain
    the model, its predictions will start to reflect arrival delays from the new airport.
    This is discussed in more detail in the [“Design Pattern 18: Continued Model Evaluation”](ch05.xhtml#design_pattern_oneeight_continued_model)
    in [Chapter 5](ch05.xhtml#design_patterns_for_resilient_serving).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 冷启动情况类似于词汇外情况。如果新机场被添加到系统中，它最初会获得与哈希桶中其他机场对应的预测。随着一个机场变得流行，将会有更多的航班从该机场起飞。只要我们定期重新训练模型，其预测将开始反映来自新机场的到达延误情况。这在[“设计模式18：持续模型评估”](ch05.xhtml#design_pattern_oneeight_continued_model)中更详细地讨论了。
- en: By choosing the number of hash buckets such that each bucket gets about five
    entries, we can ensure that any bucket will have reasonable initial results.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择哈希桶的数量，使每个桶大约有五个条目，我们可以确保任何桶都具有合理的初始结果。
- en: Trade-Offs and Alternatives
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷与替代方案
- en: Most design patterns involve some kind of a trade-off, and the Hashed Feature
    design pattern is no exception. The key trade-off here is that we lose model accuracy.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数设计模式都涉及某种折衷，哈希特征设计模式也不例外。这里的关键折衷是我们会失去模型的准确性。
- en: Bucket collision
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 桶碰撞
- en: The modulo part of the Hashed Feature implementation is a lossy operation. By
    choosing a hash bucket size of 100, we are choosing to have 3–4 airports share
    a bucket. We are explicitly compromising on the ability to accurately represent
    the data (with a fixed vocabulary and one-hot encoding) in order to handle out-of-vocabulary
    inputs, cardinality/model size constraints, and cold-start problems. It is not
    a free lunch. Do not choose Hashed Feature if you know the vocabulary beforehand,
    if the vocabulary size is relatively small (in the thousands is acceptable for
    a dataset with millions of examples), and if cold start is not a concern.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希特征实现中的模数部分是一个损失的操作。通过选择100个哈希桶的大小，我们选择让3-4个机场共享一个桶。我们明确地在能够准确表示数据（使用固定词汇表和单热编码）的能力上进行了妥协，以处理词汇外输入、基数/模型大小约束和冷启动问题。这不是一顿免费的午餐。如果您事先知道词汇表，词汇表大小相对较小（对于包含数百万示例的数据集，数千是可以接受的），或者冷启动不是问题，请不要选择哈希特征。
- en: Note that we cannot simply increase the number of buckets to an extremely high
    number hoping to avoid collisions altogether. Even if we raise the number of buckets
    to 100,000 with only 347 airports, the probability that at least two airports
    share the same hash bucket is 45%—unacceptably high (see [Table 2-2](#the_expected_number_of_entries_per_buck)).
    Therefore, we should use Hashed Features only if we are willing to tolerate multiple
    categorical inputs sharing the same hash bucket value.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不能简单地将桶的数量增加到非常高的数字，希望完全避免碰撞。即使我们将桶的数量增加到10万个，仅有347个机场，至少两个机场共享相同哈希桶的概率为45%——这是不可接受的高概率（参见[表 2-2](#the_expected_number_of_entries_per_buck)）。因此，我们应仅在愿意容忍多个分类输入共享相同哈希桶值的情况下使用哈希特征。
- en: Table 2-2\. The expected number of entries per bucket and the probability of
    at least one collision when IATA airport codes are hashed into different numbers
    of buckets
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-2\. 每个桶预期条目数及当IATA机场代码哈希到不同数量的桶时至少一次碰撞的概率
- en: '| num_hash_buckets | entries_per_bucket | collision_prob |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| num_hash_buckets | entries_per_bucket | collision_prob |'
- en: '| --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 3 | 115.666667 | 1.000000 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 115.666667 | 1.000000 |'
- en: '| 10 | 34.700000 | 1.000000 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 34.700000 | 1.000000 |'
- en: '| 100 | 3.470000 | 1.000000 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 3.470000 | 1.000000 |'
- en: '| 1000 | 0.347000 | 1.000000 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 0.347000 | 1.000000 |'
- en: '| 10000 | 0.034700 | 0.997697 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 10000 | 0.034700 | 0.997697 |'
- en: '| 100000 | 0.003470 | 0.451739 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 100000 | 0.003470 | 0.451739 |'
- en: Skew
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏斜
- en: 'The loss of accuracy is particularly acute when the distribution of the categorical
    input is highly skewed. Consider the case of the hash bucket that contains ORD
    (Chicago, one of the busiest airports in the world). We can find this using the
    following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当分类输入的分布高度倾斜时，精度损失特别严重。考虑包含ORD（芝加哥，世界上最繁忙的机场之一）的哈希桶的情况。我们可以通过以下方式找到这个哈希桶：
- en: '[PRE14]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The result shows that while there are ~3.6 million flights from ORD, there
    are only ~67,000 flights from BTV (Burlington, Vermont):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，虽然ORD有约360万次航班，但BTV（佛蒙特州伯灵顿市）只有约67000次航班：
- en: '| departure_airport | num_flights |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| departure_airport | num_flights |'
- en: '| --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ORD | 3610491 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| ORD | 3610491 |'
- en: '| BTV | 66555 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| BTV | 66555 |'
- en: '| MCI | 597761 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| MCI | 597761 |'
- en: This indicates that, for all practical purposes, the model will impute the long
    taxi times and weather delays that Chicago experiences to the municipal airport
    in Burlington, Vermont! The model accuracy for BTV and MCI (Kansas City airport)
    will be quite poor because there are so many flights out of Chicago.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，从实际目的来看，模型将芝加哥经历的长时间出租车等待和天气延误归因于佛蒙特州伯灵顿市的市政机场！对于BTV和MCI（堪萨斯城机场），模型的准确率将非常低，因为芝加哥的航班数量如此之多。
- en: Aggregate feature
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚合特征
- en: In cases where the distribution of a categorical variable is skewed or where
    the number of buckets is so small that bucket collisions are frequent, we might
    find it helpful to add an aggregate feature as an input to our model. For example,
    for every airport, we could find the probability of on-time flights in the training
    dataset and add it as a feature to our model. This allows us to avoid losing the
    information associated with individual airports when we hash the airport codes.
    In some cases, we might be able to avoid using the airport name as a feature entirely,
    since the relative frequency of on-time flights might be sufficient.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类变量的分布偏斜或桶的数量过小导致桶碰撞频繁的情况下，我们可能会发现添加一个聚合特征作为模型输入很有帮助。例如，对于每个机场，我们可以找到训练数据集中准点航班的概率，并将其添加为模型的特征。这样一来，当我们对机场代码进行哈希时，就可以避免丢失与个别机场相关联的信息。在某些情况下，我们甚至可以完全避免使用机场名称作为特征，因为准点航班的相对频率可能已经足够。
- en: Hyperparameter tuning
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数调优
- en: 'Because of the trade-offs with bucket collision frequency, choosing the number
    of buckets can be difficult. It very often depends on the problem itself. Therefore,
    we recommend that you treat the number of buckets as a hyperparameter that is
    tuned:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于桶碰撞频率的权衡，选择桶的数量可能很困难。这往往取决于问题本身。因此，我们建议将桶的数量视为一个需要调优的超参数：
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Make sure that the number of buckets remains within a sensible range of the
    cardinality of the categorical variable being hashed.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 确保桶的数量保持在哈希的分类变量的基数合理范围内。
- en: Cryptographic hash
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密码哈希
- en: What makes the Hashed Feature lossy is the modulo part of the implementation.
    What if we were to avoid the modulo altogether? After all, the farm fingerprint
    has a fixed length (an INT64 is 64 bits), and so it can be represented using 64
    feature values, each of which is 0 or 1\. This is called *binary encoding*.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希特征的丢失性质来自于实现中的取模部分。如果我们完全避免取模会怎么样？毕竟，农场指纹具有固定长度（INT64为64位），因此可以用64个特征值来表示，每个特征值为0或1。这被称为*二进制编码*。
- en: 'However, binary encoding does not solve the problem of out-of-vocabulary inputs
    or cold start (only the problem of high cardinality). In fact, the bitwise coding
    is a red herring. If we don’t do a modulo, we can get a unique representation
    by simply encoding the three characters that form the IATA code (thus using a
    feature of length 3*26=78). The problem with this representation is immediately
    obvious: airports whose names start with the letter O have nothing in common when
    it comes to their flight delay characteristics—the encoding has created a *spurious
    correlation* between airports that start with the same letter. The same insight
    holds in binary space as well. Because of this, we do not recommend binary encoding
    of farm fingerprint values.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，二进制编码并不能解决词汇表外的输入或冷启动问题（只能解决高基数的问题）。实际上，比特编码是一个误导。如果我们不进行取模操作，只需对形成IATA代码的三个字符进行编码，即可得到唯一的表示（因此使用长度为3*26=78的特征）。这种表示的问题显而易见：以字母O开头的机场在航班延误特征上毫无共同之处——编码在相同字母开头的机场之间创建了一个*虚假相关性*。在二进制空间中也是如此。因此，我们不推荐对农场指纹值进行二进制编码。
- en: Binary encoding of an MD5 hash will not suffer from this spurious correlation
    problem because the output of an MD5 hash is uniformly distributed, and so the
    resulting bits will be uniformly distributed. However, unlike the Farm Fingerprint
    algorithm, the MD5 hash is not deterministic and not unique—it is a one-way hash
    and will have many unexpected collisions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: MD5哈希的二进制编码不会遭受这种虚假相关问题的困扰，因为MD5哈希的输出是均匀分布的，所以结果位将是均匀分布的。然而，与Farm Fingerprint算法不同，MD5哈希不是确定性的，也不是唯一的——它是单向哈希，并且会有许多意外的碰撞。
- en: In the Hashed Feature design pattern, we have to use a fingerprint hashing algorithm
    and not a cryptographic hashing algorithm. This is because the goal of a fingerprint
    function is to produce a deterministic and unique value. If you think about it,
    this is a key requirement of preprocessing functions in machine learning, since
    we need to apply the same function during model serving and get the same hashed
    value. A fingerprint function does not produce a uniformly distributed output.
    Cryptographic algorithms such as MD5 or SHA1 do produce uniformly distributed
    output, but they are not deterministic and are purposefully made to be computationally
    expensive. Therefore, a cryptographic hash is not usable in a feature engineering
    context where the hashed value computed for a given input during prediction has
    to be the same as the hash computed during training, and where the hash function
    should not slow down the machine learning model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在哈希特征设计模式中，我们必须使用指纹哈希算法而不是加密哈希算法。这是因为指纹函数的目标是产生确定性和唯一的值。如果考虑一下，这是机器学习预处理函数的关键要求，因为在模型服务期间我们需要应用相同的函数并获得相同的哈希值。指纹函数不会产生均匀分布的输出。像MD5或SHA1这样的加密算法会产生均匀分布的输出，但它们不是确定性的，并且被故意设计成计算昂贵。因此，在特征工程的上下文中，加密哈希不适用，因为在预测期间对于给定的输入计算的哈希值必须与训练期间计算的哈希值相同，而且哈希函数不应减慢机器学习模型的运行速度。
- en: Note
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The reason that MD5 is not deterministic is that a “salt” is typically added
    to the string to be hashed. The salt is a [random string added to each password](https://oreil.ly/cv7PS)
    to ensure that even if two users happen to use the same password, the hashed value
    in the database will be different. This is needed to thwart attacks based on “rainbow
    tables,” which are attacks that rely on dictionaries of commonly chosen passwords
    and that compare the hash of the known password against hashes in the database.
    As computational power has increased, it is possible to carry out a brute-force
    attack on every possible salt as well, and so modern cryptographic implementations
    do their hashing in a loop to increase the computational expense. Even if we were
    to turn off the salt and reduce the number of iterations to one, the MD5 hash
    is only one way. It won’t be unique.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: MD5不确定的原因在于典型情况下会向要进行哈希的字符串添加“盐”。盐是一个[添加到每个密码的随机字符串](https://oreil.ly/cv7PS)，以确保即使两个用户使用相同的密码，数据库中的哈希值也会不同。这是为了防止基于“彩虹表”的攻击，彩虹表依赖于常用密码的字典，并将已知密码的哈希与数据库中的哈希进行比较。随着计算能力的增加，现在可以对每个可能的盐进行暴力攻击，因此现代加密实现在循环中进行哈希以增加计算开销。即使我们关闭盐并将迭代次数减少到一次，MD5哈希也只是一种方式。它不会是唯一的。
- en: The bottom line is that we need to use a fingerprint hashing algorithm, and
    we need to modulo the resulting hash.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 底线是，我们需要使用指纹哈希算法，并对得到的哈希值进行取模。
- en: Order of operations
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 操作顺序
- en: 'Note that we do the modulo first, and then the absolute value:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们首先执行模运算，然后再执行绝对值：
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The order of `ABS`, `MOD`, and `FARM_FINGERPRINT` in the preceding snippet
    is important because the range of `INT64` is not symmetric. Specifically, its
    range is between `–9,223,372,036,854,775,808` and `9,223,372,036,854,775,807`
    (both inclusive). So, if we were to do:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面片段中`ABS`、`MOD`和`FARM_FINGERPRINT`的顺序很重要，因为`INT64`的范围不对称。具体来说，它的范围在`–9,223,372,036,854,775,808`和`9,223,372,036,854,775,807`之间（包括两者）。所以，如果我们执行：
- en: '[PRE17]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: we would run into a rare and likely unreproducible overflow error if the `FARM_FINGERPRINT`
    operation happened to return `–9,223,372,036,854,775,808` since its absolute value
    can not be represented using an `INT64!`
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`FARM_FINGERPRINT`操作返回`–9,223,372,036,854,775,808`，由于其绝对值无法用`INT64`表示，我们可能会遇到罕见且可能无法重现的溢出错误！
- en: Empty hash buckets
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 空散列桶
- en: Although unlikely, there is a remote possibility that even if we choose 10 hash
    buckets to represent 347 airports, one of the hash buckets will be empty. Therefore,
    when using hashed feature columns, it [may be beneficial](https://oreil.ly/xlwAH)
    to also use L2 regularization so that the weights associated with an empty bucket
    will be driven to near-zero. This way, if an out-of-vocabulary airport does fall
    into an empty bucket, it will not cause the model to become numerically unstable.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不太可能，但有一种遥远的可能性，即使我们选择了10个哈希桶来表示347个机场，其中一个哈希桶可能为空。因此，在使用哈希特征列时，[可能有利](https://oreil.ly/xlwAH)也使用L2正则化，以便与空桶关联的权重被推向接近零。这样，如果一个超出词汇表的机场确实掉入一个空桶中，它不会导致模型在数值上不稳定。
- en: 'Design Pattern 2: Embeddings'
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式2：嵌入
- en: Embeddings are a learnable data representation that map high-cardinality data
    into a lower-dimensional space in such a way that the information relevant to
    the learning problem is preserved. Embeddings are at the heart of modern-day machine
    learning and have various incarnations throughout the field.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是一种可学习的数据表示，将高基数数据映射到低维空间，以保留与学习问题相关的信息。嵌入是现代机器学习的核心，并在该领域中有各种各样的体现。
- en: Problem
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Machine learning models systematically look for patterns in data that capture
    how the properties of the model’s input features relate to the output label. As
    a result, the data representation of the input features directly affects the quality
    of the final model. While handling structured, numeric input is fairly straightforward,
    the data needed to train a machine learning model can come in myriad varieties,
    such as categorical features, text, images, audio, time series, and many more.
    For these data representations, we need a meaningful numeric value to supply our
    machine learning model so these features can fit within the typical training paradigm.
    Embeddings provide a way to handle some of these disparate data types in a way
    that preserves similarity between items and thus improves our model’s ability
    to learn those essential patterns.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型系统地寻找数据中的模式，捕捉模型输入特征与输出标签的属性关系。因此，输入特征的数据表示直接影响最终模型的质量。虽然处理结构化的数值输入相对简单，但用于训练机器学习模型的数据可以是多种多样的，如分类特征、文本、图像、音频、时间序列等等。对于这些数据表示，我们需要提供一个有意义的数值供给我们的机器学习模型，以便这些特征能够符合典型的训练范式。嵌入提供了一种处理这些不同数据类型的方式，可以保持项目之间的相似性，从而提高我们模型学习这些重要模式的能力。
- en: 'One-hot encoding is a common way to represent categorical input variables.
    For example, consider the plurality input in the natality dataset.^([3](ch02.xhtml#ch01fn3))
    This is a categorical input that has six possible values: `[''Single(1)''`, `''Multiple(2+)''`,
    `''Twins(2)''`, `''Triplets(3)''`, `''Quadruplets(4)''`, `''Quintuplets(5)'']`.
    We can handle this categorical input using a one-hot encoding that maps each potential
    input string value to a unit vector in R⁶, as shown in [Table 2-3](#_an_example_of_one-hot_encoding_categor).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 单热编码是表示分类输入变量的常用方式。例如，考虑出生数据集中的多重输入。^([3](ch02.xhtml#ch01fn3)) 这是一个具有六种可能值的分类输入：`['Single(1)'`,
    `'Multiple(2+)'`, `'Twins(2)'`, `'Triplets(3)'`, `'Quadruplets(4)'`, `'Quintuplets(5)']`。我们可以使用单热编码来处理这种分类输入，将每个潜在的输入字符串值映射到
    R⁶ 中的单位向量，如[表 2-3](#_an_example_of_one-hot_encoding_categor)所示。
- en: Table 2-3\. An example of one-hot encoding categorical inputs for the natality
    dataset
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-3\. 对出生数据集进行分类输入单热编码的示例
- en: '| Plurality | One-hot encoding |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 多重输入 | 单热编码 |'
- en: '| --- | --- |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Single(1) | [1,0,0,0,0,0] |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Single(1) | [1,0,0,0,0,0] |'
- en: '| Multiple(2+) | [0,1,0,0,0,0] |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| Multiple(2+) | [0,1,0,0,0,0] |'
- en: '| Twins(2) | [0,0,1,0,0,0] |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Twins(2) | [0,0,1,0,0,0] |'
- en: '| Triplets(3) | [0,0,0,1,0,0] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Triplets(3) | [0,0,0,1,0,0] |'
- en: '| Quadruplets(4) | [0,0,0,0,1,0] |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Quadruplets(4) | [0,0,0,0,1,0] |'
- en: '| Quintuplets(5) | [0,0,0,0,0,1] |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Quintuplets(5) | [0,0,0,0,0,1] |'
- en: When encoded in this way, we need six dimensions to represent each of the different
    categories. Six dimensions may not be so bad, but what if we had many, many more
    categories to consider?
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式编码，我们需要六个维度来表示不同的类别。六个维度可能并不算太多，但如果我们需要考虑更多的类别呢？
- en: For example, what if our dataset consisted of customers’ view history of our
    video database and our task is to suggest a list of new videos given customers’
    previous video interactions? In this scenario, the `customer_id` field could have
    millions of unique entries. Similarly, the `video_id` of previously watched videos
    could contain thousands of entries as well. One-hot encoding *high-cardinality*
    categorical features like `video_ids` or `customer_ids` as inputs to a machine
    learning model leads to a sparse matrix that isn’t well suited for a number of
    machine learning algorithms.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们的数据集包含顾客对视频数据库的观看历史，我们的任务是根据顾客以前的视频互动建议一组新视频？在这种情况下，`customer_id`字段可能有数百万个唯一条目。同样，先前观看视频的`video_id`也可能包含数千个条目。将高基数分类特征如`video_ids`或`customer_ids`使用一位有效编码作为机器学习模型的输入，会导致一个对多种机器学习算法不太适用的稀疏矩阵。
- en: The second problem with one-hot encoding is that it treats the categorical variables
    as being *independent*. However, the data representation for twins should be close
    to the data representation for triplets and quite far away from the data representation
    for quintuplets. A multiple is most likely a twin, but could be a triplet. As
    an example, [Table 2-4](#using_lower_dimensionality_embeddings_t) shows an alternate
    representation of the plurality column in a lower dimension that captures this
    *closeness* relationship.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一位有效编码的第二个问题是它将分类变量视为*独立*的。然而，对双胞胎的数据表示应接近对三胞胎的数据表示，而与五胞胎的数据表示相距甚远。多胞胎最有可能是双胞胎，但可能是三胞胎。例如，[表 2-4](#using_lower_dimensionality_embeddings_t)显示了在较低维度中捕获这种*紧密*关系的多数列的替代表示。
- en: Table 2-4\. Using lower dimensionality embeddings to represent the plurality
    column in the natality dataset.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-4\. 使用较低维度的嵌入表示出生数据集中的多数列。
- en: '| Plurality | Candidate encoding |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Plurality | 候选编码 |'
- en: '| --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Single(1) | [1.0,0.0] |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Single(1) | [1.0,0.0] |'
- en: '| Multiple(2+) | [0.0,0.6] |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Multiple(2+) | [0.0,0.6] |'
- en: '| Twins(2) | [0.0,0.5] |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Twins(2) | [0.0,0.5] |'
- en: '| Triplets(3) | [0.0,0.7] |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Triplets(3) | [0.0,0.7] |'
- en: '| Quadruplets(4) | [0.0,0.8] |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Quadruplets(4) | [0.0,0.8] |'
- en: '| Quintuplets(5) | [0.0,0.9] |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| Quintuplets(5) | [0.0,0.9] |'
- en: These numbers are arbitrary of course. But is it possible to learn the best
    possible representation of the plurality column using just two dimensions for
    the natality problem? That is the problem that the Embeddings design pattern solves.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字当然是任意的。但是否可能仅使用两个维度学习出最佳的多数列表示法来解决出生问题？这就是嵌入设计模式解决的问题。
- en: The same problem of high cardinality and dependent data also occurs in images
    and text. Images consist of thousands of pixels, which are not independent of
    one another. Natural language text is drawn from a vocabulary in the tens of thousands
    of words, and a word like `walk` is closer to the word `run` than to the word
    `book`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 高基数和相关数据的相同问题也存在于图像和文本中。图像由数千个像素组成，这些像素并不相互独立。自然语言文本来自数万个单词的词汇表，像`walk`这样的词比`book`更接近`run`。
- en: Solution
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: The Embeddings design pattern addresses the problem of representing high-cardinality
    data densely in a lower dimension by passing the input data through an embedding
    layer that has trainable weights. This will map the high-dimensional, categorical
    input variable to a real-valued vector in some low-dimensional space. The weights
    to create the dense representation are learned as part of the optimization of
    the model (see [Figure 2-5](#the_weights_of_an_embedding_layer_are_l)). In practice,
    these embeddings end up capturing closeness relationships in the input data.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入设计模式解决了在较低维度中密集表示高基数数据的问题，通过将输入数据传递到一个具有可训练权重的嵌入层，将高维度的分类输入变量映射到某个低维度空间的实值向量中。创建密集表示的权重是作为模型优化的一部分学习的（见[图 2-5](#the_weights_of_an_embedding_layer_are_l)）。实际上，这些嵌入最终捕获了输入数据中的紧密关系。
- en: '![The weights of an embedding layer are learned as parameters during training.](Images/mldp_0205.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![嵌入层的权重在训练过程中作为参数学习。](Images/mldp_0205.png)'
- en: Figure 2-5\. The weights of an embedding layer are learned as parameters during
    training.
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 嵌入层的权重在训练过程中作为参数学习。
- en: Tip
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Because embeddings capture closeness relationships in the input data in a lower-dimensional
    representation, we can use an embedding layer as a replacement for clustering
    techniques (e.g., customer segmentation) and dimensionality reduction methods
    like principal components analysis (PCA). Embedding weights are determined in
    the main model training loop, thus saving the need to cluster or do PCA beforehand.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 因为嵌入捕捉输入数据中的接近关系，并以较低维度的表示，我们可以将嵌入层用作聚类技术（例如客户分割）和主成分分析（PCA）等降维方法的替代品。嵌入权重在主模型训练循环中确定，因此不需要事先进行聚类或进行PCA。
- en: The weights in the embedding layer would be learned as part of the gradient
    descent procedure when training the natality model.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练出生模型时，嵌入层中的权重会作为梯度下降过程的一部分而被学习。
- en: At the end of training, the weights of the embedding layer might be such that
    the encoding for the categorical variables is as shown in [Table 2-5](#one-hot_and_learned_encodings_for_the_p).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 训练结束时，嵌入层的权重可能是表 2-5 中所示分类变量的编码，详情请见[表 2-5](#one-hot_and_learned_encodings_for_the_p)。
- en: Table 2-5\. One-hot and learned encodings for the plurality column in the natality
    dataset
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-5\. 出生数据集中多胞胎列的单热编码和学习编码
- en: '| Plurality | One-hot encoding | Learned encoding |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 多胞胎 | 单热编码 | 学习编码 |'
- en: '| --- | --- | --- |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Single(1) | [1,0,0,0,0,0] | [0.4, 0.6] |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 单胞胎(1) | [1,0,0,0,0,0] | [0.4, 0.6] |'
- en: '| Multiple(2+) | [0,1,0,0,0,0] | [0.1, 0.5] |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 多胞胎(2+) | [0,1,0,0,0,0] | [0.1, 0.5] |'
- en: '| Twins(2) | [0,0,1,0,0,0] | [-0.1, 0.3] |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 双胞胎(2) | [0,0,1,0,0,0] | [-0.1, 0.3] |'
- en: '| Triplets(3) | [0,0,0,1,0,0] | [-0.2, 0.5] |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 三胞胎(3) | [0,0,0,1,0,0] | [-0.2, 0.5] |'
- en: '| Quadruplets(4) | [0,0,0,0,1,0] | [-0.4, 0.3] |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 四胞胎(4) | [0,0,0,0,1,0] | [-0.4, 0.3] |'
- en: '| Quintuplets(5) | [0,0,0,0,0,1] | [-0.6, 0.5] |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 五胞胎(5) | [0,0,0,0,0,1] | [-0.6, 0.5] |'
- en: The embedding maps a sparse, one-hot encoded vector to a dense vector in R².
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入将稀疏的单热编码向量映射到 R² 中的密集向量。
- en: 'In TensorFlow, we first construct a categorical feature column for the feature,
    then wrap that in an embedding feature column. For example, for our plurality
    feature, we would have:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，我们首先为特征构建一个分类特征列，然后将其包装在嵌入特征列中。例如，对于我们的多胞胎特征，我们将会有：
- en: '[PRE18]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The resulting feature column (`plurality_embed`) is used as input to the downstream
    nodes of the neural network instead of the one-hot encoded feature column (`plurality`).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 结果特征列 (`plurality_embed`) 作为输入传递给神经网络的下游节点，而不是单热编码特征列 (`plurality`)。
- en: Text embeddings
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本嵌入
- en: Text provides a natural setting where it is advantageous to use an embedding
    layer. Given the cardinality of a vocabulary (often on the order of tens of thousands
    of words), one-hot encoding each word isn’t practical. This would create an incredibly
    large (high-dimensional) and sparse matrix for training. Also, we’d like similar
    words to have embeddings close by and unrelated words to be far away in embedding
    space. Therefore, we use a dense word embedding to vectorize the discrete text
    input before passing to our model.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 文本提供了一个自然的背景，使用嵌入层是有利的。考虑到词汇的基数（通常是数万个词），使用单热编码每个词并不实际。这将创建一个非常大（高维度）且稀疏的矩阵进行训练。此外，我们希望相似的词在嵌入空间中靠近，不相关的词则远离。因此，在将离散文本输入模型之前，我们使用密集的词嵌入来向量化。
- en: To implement a text embedding in Keras, we first create a tokenization for each
    word in our vocabulary, as shown in [Figure 2-6](#the_tokenizer_creates_a_lookup_table_th).
    Then we use this tokenization to map to an embedding layer, similar to how it
    was done for the plurality column.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中实现文本嵌入，我们首先为词汇中的每个单词创建一个标记化，如图 2-6 所示。然后，我们使用这个标记化将其映射到嵌入层，类似于对多胞胎列的处理。
- en: '![The tokenizer creates a lookup table that maps each word to an index.](Images/mldp_0206.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![分词器创建一个查找表，将每个单词映射到一个索引。](Images/mldp_0206.png)'
- en: Figure 2-6\. The tokenizer creates a lookup table that maps each word to an
    index.
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 分词器创建一个查找表，将每个单词映射到一个索引。
- en: 'The tokenization is a lookup table that maps each word in our vocabulary to
    an index. We can think of this as a one-hot encoding of each word where the tokenized
    index is the location of the nonzero element in the one-hot encoding. This requires
    a full pass over the entire dataset (let’s assume these consist of titles of articles^([4](ch02.xhtml#ch01fn4)))
    to create the lookup table and can be done in Keras. The complete code can be
    found in the [repository](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/embeddings.ipynb)
    for this book:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Tokenization 是一个查找表，将我们词汇表中的每个单词映射到一个索引。我们可以将其视为每个单词的一热编码，其中 tokenized 索引是一热编码中非零元素的位置。这需要对整个数据集进行完整遍历（假设这些数据集由文章标题组成^([4](ch02.xhtml#ch01fn4))），以创建查找表，并可以在
    Keras 中完成。有关本书的完整代码，请查看[存储库](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/embeddings.ipynb)：
- en: '[PRE19]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here we can use the `Tokenizer` class in the *keras.preprocessing.text* library.
    The call to `fit_on_texts` creates a lookup table that maps each of the words
    found in our titles to an index. By calling `tokenizer.index_word`, we can examine
    this lookup table directly:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以使用 *keras.preprocessing.text* 库中的 `Tokenizer` 类。调用 `fit_on_texts` 方法会创建一个查找表，将标题中的每个单词映射到一个索引。通过调用
    `tokenizer.index_word`，我们可以直接查看这个查找表：
- en: '[PRE20]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can then invoke this mapping with the `texts_to_sequences` method of our
    tokenizer. This maps each sequence of words in the text input being represented
    (here, we assume that they are titles of articles) to a sequence of tokens corresponding
    to each word as in [Figure 2-7](#_using_the_tokenizercomma_each_title_is):'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用我们的 tokenizer 的 `texts_to_sequences` 方法调用这个映射。这将每个输入文本中的单词序列（这里假设它们是文章的标题）映射到与每个单词对应的
    token 序列，就像[图 2-7](#_using_the_tokenizercomma_each_title_is)中描述的那样。
- en: '[PRE21]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Using the tokenizer, each title is mapped to a sequence of integer index
    values. ](Images/mldp_0207.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![使用 tokenizer，每个标题被映射到一个整数索引值序列。](Images/mldp_0207.png)'
- en: Figure 2-7\. Using the tokenizer, each title is mapped to a sequence of integer
    index values.
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 使用 tokenizer，每个标题被映射到一个整数索引值序列。
- en: 'The tokenizer contains other relevant information that we will use later for
    creating an embedding layer. In particular, `VOCAB_SIZE` captures the number of
    elements of the index lookup table and `MAX_LEN` contains the maximum length of
    the text strings in the dataset:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Tokenizer 还包含其他相关信息，稍后我们将用于创建嵌入层。特别是，`VOCAB_SIZE` 捕获了索引查找表的元素数量，而 `MAX_LEN`
    包含了数据集中文本字符串的最大长度：
- en: '[PRE22]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Before creating the model, it is necessary to preprocess the titles in the
    dataset. We’ll need to pad the elements of our title to feed into the model. Keras
    has the helper functions `pad_sequence` for that on the top of the tokenizer methods.
    The function `create_sequences` takes both titles as well as the maximum sentence
    length as input and returns a list of the integers corresponding to our tokens
    padded to the sentence maximum length:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建模型之前，需要对数据集中的标题进行预处理。我们将需要填充标题的元素以输入模型。Keras 提供了 `pad_sequence` 辅助函数来完成这一操作。函数
    `create_sequences` 接受标题和最大句子长度作为输入，并返回整数列表，这些整数对应于我们的 token 并填充到句子的最大长度：
- en: '[PRE23]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we’ll build a deep neural network (DNN) model in Keras that implements
    a simple embedding layer to transform the word integers into dense vectors. The
    Keras `Embedding` layer can be thought of as a map from the integer indices of
    specific words to dense vectors (their embeddings). The dimensionality of the
    embedding is determined by `output_dim`. The argument `input_dim` indicates the
    size of the vocabulary, and `input_shape` indicates the length of input sequences.
    Since here we have padded the titles before passing to the model, we set `input_shape=[MAX_LEN]`:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在 Keras 中构建一个深度神经网络（DNN）模型，实现一个简单的嵌入层，将单词整数映射到密集向量。Keras 的 `Embedding`
    层可以被视为从特定单词的整数索引到密集向量（它们的嵌入）的映射。嵌入的维度由 `output_dim` 决定。参数 `input_dim` 表示词汇表的大小，`input_shape`
    表示输入序列的长度。因为我们在传递给模型之前对标题进行了填充，所以我们设置 `input_shape=[MAX_LEN]`：
- en: '[PRE24]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that we need to put a custom Keras Lambda layer in between the embedding
    layer and the dense softmax layer to average the word vectors returned by the
    embedding layer. This is the average that’s fed to the dense softmax layer. By
    doing so, we create a model that is simple but that loses information about the
    word order, creating a model that sees sentences as a “bag of words.”
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在嵌入层和密集 softmax 层之间，我们需要添加一个自定义的 Keras Lambda 层，以对嵌入层返回的单词向量进行平均。这个平均值被馈送到密集
    softmax 层。通过这样做，我们创建了一个简单的模型，但是失去了单词顺序的信息，从而创建了一个将句子视为“词袋”的模型。
- en: Image embeddings
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像嵌入
- en: While text deals with very sparse input, other data types, such as images or
    audio, consist of dense, high-dimensional vectors, usually with multiple channels
    containing raw pixel or frequency information. In this setting, an Embedding captures
    a relevant, low-dimensional representation of the input.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然文本处理的输入非常稀疏，但其他数据类型，如图像或音频，由稠密、高维度向量组成，通常具有多个包含原始像素或频率信息的通道。在这种情况下，嵌入捕获了输入的相关低维表示。
- en: For image embeddings, a complex convolutional neural network—like Inception
    or ResNet—is first trained on a large image dataset, like ImageNet, containing
    millions of images and thousands of possible classification labels. Then, the
    last softmax layer is removed from the model. Without the final softmax classifier
    layer, the model can be used to extract a feature vector for a given input. This
    feature vector contains all the relevant information of the image so it is essentially
    a low-dimensional embedding of the input image.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像嵌入，首先在大型图像数据集（如包含数百万图像和数千个可能分类标签的 ImageNet）上训练复杂的卷积神经网络（如 Inception 或 ResNet）。然后，从模型中删除最后一个
    softmax 层。没有最终的 softmax 分类器层，模型可以用于提取给定输入的特征向量。这个特征向量包含图像的所有相关信息，因此实质上是输入图像的低维嵌入。
- en: Similarly, consider the task of image captioning, that is, generating a textual
    caption of a given image, shown in [Figure 2-8](#for_the_image_translation_taskcomma_the).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，考虑图像字幕生成任务，即生成给定图像的文本说明，如图[2-8](#for_the_image_translation_taskcomma_the)所示。
- en: '![For the image translation task, the encoder produces a low-dimensional embedding
    representation of the image.](Images/mldp_0208.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![对于图像翻译任务，编码器生成图像的低维嵌入表示。](Images/mldp_0208.png)'
- en: Figure 2-8\. For the image translation task, the encoder produces a low-dimensional
    embedding representation of the image.
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8\. 对于图像翻译任务，编码器生成图像的低维嵌入表示。
- en: By training this model architecture on a massive dataset of image/caption pairs,
    the encoder learns an efficient vector representation for images. The decoder
    learns how to translate this vector to a text caption. In this sense, the encoder
    becomes an Image2Vec embedding machine.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在大量图像/说明对的数据集上训练此模型架构，编码器学习了图像的有效向量表示。解码器学习如何将这个向量翻译成文本说明。在这个意义上，编码器成为了一个
    Image2Vec 嵌入机器。
- en: Why It Works
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么它有效
- en: The embedding layer is just another hidden layer of the neural network. The
    weights are then associated to each of the high-cardinality dimensions, and the
    output is passed through the rest of the network. Therefore, the weights to create
    the embedding are learned through the process of gradient descent just like any
    other weights in the neural network. This means that the resulting vector embeddings
    represent the most efficient low-dimensional representation of those feature values
    with respect to the learning task.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层只是神经网络的另一个隐藏层。然后，将权重与每个高基数维度相关联，并将输出通过网络的其余部分。因此，通过梯度下降的过程学习用于创建嵌入的权重就像神经网络中的任何其他权重一样。这意味着生成的向量嵌入代表了相对于学习任务最有效的低维表示的特征值。
- en: While this improved embedding ultimately aids the model, the embeddings themselves
    have inherent value and allow us to gain additional insight into our dataset.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种改进的嵌入最终有助于模型，但嵌入本身具有固有的价值，并允许我们深入了解数据集。
- en: Consider again the customer video dataset. By only using one-hot encoding, any
    two separate users, user_i and user_j, will have the same similarity measure.
    Similarly, the dot product or cosine similarity for any two distinct six-dimensional
    one-hot encodings of birth plurality would have zero similarity. This makes sense
    since the one-hot encoding is essentially telling our model to treat any two different
    birth pluralities as separate and unrelated. For our dataset of customers and
    video watches, we lose any notion of similarity between customers or videos. But
    this doesn’t feel quite right. Two different customers or videos likely do have
    similarities between them. The same goes for birth plurality. The occurrence of
    quadruplets and quintuplets likely affects the birthweight in a statistically
    similar way as opposed to single child birthweights (see [Figure 2-9](#by_forcing_our_categorical_variable_int)).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑客户视频数据集。仅使用独热编码时，任意两个独立用户，user_i 和 user_j，将具有相同的相似度测量。类似地，出生多胞胎的六维独热编码的任何两个不同点积或余弦相似度将具有零相似度。这是有道理的，因为独热编码本质上告诉我们的模型将任何两个不同的出生多胞胎视为分开且不相关的。对于我们的客户和视频观看数据集，我们失去了客户或视频之间的任何相似性概念。但这感觉不太对。两个不同的客户或视频可能确实存在相似之处。出生多胞胎也是如此。四胞胎和五胞胎的出现可能会以与单胞胎出生体重统计上相似的方式影响出生体重（参见[图 2-9](#by_forcing_our_categorical_variable_int)）。
- en: '![By forcing our categorical variable into a lower-dimensional embedding space,
    we can also learn relationships between the different categories.](Images/mldp_0209.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![通过将我们的分类变量强制投射到低维嵌入空间，我们还可以学习不同类别之间的关系。](Images/mldp_0209.png)'
- en: Figure 2-9\. By forcing our categorical variable into a lower-dimensional embedding
    space, we can also learn relationships between the different categories.
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9\. 通过将我们的分类变量强制投射到低维嵌入空间，我们还可以学习不同类别之间的关系。
- en: When computing the similarity of plurality categories as one-hot encoded vectors,
    we obtain the identity matrix since each category is treated as a distinct feature
    (see [Table 2-6](#when_features_are_one-hot_encodedcomma)).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算以独热编码向量表示的多类别相似性时，我们得到了单位矩阵，因为每个类别被视为一个独特的特征（参见[表格 2-6](#when_features_are_one-hot_encodedcomma)）。
- en: Table 2-6\. When features are one-hot encoded, the similarity matrix is just
    the identity matrix
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2-6\. 当特征被独热编码时，相似性矩阵就是单位矩阵
- en: '|  | Single(1) | Multiple(2+) | Twins(2) | Triplets(3) | Quadruplets(4) | Quintuplets(5)
    |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | 单胞胎(1) | 多个(2+) | 双胞胎(2) | 三胞胎(3) | 四胞胎(4) | 五胞胎(5) |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| **Single(1)** | 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| **单胞胎(1)** | 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| **Multiple(2+)** | - | 1 | 0 | 0 | 0 | 0 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| **多个(2+)** | - | 1 | 0 | 0 | 0 | 0 |'
- en: '| **Twins(2)** | - | - | 1 | 0 | 0 | 0 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| **双胞胎(2)** | - | - | 1 | 0 | 0 | 0 |'
- en: '| **Triplets(3)** | - | - | - | 1 | 0 | 0 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| **三胞胎(3)** | - | - | - | 1 | 0 | 0 |'
- en: '| **Quadruplets(4)** | - | - | - | - | 1 | 0 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| **四胞胎(4)** | - | - | - | - | 1 | 0 |'
- en: '| **Quintuplets(5)** | - | - | - | - | - | 1 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| **五胞胎(5)** | - | - | - | - | - | 1 |'
- en: However, once the plurality is embedded into two dimensions, the similarity
    measure becomes nontrivial, and important relationships between the different
    categories emerge (see [Table 2-7](#when_the_features_are_embedded_in_two_d)).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦多样性被嵌入到两个维度中，相似度测量变得非平凡，并且不同类别之间的重要关系显现出来（参见[表格 2-7](#when_the_features_are_embedded_in_two_d)）。
- en: Table 2-7\. When the features are embedded in two dimensions, the similarity
    matrix gives us more information
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2-7\. 当特征嵌入到两个维度时，相似性矩阵提供了更多信息
- en: '|  | Single(1) | Multiple(2+) | Twins(2) | Triplets(3) | Quadruplets(4) | Quintuplets(5)
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | 单胞胎(1) | 多个(2+) | 双胞胎(2) | 三胞胎(3) | 四胞胎(4) | 五胞胎(5) |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| **Single(1)** | 1 | 0.92 | 0.61 | 0.57 | 0.06 | 0.1 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| **单胞胎(1)** | 1 | 0.92 | 0.61 | 0.57 | 0.06 | 0.1 |'
- en: '| **Multiple(2+)** | - | 1 | 0.86 | 0.83 | 0.43 | 0.48 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| **多个(2+)** | - | 1 | 0.86 | 0.83 | 0.43 | 0.48 |'
- en: '| **Twins(2)** | - |  | 1 | 0.99 | 0.82 | 0.85 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| **双胞胎(2)** | - |  | 1 | 0.99 | 0.82 | 0.85 |'
- en: '| **Triplets(3)** | - |  |  | 1 | 0.85 | 0.88 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| **三胞胎(3)** | - |  |  | 1 | 0.85 | 0.88 |'
- en: '| **Quadruplets(4)** | - |  |  |  | 1 | 0.99 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| **四胞胎(4)** | - |  |  |  | 1 | 0.99 |'
- en: '| **Quintuplets(5)** | - | - | - | - | - | 1 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| **五胞胎(5)** | - | - | - | - | - | 1 |'
- en: Thus, a learned embedding allows us to extract inherent similarities between
    two separate categories and, given there is a numeric vector representation, we
    can precisely quantify the similarity between two categorical features.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，学习得到的嵌入允许我们提取两个不同类别之间的内在相似性，并且，鉴于有数值向量表示，我们可以精确量化两个分类特征之间的相似度。
- en: This is easy to visualize with the natality dataset, but the same principle
    applies when dealing with `customer_ids` embedded into 20-dimensional space. When
    applied to our customer dataset, embeddings allow us to retrieve similar customers
    to a given `customer_id` and make suggestions based on similarity, such as which
    videos they are likely to watch, as shown in [Figure 2-10](#by_learning_a_low-dimensionalcomma_dens).
    Furthermore, these user and item embeddings can be combined with other features
    when training a separate machine learning model. Using pre-trained embeddings
    in machine learning models is referred to as *transfer learning.*
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这在出生率数据集中很容易进行可视化，但在处理嵌入到20维空间的`customer_ids`时也适用相同的原则。在应用于我们的客户数据集时，嵌入使我们能够检索与给定`customer_id`相似的客户，并基于相似性提供建议，例如他们可能观看的视频，如[图2-10](#by_learning_a_low-dimensionalcomma_dens)所示。此外，这些用户和项目嵌入可以与训练独立机器学习模型时的其他特征结合使用。在机器学习模型中使用预训练的嵌入被称为*迁移学习*。
- en: '![By learning a low-dimensional, dense embedding vector for each customer and
    video, an embedding-based model is able to generalize well with less of a manual
    feature engineering burden.](Images/mldp_0210.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![通过为每个客户和视频学习低维、密集的嵌入向量，基于嵌入的模型能够在减少手工特征工程负担的同时，表现出良好的泛化能力。](Images/mldp_0210.png)'
- en: Figure 2-10\. By learning a low-dimensional, dense embedding vector for each
    customer and video, an embedding-based model is able to generalize well with less
    of a manual feature engineering burden.
  id: totrans-309
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 2-10\. 通过为每个客户和视频学习低维、密集的嵌入向量，基于嵌入的模型能够在减少手工特征工程负担的同时，表现出良好的泛化能力。
- en: Trade-Offs and Alternatives
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷与替代方案
- en: The main trade-off with using an embedding is the compromised representation
    of the data. There is a loss of information involved in going from a high-cardinality
    representation to a lower-dimensional representation. In return, we gain information
    about closeness and context of the items.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入的主要折衷是数据表示的损失。从高基数表示转换为低维表示涉及信息的丢失。作为回报，我们获取有关项目接近度和上下文的信息。
- en: Choosing the embedding dimension
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择嵌入维度
- en: The exact dimensionality of the embedding space is something that we choose
    as a practitioner. So, should we choose a large or small embedding dimension?
    Of course, as with most things in machine learning, there is a trade-off. The
    lossiness of the representation is controlled by the size of the embedding layer.
    By choosing a very small output dimension of an embedding layer, too much information
    is forced into a small vector space and context can be lost. On the other hand,
    when the embedding dimension is too large, the embedding loses the learned contextual
    importance of the features. At the extreme, we’re back to the problem encountered
    with one-hot encoding. The optimal embedding dimension is often found through
    experimentation, similar to choosing the number of neurons in a deep neural network
    layer.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入空间的确切维度是我们作为从业者选择的。那么，我们应该选择大的还是小的嵌入维度？当然，就像大多数机器学习中的事物一样，这里存在一个权衡。表示的损失性由嵌入层的大小控制。选择一个非常小的嵌入层输出维度会强制过多的信息进入一个小的向量空间，可能导致上下文丢失。另一方面，当嵌入维度过大时，嵌入会失去对特征学习到的上下文重要性的理解。在极端情况下，我们回到了使用一热编码时遇到的问题。通过实验通常可以找到最佳的嵌入维度，类似于选择深度神经网络层中的神经元数量。
- en: If we’re in a hurry, one rule of thumb is to [use the fourth root](https://oreil.ly/ywFco)
    of the total number of unique categorical elements while another is that the embedding
    dimension should be approximately [1.6 times the square root](https://oreil.ly/github-fastai-2-blob-fastai-2-tabular-model-py)
    of the number of unique elements in the category, and no less than 600\. For example,
    suppose we wanted to use an embedding layer to encode a feature that has 625 unique
    values. Using the first rule of thumb, we would choose an embedding dimension
    for plurality of 5, and using the second rule of thumb, we’d choose 40\. If we
    are doing hyperparameter tuning, it might be worth searching within this range.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们急于求成，一个经验法则是使用[总共唯一分类元素的第四根](https://oreil.ly/ywFco)，而另一个则是嵌入维度应该约为[唯一元素数的平方根的1.6倍](https://oreil.ly/github-fastai-2-blob-fastai-2-tabular-model-py)，且不少于600\.
    例如，假设我们想要使用嵌入层来编码一个有625个唯一值的特征。按照第一个经验法则，我们会选择嵌入维度为5，按照第二个经验法则，我们会选择40\. 如果我们进行超参数调整，可能值得在这个范围内进行搜索。
- en: Autoencoders
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自编码器
- en: Training embeddings in a supervised way can be hard because it requires a lot
    of labeled data. For an image classification model like Inception to be able to
    produce useful image embeddings, it is trained on ImageNet, which has 14 million
    labeled images. Autoencoders provide one way to get around this need for a massive
    labeled dataset.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 以监督方式训练嵌入可能很困难，因为它需要大量标记数据。像Inception这样的图像分类模型要能够生成有用的图像嵌入，它是在拥有1400万标记图像的ImageNet数据集上训练的。自编码器提供了一种方法来绕过对大量标记数据的需求。
- en: The typical autoencoder architecture, shown in [Figure 2-11](#when_training_an_autoencodercomma_the_f),
    consists of a bottleneck layer, which is essentially an embedding layer. The portion
    of the network before the bottleneck (the “encoder”) maps the high-dimensional
    input into a lower-dimensional embedding layer, while the latter network (the
    “decoder”) maps that representation back to a higher dimension, typically the
    same dimension as the original. The model is typically trained on some variant
    of a reconstruction error, which forces the model’s output to be as similar as
    possible to the input.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的自编码器架构，如[图 2-11](#when_training_an_autoencodercomma_the_f)，包括一个瓶颈层，这实质上是一个嵌入层。瓶颈层之前的网络（“编码器”）将高维输入映射到低维嵌入层，而后面的网络（“解码器”）将该表示映射回与原始输入相同的更高维度。该模型通常在某种重构误差的变体上进行训练，这迫使模型的输出尽可能与输入相似。
- en: '![When training an autoencoder, the feature and the label are the same and
    the loss is the reconstruction error. This allows the autoencoder to achieve nonlinear
    dimension reduction.](Images/mldp_0211.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![当训练自编码器时，特征和标签是相同的，损失函数是重构误差。这使得自编码器能够实现非线性降维。](Images/mldp_0211.png)'
- en: Figure 2-11\. When training an autoencoder, the feature and the label are the
    same and the loss is the reconstruction error. This allows the autoencoder to
    achieve nonlinear dimension reduction.
  id: totrans-319
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-11。当训练自编码器时，特征和标签是相同的，损失函数是重构误差。这使得自编码器能够实现非线性降维。
- en: Because the input is the same as the output, no additional labels are needed.
    The encoder learns an optimal nonlinear dimension reduction of the input. Similar
    to how PCA achieves linear dimension reduction, the bottleneck layer of an autoencoder
    is able to obtain nonlinear dimension reduction through the embedding.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 因为输入与输出相同，不需要额外的标签。编码器通过学习输入的最优非线性降维。类似于PCA通过线性降维，自编码器的瓶颈层能够通过嵌入实现非线性降维。
- en: This allows us to break a hard machine learning problem into two parts. First,
    we use all the unlabeled data we have to go from high cardinality to lower cardinality
    by using autoencoders as an *auxiliary learning task*. Then, we solve the actual
    image classification problem for which we typically have much less labeled data
    using the embedding produced by the auxiliary autoencoder task. This is likely
    to boost model performance, because now the model only has to learn the weights
    for the lower-dimension setting (i.e., it has to learn fewer weights).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够将一个困难的机器学习问题分解为两个部分。首先，我们利用所有未标记的数据，通过将自编码器作为*辅助学习任务*，从高基数向低基数转换。然后，我们使用辅助自编码器任务生成的嵌入来解决实际的图像分类问题，对于这类问题，我们通常拥有较少的标记数据。这很可能会提升模型性能，因为现在模型只需学习较低维度设置的权重（即，它只需学习较少的权重）。
- en: In addition to image autoencoders, [recent work](https://oreil.ly/ywFco) has
    focused on applying deep learning techniques for structured data. TabNet is a
    deep neural network specifically designed to learn from tabular data and can be
    trained in an unsupervised manner. By modifying the model to have an encoder-decoder
    structure, TabNet works as an autoencoder on tabular data, which allows the model
    to learn embeddings from structured data via a feature transformer.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图像自编码器外，[最近的工作](https://oreil.ly/ywFco)还专注于应用深度学习技术处理结构化数据。TabNet是一种专门设计用于从表格数据中学习的深度神经网络，可以以非监督方式训练。通过修改模型以具有编码器-解码器结构，TabNet在表格数据上工作作为自编码器，允许模型通过特征转换器从结构化数据中学习嵌入。
- en: Context language models
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文语言模型
- en: Is there an auxiliary learning task that works for text? Context language models
    like Word2Vec and masked language models like Bidirectional Encoding Representations
    from Transformers (BERT) change the learning task to a problem so that there is
    no scarcity of labels.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 是否有适用于文本的辅助学习任务？像Word2Vec这样的上下文语言模型和像BERT这样的遮蔽语言模型改变了学习任务的方式，以解决标签稀缺的问题。
- en: Word2Vec is a well-known method for constructing an embedding using shallow
    neural networks and combining two techniques—Continuous Bag of Words (CBOW) and
    a skip-gram model—applied to a large corpus of text, such as Wikipedia. While
    the goal of both models is to learn the context of a word by mapping input word(s)
    to the target word(s) with an intermediate embedding layer, an auxiliary goal
    is achieved that learns low-dimensional embeddings that best capture the context
    of words. The resulting word embeddings learned through Word2Vec capture the semantic
    relationships between words so that, in the embedding space, the vector representations
    maintain meaningful distance and directionality ([Figure 2-12](#word_embeddings_capture_semantic_relati)).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是一种利用浅层神经网络构建嵌入的知名方法，结合了连续词袋（CBOW）和跳字模型两种技术，应用于大型文本语料库，如维基百科。虽然这两种模型的目标都是通过将输入单词映射到具有中间嵌入层的目标单词来学习单词的上下文，但还实现了一个辅助目标，即学习最能捕捉单词上下文的低维嵌入。通过Word2Vec学到的单词嵌入捕捉了单词之间的语义关系，因此在嵌入空间中，向量表示保持了有意义的距离和方向性（[Figure 2-12](#word_embeddings_capture_semantic_relati)）。
- en: '![Word embeddings capture semantic relationships. ](Images/mldp_0212.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![单词嵌入捕获语义关系。](Images/mldp_0212.png)'
- en: Figure 2-12\. Word embeddings capture semantic relationships.
  id: totrans-327
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-12\. 单词嵌入捕获语义关系。
- en: BERT is trained using a masked language model and next sentence prediction.
    For a masked language model, words are randomly masked from text and the model
    guesses what the missing word(s) are. Next sentence prediction is a classification
    task where the model predicts whether or not two sentences followed each other
    in the original text. So any corpus of text is suitable as a labeled dataset.
    BERT was initially trained on all of the English Wikipedia and BooksCorpus. Despite
    learning on these auxiliary tasks, the learned embeddings from BERT or Word2Vec
    have proven very powerful when used on other downstream training tasks. The word
    embeddings learned by Word2Vec are the same regardless of the sentence where the
    word appears. However, the BERT word embeddings are contextual, meaning the embedding
    vector is different depending on the context of how the word is used.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: BERT使用了掩码语言模型和下一个句子预测进行训练。对于掩码语言模型，文本中的单词会被随机掩码，模型猜测缺失的单词是什么。下一个句子预测是一个分类任务，模型预测原始文本中两个句子是否相邻。因此，任何文本语料库都适合作为标记数据集。BERT最初在整个英文维基百科和BooksCorpus上进行了训练。尽管在这些辅助任务上进行了学习，但从BERT或Word2Vec学到的嵌入在用于其他下游训练任务时已被证明非常强大。Word2Vec学到的单词嵌入与单词出现的句子无关。然而，BERT单词嵌入是上下文相关的，意味着嵌入向量依赖于单词使用的上下文。
- en: A pre-trained text embedding, like Word2Vec, NNLM, GLoVE, or BERT, can be added
    to a machine learning model to process text features in conjunction with structured
    inputs and other learned embeddings from our customer and video dataset ([Figure 2-13](#a_pre-trained_text_embedding_can_be_add)).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 类似Word2Vec、NNLM、GLoVE或BERT这样的预训练文本嵌入可以添加到机器学习模型中，以处理文本特征，同时与结构化输入和来自我们的客户和视频数据集的其他学习嵌入一起使用（[Figure 2-13](#a_pre-trained_text_embedding_can_be_add)）。
- en: Ultimately, embeddings learn to preserve information relevant to the prescribed
    training task. In the case of image captioning, the task is to learn how the context
    of the elements of an image relates to text. In the autoencoder architecture,
    the label is the same as the feature, so the dimension reduction of the bottleneck
    attempts to learn everything with no specific context of what is important.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，嵌入学习保留了与指定训练任务相关的信息。在图像字幕任务中，任务是学习图像元素的上下文如何与文本相关联。在自编码器架构中，标签与特征相同，因此瓶颈的维度缩减试图学习所有内容，没有特定上下文可以说明什么是重要的。
- en: '![A pre-trained text embedding can be added to a model to process text features.](Images/mldp_0213.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![可以向模型添加预训练文本嵌入以处理文本特征。](Images/mldp_0213.png)'
- en: Figure 2-13\. A pre-trained text embedding can be added to a model to process
    text features.
  id: totrans-332
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-13\. 可以向模型添加预训练文本嵌入以处理文本特征。
- en: Embeddings in a data warehouse
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据仓库中的嵌入
- en: Machine learning on structured data is best carried out directly in SQL on a
    data warehouse. This avoids the need to export data out of the warehouse and mitigates
    problems with data privacy and security.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据仓库上对结构化数据进行机器学习最好直接在SQL上进行。这样可以避免将数据从数据仓库导出，减少数据隐私和安全问题。
- en: Many problems, however, require a mix of structured data and natural language
    text or image data. In data warehouses, natural language text (such as reviews)
    is stored directly as columns, and images are typically stored as URLs to files
    in a cloud storage bucket. In these cases, it simplifies later machine learning
    to additionally store the embeddings of the text columns or of the images as array-type
    columns. Doing so will enable the easy incorporation of such unstructured data
    into machine learning models.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多问题需要结构化数据和自然语言文本或图像数据的混合。在数据仓库中，自然语言文本（例如评论）直接存储为列，图像通常存储为云存储桶中文件的URL。在这些情况下，将文本列的嵌入或图像的嵌入作为数组类型列存储，可以简化后续的机器学习过程。这样做将使得将这些非结构化数据轻松地整合到机器学习模型中成为可能。
- en: 'To create text embeddings, we can load a pre-trained model such as Swivel from
    TensorFlow Hub into BigQuery. The full code is on [GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/%E2%81%A0text_%E2%80%8Bembeddings.ipynb):'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建文本嵌入，我们可以将诸如TensorFlow Hub中的Swivel预训练模型加载到BigQuery中。完整的代码位于[GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/%E2%81%A0text_%E2%80%8Bembeddings.ipynb)上：
- en: '[PRE25]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, use the model to transform the natural language text column into an embedding
    array and store the embedding lookup into a new table:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用模型将自然语言文本列转换为嵌入数组，并将嵌入查找存储到新表中：
- en: '[PRE26]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It is now possible to join against this table to get the text embedding for
    any comment. For image embeddings, we can similarly transform image URLs into
    embeddings and load them into the data warehouse.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以针对此表进行连接，以获取任何评论的文本嵌入。对于图像嵌入，我们可以类似地将图像URL转换为嵌入并加载到数据仓库中。
- en: 'Precomputing features in this manner is an example of the [“Design Pattern
    26: Feature Store”](ch06_split_001.xhtml#design_pattern_twosix_feature_store)
    (see [Chapter 6](ch06_split_000.xhtml#reproducibility_design_patterns)).'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式预计算特征是“设计模式 26：特征存储”的一个示例（参见[第 6 章](ch06_split_000.xhtml#reproducibility_design_patterns)中的[“设计模式
    26：特征存储”](ch06_split_001.xhtml#design_pattern_twosix_feature_store)）。
- en: 'Design Pattern 3: Feature Cross'
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模式 3：特征交叉
- en: The Feature Cross design pattern helps models learn relationships between inputs
    faster by explicitly making each combination of input values a separate feature.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 特征交叉设计模式通过显式地将每个输入值的组合作为独立特征，帮助模型更快地学习输入之间的关系。
- en: Problem
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Consider the dataset in [Figure 2-14](#this_dataset_is_not_linearly_separable)
    and the task of creating a binary classifier that separates the + and − labels.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在[图 2-14](#this_dataset_is_not_linearly_separable)中的数据集以及创建将+和−标签分离的二元分类器的任务。
- en: Using only the *x_1* and *x_2* coordinates, it is not possible to find a linear
    boundary that separates the + and − classes.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 使用仅*x_1*和*x_2*坐标，不可能找到分离+和−类的线性边界。
- en: This means that to solve this problem, we have to make the model more complex,
    perhaps by adding more layers to the model. However, a simpler solution exists.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着要解决这个问题，我们必须使模型更复杂，也许通过向模型添加更多层次。然而，存在一个更简单的解决方案。
- en: '![This dataset is not linearly separable using only x_1 and x_2 as inputs.
    ](Images/mldp_0214.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![仅使用x_1和x_2作为输入，无法使该数据集线性可分。](Images/mldp_0214.png)'
- en: Figure 2-14\. This dataset is not linearly separable using only x_1 and x_2
    as inputs.
  id: totrans-349
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-14\. 仅使用x_1和x_2作为输入，该数据集不是线性可分的。
- en: Solution
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: In machine learning, feature engineering is the process of using domain knowledge
    to create new features that aid the machine learning process and increase the
    predictive power of our model. One commonly used feature engineering technique
    is creating a feature cross.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，特征工程是利用领域知识创建新特征的过程，这有助于机器学习过程并增强模型的预测能力。一个常用的特征工程技术是创建特征交叉。
- en: A feature cross is a synthetic feature formed by concatenating two or more categorical
    features in order to capture the interaction between them. By joining two features
    in this way, it is possible to encode nonlinearity into the model, which can allow
    for predictive abilities beyond what each of the features would have been able
    to provide individually. Feature crosses provide a way to have the ML model learn
    relationships between the features faster. While more complex models like neural
    networks and trees can learn feature crosses on their own, using feature crosses
    explicitly can allow us to get away with training just a linear model. Consequently,
    feature crosses can speed up model training (less expensive) and reduce model
    complexity (less training data is needed).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 特征交叉是通过连接两个或多个分类特征来形成的合成特征，以捕捉它们之间的交互作用。通过这种方式联结两个特征，可以在模型中编码非线性，这可以允许超出单独每个特征能够提供的预测能力。特征交叉为模型学习特征之间的关系提供了一种方法。虽然像神经网络和树这样的更复杂模型可以自行学习特征交叉，但显式使用特征交叉可以让我们只需训练线性模型就能摆脱。因此，特征交叉可以加快模型训练速度（更经济）并减少模型复杂性（需要更少的训练数据）。
- en: To create a feature column for the dataset above, we can bucketize x_1 and x_2
    each into two buckets, depending on their sign. This converts x_1 and x_2 into
    categorical features. Let A denote the bucket where x_1 >= 0 and B the bucket
    where x_1 < 0\. Let C denote the bucket where x_2 >= 0 and D the bucket where
    x_2 < 0 ([Figure 2-15](#the_feature_cross_introduces_four_new_b)).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建上述数据集的特征列，我们可以将 x_1 和 x_2 分别分桶成两个桶，取决于它们的符号。这将把 x_1 和 x_2 转换成分类特征。让 A 表示
    x_1 >= 0 的桶，B 表示 x_1 < 0 的桶。让 C 表示 x_2 >= 0 的桶，D 表示 x_2 < 0 的桶（参见 [图 2-15](#the_feature_cross_introduces_four_new_b)）。
- en: '![The feature cross introduces four new boolean features. ](Images/mldp_0215.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![特征交叉引入了四个新的布尔特征。](Images/mldp_0215.png)'
- en: Figure 2-15\. The feature cross introduces four new boolean features.
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-15\. 特征交叉引入了四个新的布尔特征。
- en: 'A feature cross of these bucketized features introduces four new boolean features
    for our model:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型，这些分桶特征的特征交叉引入了四个新的布尔特征：
- en: AC where x_1 >= 0 and x_2 >= 0
  id: totrans-357
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: AC，其中 x_1 >= 0 且 x_2 >= 0
- en: ''
  id: totrans-358
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: BC where x_1 < 0 and x_2 >= 0
  id: totrans-359
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: BC，其中 x_1 < 0 且 x_2 >= 0
- en: ''
  id: totrans-360
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: AD where x_1 >= 0 and x_2 < 0
  id: totrans-361
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: AD，其中 x_1 >= 0 且 x_2 < 0
- en: ''
  id: totrans-362
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: BD where x_1 < 0 and x_2 < 0
  id: totrans-363
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: BD，其中 x_1 < 0 且 x_2 < 0
- en: Each of these four boolean features (AC, BC, AD, and BD) would get its own weight
    when training the model. This means we can treat each quadrant as its own feature.
    Since the original dataset was split perfectly by the buckets we created, a feature
    cross of A and B is able to linearly separate the dataset.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练模型时，这四个布尔特征（AC、BC、AD 和 BD）各自会得到自己的权重。这意味着我们可以将每个象限视为其自身的特征。由于原始数据集完全按照我们创建的桶进行了分割，A
    和 B 的特征交叉能够线性分离数据集。
- en: But this is just an illustration. What about real-world data? Consider a public
    dataset of yellow cab rides in New York City (see [Table 2-8](#a_preview_of_the_public_new_york_city_t)).^([5](ch02.xhtml#ch01fn5))
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 但这只是一个例子。那么真实世界的数据呢？考虑纽约市黄色出租车的公共数据集（参见 [表 2-8](#a_preview_of_the_public_new_york_city_t)）^([5](ch02.xhtml#ch01fn5))。
- en: Table 2-8\. A preview of the public New York City taxi dataset in BigQuery
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-8\. BigQuery 中公共纽约市出租车数据集的预览
- en: '| pickup_datetime | pickuplon | pickuplat | dropofflon | dropofflat | passengers
    | fare_amount |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| pickup_datetime | pickuplon | pickuplat | dropofflon | dropofflat | passengers
    | fare_amount |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2014-05–17 15:15:00 UTC | -73.99955 | 40.7606 | -73.99965 | 40.72522 | 1
    | 31 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 2014-05–17 15:15:00 UTC | -73.99955 | 40.7606 | -73.99965 | 40.72522 | 1
    | 31 |'
- en: '| 2013–12-09 15:03:00 UTC | -73.99095 | 40.749772 | -73.870807 | 40.77407 |
    1 | 34.33 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 2013–12-09 15:03:00 UTC | -73.99095 | 40.749772 | -73.870807 | 40.77407 |
    1 | 34.33 |'
- en: '| 2013-04–18 08:48:00 UTC | -73.973102 | 40.785075 | -74.011462 | 40.708307
    | 1 | 29 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 2013-04–18 08:48:00 UTC | -73.973102 | 40.785075 | -74.011462 | 40.708307
    | 1 | 29 |'
- en: '| 2009–11-05 06:47:00 UTC | -73.980313 | 40.744282 | -74.015285 | 40.711458
    | 1 | 14.9 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 2009–11-05 06:47:00 UTC | -73.980313 | 40.744282 | -74.015285 | 40.711458
    | 1 | 14.9 |'
- en: '| 2009-05-21 09:47:06 UTC | -73.901887 | 40.764021 | -73.901795 | 40.763612
    | 1 | 12.8 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 2009-05-21 09:47:06 UTC | -73.901887 | 40.764021 | -73.901795 | 40.763612
    | 1 | 12.8 |'
- en: This dataset contains information on taxi rides in New York City with features
    such as the timestamp of pickup, the pickup and drop-off latitude and longitude,
    and number of passengers. The label here is `fare_amount`, the cost of the taxi
    ride. Which feature crosses might be relevant for this dataset?
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含有关纽约市出租车行程的信息，包括接客时间戳、接送点纬度和经度以及乘客数量。标签是 `fare_amount`，即出租车费用。这个数据集中可能与特征交叉相关的特征是哪些？
- en: There could be many. Let’s consider the `pickup_datetime`. From this feature,
    we can use information about the ride’s hour and day of the week. Each of these
    is a categorical variable, and certainly both contain predictive power in determining
    the price of a taxi ride. For this dataset, it makes sense to consider a feature
    cross of `day_of_week` and `hour_of_day` since it’s reasonable to assume that
    taxi rides at 5pm on Monday should be treated differently than taxi rides at 5
    p.m. on Friday (see [Table 2-9](#a_preview_of_the_data_weapostrophere_us)).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有很多个。我们来考虑 `pickup_datetime`。从这个特征中，我们可以使用有关行程小时和星期几的信息。每个都是分类变量，并且肯定都包含预测出租车费用的预测能力。对于这个数据集，考虑
    `day_of_week` 和 `hour_of_day` 的特征交叉是有意义的，因为合理地假设星期一下午 5 点的出租车行程应该与星期五下午 5 点的出租车行程有所不同（参见[表 2-9](#a_preview_of_the_data_weapostrophere_us)）。
- en: 'Table 2-9\. A preview of the data we’re using to create a feature cross: the
    day of week and hour of day columns'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-9\. 我们用于创建特征交叉的数据预览：星期几和小时数列
- en: '| day_of_week | hour_of_day |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| day_of_week | hour_of_day |'
- en: '| --- | --- |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Sunday | 00 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 星期日 | 00 |'
- en: '| Sunday | 01 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 星期日 | 01 |'
- en: '| ... | ... |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... |'
- en: '| Saturday | 23 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 星期六 | 23 |'
- en: A feature cross of these two features would be a 168-dimensional one-hot encoded
    vector (24 hours × 7 days = 168) with the example “Monday at 5 p.m.” occupying
    a single index denoting (`day_of_week` is Monday concatenated with `hour_of_day`
    is 17).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 特征交叉包括这两个特征的一个 168 维的 one-hot 编码向量（24 小时 × 7 天 = 168），例如“星期一下午 5 点”占据单个索引，表示
    (`day_of_week` 是星期一与 `hour_of_day` 是 17 连接)。
- en: While the two features are important on their own, allowing for a feature cross
    of hour_of_day and day_of_week makes it easier for a taxi fare prediction model
    to recognize that end-of-the-week rush hour influences the taxi ride duration
    and thus the taxi fare in its own way.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两个特征本身很重要，但允许 `hour_of_day` 和 `day_of_week` 的特征交叉使得出租车费用预测模型更容易识别周末高峰时段如何影响出租车行程持续时间，从而影响出租车费用。
- en: Feature cross in BigQuery ML
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 BigQuery ML 中的特征交叉
- en: 'To create the feature cross in BigQuery, we can use the function `ML.FEATURE_CROSS`
    and pass in a `STRUCT` of the features `day_of_week` and `hour_of_day`:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 BigQuery 中创建特征交叉，我们可以使用 `ML.FEATURE_CROSS` 函数，并传递特征 `day_of_week` 和 `hour_of_day`
    的 `STRUCT`：
- en: '[PRE27]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `STRUCT` clause creates an ordered pair of the two features. If our software
    framework doesn’t support a feature cross function, we can get the same effect
    using string concatenation:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '`STRUCT` 子句创建这两个特征的有序对。如果我们的软件框架不支持特征交叉函数，我们可以使用字符串连接来达到同样的效果：'
- en: '[PRE28]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'A complete training example for the natality problem is shown below, with a
    feature cross of the is_male and plurality columns used as a feature; see the
    full code in this book’s [repository](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/feature_cross.ipynb):'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示了一个完整的生育问题的训练示例，使用了 `is_male` 和 `plurality` 列作为特征交叉；详见本书的[代码库](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/feature_cross.ipynb)。
- en: '[PRE29]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Tip
  id: totrans-392
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: TrThe Transform pattern (see [Chapter 6](ch06_split_000.xhtml#reproducibility_design_patterns))
    is being used here when engineering features of the natality model. This also
    allows the model to “remember” to carry out the feature cross of the input data
    fields during prediction.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 当工程化出生模型的特征时，这里使用了转换模式（参见[第 6 章](ch06_split_000.xhtml#reproducibility_design_patterns)），这也允许模型在预测期间“记住”执行输入数据字段的特征交叉。
- en: When we have enough data, the Feature Cross pattern allows models to become
    simpler. On the natality dataset, the RMSE for the evaluation set for a linear
    model with the Feature Cross pattern is 1.056\. Alternatively, training a deep
    neural network in BigQuery ML on the same dataset with no feature crosses yields
    an RMSE of 1.074\. There is a slight improvement in our performance despite using
    a much simpler linear model, and the training time is also drastically reduced.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们拥有足够的数据时，特征交叉模式使模型变得更加简单。在 natality 数据集上，带有特征交叉模式的线性模型在评估集上的 RMSE 为 1.056。相比之下，在同一数据集上使用没有特征交叉的
    BigQuery ML 深度神经网络训练，其 RMSE 为 1.074。尽管使用了更简单的线性模型，但我们的性能有轻微提升，而且训练时间也大幅减少。
- en: Feature crosses in TensorFlow
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 中的特征交叉
- en: 'To implement a feature cross using the features `is_male` and `plurality` in
    TensorFlow, we use `tf.feature_column.crossed_column`. The method `crossed_column`
    takes two arguments: a list of the feature keys to be crossed and the hash bucket
    size. Crossed features will be hashed according to `hash_bucket_size` so it should
    be large enough to comfortably decrease the likelihood of collisions. Since the
    `is_male` input can take 3 values (True, False, Unknown) and the plurality input
    can take 6 values (Single(1), Twins(2), Triplets(3), Quadruplets(4), Quintuplets(5),
    Multiple(2+)), there are 18 possible `(is_male, plurality)` pairs. If we set `hash_bucket_size`
    to 1,000, we can be 85% sure there are no collisions.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中使用 `is_male` 和 `plurality` 来实现特征交叉，我们使用 `tf.feature_column.crossed_column`
    方法。`crossed_column` 方法接受两个参数：要交叉的特征键列表和哈希桶大小。交叉特征将根据 `hash_bucket_size` 进行哈希，因此它应该足够大，以便舒适地降低碰撞的可能性。由于
    `is_male` 输入可以取 3 个值（True、False、Unknown），而 plurality 输入可以取 6 个值（Single(1)、Twins(2)、Triplets(3)、Quadruplets(4)、Quintuplets(5)、Multiple(2+)），因此有
    18 种可能的 `(is_male, plurality)` 对。如果我们将 `hash_bucket_size` 设置为 1,000，我们可以确保有 85%
    的把握没有碰撞发生。
- en: 'Finally, to use a crossed column in a DNN model, we need to wrap it either
    in an `indicator_column` or an `embedding_column` depending on whether we want
    to one-hot encode it or represent it in a lower dimension (see the [“Design Pattern
    2: Embeddings”](#design_pattern_two_embeddings) in this chapter):'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，在 DNN 模型中使用交叉列，我们需要将其包装在 `indicator_column` 或 `embedding_column` 中，具体取决于我们是想要进行独热编码还是在较低维度中表示它（见本章中的
    [“设计模式 2: 嵌入”](#design_pattern_two_embeddings) ）：'
- en: '| `gender_x_plurality = fc.crossed_column(["is_male", "plurality"],` `hash_bucket_size=1000)`'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '| `gender_x_plurality = fc.crossed_column(["is_male", "plurality"],` `hash_bucket_size=1000)`'
- en: '`crossed_feature = fc.embedding_column(gender_x_plurality, dimension=2)` |'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '`crossed_feature = fc.embedding_column(gender_x_plurality, dimension=2)` |'
- en: or
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '| `gender_x_plurality = fc.crossed_column(["is_male", "plurality"],` `hash_bucket_size=1000)`'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '| `gender_x_plurality = fc.crossed_column(["is_male", "plurality"],` `hash_bucket_size=1000)`'
- en: '`crossed_feature = fc.indicator_column(gender_x_plurality)` |'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '`crossed_feature = fc.indicator_column(gender_x_plurality)` |'
- en: Why It Works
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么有效
- en: Feature crosses provide a valuable means of feature engineering. They provide
    more complexity, more expressivity, and more capacity to simple models. Think
    again about the crossed feature of `is_male` and `plurality` in the natality dataset.
    This Feature Cross pattern allows the model to treat twin males separately from
    female twins and separately from triplet males and separately from single females
    and so on. When we use an `indicator_column`, the model is able to treat each
    of the resulting crosses as an independent variable, essentially adding 18 additional
    binary categorical features to the model (see [Figure 2-16](#a_feature_cross_between_is_male_and_plu)
    ).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 特征交叉提供了一种有价值的特征工程手段。它们为简单模型提供了更多的复杂性、表现力和容量。再次思考一下在 natality 数据集中 `is_male`
    和 `plurality` 的交叉特征。这种特征交叉模式允许模型将双胞胎男性、女性双胞胎、三胞胎男性和单身女性等分开对待。当我们使用 `indicator_column`
    时，模型能够将每个结果交叉视为独立变量，实质上为模型添加了额外的 18 个二进制分类特征（见 [图 2-16](#a_feature_cross_between_is_male_and_plu)
    ）。
- en: Feature crosses scale well to massive data. While adding extra layers to a deep
    neural network could potentially provide enough nonlinearity to learn how pairs
    (`is_male, plurality`) behave, this drastically increases the training time. On
    the natality dataset, we observed that a linear model with a feature cross trained
    in BigQuery ML performs comparably with a DNN trained without a feature cross.
    However, the linear model trains substantially faster.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 特征交叉在大数据场景中表现良好。虽然向深度神经网络添加额外层可以潜在地提供足够的非线性来学习(`is_male, plurality`)对的行为方式，但这会极大地增加训练时间。在出生数据集上，我们观察到使用
    BigQuery ML 训练的带有特征交叉的线性模型与不带特征交叉的 DNN 相比表现相当。然而，线性模型的训练速度明显更快。
- en: '![A feature cross between is_male and plurality creates an additional 18 binary
    features in our ML model.](Images/mldp_0216.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![is_male 和 plurality 之间的特征交叉在我们的 ML 模型中创建了额外的 18 个二进制特征。](Images/mldp_0216.png)'
- en: Figure 2-16\. A feature cross between is_male and plurality creates an additional
    18 binary features in our ML model.
  id: totrans-407
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-16\. `is_male` 和 `plurality` 之间的特征交叉在我们的 ML 模型中创建了额外的 18 个二进制特征。
- en: '[Table 2-10](#a_comparison_of_bigquery_ml_training_me) compares the training
    time in BigQuery ML and evaluation loss for both a linear model with a feature
    cross of (`is_male, plurality`) and a deep neural network without any feature
    cross.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2-10](#a_comparison_of_bigquery_ml_training_me) 比较了 BigQuery ML 中线性模型带有
    (`is_male, plurality`) 特征交叉和不带特征交叉的深度神经网络的训练时间和评估损失。'
- en: '**Table 2-10\. A comparison of BigQuery ML training metrics for models with
    and without feature crosses'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 2-10\. BigQuery ML 训练指标的比较：带有特征交叉和不带特征交叉的模型**'
- en: '| Model type | Incl. feature cross | Training time (minutes) | Eval. loss (RMSE)
    |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 模型类型 | 包括特征交叉 | 训练时间（分钟） | 评估损失（RMSE） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Linear | Yes | 0.42 | 1.05 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 线性 | 是 | 0.42 | 1.05 |'
- en: '| DNN | No | 48 | 1.07 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| DNN | No | 48 | 1.07 |'
- en: A simple linear regression achieves comparable error on the evaluation set but
    trains one hundred times faster. Combining feature crosses with massive data is
    an alternative strategy for learning complex relationships in training data.**  **##
    Trade-Offs and Alternatives
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的线性回归在评估集上达到了可比较的误差，但训练速度快了一百倍。将特征交叉与大数据结合使用是学习训练数据中复杂关系的另一种策略。**  **## 折衷方案和替代方案
- en: While we discussed feature crosses as a way of handling categorical variables,
    they can be applied, with a bit of preprocessing, to numerical features also.
    Feature crosses cause sparsity in models and are often used along with techniques
    that counteract that sparsity.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们讨论特征交叉作为处理分类变量的一种方式，但经过一些预处理，它们也可以应用于数值特征。特征交叉会导致模型稀疏性，并常与抵消该稀疏性的技术一起使用。
- en: Handling numerical features
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理数值特征
- en: We would never want to create a feature cross with a continuous input. Remember,
    if one input takes *m* possible values and another input takes *n* possible values,
    then the feature cross of the two would result in m*n elements. A numeric input
    is dense, taking a continuum of values. It would be impossible to enumerate all
    possible values in a feature cross of continuous input data.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '我们绝不希望对连续输入进行特征交叉。记住，如果一个输入有 *m* 个可能的取值，另一个输入有 *n* 个可能的取值，那么这两者的特征交叉将导致 m*n
    个元素。数值输入是密集的，可以取连续的值。在连续输入数据的特征交叉中无法枚举所有可能的值。 '
- en: 'Instead, if our data is continuous, then we can bucketize the data to make
    it categorical before applying a feature cross. For example, latitude and longitude
    are continuous inputs, and it makes intuitive sense to create a feature cross
    using these inputs since location is determined by an ordered pair of latitude
    and longitude. However, instead of creating a feature cross using the raw latitude
    and longitude, we would bin these continuous values and cross the `binned_latitude`
    and the `binned_longitude`:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据是连续的，那么在应用特征交叉之前，我们可以将数据进行分桶处理，使其变成分类数据。例如，纬度和经度是连续的输入，使用这些输入创建特征交叉是直觉上的合理选择，因为位置由纬度和经度的有序对确定。然而，我们不会直接使用原始的纬度和经度创建特征交叉，而是对这些连续数值进行分箱处理，然后交叉`binned_latitude`和`binned_longitude`：
- en: '[PRE30]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Handling high cardinality
  id: totrans-420
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理高基数
- en: Because the cardinality of resulting categories from a feature cross increases
    multiplicatively with respect to the cardinality of the input features, feature
    crosses lead to sparsity in our model inputs. Even with the `day_of_week` and
    `hour_of_day` feature cross, a feature cross would be a sparse vector of dimension
    168 (see [Figure 2-17](#a_feature_cross_of_day_of_week_and_hour)).
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特征交叉后结果类别的基数与输入特征的基数呈乘法关系，特征交叉导致我们模型输入的稀疏性。 即使是`day_of_week`和`hour_of_day`特征交叉，特征交叉也会生成一个维数为168的稀疏向量（参见[图2-17](#a_feature_cross_of_day_of_week_and_hour)）。
- en: 'It can be useful to pass a feature cross through an Embedding layer (see the
    [“Design Pattern 2: Embeddings”](#design_pattern_two_embeddings) in this chapter)
    to create a lower-dimensional representation, as shown in [Figure 2-18](#an_embedding_layer_is_a_useful_way_to_a).'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征交叉通过嵌入层传递（参见本章的[“设计模式2：嵌入”](#design_pattern_two_embeddings)）以创建一个较低维度的表示，如[图2-18](#an_embedding_layer_is_a_useful_way_to_a)所示。
- en: '![A feature cross of day_of_week and hour_of_day produces a sparse vector of
    dimension 168.](Images/mldp_0217.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![星期几和小时的特征交叉生成一个维数为168的稀疏向量。](Images/mldp_0217.png)'
- en: Figure 2-17\. A feature cross of day_of_week and hour_of_day produces a sparse
    vector of dimension 168.
  id: totrans-424
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-17. 星期几和小时的特征交叉生成一个维数为168的稀疏向量。
- en: '![An embedding layer is a useful way to address the sparsity of a feature cross.](Images/mldp_0218.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![嵌入层是解决特征交叉稀疏性的有效方法。](Images/mldp_0218.png)'
- en: Figure 2-18\. An embedding layer is a useful way to address the sparsity of
    a feature cross.
  id: totrans-426
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-18. 嵌入层是解决特征交叉稀疏性的有效方法。
- en: 'Because the Embeddings design pattern allows us to capture closeness relationships,
    passing the feature cross through an embedding layer allows the model to generalize
    how certain feature crosses coming from pairs of hour and day combinations affect
    the output of the model. In the example of latitude and longitude above, we could
    have used an embedding feature column in place of the indicator column:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 由于嵌入设计模式允许我们捕捉接近关系，通过嵌入层传递特征交叉允许模型概括来自小时和日期组合对模型输出的影响。 在上述纬度和经度的例子中，我们可以使用嵌入特征列代替指示器列：
- en: '[PRE31]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Need for regularization
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化的必要性
- en: When crossing two categorical features both with large cardinality, we produce
    a cross feature with multiplicative cardinality. Naturally, given more categories
    for an individual feature, the number of categories in a feature cross can increase
    dramatically. If this gets to the point where individual buckets have too few
    items, it will hinder the model’s ability to generalize. Think of the latitude
    and longitude example. If we were to take very fine buckets for latitude and longitude,
    then a feature cross would be so precise it would allow the model to memorize
    every point on the map. However, if that memorization was based on just a handful
    of examples, the memorization would actually be an overfit.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 当交叉两个具有大基数的分类特征时，我们产生一个具有乘法基数的交叉特征。 自然而然，对于单个特征，给定更多类别，特征交叉中的类别数可能会显著增加。 如果这一点达到使得单个存储桶中的项目太少，这将阻碍模型的泛化能力。
    想象一下纬度和经度的例子。 如果我们对纬度和经度进行非常精细的划分，那么特征交叉将非常精确，使得模型可以记住地图上的每一点。 但是，如果这种记忆是基于只有少数示例的话，这种记忆实际上是过度拟合的。
- en: To illustrate, take the example of predicting the taxi fare in New York given
    the pickup and dropoff locations and the time of pickup:^([6](ch02.xhtml#ch01fn6))
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 举例说明，例如预测纽约的出租车费用，给定接送地点和接送时间：^([6](ch02.xhtml#ch01fn6))
- en: '[PRE32]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'There are two feature crosses here: one in time (of day of week and hour of
    day) and the other in space (of the pickup and dropoff locations). The location,
    in particular, is very high cardinality, and it is likely that some of the buckets
    will have very few examples.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个特征交叉：一个在时间上（星期几和小时），另一个在空间上（接送地点）。 特别是地点具有非常高的基数，某些存储桶可能只有非常少的示例。
- en: For this reason, it is advisable to pair feature crosses with L1 regularization,
    which encourages sparsity of features, or L2 regularization, which limits overfitting.
    This allows our model to ignore the extraneous noise generated by the many synthetic
    features and combat overfitting. Indeed, on this dataset, the regularization improves
    the RMSE slightly, by 0.3%.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，建议将特征交叉与L1正则化配对，以鼓励特征的稀疏性，或者与L2正则化配对，以限制过拟合。这使得我们的模型能够忽略由许多合成特征生成的多余噪音，并抵抗过拟合。事实上，在这个数据集上，正则化略微改善了RMSE，提高了0.3%。
- en: 'As a related point, when choosing which features to combine for a feature cross,
    we would not want to cross two features that are highly correlated. We can think
    of a feature cross as combining two features to create an ordered pair. In fact,
    the term “cross” of “feature cross” refers to the Cartesian product. If two features
    are highly correlated, then the “span” of their feature cross doesn’t bring any
    new information to the model. As an extreme example, suppose we had two features,
    x_1 and x_2, where x_2 = 5*x_1\. Bucketing values for x_1 and x_2 by their sign
    and creating a feature cross will still produce four new boolean features. However,
    due to the dependence of x_1 and x_2, two of those four features are actually
    empty, and the other two are precisely the two buckets created for x_1\.**  **#
    Design Pattern 4: Multimodal Input'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 作为相关的一点，在选择要组合成特征交叉的特征时，我们不希望交叉两个高度相关的特征。我们可以将特征交叉视为将两个特征组合成有序对。实际上，“交叉”在“特征交叉”中的术语指的是笛卡尔积。如果两个特征高度相关，那么它们的特征交叉的“跨度”不会为模型带来任何新信息。举一个极端的例子，假设我们有两个特征，x_1
    和 x_2，其中 x_2 = 5*x_1。通过它们的符号对 x_1 和 x_2 进行分桶，并创建特征交叉，仍然会产生四个新的布尔特征。然而，由于 x_1 和
    x_2 的依赖性，这四个特征中有两个实际上是空的，另外两个恰好是为 x_1 创建的两个桶。**# 设计模式 4：多模态输入**
- en: The Multimodal Input design pattern addresses the problem of representing different
    types of data or data that can be expressed in complex ways by concatenating all
    the available data representations.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态输入设计模式解决了表示不同类型数据或可以通过连接所有可用数据表示的复杂方式表达的数据问题。
- en: Problem
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Typically, an input to a model can be represented as a number or as a category,
    an image, or free-form text. Many off-the-shelf models are defined for specific
    types of input only—a standard image classification model such as Resnet-50, for
    example, does not have the ability to handle inputs other than images.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型的输入可以表示为数字或类别、图像或自由形式文本。许多现成的模型仅定义了特定类型的输入——例如，标准的图像分类模型如 Resnet-50 并不具备处理除图像外的其他类型输入的能力。
- en: To understand the need for multimodal inputs, let’s say we’ve got a camera capturing
    footage at an intersection to identify traffic violations. We want our model to
    handle both image data (camera footage) and some metadata about when the image
    was captured (time of day, day of week, weather, etc.), as depicted in [Figure 2-19](#model_combining_image_and_numerical_fea).
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解多模态输入的必要性，我们可以假设有一台摄像机在路口拍摄录像以识别交通违规行为。我们希望我们的模型处理图像数据（摄像头录像）以及有关图像捕捉时间的一些元数据（时间、星期几、天气等），如[图
    2-19](#model_combining_image_and_numerical_fea)所示。
- en: This problem also occurs when training a structured data model where one of
    the inputs is free-form text. Unlike numerical data, images and text cannot be
    fed directly into a model. As a result, we’ll need to represent image and text
    inputs in a way our model can understand (usually using the Embeddings design
    pattern), then combine these inputs with other tabular^([7](ch02.xhtml#ch01fn7))
    features. For example, we might want to predict a restaurant patron’s rating based
    on their review text and other attributes such as what they paid and whether it
    was lunch or dinner (see [Figure 2-20](#model_combining_free-form_text_input_wi)).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练结构化数据模型时，其中一个输入是自由形式文本时，也会出现这个问题。与数值数据不同，图像和文本不能直接输入模型。因此，我们需要以模型理解的方式表示图像和文本输入（通常使用嵌入设计模式），然后将这些输入与其他表格^([7](ch02.xhtml#ch01fn7))特征结合起来。例如，我们可能希望根据顾客的评价文本以及他们支付的金额和用餐时间（午餐还是晚餐等）来预测餐厅顾客的评分（见[图
    2-20](#model_combining_free-form_text_input_wi)）。
- en: '![Model combining image and numerical features to predict whether footage of
    an intersection depicts a traffic violation.](Images/mldp_0219.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![模型结合图像和数值特征以预测路口录像是否显示交通违规行为。](Images/mldp_0219.png)'
- en: Figure 2-19\. Model combining image and numerical features to predict whether
    footage of an intersection depicts a traffic violation.
  id: totrans-442
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-19\. 模型结合图像和数字特征，以预测交叉口镜头是否显示交通违规行为。
- en: '![Model combining free-form text input with tabular data to predict the rating
    of a restaurant review.](Images/mldp_0220.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![模型结合自由文本输入和表格数据，以预测餐厅评论的评分。](Images/mldp_0220.png)'
- en: Figure 2-20\. Model combining free-form text input with tabular data to predict
    the rating of a restaurant review.
  id: totrans-444
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-20\. 模型结合自由文本输入和表格数据，以预测餐厅评论的评分。
- en: Solution
  id: totrans-445
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'To start, let’s take the example above with text from a restaurant review combined
    with tabular metadata about the meal referenced by the review. We’ll first combine
    the numerical and categorical features. There are three possible options for `meal_type`,
    so we can turn this into a one-hot encoding and will represent dinner as [`0,
    0, 1`]. With this categorical feature represented as an array, we can now combine
    it with meal_total by adding the price of the meal as the fourth element of the
    array: [`0, 0, 1, 30.5`].'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们以餐厅评论中的文本为例，结合有关评论所引用的餐点的表格元数据。我们将首先结合数字和分类特征。对于`meal_type`，有三种可能的选项，所以我们可以将其转换为一种独热编码，并将晚餐表示为[`0,
    0, 1`]。通过将餐点的价格添加为数组的第四个元素，我们现在可以将其与 meal_total 结合起来：[`0, 0, 1, 30.5`]。
- en: 'The Embeddings design pattern is a common approach to encoding text for machine
    learning models. If our model had only text, we could represent it as an embedding
    layer using the following `tf.keras` code:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入设计模式是一种常见的方法，用于为机器学习模型编码文本。如果我们的模型只有文本，我们可以使用以下`tf.keras`代码将其表示为嵌入层：
- en: '[PRE33]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here, we need to flatten the embedding^([8](ch02.xhtml#ch01fn8)) in order to
    concatenate with the `meal_type` and `meal_total`:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要展平嵌入^([8](ch02.xhtml#ch01fn8))，以便与`meal_type`和`meal_total`连接：
- en: '[PRE34]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We could then use a series of Dense layers to transform that very large array^([9](ch02.xhtml#ch01fn9))
    into smaller ones, ending with our output that is an array of, say, three numbers:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用一系列稠密层将这个非常大的数组^([9](ch02.xhtml#ch01fn9))转换成较小的数组，最后以我们的输出结束，这是一个由三个数字组成的数组：
- en: '[PRE35]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We now need to concatenate these three numbers, which form the sentence embedding
    of the review with the earlier inputs: [`0, 0, 1, 30.5, 0.75, -0.82, 0.45`].'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将形成评论嵌入的这三个数字连接起来，与前面的输入一起：[`0, 0, 1, 30.5, 0.75, -0.82, 0.45`]。
- en: 'To do this, we’ll use the Keras functional API and apply the same steps. Layers
    built with the functional API are callable, enabling us to chain them together
    starting with an `Input layer`.^([10](ch02.xhtml#ch01fn10)) To make use of this,
    we’ll first define both our embedding and tabular layers:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将使用 Keras 的功能 API，并应用相同的步骤。功能 API 构建的层是可调用的，这使我们能够将它们链接在一起，从一个`输入层`开始。^([10](ch02.xhtml#ch01fn10))
    为了利用这一点，我们将首先定义我们的嵌入和表格层：
- en: '[PRE36]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Note that we’ve defined the `Input` pieces of both of these layers as their
    own variables. This is because we need to pass Input layers when we build a `Model`
    with the functional API. Next, we’ll create a concatenated layer, feed that into
    our output layer, and finally create the model by passing in the original Input
    layers we defined above:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已经将这些层的`Input`部分定义为它们自己的变量。这是因为我们在使用功能 API 构建`Model`时需要传递输入层。接下来，我们将创建一个串联层，将其馈送到输出层，最后通过传递我们上面定义的原始输入层来创建模型：
- en: '[PRE37]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now we have a single model that accepts the multimodal input.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个接受多模态输入的单一模型。
- en: Trade-Offs and Alternatives
  id: totrans-459
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折衷和替代方案
- en: 'As we just saw, the Multimodal Input design pattern explores how to represent
    *different input formats* in the same model. In addition to mixing different *types*
    of data, we may also want to represent the *same data in different ways* to make
    it easier for our model to identify patterns. For example, we may have a ratings
    field that is on an ordinal scale of 1 star to 5 stars, and treat that ratings
    field as both numeric and categorical. Here, we are referring to *multimodal inputs*
    as both:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚刚看到的，多模态输入设计模式探讨了如何在同一个模型中表示*不同的输入格式*。除了混合不同*类型*的数据之外，我们还可能希望以不同的方式表示*相同的数据*，以便我们的模型更容易识别模式。例如，我们可能有一个评级字段，它在
    1 星到 5 星的序数比例上，可以将该评级字段既作为数字又作为分类处理。在这里，我们将*多模态输入*称为以下两种形式：
- en: Combining different types of data, like images + metadata
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合不同类型的数据，如图像 + 元数据
- en: Representing complex data in multiple ways
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以多种方式表示复杂数据
- en: We’ll start by exploring how tabular data can be represented in different ways,
    and then we’ll look at text and image data.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从探索如何以不同方式表示表格数据开始，然后再看文本和图像数据。
- en: Tabular data multiple ways
  id: totrans-464
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表格数据的多种方式
- en: 'To see how we can represent tabular data in different ways for the same model,
    let’s return to the restaurant review example. We’ll imagine instead that rating
    is an *input* to our model and we’re trying to predict the review’s usefulness
    (how many people liked the review). As an input, the rating can be represented
    both as an integer value ranging from 1 to 5 and as a categorical feature. To
    represent rating categorically, we can bucket it. The way we bucket the data is
    up to us and dependent on our dataset and use case. To keep things simple, let’s
    say we want to create two buckets: “good” and “bad.” The “good” bucket includes
    ratings of 4 and 5, and “bad” includes 3 and below. We can then create a boolean
    value to encode the rating buckets and concatenate both the integer and boolean
    into a single array ([full code is on GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/mixed_representation.ipynb)).'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到如何以相同模型不同方式表示表格数据，让我们回到餐厅评论的例子。我们假设评分是我们模型的*输入*，我们试图预测评论的有用性（多少人喜欢这条评论）。作为输入，评分可以表示为从1到5的整数值，也可以作为分类特征。为了将评分进行分类表示，我们可以对其进行分桶。我们如何分桶数据取决于我们的数据集和使用情况。为了保持简单，假设我们想创建两个桶：“好”和“差”。“好”桶包括评分为4和5的情况，“差”包括3及以下的评分。然后我们可以创建一个布尔值来编码评分桶，并将整数和布尔值连接成一个数组（[完整代码在
    GitHub 上](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/mixed_representation.ipynb)）。
- en: 'Here’s what this might look like for a small dataset with three data points:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个小数据集的三个数据点示例：
- en: '[PRE38]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The resulting feature is a two-element array consisting of the integer rating
    and its boolean representation:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 结果特征是一个两元素数组，包括整数评分和其布尔表示：
- en: '[PRE39]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: If we had instead decided to create more than two buckets, we would one-hot
    encode each input and append this one-hot array to the integer representation.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们决定创建超过两个桶，我们将对每个输入进行独热编码，并将这个独热数组附加到整数表示中。
- en: The reason it’s useful to represent rating in two ways is because the value
    of rating as measured by 1 to 5 stars does not necessarily increase linearly.
    Ratings of 4 and 5 are very similar, and ratings of 1 to 3 most likely indicate
    that the reviewer was dissatisfied. Whether you give something you dislike 1,
    2, or 3 stars is often related to your review tendencies rather than the review
    itself. Despite this, it’s still useful to keep the more granular information
    present in the star rating, which is why we encode it in two ways.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 将评分以两种方式表示的原因是因为评分的价值（1到5星）并不一定是线性增长的。4和5星的评分非常相似，而1到3星的评分很可能表明评论者不满意。你给出你不喜欢的东西1、2或3星，通常与你的评论倾向相关，而不仅仅是评论本身。尽管如此，保留星级评分中更精细的信息仍然很有用，这就是我们以两种方式对其进行编码的原因。
- en: Additionally, consider features with a larger range than 1 to 5, like the distance
    between a reviewer’s home and a restaurant. If someone drives two hours to go
    to a restaurant, their review may be more critical than someone coming from across
    the street. In this case, we might have outlier values, and so it would make sense
    to both threshold the numeric distance representation at something like 50 km
    and to include a separate categorical representation of distance. The categorical
    feature could be bucketed into “in state,” “in country,” and “foreign.”
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑具有比1到5更大范围的特征，比如评论者家和餐厅之间的距离。如果有人开车两个小时去餐厅，他们的评论可能比从对面来的人更严格。在这种情况下，我们可能会有异常值，因此在50公里左右的数值距离表示上限，并包括一个单独的距离分类表示是有意义的。分类特征可以分为“本州内”，“国内”和“国外”。
- en: Multimodal representation of text
  id: totrans-473
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本的多模态表示
- en: Both text and images are unstructured and require more transformations than
    tabular data. Representing them in various formats can help our models extract
    more patterns. We’ll build on our discussion of text models in the preceding section
    by looking at different approaches for representing text data. Then we’ll introduce
    images and dive into a few options for representing image data in ML models.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 文本和图像都是非结构化的，比表格数据需要更多的转换。以不同格式表示它们有助于我们的模型提取更多的模式。我们将在前一节讨论文本模型的基础上继续讨论，探讨表示文本数据的不同方法。然后，我们将介绍图像并深入探讨几种用于表示图像数据的选项。
- en: Text data multiple ways
  id: totrans-475
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本数据的多种方式
- en: Given the complex nature of text data, there are many ways to extract meaning
    from it. The Embeddings design pattern enables a model to group similar words
    together, identify relationships between words, and understand syntactic elements
    of text. While representing text through word embeddings most closely mirrors
    how humans innately understand language, there are additional text representations
    that can maximize our model’s ability to perform a given prediction task. In this
    section, we’ll look at the bag of words approach to representing text, along with
    extracting tabular features from text.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于文本数据的复杂性，有许多方法可以从中提取含义。嵌入设计模式使模型能够将相似的单词分组在一起，识别单词之间的关系，并理解文本的句法元素。虽然通过单词嵌入表示文本最接近人类如何本能理解语言，但还有其他文本表示方法可以最大化我们模型在给定预测任务中的能力。在本节中，我们将看看词袋方法来表示文本，以及从文本中提取表格特征。
- en: 'To demonstrate text data representation, we’ll be referencing a dataset that
    contains the text of millions of questions and answers from Stack Overflow,^([11](ch02.xhtml#ch01fn11))
    along with metadata about each post. For example, the following query will give
    us a subset of questions tagged as either “keras,” “matplotlib,” or “pandas,”
    along with the number of answers each question received:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示文本数据的表示，我们将引用一个包含来自 Stack Overflow 数百万问题和答案文本的数据集，^([11](ch02.xhtml#ch01fn11))
    以及每个帖子的元数据。例如，以下查询将给出标记为“keras”，“matplotlib”或“pandas”的问题子集，并显示每个问题收到的答案数：
- en: '[PRE40]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The query results in the following output:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 查询结果如下输出：
- en: '| Row | title | answer_count | tags |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 行 | 标题 | 答案数 | 标签 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | Building a new column in a pandas dataframe by matching string values
    in a list | 6 | python,python-2.7,pandas,replace,nested-loops |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 通过匹配列表中的字符串值在 pandas dataframe 中构建新列 | 6 | python,python-2.7,pandas,replace,nested-loops
    |'
- en: '| 2 | Extracting specific selected columns to new DataFrame as a copy | 6 |
    python,pandas,chained-assignment |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 作为副本提取特定选择的列到新的 DataFrame | 6 | python,pandas,chained-assignment |'
- en: '| 3 | Where do I call the BatchNormalization function in Keras? | 7 | python,keras,neural-network,data-science,batch-normalization
    |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 在 Keras 中何时调用 BatchNormalization 函数？ | 7 | python,keras,neural-network,data-science,batch-normalization
    |'
- en: '| 4 | Using Excel like solver in Python or SQL | 8 | python,sql,numpy,pandas,solver
    |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 在 Python 或 SQL 中使用类似 Excel 的求解器 | 8 | python,sql,numpy,pandas,solver
    |'
- en: When representing text using the bag of words (BOW) approach, we imagine each
    text input to our model as a bag of Scrabble tiles, with each tile containing
    a single word instead of a letter. BOW does not preserve the order of our text,
    but it does detect the presence or absence of certain words in each piece of text
    we send to our model. This approach is a type of multi-hot encoding where each
    text input is converted into an array of 1s and 0s. Each index in this BOW array
    corresponds to a word from our vocabulary.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用词袋（BOW）方法表示文本时，我们将每个文本输入想象成一个拼字游戏的袋子，每个袋子中包含一个单词而不是一个字母。BOW 不保留文本的顺序，但它确实检测我们发送到模型的每个文本中特定单词的存在或缺失。这种方法是一种多热编码，其中每个文本输入被转换为一个由1和0组成的数组。这个
    BOW 数组中的每个索引对应于我们词汇表中的一个单词。
- en: Given that there are two different approaches for representing text (Embedding
    and BOW), which approach should we choose for a given task? As with many aspects
    of machine learning, this depends on our dataset, the nature of our prediction
    task, and the type of model we’re planning to use.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于有两种不同的文本表示方法（嵌入和词袋），在给定任务中应该选择哪种方法？与机器学习的许多方面一样，这取决于我们的数据集，我们预测任务的性质，以及我们计划使用的模型类型。
- en: Embeddings add an extra layer to our model and provide extra information about
    word meaning that is not available from the BOW encoding. However, embeddings
    require training (unless we can use a pre-trained embedding for our problem).
    While a deep learning model may achieve higher accuracy, we can also try using
    BOW encoding in a linear regression or decision-tree model using frameworks like
    scikit-learn or XGBoost. Using BOW encoding with a simpler model type can be useful
    for fast prototyping or to verify that the prediction task we’ve chosen will work
    on our dataset. Unlike embeddings, BOW doesn’t take into account the order or
    meaning of words in a text document. If either of these are important to our prediction
    task, embeddings may be the best approach.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: Embeddings（嵌入）为我们的模型增加了额外的层次，并提供了关于单词含义的额外信息，这是从BOW编码中无法获得的。然而，嵌入需要训练（除非我们可以为我们的问题使用预训练的嵌入）。深度学习模型可能会达到更高的准确率，但我们也可以尝试在像scikit-learn或XGBoost这样的框架中使用BOW编码进行线性回归或决策树模型。使用简单模型类型的BOW编码可以用于快速原型设计或验证我们选择的预测任务是否适用于我们的数据集。与嵌入不同，BOW不考虑文本文档中单词的顺序或含义。如果其中任何一个对我们的预测任务很重要，嵌入可能是最佳选择。
- en: 'There may also be benefits to building a deep model that combines *both* bag
    of words *and* text embedding representations to extract more patterns from our
    data. To do this, we can use the Multimodal Input approach, except that instead
    of concatenating text and tabular features, we can concatenate the Embedding and
    BOW representations (see [code on GitHub](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/mixed_representation.ipynb)).
    Here, the shape of our Input layer would be the vocabulary size of the BOW representation.
    Some benefits of representing text in multiple ways include:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 构建结合*包和文本嵌入*表示的深层模型也可能有益于从我们的数据中提取更多模式。为此，我们可以使用多模态输入方法，除了连接文本和表格特征之外，我们还可以连接嵌入和BOW表示（参见[GitHub上的代码](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/mixed_representation.ipynb)）。这里，我们的输入层的形状将是BOW表示的词汇量大小。表示文本的多种方式的一些好处包括：
- en: BOW encoding provides strong signals for the most significant words present
    in our vocabulary, while embeddings can identify relationships between words in
    a much larger vocabulary.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BOW编码为我们词汇表中存在的最重要单词提供了强烈信号，而嵌入可以在更大的词汇表中识别单词之间的关系。
- en: If we have text that switches between languages, we can build embeddings (or
    BOW encodings) for each one and concatenate them.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的文本在不同语言之间切换，我们可以为每种语言构建嵌入（或BOW编码）并将它们连接起来。
- en: Embeddings can encode the frequency of words in text, where the BOW treats the
    presence of each word as a boolean value. Both representations are valuable.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入可以编码文本中单词的频率，而BOW则将每个单词的存在视为布尔值。这两种表示方式都是有价值的。
- en: BOW encoding can identify patterns between reviews that all contain the word
    “amazing,” while an embedding can learn to correlate the phrase “not amazing”
    with a below-average review. Again, both of these representations are valuable.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BOW编码可以识别所有包含“amazing”单词的评论之间的模式，而嵌入可以学习将短语“not amazing”与低于平均水平的评论相关联。同样，这两种表示都是有价值的。
- en: Extracting tabular features from text
  id: totrans-494
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从文本中提取表格特征
- en: In addition to encoding raw text data, there are often other characteristics
    of text that can be represented as tabular features. Let’s say we are building
    a model to predict whether or not a Stack Overflow question will get a response.
    Various factors about the text but unrelated to the exact words themselves may
    be relevant to training a model on this task. For example, maybe the length of
    a question or the presence of a question mark influences the likelihood of an
    answer. However, when we create an embedding, we usually truncate the words to
    a certain length. The actual length of a question is lost in that data representation.
    Similarly, punctuation is often removed. We can use the Multimodal Input design
    pattern to bring back this lost information to the model.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对原始文本数据进行编码外，文本通常还具有可以表示为表格特征的其他特征。假设我们正在构建一个模型来预测是否会对Stack Overflow问题做出回应。关于文本但与确切单词本身无关的各种因素可能对训练此任务的模型有影响。例如，问题的长度或是否有问号可能影响答案的可能性。然而，当我们创建嵌入时，通常会将单词截断到一定长度，这样实际的问题长度就在数据表示中丢失了。类似地，标点通常会被移除。我们可以使用多模态输入设计模式将丢失的这些信息重新引入模型。
- en: 'In the following query, we’ll extract some tabular features from the `title`
    field of the Stack Overflow dataset to predict whether or not a question will
    get an answer:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的查询中，我们将从 Stack Overflow 数据集的 `title` 字段中提取一些表格特征，以预测问题是否会得到回答：
- en: '[PRE41]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This results in the following:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下结果：
- en: '| Row | title_len | word_count | ends_with_q_mark | is_answered |  |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| Row | title_len | word_count | ends_with_q_mark | is_answered |  |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 84 | 14 | true | 0 |  |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 84 | 14 | true | 0 |  |'
- en: '| 2 | 104 | 16 | false | 0 |  |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 104 | 16 | false | 0 |  |'
- en: '| 3 | 85 | 19 | true | 1 |  |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 85 | 19 | true | 1 |  |'
- en: '| 4 | 88 | 14 | false | 1 |  |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 88 | 14 | false | 1 |  |'
- en: '| 5 | 17 | 3 | false | 1 |  |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 17 | 3 | false | 1 |  |'
- en: In addition to these features extracted directly from a question’s title, we
    could also represent *metadata* about the question as features. For example, we
    could add features representing the number of tags the question had and the day
    of the week it was posted. We could then combine these tabular features with our
    encoded text and feed both representations into our model using Keras’s Concatenate
    layer to combine the BOW-encoded text array with the tabular metadata describing
    our text.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接从问题标题中提取的这些特征外，我们还可以表示问题的 *元数据* 作为特征。例如，我们可以添加代表问题标签数量和问题发布的星期几的特征。然后，我们可以使用
    Keras 的 Concatenate 层将这些表格特征与我们的编码文本组合起来，将两种表示同时输入模型。
- en: Multimodal representation of images
  id: totrans-507
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像的多模态表示
- en: 'Similar to our analysis of embeddings and BOW encoding for text, there are
    many ways to represent image data when preparing it for an ML model. Like raw
    text, images cannot be fed directly into a model and need to be transformed into
    a numerical format that the model can understand. We’ll start by discussing some
    common approaches to representing image data: as pixel values, as sets of tiles,
    and as sets of windowed sequences. The Multimodal Input design pattern provides
    a way to use more than one representation of an image in our model.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们对文本嵌入和 BOW 编码的分析类似，准备 ML 模型时表示图像数据的方法有很多种。像原始文本一样，图像不能直接输入模型，需要将其转换为模型能理解的数值格式。我们将首先讨论一些常见的图像数据表示方法：作为像素值、作为瓦片集以及作为窗口化序列集。多模态输入设计模式提供了一种在模型中使用图像的多种表示方式的方法。
- en: Images as pixel values
  id: totrans-509
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像作为像素值
- en: At their core, images are arrays of pixel values. A black and white image, for
    example, contains pixel values ranging from 0 to 255\. We could therefore represent
    a 28×28-pixel black-and-white image in a model as a 28×28 array with integer values
    ranging from 0 to 255\. In this section, we’ll be referencing the MNIST dataset,
    a popular ML dataset that includes images of handwritten digits.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的核心是像素值数组。例如，黑白图像的像素值范围从 0 到 255。因此，我们可以将一个 28×28 像素的黑白图像表示为一个 28×28 的整数值数组，值的范围从
    0 到 255。在本节中，我们将引用 MNIST 数据集，这是一个包含手写数字图像的流行 ML 数据集。
- en: 'With the `Sequential` API, we can represent our MNIST images of pixel values
    using a Flatten layer, which flattens the image into a one-dimensional 784 (28
    * 28) element array:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Sequential` API，我们可以使用 Flatten 层来表示我们的 MNIST 图像，将其像素值展平为一个一维的 784 (28 *
    28) 元素数组：
- en: '[PRE42]'
  id: totrans-512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'For color images, this gets more complex. Each pixel in an RGB color image
    has three values—one for red, green, and blue. If our images in the example above
    were instead color, we’d add a third dimension to the model’s `input_shape` such
    that it would be:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 对于彩色图像，情况变得更加复杂。RGB 彩色图像中的每个像素具有三个值——红色、绿色和蓝色。如果我们上面的例子中的图像是彩色的，我们会将一个第三维添加到模型的
    `input_shape`，使其变为：
- en: '[PRE43]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: While representing images as arrays of pixel values works well for simple images
    like the grayscale ones in the MNIST dataset, it starts to break down when we
    introduce images with more edges and shapes throughout. When a network is fed
    with all of the pixels in an image at once, it’s hard for it to focus on smaller
    areas of adjacent pixels that contain important information.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管将图像表示为像素值数组对于像 MNIST 数据集中的灰度图像这样的简单图像效果很好，但当我们引入具有更多边缘和形状的图像时，它开始出现问题。当网络一次性输入图像中的所有像素时，它很难集中在包含重要信息的相邻像素的较小区域上。
- en: Images as tiled structures
  id: totrans-516
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图像作为瓦片结构
- en: We need a way to represent more complex, real-world images that will enable
    our model to extract meaningful details and understand patterns. If we feed the
    network only small pieces of an image at a time, it’ll be more likely to identify
    things like spatial gradients and edges present in neighboring pixels. A common
    model architecture for accomplishing this is a *convolutional neural network*(CNN).
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种表示更复杂的真实世界图像的方法，以使我们的模型能够提取有意义的细节并理解模式。如果我们每次只向网络输入图像的小部分，它更可能识别出相邻像素中存在的空间梯度和边缘等内容。用于实现这一点的常见模型架构是卷积神经网络（*convolutional
    neural network*，CNN）。
- en: 'Keras provides convolution layers to build models that split images into smaller,
    windowed chunks. Let’s say we’re building a model to classify 28×28 color images
    as either “dog” or “cat.” Since these images are color, each image will be represented
    as a 28×28×3-dimensional array, since each pixel has three color channels. Here’s
    how we’d define the inputs to this model using a convolution layer and the `Sequential`
    API:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了卷积层来构建将图像分割为较小、窗口化块的模型。假设我们正在构建一个将28×28彩色图像分类为“狗”或“猫”的模型。由于这些图像是彩色的，每个图像将被表示为一个28×28×3维数组，因为每个像素具有三个颜色通道。下面是我们如何使用卷积层和`Sequential`
    API定义这个模型的输入：
- en: '[PRE44]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In this example, we’re dividing our input images into 3×3 chunks before passing
    them through a max pooling layer. Building a model architecture that splits images
    into chunks of sliding windows allows our model to recognize more granular details
    in an image like edges and shapes.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将输入图像分成3×3块，然后通过最大池化层传递它们。构建一个将图像分割为滑动窗口块的模型架构使我们的模型能够识别图像中更细粒度的细节，如边缘和形状。
- en: Combining different image representations
  id: totrans-521
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结合不同的图像表示方式
- en: In addition, as with the bag of words and text embedding, it may be useful to
    represent the same image data in multiple ways. Again, we can accomplish this
    with the Keras functional API.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与词袋和文本嵌入类似，用多种方式表示相同的图像数据可能是有用的。同样，我们可以通过Keras功能API实现这一点。
- en: 'Here’s how we’d combine our pixel values with the sliding window representation
    using the Keras Concatenate layer:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们如何使用Keras Concatenate层将像素值与滑动窗口表示组合起来：
- en: '[PRE45]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'To define a model that accepts that multimodal input representation, we can
    then feed our concatenated layer into our output layer:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义一个接受多模态输入表示的模型，我们可以将拼接层的输出馈送到输出层中：
- en: '[PRE46]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Choosing which image representation to use or whether to use multimodal representations
    depends largely on the type of image data we’re working with. In general, the
    more detailed our images, the more likely it is that we’ll want to represent them
    as tiles or sliding windows of tiles. For the MNIST dataset, representing images
    as pixel values alone may suffice. With complex medical images, on the other hand,
    we may see increased accuracy by combining multiple representations. Why combine
    multiple image representations? Representing images as pixel values allows the
    model to identify higher-level focus points in an image like dominant, high-contrast
    objects. Tiled representations, on the other hand, help models identify more granular,
    lower-contrast edges and shapes.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 选择使用哪种图像表示或是否使用多模态表示主要取决于我们处理的图像数据类型。通常来说，我们的图像越详细，我们越有可能将它们表示为瓷砖或瓦片的滑动窗口。对于MNIST数据集，仅将图像表示为像素值可能已经足够了。另一方面，对于复杂的医学图像，通过结合多种表示可能会提高准确性。为什么要结合多种图像表示呢？将图像表示为像素值使模型能够识别图像中的高级焦点，如显著的高对比度对象。而瓦片表示则帮助模型识别更细粒度的、低对比度的边缘和形状。
- en: '*#### Using images with metadata'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '-   使用带元数据的图像'
- en: Earlier we discussed different types of metadata that might be associated with
    text, and how to extract and represent this metadata as tabular features for our
    model. We can also apply this concept to images. To do this, let’s return to the
    example referenced in [Figure 2-19](#model_combining_image_and_numerical_fea)
    of a model using footage of an intersection to predict whether or not it contains
    a traffic violation. Our model can extract many patterns from the traffic images
    on their own, but there may be other data available that could improve our model’s
    accuracy. For example, maybe certain behavior (e.g., a right turn on red) is not
    permitted during rush hour but is OK at other times of day. Or maybe drivers are
    more likely to violate traffic laws in bad weather. If we’re collecting image
    data from multiple intersections, knowing the location of our image might also
    be useful to our model.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们讨论了可能与文本关联的不同类型的元数据，以及如何提取和表示这些元数据作为我们模型的表格特征。我们还可以将这个概念应用到图像上。为此，让我们回到一个例子，即模型使用十字路口的摄像头镜头来预测它是否包含交通违规，如[图2-19](#model_combining_image_and_numerical_fea)中引用的例子。我们的模型可以单独从交通图片中提取许多模式，但可能还有其他可用的数据可以提高我们模型的准确性。例如，也许某些行为（例如，红绿灯右转）在交通高峰期间不允许，但在其他时段可以。或者也许驾驶员在恶劣天气下更有可能违反交通法规。如果我们从多个十字路口收集图像数据，了解图像的位置对我们的模型也可能有帮助。
- en: 'We’ve now identified three additional tabular features that could enhance our
    image model:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经确定了三个可以增强我们图像模型的表格特征：
- en: Time of day
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一天中的时间
- en: Weather
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气
- en: Location
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置
- en: Next, let’s think about possible representations for each of these features.
    We could represent time as an integer indicating the *hour* of the day. This might
    help us identify patterns associated with high-traffic times like rush hour. In
    the context of this model, it may be more useful to know whether or not it was
    dark when the image was taken. In this case, we could represent time as a boolean
    feature.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们考虑每个特征的可能表示。我们可以将时间表示为一个整数，指示*一天中的小时*。这可能有助于我们识别与高流量时间相关的模式，如交通高峰时段。在这个模型的背景下，了解拍摄图片时是否天黑可能更有用。在这种情况下，我们可以将时间表示为一个布尔特征。
- en: Weather can also be represented in various ways, as both numeric and categorical
    values. We could include temperature as a feature, but in this case, visibility
    might be more useful. Another option for representing weather is through a categorical
    variable indicating the presence of rain or snow.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 天气也可以用各种方式表示，既可以是数值也可以是分类值。我们可以将温度包括为一个特征，但在这种情况下，能见度可能更有用。表示天气的另一个选项是通过指示是否有雨或雪的分类变量。
- en: If we’re collecting data from many locations, we’d likely want to encode this
    as a feature as well. This would make most sense as a categorical feature, and
    could even be multiple features (city, country, state, etc.) depending on how
    many locations we’re collecting footage from.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从许多位置收集数据，我们可能希望将其编码为一个特征。这最好作为一个分类特征，根据我们从多少个位置收集镜头，甚至可能是多个特征（城市，国家，州等）。
- en: 'For this example, let’s say we’d like to use the following tabular features:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，假设我们想使用以下表格特征：
- en: Time as hour of the day (integer)
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一天中的小时数（整数）
- en: Visibility (float)
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能见度（浮点数）
- en: 'Inclement weather (categorical: rain, snow, none)'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶劣天气（分类：下雨，下雪，无）
- en: Location ID (categorical with five possible locations)
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置ID（分类，有五个可能的位置）
- en: 'Here’s what a subset of this dataset might look like for the three examples:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对这三个示例数据集子集的一些内容：
- en: '[PRE47]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We could then combine these tabular features into a single array for each example,
    so that our model’s input shape would be 10\. The input array for the first example
    would look like the following:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将这些表格特征组合成每个示例的一个单一数组，这样我们模型的输入形状将是10。第一个示例的输入数组将如下所示：
- en: '[PRE48]'
  id: totrans-545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We could feed this input into a Dense fully connected layer, and the output
    of our model would be a single value between 0 and 1 indicating whether or not
    the instance contains a traffic violation. To combine this with our image data,
    we’ll use a similar approach to what we discussed for text models. First, we’d
    define a convolution layer to handle our image data, then a Dense layer to handle
    our tabular data, and finally we’d concatenate both into a single output.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此输入馈送到一个密集全连接层中，我们模型的输出将是一个介于0和1之间的单个值，指示该实例是否包含交通违规。要将此与我们的图像数据结合起来，我们将使用类似于我们用于文本模型的方法。首先，我们将定义一个卷积层来处理我们的图像数据，然后是一个处理我们的表格数据的密集层，最后我们将两者连接成一个单一输出。
- en: This approach is outlined in [Figure 2-25](#concatenating_layers_to_handle_image_an).
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在[图2-25](#concatenating_layers_to_handle_image_an)中有详细说明。
- en: '![Concatenating layers to handle image and tabular metadata features.](Images/mldp_0225.png)'
  id: totrans-548
  prefs: []
  type: TYPE_IMG
  zh: '![连接层以处理图像和表格元数据特征。](Images/mldp_0225.png)'
- en: Figure 2-25\. Concatenating layers to handle image and tabular metadata features.*  *###
    Multimodal feature representations and model interpretability
  id: totrans-549
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-25\. 连接层以处理图像和表格元数据特征。*### 多模态特征表示和模型可解释性
- en: Deep learning models are inherently difficult to explain. If we build a model
    that achieves 99% accuracy, we still don’t know exactly *how* our model is making
    predictions and consequently, if the way it’s making those predictions is correct.
    For example, let’s say we train a model on images of petri dishes taken in a lab
    that achieves a high accuracy score. These images also contain annotations from
    the scientist who took the pictures. What we don’t know is that the model is incorrectly
    using the annotations to make its predictions, rather than the contents of the
    petri dishes.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型本质上难以解释。即使我们构建了一个准确率达到99%的模型，我们仍然不知道模型究竟是如何进行预测的，因此也不知道它是否正确地进行预测。例如，假设我们在实验室拍摄的培养皿图像上训练了一个高准确率的模型。这些图像还包含了科学家拍摄时的注释信息。我们不知道的是，该模型实际上是错误地使用了这些注释来进行预测，而不是培养皿的内容。
- en: There are several techniques for explaining image models that can highlight
    the pixels that signaled a model’s prediction. When we combine multiple data representations
    in a single model, however, these features become dependent on one another. As
    a result, it can be difficult to explain how the model is making predictions.
    Explainability is covered in [Chapter 7](ch07.xhtml#responsible_ai).*  *# Summary
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种解释图像模型的技术可以突出显示指示模型预测的像素。然而，当我们在单一模型中结合多个数据表示时，这些特征变得相互依赖。因此，解释模型如何进行预测可能会变得困难。可解释性在[第7章](ch07.xhtml#responsible_ai)中有所涵盖。*#
    总结
- en: In this chapter, we learned different approaches to representing data for our
    model. We started by looking at how to handle numerical inputs, and how scaling
    these inputs can speed up model training time and improve accuracy. Then we explored
    how to do feature engineering on categorical inputs, specifically with one-hot
    encoding and using arrays of categorical values.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了表示模型数据的不同方法。我们首先讨论了如何处理数值输入，以及如何通过缩放这些输入来加快模型训练时间并提高准确性。然后，我们探讨了如何对分类输入进行特征工程，特别是使用独热编码和使用分类值数组。
- en: Throughout the rest of the chapter, we discussed four design patterns for representing
    data. The first was the *Hashed Feature* design pattern, which involves encoding
    categorical inputs as unique strings. We explored a few different approaches to
    hashing using the airport dataset in BigQuery. The second pattern we looked at
    in this chapter was *Embeddings*, a technique for representing high-cardinality
    data such as inputs with many possible categories or text data. Embeddings represent
    data in multidimensional space, where the dimension is dependent on our data and
    prediction task. Next we looked at *Feature Crosses*, an approach that joins two
    features to extract relationships that may not have been easily captured by encoding
    the features on their own. Finally, we looked at *Multimodal Input* representations
    by addressing the problem of how to combine inputs of different types into the
    same model, and how a single feature can be represented multiple ways.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们讨论了四种表示数据的设计模式。第一种是*散列特征*设计模式，它涉及将分类输入编码为唯一的字符串。我们使用BigQuery中的机场数据集探讨了几种不同的散列方法。本章中我们看到的第二种模式是*嵌入*，这是一种用于表示高基数数据的技术，例如具有许多可能类别或文本数据的输入。嵌入将数据表示为多维空间中的点，其中维数取决于我们的数据和预测任务。接下来，我们看了*特征交叉*，这是一种将两个特征结合起来以提取关系的方法，这些关系可能无法通过单独编码特征来捕捉。最后，我们看了*多模态输入*表示法，解决了如何将不同类型的输入合并到同一模型中的问题，以及如何以多种方式表示单一特征。
- en: This chapter focused on preparing *input* data for our models. In the next chapter,
    we’ll focus on model *output* by diving into different approaches for representing
    our prediction task.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点是为我们的模型准备*输入*数据。在下一章中，我们将着眼于模型*输出*，深入探讨表示我们预测任务的不同方法。
- en: ^([1](ch02.xhtml#ch01fn1-marker)) Here, the learned data representation consists
    of `baby weight` as the input variable, the less than operator, and the threshold
    of 3 kg.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.xhtml#ch01fn1-marker)) 在这里，学习的数据表示包括`baby weight`作为输入变量，小于操作符，以及3公斤的阈值。
- en: ^([2](ch02.xhtml#ch01fn2-marker)) If twins, the plurality is 2\. If triplets,
    the plurality is 3.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.xhtml#ch01fn2-marker)) 如果是双胞胎，则多数是2。如果是三胞胎，则多数是3。
- en: '^([3](ch02.xhtml#ch01fn3-marker)) This dataset is available in BigQuery: *bigquery-public-data.samples.natality*.'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.xhtml#ch01fn3-marker)) 此数据集在BigQuery中可用：*bigquery-public-data.samples.natality*。
- en: '^([4](ch02.xhtml#ch01fn4-marker)) This dataset is available in BigQuery: *bigquery-public-data.hacker_news.stories*.'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.xhtml#ch01fn4-marker)) 此数据集在BigQuery中可用：*bigquery-public-data.hacker_news.stories*。
- en: ^([5](ch02.xhtml#ch01fn5-marker)) The feature_cross.ipynb notebook in the book’s
    [repository](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/feature_cross.ipynb)
    of this book will help you follow the discussion better.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch02.xhtml#ch01fn5-marker)) 本书的[代码库](https://github.com/GoogleCloudPlatform/ml-design-patterns/blob/master/02_data_representation/feature_cross.ipynb)中的feature_cross.ipynb笔记本将帮助您更好地跟进讨论。
- en: ^([6](ch02.xhtml#ch01fn6-marker)) Full code is in *02_data_representation/feature_cross.ipynb*
    in the code repository of this book.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch02.xhtml#ch01fn6-marker)) 本书的代码库中*02_data_representation/feature_cross.ipynb*包含完整代码。
- en: ^([7](ch02.xhtml#ch01fn7-marker)) We use the term “tabular data” to refer to
    numerical and categorical inputs, but not free-form text. You can think of tabular
    data as anything you might commonly find in a spreadsheet. For example, values
    like age, type of car, price, or number of hours worked. Tabular data does not
    include free-form text like descriptions or reviews.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch02.xhtml#ch01fn7-marker)) 我们使用术语“表格数据”来指代数值和分类输入，但不包括自由形式的文本。您可以将表格数据视为在电子表格中常见的任何内容。例如，如年龄、汽车类型、价格或工作小时数等数值。表格数据不包括如描述或评论等自由形式文本。
- en: ^([8](ch02.xhtml#ch01fn8-marker)) When we pass an encoded 30-word array to our
    model, the Keras layer will transform it into a 64-dimensional embedding representation,
    so we’ll have a [64×30] matrix representing the review.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch02.xhtml#ch01fn8-marker)) 当我们将一个编码为30个词的数组传递给我们的模型时，Keras层将其转换为64维嵌入表示，因此我们将拥有一个表示评论的[64×30]矩阵。
- en: ^([9](ch02.xhtml#ch01fn9-marker)) The starting point is an array that is 1,920
    numbers.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch02.xhtml#ch01fn9-marker)) 起点是一个包含1,920个数字的数组。
- en: ^([10](ch02.xhtml#ch01fn10-marker)) See *02_data_representation/mixed_representation.ipynb*
    in the code repository of this book for the full model code.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch02.xhtml#ch01fn10-marker)) 请参阅本书代码库中的*02_data_representation/mixed_representation.ipynb*获取完整的模型代码。
- en: '^([11](ch02.xhtml#ch01fn11-marker)) This dataset is available in BigQuery:
    *bigquery-public-data.stackoverflow.posts_questions*.***'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch02.xhtml#ch01fn11-marker)) 此数据集在BigQuery中可用：*bigquery-public-data.stackoverflow.posts_questions*。
