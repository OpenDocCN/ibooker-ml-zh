- en: Chapter 6\. Training Models with Spark MLlib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。使用Spark MLlib训练模型
- en: Now that you’ve learned about managing machine learning experiments, getting
    a feel for the data, and feature engineering*,* it’s time to train some models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了管理机器学习实验、感受数据和特征工程*，* 是时候开始训练一些模型了。
- en: What does that involve exactly? *Training* a model is the process of adjusting
    or changing model parameters so that its performance improves. The idea here is
    to feed your machine learning model training data that teaches it how to solve
    a specific task—for example, classifying an object on a photo as a cat by identifying
    its “cat” properties.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这究竟涉及什么？*训练*模型是调整或更改模型参数的过程，以提高其性能。这里的思想是向您的机器学习模型提供训练数据，教它如何解决特定任务，例如通过识别其“猫”属性将照片中的对象分类为猫。
- en: In this chapter, you will learn how machine learning algorithms work, when to
    use which tool, how to validate your model, and, most importantly, how to automate
    the process with the Spark MLlib Pipelines API.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习机器学习算法的工作原理，何时使用哪个工具，如何验证您的模型，以及最重要的是如何使用Spark MLlib管道API自动化这一过程。
- en: 'At a high level, this chapter covers the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，本章涵盖以下内容：
- en: Basic Spark machine learning algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的Spark机器学习算法
- en: Supervised machine learning with Spark machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark机器学习进行监督学习
- en: Unsupervised machine learning with Spark machine learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark机器学习进行无监督学习
- en: Evaluating your model and testing it
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估您的模型并对其进行测试
- en: Hyperparameters and tuning your model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数和调整您的模型
- en: Using Spark machine learning pipelines
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark机器学习管道
- en: Persisting models and pipelines to disk
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型和管道持久化到磁盘
- en: Algorithms
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法
- en: Let’s start with algorithms, the essential part of your model training activities.
    The input of a machine learning algorithm is sample data, and its output is a
    model. The algorithm’s goal is to generalize the problem and extract a set of
    logic for making predictions and decisions without being explicitly programmed
    to do so. Algorithms can be based on statistics, mathematical optimization, pattern
    detection, and so on. Spark MLlib provides us with distributed training implementations
    for the classic *supervised* machine learning algorithms, such as classification,
    regression, and recommendation. It also includes implementations for *unsupervised*
    machine learning algorithms, such as clustering and pattern mining, which are
    often used to detect anomalies.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从算法开始，这是您模型训练活动中不可或缺的部分。机器学习算法的输入是样本数据，输出是模型。算法的目标是泛化问题，并提取一组逻辑，用于进行预测和决策，而无需明确编程。算法可以基于统计学、数学优化、模式检测等。Spark
    MLlib为我们提供了经典*监督*机器学习算法的分布式训练实现，例如分类、回归和推荐。它还包括了*无监督*机器学习算法的实现，例如聚类和模式挖掘，这些算法通常用于检测异常。
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It is worth noting that at the time of writing, there is not yet feature parity
    between the MLlib [RDD-based](https://oreil.ly/r0MAS) and [DataFrame-based](https://oreil.ly/lQOLE)
    APIs, so there may be cases where the functionality you need can only be found
    in the RDD-based API. Singular value decomposition (SVD) is one example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在撰写本文时，MLlib的RDD-based和DataFrame-based API尚未具备功能平等性，因此可能存在只能在RDD-based
    API中找到所需功能的情况。奇异值分解（SVD）就是一个例子。
- en: How do you pick the right algorithm for the job? Your choice always depends
    on your objectives and your data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如何选择适合任务的算法？您的选择始终取决于您的目标和数据。
- en: While this chapter will cover many algorithms and their respective use cases,
    the topics of deep learning, integration with PyTorch, and TensorFlow distributed
    strategies will be discussed in Chapters [7](ch07.xhtml#bridging_spark_and_deep_learning_framew)
    and [8](ch08.xhtml#tensorflow_distributed_ml_approach).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章将涵盖许多算法及其各自的用例，但深度学习、与PyTorch集成以及TensorFlow分布式策略的主题将在第[7](ch07.xhtml#bridging_spark_and_deep_learning_framew)章和第[8](ch08.xhtml#tensorflow_distributed_ml_approach)章讨论。
- en: 'I would like to draw your attention to the fact that the MLlib model instance
    has dedicated functionality for parameter documentation. The following code sample
    illustrates how you can immediately access the documentation for the individual
    parameters once you have created an instance of the model:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我想提醒您，MLlib模型实例具有专门的参数文档功能。以下代码示例说明了一旦创建了模型实例，如何立即访问各个参数的文档：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Example 6-1](#example_of_pretty_printing_the_gaussian) shows some sample output
    from the `model.explainParams` function*.* Since this is a `GaussianMixture` model
    (discussed in [“Gaussian Mixture”](#GaussianMixture_desc)), it contains descriptions
    of the params that are available for tuning with that type of model. This is a
    great tool to help you in your educational journey as you explore MLlib algorithms
    and learn about each one and their model outputs.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-1](#example_of_pretty_printing_the_gaussian) 展示了`model.explainParams`函数的一些样本输出*.*
    由于这是一个`GaussianMixture`模型（在[“高斯混合”](#GaussianMixture_desc)讨论中），它包含可用于调整此类型模型的参数的描述。这是一个很好的工具，可以帮助您在探索MLlib算法并了解每个算法及其模型输出时进行教育性的旅程。'
- en: Example 6-1\. Example of pretty-printing the `GaussianMixture` model params
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. 漂亮地打印`GaussianMixture`模型参数的示例
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we’ve covered the basics, our learning journey for this chapter starts
    with supervised machine learning. Let’s dive in.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了基础知识，本章的学习之旅从监督机器学习开始。让我们深入探讨。
- en: Supervised Machine Learning
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督机器学习
- en: 'All supervised algorithms expect to have a `label` column as part of the data.
    This allows the algorithm to “validate” itself during the training phase and estimate
    how well it performs. In other words, the `label` column is what the algorithm
    uses to correct its decisions. In the testing phase, we use it to assess the quality
    of the algorithm by comparing the model’s predictions to the real outcomes. The
    label can be a discrete/categorical variable—that is, a specific value among a
    set of all possible values, such as `apple` when you are categorizing between
    apples and oranges—or a continuous variable, such as a person’s height or age.
    This difference defines which kind of task we want our model to solve: classification
    or regression.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所有监督算法都期望数据中有一个`label`列。这允许算法在训练阶段“验证”自己的表现并估计其表现如何。换句话说，在测试阶段，我们使用它来通过比较模型的预测与真实结果来评估算法的质量。标签可以是离散/分类变量，即在所有可能值的集合中的具体值，例如在分类苹果和橙子时是`apple`，或者连续变量，例如一个人的身高或年龄。这种差异定义了我们希望我们的模型解决的任务类型：分类还是回归。
- en: In some cases, the label itself might be a set of labels; we’ll talk about that
    possibility in the next section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，标签本身可能是一组标签；我们将在下一节讨论这种可能性。
- en: Classification
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类
- en: '*Classification* is the task of calculating the probability of a data point
    belonging to discrete categories, or classes, by examining input features. The
    output of that process is a prediction of the probability of the data point belonging
    to each possible category. Many practitioners confuse regression and classification
    due to the existence of the *logistic regression* algorithm, often used for binary
    classification. While logistic regression outputs the probability of a discrete
    class, similar to classification algorithms, other regression algorithms are used
    for predicting continuous numeric values. Pay attention to this difference!'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*分类* 是通过检查输入特征来计算数据点属于离散类别或类别的概率的任务。这个过程的输出是对数据点属于每个可能类别的概率的预测。许多实践者因为存在*逻辑回归*算法而混淆回归和分类。虽然逻辑回归输出离散类的概率，类似于分类算法，其他回归算法用于预测连续数值。请注意这种区别！'
- en: 'There are three types of classification:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种类型的分类：
- en: Binary
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制
- en: Each input is classified into one of two classes (yes or no, true or false,
    etc.).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入被分类到两个类别中的一个（是或否，真或假等）。
- en: Multiclass
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 多类
- en: Each input is classified into one of a set of more than two classes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入被分类到一组超过两个类别中的一个。
- en: Multilabel
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签
- en: Each given input can have multiple labels in practice. For example, a sentence
    might have two sentiment classifications, such as *happy* and *fulfilled*. Spark
    does not support this out of the box; you will need to train each classifier separately
    and combine the outcomes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，每个给定的输入可以有多个标签。例如，一句话可能有两个情感分类，如*开心*和*满足*。Spark 不支持此功能；您需要分别训练每个分类器并组合结果。
- en: 'In addition, the distribution of classes in the training data impacts the classification
    process. Data labels are said to be *imbalanced* when the input data is distributed
    unevenly across classes. You often see this in fraud detection and medical diagnosis
    use cases, and when facing such scenarios, you will need to consider and weight
    your features accordingly, if possible. Imbalances can also appear in the training,
    validation, and testing sets: all three need to be balanced in order to provide
    the desired results. We’ll look more closely at this problem, and at dealing with
    multilabel classification scenarios, in the following subsections.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，训练数据中类的分布影响分类过程。当输入数据在类别之间分布不均匀时，数据标签被称为*不平衡*。在欺诈检测和医学诊断用例中经常见到这种情况，面对这些场景时，您需要考虑并根据可能的情况对特征加权。在训练、验证和测试集中也可能出现不平衡：为了提供期望的结果，这三者都需要平衡。在接下来的小节中，我们将更详细地探讨这个问题，以及如何处理多标签分类场景。
- en: MLlib classification algorithms
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLlib 分类算法
- en: Classification algorithms expect an indexed label (often in the range of `[0,1]`)
    and a vector of indexed features. APIs for transforming categorical features into
    indices, such as `StringIndexer` and `VectorIndexer`, were discussed in [Chapter 4](ch04.xhtml#data_ingestioncomma_preprocessingcomma).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分类算法期望一个索引标签（通常在 `[0,1]` 范围内）和一个索引特征向量。用于将分类特征转换为索引的 API，例如 `StringIndexer`
    和 `VectorIndexer`，在[第 4 章](ch04.xhtml#data_ingestioncomma_preprocessingcomma)中已经讨论过。
- en: 'MLlib implements several popular classification algorithms, listed in [Table 6-1](#mllib_classification_algorithms).
    The class name pattern is typically `*{name}*``Classifier` or just `*{name}*`,
    and after training, the classifiers produce a model with a corresponding name:
    `*{name}*``Classification​Model` or `*{name}*``Model`. For example, MLlib’s `GBTClassifier`
    fits a `GBT​Classifica⁠tion​Model`, and `NaiveBayes` fits a `NaiveBayesModel`.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 实现了几种流行的分类算法，列在[表 6-1](#mllib_classification_algorithms)中。类名模式通常为 `*{name}*``Classifier`
    或仅为 `*{name}*`，训练后，分类器会生成一个相应名称的模型：`*{name}*``Classification​Model` 或 `*{name}*``Model`。例如，MLlib
    的 `GBTClassifier` 拟合一个 `GBT​Classifica⁠tion​Model`，而 `NaiveBayes` 拟合一个 `NaiveBayesModel`。
- en: Table 6-1\. MLlib classification algorithms
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-1\. MLlib 分类算法
- en: '| API | Usage |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| API | 用法 |'
- en: '| --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `Logistic​Re⁠gression` | Binary and multiclass classifier. Can be trained
    on streaming data with the RDD-based API. Expects an indexed label and a vector
    of indexed features. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| `Logistic​Re⁠gression` | 二元和多类分类器。可以使用基于 RDD 的 API 在流数据上训练。期望一个索引标签和一个索引特征向量。
    |'
- en: '| `DecisionTree​Classifier` | Binary and multiclass decision tree classifier.
    Expects an indexed label and a vector of indexed features. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| `DecisionTree​Classifier` | 二元和多类决策树分类器。期望一个索引标签和一个索引特征向量。 |'
- en: '| `RandomForest​Classifier` | Binary and multiclass classifier. A random forest
    is a group or ensemble of individual decision trees, each trained on discrete
    values. Expects an indexed label and a vector of indexed features. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `RandomForest​Classifier` | 二元和多类分类器。随机森林是由多个单独的决策树组成的集合或集成，每棵树都训练有离散值。期望一个索引标签和一个索引特征向量。
    |'
- en: '| `GBTClassifier` | Binary gradient boosted trees classifier (supported in
    Spark v3.1.1 and later). Like the `RandomForestClassifier`, this is an ensemble
    of decision trees. However, its training process is different; as a result, it
    can be used for regression as well. Expects an indexed label and a vector of indexed
    features. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| `GBTClassifier` | 二元梯度提升树分类器（在 Spark v3.1.1 及更高版本中支持）。与 `RandomForestClassifier`
    类似，这是一组决策树的集合。然而，其训练过程不同；因此，它也可用于回归问题。期望一个索引标签和一个索引特征向量。 |'
- en: '| `Multilayer​Per⁠ceptron Clas⁠sifier` | Multiclass classifier based on a feed-forward
    artificial neural network. Expects layer sizes, a vector of indexed features,
    and indexed labels. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| `Multilayer​Per⁠ceptron Clas⁠sifier` | 基于前馈人工神经网络的多类分类器。期望层大小、一个索引特征向量和索引标签。
    |'
- en: '| `LinearSVC` | Linear support vector machine classifier (binary). Expects
    an indexed label and a vector of indexed features. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| `LinearSVC` | 线性支持向量机分类器（二元）。期望一个索引标签和一个索引特征向量。 |'
- en: '| `OneVsRest` | Used to reduce multiclass classification to binary classification,
    using a one-versus-all strategy. Expects a binary classifier, a vector of indexed
    features, and indexed labels. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| `OneVsRest` | 用于将多类分类减少为二元分类，采用一对多策略。期望一个二元分类器，一个索引特征向量和索引标签。 |'
- en: '| `NaiveBayes` | Multiclass classifier, considered efficient as it runs only
    one pass over the training data. Expects a `Double` for the weight of a data point
    (to correct for a skewed label distribution), indexed labels, and a vector of
    indexed features. Returns the probability for each label. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| `NaiveBayes` | 多类分类器，由于仅在训练数据上运行一次，因此被认为效率高。期望数据点的`Double`权重（用于校正偏斜的标签分布）、索引标签和索引特征的向量。返回每个标签的概率。
    |'
- en: '| `FMClassifier` | Binary factorization machines classifier. Expects indexed
    labels and a vector of indexed features. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| `FMClassifier` | 二元因子化机器分类器。期望索引标签和索引特征的向量。 |'
- en: Implementing multilabel classification support
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现多标签分类支持
- en: 'MLlib doesn’t support multilabel classification out of the box, but there are
    several approaches we can take to work around this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 不直接支持多标签分类，但我们可以采取几种方法来解决这个问题：
- en: Search for another tool that does and that can train on a large dataset.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 寻找另一个可以在大型数据集上训练的工具。
- en: Train binary classifiers for each of the labels and output multilabel classifications
    by running relevant/irrelevant predictions for each one.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个标签训练二元分类器，并通过运行相关/不相关预测来输出多标签分类。
- en: Think of a way to leverage your existing tools by breaking the task into pieces,
    solving each subtask independently, and then combining the results using code.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 想出一种方法，通过将任务分解成几个部分，分别解决每个子任务，然后使用代码将结果合并，以利用现有工具。
- en: The good news with regard to the first option is that PyTorch and TensorFlow
    both support multilabel classification algorithms, so we can take advantage of
    their capabilities for multilabel use cases.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 就第一个选项而言的好消息是，PyTorch 和 TensorFlow 都支持多标签分类算法，因此我们可以利用它们在多标签用例中的功能。
- en: 'As for the second option, if you are an AI engineer or an experienced Spark
    developer, Spark provides a rich API you can use to execute the following steps:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 至于第二个选项，如果您是AI工程师或经验丰富的Spark开发人员，Spark提供了丰富的API，您可以使用它来执行以下步骤：
- en: Add multiple columns to the existing DataFrame, each representing a given label.
    So, for example, if your original DataFrame had only the columns `id`, `sentence`,
    and `sentiment`, you would add each sentiment category as its own column. A row
    with the value `[happy]` in the `sentiment` column would get the value `1.0` in
    the new column named `is_happy` and `0.0` in the `is_indifferent`, `is_​ful⁠fil⁠led`,
    and `is_sad` columns; a row with `[happy, indifferent]` in the `sen⁠ti⁠ment` column
    would get `1.0` in the `is_happy` and `is_indifferent` columns and `0.0` in the
    others. This way, a sentence can be classified as belonging to multiple categories.
    [Figure 6-1](#dataframe_output_example_for_multilabel) illustrates what this looks
    like.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向现有 DataFrame 添加多个列，每个列代表一个给定的标签。例如，如果您的原始 DataFrame 只有`id`、`sentence`和`sentiment`列，您将为每个情绪类别添加自己的列。在`sentiment`列中值为`[happy]`的行将在名为`is_happy`的新列中获得值`1.0`，在`is_indifferent`、`is_fulfilled`和`is_sad`列中获得值`0.0`；在`sentiment`列中值为`[happy,
    indifferent]`的行将在`is_happy`和`is_indifferent`列中获得值`1.0`，其他列中获得值`0.0`。这样，一个句子可以被分类为属于多个类别。[图 6-1](#dataframe_output_example_for_multilabel)说明了这是什么样子。
- en: '![DataFrame output example for multilabel classification](assets/smls_0601.png)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![多标签分类的 DataFrame 输出示例](assets/smls_0601.png)'
- en: Figure 6-1\. DataFrame output example for multilabel classification
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. 多标签分类的 DataFrame 输出示例
- en: Continue the featurization process for each label. The book’s [GitHub repo](https://oreil.ly/smls-git)
    includes code that uses `HashingTF`, `IDF`, and other methods to prepare the DataFrame
    for training the classifier, as shown in [Example 6-2](#dataframe_ready_for_training_the_first).
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续为每个标签进行特征化处理。该书的[GitHub 仓库](https://oreil.ly/smls-git)包含使用`HashingTF`、`IDF`和其他方法准备
    DataFrame 以训练分类器的代码，如[示例 6-2](#dataframe_ready_for_training_the_first)所示。
- en: Example 6-2\. DataFrame ready for training the first classifier for the `happy`
    label
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-2\. 为`happy`标签准备的 DataFrame，用于训练第一个分类器
- en: '[PRE2]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Build a binary classifier for each of the labels. This code snippet shows you
    how to build a `LogisticRegression` classifier after the transformations of adding
    columns and indexing:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在为每个标签构建二元分类器。此代码片段显示了如何在添加列和索引转换后构建`LogisticRegression`分类器：
- en: '[PRE3]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You’ll need to apply the same process to all the rest of the labels.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您需要将相同的过程应用于其余所有标签。
- en: Remember, there are more steps to the machine learning pipeline, like evaluating
    the outcome with the testing dataset and using the classifiers you just built
    together.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，机器学习流水线还有更多步骤，例如使用测试数据集评估结果，并使用刚刚构建的分类器。
- en: 'For testing the model, you call the `transform` function on your test DataFrame:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试模型，请在您的测试DataFrame上调用`transform`函数：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Example 6-3](#spearman_correlation_matrix) shows the output of testing the
    model.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-3](#spearman_correlation_matrix) 展示了对模型进行测试的输出。'
- en: Example 6-3\. Spearman correlation matrix
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-3\. 斯皮尔曼相关矩阵
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is a `DenseMatrix`, as discussed in [Chapter 4](ch04.xhtml#data_ingestioncomma_preprocessingcomma),
    that enables us to understand the result of the `LogisticRegression` prediction.
    You can find it in the `rawPrediction` column (a vector of doubles giving the
    measure of confidence in each possible label) of the prediction DataFrame, followed
    by the probability vector (the conditional probability for each class) in the
    `probability` column and the prediction itself in the `prediction` column. Note
    that not all models output accurate probabilities; therefore, the probability
    vector should be used with caution.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第四章](ch04.xhtml#data_ingestioncomma_preprocessingcomma)中讨论的那样，这是一个`DenseMatrix`，它能让我们理解`LogisticRegression`预测的结果。您可以在预测DataFrame的`rawPrediction`列中找到它（一个包含对每个可能标签的置信度测量的双精度向量），接着是`probability`列中的概率向量（每个类别的条件概率）和`prediction`列中的预测结果。请注意，并非所有模型都能准确输出概率，因此应谨慎使用概率向量。
- en: What about imbalanced class labels?
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么不平衡的类标签怎么办？
- en: As mentioned earlier, imbalanced data can be problematic in classification tasks.
    If you have one class label with a very high number of observations and another
    with a very low number of observations, this can produce a biased model. The bias
    will be toward the class label with the higher number of observations, as it is
    statistically more dominant in the training dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，不平衡的数据在分类任务中可能会造成问题。如果一个类别的观测数量非常高，而另一个类别的观测数量非常低，这可能会产生一个有偏见的模型。偏见将倾向于训练数据集中观测数量更多的类别标签，因为在统计上它在训练数据集中更占主导地位。
- en: 'It is possible to introduce bias in various phases of the model’s development.
    Insufficient data, inconsistent data collection, and poor data practices can all
    lead to bias in a model’s decisions. We are not going to delve into how to solve
    existing model bias problems here but rather will focus on strategies for working
    with the dataset to mitigate potential sources of bias. These strategies include
    the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型开发的各个阶段可能会引入偏见。数据不足、数据收集不一致和数据管理不当都可能导致模型决策中的偏见。我们在这里不会深入探讨如何解决现有的模型偏见问题，而是将专注于处理数据集以减少潜在偏见来源的策略。这些策略包括以下几点：
- en: Filtering the more representative classes and sampling them to downsize the
    number of entries in the overall dataset.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 筛选更具代表性的类别并对其进行采样，以减少整体数据集中的条目数量。
- en: Using ensemble algorithms based on decision trees, such as `GBTClassifier`,
    `GBTRegressor`, and `RandomForestClassifier`. During the training process, these
    algorithms have a dedicated `featureSubsetStrategy` parameter that you can set,
    with supported values of `auto`, `all`, `sqrt`, `log2`, and `onethird`. With the
    default of `auto`, the algorithm chooses the best strategy to go with based on
    the given features. In every tree node, the algorithm processes a random subset
    of features and uses the result to build the next node. It iterates on the same
    procedure until it finishes using all the datasets. This is useful because of
    its random approach to the parameters, but depending on the distribution of the
    observations, there can still be bias in the resulting model. Imagine that you
    have an apples and oranges dataset with 99 apples and 1 orange. Suppose that in
    a random process the algorithm picks a batch of 10 units. It will include at most
    1 orange and either 9 or 10 apples. The distribution remains heavily skewed toward
    apples, so the model will likely end up predicting `apple` 100% of the time—which
    is correct for the training dataset but might be completely off the mark in the
    real world. You can read more about this issue in the [documentation](https://oreil.ly/6sFNX).
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基于决策树的集成算法，如`GBTClassifier`、`GBTRegressor`和`RandomForestClassifier`。在训练过程中，这些算法有一个专用的`featureSubsetStrategy`参数，您可以设置为`auto`、`all`、`sqrt`、`log2`和`onethird`。默认情况下，算法根据给定的特征选择最佳策略。在每个树节点中，算法处理特征的随机子集，并使用结果构建下一个节点。它重复同样的过程，直到使用完所有数据集。这是因为它对参数的随机方法很有用，但是根据观察值的分布，仍然可能存在模型结果的偏差。假设您有一个包含99个苹果和1个橙子的数据集。假设在随机过程中，算法选择了一个批次包含10个单位。它将包含最多1个橙子和9或10个苹果。分布仍然严重偏向苹果，因此模型很可能会始终预测`apple`—这对于训练数据集来说是正确的，但在实际世界中可能完全不准确。您可以在[文档](https://oreil.ly/6sFNX)中详细了解此问题。
- en: 'Here is how to set the strategy:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是设置策略的方法：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Regression
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归
- en: It’s time to learn about regression! This task is also known as *regression
    analysis*—the process of estimating the relationships between one or more dependent
    variables and one or more independent variables. The values of the independent
    variables should allow us to predict the values of the dependent variables. If
    that is not the case, you can use the APIs discussed in [Chapter 5](ch05.xhtml#feature_engineering)
    to select only the features that add value.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候学习回归了！这个任务也被称为*回归分析*—估计一个或多个因变量与一个或多个自变量之间关系的过程。自变量的值应允许我们预测因变量的值。如果情况不是这样，您可以使用第5章讨论的API选择仅添加价值的特征。
- en: 'From a bird’s-eye view, there are three types of regression:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从俯视角度看，有三种类型的回归：
- en: Simple
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 简单
- en: 'There is only one independent and one dependent variable: one value for training
    and one to predict.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个自变量和一个因变量：一个值用于训练，一个值用于预测。
- en: Multiple
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 多重
- en: Here we have one dependent variable to predict using multiple independent variables
    for training and input.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有一个因变量需要使用多个独立变量进行训练和输入预测。
- en: Multivariate
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 多元
- en: Similar to multilabel classification, there are multiple variables to predict
    with multiple independent variables for training and input. Accordingly, the input
    and output are vectors of numeric values.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与多标签分类类似，有多个变量需要使用多个独立变量进行训练和输入预测。因此，输入和输出都是数值向量。
- en: Many of the algorithms used for classification are also used for simple and
    multiple regression tasks. This is because they support both discrete and continuous
    numerical predictions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 许多用于分类的算法也用于简单和多重回归任务。这是因为它们支持离散和连续数值预测。
- en: 'There is no dedicated API available for multivariate regression at the time
    of writing, so you’ll need to architect your system to support this use case.
    This process is similar to what we did for multilabel classification: prepare
    the data, train each variant independently, test and tune multiple models, and
    finally assemble the predictions.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，尚无专用的 API 可用于多元回归，因此您需要设计系统以支持此用例。这个过程类似于我们为多标签分类所做的：准备数据，独立训练每个变体，测试和调整多个模型，最后组装预测结果。
- en: To learn about regression, we are going to try to predict a vehicle’s CO[2]
    emissions using the CO[2] Emission by Vehicles dataset on [Kaggle](https://oreil.ly/GND1E).
    To do this, we are going to look at features such as company, car model, engine
    size, fuel type, consumption, and more!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解回归，我们将尝试使用[Kaggle](https://oreil.ly/GND1E)上的CO[2]排放数据集来预测车辆的CO[2]排放量。为此，我们将查看诸如公司、车型、发动机尺寸、燃料类型、消耗量等特征！
- en: As you will see, working with data to solve a problem like this requires featurization,
    cleaning, and formatting it to fit into the algorithm.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将看到的，处理数据来解决这样的问题需要对其进行特征化、清理和格式化，以适应算法。
- en: 'There are 13 columns in the dataset. To speed up the process of indexing and
    hashing, we will use `FeatureHasher` on only the continuous features. This selector
    asks us to specify the nature of the numeric features, discrete or continuous:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中有13列。为了加快索引和散列的速度，我们只会对连续特征使用`FeatureHasher`。该选择器要求我们指定数值特征的性质，是离散的还是连续的：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notice how nicely `inputCols` takes a list—this makes reusing our code and developing
    cleaner code easier!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`inputCols`如何很好地接受一个列表——这使得重用我们的代码和开发更干净的代码变得更容易！
- en: '`hashed_features` is of type `SparseVector`. Take a look at [Example 6-4](#the_hashed_features_sparse_vector).
    Due to the complexity of the hashing function, we ended up with a vector of size
    262,144.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`hashed_features`的类型是`SparseVector`。查看[示例 6-4](#the_hashed_features_sparse_vector)。由于散列函数的复杂性，我们最终得到了大小为262,144的向量。'
- en: Example 6-4\. The `hashed_features` sparse vector
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-4\. `hashed_features`稀疏向量
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'There’s lots of room for improvement here, since most of the vectors are sparse
    and might not be meaningful for us. So it’s time to select the features automatically:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很大的改进空间，因为大多数向量都是稀疏的，对我们可能没有意义。所以现在是自动选择特征的时候了：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The selector reduces the number of features from 262,144 to 50.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选择器将特征数量从262,144个减少到50个。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that we actually increased the dimensions with `FeatureHasher`. This is
    because we didn’t normalize the data first to make it simpler for us to backtrack
    the experiment. For real use cases, it is best to normalize the data before hashing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们实际上使用`FeatureHasher`增加了维度。这是因为我们没有首先对数据进行归一化，以使我们更容易回溯实验。对于实际用例，最好在散列之前对数据进行归一化。
- en: The next step is to build the machine learning model. MLlib provides multiple
    algorithms that we can choose from, such as `AFTSurvival​Re⁠gression`, `DecisionTreeRegressor`,
    and `GBT​Re⁠gressor` (for a full list, see the [documentation](https://oreil.ly/IYkDB)).
    AFT stands for *accelerated failure time*; this algorithm can be used to discover
    how long a machine in a factory will last. The `DecisionTreeRegressor` operates
    at its best on categorical features, which have a finite number of categories.
    As a result, it won’t be able to predict unseen values, like other regressors.
    The `GBTRegres⁠sor` is a gradient boosted trees regressor that uses an ensemble
    of decision trees trained in a serial fashion. It splits the training data into
    a training dataset and a validation dataset and uses the validation set to reduce
    the error on each iteration of the algorithm over the training data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建机器学习模型。MLlib提供了多种算法供我们选择，例如`AFTSurvival​Re⁠gression`、`DecisionTreeRegressor`和`GBT​Re⁠gressor`（完整列表请参见[文档](https://oreil.ly/IYkDB)）。AFT代表*加速失效时间*；这种算法可用于发现工厂中机器的使用寿命。`DecisionTreeRegressor`在分类特征上表现最佳，这些特征具有有限的类别数。因此，它无法预测看不见的值，就像其他回归器一样。`GBTRegres⁠sor`是一个梯度增强树回归器，它使用串行方式训练的一组决策树集成。它将训练数据拆分为训练数据集和验证数据集，并在算法的每次迭代中使用验证集来减少训练数据的误差。
- en: 'If you’re wondering how it differs from the `RandomForestClassifier` we saw
    earlier, the main difference is that a GBT algorithm builds one tree at a time
    that helps correct the errors made by the previous one, while a random forest
    algorithm builds the trees randomly in parallel: each subset of worker nodes forms
    its own tree, and these are then collected on the main node, which assembles the
    workers’ output into the final model. Both `GBTRegressor` and `RandomForestClassifier`
    support continuous and categorical features.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道它与我们之前看到的`RandomForestClassifier`有何不同，主要区别在于GBT算法一次构建一个树，帮助修正前一个树的错误，而随机森林算法并行地随机构建树：每个工作节点的子集形成自己的树，然后这些树被收集到主节点上，主节点将工作节点的输出汇总成最终模型。`GBTRegressor`和`RandomForestClassifier`都支持连续和分类特征。
- en: 'In the next example, we’ll try out MLlib’s `GBTRegressor` to see if it performs
    any better. While training might take longer due to its sequential nature, the
    optimization function should help it to produce more accurate results:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们将尝试使用MLlib的`GBTRegressor`来查看它是否表现更好。虽然由于其顺序性质而可能需要更长时间来训练，但优化函数应该有助于产生更精确的结果：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have a model, we can ingest the data we will use to train it. We’ll
    also need to validate that there is no overfitting. If `test01`’s predictions
    are 100% accurate, it is highly probable that it is overfitting—this can happen
    with lower accuracy as well, but any time you see accuracy at or close to 100%,
    you should be suspicious. We’ll talk more about evaluating models in [“Machine
    Learning Pipelines”](#ml_pipelines). For now, let’s take a look at a sample from
    the `prediction` column, shown in [Example 6-5](#predicted_versus_actual_cotwo_emissions).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个模型，可以输入用于训练的数据。我们还需要验证没有过拟合的情况。如果`test01`的预测是100%准确的，那么很可能是过拟合——这种情况也可能发生在低准确率下，但是每当您看到准确率接近或达到100%时，都应该持怀疑态度。我们将在[“机器学习流水线”](#ml_pipelines)中更详细地讨论评估模型的问题。现在，让我们看一下在`prediction`列中显示的示例，如[示例 6-5](#predicted_versus_actual_cotwo_emissions)所示。
- en: Example 6-5\. Predicted versus actual CO[2] emissions of vehicles
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-5\. 车辆二氧化碳排放的预测与实际情况
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, the `prediction` column is outputting data points that are very
    similar to the actual data in the `co2` column. For example, in the first line,
    the prediction was 32.879... and the actual `co2` value is 33.0\. The error here
    is manageable, and this holds true for the rest of the rows. This fact acts as
    a leading indicator that the algorithm training is heading in the right direction,
    as the predicted results are not identical to the actual values (which means there’s
    a low probability of overfitting), yet they are pretty close. As mentioned previously,
    we still need to run statistical evaluation tests to measure the overall effectiveness
    of the model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`prediction`列输出的数据点与`co2`列中的实际数据非常相似。例如，在第一行中，预测值为32.879...，而实际的`co2`值为33.0。这里的误差是可以管理的，对于其余的行也是如此。这个事实充当了算法训练正确方向的一个主要指标，因为预测结果与实际值并非完全相同（这意味着过拟合的可能性较低），但它们非常接近。如前所述，我们仍然需要运行统计评估测试来衡量模型的整体有效性。
- en: MLlib supports other machine learning algorithms that can solve this problem
    too, such as `FMRegression` (FM stands for *factorized machines*). This algorithm
    is based on a gradient descent algorithm with a dedicated loss function, also
    called an *optimization function*. Gradient descent is an iterative optimization
    algorithm. It iterates over the data, searching for rules or definitions that
    result in the minimum loss of accuracy. In theory, its performance improves with
    every iteration until it reaches the optimum value for the loss function.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib还支持其他可以解决这个问题的机器学习算法，例如`FMRegression`（FM代表*因子分解机*）。这种算法基于梯度下降算法，具有专用的损失函数，也称为*优化函数*。梯度下降是一种迭代优化算法，它遍历数据，搜索能够使精度损失最小化的规则或定义。理论上，其性能随着每次迭代的进行而改善，直到达到损失函数的最佳值。
- en: 'The `FMRegression` algorithm’s maximum number of iterations is set to 100 by
    default, but we can tweak this using the `setMaxIter` function. The optimization
    function used here is `SquaredError`. `SquaredError` implements the `MSE` function,
    which calculates the overall mean squared error in every iteration. This is what
    the algorithm seeks to reduce: the sum of squares of “distances” between the actual
    value and the predicted value in a given iteration. MSE is considered an unbiased
    estimator of error variance under standard assumptions of linear models.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`FMRegression`算法的最大迭代次数默认设置为100，但我们可以使用`setMaxIter`函数进行调整。这里使用的优化函数是`SquaredError`。`SquaredError`实现了`MSE`函数，用于计算每次迭代中的平均平方误差。这是算法试图减少的内容：在给定迭代中实际值与预测值之间的“距离”的平方和。在标准线性模型的假设下，MSE被认为是误差方差的无偏估计量。'
- en: If FM sounds familiar, this is because there is also an `FMClassifier`. The
    main difference between them is the loss function. The classifier uses `LogisticLoss`,
    sometimes referred to as *entropy loss* or *log loss*. The `LogisticLoss` function
    is used in `Logistic​Regression` as well. We will not dive into the theoretical
    math behind it as there are many introductory machine learning books that cover
    this (such as Hala Nelson’s [*Essential Math for AI*](https://oreil.ly/ess-math-ai),
    also from O’Reilly). However, it is important that you grasp the similarities
    and differences between classification and regression algorithms.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果FM听起来很熟悉，那是因为也有一个`FMClassifier`。它们之间的主要区别在于损失函数。分类器使用`LogisticLoss`，有时称为*熵损失*或*对数损失*。`LogisticLoss`函数也用于`Logistic​Regression`中。我们不会深入探讨它背后的理论数学，因为有很多介绍性的机器学习书籍涵盖了这一点（例如哈拉·尼尔森的[*Essential
    Math for AI*](https://oreil.ly/ess-math-ai)，同样出自O’Reilly）。但重要的是，您要掌握分类和回归算法之间的相似性和差异。
- en: Recommendation systems
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推荐系统
- en: Recommendation systems are often taught using a movie dataset, such as [MovieLens](https://movielens.org),
    where the objective is recommending movies to users based on what other users
    liked and/or user preferences like genres. You can find recommendation systems
    implemented in many online platforms—for instance, ecommerce systems such as Amazon
    or streaming platforms like Netflix. They are based on *association rule learning*,
    where the algorithm aims to learn the associations between movies and users.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统通常使用电影数据集来教授，例如[MovieLens](https://movielens.org)，其目标是根据其他用户喜欢的内容和/或用户偏好（如流派）向用户推荐电影。您可以在许多在线平台上找到实施推荐系统的例子，例如亚马逊的电子商务系统或Netflix等流媒体平台。它们基于*关联规则学习*，算法旨在学习电影和用户之间的关联。
- en: 'At a high level, we can split them into three categories, depending on the
    data available (metadata about the users and content, and data about the interactions
    between the users and the content):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，我们可以根据可用的数据（关于用户和内容的元数据以及用户与内容之间的互动数据）将它们分为三类：
- en: Content-based
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内容
- en: Algorithms use the available metadata about the content and the users, including
    what content the user has watched before and rated, favorite genres, movie genres,
    etc., and produce a recommendation based on this information. This can be implemented
    with rule-based functionality and doesn’t necessarily require machine learning.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 算法利用关于内容和用户的可用元数据，包括用户以前观看和评分的内容，喜爱的流派，电影流派等，并根据这些信息生成推荐。这可以通过基于规则的功能来实现，并不一定需要机器学习。
- en: Collaborative filtering
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤
- en: In this instance, no metadata is available about the movies and users; we only
    have the *interaction matrix* defining the interactions between the users and
    the content (i.e., which movies each user has watched or rated). The algorithm
    searches for similarities between the user interactions to provide a recommendation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，电影和用户没有可用的元数据；我们只有定义用户与内容之间互动的*互动矩阵*（即每个用户观看或评分的电影）。该算法搜索用户互动之间的相似性来提供推荐。
- en: Neural networks
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络
- en: Given metadata on the users and content together with the interaction matrix,
    you can leverage neural networks to make recommendations.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到用户和内容的元数据以及互动矩阵，您可以利用神经网络进行推荐。
- en: ALS for collaborative filtering
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 协同过滤的ALS
- en: MLlib provides a well-documented solution for collaborative filtering called
    `ALS` (alternating least squares). Its goal is to fill in missing values in a
    user–item interaction matrix. It also provides a solution to cold-start scenarios
    where the user is new to the system and there is no previous data to draw upon
    to make accurate recommendations. You can read more about it in the [MLlib docs](https://oreil.ly/66Vyt).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib为协同过滤提供了一个文档完善的解决方案，称为`ALS`（交替最小二乘）。其目标是填补用户-项目互动矩阵中的缺失值。它还提供了解决冷启动场景的解决方案，即用户对系统是新的，没有之前的数据可用来进行准确的推荐。您可以在[MLlib文档](https://oreil.ly/66Vyt)中了解更多信息。
- en: Unsupervised Machine Learning
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督机器学习
- en: Unsupervised algorithms are used when the data doesn’t have labels but we still
    want to automatically find interesting patterns, predict behaviors, or calculate
    resemblance without knowing the desired outcome. Those algorithms can be used
    interchangeably with supervised algorithms as part of the feature extraction procedure.
    Common unsupervised machine learning tasks include frequent pattern mining and
    clustering. Let’s take a look at how MLlib supports these tasks.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督算法用于当数据没有标签但我们仍然希望自动发现有趣的模式、预测行为或计算相似性时。这些算法可以与监督算法交替使用作为特征提取过程的一部分。常见的无监督机器学习任务包括频繁模式挖掘和聚类。让我们看看MLlib如何支持这些任务。
- en: Frequent Pattern Mining
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 频繁模式挖掘
- en: Frequent pattern mining falls into the category of *association rule learning*,
    based on identifying rules to uncover relationships between variables in the data.
    Association rule mining algorithms typically first look for frequent items in
    the dataset, then for frequent pairs or itemsets (e.g., items that are often viewed
    or purchased together). The rules follow the basic structure of *antecedent* (if)
    and *consequent* (then).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 频繁模式挖掘属于*关联规则学习*的范畴，其基于识别规则来揭示数据中变量之间的关系。关联规则挖掘算法通常首先在数据集中查找频繁项，然后查找频繁对或项集（例如，经常一起查看或购买的项目）。规则遵循*前件*（if）和*后件*（then）的基本结构。
- en: 'MLlib provides two frequent pattern mining functions that can be used as preprocessing
    procedures for recommendation engines, like extracting meaningful patterns out
    of a corpus of text to detect user sentiment toward a movie: `FPGrowth` and `PrefixSpan`.
    I’ll focus on the clustering algorithms here, because those can be used on their
    own, whereas you’ll often need to stack more than one of these algorithms together
    to reach the end result. You are invited to read more about the frequent pattern
    mining algorithms in the [MLlib docs](https://oreil.ly/VQQCQ).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib提供两个频繁模式挖掘函数，可以用作推荐引擎的预处理过程，例如从文本语料库中提取有意义的模式以检测用户对电影的情感：`FPGrowth` 和 `PrefixSpan`。我将重点介绍这里的聚类算法，因为这些可以单独使用，而通常您需要将多个频繁模式挖掘算法堆叠在一起才能达到最终结果。您可以在[MLlib文档](https://oreil.ly/VQQCQ)中进一步了解频繁模式挖掘算法。
- en: Clustering
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: '*Clustering* is a grouping technique for discovering hidden relationships between
    data points. Clustering is regularly used for customer segmentation, image processing
    and detection, spam filtering, anomaly detection, and more.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*聚类*是一种发现数据点之间隐藏关系的分组技术。聚类经常用于客户分割、图像处理和检测、垃圾邮件过滤、异常检测等领域。'
- en: In the clustering process, each item is assigned to a group, defined by its
    center. The likelihood of an item belonging to a group is calculated by its distance
    from the center. The algorithms usually try to optimize the model by changing
    the center points of the groups.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类过程中，每个项目被分配到一个由其中心定义的组中。项目属于某个组的可能性通过其与中心的距离计算得出。算法通常尝试通过改变组的中心点来优化模型。
- en: The names of clustering algorithms often include the letter *k*, as in *k*-nearest
    neighbors (*k*-NN) and *k*-means. Its meaning depends on the algorithm itself.
    It often represents the number of predefined clusters/topics. MLlib algorithms
    have a default integer *k* value, and you can set it using the method `setK` or
    pass it as a parameter. Some algorithms require the data to have a `weightCol`—specifically,
    MLlib’s `KMeans`, `GaussianMixture`, `PowerIterationClustering`, and `BisectingMeans`
    expect a nonnegative `weightCol` in the training dataset that represents the data
    points’ weights relative to the center of the cluster. If a specific data point
    has a high weight and is relatively far from the cluster center, the “cost” it
    imposes on the optimization function (in other words, the loss for that point)
    will be high. The algorithm will try to minimize the loss across all data points
    by moving the cluster center closer to such data points, if possible, to reduce
    the overall loss.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法的名称通常包含字母*k*，例如*k*-最近邻 (*k*-NN) 和 *k*-均值 (*k*-means)。其含义取决于算法本身。通常代表预定义的簇/主题数目。MLlib算法有一个默认整数*k*值，并且您可以使用`setK`方法或作为参数传递它。一些算法要求数据具有一个`weightCol`——特别是MLlib的`KMeans`、`GaussianMixture`、`PowerIterationClustering`
    和 `BisectingMeans` 预期在训练数据集中有一个非负`weightCol`，表示数据点相对于簇中心的权重。如果特定数据点的权重很高且相对于簇中心距离较远，则它对优化函数（换句话说，该点的损失）施加的“成本”将很高。算法将尝试通过将簇中心移动到这样的数据点附近（如果可能的话）来减少整体损失。
- en: Almost all clustering algorithms require seed values (the exception is `Power​Itera⁠tion​Clustering`).
    The seed value is used to initialize a set of cluster center points at random
    (think *x* and *y* coordinates), and with every iteration of the algorithm, the
    centers are updated based on the optimization function.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的聚类算法都需要种子值（唯一的例外是 `Power​Itera⁠tion​Clustering`）。种子值用于随机初始化一组聚类中心点（类似于
    *x* 和 *y* 坐标），并且随着算法的每次迭代，中心点根据优化函数进行更新。
- en: 'Now that you know what clustering is, we can return to our CO[2] emissions
    prediction objective and see if we can identify commonalities between columns
    such as fuel type, consumption, cylinders, etc. MLlib provides five clustering
    algorithms. Let’s take a look at these to see which ones might be appropriate
    for this task:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您了解了聚类是什么，我们可以回到我们的 CO[2] 排放预测目标，看看是否可以识别诸如燃料类型、消耗、汽缸等列之间的共性。MLlib 提供五种聚类算法。让我们来看看这些算法，看看哪些可能适合这个任务：
- en: '`LDA`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`LDA`'
- en: '`LDA` (Latent Dirichlet Allocation) is a generic statistical algorithm used
    in evolutionary biology, biomedicine, and natural language processing. It expects
    a vector representing the counts of individual words in a document; since in our
    scenario we’re focusing on variables like fuel type, LDA does not match our data.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`LDA`（Latent Dirichlet Allocation）是一种通用的统计算法，用于进化生物学、生物医学和自然语言处理。它期望一个向量，表示文档中每个单词的计数；由于我们的场景专注于诸如燃料类型之类的变量，LDA
    不适合我们的数据。'
- en: '`GaussianMixture`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`GaussianMixture`'
- en: The `GaussianMixture` algorithm is often used to identify the presence of a
    group within a bigger group. In our context, it could be useful for identifying
    the subgroups of different classes inside each car manufacturer’s group, such
    as the compact car class in the Audi group and the Bentley group. However, `Gaussian​Mixture`
    is known to perform poorly on high-dimensional data, making it hard for the algorithm
    to converge to a satisfying conclusion. Data is said to be high-dimensional when
    the number of features/columns is close to or larger than the number of observations/rows.
    For example, if I have five columns and four rows, my data is considered high-dimensional.
    In the world of large datasets, this is less likely to be the case.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`GaussianMixture` 算法通常用于识别更大组内的子组的存在。在我们的上下文中，它可以用于识别每个汽车制造商组内不同类别的子组，例如奥迪组中的紧凑型车类别和宾利组。然而，`GaussianMixture`
    在高维数据上表现不佳，这使得算法难以收敛到令人满意的结论。当特征/列的数量接近或大于观察/行的数量时，数据被认为是高维的。例如，如果我有五列和四行，我的数据就被认为是高维的。在大数据集的世界中，这种情况不太可能发生。'
- en: '`KMeans`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`KMeans`'
- en: '`KMeans` is the most popular algorithm for clustering, due to its simplicity
    and efficiency. It takes a group of classes, creates random centers, and starts
    iterating over the data points and the centers, aiming to group similar data points
    together and find the optimal centers. The algorithm always converges, but the
    quality of the results depends on the number of clusters (*k*) and the number
    of iterations.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`KMeans` 是最流行的聚类算法，因其简单和高效而广受欢迎。它将一组类别作为输入，创建随机中心点，并开始迭代数据点和中心点，旨在将相似的数据点分组在一起，并找到最优的中心点。该算法始终收敛，但结果的质量取决于聚类数（*k*）和迭代次数。'
- en: '`BisectingKMeans`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`BisectingKMeans`'
- en: '`BisectingKMeans` is based on the `KMeans` algorithm with the hierarchy of
    groups. It supports calculating distance in two ways: `euclidean` or `cosine`.
    The model can be visualized as a tree, with leaf clusters; when training begins,
    there is only a root node, and the nodes are split in two on each iteration to
    optimize the model. This is a great option if you want to represent groups and
    subgroups.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`BisectingKMeans` 基于 `KMeans` 算法，具有分层组的层次结构。它支持两种方式计算距离：`euclidean` 或 `cosine`。模型可以被视为一棵树，具有叶子聚类；当训练开始时，只有一个根节点，并且每次迭代时节点分为两个以优化模型。如果您想要表示组和子组，这是一个很好的选择。'
- en: '`PowerIterationClustering`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`PowerIterationClustering`'
- en: '`PowerIterationClustering` (PIC) implements the [Lin and Cohen algorithm](https://oreil.ly/-Gu9c).
    It’s a scalable and efficient option for clustering vertices of a graph given
    pairwise similarities as edge properties. Note that this algorithm cannot be used
    in Spark pipelines as it does not yet implement the `Estimator`/`Transformer`
    pattern (more on that in [“Machine Learning Pipelines”](#ml_pipelines)).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`PowerIterationClustering`（PIC）实现了[Lin and Cohen算法](https://oreil.ly/-Gu9c)。它是一个可扩展和高效的选项，用于根据边属性的成对相似性来聚类图的顶点。请注意，此算法不能在Spark管道中使用，因为它尚未实现`Estimator`/`Transformer`模式（关于此的更多信息请见[“机器学习管道”](#ml_pipelines)）。'
- en: Cool, cool, cool! Now that we understand our options, let’s try one out. We’ll
    go with `GaussianMixture`, since our dataset has only 11 columns and much more
    data than that. We are going to use the CO[2] Emission by Vehicles dataset after
    preprocessing and feature engineering with all the columns, including the `label`
    column (for the end-to-end tutorial, check out the file *ch06_gm_pipeline.ipynb*
    in the book’s [GitHub repository](https://oreil.ly/Dl9nO)).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 酷，酷，酷！现在我们了解了我们的选项，让我们尝试其中一个。我们将选择`GaussianMixture`，因为我们的数据集仅有11列，比这多得多的数据。我们将使用经过预处理和特征工程处理后的CO[2]排放汽车数据集，其中包括`label`列（要了解端到端教程，请查看书籍的[GitHub仓库](https://oreil.ly/Dl9nO)中的*ch06_gm_pipeline.ipynb*文件）。
- en: 'The value of `k` in this case will be the number of car manufacturers in our
    dataset. To extract it, we use `distinct().count()`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`k`的值将是我们数据集中的汽车制造商数量。要提取它，我们使用`distinct().count()`：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The result is `42`. What an interesting number. :) We’ll pass this in to the
    constructor, along with setting a few other parameters:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是`42`。多么有趣的数字啊。 :) 我们将其传递给构造函数，同时设置了几个其他参数：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we have the model, we can get the summary object that represents the
    model:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了模型，我们可以获取表示模型的摘要对象：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: All clustering and classification algorithms have a summary object. In clustering,
    it contains the predicted cluster centers, the transformed predictions, the cluster
    size (i.e., the number of objects in each cluster), and dedicated parameters based
    on the specific algorithm.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的聚类和分类算法都有一个摘要对象。在聚类中，它包含了预测的集群中心、转换后的预测、集群大小（即每个集群中的对象数）以及基于特定算法的专用参数。
- en: 'For example, we can understand how many groups the algorithm has converged
    to at the end by running `distinct().count()` on the summary:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通过在摘要上运行`distinct().count()`，我们可以了解算法在最后收敛到了多少个群组：
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In our case, we get `17`. Now, we can try reducing the value of `k` to see if
    we get better convergence or try increasing the number of iterations to see if
    this has an effect on that number. Of course, the more iterations the algorithm
    does, the more processing time it takes, so when running on a large set of data,
    you should be cautious with this amount. Determining how many iterations you need
    is a matter of trial and error. Be sure to add this to your experiment testing,
    together with trying different measures of performance.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们得到了`17`。现在，我们可以尝试减少`k`的值，看看是否能获得更好的收敛性，或者尝试增加迭代次数，看看这是否对那个数字有影响。当然，算法执行的迭代次数越多，处理时间就越长，因此在大数据集上运行时，您应该对此数量保持谨慎。确定需要多少次迭代是一个试错过程。务必将其添加到您的实验测试中，同时尝试不同的性能指标。
- en: 'Another way to measure the model’s performance is by looking at the `logLikelihood`,
    which represents the statistical significance of the difference between the groups
    the model found:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种衡量模型性能的方法是查看`logLikelihood`，它代表模型发现的群组之间差异的统计显著性：
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With 200 iterations, we received about 508,076 likelihood scores. These scores
    are not normalized, and it’s hard to compare them directly; however, a higher
    score indicates a greater likelihood of an instance being related to its cluster.
    All we know is that a higher score means a greater likelihood of instances being
    related to their cluster. Therefore, this is a good way of comparing the performance
    of one model against another on the same data but not necessarily of evaluating
    the performance of a model on its own. This is one reason why it’s important to
    define the objectives of your experiments up front. If you would like to learn
    more about statistics and likelihood measurement, I recommend reading [*Practical
    Statistics for Data Scientists*](https://oreil.ly/prac-stats), by Peter Bruce,
    Andrew Bruce, and Peter Gedeck (O’Reilly).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 200 次迭代，我们得到约 508,076 个可能性评分。这些评分没有被标准化，因此直接比较它们是困难的；然而，较高的评分表示实例与其簇之间的相关性可能性更大。我们只知道较高的评分意味着实例更可能与它们的簇相关联。因此，这是比较同一数据上一个模型与另一个模型性能的好方法，但不一定能评估模型本身的性能。这也是为什么提前定义实验目标如此重要的一个原因。如果你想了解更多关于统计和可能性测量的知识，我建议阅读《[*数据科学实用统计*](https://oreil.ly/prac-stats)》，作者是彼得·布鲁斯、安德鲁·布鲁斯和彼得·格德克（O’Reilly）。
- en: 'Suppose we continue our explorations using maxIter = 200 and receive the same
    number of distinct predictions: 17\. Based on this, we decide to change *k* to
    17.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们继续使用 maxIter = 200 进行探索，并获得相同数量的不同预测：17\. 基于此，我们决定将 *k* 改为 17。
- en: 'Our next step might be to examine the cluster sizes, to make sure there are
    no groups with zero data points in them:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步可能是检查簇的大小，以确保没有包含零数据点的组：
- en: '[PRE17]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This produces the following output, where the indices represent the group index:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出，其中索引表示组索引：
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since `clusterSizes` is of type `Array`, you can use tools such as `numpy` and
    `matplotlib` to create a histogram of the values. [Figure 6-2](#histogram_of_clusters_or_group_sizes)
    shows the resulting distribution of group/cluster sizes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `clusterSizes` 是 `Array` 类型，您可以使用 `numpy` 和 `matplotlib` 等工具创建值的直方图。[图 6-2](#histogram_of_clusters_or_group_sizes)
    显示了组/簇大小的分布结果。
- en: '![](assets/smls_0602.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0602.png)'
- en: Figure 6-2\. Histogram of clusters or group sizes
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 组或簇大小的直方图
- en: Evaluating
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: 'The evaluation phase is an essential part of the machine learning process—this
    is how we estimate the model’s performance. MLlib has six evaluators, all of which
    implement the Spark abstract class `Evaluator`. They can be roughly split into
    two groups: supervised and unsupervised.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 评估阶段是机器学习过程中的一个重要部分——这是我们估计模型性能的方式。MLlib 有六个评估器，它们都实现了 Spark 抽象类 `Evaluator`。它们大致可以分为两组：监督和无监督。
- en: Note
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '`Evaluator` is a class that allows us to see how a given model performs according
    to specific machine learning evaluation criteria.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`Evaluator` 是一个类，允许我们根据特定的机器学习评估标准来查看给定模型的表现。'
- en: Supervised Evaluators
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督评估器
- en: In supervised learning, we have the labels for the test data, so we can produce
    multiple metrics for estimating performance. To allow us to do this, the estimator
    first calculates the *confusion matrix*, which compares the predicted labels and
    the actual labels. For a binary classification task, conceptually the result would
    look like [Figure 6-3](#binary_confusion_matrix), with each box having a range
    of `[0,``*dataset_size*``]`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们有测试数据的标签，因此我们可以生成多个指标来估计性能。为了能够做到这一点，评估器首先计算*混淆矩阵*，它比较了预测标签和实际标签。对于二元分类任务，从概念上来说，结果看起来会像
    [图 6-3](#binary_confusion_matrix)，每个方框的范围为 `[0,``*数据集大小*``]`。
- en: '![](assets/smls_0603.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0603.png)'
- en: Figure 6-3\. Binary confusion matrix
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 二元混淆矩阵
- en: 'True and False stand for the accuracy of the prediction, and Positive and Negative
    stand for the binary predictions (these can also be 1 and 0). There are four categories:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 真实和虚假代表预测的准确性，正和负代表二元预测（也可以是 1 和 0）。有四个类别：
- en: True positive (TP)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 真阳性（TP）
- en: Positive labels where the prediction is also positive
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为正的正标签
- en: True negative (TN)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 真阴性（TN）
- en: Negative labels where the prediction is also negative
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为负的负标签
- en: False positive (FP)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 假阳性（FP）
- en: Negative labels where the prediction is positive
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为正的负标签
- en: False negative (FN)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 假阴性（FN）
- en: Positive labels where the prediction is negative
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为负的正标签
- en: Based on those values, Spark’s estimators can provide many metrics; you can
    find full details in the [documentation](https://oreil.ly/knYwj).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些数值，Spark 的估算器可以提供许多指标；您可以在[文档](https://oreil.ly/knYwj)中找到详细信息。
- en: When we want to use the same process for *multiclass* and *multilabel* classification,
    the confusion matrix will grow accordingly to capture all of the labeling possibilities.
    In the case of imbalanced data, you would probably need to write your own evaluator.
    You can do that using Spark’s extensive API.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望对*多类*和*多标签*分类使用相同的流程时，混淆矩阵将相应增长，以捕获所有的标签可能性。在数据不平衡的情况下，您可能需要编写自己的评估器。您可以使用Spark的广泛API来完成这一点。
- en: 'In addition to the base `Evaluator`, Spark provides the following APIs to calculate
    performance metrics:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基本的`Evaluator`，Spark还提供以下API来计算性能指标：
- en: '`BinaryClassificationEvaluator`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`BinaryClassificationEvaluator`'
- en: A binary evaluator that expects the input columns `rawPrediction`, `label`,
    and `weight` (optional). It can be used to output the area under the receiver
    operating characteristic (ROC) and precision–recall (PR) curves.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一个二元评估器，期望输入列为`rawPrediction`、`label`和`weight`（可选）。它可以用于输出接收器操作特征曲线（ROC）和精确率-召回率（PR）曲线下面积。
- en: '`MulticlassClassificationEvaluator`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`MulticlassClassificationEvaluator`'
- en: An evaluator for multiclass classification that expects the input columns `prediction`,
    `label`, `weight` (optional), and `probabilityCol` (only for `logLoss`). It has
    dedicated metrics such as `precisionByLabel`, `recallByLabel`, etc., and a special
    matrix `hammingLoss` that calculates the fraction of labels that are incorrectly
    predicted. When comparing binary and multiclass classification models, remember
    that precision, recall, and F1 score are designed for the binary classification
    models; therefore, it is better to compare the `hammingLoss` to `accuracy`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于多类分类的评估器，期望输入列为`prediction`、`label`、`weight`（可选）、以及`probabilityCol`（仅适用于`logLoss`）。它具有专用指标，如`precisionByLabel`、`recallByLabel`等，以及一个特殊的矩阵`hammingLoss`，用于计算错误预测的标签比例。当比较二元和多类分类模型时，请记住精确度、召回率和F1分数是设计用于二元分类模型的；因此，最好将`hammingLoss`与`accuracy`进行比较。
- en: '`MultilabelClassificationEvaluator`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultilabelClassificationEvaluator`'
- en: An evaluator for multilabel classification (added in Spark 3.0). This evaluator
    is still in the experimental phase at the time of writing. It expects two input
    columns, `prediction` and `label`, and provides dedicated metrics such as `micro​Precision`,
    `microRecall`, etc. that are averages across all the prediction classes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 用于多标签分类的评估器（在Spark 3.0中添加）。在撰写本文时，此评估器仍处于实验阶段。它期望两个输入列，即`prediction`和`label`，并提供专用指标，如`microPrecision`、`microRecall`等，这些指标是跨所有预测类别的平均值。
- en: Note
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A feature in the experimental phase is usually still under development and not
    yet ready to be implemented fully. In most cases, these features are limited to
    just a few modules and are incorporated mainly to allow the developers to gain
    knowledge and insights for the future development of the software. Open source
    technologies frequently introduce experimental features that can be leveraged
    in production code, but these features might change in future versions of the
    software, and the contributors are not committing to continue supporting them
    or to turn them into well-polished features.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 实验阶段中的功能通常仍在开发中，尚未完全准备好实施。在大多数情况下，这些功能仅限于少数模块，并主要用于允许开发人员获取未来软件开发的知识和见解。开源技术经常引入实验性功能，可以在生产代码中加以利用，但这些功能可能会在软件的未来版本中进行更改，并且贡献者并不承诺继续支持它们或将它们转变为成熟的功能。
- en: '`RegressionEvaluator`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`RegressionEvaluator`'
- en: This evaluator expects the input columns `prediction`, `label`, and `weight`
    (optional) and produces metrics such as `mse` (the mean squared error of the distances
    between the predictions and the actual labels) and `rmse` (the root of the previous
    value).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 此评估器期望输入列为`prediction`、`label`和`weight`（可选），并生成指标，如`mse`（预测与实际标签之间距离的均方误差）和`rmse`（前述值的平方根）。
- en: '`RankingEvaluator`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`RankingEvaluator`'
- en: 'This evaluator (added in Spark 3.0 and also still in the experimental phase
    at the time of writing) expects the input columns `prediction` and `label`. It
    is often used for evaluating the ranking of search results. It has a variable
    `k` that you can set to get the matrix averages across the *k* first results.
    Think of a movie recommendation system that can output either 5 or 10 recommendations:
    the averages will change depending on the number of recommendations returned,
    and evaluating the results can help you make an informed decision about which
    movie to select. This evaluator’s output is based on MLlib’s RDD-based `RankingMetrics`
    API; you can read more about it in the [docs](https://oreil.ly/vobZH).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此评估器（在写作时添加到Spark 3.0中，并且仍处于实验阶段）期望输入列`prediction`和`label`。它通常用于评估搜索结果的排名。它有一个变量`k`，你可以设置以获取*前k个*结果的矩阵平均值。想象一个电影推荐系统，可以输出5或10个推荐：平均值将根据返回的推荐数量而改变，评估结果可以帮助你做出选择。此评估器的输出基于MLlib的基于RDD的`RankingMetrics`API；你可以在[文档](https://oreil.ly/vobZH)中详细了解它。
- en: Unsupervised Evaluators
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督评估器
- en: 'MLlib also provides an evaluator for unsupervised learning, and we can access
    even more options by bridging to TensorFlow, PyTorch, or other libraries that
    can process Spark DataFrames. MLlib’s evaluator for clustering results is the
    `ClusteringEvalua⁠tor`. It expects two input columns, `prediction` and `features`,
    and an optional `weight` column. It computes the *Silhouette measure*, where you
    can choose between two distance measures: `squaredEuclidean` and `cosine`. Silhouette
    is a method to evaluate the consistency and validity of the clusters; it does
    this by calculating the distance between each data point and the other data points
    in its cluster and comparing this to its distance from the points in other clusters.
    The output is the mean Silhouette values of all points based on point weight.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib还为无监督学习提供了一个评估器，我们可以通过与TensorFlow、PyTorch或其他能处理Spark DataFrames的库建立桥接来访问更多选项。MLlib的聚类结果评估器是`ClusteringEvalua⁠tor`。它期望两个输入列`prediction`和`features`，以及一个可选的`weight`列。它计算*轮廓系数*，你可以在两种距离度量`平方欧氏`和`余弦`之间进行选择。轮廓系数是一种评估聚类一致性和有效性的方法；它通过计算每个数据点与其所在簇中其他数据点的距离，并将其与其与其他簇中点的距离进行比较来实现这一点。输出是基于点权重的所有点的平均轮廓值。
- en: 'Reflecting back on our classification example with `GaussianMixture` and `KMeans`,
    we can evaluate the models to make better decisions. You can find this and the
    other code from this chapter in the book’s [GitHub repository](https://oreil.ly/smls-git6):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们使用`GaussianMixture`和`KMeans`进行分类的示例，我们可以评估这些模型以做出更好的决策。你可以在本书的[GitHub仓库](https://oreil.ly/smls-git6)中找到本章节的代码以及其他内容：
- en: '[PRE19]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The default distance computation method is the squared Euclidean distance.
    It gives the following output:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的距离计算方法是平方欧氏距离。它给出以下输出：
- en: '[PRE20]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'How can we tell which one is better? The evaluator has a dedicated function
    named `isLargerBetter` that can help us determine this:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何判断哪一个更好？评估器有一个专门的函数名为`isLargerBetter`，可以帮助我们确定这一点：
- en: '[PRE21]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In our case, it returns `True`, suggesting that `KMeans` performs better on
    our data. We’re not done yet, though—let’s take a look at the `cosine` distance:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，它返回`True`，表明`KMeans`在我们的数据上表现更好。不过，我们还没有结束，让我们看看`余弦`距离：
- en: '[PRE22]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Its output is:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 其输出是：
- en: '[PRE23]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`KMeans` still performs better, but in this case, the difference is much less
    pronounced. This is likely because of how the models themselves are implemented;
    for example, `KMeans` uses Euclidean distance as the default distance measure
    for clustering and so generally performs better when evaluating based on squared
    Euclidean distance. In other words, it’s important to interpret indicators like
    this carefully. To add more clarity, we can combine the evaluating process with
    adjustments to the test and training datasets, the algorithms, and the evaluators.
    Time for tuning!'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`KMeans`仍然表现更好，但在这种情况下，差异要小得多。这可能是因为模型本身的实现方式不同；例如，`KMeans`将欧氏距离作为聚类的默认距离度量，因此在基于平方欧氏距离评估时通常表现更好。换句话说，需要谨慎解释这样的指标。为了更加清晰，我们可以将评估过程与对测试和训练数据集、算法和评估器的调整结合起来。是时候进行调优了！'
- en: Hyperparameters and Tuning Experiments
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数和调优实验
- en: What if I told you that there are tools that allow us to run multiple experiments,
    produce numerous models, and extract the best one in an automated manner? This
    is precisely what we are going to cover in this section!
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我告诉您，有工具可以让我们运行多个实验，生成多个模型，并以自动化方式提取最佳模型，您会怎么想？这正是我们将在本节中介绍的内容！
- en: All machine learning processes require tuning and experimentation before they
    can accurately predict real-world events. This is done by splitting the dataset
    into multiple training and test sets and/or tweaking the algorithms’ parameters.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的机器学习过程在能够准确预测现实世界事件之前都需要调整和实验。这是通过将数据集分割为多个训练和测试集和/或调整算法参数来完成的。
- en: Building a Parameter Grid
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建参数网格
- en: 'In the following code, for example, we build a parameter (param) grid by using
    `Param​Grid​Builder().addGrid` to define multiple max iteration values for building
    *k*-means models using both available distance metrics:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，例如，我们使用`Param​Grid​Builder().addGrid`来定义多个最大迭代值，以使用两种可用的距离度量构建*k*-means模型的参数（param）网格：
- en: '[PRE24]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'A parameter grid is a grid or table of parameters with a discrete number of
    values for each one that can be used to iterate over different parameter value
    combinations as part of the training process, looking for the optimum values.
    `ParamGridBuilder` is just a tool that allows us to build it faster. You can use
    it with any MLlib function that takes in an array of params. Before continuing,
    let’s tweak the params of our evaluator as well by adding a dedicated grid for
    it:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 参数网格是一种包含每个参数离散数值的网格或表格，可用于在训练过程中迭代不同的参数值组合，以寻找最优值。`ParamGridBuilder`只是一个工具，允许我们更快地构建它。您可以将其与任何接受参数数组的MLlib函数一起使用。在继续之前，让我们也通过为其添加专用网格来调整我们评估器的参数：
- en: '[PRE25]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Splitting the Data into Training and Test Sets
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据分割为训练集和测试集
- en: 'Next, we’ll use `Train​Val⁠ida⁠tion​Split` to randomly split our data into
    a training set and a test set, which will be used to evaluate each parameter combination:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`Train​Val⁠ida⁠tion​Split`来随机将数据分为训练集和测试集，用于评估每个参数组合：
- en: '[PRE26]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: By default, `TrainValidationSplit` uses 75% of the data for training and 25%
    testing. You can change this by setting the param `trainRatio` on initialization.
    `Train​Val⁠ida⁠tion​Split` is an estimator, so it implements `fit` and outputs
    a transformer. `tvs_model` represents the best model that was identified after
    validating various parameter combinations. We can also tell `TrainValidationSplit`
    to collect all the submodels that performed less well, rather than keeping only
    the best one; we do this by setting the `collectSubModels` param to `True`, as
    shown in this example.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`TrainValidationSplit`使用数据的75%进行训练和25%进行测试。您可以通过初始化时设置参数`trainRatio`来更改这一比例。`Train​Val⁠ida⁠tion​Split`是一个估计器，因此它实现了`fit`并输出一个转换器。`tvs_model`表示在验证各种参数组合后识别出的最佳模型。我们还可以告诉`TrainValidationSplit`收集所有表现不佳的子模型，而不是仅保留最佳模型；我们可以通过将参数`collectSubModels`设置为`True`来实现，就像这个示例中展示的那样。
- en: Warning
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Use `collectSubModels` with caution. Consider this option when you want to stack
    multiple machine learning models as part of your workload or when you get similar
    results for the validation metrics and want to keep all the models to continue
    experimenting to identify the best one. For details on accessing the submodels
    and on why you should be careful when doing this, see [“How Can I Access Various
    Models?”](#how_can_i_access_various_modelsquestion).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`collectSubModels`时请谨慎。当您希望将多个机器学习模型堆叠作为工作负载的一部分，或者当您对验证指标获得类似结果并希望保留所有模型以继续实验以确定最佳模型时，考虑此选项。有关访问子模型的详细信息以及在执行此操作时需要小心的原因，请参见[“如何访问各种模型？”](#how_can_i_access_various_modelsquestion)。
- en: 'How can you tell that it’s picked the best model? Let’s check out the validation
    metrics:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如何确定它选择了最佳模型？让我们看看验证指标：
- en: '[PRE27]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This gives us a view of how the various experiments performed given our evaluator,
    as shown in [Example 6-6](#validation_metrics_output).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们能够查看在我们的评估器下各种实验的表现，如[示例 6-6](#validation_metrics_output)所示。
- en: Example 6-6\. Validation metrics output
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-6\. 验证指标输出
- en: '[PRE28]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Remember that every param in the process can be added to `ParamGridBuilder`.
    You can also set the specific columns for labels and features using this function.
    If you have a parameter dictionary or a list of `(``*parameter*``,` `*value*``)`
    pairs, instead of adding them one by one with `addGrid`, you can leverage the
    `baseOn` function, which behind the scenes runs a `foreach` loop for you.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，流程中的每个参数都可以添加到 `ParamGridBuilder` 中。您还可以使用该函数设置标签和特征的具体列。如果您有参数字典或`(``*parameter*``,`
    `*value*``)` 对的列表，可以使用 `baseOn` 函数，它在后台为您运行 `foreach` 循环，而不是使用 `addGrid` 逐个添加。
- en: 'Cross-Validation: A Better Way to Test Your Models'
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证：测试模型的更好方法
- en: One thing we can’t do with `TrainValidationSplit` is try out multiple combinations
    of data splits. For that, MLlib provides us with `CrossValidator`, an estimator
    that implements *k*-fold cross-validation. This is a technique that splits the
    dataset into a set of nonoverlapping randomly partitioned “folds” used separately
    as training and test sets.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `TrainValidationSplit` 无法尝试多个数据拆分的组合。为此，MLlib 提供了 `CrossValidator`，它是一个实现了*k*-折交叉验证的评估器。这是一种将数据集拆分成一组非重叠随机分区“折叠”，分别用作训练和测试集的技术。
- en: This operation is considered computation-heavy since when using it, we train
    *k* times the number of parameter maps models. In other words, if `numFolds` is
    `3`, and we have one parameter grid with 2 values, we will train 6 machine learning
    models. With `num⁠Folds=3` and the previous param grid for the evaluator and the
    algorithm, we will train `numFolds` times the grid size, which is 12,^([1](ch06.xhtml#ch01fn14))
    machine learning models—i.e., a total of 36.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用该操作时需要进行大量计算，因此我们会训练*k*倍于参数映射模型的数量。换句话说，如果 `numFolds` 是 `3`，并且我们有一个包含 2
    个值的参数网格，则会训练 6 个机器学习模型。当 `num⁠Folds=3` 并且用于评估器和算法的先前参数网格时，我们将训练 `numFolds` 倍于网格大小的模型，即
    12 个，^([1](ch06.xhtml#ch01fn14)) 机器学习模型——总共 36 个。
- en: 'This is how we define it:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们如何定义它的方式：
- en: '[PRE29]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Similar to `TrainValidationSplit`’s `validationMetrics`, `CrossValidatorModel`
    has an `avgMetrics` parameter that can be used to retrieve the training metrics.
    For every parameter grid combination, it holds the average evaluation of the `numFold​.par⁠al⁠lel⁠ism`
    is used for evaluating models in parallel; when set to `1`, sequential evaluation
    is performed. `parallelism` has the same meaning in `TrainValidationSplit`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `TrainValidationSplit` 的 `validationMetrics` 类似，`CrossValidatorModel` 具有 `avgMetrics`
    参数，可用于获取训练指标。对于每个参数网格组合，它保存了 `numFold​.par⁠al⁠lel⁠ism` 用于并行评估模型；当设置为 `1` 时，执行顺序评估。`parallelism`
    在 `TrainValidationSplit` 中具有相同的含义。
- en: Executing `cv_model.avgMetrics` would result in the output shown in [Example 6-7](#model_training_average_evaluation_metri).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 `cv_model.avgMetrics` 将会得到如 [示例 6-7](#model_training_average_evaluation_metri)
    所示的输出。
- en: Example 6-7\. Model training average evaluation metrics output
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-7\. 模型训练平均评估指标输出
- en: '[PRE30]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Each cell in the `avgMetrics` array is calculated using the following equation:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`avgMetrics` 数组中的每个单元格都是使用以下方程计算得出的：'
- en: <math alttext="a v e r a g e upper E v a l equals StartFraction sum left-parenthesis
    upper E v a l upper F o r upper F o l d right-parenthesis Over upper N u m upper
    O f upper F o l d EndFraction"><mrow><mi>a</mi> <mi>v</mi> <mi>e</mi> <mi>r</mi>
    <mi>a</mi> <mi>g</mi> <mi>e</mi> <mi>E</mi> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mo>=</mo>
    <mfrac><mrow><mtext>sum</mtext><mo>(</mo><mi>E</mi><mi>v</mi><mi>a</mi><mi>l</mi><mi>F</mi><mi>o</mi><mi>r</mi><mi>F</mi><mi>o</mi><mi>l</mi><mi>d</mi><mo>)</mo></mrow>
    <mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>O</mi><mi>f</mi><mi>F</mi><mi>o</mi><mi>l</mi><mi>d</mi></mrow></mfrac></mrow></math>
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="a v e r a g e upper E v a l equals StartFraction sum left-parenthesis
    upper E v a l upper F o r upper F o l d right-parenthesis Over upper N u m upper
    O f upper F o l d EndFraction"><mrow><mi>a</mi> <mi>v</mi> <mi>e</mi> <mi>r</mi>
    <mi>a</mi> <mi>g</mi> <mi>e</mi> <mi>E</mi> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mo>=</mo>
    <mfrac><mrow><mtext>sum</mtext><mo>(</mo><mi>E</mi><mi>v</mi><mi>a</mi><mi>l</mi><mi>F</mi><mi>o</mi><mi>r</mi><mi>F</mi><mi>o</mi><mi>l</mi><mi>d</mi><mo>)</mo></mrow>
    <mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>O</mi><mi>f</mi><mi>F</mi><mi>o</mi><mi>l</mi><mi>d</mi></mrow></mfrac></mrow></math>
- en: Machine Learning Pipelines
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习流水线
- en: 'This section introduces the concept of the machine learning pipeline, which
    is constructed from the various building blocks you’ve learned about so far: featurization,
    model training, model evaluation, and model tuning. [Figure 6-5](#the_ml_workflow_in_spark)
    visualizes the end-to-end process: ingesting data, preprocessing and cleaning
    it, transforming the data, building and tuning the model, and evaluating it.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了机器学习流水线的概念，它由您到目前为止学到的各种构建模块组成：特征处理、模型训练、模型评估和模型调优。[图 6-5](#the_ml_workflow_in_spark)
    可视化了端到端的过程：数据摄取、预处理和清理、数据转换、模型构建和调优，以及模型评估。
- en: '![](assets/smls_0605.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0605.png)'
- en: Figure 6-5\. The machine learning workflow in Spark
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. Spark 中的机器学习工作流程
- en: 'The Pipelines API provides two main components, built on top of DataFrames
    and Datasets:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 管道 API 提供了基于 DataFrame 和 Dataset 的两个主要组件：
- en: Transformer
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器
- en: A function that converts data in some way. As you learned in [Chapter 4](ch04.xhtml#data_ingestioncomma_preprocessingcomma),
    a transformer takes in a DataFrame and outputs a new DataFrame with the columns
    modified as desired. Transformers can take in Spark parameters as well, as you’ll
    see in the following example; this allows us to influence the function itself,
    including specifying the names of input and output columns. Providing temporary
    parameters gives us the flexibility to evaluate and test multiple variations using
    the same object.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 一个以某种方式转换数据的函数。正如您在[第四章](ch04.xhtml#data_ingestioncomma_preprocessingcomma)中学到的那样，转换器接收一个DataFrame，并输出一个修改后的新DataFrame。转换器还可以接收Spark参数，正如您将在接下来的示例中看到的那样；这允许我们影响函数本身，包括指定输入和输出列的名称。提供临时参数使我们能够评估和测试使用同一对象的多种变体。
- en: All transformers implement the function `Transformer.transform`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 所有转换器实现函数`Transformer.transform`。
- en: Estimator
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器
- en: An object that abstracts the concept of any algorithm that fits or trains on
    data. An estimator outputs a model or a transformer. For example, a learning algorithm
    such as `GaussianMixture` is an estimator, and calling its `fit` method trains
    a `GaussianMixtureModel`, which is a model and thus a transformer. Like a transformer,
    an estimator can take parameters as input.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象任何适合或训练数据的算法概念的对象。估计器输出模型或转换器。例如，像`GaussianMixture`这样的学习算法是一个估计器，调用其`fit`方法训练`GaussianMixtureModel`，这是一个模型，因此也是一个转换器。与转换器一样，估计器可以接收输入参数。
- en: All estimators implement the function `Estimator.fit`.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 所有估计器实现函数`Estimator.fit`。
- en: 'Going back to our CO[2] emissions prediction example, our `UnivariateFeatureSelector`
    is an estimator. As we specified in the `outputCol` and `featuresCol` parameters,
    it takes in a column called `hashed_features` and produces a new DataFrame with
    a new column called `selectedFeatures` appended:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的CO[2]排放预测示例，我们的`UnivariateFeatureSelector`是一个估计器。正如我们在`outputCol`和`featuresCol`参数中指定的那样，它接收一个名为`hashed_features`的列，并生成一个新的DataFrame，其中附加了一个名为`selectedFeatures`的新列：
- en: '[PRE31]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Calling `fit` on the estimator creates an instance of `UnivariateFeatureSelectorModel`
    that we assign to `model_select`. `model_select` is now a transformer that we
    can use to create a new DataFrame with the appended column, `selectedFeatures`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对估计器调用`fit`会创建一个`UnivariateFeatureSelectorModel`的实例，我们将其分配给`model_select`。`model_select`现在是一个转换器，我们可以使用它来创建一个新的DataFrame，并附加一个名为`selectedFeatures`的列。
- en: Both estimators and transformers on their own are stateless. This means that
    once we use them to create a model or another transformer instance, they do not
    change or keep any information about the input data. They solely retain the parameters
    and the model representation.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器和转换器本身都是无状态的。这意味着一旦我们使用它们创建模型或另一个转换器实例，它们不会更改或保留关于输入数据的任何信息。它们仅保留参数和模型表示。
- en: For machine learning, that means that the model’s state does not change over
    time. Hence, for online/adaptive machine learning, where new real-time data becomes
    available in sequential order and is used to update the model, you’ll need to
    use PyTorch, TensorFlow, or another framework that supports this.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习而言，这意味着模型的状态随时间不变。因此，在在线/自适应机器学习中，新的实时数据按顺序变得可用并用于更新模型时，您需要使用PyTorch、TensorFlow或支持此功能的另一个框架。
- en: Constructing a Pipeline
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建管道
- en: In machine learning, it is common to run multiple steps in a sequential order
    to produce and evaluate a model. MLlib provides a dedicated `Pipeline` object
    for constructing a sequence of stages to run as a unit. An MLlib pipeline is an
    estimator with a dedicated `stages` param; each stage is a transformer or estimator.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，通常会按顺序运行多个步骤来生成和评估模型。MLlib提供了一个专用的`Pipeline`对象，用于构建作为单元运行的一系列阶段。MLlib管道是一个估计器，具有专用的`stages`参数；每个阶段都是一个转换器或估计器。
- en: 'Earlier in this chapter we defined a hasher, a selector, and a Gaussian mixture
    algorithm. Now, we are going to put them all together in a pipeline by assigning
    an array to the `stages` param:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，我们定义了一个哈希器、一个选择器和一个高斯混合算法。现在，我们将通过将数组分配给`stages`参数将它们全部放在管道中：
- en: '[PRE32]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Remember to set the input and output columns according to the stage’s sequence!
    For example, the `hasher`’s output columns can be inputs for either the `selector`
    or the `gm` stage. Try to produce only the columns you will use.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 记得根据阶段的顺序设置输入和输出列！例如，`hasher`的输出列可以作为`selector`或`gm`阶段的输入。尽量只生成您将使用的列。
- en: Warning
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Your pipelines may fail with all kinds of exceptions if you don’t initialize
    them correctly. If you are adding stages on the fly, make sure to initialize the
    `stages` property with an empty list, as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您未正确初始化管道，您的管道可能会因各种异常而失败。如果您正在动态添加阶段，请确保用空列表初始化 `stages` 属性，如下所示：
- en: '[PRE33]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How Does Splitting Work with the Pipeline API?
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道 API 如何分割工作？
- en: Since the pipeline instance is an estimator, we can pass the pipeline to any
    function that takes an estimator as an argument. That includes all the functions
    for splitting a dataset that we learned about earlier, like `CrossValidator`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 由于管道实例是一个评估器，我们可以将管道传递给任何接受评估器作为参数的函数。这包括我们之前学习的所有用于数据集拆分的函数，例如 `CrossValidator`。
- en: 'Here’s an example of how this works:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是这个工作的一个示例：
- en: '[PRE34]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, it’s pretty straightforward!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这非常简单明了！
- en: Persistence
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持久化
- en: 'An important part of the machine learning pipeline is persisting the output
    by saving it to disk. This will enable you to deploy the model to a staging or
    production environment, share it with colleagues for collaborative work, or just
    save it for future reference. MLlib provides this functionality using `.write().save(``*model_path*``)`
    for all of its models, including the `PipelineModel`:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习管道的一个重要部分是通过将其保存到磁盘来持久化输出。这将使您能够将模型部署到暂存或生产环境中，与同事分享以进行协作工作，或仅仅为将来参考而保存。MLlib
    提供了使用 `.write().save(``*model_path*``)` 将其所有模型（包括 `PipelineModel`）保存到磁盘的功能：
- en: '[PRE35]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To load an MLlib model from disk, you must know the model class used for saving
    it. In our case, `CrossValidator` produces a `CrossValidatorModel`, and this is
    what we use to load the model via the `load` function:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 要从磁盘加载 MLlib 模型，您必须知道用于保存模型的模型类。在我们的情况下，`CrossValidator` 生成 `CrossValidatorModel`，这是我们通过
    `load` 函数加载模型所使用的内容：
- en: '[PRE36]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now we have loaded our model into memory, and it is ready for use.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将模型加载到内存中，并且已经准备好供使用。
- en: You can also export your model to a portable format like ONNX and then use the
    ONNX runtime to run the model, though not all MLlib models support this. We will
    discuss this and other formats in [Chapter 8](ch08.xhtml#tensorflow_distributed_ml_approach).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将模型导出为像 ONNX 这样的可移植格式，然后使用 ONNX 运行时来运行模型，尽管并非所有 MLlib 模型都支持此功能。我们将在[第 8
    章](ch08.xhtml#tensorflow_distributed_ml_approach)中讨论这个及其他格式。
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter provided an introduction to MLlib’s supervised and unsupervised
    machine learning algorithms, training and evaluating them, and constructing a
    pipeline for collaborative, structured work. It contained a lot of information
    and insights into working with MLlib that you might want to revisit as you learn
    more about machine learning and Spark.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 MLlib 的监督和无监督机器学习算法，训练和评估这些算法，并构建用于协作结构化工作的管道。它包含了许多关于使用 MLlib 工作的信息和见解，您在学习更多有关机器学习和
    Spark 的知识时可能会想要重新访问。
- en: The next chapters are going to show you how to leverage all the work you’ve
    done thus far and extend Spark’s machine learning capabilities by bridging into
    other frameworks, such as PyTorch and TensorFlow.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将向您展示如何利用迄今为止所做的所有工作，并通过与其他框架（如 PyTorch 和 TensorFlow）的桥接来扩展 Spark 的机器学习能力。
- en: ^([1](ch06.xhtml#ch01fn14-marker)) We specified 2 options for `evaluator.distanceMeasure`,
    2 options for `kmeans.distanceMeasure`, and 3 options for `kmeans.maxIter`, so
    the grid size is 2 * 2 * 3 = 12.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.xhtml#ch01fn14-marker)) 我们为 `evaluator.distanceMeasure` 指定了 2 个选项，为
    `kmeans.distanceMeasure` 指定了 2 个选项，并为 `kmeans.maxIter` 指定了 3 个选项，因此网格大小为 2 * 2
    * 3 = 12。
