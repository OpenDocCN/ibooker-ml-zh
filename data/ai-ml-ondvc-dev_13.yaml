- en: Chapter 13\. Create ML and Core ML for Simple iOS Apps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 13 章\. 为简单的 iOS 应用程序创建 ML 和 Core ML
- en: In this book so far, you’ve been looking at technologies that bring machine
    learning to *multiple* devices, so that you could use a single API to reach Android,
    iOS, embedded systems, microcontrollers, and more. This was made possible by the
    TensorFlow ecosystem, and in particular TensorFlow Lite, which underpins ML Kit,
    which you used as a higher level API. And while we didn’t go into embedded systems
    and microcontrollers, the concepts are the same, other than hardware limitations
    the smaller you go. To learn more about that space, check out the great book [*TinyML*](https://learning.oreilly.com/library/view/tinyml/9781492052036/)
    by Pete Warden and Daniel Situnayake (O’Reilly).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到了将机器学习带到*多个*设备的技术，以便您可以使用单个 API 来访问 Android、iOS、嵌入式系统、微控制器等。这得益于
    TensorFlow 生态系统，特别是 TensorFlow Lite，它支持 ML Kit，您可以将其用作更高级别的 API。虽然我们没有深入讨论嵌入式系统和微控制器，但概念是相同的，除了硬件限制随着尺寸的缩小而变得更小。要了解更多关于这个领域的信息，请参阅
    Pete Warden 和 Daniel Situnayake（O’Reilly）的优秀著作 [*TinyML*](https://learning.oreilly.com/library/view/tinyml/9781492052036/)。
- en: But I would be remiss if I didn’t at least cover the iOS-specific Create ML
    tool and Core ML libraries from Apple, which are designed to let you use ML models
    when creating apps for iOS, iPadOS or MacOS. Create ML in particular is a really
    nice visual tool that lets you create models without any prior ML programming
    experience.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果我不至少涵盖 Apple 的 iOS 特定的 Create ML 工具和 Core ML 库，我会觉得遗憾。这些工具旨在让您在为 iOS、iPadOS
    或 MacOS 创建应用程序时使用 ML 模型。特别是 Create ML 是一个非常好的可视化工具，让您可以在没有任何先前 ML 编程经验的情况下创建模型。
- en: We’ll look at a few scenarios, starting with creating a model that recognizes
    flowers, similar to the ones we did earlier with TensorFlow and TensorFlow Lite.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看几种场景，首先是创建一个类似之前使用 TensorFlow 和 TensorFlow Lite 做的花卉识别模型。
- en: A Core ML Image Classifier Built Using Create ML
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Create ML 构建的 Core ML 图像分类器
- en: We’ll start with creating our model. We can do this codelessly using the Create
    ML tool. You can find this by right-clicking on Xcode in the doc, and then in
    the Open Developer Tool menu, you can find Create ML. See [Figure 13-1](#launching_create_ml).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从创建我们的模型开始。我们可以使用 Create ML 工具无需编写代码来完成这个过程。您可以通过右键单击 Dock 中的 Xcode，然后在“打开开发者工具”菜单中找到
    Create ML。请参阅 [图 13-1](#launching_create_ml)。
- en: '![](assets/aiml_1301.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1301.png)'
- en: Figure 13-1\. Launching Create ML
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-1\. 启动 Create ML
- en: When the tool launches, you’ll first be asked *where* you want to store the
    finished model. It’s a little jarring if you aren’t used to it, as you’d normally
    go through a template to select the type before you pick a location. It fooled
    me a couple of times where I thought this was a file dialog from another open
    app! From the dialog, select New Document at the bottom left ([Figure 13-2](#starting_a_new_model_with_create_ml)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当工具启动时，首先会询问您 *想要* 存储完成的模型的位置。如果您不习惯这种方式，可能会感到有些突兀，因为通常在选择位置之前会通过模板选择类型。这让我几次以为这是来自另一个打开应用的文件对话框！从对话框中，在左下角选择新文档（[图 13-2](#starting_a_new_model_with_create_ml)）。
- en: '![](assets/aiml_1302.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1302.png)'
- en: Figure 13-2\. Starting a new model with Create ML
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-2\. 使用 Create ML 开始新模型
- en: After selecting the location and clicking New Document, you’ll be given a list
    of templates for the type of model you can create with Create ML. See [Figure 13-3](#choosing_a_create_ml_template).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择位置并单击新文档后，您将获得一个模板列表，用于创建 Create ML 的模型类型。请参见 [图 13-3](#choosing_a_create_ml_template)。
- en: '![](assets/aiml_1303.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1303.png)'
- en: Figure 13-3\. Choosing a Create ML template
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-3\. 选择 Create ML 模板
- en: In this scenario, we’ll do an image classification model, so choose Image Classification
    and click Next. You’ll be asked to give your project a name and other details
    like an Author, License, Description, etc. Fill these out and click Next.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将进行图像分类模型，因此选择图像分类并单击“下一步”。您将被要求为项目命名，并填写其他细节，如作者、许可证、描述等。填写完毕后，单击“下一步”。
- en: You’ll then be asked *again* where to store the model. You can create a new
    folder and put it in there, or just click Create. The model designer will open.
    You can see it in [Figure 13-4](#the_model_designer).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后再次询问您想要 *哪里* 存储模型。您可以创建一个新文件夹并放入其中，或者只需单击“创建”。模型设计师将会打开。您可以在 [图 13-4](#the_model_designer)
    中看到它。
- en: To train a model with the model designer in Create ML, you’ll need a set of
    images. You’ll need to organize them into subfolders of each particular type of
    item you want to classify (i.e., the label), so, for example, if you consider
    the flowers dataset we’ve used throughout this book, your directory structure
    will probably look like [Figure 13-5](#images_stored_in_labeled_subdirectories).
    If you download the flowers from the Google API directory and unzip, they’ll already
    be in this structure. You can find the data at [*https://oreil.ly/RuN2o*](https://oreil.ly/RuN2o).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 若要在 Create ML 的模型设计师中训练模型，您需要一组图像。您需要将它们组织成每种要分类的特定类型项目的子文件夹（即标签），因此，例如，如果您考虑我们在本书中一直使用的花卉数据集，您的目录结构可能看起来像
    [图 13-5](#images_stored_in_labeled_subdirectories)。如果您从 Google API 目录下载并解压这些花卉，它们已经处于这种结构中。您可以在
    [*https://oreil.ly/RuN2o*](https://oreil.ly/RuN2o) 找到这些数据。
- en: '![](assets/aiml_1304.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1304.png)'
- en: Figure 13-4\. The model designer
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-4\. 模型设计师
- en: '![](assets/aiml_1305.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1305.png)'
- en: Figure 13-5\. Images stored in labeled subdirectories
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-5\. 图像存储在带标签的子目录中
- en: So, in this case, the folder called *daisy* contains images of daisies, *dandelion*
    contains images of dandelions, and so on. To train the dataset, drag this folder
    over the Training Data section in the model designer. When you’re done it should
    look like [Figure 13-6](#adding_the_data_to_the_designer).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，名为*daisy*的文件夹包含雏菊的图片，*dandelion*包含蒲公英的图片，依此类推。要训练数据集，请将此文件夹拖放到模型设计师的训练数据部分上。完成后，它应该看起来像
    [图 13-6](#adding_the_data_to_the_designer)。
- en: '![](assets/aiml_1306.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1306.png)'
- en: Figure 13-6\. Adding the data to the designer
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-6\. 将数据添加到设计师
- en: Note that there were five folders shown in [Figure 13-5](#images_stored_in_labeled_subdirectories),
    and that correlates to the five classes you can see in [Figure 13-6](#adding_the_data_to_the_designer).
    Between these there are 3,670 images. Note also that the tool will automatically
    create a validation dataset from these by splitting from the training data. This
    saves you a lot of work! In this case, a percentage of your images will be held
    out of the training set so that on each epoch the model can be tested using images
    it hasn’t previously seen. That way you can get a better estimate for its accuracy.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[图 13-5](#images_stored_in_labeled_subdirectories) 中显示了五个文件夹，这对应于 [图 13-6](#adding_the_data_to_the_designer)
    中显示的五个类别。在这些类别之间，共有 3,670 张图片。还请注意，该工具将通过从训练数据中分割来自动创建验证数据集。这为您节省了大量工作！在这种情况下，一部分图像将被保留在训练集之外，以便在每个
    epoch 中使用先前未见过的图像对模型进行测试。这样，您可以更好地估计其准确性。
- en: Note that at the bottom of the screen, you can choose Augmentations. This allows
    you to artificially extend the scope of your dataset by amending it on the fly
    as you are training. So, for example, pictures of flowers are *usually* taken
    with the stem at the bottom and the petals at the top. If your training data is
    oriented this way, then it will only be accurate with pictures taken the same
    way. If you give it an image that has a flower lying on its side, it might not
    accurately classify it. So, instead of taking the expensive action of taking lots
    more pictures of flowers in other orientations to get effective training coverage,
    you can use augmentation. So, for example, if you check the Rotate box, then some
    of the images will be randomly rotated while training, in order to simulate the
    effect of you taking new flower pictures. If your models overfit to your training
    data—they get really good at recognizing data that looks like the training data,
    but not so good for other images—it’s worth investigating different augmentation
    settings. But for now you don’t need them.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可以在屏幕底部选择增强选项。这使您可以在训练过程中通过修改来人为扩展数据集的范围。例如，花卉的图片通常是*底部为茎，顶部为花瓣*的方式拍摄的。如果您的训练数据是这样定向的，那么只有采用相同方向的花朵图片才能准确分类。如果您给它一张侧躺的花朵图片，它可能无法准确分类。因此，与采取大量拍摄其他方向花朵图片的昂贵行动相比，您可以使用增强技术。例如，如果您勾选旋转框，那么在训练过程中，某些图像将被随机旋转，以模拟您拍摄新花朵图片的效果。如果您的模型过度拟合于训练数据——即它在识别看起来像训练数据的数据方面非常擅长，但对于其他图像效果不佳——则值得研究不同的增强设置。但目前您不需要它们。
- en: When ready, press the Train button at the top left of the screen. Create ML
    will process the features in the images, and after a couple of minutes will present
    you with a trained model. Note that it’s using t*ransfer learning* here, instead
    of training from scratch, similar to Model Maker, and as such, the training can
    be both accurate *and* fast.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好后，点击屏幕左上角的“训练”按钮。Create ML将处理图片中的特征，并在几分钟后向你呈现一个训练好的模型。请注意，这里使用的是*迁移学习*，而不是从头开始训练，类似于Model
    Maker，因此训练既准确*又*快速。
- en: When a model is stable in its accuracy metrics for a number of epochs, it’s
    generally considered to have *converged*, where it’s not likely to get any better
    with continued training, so it will be stopped early. The default number of epochs
    that Create ML uses for training a model is 25, but the flowers model will likely
    converge at around 10, and you’ll see its accuracy metrics appearing to look a
    bit like [Figure 13-7](#the_model_converges).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型在准确度指标上稳定一段时间后，通常被认为已经*收敛*，即继续训练不太可能使其变得更好，因此会提前停止。Create ML用于训练模型的默认时代数是25，但花卉模型可能会在大约10次时达到收敛状态，此时你会看到其准确度指标看起来有点像[图 13-7](#the_model_converges)。
- en: '![](assets/aiml_1307.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1307.png)'
- en: Figure 13-7\. The model converges
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-7\. 模型收敛
- en: You can click the Evaluation tab to explore how well the model did on each different
    class. On the lefthand side you can choose the Training Set, Validation Set, or
    Testing Set. As I didn’t create a testing set in this instance, I just have the
    first two, and you can see the results of the training in [Figure 13-8](#exploring_the_training_accuracy).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以点击评估选项卡，查看模型在每个不同类别上的表现。在左侧，你可以选择训练集、验证集或测试集。由于我在这个实例中没有创建测试集，所以我只有前两个，你可以在[图 13-8](#exploring_the_training_accuracy)中看到训练结果。
- en: In this case you can see that 594 of the daisy images were used for training,
    and 39 for validation. Similar splits can be seen in the other flowers. There
    are two columns, precision and recall, where precision is the percentage of instances
    where the classifier classified an image correctly, i.e., of the 594 daisies,
    the classifier correctly identified it as a daisy 95% of the time. The recall
    value in this case should be very close to the accuracy value and is generally
    only one you should pay attention to if there are *other* elements in the picture
    other than the particular flower. As this dataset is a simple one, i.e., a daisy
    picture *only* contains a daisy, or a rose picture *only* contains a rose, then
    it’s safe not to pay attention to it. You can learn more about precision and recall
    on [*Wikipedia*](https://oreil.ly/4dscv).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你可以看到有594张雏菊图片用于训练，39张用于验证。其他花卉也有类似的分割。有两列，精确度和召回率，其中精确度是分类器正确分类图像的百分比，即在594个雏菊中，分类器95%的时间能正确识别为雏菊。在这种情况下，召回率的值应该非常接近准确率值，通常只有当图片中除了特定花卉之外还有*其他*元素时才需要注意。由于这个数据集很简单，即一个雏菊图片*只*包含一个雏菊，或者玫瑰图片*只*包含一朵玫瑰，所以可以放心忽略它。你可以在[*维基百科*](https://oreil.ly/4dscv)上了解更多关于精确度和召回率的信息。
- en: '![](assets/aiml_1308.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1308.png)'
- en: Figure 13-8\. Exploring the training accuracy
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-8\. 探索训练准确度
- en: You can go to the Preview tab and drag/drop images onto it to try out the model.
    So, for example, in [Figure 13-9](#using_the_preview_to_test_my_model), I dropped
    images that are neither in the training set nor the validation set onto Create
    ML and checked out the classifications. As you can see, it correctly identified
    these tulips with 99% confidence.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以转到预览选项卡，并将图片拖放到其中以测试模型。例如，在[图 13-9](#using_the_preview_to_test_my_model)中，我放入了不属于训练集或验证集的图片，并检查了分类结果。如你所见，它以99%的置信度正确识别了这些郁金香。
- en: '![](assets/aiml_1309.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1309.png)'
- en: Figure 13-9\. Using the Preview to test my model
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-9\. 使用预览测试我的模型
- en: Finally on the output tab you can export the model. You’ll see a button called
    Get at the top left. Click that, and you’ll be given the option to save an MLModel
    file. Save it as something simple, like *flowers.mlmodel*, and you’ll use it in
    an iOS app in the next step.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在输出选项卡中，你可以导出模型。你会看到左上角有一个名为“获取”的按钮。点击它，你将有选项保存MLModel文件。将其保存为类似*flowers.mlmodel*这样简单的名称，你将在下一步中在iOS应用中使用它。
- en: Making a Core ML App That Uses a Create ML Model
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制作一个使用Create ML模型的Core ML应用
- en: Let’s now explore how this will look in an app. You can get the full app in
    the repo for this book, so I won’t go into the specifics of how to set up the
    user interface. It will have six images stored as assets named “1” through “6,”
    with buttons to allow the user to cycle through these, and a classify button to
    perform the inference. You can see the storyboard for this in [Figure 13-10](#storyboard_for_flowers_classifier).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探讨这在应用程序中的表现。您可以在本书的存储库中获取完整的应用程序，因此我不会详细介绍如何设置用户界面。它将有六张存储为资产命名为“1”到“6”的图像，并有按钮允许用户在这些图像间切换，以及一个分类按钮来执行推理。您可以在
    [第 13-10 图](#storyboard_for_flowers_classifier) 中看到此故事板。
- en: '![](assets/aiml_1310.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1310.png)'
- en: Figure 13-10\. Storyboard for flowers classifier
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第 13-10 图。花卉分类器的故事板
- en: Add the MLModel File
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加 MLModel 文件
- en: To add the MLModel file that you created with Create ML, simply drag and drop
    it onto the project window in Xcode. Xcode will import the model *and* create
    a Swift wrapper class for it. If you select the model within Xcode you should
    see lots of details for it, including the list of labels, version, author, and
    more. See [Figure 13-11](#browsing_the_model).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加您使用 Create ML 创建的 MLModel 文件，只需将其拖放到 Xcode 项目窗口中即可。Xcode 将导入该模型 *并* 为其创建一个
    Swift 封装类。如果您在 Xcode 中选择该模型，您应该可以看到其包括标签列表、版本、作者等的许多详细信息。参见 [第 13-11 图](#browsing_the_model)。
- en: '![](assets/aiml_1311.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1311.png)'
- en: Figure 13-11\. Browsing the model
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第 13-11 图。浏览模型
- en: You can even test out the model in a Preview tab just like in Create ML! With
    the Utilities tab you can also encrypt your model and prepare it for cloud deployment.
    That’s beyond the scope of this book; you can find out details on the [Apple developer
    site](https://oreil.ly/NZdPM).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您甚至可以像在 Create ML 中一样在预览选项卡中测试模型！在 Utilities 选项卡中，您还可以对模型进行加密并准备进行云部署。这超出了本书的范围；您可以在
    [Apple 开发者网站](https://oreil.ly/NZdPM) 上找到详细信息。
- en: Finally, before going further, in the Model Class section at the top center
    of the screen, you can see the automatically generated Swift model class, in this
    case called “flower.” You can click on it to see the autogenerated code. The important
    thing to note is the name—which in this case is “flower,” as you’ll need that
    later.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在屏幕顶部中心的 Model Class 部分，您可以看到自动生成的 Swift 模型类，本例中称为“flower”。您可以点击它查看自动生成的代码。需要注意的重要事项是名称——在本例中是“flower”，因为您稍后会用到它。
- en: Run the Inference
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行推理
- en: When the user presses the button, we want to load the current image,and pass
    it to Core ML to invoke our model and get an inference. Before getting into the
    code for this, it might be good to review the coding pattern used, as it is a
    little complex.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户按下按钮时，我们希望加载当前图像，并将其传递给 Core ML 来调用我们的模型并进行推理。在深入研究这个过程的代码之前，最好先回顾一下使用的编码模式，因为它有些复杂。
- en: The Core ML inference pattern
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Core ML 推理模式
- en: You can use this model in an app that uses Core ML. This API has been designed
    to make it easy to use ML models in an iOS app, but until you understand the overall
    pattern of building using ML with Core ML, it may seem a little convoluted.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在使用 Core ML 的应用程序中使用此模型。这个 API 已经被设计成在 iOS 应用程序中使用 ML 模型变得很容易，但是在理解使用 Core
    ML 构建 ML 的整体模式之前，它可能看起来有些复杂。
- en: The idea with Core ML is to ensure asynchronous performance wherever possible,
    and model inference can be a bottleneck. As Core ML is designed as a mobile API,
    it uses patterns to ensure that the user experience doesn’t pause or break while
    model inference is going on. Thus, to use an image model like this in a Core ML
    app, you’ll see a number of asynchronous steps. You can see this in [Figure 13-12](#using_core_ml_to_asynchronously_infer_i).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Core ML 的理念是尽可能确保异步性能，并且模型推理可能是一个瓶颈。由于 Core ML 设计为移动 API，它使用模式确保在进行模型推理时用户体验不会中断或中断。因此，在
    Core ML 应用程序中使用像这样的图像模型，您会看到许多异步步骤。您可以在 [第 13-12 图](#using_core_ml_to_asynchronously_infer_i)
    中看到这一点。
- en: '![](assets/aiml_1312.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1312.png)'
- en: Figure 13-12\. Using Core ML to asynchronously infer images and update the UI
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第 13-12 图。使用 Core ML 异步推理图像并更新 UI
- en: The pattern will be to create a handler within a dispatch queue to ensure asynchronicity.
    This is represented by the larger of the two downward arrows in [Figure 13-12](#using_core_ml_to_asynchronously_infer_i).
    This handler will be a `VNImageRequestHandler` as we are doing an image classification
    (VN for “VisioN”). This handler will perform a classification request.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 模式是在调度队列内创建一个处理程序，以确保异步性。这由[Figure 13-12](#using_core_ml_to_asynchronously_infer_i)中较大的向下箭头表示。这个处理程序将是一个`VNImageRequestHandler`，因为我们正在进行图像分类（VN代表“VisioN”）。这个处理程序将执行分类请求。
- en: The classification request (of type `VNCoreMLRequest`) will initialize the model,
    and specify a request to the model, with a callback function to process the results.
    This callback will happen upon a successful `VNCoreMLRequest`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 分类请求（类型为`VNCoreMLRequest`）将初始化模型，并指定一个请求到模型，带有一个处理结果的回调函数。这个回调将在成功的`VNCoreMLRequest`时发生。
- en: The callback will generally be asynchronous, as it updates the UI, and will
    read the classification results (as `VNClassificationObservation`s), and write
    them out to the UI. This is represented by the smaller dispatch queue arrow in
    [Figure 13-12](#using_core_ml_to_asynchronously_infer_i).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 回调通常是异步的，因为它更新UI，并读取分类结果（作为`VNClassificationObservation`），并将它们写入UI。这由[Figure 13-12](#using_core_ml_to_asynchronously_infer_i)中较小的调度队列箭头表示。
- en: Writing the code
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写代码
- en: 'Let’s now explore the code for this. When the user takes the action of pressing
    the button, you’ll call a function called `interpretImage` to kick off the inference
    workflow, and it looks like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来探索这段代码。当用户执行按按钮的操作时，你将调用一个名为`interpretImage`的函数来启动推理工作流程，代码如下所示：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This simply creates a UIImage from the currently selected image and passes
    it to a function called `getClassification`. This function will implement the
    pattern from [Figure 13-10](#storyboard_for_flowers_classifier), so let’s explore
    it. I’ve abbreviated the output strings to make the printed code here more readable:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅是从当前选定的图像创建一个UIImage，并将其传递给名为`getClassification`的函数。这个函数将实现来自[Figure 13-10](#storyboard_for_flowers_classifier)的模式，所以让我们来探索一下。我已经缩短了输出字符串，以使这里打印的代码更易读：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code will first get our UIImage, and turn it into a CIImage. Core ML is
    built using Core Image, which requires the image to be represented in that format.
    So we’ll need to start with that.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将首先获取我们的UIImage，并将其转换为CIImage。Core ML是使用Core Image构建的，它要求图像以那种格式表示。因此，我们需要从那里开始。
- en: 'Then, we’ll invoke our first DispatchQueue, which is the larger, outer one
    in [Figure 13-10](#storyboard_for_flowers_classifier). Within it, we’ll create
    our handler, and have it call its perform method on a `classificationRequest`.
    We haven’t created that yet, so let’s explore it now:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将调用我们的第一个DispatchQueue，这是[Figure 13-10](#storyboard_for_flowers_classifier)中较大的外部之一。在其中，我们将创建我们的处理程序，并要求它在`classificationRequest`上执行其perform方法。我们还没有创建它，所以现在让我们来探索一下：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `classificationRequest` is a `VNCoreMLRequest`, which works for a model
    that it initializes internally. Note that the `init` method takes a `flower()`
    type, and reads the `model` property from it. This is the class that was autocreated
    when you imported the MLModel. Refer back to [Figure 13-11](#browsing_the_model)
    and you’ll see the autogenerated code discussed there. You noted the name of your
    class—in my case it was flower—and that’s what you’ll use here.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`classificationRequest`是一个`VNCoreMLRequest`，适用于它内部初始化的模型。请注意，`init`方法接受一个`flower()`类型，并从中读取`model`属性。这是当你导入MLModel时自动生成的类。参考[Figure 13-11](#browsing_the_model)，你会看到讨论过的自动生成的代码。你注意到了你的类的名称——在我的情况下是flower——这就是你将在这里使用的。'
- en: 'Once you have a model, you can create the `VNCoreMLRequest`, specifying the
    model and the completion handler function, which in this case is `processResults`.
    You’ve now constructed the `VNCoreMLRequest` that the `getClassification` function
    required. If you look back to that function, you’ll see that it called the `perform`
    method; this code implements that for you. If that runs successfully, the `processResults`
    callback will be called, so let’s look at that next:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了模型，你可以创建`VNCoreMLRequest`，指定模型和完成处理函数，在这种情况下是`processResults`。现在你已经构建了`VNCoreMLRequest`，这是`getClassification`函数所需的。如果你回头看那个函数，你会看到它调用了`perform`方法；这段代码实现了这一点。如果运行成功，将调用`processResults`回调函数，那么让我们接着看：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This function begins with another `DispatchQueue`, as it will update the user
    interface. It receives results from the initial request, and if they are valid,
    it can cast them into a set of `VNClassificationObservation` objects. Then it’s
    simply a matter of iterating through these and getting the confidence and identifier
    for each classification and outputting these. This code will also sort them into
    the top classifications, giving you the probability output for each class. `NUM_CLASSES`
    is a constant for the number of classes, and in the case of the flowers model
    I, set it to 5.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数以另一个 `DispatchQueue` 开始，因为它将更新用户界面。它接收来自初始请求的结果，如果它们有效，它可以将它们转换为一组 `VNClassificationObservation`
    对象。然后只需遍历这些对象，获取每个分类的置信度和标识符，并输出它们。这段代码还将它们排序为前几个分类，为每个类别提供概率输出。`NUM_CLASSES`
    是一个表示类别数量的常数，在花卉模型中我将其设置为 5。
- en: And that’s pretty much it. Using Create ML simplified the process of making
    the model, and the Xcode integration, including class file generation, made it
    relatively straightforward to do the inference. The complexity is necessarily
    added by the need to keep the process as asynchronous as possible to avoid breaking
    the user experience when running inference on the model!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。使用 Create ML 简化了制作模型的过程，而 Xcode 集成，包括类文件生成，使推理过程相对简单。复杂性必须通过尽可能使过程异步化来保持，以避免在运行模型推理时破坏用户体验！
- en: You can see the app, with its inferences for a rose picture, in [Figure 13-13](#core_ml_inference_on_flowers).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到应用程序在一张玫瑰图片的推理结果，参见[图 13-13](#core_ml_inference_on_flowers)。
- en: '![](assets/aiml_1313.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1313.png)'
- en: Figure 13-13\. Core ML inference on flowers
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-13\. 花卉的 Core ML 推理
- en: Next we’ll explore a natural language processing (NLP) example, beginning with
    creating a model using Create ML.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将探讨一个自然语言处理（NLP）的例子，首先是使用 Create ML 创建模型。
- en: Using Create ML to Build a Text Classifier
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Create ML 构建文本分类器
- en: Create ML allows you to import CSV files for classification, but your text must
    be in a column called “text,” so if you’ve been following this book and using
    the emotion sentiment dataset, you’ll need to amend it slightly, or use the one
    I provide in the repo for this chapter. The only amendment is to name the first
    column (containing the text) “text.”
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Create ML 允许您导入 CSV 文件进行分类，但您的文本必须在名为 “text” 的列中，因此如果您一直在跟随本书并使用情感情绪数据集，您需要稍作修改，或者使用我在本章节中提供的数据集。唯一的修改是将包含文本的第一列命名为
    “text”。
- en: At that point you can create a new Create ML document using the steps outlined
    earlier, but in this case, choose a Text Classifier template. As before you can
    drop your data onto the data field, and you’ll see that there are two classes
    (in this case for positive and negative sentiment), with over 35,000 items. You
    should split your validation data from your training data as before.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您可以按照之前概述的步骤创建一个新的 Create ML 文档，但在这种情况下，请选择文本分类模板。与之前一样，您可以将数据拖放到数据字段中，您会看到有两类（在这种情况下用于正面和负面情感），共有超过
    35,000 个条目。您应该像之前一样将验证数据与训练数据分开。
- en: In the Parameters section, there are a number of options for the algorithm.
    I have found I can get excellent results by choosing Transfer Learning, and then
    for the feature extractor choosing Dynamic Embedding. This will be slow, as all
    of the embeddings will be learned from scratch, but can give very good results.
    Training with these settings will be slow—for me, on an M1 Mac Mini, it took about
    an hour, but when it was done, it hit a training accuracy of 89.2% after 75 iterations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数部分，有多种算法选项。我发现选择迁移学习，并选择动态嵌入提取器可以获得非常好的结果。这将会很慢，因为所有嵌入都将从头开始学习，但可以得到非常好的结果。使用这些设置进行训练会很慢——对我来说，在
    M1 Mac Mini 上，大约需要一个小时，但完成后，训练精度达到了 89.2%，经过 75 次迭代。
- en: The Preview tab allows you to type in a sentence, and it will be automatically
    classified! See [Figure 13-14](#testing_negative_sentiment) where I typed in what
    is obviously a negative sentence, and we can see it hit label 0 with 98% confidence!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 预览选项卡允许您输入一个句子，它将被自动分类！请参见[图 13-14](#testing_negative_sentiment)，我输入了一个明显是负面句子的例子，我们可以看到它以
    98% 的置信度命中标签 0！
- en: '![](assets/aiml_1314.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1314.png)'
- en: Figure 13-14\. Testing negative sentiment
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-14\. 测试负面情感
- en: But of course, that’s not true. I’m having a wonderful time writing this chapter
    and playing with this technology, so let me see what will happen if I change the
    text to suit! See [Figure 13-15](#a_sentence_with_positive_sentiment).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 但当然，这并不是真的。我在写这一章节和玩弄这项技术时过得非常愉快，所以让我看看如果我改变文本来适应会发生什么！参见 [Figure 13-15](#a_sentence_with_positive_sentiment)。
- en: '![](assets/aiml_1315.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1315.png)'
- en: Figure 13-15\. A sentence with positive sentiment
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-15\. 具有正面情感的句子
- en: As you can see in that case, label 1 scored with 94% confidence. What’s really
    cool is that the classification updates on the fly as you type!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在那个案例中看到的那样，标签 1 以 94% 的置信度得分。真正酷的是，分类会随着你的输入即时更新！
- en: Anyway, enough playing. Let’s get back to work. To build an app that uses this,
    you’ll first need to export your model. You can do that on the Output tab. Use
    the Get button to save it and give it an easy-to-remember name. In my case, I
    called it *emotion.mlmodel*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，玩够了。让我们回到工作中来。要构建一个使用这个的应用程序，你首先需要导出你的模型。你可以在输出选项卡中这样做。使用“获取”按钮保存它，并给它一个容易记住的名称。在我的案例中，我称之为
    *emotion.mlmodel*。
- en: Use the Model in an App
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在应用程序中使用模型
- en: Language models like this one are super simple to use in an app. Create a new
    app and add a UITextView with an outlet called `txtInput`, a UILabel with an outlet
    called `txtOutput`, and a button with an action called `classifyText`. Your storyboard
    should look like [Figure 13-16](#the_storyboard_for_a_simple_language_ap).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样的语言模型在应用程序中使用起来非常简单。创建一个新的应用程序，并添加一个带有名为 `txtInput` 的输出口的 UITextView，一个带有名为
    `txtOutput` 的输出口的 UILabel，以及一个带有名为 `classifyText` 的操作的按钮。你的故事板应该看起来像 [Figure 13-16](#the_storyboard_for_a_simple_language_ap)。
- en: '![](assets/aiml_1316.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1316.png)'
- en: Figure 13-16\. The storyboard for a simple language app
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-16\. 一个简单语言应用程序的故事板
- en: 'Within the `classifyText` action, add a call to `doInference()`. This function
    doesn’t exist yet; you’ll add it shortly. The code at the top of your class should
    look like this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `classifyText` 操作中，添加一个调用 `doInference()` 的语句。这个函数目前还不存在；你马上会添加它。你的类顶部的代码应该是这样的：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To use Core ML with natural language processing, you should also be sure to
    import both these libraries using:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用带有自然语言处理的 Core ML，你还应该确保导入这两个库：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now you can do your inference. Drag and drop the model that you created earlier
    into Xcode, and it will generate a class for you that has the same name as the
    saved model. In my case I called it “emotion,” so I’ll have a class of that name.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以进行推断了。将你之前创建的模型拖放到 Xcode 中，它将为你生成一个与保存模型同名的类。在我的案例中，我称之为“emotion”，所以我会有一个同名的类。
- en: 'You’ll start by creating an `mlModel` type using `emotion`, like this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你将从使用 `emotion` 创建一个 `mlModel` 类型开始，就像这样：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once you have this, you can use it in turn to create an `NLModel` (NL stands
    for natural language):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有了这个，你可以依次使用它来创建一个 `NLModel`（NL 代表自然语言）：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can read the input string from the `txtInput` and pass that to the `sentimentPredictor`
    to get the label for it:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 `txtInput` 中读取输入字符串，并将其传递给 `sentimentPredictor` 以获取其标签：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This label will be a string representing the classes. As you saw in the data
    for this model, they were `"0"` and `"1"`. So, you can output the predictions
    simply like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此标签将是一个表示类别的字符串。正如你在这个模型的数据中看到的那样，它们是 `"0"` 和 `"1"`。因此，你可以简单地输出预测结果，如下所示：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: And that’s it! As you can see the natural language libraries make this really
    easy! You don’t have to deal with tokenizing or embedding; just give it a string
    and you’re done!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！正如你所看到的，自然语言库使这变得非常简单！你不必处理标记化或嵌入；只需给它一个字符串，你就完成了！
- en: You can see the app in action in [Figure 13-17](#using_the_emotion_classifier).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [Figure 13-17](#using_the_emotion_classifier) 中看到应用程序的运行情况。
- en: '![](assets/aiml_1317.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_1317.png)'
- en: Figure 13-17\. Using the emotion classifier
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-17\. 使用情感分类器
- en: This is very much a barebones app, but you can see how you might use it to create
    new functionality into your apps; you could, for example, detect if the app was
    being used to send spam or toxic messages, and deter the user from sending them.
    This could be used in conjunction with backend security to ensure the best possible
    user experience.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个非常简单的应用程序，但你可以看到你如何使用它来为你的应用程序创建新功能；例如，检测应用程序是否被用于发送垃圾邮件或有毒消息，并阻止用户发送它们。这可以与后端安全性结合使用，以确保最佳用户体验。
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter introduced you to two of the templates in Create ML—image classification
    and text sentiment analysis—and guided you through training models with no ML
    experience, before using them in simple apps. You saw how Create ML gave you a
    tool that trains models for you, typically using transfer learning, very quickly,
    and how its output can be dropped into Xcode to take advantage of code generation
    to encapsulate the complexity of the ML model and let you focus on your user interface.
    You stepped through a complex interaction when doing an image classification,
    where you ensure that your user experience isn’t broken when you try to do the
    inference. That being said, it was still pretty easy for you to write something
    that manages the inference; in particular, you don’t have to worry about the image
    format and stripping the image down into tensors in order to pass it to the inference
    engine. As such, if you are only writing for iOS and not thinking about other
    platforms, Create ML and Core ML are a great option, and they are definitely worth
    looking into.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您介绍了 Create ML 中的两个模板——图像分类和文本情感分析，并指导您在没有机器学习经验的情况下训练模型，然后在简单的应用程序中使用它们。您看到了
    Create ML 如何为您提供一个工具，可以非常快速地训练模型，通常使用迁移学习，并且其输出可以轻松集成到 Xcode 中，利用代码生成来封装机器学习模型的复杂性，让您专注于用户界面。在进行图像分类时，您经历了一个复杂的交互过程，确保在进行推断时不会破坏用户体验。尽管如此，对您来说编写一些管理推断的东西仍然相当容易；特别是，您无需担心图像格式以及将图像剥离成张量以传递给推断引擎。因此，如果您仅为
    iOS 编写，并且不考虑其他平台，Create ML 和 Core ML 是一个很好的选择，绝对值得一试。
