- en: Chapter 9\. How Microsoft Runs Cognitive Services for Millions of Users
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 微软如何为数百万用户运行认知服务
- en: In the last two chapters, we looked at how important it is to use AI responsibly
    and with the best practices for machine learning that deliver a practical and
    responsible AI system. But what does the infrastructure and process for doing
    that look like in action?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两章中，我们看到了负责任地使用AI以及为机器学习提供最佳实践有多么重要，这些实践能够提供一个实用和负责任的AI系统。但是在实践中，这样做的基础设施和过程是什么样的呢？
- en: The Azure Cognitive Services we covered in [Chapter 4](ch04.xhtml#using_azure_cognitive_services_to_build)
    run 24-7 in more than 30 Azure regions, underpinning features in Microsoft’s own
    applications as well as for large organizations like Airbus, Progressive Insurance,
    Uber, and Vodafone, powering apps for thousands of employees and millions of their
    customers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4章](ch04.xhtml#using_azure_cognitive_services_to_build)中介绍的Azure认知服务在30多个Azure区域中24小时运行，支持微软自己的应用程序功能以及像Airbus、Progressive
    Insurance、Uber和Vodafone等大型组织的特性，为数千名员工和数百万名客户提供应用程序支持。
- en: There are more than 54 billion Cognitive Services transactions a month; the
    Speech services alone transcribe over 18 million hours of speech a month. Decision
    APIs power 6 million personalized experiences on Xbox every day, and over a billion
    images have been captioned in PowerPoint and Word with automatic alt text created
    by the Vision services. As Teams usage grew during the pandemic, so did Cognitive
    Services usage, because it powers live captioning in meetings and transcription
    of recorded meetings. The Speech services had to scale sevenfold to handle Teams
    caption needs, which consume 2 million core hours of compute a day on Azure.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 每个月超过540亿次认知服务交易；仅语音服务每月转录超过1800万小时的语音。决策API每天为Xbox提供600万个个性化体验，而视觉服务为PowerPoint和Word中的超过10亿张图片添加了自动alt文本。随着大流行期间Teams的使用增长，认知服务的使用量也增加了，因为它为会议中的实时字幕和录制会议的转录提供动力。语音服务必须扩展七倍以处理Teams字幕需求，每天在Azure上消耗200万核心小时的计算资源。
- en: And the different Cognitive Services are updated continuously with previews,
    new features, and fixes to any vulnerabilities in the underlying technology stack.
    If you want to run your own machine learning models, or some of the Cognitive
    Services in containers on your own infrastructure, you have to handle all those
    deployments, updates, and security boundaries yourself.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的认知服务不断进行更新，包括预览、新功能以及修复底层技术堆栈中的任何漏洞。如果您希望在自己的基础设施上运行自己的机器学习模型或一些认知服务容器，那么您必须自行处理所有这些部署、更新和安全边界。
- en: That’s important if you have data sovereignty issues or if you want to use AI
    at the edge where you don’t have connectivity or need real-time decisions—on an
    oil rig or a factory production line.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有数据主权问题或者希望在边缘使用AI而没有连接性或需要实时决策——例如在石油钻井平台或工厂生产线上，这一点就显得很重要了。
- en: But the models used in the Cognitive Services containers you can run have been
    optimized for that. If you use Azure Percept IoT devices to get AI insights at
    the edge, they rely on Cognitive Services, but they use tiny versions of the models
    that can run on a camera rather than in the powerful VMs available in the cloud.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在您可以运行的认知服务容器中使用的模型已经过优化。如果您使用Azure Percept IoT设备在边缘获取AI洞见，它们依赖于认知服务，但它们使用的是可以在摄像头上运行而不是在云中强大虚拟机上运行的微小版本的模型。
- en: It’s also easier for a cloud service to handle bursts of traffic efficiently
    and economically. It’s not just that a hyperscale provider like Azure will get
    a better deal buying hardware, network bandwidth, and electricity because they
    buy so much or even have the operational expertise—important as that is. If you
    have only one customer and they have a burst of traffic, having the capacity for
    that means provisioning hardware that may be underutilized the rest of the time.
    With thousands of customers, cloud services have a bigger buffer to handle surges
    in demand from any one customer, meaning it costs less to deliver that scale.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务还能更有效和经济地处理突发的流量。Azure等超大规模供应商之所以能够更好地购买硬件、网络带宽和电力，是因为它们购买的规模如此之大，甚至具有操作经验——这一点非常重要。如果您只有一个客户并且他们有突发流量，那么拥有容量处理这一流量意味着要为硬件提供资源，这在其他时间可能会被低效利用。云服务因为有数千个客户，因此有更大的缓冲区来处理任何一个客户的需求激增，这意味着以更低的成本提供这种规模。
- en: AI for Anyone
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人人都能用AI
- en: What is now Cognitive Services launched in 2015 as four APIs under the codename
    Project Oxford, for vision, speech, face recognition, and language understanding.
    Another five APIs soon followed, then another dozen, with the latest techniques
    and models developed by Microsoft Research moving quickly into production. But
    by the time there were 30 Cognitive Services, each service ran on its own instance
    of any of four different platforms with its own support team.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如今的认知服务在 2015 年作为名为 Project Oxford 的四个 API 推出，用于视觉、语音、面部识别和语言理解。不久之后又推出了另外五个
    API，然后又推出了另外十几个，使用了微软研究开发的最新技术和模型，并迅速投入生产。但到达 30 个认知服务的时候，每个服务都在四种不同平台的自己的实例上运行，并有自己的支持团队。
- en: That kind of fragmentation happens as you develop new products quickly but isn’t
    sustainable if you want to keep quality and efficiency as you grow. There’s always
    new research in AI that can make an existing service more accurate or power a
    new feature, and customers want to have Cognitive Services availability in more
    Azure regions, working with more languages and giving them more options to customize
    models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速开发新产品的过程中会发生这种碎片化，但如果您希望在增长过程中保持质量和效率，这种情况是不可持续的。在 AI 领域总是有新的研究可以使现有服务更加精确，或者为新功能提供动力，客户希望在更多
    Azure 区域中可用认知服务，支持更多语言，并提供更多定制模型的选项。
- en: To carry on scaling for more customers, with new services coming from research,
    as well as updates and operational maintenance to meet the Azure SLAs and deliver
    on certification, compliance, and data regulation obligations, the team created
    a single platform for Cognitive Services to run on, with the architecture you
    can see in [Figure 9-1](#running_on_a_single_platform_makes_cogn). There’s also
    a single 24-7 support team with a rotating “Directly Responsible Individual” role;^([1](ch09.xhtml#ch01fn8))
    someone who proactively looks at logs, responds to incidents, does root cause
    analysis, and assigns defects across all the services, so the demanding support
    role can be shared between more people. That level of operational commitment is
    hard for even large enterprises to achieve when running their own AI systems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足更多客户的需求，不断推出新服务，以及更新和运营维护以满足 Azure SLA，并履行认证、合规性和数据监管义务，团队创建了一个单一平台用于运行认知服务，其架构可以在
    [图 9-1](#running_on_a_single_platform_makes_cogn) 中看到。此外还有一个全天候的售后支持团队，设有轮换的“直接负责人”角色；^([1](ch09.xhtml#ch01fn8))
    负责积极监控日志、响应事件、进行根本原因分析，并跨所有服务分配缺陷，以便将高要求的支持角色分摊给更多人。即使是大型企业运行自己的 AI 系统时，也很难达到这种运营承诺水平。
- en: '![Running on a single platform makes Cognitive Services easier to update, operate,
    and scale; this is the architecture for Cognitive Services in a single Azure region](Images/aasc_0901.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![在单一平台上运行使得认知服务更易于更新、运营和扩展；这是在单个 Azure 区域中认知服务的架构](Images/aasc_0901.png)'
- en: Figure 9-1\. Running on a single platform makes Cognitive Services easier to
    update, operate, and scale; this is the architecture for Cognitive Services in
    a single Azure region
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1\. 在单一平台上运行使认知服务更易于更新、运营和扩展；这是在单个 Azure 区域中认知服务的架构
- en: Building a more turnkey infrastructure makes Cognitive Services suitable for
    the “air-gapped” clouds required by some government contracts—where the operations
    team might not be experts in machine learning—but it also has other benefits.
    The completely automated deployment required for that means fewer errors in manual
    deployment anywhere the services are running, and faster disaster recovery. If
    an entire cluster needs to be rebuilt because something goes wrong, clicking a
    button runs the rebuild and sends traffic to the new cluster as soon as it’s available.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 构建更加即插即用的基础设施使认知服务适用于一些政府合同所需的“空隙”云环境——在这些环境中，运维团队可能并非机器学习专家——但这也带来了其他好处。这种完全自动化的部署意味着服务运行的任何地方手动部署错误更少，灾难恢复更快。如果整个集群需要重建，只需点击按钮即可运行重建操作，并在新集群可用时将流量发送至新集群。
- en: Similarly, because those air-gapped deployments were on the same platform, features
    they required like private endpoints could be developed once and made available
    for the public cloud Cognitive Services too.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，因为这些空隙部署在同一平台上，它们所需的私有端点等功能可以开发一次，然后为公共云认知服务提供。
- en: Using deep learning for all of the Cognitive Services models makes it straightforward
    to build custom models, where the final output layer of a trained deep neural
    network is replaced by one trained on much more specific data. That means you
    can take advantage of a model trained on a large dataset—which takes time—and
    quickly tweak it to handle your specific problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习来构建所有认知服务模型，使得构建定制模型变得简单，其中一个经过训练的深度神经网络的最终输出层被替换为针对更具体数据进行训练的输出层。这意味着您可以利用经过大型数据集训练的模型（这需要时间），并快速调整它以处理您的特定问题。
- en: The Cognitive Services team also includes a group of researchers, known internally
    as Loonshots after the book about turning “crazy ideas” into successful projects,
    that shepherds new algorithms, models, and approaches from Microsoft Research—whose
    remit is to look two to three years ahead—into services that can be delivered
    within a year, all on the same platform.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 认知服务团队还包括一组研究人员，他们被称为“疯狂想法”变成成功项目的书籍中所提到的“Loonshots”内部群体，负责从 Microsoft Research（其使命是展望两三年）引入新的算法、模型和方法，这些服务可以在一年内交付到同一平台上。
- en: Clusters and Containers
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群和容器
- en: The architecture of the Cognitive Services platform uses containers running
    on Kubernetes (although it uses the same backend as the Azure Kubernetes Service,
    it’s a separate deployment).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 认知服务平台的架构使用在 Kubernetes 上运行的容器（尽管它使用与 Azure Kubernetes 服务相同的后端，但它是一个独立的部署）。
- en: There are multiple clusters in each Azure region, each with its own VM agent
    pools; some services share clusters, while more demanding services like speech
    run on dedicated clusters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Azure 区域中都有多个集群，每个集群都有自己的 VM 代理池；一些服务共享集群，而像语音这样的需求更高的服务则在专用集群上运行。
- en: The containers that run in Azure for Cognitive Services aren’t the same containers
    that you can run at the edge to host specific Cognitive Services, because Microsoft
    takes advantage of the orchestrator it’s built in the Cognitive Services platform
    to allocate different containers to different VM SKUs that might have GPUs and
    fast local storage or just CPUs. The different Cognitive Services are broken up
    into many microservices, and metadata defines what hardware and software requirements
    each of them needs, so the containers can be provisioned onto the appropriate
    infrastructure in the right agent pool. The container for one microservice might
    run on a low-power CPU, while another will be on a VM with a powerful GPU.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 中运行的认知服务容器与您可以在边缘运行的用于托管特定认知服务的容器并不相同，因为 Microsoft 利用其在认知服务平台中构建的编排器来为不同的
    VM SKU 分配不同的容器，这些 SKU 可能具有 GPU 和快速本地存储或仅具有 CPU。不同的认知服务被分解为许多微服务，元数据定义了每个服务所需的硬件和软件要求，因此容器可以部署到适当的基础架构上的正确代理池中。一个微服务的容器可能在低功耗
    CPU 上运行，而另一个则在配备强大 GPU 的 VM 上运行。
- en: The orchestrator can send a request to one container to do, say, language detection
    as part of the text analytics service, and then forward that on to other containers
    for further processing. That fits with the way developers use multiple Cognitive
    Services together in scenarios like call center handling, where one cloud request
    can call multiple services, like speech, text analytics, language understanding,
    translation, and text-to-speech.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 编排器可以向一个容器发送请求，例如作为文本分析服务的一部分进行语言检测，然后将其转发到其他容器进行进一步处理。这符合开发人员在诸如呼叫中心处理等场景中同时使用多个认知服务的方式，其中一个云请求可以调用多个服务，如语音、文本分析、语言理解、翻译和文本转语音。
- en: 'It’s also how Applied AI Services like Forms Recognizer are composed: the orchestrator
    decomposes the customer request into multiple API calls to individual Cognitive
    Services behind the scenes and then merges them back together. That makes it easier
    to build and launch new Applied AI Services, because composing them is just metadata
    that the orchestrator can use.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 AI 服务（如表单识别器）也是如何组成的：编排器将客户请求分解为多个认知服务的后台 API 调用，然后将它们合并在一起。这使得构建和推出新的应用
    AI 服务变得更容易，因为它们的组合只是编排器可以使用的元数据。
- en: The orchestrator also requires more standardization across the individual Cognitive
    Services APIs so the calls developers make to different APIs are more closely
    aligned. That means once you’re familiar with one API, it’s easier to understand
    the structure of new APIs as you start using them.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 编排器还需要跨个别认知服务 API 进行更多的标准化，以便开发人员调用不同 API 时更加一致。这意味着一旦您熟悉了一个 API，开始使用新 API 时就更容易理解其结构。
- en: Using containers means fewer Linux VMs are needed to host Cognitive Services
    than if each service ran in its own VM. It’s also faster and more efficient to
    scale.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用容器意味着托管认知服务所需的 Linux 虚拟机数量较少，而不是每个服务都在自己的虚拟机中运行。这样做不仅更快速、更高效地进行扩展，而且还能节省成本。
- en: Because of the time it takes to spin up an entire VM to handle increased traffic,
    a VM-based service will typically run with extra capacity as a buffer so customer
    requests are never dropped. Even running on Kubernetes, Cognitive Services still
    needs a certain amount of buffer capacity because, while it’s fast to start up
    a new container, it still takes time to copy a large machine learning model into
    a new container. But that buffer can be even smaller because, rather than deploying
    the model into a new container, it can be kept outside and just attached once
    the container has scaled up.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于启动整个虚拟机以处理增加的流量需要时间，基于虚拟机的服务通常会多运行一些额外的容量作为缓冲，以确保不会丢失客户请求。即使在 Kubernetes 上运行，认知服务仍然需要一定的缓冲容量，因为虽然启动新容器速度很快，但将大型机器学习模型复制到新容器中仍需要时间。但是这种缓冲可以更小，因为不必将模型部署到新容器中，而是可以在容器扩展后再附加。
- en: Running Cognitive Services for so many customers means Microsoft gets a lot
    of telemetry that can be used to make running them more efficient. As different
    Azure VM SKUs become available or the prices of SKUs change, they can look at
    which VM to run a particular container on. Not all SKUs are available in all Azure
    regions; so while running on a VM with a GPU might give better performance for
    vision models, they also have to be able to run on CPU-only SKUs so they can be
    deployed in more regions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为如此多的客户运行认知服务意味着微软可以获得大量的遥测数据，这些数据可以用来提高服务的运行效率。随着不同的 Azure VM SKU 变得可用或 SKU
    的价格发生变化，他们可以确定在哪种 VM 上运行特定的容器。并非所有 SKU 都在所有 Azure 区域可用；因此，虽然在带 GPU 的 VM 上运行可能会为视觉模型带来更好的性能，但也必须能够在仅有
    CPU 的 SKU 上运行，以便能够在更多的区域部署。
- en: That might mean going back not just to the developers who built that Cognitive
    Service but to the researchers and data scientists who created the model and to
    framework teams like ONNX to make sure the models and frameworks run on the variety
    of hardware in Azure. As new hardware becomes available, they can run performance
    and validation tests, spin up a new agent pool in the Kubernetes cluster, redeploy
    the service to that, and spin down the existing agent pool.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能意味着不仅需要与构建该认知服务的开发人员联系，还需要与创建模型的研究人员和数据科学家以及像 ONNX 这样的框架团队联系，以确保模型和框架能够在
    Azure 的各种硬件上运行。随着新硬件的推出，他们可以进行性能和验证测试，启动 Kubernetes 集群中的新代理池，重新部署服务，并关闭现有的代理池。
- en: Making Cognitive Services not only scale but get cheaper and cheaper to run
    is key to making AI available to everyone. When the Edge browser added the ability
    to automatically translate web pages into a different language as they load, which
    you can see in action in [Figure 9-2](#putting_the_translator_service_inside_t),
    it quickly became the biggest user of the Translator service—and the bill for
    running it went through the roof. Finding ways to bring the cost down by optimizing
    Translator to run on less powerful hardware in Azure meant the service could stay
    available in Edge and keep adding more languages.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使认知服务不仅能够扩展，而且能够运行成本越来越低是使人工智能普及的关键。Edge 浏览器增加了在加载网页时自动将网页翻译成不同语言的功能，您可以在[图 9-2](#putting_the_translator_service_inside_t)中看到其效果，这迅速使其成为
    Translator 服务的最大用户，因此运行费用也随之飙升。通过优化 Translator 在 Azure 上运行于更低性能硬件上的方式，使得服务能够继续在
    Edge 上提供，并且能够添加更多语言。
- en: '![Putting the Translator service inside the Edge browser means Cognitive Services
    has to scale up to serve millions of users without breaking the bank](Images/aasc_0902.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![将 Translator 服务放入 Edge 浏览器意味着认知服务必须进行扩展，以服务数百万用户而不会让成本失控](Images/aasc_0902.png)'
- en: Figure 9-2\. Putting the Translator service inside the Edge browser means Cognitive
    Services has to scale up to serve millions of users without breaking the bank
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. 将 Translator 服务放入 Edge 浏览器意味着认知服务必须进行扩展，以服务数百万用户而不会让成本失控
- en: In this chapter we’ve looked at how cloud AI services are built, so you can
    be confident in relying on them for scale, as well as the trade-offs if you need
    to run any of the individual Cognitive Services in containers on your own infrastructure.
    But what does it look like to use Cognitive Services at scale in your own applications?
    In the next chapters we’re going to look at some real-world examples of solving
    problems on mobile devices, at the edge, and in the cloud as examples of what
    you can build with the help of those AI services.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们已经了解了云AI服务的构建方式，因此您可以放心依赖它们来实现规模化，以及如果需要在自己的基础设施中运行任何单个认知服务时的权衡。但是，在您自己的应用程序中规模化使用认知服务是什么样子呢？在接下来的章节中，我们将看一些真实世界的例子，解决在移动设备、边缘和云中的问题，这些例子展示了借助这些AI服务您可以构建的内容。
- en: ^([1](ch09.xhtml#ch01fn8-marker)) Learn more about [DRI](https://go.microsoft.com/fwlink/?linkid=2190185).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.xhtml#ch01fn8-marker)) 了解更多关于[DRI](https://go.microsoft.com/fwlink/?linkid=2190185)的信息。
