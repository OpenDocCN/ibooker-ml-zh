- en: Chapter 5\. Train and Evaluate Your Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。训练和评估你的模型
- en: In the previous chapters we’ve covered how to identify the right problem to
    tackle, make a plan to tackle it, build a simple pipeline, explore a dataset,
    and generate an initial set of features. These steps have allowed us to gather
    enough information to begin training an adequate model. An adequate model here
    means a model that is a good fit for the task at hand and that has good chances
    of performing well.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们已经介绍了如何识别要解决的正确问题，制定解决方案的计划，构建一个简单的流水线，探索数据集，并生成一组初始特征。这些步骤已经为我们收集到足够的信息，以开始训练一个适当的模型。这里的适当模型意味着适合当前任务并有良好表现机会的模型。
- en: In this chapter, we will start by briefly going over some concerns when choosing
    a model. Then, we will describe best practices to separate your data, which will
    help evaluate your models in realistic conditions. Finally, we’ll look at methods
    to analyze modeling results and diagnose errors.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先简要讨论选择模型时的一些注意事项。然后，我们将描述最佳实践来分离你的数据，这将有助于在真实条件下评估你的模型。最后，我们将探讨分析建模结果和诊断错误的方法。
- en: The Simplest Appropriate Model
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最简单合适的模型
- en: Now that we are ready to train a model, we need to decide which model to start
    with. It may be tempting to try every possible model, benchmark them all, and
    pick the one with the best results on a held-out test set according to some metrics.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备训练模型了，我们需要决定从哪个模型开始。或许尝试每个可能的模型、对它们进行基准测试，然后根据一些指标在一个保留的测试集上选择表现最好的那个模型，这看起来很诱人。
- en: In general, this is not the best approach. Not only is it computationally intensive
    (there are many sets of models and many parameters for each model, so realistically
    you will only be able to test a suboptimal subset), it also treats models as predictive
    black boxes and entirely ignores that *ML models encode implicit assumptions about
    the data in the way they learn*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这并不是最佳方法。它不仅计算密集（有许多模型集和每个模型的许多参数，所以实际上你只能测试一个次优子集），而且将模型视为预测性黑匣子，并完全忽略了*机器学习模型在学习方式中对数据的隐含假设*。
- en: Different models make different assumptions about the data and so are suited
    for different tasks. In addition, since ML is an iterative field, you’ll want
    to pick models that you can build and evaluate quickly.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的模型对数据有不同的假设，因此适合不同的任务。此外，由于机器学习是一个迭代的领域，你会想要选择可以快速构建和评估的模型。
- en: Let’s first define how to identify simple models. Then, we will cover some examples
    of data patterns and appropriate models to leverage them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义如何识别简单模型。然后，我们将涵盖一些数据模式的示例和适合利用它们的模型。
- en: Simple Models
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单模型
- en: 'A simple model should be quick to implement, understandable, and deployable:
    quick to implement because your first model will likely not be your last, understandable
    because it will allow you to debug it more easily, and deployable because that
    is a fundamental requirement for an ML-powered application. Let’s start by exploring
    what I mean by quick to implement.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的模型应该快速实现，易于理解和可部署：快速实现是因为你的第一个模型可能不是最后一个，易于理解是因为这将让你更容易调试它，可部署是因为这是一个基本要求，适用于基于机器学习的应用程序。让我们首先探索我所说的快速实现是什么意思。
- en: Quick to implement
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 快速实现
- en: Choose a model that will be simple for you to implement. Generally, this means
    picking a well-understood model that has multiple tutorials written about it and
    that people will be able to help you with (especially if you ask well-formulated
    questions using our ML Editor!). For a new ML-driven application, you will have
    enough challenges to tackle in terms of processing data and deploying a reliable
    result that you should initially do your best to avoid all model headaches.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个对你来说实现简单的模型。通常，这意味着选择一个大家对其有很多教程并且愿意帮助你的广为人知的模型（特别是如果你能用我们的 ML 编辑器提出形式良好的问题的话！）。对于一个新的基于机器学习的应用程序来说，在处理数据和部署可靠结果方面已经有足够的挑战，所以你最初应尽量避免所有模型问题。
- en: If possible, start by using models from popular libraries such as Keras or scikit-learn,
    and hold off before diving into an experimental GitHub repository that has no
    documentation and hasn’t been updated in the last nine months.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，首先使用来自流行库如 Keras 或 scikit-learn 的模型，然后在深入研究那些没有文档且在过去九个月内没有更新的实验性 GitHub
    存储库之前保留观望。
- en: Once your model is implemented, you’ll want to inspect and understand how it
    is leveraging your dataset. To do so, you need a model that is understandable.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的模型实施完成，您将希望检查并理解它如何利用您的数据集。为此，您需要一个可以理解的模型。
- en: Understandable
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可理解的
- en: Model *explainability* and *interpretability* describe the ability for a model
    to expose reasons (such as a given combination of predictors) that caused it to
    make predictions. Explainability can be useful for a variety of reasons, such
    as verifying that our models are not biased in undesirable ways or explaining
    to a user what they could do to improve prediction results. It also makes iterating
    and debugging much easier.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的*可解释性*和*解释性*描述了模型暴露出导致其进行预测的原因的能力（例如给定的预测器组合）。解释性对于多种原因非常有用，例如验证我们的模型没有以不希望的方式偏见，或向用户解释他们可以做什么来改善预测结果。它还使迭代和调试变得更加容易。
- en: If you can extract the features a model relies on to make decisions, you’ll
    have a clearer view of which features to add, tweak, or remove, or which model
    could make better choices.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您可以提取模型依赖于做出决策的特征，您将更清楚地了解哪些特征可以添加、调整或删除，或者哪个模型可以做出更好的选择。
- en: Unfortunately, model interpretability is often complex even for simple models
    and sometimes intractable for larger ones. In [“Evaluate Feature Importance”](#eval_feat_imp_sect),
    we will see ways to tackle this challenge and help you identify points of improvement
    for your model. Among other things, we will use black-box explainers that attempt
    to provide explanations of a model’s prediction regardless of its internal workings.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，即使对于简单模型，模型可解释性通常也很复杂，对于较大的模型有时甚至无法解决。在[“评估特征重要性”](#eval_feat_imp_sect)中，我们将看到解决这一挑战的方法，帮助您识别模型改进的要点。除其他事项外，我们将使用黑盒解释器，试图提供模型预测的解释，而不考虑其内部运作方式。
- en: Simpler models such as logistic regression or decision trees tend to be easier
    to explain as they provide some measure of feature importance, which is another
    reason they are usually good models to try first.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 简单模型如逻辑回归或决策树往往更容易解释，因为它们提供了某种程度的特征重要性，这也是它们通常是首选的模型之一的原因。
- en: Deployable
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可部署的
- en: As a reminder, the end goal of your model is to provide a valuable service to
    people who will use it. This means when you think of which model to train, you
    should always consider whether you will be able to deploy it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，您模型的最终目标是为将要使用它的人们提供有价值的服务。这意味着当您考虑要训练哪种模型时，您应始终考虑是否能够部署它。
- en: 'We will cover deployment in [Part IV](part04.html#section_4), but you should
    already be thinking about questions such as the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第四部分](part04.html#section_4)介绍部署，但您应该已经在考虑以下问题：
- en: How long will it take a trained model to make a prediction for a user? When
    thinking of prediction latency, you should include not only the time it takes
    for a model to output a result, but the delay in between when a user submits a
    prediction request and receives the result. This includes any preprocessing steps
    such as feature generation, any network calls, and any postprocessing steps that
    happen in between a model’s output and the data that is presented to a user.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型为用户做出预测需要多长时间？在考虑预测延迟时，您应该包括不仅模型输出结果所需的时间，还包括用户提交预测请求和接收结果之间的延迟。这包括任何预处理步骤，如特征生成，任何网络调用，以及在模型输出和呈现给用户的数据之间发生的任何后处理步骤。
- en: Is this inference pipeline fast enough for our use case if we take into account
    the number of concurrent users we expect?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果考虑到我们预期的并发用户数量，这个推理管道是否足够快速？
- en: How long does it take to train the model, and how often do we need to train
    it? If training takes 12 hours and you need to retrain your model every 4 hours
    to be fresh, not only will your compute bill be quite expensive, but your model
    will always be out of date.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型需要多长时间，以及我们需要多频繁地进行训练？如果训练需要12小时，而您需要每4小时重新训练模型以保持新鲜，不仅计算成本会很昂贵，而且您的模型将永远处于过时状态。
- en: We can compare how simple models are by using a table such as [Figure 5-1](#simple_understandable_deployable).
    As the field of ML evolves and new tooling is built, models that may be complex
    to deploy or hard to interpret today may become simpler to use, and this table
    will need to be updated. For this reason, I suggest you build your own version
    based on your particular problem domain.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用诸如[图5-1](#simple_understandable_deployable)的表格来比较简单模型的情况。随着机器学习领域的发展和新工具的建立，今天可能复杂或难以解释的模型未来可能变得更容易使用，因此这个表格需要定期更新。基于这个原因，我建议您根据您特定的问题领域建立自己的版本。
- en: '![Scoring models based on their simplicity](assets/bmla_0501.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![基于简易性评分模型](assets/bmla_0501.png)'
- en: Figure 5-1\. Scoring models based on their simplicity
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 基于简易性评分模型
- en: Even among models that are simple, interpretable, and deployable, there are
    still many potential candidates. To choose a model, you should also take into
    account the patterns you identified in [Chapter 4](ch04.html#initial_dataset).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在简单、可解释和可部署的模型中，仍然有许多潜在的候选项。为了选择一个模型，您还应考虑在[第四章](ch04.html#initial_dataset)中识别出的模式。
- en: From Patterns to Models
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从模式到模型
- en: The patterns we have identified and the features we have generated should guide
    our model choice. Let’s cover a few examples of patterns in the data and appropriate
    models to leverage them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经识别出的模式和我们生成的特征应该指导我们选择模型。让我们来看几个数据中的模式和适当的模型示例，以利用它们。
- en: We want to ignore feature scale
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们希望忽略特征尺度
- en: Many models will leverage larger features more heavily than smaller ones. This
    can be fine in some cases, but undesirable in others. For models using optimization
    procedures like gradient descent such as neural networks, differences in feature
    scale can sometimes lead to instability in the training procedure.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型会更加重视较大的特征而忽略较小的特征。在某些情况下这是可以接受的，但在其他情况下可能不合适。对于使用梯度下降等优化程序的模型（如神经网络），特征尺度的差异有时会导致训练过程的不稳定性。
- en: If you want to use both age in years (ranging from one to a hundred) and income
    in dollars (let’s say our data reaches up to nine figures) as two predictors,
    you need to make sure that your model is able to leverage the most predictive
    features, regardless of their scale.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望同时使用年龄（从一到一百岁）和收入（假设我们的数据高达九位数）作为两个预测变量，您需要确保您的模型能够利用最具预测性的特征，而不受其尺度影响。
- en: You can ensure this by preprocessing features to normalize their scale to have
    zero mean and unit variance. If all features are normalized to the same range,
    a model will consider each of them equally (at least initially).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过预处理特征以使其标准化到零均值和单位方差，可以确保这一点。如果所有特征都被归一化到相同的范围，模型将最初视每个特征为平等重要。
- en: Another solution is to turn to models that are not affected by differences in
    feature scale. The most common practical examples are decision trees, random forests,
    and gradient-boosted decision trees. [XGBoost](https://oreil.ly/CWpnk) is an implementation
    of gradient-boosted trees commonly used in production because of its robustness,
    as well as its speed.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决方案是使用不受特征尺度差异影响的模型。最常见的实际例子是决策树、随机森林和梯度提升决策树。[XGBoost](https://oreil.ly/CWpnk)
    是一种常用的梯度提升树实现，在生产环境中因其稳健性和速度而广泛使用。
- en: Our predicted variable is a linear combination of predictors
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的预测变量是预测因子的线性组合
- en: Sometimes, there is good reason to believe that we can make good predictions
    using only a linear combination of our features. In these cases, we should use
    a linear model such as a linear regression for continuous problems or a logistic
    regression or naive Bayes classifier for classification problems.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们有充分理由相信，只使用特征的线性组合就可以做出良好的预测。在这些情况下，我们应该使用像线性回归这样的线性模型来处理连续性问题，或者像逻辑回归或朴素贝叶斯分类器这样的模型来解决分类问题。
- en: These models are simple, efficient, and often allow for a direct interpretation
    of their weights that can help us identify important features. If we believe the
    relationships between our features and our predicted variable are more complex,
    using a nonlinear model such as a multilayer neural network or generating feature
    crosses (see the beginning of [“Let Data Inform Features and Models”](ch04.html#feature_models))
    can help.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型简单高效，并且通常允许直接解释其权重，有助于我们识别重要特征。如果我们认为特征与预测变量之间的关系更复杂，可以使用非线性模型，如多层神经网络或生成特征交叉（见[“让数据驱动特征和模型”](ch04.html#feature_models)的开始）。
- en: Our data has a temporal aspect
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的数据具有时间性
- en: If we are dealing with time series of data points where the value at a given
    time depends on previous values, we would want to leverage models that explicitly
    encode this information. Examples of such models include statistical models such
    as autoregressive integrated moving average (ARIMA) or recurrent neural networks
    (RNN).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们处理的是数据点的时间序列，其中给定时间点的值依赖于先前的值，我们将希望利用能够显式编码此信息的模型。此类模型的示例包括统计模型，如自回归积分移动平均（ARIMA）或递归神经网络（RNN）。
- en: Each data point is a combination of patterns
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每个数据点都是模式的组合
- en: When tackling problems in the image domain, for example, convolutional neural
    networks (CNNs) have proven useful through their ability to learn *translation-invariant
    filters*. This means that they are able to extract local patterns in an image
    regardless of their position. Once a CNN learns how to detect an eye, it can detect
    it anywhere in an image, not just in the places that it appeared in the training
    set.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理图像域中的问题时，例如卷积神经网络（CNN）通过其学习*平移不变滤波器*的能力已被证明是有用的。这意味着它们能够提取图像中的局部模式，而不考虑它们的位置。一旦CNN学会了如何检测眼睛，它就可以在图像的任何位置检测到它，而不仅仅是在训练集中出现的位置。
- en: Convolutional filters have proven useful in other fields that contain local
    patterns, such as speech recognition or text classification, where CNNs have been
    used successfully for sentence classification. For an example, see the implementation
    by Yoon Kim in the paper, [“Convolutional Neural Networks for Sentence Classification”](https://arxiv.org/abs/1408.5882).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积滤波器已被证明在其他包含局部模式的领域中非常有用，例如语音识别或文本分类，CNN已成功用于句子分类。例如，可以参考Yoon Kim在论文中的实现，[“用于句子分类的卷积神经网络”](https://arxiv.org/abs/1408.5882)。
- en: There are many additional points to consider when thinking of the right models
    to use. For most classical ML problems, I recommend using this handy [flowchart](https://oreil.ly/tUsD6)
    that the scikit-learn team helpfully provides. It provides model suggestions for
    many common use cases.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑使用正确的模型时，还有许多其他要考虑的因素。对于大多数经典机器学习问题，我建议使用这个便捷的[流程图](https://oreil.ly/tUsD6)，这是scikit-learn团队提供的有用参考。它为许多常见用例提供了模型建议。
- en: ML Editor model
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ML编辑器模型
- en: For the ML Editor, we would like our first model to be fast and reasonably easy
    to debug. In addition, our data consists of individual examples, without a need
    to consider a temporal aspect (such as a series of questions, for example). For
    that reason, we will start with a popular and resilient baseline, a random forest
    classifier.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ML编辑器，我们希望我们的第一个模型快速且相对容易调试。此外，我们的数据由个别示例组成，无需考虑时间方面的因素（例如一系列问题）。因此，我们将从一个流行且有韧性的基准开始，即随机森林分类器。
- en: Once you’ve identified a model that seems reasonable, it is time to train it.
    As a general guideline, you should not train your model on the entirety of the
    dataset you gathered in [Chapter 4](ch04.html#initial_dataset). You’ll want to
    start by holding out some data from your training set. Let’s cover why and how
    you should do that.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您确定了一个看起来合理的模型，就该是训练它的时候了。作为一般指导方针，您不应该在您在[第四章](ch04.html#initial_dataset)中收集的整个数据集上训练模型。您需要从训练集中保留一些数据。让我们来看看为什么以及如何做到这一点。
- en: Split Your Dataset
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割您的数据集
- en: The main goal of our model is to provide valid predictions for data that our
    users will submit. This means that our model will eventually have to perform well
    on data that it has *never seen before*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的主要目标是为我们的用户提交的数据提供有效的预测。这意味着我们的模型最终将必须在它*以前从未见过的数据*上表现良好。
- en: When you train a model on a dataset, measuring its performance on the same dataset
    only tells you how good it is at making predictions on data it has already seen.
    If you only train a model on a subset of your data, you can then use the data
    the model was not trained on to estimate how well it would perform on unseen data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当您在数据集上训练模型时，仅在相同数据集上测量其性能只能告诉您它在已经看过的数据上进行预测的能力有多好。如果您仅在数据的子集上训练模型，然后可以使用模型未经训练的数据来估计其在未见数据上的表现。
- en: In [Figure 5-2](#train_val_test), you can see an example of a split into three
    separate sets (train, validation, and test) based on an attribute of our dataset
    (the author of a question). In this chapter, we will cover what each of these
    sets means, and how to think about them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图5-2](#train_val_test)中，您可以看到一个基于数据集的属性（问题的作者）将数据集分割成三个单独的集合（训练集、验证集和测试集）的示例。在本章中，我们将介绍每个集合的含义以及如何考虑它们。
- en: '![An example of a dataset split](assets/bmla_0502A.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![数据集分割示例](assets/bmla_0502A.png)'
- en: Figure 5-2\. Splitting data on author while attributing the right proportion
    of questions to each split
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. 在保留作者数据的同时将问题正确分配到每个拆分的示例
- en: The first held-out set to consider is the validation set.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要考虑的第一个保留集是验证集。
- en: Validation set
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证集
- en: To estimate how our model performs on unseen data, we purposefully hold out
    part of our dataset from training and then use the performance on this held-out
    dataset as a proxy for our model’s performance in production. The held-out set
    allows us to validate that our model can generalize to unseen data and thus is
    often called a *validation set*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计我们的模型在未见数据上的表现，我们有意地从训练集中保留部分数据集，然后使用这个保留的数据集的性能作为我们模型在生产中表现的代理。保留的集合允许我们验证我们的模型能够推广到未见数据，因此通常称为*验证集*。
- en: You can choose different sections of your data to hold out as a validation set
    to evaluate your model and train it on the remaining data. Doing multiple rounds
    of this process helps control for any variance due to a particular choice of validation
    set and is called *cross-validation*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择数据的不同部分作为验证集来评估您的模型，并在剩余数据上训练它。执行此过程的多轮有助于控制由于特定验证集选择而引起的任何方差，这称为*交叉验证*。
- en: As you change your data preprocessing strategy and the type of model you use
    or its hyperparameters, your model’s performance on the validation set will change
    (and ideally improve). Using the validation set allows you to tune hyperparameters
    the same way that using a training set allows a model to tune its parameters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当您更改数据预处理策略、使用的模型类型或其超参数时，您的模型在验证集上的表现将会改变（并且理想情况下会改善）。使用验证集允许您像使用训练集调整模型参数一样调整超参数。
- en: After multiple iterations of using the validation set to make model adjustments,
    your modeling pipeline can become tailored specifically to performing well on
    your validation data. This defeats the purpose of the validation set, which is
    supposed to be a proxy for unseen data. For this reason, you should hold out an
    additional test set.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在多次迭代使用验证集进行模型调整后，您的建模流程可能会专门针对在验证数据上表现良好。这违背了验证集的目的，即应该是未见数据的代理。因此，您应该保留一个额外的测试集。
- en: Test set
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试集
- en: Since we will go through multiple cycles of iteration on our model and measure
    its performance on a validation set at each cycle, we may bias our model so that
    it performs well on the validation set. This helps our model generalize beyond
    the training set but also carries the risk of simply learning a model that performs
    well only on our particular validation sets. Ideally, we would want to have a
    model that works well on new data that is hence not contained in the validation
    set.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将在模型上进行多次迭代循环，并在每个周期中测量其在验证集上的表现，所以我们可能会偏向于使模型在验证集上表现良好。这有助于我们的模型推广超出训练集，但也存在仅仅学习在特定验证集上表现良好的模型的风险。理想情况下，我们希望拥有一个在新数据上表现良好的模型，因此不包含在验证集中。
- en: For this reason, we usually hold out a third set called a *test set*, which
    serves as a final benchmark of our performance on unseen data, once we are satisfied
    with our iterations. While using a test set is a best practice, practitioners
    sometimes use the validation set as a test set. This increases the risk of biasing
    a model toward the validation set but can be appropriate when running only a few
    experiments.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通常我们会保留第三个称为*测试集*的数据集，这将作为我们在满意迭代后对未见数据性能的最终基准。虽然使用测试集是最佳实践，但从业者有时会将验证集作为测试集使用。这增加了使模型偏向验证集的风险，但在仅运行少量实验时可能是合适的。
- en: It is important to avoid using performance on the test set to inform modeling
    decisions, as this set is supposed to represent the unseen data we will face in
    production. Adapting a modeling approach to perform well on the test set risks
    leading to overestimating the performance of the model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 避免使用测试集上的表现来指导建模决策非常重要，因为这个数据集应该代表我们在生产中将面对的未见数据。调整建模方法以在测试集上表现良好会增加高估模型性能的风险。
- en: To have a model that performs in production, the data you train on should resemble
    data produced by users who will interact with your product. Ideally, any kind
    of data you could receive from users should be represented in your dataset. If
    that is not the case, then keep in mind that your test set performance is indicative
    of performance for only a subset of your users.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要让模型在生产中表现良好，你训练的数据应该与将与产品互动的用户生成的数据相似。理想情况下，你从用户那里接收到的任何数据类型都应该在你的数据集中有所体现。如果不是这样的话，那么请记住，你的测试集表现只能反映出部分用户的表现。
- en: For the ML Editor, this means that users who do not conform to the demographics
    of *writers.stackoverflow.com* may not be as well served by our recommendations.
    If we wanted to address this problem, we should expand the dataset to contain
    questions more representative of these users. We could start by incorporating
    questions from other Stack Exchange websites to cover a broader set of topics,
    or different question and answering websites altogether.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ML 编辑器来说，这意味着不符合*writers.stackoverflow.com*的人群可能无法得到我们的推荐服务。如果我们想要解决这个问题，我们应该扩展数据集，包含更多符合这些用户的问题。我们可以从其他
    Stack Exchange 网站收录问题，以涵盖更广泛的主题，或者选择完全不同的问答网站。
- en: Correcting a dataset in such a manner can be challenging for a side project.
    When building consumer-grade products, however, it is necessary to help model
    weaknesses be caught early before users are exposed to them. Many of the failure
    modes we will cover in [Chapter 8](ch08.html#final_validation) could have been
    avoided with a more representative dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个旁边的项目来说，对数据集进行这样的修正可能具有挑战性。然而，在构建消费级产品时，有必要在用户接触到之前帮助模型的弱点被早期捕捉。我们将在[第8章](ch08.html#final_validation)中涵盖的许多失败模式可以通过更具代表性的数据集来避免。
- en: Relative proportions
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相对比例
- en: In general, you should maximize the amount of data the model can use to learn
    from, while holding out large enough validation and test sets to provide accurate
    performance metrics. Practitioners often use 70% of the data for training, 20%
    for validation, and 10% for testing, but this depends entirely on the quantity
    of data. For very large datasets, you can afford to use a larger proportion of
    data for training while still having enough data to validate models. For smaller
    datasets, you may need to use a smaller proportion for training in order to have
    a validation set that is large enough to provide an accurate performance measurement.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，你应该最大化模型可以学习的数据量，同时保留足够大的验证和测试集来提供准确的性能指标。实践者通常使用数据的70%进行训练，20%进行验证，10%进行测试，但这完全取决于数据的量。对于非常大的数据集，你可以负担得起使用更大比例的数据进行训练，同时仍有足够的数据来验证模型。对于较小的数据集，你可能需要使用较小比例的数据进行训练，以便拥有足够大的验证集来提供准确的性能测量。
- en: Now you know why you’d want to split data, and which splits to consider, but
    how should you decide which datapoint goes in each split? The splitting methods
    you use have a significant impact on modeling performance and should depend on
    the particular features of your dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道为什么要分割数据，以及应该考虑哪些分割方法，但是你应该如何决定哪个数据点放入每个分割中呢？你使用的分割方法对建模性能有重大影响，并且应该依赖于数据集的特定特征。
- en: Data leakage
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据泄露
- en: The method you use to separate your data is a crucial part of validation. You
    should aim to make your validation/test set close to what you expect unseen data
    to be like.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你用来分离数据的方法是验证的一个关键部分。你应该努力使你的验证/测试集与你预期的未见数据尽可能接近。
- en: Most often, train, validation, and test sets are separated by sampling data
    points randomly. In some cases, this can lead to *data leakage*. Data leakage
    happens when (because of our training procedure) a model receives information
    during training that it won’t have access to when being used in front of real
    users in production.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，训练集、验证集和测试集是通过随机抽样数据点来分离的。在某些情况下，这可能会导致*数据泄露*。数据泄露发生在（由于我们的训练过程）模型在训练期间接收到了在实际使用时不会访问到的信息。
- en: Data leakage should be avoided at all costs, because it leads to an inflated
    view of the performance of our model. A model trained on a dataset exhibiting
    data leakage is able to leverage information to make predictions that it will
    not have when it encounters different data. This makes the task artificially easier
    for the model, but only due to the leaked information. The model’s performance
    appears high on the held-out data but will be much worse in production.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄露必须尽量避免，因为它会导致我们模型性能的夸大看法。在数据集中训练的模型存在数据泄露时，可以利用信息进行预测，而当它遇到不同的数据时，这些信息是不可得的。这使得模型的任务在保留数据上看起来人为更容易，但这仅仅是由于泄露的信息。模型在保留数据上的性能表现很高，但在生产中会差得多。
- en: In [Figure 5-3](#data_leakage_viz), I’ve drawn a few common causes where randomly
    splitting your data into sets will cause data leakage. There are many potential
    causes of data leakage, and we will explore two frequent ones next.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 5-3](#data_leakage_viz)中，我画出了几种常见的情况，随机将数据分割为集合会导致数据泄露。数据泄露有许多潜在原因，接下来我们将探讨两种常见的情况。
- en: To start our exploration, let’s tackle the example at the top of [Figure 5-3](#data_leakage_viz),
    temporal data leakage. Then, we will move on to sample contamination, a category
    that encompasses the bottom two examples in [Figure 5-3](#data_leakage_viz).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始我们的探索，让我们先看看[图 5-3](#data_leakage_viz)顶部的例子，即时间数据泄露。接着，我们将转向样本污染，这是包含在[图 5-3](#data_leakage_viz)底部两个例子中的一类。
- en: '![Splitting data randomly can often lead to data leakage](assets/bmla_0502.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![随机分割数据通常会导致数据泄露](assets/bmla_0502.png)'
- en: Figure 5-3\. Splitting data randomly can often lead to data leakage
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 随机分割数据通常会导致数据泄露
- en: Temporal data leakage
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 时间数据泄露
- en: 'In time-series forecasting, a model needs to learn from data points in the
    past to predict events that have not happened yet. If we perform a random split
    on a forecasting dataset, we will introduce data leakage: a model that is trained
    on a random set of points and evaluated on the rest will have access to training
    data that happens *after* events it is trying to predict.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列预测中，模型需要从过去的数据点学习，以预测尚未发生的事件。如果我们对预测数据集进行随机拆分，我们将引入数据泄露：一个在随机点集上训练并在剩余点上评估的模型将可以访问在其试图预测的事件*之后*发生的训练数据。
- en: The model will perform artificially well on the validation and test sets but
    fail in production, because all it has learned is to leverage future information,
    which is unavailable in the real world.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在验证集和测试集上的表现会人为地非常好，但在生产环境中会失败，因为它所学到的都是利用未来信息，而在真实世界中这些信息是不可用的。
- en: Once you are aware of it, temporal data leakage is usually easy to catch. Other
    types of data leakage can give a model access to information it should not have
    during training and artificially inflate its performance by “contaminating” its
    training data. They can often be much harder to detect.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你意识到了它，时间数据泄露通常很容易被发现。其他类型的数据泄露可能会使模型在训练过程中获得不应该有的信息，并通过“污染”其训练数据来人为地提高其性能。它们通常更难检测到。
- en: Sample contamination
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 样本污染
- en: A common source of data leakage lies in the level at which the randomness occurs.
    When building a model to predict the grade students’ essays will receive, a data
    scientist I was assisting once found that his model performed close to perfect
    on a held-out test set.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据泄露的一个常见来源在于随机性发生的层次。当建立一个预测学生文章评分的模型时，一位我曾经协助的数据科学家发现他的模型在一个保留测试集上表现接近完美。
- en: On such a hard task, a model that performs so well should be closely examined
    as it frequently indicates the presence of a bug or *data leakage*. Some would
    say that the ML equivalent of Murphy’s law is that the more pleasantly surprised
    you are by the performance of your model on your test data, the more likely you
    are to have an error in your pipeline.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样一个艰巨的任务上，一个表现如此优秀的模型应该进行仔细检查，因为这通常表明管道中存在错误或*数据泄露*。有人说，机器学习中墨菲定律的等效物是，当你对模型在测试数据上的表现感到愉快惊讶时，你越有可能在流程中有错误。
- en: In this example, because most students had written multiple essays, splitting
    data randomly led to essays by the same students being present both in the training
    and in the test sets. This allowed the model to pick up on features that identified
    students and use that information to make accurate predictions (students in this
    dataset tended to have similar grades across all their essays).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，因为大多数学生写了多篇文章，随机分割数据导致了同一学生的文章同时出现在训练集和测试集中。这使得模型能够捕捉到识别学生的特征，并利用这些信息进行准确预测（这个数据集中的学生倾向于在所有文章中有类似的成绩）。
- en: If we were to deploy this essay score predictor for future use, it would not
    be able to predict useful scores for students it hadn’t seen before and would
    simply predict historical scores for students whose essays it has been trained
    on. This would not be useful at all.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要部署这个文章评分预测器以便将来使用，它将无法为它之前没有见过的学生预测有用的分数，并且只会为那些它已经训练过的学生预测历史分数。这将毫无用处。
- en: To solve the data leakage in this example, a new split was made at the student
    rather than the essay level. This meant that each student appeared either only
    in the training set or only in the validation set. Since the task became much
    harder, this led to a decrease in model accuracy. However, since the training
    task was now much closer to what it would be in production, this new model was
    much more valuable.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中解决数据泄露的方法是，在学生而不是文章级别进行新的分割。这意味着每个学生只出现在训练集或验证集中的其中一个。由于任务变得更加困难，这导致了模型准确性的降低。然而，由于训练任务现在更接近于生产环境，这个新模型变得更有价值。
- en: Sample contamination can happen in nuanced ways in common tasks. Let’s take
    the example of an apartment rental booking website. This website incorporates
    a click prediction model that, given a user query and an item, predicts whether
    the user will click on the item. This model is used to decide which listings to
    display to users.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在常见任务中，样本污染可能以微妙的方式发生。让我们以公寓租赁预订网站为例。该网站整合了点击预测模型，根据用户查询和项目，预测用户是否会点击项目。该模型用于决定向用户展示哪些房源。
- en: To train such a model, this website could use a dataset of user features such
    as their number of previous bookings, paired with apartments that were presented
    to them and whether they clicked on them. This data is usually stored in a production
    database that can be queried to produce such pairs. If engineers of this website
    were to simply query the database to build such a dataset, they would likely be
    faced with a case of data leakage. Can you see why?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练这样的模型，该网站可以使用用户特征数据集，例如他们的先前预订次数，与展示给他们的公寓以及他们是否点击了它们。这些数据通常存储在可以查询以生成这些配对的生产数据库中。如果该网站的工程师仅仅是查询数据库来构建这样的数据集，他们可能会面临数据泄露的情况。你能看出原因吗？
- en: In [Figure 5-4](#right_wrong_split), I’ve sketched out an illustration of what
    can go wrong by depicting a prediction for a specific user. At the top, you can
    see the features that a model could use in production to provide a click prediction.
    Here, a new user with no previous bookings is presented with a given apartment.
    At the bottom, you can see the state of the features a few days later when engineers
    extract data from the database.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图5-4](#right_wrong_split)中，我勾画了一个针对特定用户的预测的插图，说明了可能出错的情况。在顶部，你可以看到模型在生产中可能使用的特征。在这里，一个没有先前预订的新用户被呈现给一个特定的公寓。在底部，你可以看到几天后工程师从数据库中提取数据时特征的状态。
- en: '![Data leakage can happen in subtle ways such as due to a lack of data versioning](assets/bmla_0503A.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![数据泄露可能以数据版本不一致为由，发生在微妙的方式](assets/bmla_0503A.png)'
- en: Figure 5-4\. Data leakage can happen for subtle reasons, such as due to a lack
    of data versioning
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4。数据泄露可能出现在微妙的原因，比如由于缺乏数据版本控制。
- en: Notice the difference in `previous_bookings`, which is due to user activity
    that happened after they were initially presented with the listing. By using a
    snapshot of a database, information about future actions of the user was leaked
    into the training set. We now know that the user will eventually book five apartments!
    Such leakage can lead a model trained with information at the bottom to output
    a correct prediction on the incorrect training data. The accuracy of the model
    on the generated dataset will be high because it is leveraging data it will not
    have access to in production. When the model is deployed, it will perform worse
    than expected.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `previous_bookings` 的差异，这是由于用户在最初看到列表后发生的活动造成的。通过使用数据库的快照，用户未来的行为信息泄漏到了训练集中。我们现在知道用户最终将预订五套公寓！这种泄漏可能导致模型在不正确的训练数据上进行正确预测。模型在生成的数据集上的准确率会很高，因为它利用了生产环境中无法访问的数据。当模型部署后，其性能将低于预期。
- en: If you take anything away from this anecdote, it is to always investigate the
    results of a model, especially if it shows surprisingly strong performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从这个轶事中学到了什么，那就是要始终调查模型的结果，特别是如果它显示出令人惊讶的强大性能。
- en: ML Editor Data Split
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML Editor 数据分割
- en: 'The dataset we are using to train our ML Editor contains questions asked on
    Stack Overflow, as well as their answers. At first glance, a random split can
    seem sufficient and is quite simple to implement in scikit-learn. We could, for
    example, write a function like the one shown here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来训练我们的 ML Editor 的数据集包含在 Stack Overflow 上提出的问题及其答案。乍一看，随机分割似乎足够，并且在 scikit-learn
    中实现起来非常简单。例如，我们可以编写如下所示的函数：
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There is a potential reach for leakage with such an approach; can you identify
    it?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法存在潜在的泄漏风险；你能识别出它吗？
- en: If we think back to our use case, we know we would like our model to work on
    questions it has not seen before, only looking at their content. On a question
    and answering website, however, many other factors can play into whether a question
    is answered successfully. One of these factors is the identity of the author.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾一下我们的使用案例，我们知道我们希望我们的模型能够处理以前没有见过的问题，只看其内容。然而，在问答网站上，许多其他因素可能影响问题是否成功得到回答。其中一个因素是作者的身份。
- en: If we split our data randomly, a given author could appear both in our training
    and validation sets. If certain popular authors have a distinctive style, our
    model could overfit on this style and reach artificially high performance on our
    validation set due to data leakage. To avoid this, it would be safer for us to
    make sure each author appears only in training or validation. This is the same
    type of leakage we described in the student grading example earlier.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们随机分割数据，某个作者可能同时出现在我们的训练集和验证集中。如果某些知名作者具有独特的风格，我们的模型可能会在验证集上因数据泄漏而过度拟合到这种风格，并达到人为高的性能。为了避免这种情况，最好确保每个作者只出现在训练集或验证集中。这与之前描述的学生分级示例中描述的泄漏类型相同。
- en: Using scikit-learn’s `GroupShuffleSplit` class and passing the feature representing
    an author’s unique ID to its split method, we can guarantee that a given author
    appears in only one of the splits.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的 `GroupShuffleSplit` 类，并将表示作者唯一 ID 的特征传递给其分割方法，我们可以保证给定作者仅出现在其中一个分割中。
- en: '[PRE1]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To see a comparison between both splitting methods, refer to the splitting data
    notebook in [this book’s GitHub repository](https://oreil.ly/ml-powered-applications).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看两种分割方法之间的比较，请参考这本书的 GitHub 存储库中的分割数据笔记本。
- en: Once a dataset is split, a model can be fit to the training set. We’ve covered
    the required parts of a training pipeline in [“Start with a Simple Pipeline”](ch02.html#pipeline_description).
    In the training of a simple model notebook in the [GitHub repository for this
    book](https://oreil.ly/ml-powered-applications), I show an example of an end-to-end
    training pipeline for the ML Editor. We will analyze the results of this pipeline.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集被分割，模型就可以拟合到训练集中。我们已经在 [“从简单的流水线开始”](ch02.html#pipeline_description) 中覆盖了训练流水线的必要部分。在本书的
    GitHub 存储库中的简单模型笔记本的训练中，我展示了一个端到端的训练流水线示例。我们将分析这个流水线的结果。
- en: We’ve covered the main risks we want to keep in mind when splitting data, but
    what should we do once our dataset is split and we’ve trained a model on the training
    split? In the next section, we’ll talk about different practical ways to evaluate
    trained models and how to leverage them best.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了我们在分割数据时要牢记的主要风险，但是一旦我们的数据集被分割并且我们在训练分割上训练了一个模型，我们应该做些什么呢？在下一节中，我们将讨论评估训练模型的不同实用方法以及如何最好地利用它们。
- en: Judge Performance
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估性能
- en: Now that we have split our data, we can train our model and judge how it performed.
    Most models are trained to minimize a cost function, which represents how far
    a model’s predictions are from the true labels. The smaller the value of the cost
    function, the better the model fits the data. Which function you minimize depends
    on your model and your problem, but it is generally a good idea to take a look
    at its value both on the training set and on the validation set.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经分割了我们的数据，我们可以训练我们的模型并评估其表现。大多数模型被训练来最小化成本函数，该函数代表模型预测与真实标签的偏离程度。成本函数的值越小，模型对数据的拟合越好。你要最小化的具体函数取决于你的模型和问题，但总体来说，查看它在训练集和验证集上的值通常是个好主意。
- en: This commonly helps estimate the *bias-variance trade-off* of our model, which
    measures the degree to which our model has learned valuable generalizable information
    from the data, without memorizing the details of our training set.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常有助于估计我们模型的*偏差-方差权衡*，即衡量我们的模型在多大程度上从数据中学习到了有价值且可推广的信息，而不是记住训练集的细节。
- en: Note
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: I’m assuming familiarity with standard classification metrics, but here is a
    short reminder just in case. For classification problems, accuracy represents
    the proportion of examples a model predicts correctly. In other words, it is the
    proportion of true results, which are both true positives and true negatives.
    In cases with a strong imbalance, a high accuracy can mask a poor model. If 99%
    of cases are positive, a model that always predicts the positive class will have
    99% accuracy but may not be very useful. Precision, recall, and f1 score address
    this limitation. Precision is the proportion of true positives among examples
    predicted as positive. Recall is the proportion of true positives among elements
    that had a positive label. The f1 score is the harmonic mean of precision and
    recall.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设你对标准分类指标很熟悉，但万一你不熟悉，这里是一个简短的提醒。对于分类问题，准确率表示模型正确预测的示例比例。换句话说，它是真实结果的比例，即真正例和真负例的比例。在存在严重不平衡的情况下，高准确率可能掩盖了模型的糟糕表现。如果99%的情况都是正例，一个总是预测正类的模型将具有99%的准确率，但可能并不是很有用。精确率、召回率和
    f1 分数解决了这个问题。精确率是预测为正类的示例中真正例的比例。召回率是真正例在所有标签为正的元素中的比例。f1 分数是精确率和召回率的调和平均。
- en: In the training of a simple model notebook in [this book’s GitHub repository](https://oreil.ly/ml-powered-applications),
    we train a first version of a random forest using TF-IDF vectors and the features
    we identified in [“ML Editor Features”](ch04.html#ml_e_features).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在[这本书的 GitHub 仓库](https://oreil.ly/ml-powered-applications)中，我们训练了一个简单模型笔记本的训练过程，使用了
    TF-IDF 向量和我们在[“ML 编辑器特性”](ch04.html#ml_e_features)中确定的特征，训练了第一个随机森林版本。
- en: Here are the accuracy, precision, recall, and f1 scores for our training set
    and our validation set.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们训练集和验证集的准确率、精确率、召回率和 f1 分数。
- en: '[PRE2]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Taking a quick look at these metrics allows us to notice two things:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过快速查看这些指标，我们可以注意到两件事情：
- en: Since we have a balanced dataset consisting of two classes, picking a class
    at random for every example would give us roughly 50% accuracy. Our model’s accuracy
    reaches 61%, better than a random baseline.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们有一个由两个类组成的平衡数据集，对每个示例随机选择一个类别会给我们大约50%的准确率。我们模型的准确率达到了61%，比随机基线要好。
- en: Our accuracy on the validation set is higher than on the training set. It seems
    our model works well on unseen data.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在验证集上的准确率高于训练集。看来我们的模型在未见过的数据上表现良好。
- en: Let’s dive deeper to find out more about the performance of the model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解模型的表现更多细节。
- en: Bias variance trade-off
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏差方差权衡
- en: 'Weak performance on the training set is a symptom of high bias, also called
    *underfitting*, which means a model has failed to capture useful information:
    it is not even able to perform well on data points it has already been given the
    label for.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集上的性能差表明存在高偏差，也称为*欠拟合*，这意味着模型未能捕捉到有用的信息：它甚至不能在它已经知道标签的数据点上表现良好。
- en: Strong performance on the training set but weak performance on the validation
    set is a symptom of high variance, also called *overfitting*, meaning that a model
    has found ways to learn the input/output mapping for the data it has been trained
    on, but what it has learned does not generalize to unseen data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集表现强劲但验证集表现疲软是高方差的症状，也称为*过拟合*，意味着模型已经找到了学习输入/输出映射的方法，但它所学习的内容并不能泛化到未见数据。
- en: Underfitting and overfitting are two extreme cases of the bias-variance trade-off,
    which describes how the types of errors a model makes change as its complexity
    increases. As model complexity grows, variance increases and bias decreases, and
    the model goes from underfitting to overfitting. You can see this depicted in
    [Figure 5-5](#bias_variance_tradeoff).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合和过拟合是偏差-方差权衡的两个极端情况，描述了模型在复杂度增加时错误类型的变化。随着模型复杂度的增加，方差增加而偏差减少，模型由欠拟合逐渐转变为过拟合。你可以在[图 5-5](#bias_variance_tradeoff)中看到这一点。
- en: '![A visualization of the bias variance tradeoff](assets/bmla_0503.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![偏差-方差权衡的可视化](assets/bmla_0503.png)'
- en: Figure 5-5\. As complexity increases, bias decreases but variance increases
    as well
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. 随着复杂度增加，偏差减少但方差也增加
- en: In our case, since our validation performance is better than our training performance,
    we can see that our model is not overfitting the training data. We can likely
    increase the complexity of our model or features to improve performance. Fighting
    the bias-variance trade-off requires finding an optimal point between reducing
    bias, which increases a model’s performance on the training set, and reducing
    variance, which increases its performance on the validation set (often worsening
    training performance as a byproduct).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，由于验证性能优于训练性能，我们可以看出模型没有过度拟合训练数据。我们可能可以增加模型的复杂度或特征以改善性能。应对偏差-方差权衡需要找到在减少偏差（提高模型在训练集上的性能）和减少方差（提高模型在验证集上的性能，通常作为副产品恶化训练性能）之间的最佳点。
- en: Performance metrics help generate an aggregate perspective of a model’s performance.
    This is helpful to guess how a model is doing but does not provide much intuition
    as to what precisely a model is succeeding or failing at. To improve our model,
    we need to dive deeper.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标有助于生成模型性能的整体视角。这对猜测模型的表现很有帮助，但并不能提供对模型在具体方面成功或失败的深刻理解。要改进我们的模型，我们需要深入挖掘。
- en: Going beyond aggregate metrics
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超越汇总指标
- en: 'A performance metric helps determine whether a model has learned correctly
    from a dataset or whether it needs to be improved. The next step is to examine
    results further in order to understand in which way a model is failing or succeeding.
    This is crucial for two reasons:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 性能度量帮助确定模型是否从数据集中正确学习，或者是否需要改进。下一步是进一步检查结果，以了解模型是失败还是成功的方式。这一点至关重要，原因有两个：
- en: Performance validation
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 性能验证
- en: Performance metrics can be very deceptive. When working on a classification
    problem with severely imbalanced data such as predicting a rare disease that appears
    in fewer than 1% of patients, any model that always predicts that a patient is
    healthy will reach an accuracy of 99%, even though it has no predictive power
    at all. There exists performance metrics suited for most problems (the [f1 score](https://oreil.ly/fQAq9)
    would work better for the previous problem), but the key is to remember that they
    are aggregate metrics and paint an incomplete picture of the situation. To trust
    the performance of a model, you need to inspect results at a more granular level.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标可能非常具有误导性。在处理严重不平衡数据（例如预测少于 1% 患者中出现的罕见疾病）的分类问题时，任何总是预测患者健康的模型将达到 99% 的准确率，尽管它根本没有预测能力。大多数问题适用的性能指标（[f1
    分数](https://oreil.ly/fQAq9) 对于前述问题更合适），但关键是要记住它们是汇总指标，描绘了情况的不完整图景。要信任模型的性能，需要在更细粒度的水平上检查结果。
- en: Iteration
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代
- en: Model building is an iterative process, and the best way to start an iteration
    loop is by identifying both what to improve and how to improve it. Performance
    metrics do not help identify where a model is struggling and which part of the
    pipeline needs improvement. Too often, I’ve seen data scientists try to improve
    model performance by simply trying many other models or hyperparameters, or building
    additional features haphazardly. This approach amounts to throwing darts at the
    wall while blindfolded. The key to building successful models quickly is to identify
    and address specific reasons models are failing.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建是一个迭代过程，开始迭代循环的最佳方式是确定需要改进的内容及其改进方法。性能指标不能帮助确定模型在哪些方面存在问题，以及管道的哪一部分需要改进。我经常看到数据科学家仅仅通过尝试许多其他模型或超参数，或者随意构建额外特征来试图提高模型性能。这种方法就像闭着眼睛往墙上扔飞镖一样。快速构建成功模型的关键是识别和解决模型失败的具体原因。
- en: With these two motivations in mind, we will cover a few ways to dive deeper
    into the performance of a model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑这两个动机的基础上，我们将介绍几种深入了解模型性能的方法。
- en: 'Evaluate Your Model: Look Beyond Accuracy'
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估您的模型：超越准确率
- en: There are a myriad of ways to inspect how a model is performing, and we will
    not cover every potential evaluation method. We will focus on a few that are often
    helpful to tease out what might be happening below the surface.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方法可以检查模型的表现，我们不会涵盖每种潜在的评估方法。我们将专注于一些通常有助于挖掘潜在问题的方法。
- en: When it comes to investigating model performance, think of yourself as a detective
    and each of the methods covered next as different ways to surface clues. We’ll
    start by covering multiple techniques that contrast a model’s prediction with
    the data to uncover interesting patterns.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及调查模型性能时，把自己想象成侦探，每种涵盖的方法都是揭示线索的不同方式。我们将首先介绍多种技术，通过对比模型预测与数据来发现有趣的模式。
- en: Contrast Data and Predictions
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对比数据和预测
- en: The first step to evaluating a model in depth is to find more granular ways
    than aggregate metrics to contrast data and predictions. We’d like to break down
    aggregate performance metrics such as accuracy, precision, or recall on different
    subsets of our data. Let’s see how to do this for the common ML challenge of classification.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 深入评估模型的第一步是找到比聚合指标更细粒度的方式，以对比数据和预测。我们希望分析不同数据子集上的聚合性能指标，例如分类中的准确率、精确率或召回率。让我们看看如何针对常见的机器学习挑战来实现这一点。
- en: You can find all the code examples in the comparing data to predictions notebook
    in [this book’s GitHub repository](https://oreil.ly/ml-powered-applications).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[该书的 GitHub 代码库](https://oreil.ly/ml-powered-applications)的“将数据与预测进行比较”笔记本中找到所有代码示例。
- en: For classification problems, I usually recommend starting by looking at a confusion
    matrix, shown in [Figure 5-6](#confusion_matrix), whose rows represent each true
    class, and columns represent the predictions of our model. A model with perfect
    predictions will have a confusion matrix with zeros everywhere except in the diagonal
    going from the top left to the bottom right. In reality, that is rarely the case.
    Let’s take a look at why a confusion matrix is often very useful.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，我通常建议首先查看混淆矩阵，显示在[图5-6](#confusion_matrix)中，其行表示每个真实类别，列表示我们模型的预测。一个预测完美的模型将拥有一个只有对角线上非零的混淆矩阵。然而在现实中，这种情况很少见。让我们看看为什么混淆矩阵通常非常有用。
- en: Confusion Matrix
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: A confusion matrix allows us at a glance to see whether our model is particularly
    successful on certain classes and struggles on some others. This is particularly
    useful for datasets with many different classes or classes that are imbalanced.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵能够让我们一眼看出模型在某些类别上是否特别成功，而在其他类别上是否有困难。这对于包含许多不同类别或类别不平衡的数据集尤其有用。
- en: Oftentimes, I’ve seen models with impressive accuracy show a confusion matrix
    with one column entirely empty, meaning that there is a class that the model never
    predicts. This often happens for rare classes and can sometimes be harmless. If
    the rare class represents an important outcome, however, such as a borrower defaulting
    on a loan, a confusion matrix will help us notice the problem. We can then correct
    it by weighing the rare class more heavily in our model’s loss function, for example.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常看到那些准确率惊人的模型展示一个完全空的列的混淆矩阵，这意味着有一个模型从不预测的类别。这通常发生在稀有类别上，并且有时是无害的。然而，如果稀有类别代表一个重要的结果，比如借款人违约，混淆矩阵将帮助我们注意到问题。我们可以通过在模型损失函数中更加重视稀有类别来纠正这一问题，例如。
- en: The top row of [Figure 5-6](#confusion_matrix) shows that the initial model
    we’ve trained does well when it comes to predicting low-quality questions. The
    bottom row shows that the model struggles to detect all high-quality questions.
    Indeed, out of all the questions that received a high score, our model only predicts
    their class correctly half of the time. Looking at the right column, however,
    we can see that when the model predicts that a question is high quality, its prediction
    tends to be accurate.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-6](#confusion_matrix)的顶行显示，我们训练的初始模型在预测低质量问题时表现良好。底行显示，模型在检测所有高质量问题方面存在困难。确实，在所有得分高的问题中，我们的模型只有一半时间正确预测其类别。然而，观察右列，我们可以看到当模型预测问题是高质量时，其预测往往是准确的。'
- en: Confusion matrices can be even more useful when working on problems with more
    than two classes. For example, I once worked with an engineer who was trying to
    classify words from speech utterances who plotted a confusion matrix for his latest
    model. He immediately noticed two symmetrical, off-diagonal values that were abnormally
    high. These two classes (which each represented a word) were confusing the model
    and the cause of a majority of its errors. Upon further inspection, it turns out
    that the words that were confusing the model were *when* and *where*. Gathering
    additional data for these two examples was enough to help the model better differentiate
    these similar-sounding words.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵在处理多类问题时尤为有用。例如，我曾与一名工程师合作，他试图对语音中的单词进行分类，并绘制了最新模型的混淆矩阵。他立即注意到两个对称的非对角线数值异常高。这两个类别（每个代表一个单词）混淆了模型，并导致了大部分错误的原因。进一步检查后发现，混淆模型的单词是*when*和*where*。为这两个示例收集额外数据足以帮助模型更好地区分这些听起来相似的单词。
- en: A confusion matrix allows us to compare a model’s predictions with the true
    classes for each class. When debugging models, we may want to look deeper than
    their predictions and examine the probabilities output by the model.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵允许我们将模型的预测与每个类别的真实类别进行比较。在调试模型时，我们可能希望深入查看模型输出的概率，而不仅仅是它们的预测结果。
- en: '![A confusion matrix for an initial baseline on our question classification
    task](assets/bmla_0504.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![我们问题分类任务的初始基线混淆矩阵](assets/bmla_0504.png)'
- en: Figure 5-6\. Confusion matrix for an initial baseline on our question classification
    task
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6\. 我们问题分类任务的初始基线混淆矩阵
- en: ROC Curve
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROC曲线
- en: For binary classification problems, receiver operating characteristic (ROC)
    curves can also be very informative. An ROC curve plots the true positive rate
    (TPR) as a function of the false positive rate (FPR).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类问题，接收者操作特征（ROC）曲线也可以提供非常有用的信息。ROC曲线将真正率（TPR）作为误报率（FPR）的函数进行绘制。
- en: The vast majority of models used in classification return a probability score
    that a given example belongs to a certain class. This means that at inference
    time, we can choose to attribute an example to a certain class if the probability
    given by the model is above a certain threshold. This is usually called the *decision
    threshold*.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类中使用的绝大多数模型返回一个给定示例属于某一类的概率分数。这意味着在推断时，如果模型给出的概率高于某个阈值，我们可以选择将示例归属于某一类。这通常称为*决策阈值*。
- en: By default, most classifiers use a probability of 50% for their decision threshold,
    but this is something we can change based on our use case. By varying the threshold
    regularly from 0 to 1 and measuring the TPR and FPR at each point, we obtain an
    ROC curve.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数分类器默认使用50%的概率作为决策阈值，但这是可以根据我们的用例进行更改的。通过定期从0到1变化阈值，并在每个点测量TPR和FPR，我们得到一个ROC曲线。
- en: Once we have a model’s prediction probability and the associated true labels,
    getting FPRs and TPRs is simple using scikit-learn. We can then generate an ROC
    curve.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了模型的预测概率和相关的真实标签，使用scikit-learn简单地获取FPR和TPR，然后生成ROC曲线。
- en: '[PRE3]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Two details are important to understand for ROC curves such as the one plotted
    in [Figure 5-7](#roc_curve). First, the diagonal line going between the bottom
    left to the top right represents guessing randomly. This means that to beat a
    random baseline, a classifier/threshold pair should be above this line. In addition,
    the perfect model would be represented by the green dotted line on the top left.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 了解ROC曲线的两个重要细节，例如在[图5-7](#roc_curve)中绘制的那条。首先，沿着从左下到右上的对角线表示随机猜测。这意味着为了超越随机基线，分类器/阈值对应应该在这条线的上方。此外，完美模型将由左上角的绿色虚线表示。
- en: '![ROC curve for an initial model](assets/bmla_0505.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![初始模型的ROC曲线](assets/bmla_0505.png)'
- en: Figure 5-7\. ROC curve for an initial model
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7\. 初始模型的ROC曲线
- en: Because of these two details, classification models often use the area under
    the curve (AUC) to represent performance. The larger the AUC, the closer to a
    “perfect” model our classifier could be. A random model will have an AUC of 0.5,
    while a perfect model has an AUC of 1\. When concerning ourselves with a practical
    application, however, we should choose one specific threshold that gives us the
    most useful TPR/FPR ratio for our use case.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个细节，分类模型通常使用曲线下面积（AUC）来表示性能。AUC越大，我们的分类器就越接近“完美”模型。随机模型的AUC为0.5，而完美模型的AUC为1。然而，在考虑实际应用时，我们应选择一个特定的阈值，以获得对我们用例最有用的TPR/FPR比率。
- en: For that reason, I recommend adding vertical or horizontal lines to an ROC curve
    that represent our product needs. When building a system that routes customer
    requests to staff if it is deemed urgent enough, the FPR you can afford is then
    entirely determined by the capacity of your support staff and the number of users
    you have. This means that any models with an FPR higher than that limit should
    not even be considered.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我建议在ROC曲线上添加垂直或水平线，以表示我们的产品需求。在构建一个系统，如果认为紧急情况足够紧急，则将客户请求路由到员工的情况下，您能够承受的FPR完全由您支持人员的能力和您拥有的用户数量决定。这意味着任何FPR高于该限制的模型都不应被考虑。
- en: Plotting a threshold on an ROC curve allows you to have a more concrete goal
    than simply getting the largest AUC score. Make sure your efforts count toward
    your goal!
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在ROC曲线上绘制一个阈值使您可以设定一个比仅仅获取最大AUC分数更具体的目标。确保您的努力能够达到您的目标！
- en: Our ML Editor model classifies questions as good or bad. In this context, the
    TPR represents the proportion of high-quality questions our model correctly judges
    as good. The FPR is the proportion of bad questions our model claims is good.
    If we do not help our users, we’d like to at least guarantee we don’t harm them.
    This means that we should not use any model that risks recommending bad questions
    too frequently. We should thus set a threshold for our FPR, such as 10%, for example,
    and use the best model we can find under that threshold. In [Figure 5-8](#roc_product_lines),
    you can see this requirement represented on our ROC curve; it has significantly
    reduced the space of acceptable decision thresholds for models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的ML编辑器模型将问题分类为好的或坏的。在这种情况下，TPR表示我们的模型正确判断为好的高质量问题的比例。FPR是我们的模型声称是好问题的坏问题的比例。如果我们不能帮助我们的用户，那么我们至少希望不要伤害他们。这意味着我们不应使用任何可能频繁推荐坏问题的模型。因此，我们应该设置一个FPR的阈值，例如10%，并使用我们能找到的在该阈值以下的最佳模型。在[图5-8](#roc_product_lines)中，您可以看到这一要求在我们的ROC曲线上的表示；这显著减少了适用于模型的可接受决策阈值的空间。
- en: An ROC curve gives us a more nuanced view of how a model’s performance changes
    as we make its predictions more or less conservative. Another way to look at a
    model’s prediction probability is to compare its distributions with the true class
    distributions to see whether it is well calibrated.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线为我们提供了一个更细致的视角，显示了模型在我们使其预测更保守或更激进时的性能变化。观察模型的预测概率分布与真实类分布的比较是另一种方式，用于检查其是否校准良好。
- en: '![Adding ROC lines representing our product need](assets/bmla_0506.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![添加表示我们产品需求的ROC线条](assets/bmla_0506.png)'
- en: Figure 5-8\. Adding ROC lines representing our product need
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-8\. 添加表示我们产品需求的ROC线条
- en: Calibration Curve
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 校准曲线
- en: Calibration plots are another informative plot for binary classification tasks,
    as they can help us get a sense for whether our model’s outputted probability
    represents its confidence well. A calibration plot shows the fraction of true
    positive examples as a function of the confidence of our classifier.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 校准图是二元分类任务中另一个信息丰富的图表，因为它可以帮助我们了解我们的模型输出的概率是否很好地代表了其置信度。校准图显示了我们分类器的置信度作为真正例子的分数的函数。
- en: For example, out of all the data points our classifier gives a probability of
    being classified as positive that is higher than 80%, how many of those data points
    are actually positive? A calibration curve for a perfect model will be a diagonal
    line from bottom left to top right.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们的分类器对所有数据点都给出了高于80%的正分类概率，其中有多少数据点实际上是正的？一个完美模型的校准曲线将是一个从左下到右上的对角线。
- en: In [Figure 5-9](#calib_curve), we can see at the top that our model is well
    calibrated between .2 and .7, but not for probabilities outside of that range.
    Taking a look at the histogram of predicted probabilities below reveals that our
    model very rarely predicts probabilities outside of that range, which is likely
    leading to the extreme results shown earlier. The model is rarely confident in
    its predictions.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图5-9](#calib_curve)中，我们可以看到在顶部，我们的模型在0.2到0.7之间的校准很好，但在该范围之外的概率上并不好。在下面查看预测概率直方图，我们可以看出，我们的模型很少预测超出该范围的概率，这可能导致之前显示的极端结果。该模型对其预测很少有信心。
- en: '![A calibration curve, the diagonal line represents a perfect model](assets/bmla_0507.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![校准曲线，对角线代表一个完美的模型](assets/bmla_0507.png)'
- en: 'Figure 5-9\. A calibration curve: the diagonal line represents a perfect model
    (top); histogram of predicted values (bottom)'
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-9\. 校准曲线：对角线代表一个完美的模型（顶部）；预测值的直方图（底部）
- en: For many problems, such as predicting CTR in ad serving, the data will lead
    to our models being quite skewed when probabilities get close to 0 or 1, and a
    calibration curve will help us see this at a glance.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多问题，例如在广告投放中预测点击率，当概率接近0或1时，数据将导致我们的模型相当倾斜，而校准曲线将帮助我们一目了然地看到这一点。
- en: To diagnose the performance of a model, it can be valuable to visualize individual
    predictions. Let’s cover methods to make this visualization process efficient.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了诊断模型的性能，可视化单个预测结果非常有价值。让我们讨论一些方法来使这个可视化过程更加高效。
- en: Dimensionality Reduction for Errors
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误的降维
- en: We described vectorization and dimensionality reduction techniques for data
    exploration in [“Vectorizing”](ch04.html#vectorizing) and [“Dimensionality reduction”](ch04.html#dim_red).
    Let’s see how the same techniques can be used to make error analysis more efficient.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[“向量化”](ch04.html#vectorizing)和[“降维”](ch04.html#dim_red)章节描述了数据探索中的向量化和降维技术。让我们看看这些技术如何用于使错误分析更加高效。
- en: When we first covered how to use dimensionality reduction methods to visualize
    data, we colored each point in a dataset by its class to observe the topology
    of labels. When analyzing model errors, we can use different color schemes to
    identify errors.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们首次介绍如何使用降维方法来可视化数据时，我们通过其类别对数据集中的每个点进行了颜色标记，以观察标签的拓扑结构。在分析模型错误时，我们可以使用不同的颜色方案来识别错误。
- en: To identify error trends, color each data point by whether a model’s prediction
    was correct or not. This will allow you to identify types of similar data points
    a model performs poorly on. Once you identify a region in which a model performs
    poorly, visualize a few data points in it. Visualizing hard examples is a great
    way to generate features represented in these examples to help a model fit them
    better.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要识别错误趋势，可以根据模型预测的准确性对每个数据点进行颜色标记。这样可以帮助您找出模型在哪些类型的相似数据点上表现不佳。一旦确定了模型表现不佳的区域，可以对其中的几个数据点进行可视化。可视化难例是生成能帮助模型更好拟合这些例子中所表示特征的一种很好的方式。
- en: To help surface trends in hard examples, you can also use the clustering methods
    from [“Clustering”](ch04.html#clustering). After clustering data, measure model
    performance on each cluster and identify clusters where the model performs worst.
    Inspect data points in these clusters to help you generate more features.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助发现难例中的趋势，还可以使用来自[“聚类”](ch04.html#clustering)的聚类方法。在对数据进行聚类后，评估模型在每个簇上的性能，并识别模型表现最差的簇。检查这些簇中的数据点可以帮助您生成更多的特征。
- en: Dimensionality reduction techniques are one way of surfacing challenging examples.
    To do so, we can also directly use a model’s confidence score.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 降维技术是浮现挑战性示例的一种方式。为此，我们还可以直接使用模型的置信度分数。
- en: The Top-k Method
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Top-k 方法
- en: Finding dense error regions helps identify failure modes for a model. Above,
    we used dimensionality reduction to help us find such regions, but we can also
    directly use a model itself. By leveraging prediction probabilities, we can identify
    data points that were most challenging or for which a model was most uncertain.
    Let’s call this approach the *top-k method*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 找到密集错误区域有助于识别模型的失败模式。在上文中，我们使用降维技术帮助我们找到这些区域，但我们也可以直接使用模型本身。通过利用预测概率，我们可以识别模型最具挑战性或最不确定的数据点。让我们称这种方法为
    *top-k 方法*。
- en: 'The top-k method is straightforward. First, pick a manageable number of examples
    to visualize that we will call k. For a personal project where a single person
    will be visualizing results, start with ten to fifteen examples. For each class
    or cluster you have previously found, visualize:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k 方法很直接。首先，选择一些易于可视化的示例，我们将其称为 k。对于单人进行的个人项目，建议从十到十五个示例开始。对于您先前找到的每个类别或聚类，请进行可视化：
- en: The k best performing examples
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最优表现的 k 个示例
- en: The k worst performing examples
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差表现的 k 个示例
- en: The k most uncertain examples
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最不确定的 k 个示例
- en: Visualizing these examples will help you identify examples that are easy, hard,
    or confusing to your model. Let’s dive into each category in more detail.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通过可视化这些示例，您可以识别模型认为易、难或令人困惑的示例。让我们更详细地探讨每个类别。
- en: The k best performing examples
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最优表现的 k 个示例
- en: First, display the k examples your model predicted correctly and was most confident
    about. When visualizing those examples, aim to identify any commonality in feature
    value among them that could explain model performance. This will help you identify
    features that are successfully leveraged by a model.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，显示模型预测正确且最有信心的 k 个示例。在可视化这些示例时，目标是识别它们之间的特征值共性，以解释模型的性能。这将帮助您识别模型成功利用的特征。
- en: After visualizing successful examples to identify features leveraged by a model,
    plot unsuccessful ones to identify features it fails to pick up on.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在可视化成功示例以识别模型利用的特征后，再绘制不成功的示例以识别模型未能捕捉到的特征。
- en: The k worst performing examples
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最差表现的 k 个示例
- en: Display the k examples your model predicted incorrectly and was most confident
    about. Start with k examples in training data and then validation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 显示模型预测错误且最有信心的 k 个示例。首先使用训练数据中的 k 个示例，然后再使用验证数据。
- en: Just like visualizing error clusters, visualizing k examples that a model performs
    the worst on in a training set can help identify trends in data points the model
    fails on. Display these data points to help you identify additional features that
    would make them easier for a model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 就像可视化错误聚类一样，通过可视化模型在训练集中表现最差的 k 个示例，可以帮助识别模型失败的数据点的趋势。显示这些数据点以帮助您识别能够使模型更轻松的其他特征。
- en: When exploring an initial model’s error for the ML Editor, for example, I found
    that some questions posted received a low score because they did not contain an
    actual question. The model was not initially able to predict a low score for such
    questions, so I added a feature to count question marks in the body of the text.
    Adding this feature allowed the model to make accurate predictions for these “nonquestion”
    questions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当探索 ML 编辑器的初始模型错误时，例如，我发现一些发布的问题由于没有包含实际问题而得分低。初始模型无法预测这些“非问题”问题，因此我添加了一个功能来计算文本主体中的问号数。添加此功能使得模型能够准确预测这些问题。
- en: Visualizing the k worst examples in the validation data can help identify examples
    that significantly differ from the training data. If you do identify examples
    in the validation set that are too hard, refer to tips in [“Split Your Dataset”](#splitting_data)
    to update your data splitting strategy.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化验证数据中最差的 k 个示例有助于识别与训练数据显著不同的示例。如果您在验证集中确实识别出太难的示例，请参考[“分割数据集”](#splitting_data)中的提示更新您的数据分割策略。
- en: Finally, models are not always confidently right or wrong; they can also output
    uncertain predictions. I’ll cover those next.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型并非始终自信地正确或错误；它们也可能输出不确定的预测。接下来我将介绍这些情况。
- en: The k most uncertain examples
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最不确定的 k 个示例
- en: Visualizing the k most uncertain examples consists of displaying examples for
    which a model was least confident in its predictions. For a classification model,
    which this book focuses mostly on, uncertain examples are ones where a model outputs
    as close to an equal probability as possible for each class.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化最不确定的 k 个示例包括显示模型在其预测中最不自信的示例。对于本书主要关注的分类模型，不确定的示例是模型对每个类输出尽可能接近相等概率的示例。
- en: If a model is well calibrated (see [“Calibration Curve”](#cal_curve_sect) for
    an explanation of calibration), it will output uniform probabilities for examples
    that a human labeler would be uncertain about as well. For a cat versus dog classifier,
    for example, a picture containing both a dog and a cat would fall into that category.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型经过良好的校准（参见[“校准曲线”](#cal_curve_sect)以了解校准的解释），它将为人工标注员也会不确定的示例输出均匀的概率。例如，对于猫与狗分类器，包含狗和猫的图片会属于这一类别。
- en: Uncertain examples in the training set are often a symptom of conflicting labels.
    Indeed, if a training set contains two duplicate or similar examples that are
    each labeled a different class, a model will minimize its loss during training
    by outputting an equal probability for each class when presented with this example.
    Conflicting labels thus lead to uncertain predictions, and you can use the top-k
    method to attempt to find these examples.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集中的不确定示例通常是冲突标签的症状。确实，如果训练集包含两个重复或相似的示例，每个示例被标记为不同的类，则当模型使用此示例时，为了最小化损失，它将为每个类输出相等的概率。因此，冲突标签会导致不确定的预测，您可以使用
    top-k 方法来尝试找到这些示例。
- en: Plotting the top-k most uncertain examples in your validation set can help find
    gaps in your training data. Validation examples that a model is uncertain about
    but are clear to a human labeler are often a sign that the model has not been
    exposed to this kind of data in its training set. Plotting the top-k uncertain
    examples for a validation set can help identify data types that should be present
    in the training set.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制验证集中最不确定的 top-k 示例可以帮助找出训练数据中的差距。对于模型不确定但对于人工标注员明确的验证示例，通常表明模型在其训练集中未接触到这种数据类型。绘制验证集中最不确定的
    top-k 示例可以帮助识别应该存在于训练集中的数据类型。
- en: Top-k evaluation can be implemented in a straightforward manner. In the next
    section, I’ll share a working example.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Top-k 评估可以以直接的方式实现。在下一节中，我将分享一个工作示例。
- en: Top-k implementation tips
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Top-k 实现技巧
- en: The following is a simple top-k implementation that works with pandas DataFrames.
    The function takes as input a DataFrame containing predicted probabilities and
    labels and returns each of the top-k above. It can be found in [this book’s GitHub
    repository](https://oreil.ly/ml-powered-applications).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是与 pandas DataFrame 一起工作的简单的 top-k 实现。该函数以包含预测概率和标签的 DataFrame 作为输入，并返回每个
    top-k 以上的结果。它可以在[这本书的 GitHub 仓库](https://oreil.ly/ml-powered-applications)中找到。
- en: '[PRE4]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s illustrate the top-k method by using it for the ML Editor.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过 ML Editor 来说明 top-k 方法。
- en: Top-k method for the ML Editor
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ML Editor 的 top-k 方法
- en: We’ll apply the top-k method to the first classifier that we trained. A notebook
    containing usage examples for the top-k method is available in [this book’s GitHub
    repository](https://oreil.ly/ml-powered-applications).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用 top-k 方法于我们训练的第一个分类器。关于 top-k 方法的使用示例的笔记本可以在[这本书的 GitHub 仓库](https://oreil.ly/ml-powered-applications)中找到。
- en: '[Figure 5-10](#top_k_correct) shows the top two most correct examples for each
    class for our first ML Editor model. The feature that differs the most between
    both classes is `text_len`, which represents the length of the text. The classifier
    has learned that good questions tend to be long, and poor ones are short. It relies
    heavily on text length to discriminate between classes.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-10](#top_k_correct)显示了我们的第一个 ML Editor 模型每个类别最正确的前两个示例。两个类别之间最大的特征差异是`text_len`，表示文本的长度。分类器已学会好问题往往较长，而差问题较短。它严重依赖文本长度来区分类别。'
- en: '![Top-k our classifier got most correct](assets/bmla_0508.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![我们的分类器得到最正确的 top-k](assets/bmla_0508.png)'
- en: Figure 5-10\. Top-k most correct
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-10\. 最正确的 top-k
- en: '[Figure 5-11](#top_k_incorrect) confirms this hypothesis. The unanswered questions
    our classifier predicts as most likely to be answered are the longest ones, and
    vice versa. This observation also corroborates what we found in [“Evaluate Feature
    Importance”](#eval_feat_imp_sect), where we saw `text_len` was the most important
    feature.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-11](#top_k_incorrect)证实了这一假设。我们的分类器预测未回答的问题最有可能被回答的是最长的问题，反之亦然。这一观察结果也证实了我们在[“评估特征重要性”](#eval_feat_imp_sect)中发现的情况，即`text_len`是最重要的特征。'
- en: '![Top-k our classifier got most wrong](assets/bmla_0509.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![Top-k我们分类器最错的地方](assets/bmla_0509.png)'
- en: Figure 5-11\. Top-k most incorrect
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-11. 最不正确的Top-k
- en: We’ve established that the classifier leverages `text_len` to easily identify
    answered and unanswered questions, but that this feature is not sufficient and
    leads to misclassifications. We should add more features to improve our model.
    Visualizing more than two examples would help identify more candidate features.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定分类器利用`text_len`轻松识别已回答和未回答的问题，但这个特征并不足够，会导致错误分类。我们应该添加更多特征来改进我们的模型。通过可视化超过两个示例可以帮助识别更多候选特征。
- en: Using the top-k method on both training and validation data helps identify limits
    of both our model and dataset. We’ve covered how it can help identify whether
    a model has the capacity to represent data, whether a dataset is balanced enough,
    and whether it contains enough representative examples.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和验证数据上使用top-k方法有助于确定我们模型和数据集的限制。我们已经讨论了它如何帮助确定模型是否具备表示数据的能力，数据集是否足够平衡以及是否包含足够的代表性示例。
- en: We mostly covered evaluation methods for classification models, since such models
    are applicable for many concrete problems. Let’s briefly take a look at ways to
    inspect performance when not doing classification.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要讨论了分类模型的评估方法，因为这些模型适用于许多具体问题。让我们简要地看看在不进行分类时如何检查性能。
- en: Other Models
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他模型
- en: Many models can be evaluated using a classification framework. In object detection,
    for example, where the goal is for a model to output bounding boxes around objects
    of interest in an image, accuracy is a common metric. Since each image can have
    multiple bounding boxes representing objects and predictions, calculating an accuracy
    requires an additional step. First, computing the overlap between predictions
    and labels (often using the [Jaccard index](https://oreil.ly/eklQm)) allows each
    prediction to be marked as correct or incorrect. From there, one can calculate
    accuracy and use all the previous methods in this chapter.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型可以使用分类框架进行评估。例如，在目标检测中，模型的目标是在图像中输出围绕感兴趣对象的边界框，准确率是常见的指标。由于每个图像可以有多个表示对象和预测的边界框，计算准确度需要额外的步骤。首先，计算预测和标签之间的重叠（通常使用[Jaccard指数](https://oreil.ly/eklQm)）允许将每个预测标记为正确或不正确。然后，可以计算准确度并在本章节中使用所有先前的方法。
- en: Similarly, when building models aiming to recommend content, the best way to
    iterate is often to test the model on a variety of categories and report its performance.
    The evaluation then becomes similar to a classification problem, where each category
    represents a class.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当构建旨在推荐内容的模型时，迭代的最佳方式通常是在各种类别上测试模型并报告其性能。评估过程类似于一个分类问题，其中每个类别代表一个类。
- en: For types of problems where such methods may prove tricky, such as with generative
    models, you can still use your previous exploration of data to separate a dataset
    in multiple categories and generate performance metrics for each category.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可能证明棘手的问题类型，例如生成模型，您仍然可以使用之前对数据的探索来将数据集分成多个类别，并为每个类别生成性能指标。
- en: When I worked with a data scientist building a sentence simplification model,
    examining the model’s performance conditioned on sentence length showed that longer
    sentences proved much harder for the model. This took inspection and hand labeling
    but led to a clear next action step of augmenting the training data with longer
    sentences, which helped improve performance significantly.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当我与一位数据科学家合作构建句子简化模型时，检查模型在句子长度条件下的表现显示，较长的句子对模型来说更加困难。这需要检查和手动标记，但却指明了明确的下一步行动，即通过增加包含长句子的训练数据来改善性能，这帮助显著提高了表现。
- en: We’ve covered many ways to inspect a model’s performance by contrasting its
    predictions with labels, but we can also inspect the model itself. If a model
    isn’t performing well at all, it may be worthwhile to try to interpret its predictions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了许多检验模型性能的方法，通过将其预测与标签进行对比，但我们也可以检查模型本身。如果一个模型表现不佳，尝试解释其预测可能是值得的。
- en: Evaluate Feature Importance
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估特征重要性
- en: An additional way to analyze a model’s performance is to inspect which features
    of the data it is using to make predictions. Doing so is called feature importance
    analysis. Evaluating feature importance is helpful to eliminate or iterate on
    features that are currently not helping the model. Feature importance can also
    help identify features that are suspiciously predictive, which is often a sign
    of data leakage. We will start by generating feature importance for models that
    can do so easily and then cover cases where such features may not be easy to extract
    directly.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 分析模型性能的另一种方法是检查模型用于进行预测的数据特征。这样做被称为特征重要性分析。评估特征重要性有助于消除或迭代当前对模型没有帮助的特征。特征重要性还可以帮助识别那些可疑预测的特征，这通常是数据泄漏的迹象。我们将首先为可以轻松生成特征重要性的模型生成特征重要性，然后涵盖那些直接提取这些特征可能不容易的情况。
- en: Directly from a Classifier
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直接从分类器获取
- en: To validate that a model is working correctly, visualize which features the
    model is using or ignoring. For simple models such as regression or decision trees,
    extracting the importance of features is straightforward by looking at the learned
    parameters of the model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证模型是否正常工作，可视化模型正在使用或忽略的特征。对于简单模型如回归或决策树，通过查看模型的学习参数，可以直观地提取特征的重要性。
- en: For the first model we used in the ML Editor case study, which is a random forest,
    we can simply use scikit-learn’s API to obtain a ranked list of the importance
    of all features. The feature importance code and its usages can be found in the
    feature importance notebook in [this book’s GitHub repository](https://oreil.ly/ml-powered-applications).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在 ML 编辑案例研究中使用的第一个模型，即随机森林，我们可以简单地使用 scikit-learn 的 API 来获取所有特征重要性的排名列表。特征重要性代码及其使用方法可以在[此书的
    GitHub 仓库中的特征重要性笔记本](https://oreil.ly/ml-powered-applications)找到。
- en: '[PRE5]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If we use the function above on our trained model, with some simple list processing
    we can get a simple list of the ten most informative features:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在训练好的模型上使用上述函数，并进行一些简单的列表处理，我们可以得到一个简单的十大最具信息量特征列表：
- en: '[PRE6]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'There are a few things to notice here:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要注意几点：
- en: The length of the text is the most informative feature.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本长度是最具信息量的特征。
- en: The other features we generated do not appear at all, with importances more
    than an order of magnitude lower than others. The model was not able to leverage
    them to meaningfully separate classes.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们生成的其他特征根本不会出现，其重要性比其他特征低一个数量级以上。该模型无法利用它们有效地区分类别。
- en: The other features represent either very common words, or nouns relevant to
    the topic of writing.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他特征代表的是非常常见的词汇，或者与写作主题相关的名词。
- en: Because our model and features are simple, these results can actually give us
    ideas for new features to build. We could, for example, add a feature that counts
    the usage of common and rare words to see if they are predictive of an answer
    receiving a high score.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的模型和特征都很简单，这些结果实际上可以给我们提供构建新特征的思路。例如，我们可以添加一个计数常见和罕见词汇使用情况的特征，以查看它们是否能预测答案得到高分。
- en: If features or models become complex, generating feature importances requires
    using model explainability tools.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果特征或模型变得复杂，生成特征重要性需要使用模型可解释性工具。
- en: Black-Box Explainers
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 黑盒解释器
- en: When features become complicated, feature importances can become harder to interpret.
    Some more complex models such as neural networks may not even be able to expose
    their learned feature importances. In such situations, it can be useful to leverage
    black-box explainers, which attempt to explain a model’s predictions independently
    of its inner workings.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征变得复杂时，特征的重要性可能变得更难解释。一些复杂模型如神经网络甚至可能无法公开其学习到的特征重要性。在这种情况下，利用黑盒解释器可能是有用的，它们试图独立解释模型的预测，而不关注其内部运作。
- en: Commonly, these explainers identify predictive features for a model on a given
    data point instead of globally. They do this by changing each feature value for
    a given example and observing how the model’s predictions change as a consequence.
    [LIME](https://github.com/marcotcr/lime) and [SHAP](https://github.com/slundberg/shap)
    are two popular black-box explainers.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些解释器识别给定数据点上模型的预测特征，而不是全局性的。它们通过改变给定示例的每个特征值，并观察模型预测如何随之变化来实现这一点。[LIME](https://github.com/marcotcr/lime)
    和 [SHAP](https://github.com/slundberg/shap) 是两种流行的黑盒解释器。
- en: For an end-to-end example of using these, see the black-box explainer notebook
    in [the book’s GitHub repository](https://oreil.ly/ml-powered-applications).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何使用这些的端到端示例，请参阅[书籍的 GitHub 仓库中的黑盒解释器笔记本](https://oreil.ly/ml-powered-applications)。
- en: '[Figure 5-12](#explanation_one) shows an explanation provided by LIME around
    which words were most important in deciding to classify this example question
    as high quality. LIME generated these explanations by repeatedly removing words
    from the input question and seeing which words make our model lean more towards
    one class or another.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-12](#explanation_one) 展示了由 LIME 提供的解释，说明了决定将这个示例问题分类为高质量的最重要单词。LIME 通过反复从输入问题中删除单词并观察哪些单词使我们的模型更倾向于一个类别而生成这些解释。'
- en: '![Explaining one particular example](assets/bmla_0510.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![解释一个特定的示例](assets/bmla_0510.png)'
- en: Figure 5-12\. Explaining one particular example
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-12\. 解释一个特定的示例
- en: We can see that the model correctly predicted that the question would receive
    a high score. However, the model was not confident, outputting only a 52% probability.
    The right side of [Figure 5-12](#explanation_one) shows the most impactful words
    for the prediction. These words do not seem like they should be particularly relevant
    to a question being of high quality, so let’s examine more examples to see if
    the model leverages more useful patterns.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型正确预测该问题将获得高分。然而，模型并不自信，仅输出52%的概率。[图 5-12](#explanation_one) 右侧显示了预测中最具影响力的单词。这些词似乎不应特别与问题高质量相关联，因此让我们查看更多示例，看看模型是否利用了更有用的模式。
- en: To get a quick sense of trends, we can use LIME on a larger sample of questions.
    Running LIME on each question and aggregating the results can give us an idea
    of which word our model finds predictive overall to make its decisions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速了解趋势，我们可以在更大的问题样本上使用 LIME。对每个问题运行 LIME 并汇总结果可以让我们了解我们的模型在整体上认为哪个词具有预测性来做出其决策。
- en: In [Figure 5-13](#explanation_all) we plot the most important predictions across
    500 questions in our dataset. We can see the trend of our model leveraging common
    words is apparent in this larger sample as well. It seems that the model is having
    a hard time generalizing beyond leveraging frequent words. The bag of words features
    representing rare words most often have a value of zero. To improve on this, we
    could either gather a larger dataset to expose our models to a more varied vocabulary,
    or create features that will be less sparse.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 5-13](#explanation_all) 中，我们绘制了数据集中500个问题的最重要预测。我们可以看到我们的模型倾向于利用常见单词的趋势在这个更大的样本中也很明显。看起来模型在推广到利用频繁单词之外时遇到了困难。表示稀有单词的词袋特征通常有零值。为了改进这一点，我们可以收集更大的数据集，让我们的模型接触更多样化的词汇，或者创建少量稀疏的特征。
- en: '![Explaining multiple examples](assets/bmla_0511.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![解释多个示例](assets/bmla_0511.png)'
- en: Figure 5-13\. Explaining multiple examples
  id: totrans-246
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-13\. 解释多个示例
- en: You’ll often be surprised by the predictors your model ends up using. If any
    features are more predictive for the model than you’d expect, try to find examples
    containing these features in your training data and examine them. Use this opportunity
    to double check how you split your dataset and watch for data leakage.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 你经常会对你的模型最终使用的预测器感到惊讶。如果某些特征对模型的预测比你预期的更具预测性，请尝试找到训练数据中包含这些特征的示例并检查它们。利用这个机会再次检查你如何分割你的数据集，并注意数据泄漏的情况。
- en: When building a model to automatically classify emails into different topics
    based on their content, for example, an ML Engineer I was mentoring once found
    that the best predictor was a three-letter code at the top of the email. It turns
    out this was an internal code to the dataset that mapped almost perfectly to the
    labels. The model was entirely ignoring the content of the email, and memorizing
    a pre-existing label. This was a clear example of data leakage, which was only
    caught by looking at feature importance.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当构建一个模型自动将电子邮件根据其内容分类到不同主题时，有一次我在指导的机器学习工程师发现，最佳预测器是电子邮件顶部的一个三字母代码。结果发现这是数据集的内部代码，几乎完美地映射到了标签。模型完全忽略了电子邮件的内容，而是记忆了一个预先存在的标签。这是数据泄漏的一个明显例子，只有通过查看特征重要性才能发现。
- en: Conclusion
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We started this chapter by covering criteria to decide on an initial model based
    on all we’ve learned so far. Then, we covered the importance of splitting data
    into multiple sets, and methods to avoid data leakage.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从覆盖到如何根据我们到目前为止学到的所有知识来决定初始模型的标准开始了本章。然后，我们讨论了将数据分割成多组的重要性，以及避免数据泄漏的方法。
- en: After training an initial model, we took a deep dive into ways to judge how
    well it is performing by finding different ways to compare and contrast its predictions
    to the data. Finally, we went on to inspect the model itself by displaying feature
    importances and using a black-box explainer to gain an intuition for the feature
    it uses to make predictions.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 训练了一个初始模型后，我们深入研究了如何评估其性能，通过不同的方式比较和对比其预测与数据。最后，我们检查了模型本身，显示了特征重要性，并使用黑盒解释器来直观地理解它用于预测的特征。
- en: By now, you should have some intuition about improvements you could make to
    your modeling. This takes us to the topic of [Chapter 6](ch06.html#debugging)
    where we will take a deeper dive on methods to tackle the problems we’ve surfaced
    here, by debugging and troubleshooting an ML pipeline.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该对如何改进你的建模有了一些直觉。这将引导我们到第[6章](ch06.html#debugging)的话题，深入探讨我们在此处提到的问题处理方法，通过调试和故障排除机器学习管道。
