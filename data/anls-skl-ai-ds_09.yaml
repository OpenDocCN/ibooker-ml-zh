- en: Appendix A. A Brief Introduction to Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 A. 机器学习简介
- en: In this Appendix I briefly summarize what machine learning is with the purpose
    of providing a self-contained guide. I have not attempted to go into the intricacies
    of the methods, as the topic of the book is to learn to create value from these
    technologies, and not to learn each of these many different methods. It will provide
    some background knowledge and hopefully some intuition on how machine learning
    works. For interested readers, I will also cover the basics of A/B testing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个附录中，我简要总结了机器学习的概念，目的是提供一个自包含的指南。我没有试图深入了解各种方法的复杂性，因为本书的主题是学习如何从这些技术中创造价值，而不是学习每一种不同的方法。它将提供一些背景知识，希望能对机器学习的工作原理有一些直观理解。对于感兴趣的读者，我还将介绍
    A/B 测试的基础知识。
- en: What Is Machine Learning?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Machine learning is the scientific discipline that studies ways that machines
    learn to accomplish certain tasks by using data and algorithms. Algorithms are
    recipes or sequences of instructions that are applied over and over until a precise
    objective is attained, and are written with programming languages that enable
    human interaction with computers. These are then translated to machine language
    that can then be processed and computed.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一门科学学科，研究机器如何通过数据和算法学习完成某些任务。算法是一种配方或一系列指令，反复应用直至达到精确的目标，并且是用编程语言编写的，使人能够与计算机进行交互。然后这些指令被转换成机器语言，然后进行处理和计算。
- en: A Taxonomy of ML Models
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML 模型的分类法
- en: Any taxonomy of machine learning algorithms starts by describing *supervised*
    and *unsupervised* methods ([Figure A-1](#fig_ch01_taxonomy)). In general, learning
    is supervised when someone or something tells us when the task was completed successfully.
    For instance, if you are learning to play a musical instrument, say the conga
    drums, your teacher might show you first how a good slap tone sounds. You try
    it yourself and she tells you whether the technique and sound were close to a
    perfect tone; we call this process of comparing an attempt with the ideal case
    *supervision*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的任何分类法都从描述*监督*和*无监督*方法开始（[图 A-1](#fig_ch01_taxonomy)）。通常情况下，当有人或某物告诉我们任务成功完成时，我们称这种学习为监督学习。例如，如果你正在学习弹奏一种乐器，比如康加鼓，你的老师可能会首先展示给你一个好的掌声音调的例子。你自己尝试后，她会告诉你你的技术和音调是否接近完美音调；我们称这种将尝试与理想情况进行比较的过程为*监督*。
- en: '![Taxonomy of supervised and unsupervised models](assets/asai_aa01.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![监督和无监督模型的分类法](assets/asai_aa01.png)'
- en: Figure A-1\. A taxonomy of learning models
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A-1\. 学习模型的分类法
- en: Supervised Learning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: Supervised learning algorithms work the same, with humans providing guidance
    on how the world is and how different it is from the algorithm’s current guess.
    For this to happen we must first *label* the data. Take an image recognition example
    where we input a picture and the algorithm needs to correctly identify what appears
    in the picture (say dogs). We feed the algorithm with enough pictures of dogs
    with the “dog” label, as well as pictures of cats, drums, elephants, and any other
    object in the world with their corresponding “cat,” “drum,” and “elephant” labels.
    Thanks to labeling, the algorithm is able to compare the most recent prediction
    with reality and adjust accordingly.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法同样如此，人类提供指导，告诉算法世界的情况与其当前猜测的差异。为了实现这一点，我们必须首先对数据进行*标记*。以图像识别为例，我们输入一张图片，算法需要正确识别图片中显示的内容（比如狗）。我们向算法提供足够多带有“狗”标签的狗的图片，以及其他物体（如猫、鼓、大象等）的图片，并标有相应的“猫”、“鼓”和“大象”标签。通过标记，算法能够将最新的预测与现实进行比较，并相应调整。
- en: 'The supervised world is relatively straightforward; all we need is a method
    to generate predicted labels, a method to compare these with the real ones, and
    an updating rule to improve performance over time. We evaluate the overall quality
    of our current guesses by means of a *loss function*: it achieves a minimum whenever
    our data is perfectly predicted and increases as the predictive power worsens.
    As you might imagine, our objective is to minimize the loss, and smart updating
    rules allow us to keep descending until it is no longer feasible to get significant
    improvements.^([1](app01.html#idm46359801798360))'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的世界相对简单；我们只需要一种生成预测标签的方法，一种与实际标签比较的方法，以及一种随时间改进性能的更新规则。我们通过*损失函数*评估当前猜测的整体质量：当我们的数据完全预测时，它达到最小值，并且随着预测能力变差而增加。正如您可能想象的那样，我们的目标是最小化损失，聪明的更新规则使我们能够持续下降，直到不再可能获得显著改进。^([1](app01.html#idm46359801798360))
- en: Unsupervised Learning
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Unsupervised learning is much harder since there’s no explicit measure of what
    a right or wrong prediction is. Our objective is to uncover some underlying pattern
    in the data that is also informative for the problem at hand. Two examples are
    clustering and anomaly detection.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习要困难得多，因为没有明确的衡量标准来判断预测的对错。我们的目标是发现数据中一些潜在的模式，这些模式对问题解决同样有信息性。聚类和异常检测是两个例子。
- en: In clustering, our aim is to find relatively homogenous groups of customers,
    say, where the different groups are also different from each other. This is commonly
    used in data-driven segmentation.^([2](app01.html#idm46359801789096))
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类中，我们的目标是找到相对同质的客户群体，例如，这些不同群体之间也应该有所不同。这在数据驱动的分割中经常使用。^([2](app01.html#idm46359801789096))
- en: In unsupervised anomaly detection, the algorithm tries to distinguish “expected”
    from “unexpected” by looking at the distribution of observed characteristics.
    For instance, it may be that 95% of our executives sell between 7 and 13 computers
    in a regular day; anything off these limits could be labeled as abnormal.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督异常检测中，算法试图通过观察特征的分布来区分“预期”的和“意外”的。例如，也许我们的高管中有95%在正常的一天内销售7到13台电脑；超出这些限制的任何事物都可能被标记为异常。
- en: Semisupervised Learning
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半监督学习
- en: There is an intermediate category not depicted in [Figure A-1](#fig_ch01_taxonomy),
    sometimes called *semisupervised* learning, where an algorithm is able to generalize
    knowledge from just a few examples of data. Some practitioners believe that this
    is closer to how humans learn, in sharp contrast to the most advanced supervised
    techniques that have to be fed with thousands or millions of data points to provide
    reliable predictions. Children learn to recognize objects from just a few examples.
    Some researchers believe that this semisupervised learning problem is one of the
    most important challenges that current AI must overcome.^([3](app01.html#idm46359801777848))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个中间类别未在[图 A-1](#fig_ch01_taxonomy)中显示，有时称为*半监督*学习，在这种算法中，只需少量数据示例即可泛化知识。一些从业者认为，这更接近人类学习的方式，与最先进的需要数千或数百万数据点才能提供可靠预测的监督技术形成鲜明对比。儿童只需少量示例即可学会识别物体。一些研究人员认为，当前人工智能必须克服的主要挑战之一是这种半监督学习问题。^([3](app01.html#idm46359801777848))
- en: Regression and Classification
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归和分类
- en: 'To continue with our taxonomy of learning algorithms, let us talk about the
    two most general classes of supervised learning tasks: regression and classification.
    In regression our labels take the form of quantitative measures. For instance,
    you may want to predict the age or income level of your customers, or the time
    we believe they will be loyal to our company; all of these can be accurately represented
    as numbers: your 40-year-old neighbor is one year older than your 39-year-old
    brother and seven years younger than your 47-year-old friend. In regression, *values
    have a strict numeric interpretation*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的学习算法分类，让我们讨论两种最常见的监督学习任务：回归和分类。在回归中，我们的标签采用量化的形式。例如，您可能希望预测客户的年龄或收入水平，或者他们将忠诚于我们公司的时间；所有这些都可以准确表示为数字：您40岁的邻居比您39岁的兄弟大一岁，比您47岁的朋友小七岁。在回归中，*数值具有严格的数值解释*。
- en: 'In contrast, in classification tasks the objective is to predict a categorical
    label: you may want to predict your customers’ gender, if they will default on
    their loan or not, or whether one specific sale is fraudulent or not. Notice that
    the labels here are categories and you can assign them any name you want. In gender
    classification, “men” can alternatively be labeled “0” and “women” labeled “1,”
    but these apparently numeric labels lack numeric meaning: you can just switch
    the labels and redefine everything accordingly and nothing has changed in your
    learning task. There is no sense that one is larger than the other or you cannot
    perform arithmetic calculations.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在分类任务中，目标是预测一个分类标签：你可能想预测客户的性别、他们是否会违约贷款，或者某个特定销售是否存在欺诈行为。请注意，这里的标签是类别，你可以为它们指定任何你想要的名称。在性别分类中，“男性”也可以被标记为“0”，“女性”标记为“1”，但这些显然是数值标签缺乏数值意义：你可以随意交换标签并相应地重新定义一切，而你的学习任务中没有任何改变。没有意义表明其中一个大于另一个或者你不能进行算术计算。
- en: Regression can be easily converted into a classification problem by taking advantage
    of the ordered property of numbers. One common regression task in business is
    to predict the profits from a specific activity, but sometimes you may not need
    the exact numerical value; say you are happy to know if your company is going
    to lose money (negative profits) or not (zero or positive profits). Similarly,
    for advertising and marketing purposes we usually need age ranges associated with
    behavioral differences, instead of an exact estimate of a customer’s age. In both
    examples we end up with classification problems instead of the original regression
    tasks. Note, in passing, that this is the same trick advocated in [Chapter 7](ch07.html#ch07_optimization)
    to binarize decisions where a continuum of actions can be taken.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 回归可以通过利用数字的有序属性轻松转换为分类问题。业务中一个常见的回归任务是预测特定活动的利润，但有时你可能不需要确切的数值：比如你只想知道你的公司是否会亏钱（负利润）或者不会（零或正利润）。同样，对于广告和市场目的，我们通常需要与行为差异相关联的年龄范围，而不是客户年龄的精确估计。在这两个例子中，我们最终得到了分类问题，而不是最初的回归任务。顺便提一下，在[第7章](ch07.html#ch07_optimization)中也提倡使用相同的技巧来将决策二元化，在那里可以采取一系列行动。
- en: 'Notice that while we can easily bucket any numbers into ordered groups, reverting
    the process to recover the original numeric labels may not be feasible. We might
    be tempted to approximate these by means of some statistic such as the average
    value in the category, but this is generally not recommended. As an example, take
    your customers’ schooling level, accurately measured as years of schooling. Imagine
    that for your business problem, you only care about predicting if your customers
    completed college or not. We can now use the college threshold and divide the
    world in two: a customer’s schooling level is either higher than completed college,
    or not. Note that later, if you try to revert the labeling, you may run into trouble
    as the strict numeric interpretation in regression might be broken. And most certainly,
    you should always avoid arbitrary relabeling of each category (say 1 for completed
    college and 0 otherwise) and using this in a *regression setting*, as the regression
    algorithm will literally treat the labels as numbers (and not metaphorically as
    wanted).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到，尽管我们可以将任何数字轻松分组成有序的组，但逆转这一过程以恢复原始的数值标签可能是不可行的。我们可能会尝试通过某些统计量（例如类别中的平均值）来近似这些值，但这通常是不建议的。举例来说，考虑你的客户的学历水平，精确测量为受教育年限。想象一下，针对你的业务问题，你只关心预测客户是否完成了大学教育。现在我们可以使用大学阈值将世界分为两部分：客户的学历水平要么高于完成大学教育，要么低于。请注意，如果稍后尝试恢复标签，可能会遇到问题，因为在回归中的严格数值解释可能会被打破。而且，绝对应避免对每个类别进行任意的重新标记（例如对完成大学教育的标记为1，否则为0），并在*回归设置*中使用这种方法，因为回归算法会将标签字面上视为数字（而不是象征性地）。
- en: Making Predictions
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**做预测**'
- en: Since current AI are prediction technologies, it is useful to get a sense of
    how prediction comes about. To this end let us start with a rather abstract, but
    general enough description of the task. Our objective is to predict an outcome
    <math alttext="y"><mi>y</mi></math> that we, humans, conjecture depends on inputs
    (also called features) <math alttext="x 1 comma x 2 comma ellipsis comma x Subscript
    upper K Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>K</mi></msub></mrow></math>
    . To get a prediction we need to transform inputs into outcome, which in mathematical
    terms can be described as a function <math alttext="y equals f left-parenthesis
    x 1 comma x 2 comma ellipsis comma x Subscript upper K Baseline right-parenthesis"><mrow><mi>y</mi>
    <mo>=</mo> <mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi>
    <mi>K</mi></msub> <mo>)</mo></mrow></math> . For instance, we may want to predict
    our next quarter’s business profits ( <math alttext="y"><mi>y</mi></math> ), and
    conjecture that they depend on the severity of the weather ( <math alttext="x
    1"><msub><mi>x</mi> <mn>1</mn></msub></math> ) and our projected labor costs for
    the quarter ( <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math> ).
    In this case we only have two features, so <math alttext="Profits equals f left-parenthesis
    severity comma labor right-parenthesis"><mrow><mtext>Profits</mtext> <mo>=</mo>
    <mi>f</mi> <mo>(</mo> <mtext>severity</mtext> <mo>,</mo> <mtext>labor</mtext>
    <mo>)</mo></mrow></math> .
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于当前的AI技术是预测技术，了解预测的产生方式非常有用。为此，让我们从一个相当抽象但足够一般的任务描述开始。我们的目标是预测一个结果<math alttext="y"><mi>y</mi></math>，我们人类推测它依赖于输入（也称为特征）<math
    alttext="x 1 comma x 2 comma ellipsis comma x Subscript upper K Baseline"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>K</mi></msub></mrow></math>。为了得到预测，我们需要将输入转化为结果，数学上可以描述为一个函数<math
    alttext="y equals f left-parenthesis x 1 comma x 2 comma ellipsis comma x Subscript
    upper K Baseline right-parenthesis"><mrow><mi>y</mi> <mo>=</mo> <mi>f</mi> <mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>K</mi></msub> <mo>)</mo></mrow></math>。例如，我们可能想预测下个季度的业务利润（<math
    alttext="y"><mi>y</mi></math>），并推测它们依赖于天气的严重程度（<math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math>）和季度内预计的劳动成本（<math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math>）。在这种情况下，我们只有两个特征，所以<math
    alttext="Profits equals f left-parenthesis severity comma labor right-parenthesis"><mrow><mtext>Profits</mtext>
    <mo>=</mo> <mi>f</mi> <mo>(</mo> <mtext>severity</mtext> <mo>,</mo> <mtext>labor</mtext>
    <mo>)</mo></mrow></math>。
- en: 'At this point, it is unclear where this function comes from, but notice that
    if we had such a function, prediction should be relatively straightforward: we
    just plug in the values of our inputs on the right-hand side of the equation and
    the output is our prediction. For instance, we can evaluate our prediction function
    for weather severity of 100 inches of rainfall and labor costs of $15,000 as <math
    alttext="f left-parenthesis 100 comma 15000 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mn>100</mn> <mo>,</mo> <mn>15000</mn> <mo>)</mo></mrow></math> . If
    we had a linear function <math alttext="2 comma 000 x 1 minus 2 x 2"><mrow><mn>2</mn>
    <mo>,</mo> <mn>000</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>-</mo> <mn>2</mn>
    <msub><mi>x</mi> <mn>2</mn></msub></mrow></math> , we plug in the values and get
    that the profits are 2,000 × 100 - 2 × 15,000 = $170K.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，不清楚这个函数从何而来，但请注意，如果我们有这样一个函数，预测应该相对简单：我们只需将输入的值插入方程式右侧，并输出我们的预测。例如，我们可以评估我们的预测函数，对于100英寸降雨和季度劳动成本为$15,000，如<math
    alttext="f left-parenthesis 100 comma 15000 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mn>100</mn> <mo>,</mo> <mn>15000</mn> <mo>)</mo></mrow></math>。如果我们有一个线性函数<math
    alttext="2 comma 000 x 1 minus 2 x 2"><mrow><mn>2</mn> <mo>,</mo> <mn>000</mn>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>-</mo> <mn>2</mn> <msub><mi>x</mi> <mn>2</mn></msub></mrow></math>，我们插入这些值，得到利润为2,000
    × 100 - 2 × 15,000 = $170K。
- en: Caveats to the Plug-in Approach
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 插件方法的注意事项
- en: There are two caveats to the simplicity of this approach. First, to make a prediction
    we must plug in the values of the features, so we are assuming we know these values.
    Going back to the example, to predict profits for the next quarter we must know
    and plug in the value for weather severity. Weather in which quarter? The current
    quarter or the next quarter? We might be able to measure the amount of rainfall
    for this quarter, but if what really matters is next quarter’s rainfall, then
    we must *make a prediction in order to make a prediction*. The moral of this story
    is that if we care about prediction, we must choose our inputs carefully to avoid
    this kind of circular dependence.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的简易性有两个要注意的地方。首先，为了进行预测，我们必须插入特征的值，所以我们假设我们知道这些值。回到例子中，要预测下一个季度的利润，我们必须知道并插入天气严重程度的值。是哪个季度的天气？当前季度还是下一个季度？我们可能能够测量本季度的降雨量，但如果真正重要的是下一个季度的降雨量，那么我们必须*预测以便预测*。这个故事的寓意是，如果我们关心预测，我们必须仔细选择我们的输入，以避免这种循环依赖。
- en: The second subtle point concerns making predictions for regression and classification
    problems. In the case of regression, the plug-in method works well since the function
    <math alttext="f left-parenthesis right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mo>)</mo></mrow></math> maps numeric features into a numeric outcome. This is
    the what we expect of mathematical functions. How, then, can we define functions
    that output labels of categories, say “dog,” “cat,” or “train”? The trick used
    in classification is to use functions that map numeric features into probabilities,
    numbers between 0 and 1 that quantify our degree of belief that the current example
    is of a given category. Thus, in classification tasks we generally predict *probability
    scores*, transformed into categories with the help of a *decision rule*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个微妙的问题涉及回归和分类问题的预测。在回归的情况下，插入方法效果很好，因为函数<math alttext="f left-parenthesis
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mo>)</mo></mrow></math>将数值特征映射到数值结果。这是我们对数学函数的期望。那么，我们如何定义输出标签的函数，比如“狗”，“猫”或“火车”？在分类中使用的技巧是使用将数值特征映射到概率的函数，即0到1之间的数字，量化我们对当前示例属于某一类别的信念程度。因此，在分类任务中，我们通常预测*概率分数*，并使用*决策规则*将其转换为类别。
- en: Using Supervised Learning as Input for Optimal Decision-Making
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用监督学习作为优化决策的输入
- en: 'I’ve said throughout that AI is one fundamental input in our decision-making
    processes. To understand how this works, recall from [Chapter 6](ch06.html#ch06_uncertainty)
    that when making decisions under uncertainty, we need to compare the expected
    value of our metric to be optimized and choose the lever that generates the maximum.
    The expected value for the case of two consequences is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直在说AI是我们决策过程中的一个基本输入。为了理解这是如何运作的，请回顾[第6章](ch06.html#ch06_uncertainty)，即在不确定性下做出决策时，我们需要将我们要优化的指标的期望值进行比较，并选择生成最大的杠杆。对于两种结果的情况，期望值为：
- en: <math alttext="upper E left-parenthesis x right-parenthesis equals p 1 x 1 plus
    p 2 x 2" display="block"><mrow><mi>E</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>p</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>p</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub></mrow></math>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper E left-parenthesis x right-parenthesis equals p 1 x 1 plus
    p 2 x 2" display="block"><mrow><mi>E</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>p</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>p</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub></mrow></math>
- en: Classification models provide the probability estimates ( <math alttext="p 1
    comma p 2"><mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>p</mi>
    <mn>2</mn></msub></mrow></math> ). Alternatively, you may sometimes prefer to
    use regression directly and estimate the expected utility for each of your levers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型提供概率估计（<math alttext="p 1 comma p 2"><mrow><msub><mi>p</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>p</mi> <mn>2</mn></msub></mrow></math>）。或者，有时您可能更愿意直接使用回归，并为每个杠杆估计预期效用。
- en: Where Do These Functions Come From?
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这些函数从哪里来？
- en: Now that we know how to make predictions when we have at hand a function that
    maps inputs into our outcome, the final question is where do these functions come
    about? Going back to our profits example, how exactly do profits vary with changes
    in rainfall or labor costs?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何在手头有一个将输入映射到结果的函数时进行预测，最后一个问题是这些函数是从哪里来的？回到我们的利润例子，利润如何随降雨或劳动力成本的变化而变化？
- en: The data-driven approach uses supervised algorithms to fit the function that
    provides the best prediction without imposing any other restrictions. This is
    by far the most common approach among practitioners. In contrast, the model-driven
    approach starts from first principles (in some cases without even looking at the
    data) and imposes restrictions on what type of functions are allowed. This latter
    approach is mostly used by econometricians and in the industry it’s quite uncommon
    to see. The main advantage of the latter approach is that it provides *interpretable*
    predictions since theories are built from the ground up. But predictive power
    is generally lower too.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动方法使用监督算法来拟合最佳预测函数，而不施加任何其他限制。这是从业者中最常见的方法。相比之下，模型驱动方法从第一原则出发（有时甚至不看数据）并对允许的函数类型施加限制。后一种方法主要被计量经济学家和工业界使用，而在业界中很少见。后一种方法的主要优势在于提供*可解释*的预测，因为理论是从头开始建立的。但预测能力通常也较低。
- en: This trade-off between interpretability and predictive power is pervasive in
    ML and has several important consequences. First, there may be ethical considerations
    when we use predictive algorithms to assign offers or make decisions and cannot
    explain why the decision was made (think of problems like “should I hire that
    person?” or “is the defendant guilty or not guilty?”). Ethical considerations
    were discussed in depth in [Chapter 8](ch08.html#ch08_wrappingup). Second, this
    lack of transparency can also be a barrier to the widespread adoption of these
    methods in our businesses. Human beings like to understand *why* things happen
    before making choices. If data scientists cannot explain *why*, their business
    counterparts may prefer to keep doing things the traditional way, even if the
    predictive solution is superior.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，解释性与预测能力之间的这种权衡普遍存在，并具有几个重要的后果。首先，当我们使用预测算法分配优惠或做出决策时，并且无法解释为何做出了这个决定时（想象一下类似“我应该雇佣那个人吗？”或“被告有罪还是无罪？”的问题），可能涉及伦理考虑。伦理问题在[第8章](ch08.html#ch08_wrappingup)中有详细讨论。其次，这种透明度的缺乏也可能成为我们业务中广泛采用这些方法的障碍。人类喜欢在做出选择之前理解*为什么*事情会发生。如果数据科学家无法解释*为什么*，他们的业务伙伴可能更愿意继续以传统方式做事，即使预测解决方案更优。
- en: Making Good Predictions
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 做出良好的预测
- en: By now I hope I convinced you that making predictions is relatively straightforward
    if we are armed with a function that maps inputs into outcome and we know the
    values for the features. But how do we make *good* predictions? After all, I can
    predict that this book is going to be a best-seller, but this can be way off target.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我希望我已经说服了你，如果我们有一个将输入映射为结果的函数，并且我们知道特征的值，那么做出预测相对简单。但是，我们如何做出*好*的预测呢？毕竟，我可以预测这本书会成为畅销书，但这可能大错特错。
- en: Unfortunately there is no magic recipe for making good predictions. There are,
    however, some good practices we can follow. The first one is that fitting a function
    to our data is not the same as making good predictions. As a matter of fact, by
    memorizing the actual dataset we get a perfect prediction over this data. The
    key to making good predictions is the ability to *generalize* whenever we receive
    new data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，关于做出良好预测的魔法配方并不存在。但是，我们可以遵循一些良好的实践。首先，将函数拟合到我们的数据并不等同于做出良好的预测。事实上，通过记忆实际数据集，我们可以在这些数据上获得完美的预测。做出良好预测的关键在于在收到新数据时能够*泛化*。
- en: Data scientists usually tackle this problem by randomly dividing the dataset
    into *training* and *test* samples. They first use their algorithms to fit a best
    function over the training sample and later evaluate the quality of the prediction
    on the test set. If we focus too much on the training set we might end up *overfitting*,
    which is a sign of bad generalization in the real world. Notice that in practice
    we are simulating the “real world” by holding out a subset of the data—the test
    sample. The quality of this simulation is usually good, unless the world has meaningfully
    (for the prediction problem) changed between the time we trained the model and
    time we make a prediction. AI is usually criticized for its lack of ability to
    generalize in these *nonstationary* settings. We will later encounter an example
    where overfitting hinders our ability to make good extrapolations to new data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家通常通过随机将数据集分成*训练*和*测试*样本来解决这个问题。他们首先使用他们的算法在训练样本上拟合最佳函数，然后在测试集上评估预测的质量。如果我们过于关注训练集，我们可能会*过拟合*，这是在现实世界中泛化能力差的表现。请注意，在实践中，我们通过保留数据的子集——测试样本来模拟“真实世界”。这种模拟的质量通常很好，除非在我们训练模型和进行预测之间，世界在预测问题上发生了有意义的变化。AI通常因其在这些*非静态*环境中泛化能力不足而受到批评。我们稍后将遇到一个例子，过拟合会阻碍我们对新数据进行良好外推的能力。
- en: Another good practice is to ensure that our estimates are robust to small variations
    of the data. This comes in several flavors. For instance, we can exclude anomalous
    observations in the training phase as some algorithms are extremely sensitive
    to outliers.^([4](app01.html#idm46359801628808)) We can also use algorithms that
    are robust to such datasets, for instance, by averaging or smoothing predictions
    from different models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个好的实践是确保我们的估计对数据的小变化具有鲁棒性。这有几种不同的方式。例如，在训练阶段可以排除异常观测值，因为某些算法对离群值非常敏感。^([4](app01.html#idm46359801628808))
    我们也可以使用对这类数据集具有鲁棒性的算法，例如通过对不同模型的预测进行平均或平滑处理。
- en: From Linear Regression to Deep Learning
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从线性回归到深度学习
- en: 'Humans have been using different prediction techniques since the beginning
    of time, but deep learning is the state of the art and the reason why everybody
    is currently talking about AI. While this is not the place to delve into the technical
    details of deep artificial neural networks (ANN), it is nonetheless useful to
    give an intuitive explanation of how these algorithms work, at least for demystification
    purposes. To do so we better start with the simplest kind of predictive algorithm:
    the linear regression.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 自时间开始以来，人类一直在使用不同的预测技术，但深度学习是最先进的技术，也是当前人们谈论AI的原因。虽然这里不是深入讨论深度人工神经网络（ANN）的技术细节的地方，但至少对于解密的目的，给出这些算法如何工作的直观解释仍然是有用的。为了这样做，我们最好从最简单的预测算法开始：线性回归。
- en: Linear Regression
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'In a linear regression we model a quantitative output <math alttext="y"><mi>y</mi></math>
    as a function of different inputs or features <math alttext="x 1 comma x 2 comma
    ellipsis comma x Subscript upper K Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>x</mi> <mi>K</mi></msub></mrow></math> , but we restrict ourselves to
    linear representations, that is: <math alttext="y equals alpha 0 plus alpha 1
    x 1 plus alpha 2 x 2 plus ellipsis plus alpha Subscript upper K Baseline x Subscript
    upper K"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>α</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>α</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <msub><mi>α</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo>
    <mo>⋯</mo> <mo>+</mo> <msub><mi>α</mi> <mi>K</mi></msub> <msub><mi>x</mi> <mi>K</mi></msub></mrow></math>
    . Notice how the choice of the algorithm restricts the class of functions we can
    fit to the data. In this case, the learning task is to find weights or coefficients
    <math alttext="alpha Subscript k"><msub><mi>α</mi> <mi>k</mi></msub></math> such
    that we approximate our data well, and most importantly, that we can generalize
    new data as well.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，我们将一个定量输出 <math alttext="y"><mi>y</mi></math> 建模为不同输入或特征 <math alttext="x
    1 comma x 2 comma ellipsis comma x Subscript upper K Baseline"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>K</mi></msub></mrow></math> 的函数，但我们限制自己于线性表示，即：
    <math alttext="y equals alpha 0 plus alpha 1 x 1 plus alpha 2 x 2 plus ellipsis
    plus alpha Subscript upper K Baseline x Subscript upper K"><mrow><mi>y</mi> <mo>=</mo>
    <msub><mi>α</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>α</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>α</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>α</mi>
    <mi>K</mi></msub> <msub><mi>x</mi> <mi>K</mi></msub></mrow></math> 。注意算法选择如何限制我们能够拟合数据的函数类。在这种情况下，学习任务是找到权重或系数
    <math alttext="alpha Subscript k"><msub><mi>α</mi> <mi>k</mi></msub></math>，使我们能够很好地逼近我们的数据，并且最重要的是，能够对新数据进行泛化。
- en: 'Imagine that we want to open a new store, but given alternative locations,
    we want to find the one that maximizes the return on investment (ROI). If we could
    predict profits for alternative locations, this would be a rather easy task: given
    our budget, among different locations we would open the store with higher expected
    profits.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们想要开一家新店，但是考虑到不同的位置，我们希望找到能够最大化投资回报率（ROI）的位置。如果我们能够预测各个位置的利润，这将是一项相当容易的任务：在预算范围内，在不同位置中选择预期利润更高的位置开店。
- en: 'Since profits are revenues minus the cost, the ideal dataset should include
    these two quantities for all possible locations. As you might imagine, such a
    dataset may be difficult to come by. Let’s say that we were able to get ahold
    of a dataset that tells us the number of passersby on any given street in our
    city between 9am and noon. As a starting point, it feels safe to conjecture that
    more passersby generate higher profits. Our rationale could be as follows: our
    revenue is derived from sales, which are themselves associated with people that
    enter our store and end up buying something. The likelihood of making a sale is
    rather small if no one walks by. On the other hand, if some people happen to pass
    by, with some luck our shopfront may attract some fraction who might end up buying.
    This takes care of revenues, but what about costs? It is likely that other store
    managers think like us, so we may have to compete for the rent, thereby increasing
    our costs. Notice that this story only talks about what we believe are plausible
    *directions* of the effects and not the actual *magnitudes*; we will let the algorithms
    fit the data and check whether the results match our intuition.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于利润等于收入减去成本，理想的数据集应包括所有可能位置的这两个量。正如您可以想象的那样，这样的数据集可能难以获得。假设我们能够获得一个数据集，告诉我们在我们城市的任何街道上午9点到中午之间经过的人数。作为一个起点，我们可以安全地推测，更多的过路人会带来更高的利润。我们的推理可能是这样的：我们的收入来自于销售，销售本身与进入我们店铺并最终购买东西的人有关。如果没有人经过，我们很难做出销售。另一方面，如果一些人经过，有些人可能会进店并购买。这解决了收入问题，但成本呢？其他店铺经理可能也像我们一样思考，因此我们可能需要竞争租金，从而增加成本。请注意，这个故事只谈论我们认为可能的影响*方向*，而不是实际的*大小*；我们将让算法拟合数据，并检查结果是否与我们的直觉一致。
- en: 'A simple linear model could then pose that profits ( <math alttext="y"><mi>y</mi></math>
    ) are increasing with the number of passersby ( <math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math> ), or <math alttext="y equals alpha 0 plus alpha 1 x
    1"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>α</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>α</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub></mrow></math> . Note that
    we, the human experts, have decided three things: (1) the outcome we want to predict
    is profits for each store; (2) profits depend *only* on the number of passersby
    at each location, a notable simplification; and (3) we restrict ourselves to a
    linear world, another notable simplification. In linear regression we then ask:
    what are the parameters <math alttext="alpha 0 comma alpha 1"><mrow><msub><mi>α</mi>
    <mn>0</mn></msub> <mo>,</mo> <msub><mi>α</mi> <mn>1</mn></msub></mrow></math>
    that make our *prediction most accurate*?'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的线性模型可以假设利润（<math alttext="y"><mi>y</mi></math>）随过路人数（<math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math>）的增加而增加，即 <math alttext="y equals alpha 0 plus alpha 1
    x 1"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>α</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>α</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub></mrow></math>
    。请注意，我们人类专家已经决定了三件事：（1）我们想要预测的结果是每家商店的利润；（2）利润仅依赖于每个位置的过路人数，这是一个显著的简化；（3）我们限制自己在一个线性世界中，这是另一个显著的简化。在线性回归中，我们要问：哪些参数
    <math alttext="alpha 0 comma alpha 1"><mrow><msub><mi>α</mi> <mn>0</mn></msub>
    <mo>,</mo> <msub><mi>α</mi> <mn>1</mn></msub></mrow></math> 能使我们的 *预测最准确*？
- en: '[Figure A-2](#fig_ch01_scatterplot_stores) shows a scatterplot of profits as
    function of passersby for 10 hypothetical stores currently operating for our company.
    We can immediately see that there is an apparent trend in the data as higher profits
    are associated with more passersby.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 A-2](#fig_ch01_scatterplot_stores) 显示了我们公司目前运营的10家假设商店的利润与过路人数的散点图。我们可以立即看到数据中存在明显的趋势，即更高的利润与更多的过路人相关联。'
- en: '![Profits and passersby for 10 hypothetical stores](assets/asai_aa02.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![10家假设商店的利润和过路人数](assets/asai_aa02.png)'
- en: Figure A-2\. Profits and passersby for 10 hypothetical stores
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: A-2\. 10家假设商店的利润和过路人数
- en: The least-squares method is the most common solution of what we mean by “the
    prediction being most accurate.” Under this loss function the algorithm finds
    parameters that minimize the average error, as seen in [Figure A-3](#fig_ch01_error_stores).^([5](app01.html#idm46359801547704))
    The prediction under this linear model is given by the dashed line, and errors—depicted
    with vertical lines—can be either positive or negative. For store 10, for example,
    our prediction is almost $2K dollars below the actual number, and the error is
    positive. For stores 2, 5, 6, and 9, on the other hand, our linear model overestimates
    actual profits given their number of passersby. At the bottom of the plot the
    actual predicted equation is presented. For this dataset, the algorithm found
    that <math alttext="alpha 0 equals 0.9"><mrow><msub><mi>α</mi> <mn>0</mn></msub>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>9</mn></mrow></math> and <math alttext="alpha
    1 equals 0.04"><mrow><msub><mi>α</mi> <mn>1</mn></msub> <mo>=</mo> <mn>0</mn>
    <mo>.</mo> <mn>04</mn></mrow></math> . The details of how this happened are beyond
    the scope of this book, but let me just point out that the algorithm tries different
    possible values in a way that reduces the amount of computation. A naive alternative
    is to try many different random possible combinations, but since this set is huge,
    it would take years for us to find the best possible values.^([6](app01.html#idm46359801537416))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘法是我们所说的“预测最准确”的最常见解决方案。在这种损失函数下，算法找到能使平均误差最小化的参数，如 [图 A-3](#fig_ch01_error_stores)
    所示。这一线性模型下的预测由虚线表示，误差用垂直线表示，可以是正数也可以是负数。例如，对于商店10，我们的预测几乎比实际数额低了2K美元，误差是正数。另一方面，对于商店2、5、6和9，我们的线性模型高估了实际利润与他们的过路人数相比。在图表底部展示了实际的预测方程式。对于这个数据集，算法发现
    <math alttext="alpha 0 equals 0.9"><mrow><msub><mi>α</mi> <mn>0</mn></msub> <mo>=</mo>
    <mn>0</mn> <mo>.</mo> <mn>9</mn></mrow></math> 和 <math alttext="alpha 1 equals
    0.04"><mrow><msub><mi>α</mi> <mn>1</mn></msub> <mo>=</mo> <mn>0</mn> <mo>.</mo>
    <mn>04</mn></mrow></math> 。这是如何发生的细节超出了本书的范围，但我想指出的是，该算法尝试不同的可能值，以减少计算量。一个天真的替代方案是尝试许多不同的随机组合，但由于这个集合非常庞大，我们将需要多年时间才能找到最佳的可能值。^([6](app01.html#idm46359801537416))
- en: '![Prediction errors in linear regression](assets/asai_aa03.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归中的预测误差](assets/asai_aa03.png)'
- en: Figure A-3\. Prediction errors in linear regression
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: A-3\. 线性回归中的预测误差
- en: 'This equation is not just an intellectual curiosity for the data scientists
    that took pains to compute it; it actually provides some interpretation of the
    plausible mechanism that converts passersby into profits: each additional 100
    people that walk through the location “generate” $40 in profits. We discussed
    the risks of interpreting these results causally in [Chapter 2](ch02.html#ch02_introanalytics),
    but for now it is important to highlight that this mathematical object may have
    substantial business content, the type of content necessary to create value using
    AI, as it allows us to interpret our results in a very simple way. As discussed
    previously, linear regression is on one extreme of the predictive power versus
    interpretability spectrum, making it a common choice of algorithm if understanding
    and explainability are sufficiently appreciated.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程不仅仅是对计算它费心的数据科学家的一个智力好奇，它实际上提供了一些将过路人转化为利润的可能机制的解释：每多经过100人的地点，“生成”40美元的利润。我们在[第2章](ch02.html#ch02_introanalytics)中讨论了解释这些结果具有因果关系的风险，但现在重要的是强调，这个数学对象可能具有重要的商业内容，这种内容是使用AI创造价值所必需的，因为它使我们能够以非常简单的方式解释我们的结果。正如之前讨论的那样，线性回归处于预测能力与解释性的极端之一，如果理解和可解释性得到足够重视，这是一种常见的算法选择。
- en: 'Going back to our results, notice that the prediction is really good for some
    stores and rather lousy for others: our linear model gave almost perfect predictions
    for stores 1, 3, 5, 7, and 8, but did poorly for stores 9 and 10\. One property
    of the least squares solution is that the average error is zero,^([7](app01.html#idm46359801525560))
    so positive and negative errors end up balancing out.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的结果，注意对于某些商店预测非常好，而对于其他商店则相当糟糕：我们的线性模型对1、3、5、7和8号商店几乎给出了完美的预测，但对于9号和10号商店则表现不佳。最小二乘解的一个特性是平均误差为零，^([7](app01.html#idm46359801525560))
    所以正误差和负误差最终会互相抵消。
- en: '![Fitting a quadratic equation ](assets/asai_aa04.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![拟合二次方程 ](assets/asai_aa04.png)'
- en: Figure A-4\. Fitting a quadratic equation
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A-4\. 拟合二次方程
- en: Controlling for other variables
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制其他变量
- en: Linear regression is great to capture some intuition about relationships found
    in the data. In my opinion, one of the most important results in linear regression
    is known as the [*Frisch–Waugh–Lovell theorem* (FWL)](https://oreil.ly/WHtde)
    and states that by including control variables (confounders in the language of
    [Chapter 2](ch02.html#ch02_introanalytics)) in the regression, we estimate the
    net effect of a variable of interest.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归非常适合捕捉数据中发现的一些关系的直觉。在我看来，线性回归中最重要的结果之一被称为[*弗里斯-沃-洛弗尔定理* (FWL)](https://oreil.ly/WHtde)，它指出通过在回归中包含控制变量（在[第2章](ch02.html#ch02_introanalytics)中称为混淆因素）来估计感兴趣变量的净效应。
- en: To see this in action, suppose that you suspect that the results in the previous
    regression may be contaminated by the fact that there are more potential customers
    in highly commercial neighborhoods, so your lower profits may be related to price
    effects in highly competitive neighborhoods and not to the sheer volume of customers.
    This third variable could act as a confounder of the pure volume effect you were
    trying to estimate.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这一点的作用，假设您怀疑在先前的回归中的结果可能受到高商业化社区潜在顾客更多的影响，因此您的低利润可能与高竞争社区中的价格效应有关，而不是仅仅与顾客数量有关。这第三个变量可以作为您试图估计的纯量效应的混杂物。
- en: 'You can control for this effect by including the extra variable—number of commercial
    establishments in the neighborhood—in your linear regression:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将额外变量——附近商业机构数量——包含在您的线性回归中来控制此效应：
- en: <math alttext="y Subscript i Baseline equals alpha 0 plus alpha 1 x Subscript
    1 i Baseline plus alpha 2 x Subscript 2 i Baseline plus epsilon Subscript i" display="block"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>α</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>α</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mrow><mn>1</mn><mi>i</mi></mrow></msub> <mo>+</mo>
    <msub><mi>α</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mrow><mn>2</mn><mi>i</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ϵ</mi> <mi>i</mi></msub></mrow></math>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y Subscript i Baseline equals alpha 0 plus alpha 1 x Subscript
    1 i Baseline plus alpha 2 x Subscript 2 i Baseline plus epsilon Subscript i" display="block"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>α</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>α</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mrow><mn>1</mn><mi>i</mi></mrow></msub> <mo>+</mo>
    <msub><mi>α</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mrow><mn>2</mn><mi>i</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ϵ</mi> <mi>i</mi></msub></mrow></math>
- en: As before, <math alttext="x Subscript 1 i"><msub><mi>x</mi> <mrow><mn>1</mn><mi>i</mi></mrow></msub></math>
    denotes the number of passersby per period of time and <math alttext="x Subscript
    2 i"><msub><mi>x</mi> <mrow><mn>2</mn><mi>i</mi></mrow></msub></math> represents
    the number of different stores in the neighborhood, both for store <math alttext="i"><mi>i</mi></math>
    .
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，<math alttext="x Subscript 1 i"><msub><mi>x</mi> <mrow><mn>1</mn><mi>i</mi></mrow></msub></math>
    表示每个时间段的过路人数，<math alttext="x Subscript 2 i"><msub><mi>x</mi> <mrow><mn>2</mn><mi>i</mi></mrow></msub></math>
    表示附近不同商店的数量，都是店铺 <math alttext="i"><mi>i</mi></math> 的属性。
- en: 'The FWL theorem says that if you run this regression, the estimated coefficient
    for the number of passersby (or stores in the neighborhood) is *net* of any other
    control variables included. You can actually check that you get the *exact same
    result* by estimating the following three alternative regressions:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: FWL 定理指出，如果你运行这个回归，通过包括其他控制变量，路人（或邻里商店）的估计系数是净的。你实际上可以通过估计以下三种备选回归来验证得到*完全相同的结果*：
- en: Regress <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math>
    on <math alttext="x Subscript 2 i"><msub><mi>x</mi> <mrow><mn>2</mn><mi>i</mi></mrow></msub></math>
    (and a constant) and save the residuals (call them <math alttext="eta Subscript
    i"><msub><mi>η</mi> <mi>i</mi></msub></math> ).
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 <math alttext="y Subscript i"><msub><mi>y</mi> <mi>i</mi></msub></math> 进行回归，包括
    <math alttext="x Subscript 2 i"><msub><mi>x</mi> <mrow><mn>2</mn><mi>i</mi></mrow></msub></math>（以及一个常数），并保存残差（称为
    <math alttext="eta Subscript i"><msub><mi>η</mi> <mi>i</mi></msub></math>）。
- en: Regress <math alttext="x Subscript 1 i"><msub><mi>x</mi> <mrow><mn>1</mn><mi>i</mi></mrow></msub></math>
    on <math alttext="x Subscript 2 i"><msub><mi>x</mi> <mrow><mn>2</mn><mi>i</mi></mrow></msub></math>
    (and a constant) and save the residuals (call them <math alttext="nu Subscript
    i"><msub><mi>ν</mi> <mi>i</mi></msub></math> ).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 <math alttext="x Subscript 1 i"><msub><mi>x</mi> <mrow><mn>1</mn><mi>i</mi></mrow></msub></math>
    进行回归，包括 <math alttext="x Subscript 2 i"><msub><mi>x</mi> <mrow><mn>2</mn><mi>i</mi></mrow></msub></math>（以及一个常数），并保存残差（称为
    <math alttext="nu Subscript i"><msub><mi>ν</mi> <mi>i</mi></msub></math>）。
- en: Finally, regress <math alttext="eta Subscript i"><msub><mi>η</mi> <mi>i</mi></msub></math>
    on <math alttext="nu Subscript i"><msub><mi>ν</mi> <mi>i</mi></msub></math> (and
    a constant). The corresponding coefficient for <math alttext="nu Subscript i"><msub><mi>ν</mi>
    <mi>i</mi></msub></math> is exactly the same as the estimated one from the longer
    regression <math alttext="ModifyingAbove alpha With caret Subscript 2"><msub><mover
    accent="true"><mi>α</mi> <mo>^</mo></mover> <mn>2</mn></msub></math> .
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，对 <math alttext="eta Subscript i"><msub><mi>η</mi> <mi>i</msub></math> 进行回归，包括
    <math alttext="nu Subscript i"><msub><mi>ν</mi> <mi>i</msub></math>（以及一个常数）。与更长的回归中估计得到的相同，<math
    alttext="ModifyingAbove alpha With caret Subscript 2"><msub><mover accent="true"><mi>α</mi>
    <mo>^</mo></mover> <mn>2</mn></msub></math> 的相应系数完全相同。
- en: Note that in steps 1 and 2 you’re *cleaning* the variables of interest of the
    impact that the confounder may have. Once you have these new variables (the corresponding
    residuals), you can run a simple bivariate regression to get the desired estimate.
    The book’s Github repository provides a demo of the theorem in action as well
    as some additional details.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在步骤 1 和 2 中，你正在*清理*感兴趣的变量受混淆变量影响的影响。一旦你有了这些新变量（相应的残差），你可以运行一个简单的双变量回归以获得所需的估计值。该书的
    Github 仓库提供了该定理的演示以及一些额外的细节。
- en: The very nice conclusion is that you can get net effects by including the controls
    or confounders in the same regression.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 非常好的结论是，通过在同一回归中包括控制变量或混杂因素，你可以得到净效应。
- en: Overfitting
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过度拟合
- en: '[Figure A-5](#fig_ch01_overfitting_regression_stores) shows the result from
    fitting a linear regression that includes a polynomial of order 6\. The top panel
    shows that this function fits the training data outstandingly, don’t you think?
    However, remember that we want to generalize to new data. The bottom panel shows
    the prediction on our test data consisting of 10 new stores that were *not* used
    in the training stage. It appears that the fit is not as good as on the test set,
    though. To check for overfitting we would actually need to plot the prediction
    error for both the training and test data as a function of the degree of the polynomial.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 A-5](#fig_ch01_overfitting_regression_stores) 显示了拟合包括 6 次多项式的线性回归的结果。顶部面板显示这个函数非常适合训练数据，你认为呢？然而，请记住我们希望推广到新数据。底部面板显示了我们的测试数据的预测结果，包括
    10 家新商店，这些商店在训练阶段*未*使用。看起来拟合效果并不如测试集上好。要检查是否过度拟合，我们实际上需要绘制训练数据和测试数据的预测误差作为多项式次数的函数。'
- en: '![Overfitting](assets/asai_aa05.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![过度拟合](assets/asai_aa05.png)'
- en: Figure A-5\. A case of overfitting
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A-5\. 过度拟合的一个案例
- en: '[Figure A-6](#fig1_test_train_mse_stores) depicts the average prediction error
    for both the training and test data, as a function of the flexibility we allow
    in the linear regression in terms of the degree of the polynomial. As you can
    see, in the training data we are always doing a little better, with errors consistently
    falling with higher degrees. This is always the case with training data and is
    the reason why data scientists evaluate the quality of their prediction on another
    set. For test data, there is evidence of overfitting with polynomials of degree
    3 or higher: the lowest prediction error is obtained with polynomial of degree
    2, and afterward it starts increasing, showing the lower ability to generalize.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 A-6](#fig1_test_train_mse_stores) 描述了在训练和测试数据上的平均预测误差，作为我们允许的线性回归灵活性的函数，即多项式的次数。正如你所看到的，在训练数据中，我们总是做得更好，错误随着更高次数的多项式而持续减少。这总是在训练数据中发生的情况，这也是数据科学家在另一套数据上评估他们预测质量的原因。对于测试数据，有过拟合的证据，出现
    3 次或更高次多项式：最低的预测误差是通过 2 次多项式得到的，然后开始增加，显示了较低的泛化能力。'
- en: '![Test and train MSE](assets/asai_aa06.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![测试和训练MSE](assets/asai_aa06.png)'
- en: Figure A-6\. Prediction error for training and test samples
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A-6\. 训练样本和测试样本的预测误差
- en: Neural Networks
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'To introduce neural networks, let me start by drawing a somewhat peculiar diagram
    for linear regression just for the purpose of building a bridge between the two
    techniques. [Figure A-7](#fig1_regression_asa_net) shows circular nodes that are
    connected though edges or lines; the strength of each connection is given by weights
    <math alttext="alpha Subscript k"><msub><mi>α</mi> <mi>k</mi></msub></math> .
    This collection of nodes and the corresponding edges is called a network, and
    it provides a way to visualize the relationship between inputs and the output:
    <math alttext="upper X 1"><msub><mi>X</mi> <mn>1</mn></msub></math> has an effect
    on the output of magnitude <math alttext="alpha 1"><msub><mi>α</mi> <mn>1</mn></msub></math>
    , and similarly for <math alttext="upper X 2"><msub><mi>X</mi> <mn>2</mn></msub></math>
    . In the discussion that follows, keep in mind that the learning algorithm in
    the case of linear regression finds the set of weights that minimize the prediction
    error, as *this will also be the case for deep neural nets*.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍神经网络，让我从绘制一个略显奇特的线性回归图开始，仅仅是为了在两种技术之间建立桥梁。[图 A-7](#fig1_regression_asa_net)
    显示通过边缘或线条连接的圆形节点；每个连接的强度由权重 <math alttext="alpha 下标 k"><msub><mi>α</mi> <mi>k</mi></msub></math>
    给出。这个节点和相应的边被称为网络，它提供了一种可视化输入和输出之间关系的方法： <math alttext="上标 X 1"><msub><mi>X</mi>
    <mn>1</mn></msub></math> 对输出产生大小为 <math alttext="alpha 1"><msub><mi>α</mi> <mn>1</mn></msub></math>
    的影响，对于 <math alttext="上标 X 2"><msub><mi>X</mi> <mn>2</mn></msub></math> 也是如此。在接下来的讨论中，请记住，在线性回归案例中，学习算法找到了最小化预测误差的权重集合，*深度神经网络也是如此*。
- en: '![regression as a net](assets/asai_aa07.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归为网络](assets/asai_aa07.png)'
- en: Figure A-7\. Linear regression depicted as a network
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A-7\. 线性回归示为一个网络
- en: In regression, since <math alttext="y equals alpha 0 plus alpha 1 upper X 1
    plus alpha 2 upper X 2"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>α</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>α</mi> <mn>1</mn></msub> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>α</mi> <mn>2</mn></msub> <msub><mi>X</mi> <mn>2</mn></msub></mrow></math>
    the quantity on the righthand side maps one-to-one to outcome <math alttext="y"><mi>y</mi></math>
    . For instance, if <math alttext="alpha 1 equals 1"><mrow><msub><mi>α</mi> <mn>1</mn></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> , <math alttext="alpha 2 equals 0"><mrow><msub><mi>α</mi>
    <mn>2</mn></msub> <mo>=</mo> <mn>0</mn></mrow></math> and there is no intercept
    or bias, if the corresponding inputs have values 2 and 1, thanks to the equality,
    we know that <math alttext="y equals 2"><mrow><mi>y</mi> <mo>=</mo> <mn>2</mn></mrow></math>
    . This is one of the properties that linearity buys for us. But we can imagine
    having something more general, say <math alttext="y equals g left-parenthesis
    alpha 0 plus alpha 1 upper X 1 plus alpha 2 upper X 2 right-parenthesis"><mrow><mi>y</mi>
    <mo>=</mo> <mi>g</mi> <mo>(</mo> <msub><mi>α</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>α</mi> <mn>1</mn></msub> <msub><mi>X</mi> <mn>1</mn></msub> <mo>+</mo>
    <msub><mi>α</mi> <mn>2</mn></msub> <msub><mi>X</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math>
    for some function <math alttext="g left-parenthesis right-parenthesis"><mrow><mi>g</mi>
    <mo>(</mo> <mo>)</mo></mrow></math> . This would most likely take us out of the
    realm of linear regression, but depending on the problem it may fit the data better,
    and hopefully also generalize better to new data.^([8](app01.html#idm46359801387880))
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中，由于<math alttext="y equals alpha 0 plus alpha 1 upper X 1 plus alpha 2 upper
    X 2"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>α</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>α</mi> <mn>1</mn></msub> <msub><mi>X</mi> <mn>1</mn></msub> <mo>+</mo>
    <msub><mi>α</mi> <mn>2</mn></msub> <msub><mi>X</mi> <mn>2</mn></msub></mrow></math>右侧的数量与结果<math
    alttext="y"><mi>y</mi></math>一一映射。例如，如果<math alttext="alpha 1 equals 1"><mrow><msub><mi>α</mi>
    <mn>1</mn></msub> <mo>=</mo> <mn>1</mn></mrow></math>，<math alttext="alpha 2 equals
    0"><mrow><msub><mi>α</mi> <mn>2</mn></msub> <mo>=</mo> <mn>0</mn></mrow></math>且没有截距或偏置，如果相应的输入值为2和1，由于等式，我们知道<math
    alttext="y equals 2"><mrow><mi>y</mi> <mo>=</mo> <mn>2</mn></mrow></math>。这是线性性质之一。但是我们可以想象有更一般的情况，比如<math
    alttext="y equals g left-parenthesis alpha 0 plus alpha 1 upper X 1 plus alpha
    2 upper X 2 right-parenthesis"><mrow><mi>y</mi> <mo>=</mo> <mi>g</mi> <mo>(</mo>
    <msub><mi>α</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>α</mi> <mn>1</mn></msub>
    <msub><mi>X</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>α</mi> <mn>2</mn></msub>
    <msub><mi>X</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math>对于某些函数<math alttext="g
    left-parenthesis right-parenthesis"><mrow><mi>g</mi> <mo>(</mo> <mo>)</mo></mrow></math>。这很可能使我们摆脱线性回归的领域，但根据问题，它可能更适合数据，并希望对新数据也有更好的泛化能力。^([8](app01.html#idm46359801387880))
- en: 'Activation functions: adding some extra nonlinearity'
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数：增加一些额外的非线性效果
- en: '*Activation* or *transfer* functions are one such class of functions, and are
    commonly used in deep learning (DL). The implicit activation function in linear
    regression is, of course, linear (see [Figure A-8](#fig1_activation_functions)).
    But there are several more interesting alternatives that have become essential
    for practitioners. In the rectified linear unit (ReLU) case, output is zero when
    <math alttext="alpha 0 plus alpha 1 upper X 1 plus alpha 2 upper X 2"><mrow><msub><mi>α</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>α</mi> <mn>1</mn></msub> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>α</mi> <mn>2</mn></msub> <msub><mi>X</mi>
    <mn>2</mn></msub></mrow></math> is negative or zero, and for positive values we
    are again in the world of linear regression. Notice how the function gets activated
    only when the joint effort is positive, hence its name. A smoother version is
    the sigmoid activation. Activation functions are another way to include nonlinear
    effects in a predictive model. They were first introduced to capture our understanding
    of how neurons in our brain fire, but today we use them because they improve the
    predictive power of our models.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*激活*或*传递*函数是这样一类函数，通常用于深度学习（DL）。线性回归中的隐式激活函数当然是线性的（见[图 A-8](#fig1_activation_functions)）。但是有几种更有趣的替代方法，已经成为从业者的必备工具。在修正线性单元（ReLU）的情况下，当<math
    alttext="alpha 0 plus alpha 1 upper X 1 plus alpha 2 upper X 2"><mrow><msub><mi>α</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>α</mi> <mn>1</mn></msub> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>α</mi> <mn>2</mn></msub> <msub><mi>X</mi>
    <mn>2</mn></msub></mrow></math>为负或零时，输出为零，对于正值，我们又回到线性回归的世界。请注意，这个函数仅在联合作用为正时被激活，因此得名。更平滑的版本是
    S 型激活。激活函数是在预测模型中引入非线性效果的另一种方式。它们最初被引入是为了捕捉我们对大脑中神经元如何激活的理解，但今天我们使用它们是因为它们提高了我们模型的预测能力。'
- en: '![activation functions](assets/asai_aa08.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数](assets/asai_aa08.png)'
- en: Figure A-8\. Different transfer or activation functions
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A-8\. 不同的转移或激活函数
- en: With these tools we are ready to introduce neural networks. A neural network
    is a set of nodes and edges or connections between the nodes like the one depicted
    in [Figure A-9](#fig1_neural_nets_examples). The left panel shows a network with
    only two inputs, each of which affects two hidden or intermediate nodes, the strength
    mediated by the corresponding parameters. We may decide to use nonlinear activation
    functions in each of the intermediate nodes. Finally, once we have the strength
    of each hidden unit we are ready to aggregate the impact onto the output, mediated
    again by corresponding weights and possibly an activation function. It may not
    be immediately obvious, but these arrangements of nodes and edges were originally
    designed to emulate how our brains work, nodes being the neurons that are connected
    through synapses or edges in the network. These days, few practitioners take this
    analogy literally. The right panel shows a somewhat deeper network with two hidden
    layers, each one with different widths or number of nodes. It is fully connected
    since every node in a layer is connected to nodes in the subsequent layer, but
    this need not be the case. As a matter of fact, one way to control for overfitting
    in DL is by systematically deleting some of these edges between nodes, a technique
    known as *dropout*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些工具，我们准备介绍神经网络。神经网络是一组节点和边或节点之间的连接，就像[图 A-9](#fig1_neural_nets_examples)中所示的一样。左侧面板显示了一个仅有两个输入的网络，每个输入影响两个隐藏或中间节点，其强度由相应的参数中介。我们可以决定在每个中间节点中使用非线性激活函数。最后，一旦我们有了每个隐藏单元的强度，我们就可以通过相应的权重和可能的激活函数再次整合对输出的影响。这些节点和边的安排原本设计用来模拟我们大脑的工作方式，节点是通过突触或网络中的边连接的神经元。如今，很少有从业者会字面上理解这种类比。右侧面板显示了一个稍深的网络，具有两个隐藏层，每个层具有不同的宽度或节点数。它是全连接的，因为每个层中的节点都连接到下一个层中的节点，但这并非一定如此。事实上，深度学习中一种控制过拟合的方法是通过系统性地删除一些节点之间的这些边，这种技术被称为*dropout*。
- en: Deep ANNs keep growing larger, with very deep nets having hidden layers in the
    hundreds and neurons in the hundreds of thousands and even millions. Moreover,
    richer architectures or ways to assemble these neurons have proved invaluable
    in solving specific problems in computer vision and natural language processing,
    and this is today an active line of research.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 深度人工神经网络（Deep ANNs）正在变得越来越大，非常深的网络具有数百个隐藏层和数十万甚至数百万个神经元。此外，更丰富的架构或组装这些神经元的方法已被证明在解决计算机视觉和自然语言处理中的特定问题中非常宝贵，这在今天是一个活跃的研究领域。
- en: '![examples of neural nets](assets/asai_aa09.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络示例](assets/asai_aa09.png)'
- en: 'Figure A-9\. Two examples of deep neural nets: the left plot shows a network
    with one hidden layer and two inputs, while the plot on the right depicts a network
    with five inputs and two hidden layers'
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A-9\. 深度神经网络的两个示例：左图显示了一个具有一个隐藏层和两个输入的网络，而右图描述了一个具有五个输入和两个隐藏层的网络
- en: The success of deep learning
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习的成功
- en: So what is the buzz around DL? ANNs and their distinct architectural variations,
    such as convolutional neural networks (CNNs) and the like, have proven to be highly
    successful at solving problems that were previously thought to be solvable at
    the human level only by us.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，围绕深度学习（DL）的兴奋点在哪里呢？人工神经网络（ANNs）及其各种不同的架构变体，如卷积神经网络（CNNs）等，已被证明在解决那些过去被认为只能由我们人类达到的问题上非常成功。
- en: In the field of image recognition, deep CNNs have achieved results comparable
    to those of humans, and have even surpassed our species in image classification.
    In 2012, AlexNet, one such neural net, showed DL’s power relative to other approaches
    by winning the ImageNet competition and reducing the classification error from
    26% to 15.3%.^([9](app01.html#idm46359801339704)) This error has kept decreasing
    every year with deeper networks, now even beating our capacity to recognize and
    classify objects in up to 1,000 categories.^([10](app01.html#idm46359801337336))
    Each layer in a deep CNN detects different levels of abstractions and details
    in an image. For instance, the first layer may detect the shapes or edges of the
    objects, the second layer patterns such as lines or stripes, and so on.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像识别领域，深度卷积神经网络（**CNNs**）已经取得了与人类可比的结果，甚至在图像分类方面超过了我们人类。2012年，其中一种名为AlexNet的神经网络通过赢得ImageNet比赛，将分类错误率从26%降低到15.3%，展示了深度学习相对于其他方法的强大能力。^([9](app01.html#idm46359801339704))
    这个错误率随着网络深度的增加而每年逐步减少，现在甚至超过了我们识别和分类多达1,000个类别的能力。^([10](app01.html#idm46359801337336))
    深度CNN中的每一层都能检测图像中不同抽象级别和细节。例如，第一层可以检测对象的形状或边缘，第二层可以检测线条或条纹等模式，依此类推。
- en: 'Language is an example of sequence data where RNNs have had a profound impact.
    Sequential data has the property that the order of occurrence matters. For instance,
    take these two variations: “I had my room cleaned” and “I had cleaned my room.”
    The two sentences have the exact same words in different order but slightly different
    meanings. Another example of how order matters is the prediction of one word given
    the neighboring words, a process analogous to the way we extract context in a
    sentence. One difficulty with sequence data is that the algorithm must have some
    type of memory that remembers what happened previously and what the current state
    is. In contrast to the multilayer neural nets, when successive inputs are used,
    an RNN accesses this memory and combines it with the current input to update the
    prediction. This class of networks has shown incredible power in problems such
    as speech recognition, image captioning, machine translation, and question answering,
    and is behind every virtual assistant that is commercially available. Thanks to
    these technologies, companies are transforming the ways we interact and communicate
    with our customers. This is only the tip of the iceberg for natural language applications.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是序列数据的一个例子，在这方面，递归神经网络（**RNNs**）产生了深远的影响。序列数据具有发生顺序重要的特性。例如，比较这两个变体：“我让我的房间打扫了”和“我已经打扫了我的房间”。这两个句子使用了完全相同的词汇，但顺序不同，意思略有不同。另一个展示顺序重要性的例子是预测给定相邻词的一个词，这个过程类似于我们在句子中提取上下文的方式。序列数据的一个困难在于算法必须有某种记忆，以记住之前发生了什么以及当前状态是什么。与多层神经网络不同，当使用连续输入时，RNN访问这种记忆并将其与当前输入结合起来更新预测。这类网络在语音识别、图像字幕生成、机器翻译和问答等问题中显示出了非凡的能力，并且是每一个商业可用虚拟助手背后的技术。由于这些技术，企业正在转变我们与客户互动和沟通的方式。这只是自然语言应用的冰山一角。
- en: Another area where DL has had a profound effect is video games and games of
    strategy playing. Starting in 2015, Google’s DeepMind AlphaGo has beaten human
    professionals and top players in the game of Go. This is done by combining the
    predictive power of DL algorithms with *reinforcement learning*. In the latter,
    a well-defined system of rewards operates when certain actions are taken. The
    task of the algorithm is to learn what actions earn the highest rewards possible
    into the future by interacting with the environment, and good or bad decisions
    generate a flow of rewards that reinforce the learning process. At this point,
    we have only been able to use these kinds of algorithms in highly constrained
    settings, such as games where the reward function is relatively simple and where
    we can easily generate massive datasets of play for the algorithms to learn. But
    researchers are already using these technologies to improve autonomous or self-driving
    vehicles, robots, and factories, so the future seems promising for deep reinforcement
    learning.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）对另一个领域的深远影响是视频游戏和战略游戏。从2015年开始，谷歌的DeepMind AlphaGo已经击败了人类职业选手和顶级玩家。这是通过将DL算法的预测能力与*强化学习*相结合来实现的。在后者中，当采取某些行动时，一套明确定义的奖励系统起作用。算法的任务是通过与环境互动学习未来能够获得最高奖励的行动，而良好或不良的决策则会产生一系列奖励，从而加强学习过程。到目前为止，我们只能在高度受限的环境中使用这些算法，例如游戏，其中奖励函数相对简单，我们可以轻松生成大量的游戏数据集供算法学习。但研究人员已经开始利用这些技术来改进自动驾驶车辆、机器人和工厂，因此对于深度强化学习来说，未来看起来充满希望。
- en: While DL (and ML more broadly) has and will keep having a profound impact, it
    is not a general-purpose learning technology (like the one humans have). Some
    of the areas where ML doesn’t fare well compared to humans are the identification
    and learning of causal relationships, learning with little experience (semisupervised
    or self-supervised learning), common-sense calculations, and context extraction,
    just to name a few. Machine learning algorithms are powerful pattern-recognition
    techniques and should be treated as such.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习（以及更广泛的机器学习）已经并将继续产生深远影响，但它并不是一种类似于人类拥有的通用学习技术。在一些领域中，机器学习与人类相比表现不佳的领域包括识别和学习因果关系、在少有经验的情况下学习（半监督或自监督学习）、常识推理、以及上下文提取，仅举几例。机器学习算法是强大的模式识别技术，应该作为这样的技术来对待。
- en: A Primer on A/B Testing
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: A/B测试初探
- en: The previous sections introduced some basic concepts in ML, with emphasis on
    supervised learning techniques that are most commonly used by practitioners. In
    this last section we will take a look at A/B testing, the technique that was introduced
    in [Chapter 2](ch02.html#ch02_introanalytics).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的章节介绍了一些机器学习的基本概念，重点放在了实践者们最常用的监督学习技术上。在本节中，我们将来看一下A/B测试，这是在[第2章](ch02.html#ch02_introanalytics)中介绍的技术。
- en: 'Recall that our objective here is to simulate counterfactuals that eliminate
    the pervasive effect that selection bias has. The idea is that while we may not
    be able to get exact copies of our customers, we may still be able to simulate
    such a copying device using *randomization*, that is, by randomly assigning customers
    to two groups: those who receive the treatment and those who don’t (the control
    group). Note that the choice of two groups is done for ease of exposition, as
    the methodology applies for more than two treatments.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆我们在这里的目标是模拟反事实，消除选择偏差的普遍影响。这个想法是，虽然我们可能无法得到我们客户的确切副本，但我们仍然可以通过*随机化*来模拟这样一种复制设备，即通过将客户随机分配到两个组：接受治疗的组和不接受治疗的组（对照组）。请注意，选择两个组是为了便于说明，因为该方法适用于超过两种治疗方法的情况。
- en: 'We know that customers in each group are different, but by correctly using
    a random assignment we dispose of any selection bias: our customers were selected
    by chance, and chance is thought to be unbiased. Good randomization buys for us
    that the treatment and control groups are *ex-ante indistinguishable* on average.
    To give a more precise idea of how this works, suppose that we know the age and
    gender of our customers. *Before running the experiment* we would check if the
    gender distribution and average ages are the same in the treatment and control
    groups. If the answer is positive, then our randomization was done correctly.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道每组客户都不同，但通过正确使用随机分配，我们消除了任何选择偏差：我们的客户是随机选择的，而且认为随机选择是无偏的。良好的随机化使我们能够确保治疗组和对照组在平均上*事前不可区分*。为了更准确地说明其工作原理，假设我们知道客户的年龄和性别。*在运行实验之前*，我们会检查治疗组和对照组的性别分布和平均年龄是否相同。如果答案是肯定的，那么我们的随机化是正确的。
- en: A/B testing in practice
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际应用中的A/B测试
- en: 'In the industry, the process of randomizing to assign different treatments
    is called A/B testing. The name comes from the idea that we want to test an alternative
    <math alttext="upper B"><mi>B</mi></math> to our default action <math alttext="upper
    A"><mi>A</mi></math> , the one we commonly use. As opposed to many of the techniques
    in the machine learning toolbox, A/B testing can be performed by anyone, even
    without a strong technical background. We may need, however, to guarantee that
    our test satisfies a couple of technical statistical properties, but these are
    relatively easy to understand and put into practice. The process usually goes
    as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在行业中，随机分配不同治疗方案的过程被称为A/B测试。其名称来源于我们希望测试一个替代<math alttext="upper B"><mi>B</mi></math>，与我们通常使用的默认操作<math
    alttext="upper A"><mi>A</mi></math>相对。与机器学习工具箱中的许多技术不同，A/B测试可以由任何人执行，即使没有强大的技术背景也可以。然而，我们可能需要确保我们的测试满足一些技术上的统计性质，但这些相对容易理解和实施。过程通常如下所示：
- en: 'Select an actionable hypothesis you want to test: for example, female call
    center representatives have a higher conversion rate than male representatives.
    This is a crisp hypothesis that is falsifiable.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个可操作的假设进行测试：例如，女性呼叫中心代表的转化率比男性代表高。这是一个明确的可以被证伪的假设。
- en: Choose a relevant and measurable KPI to quantify the results from the test;
    in this example, we choose conversion rates as our outcome. If average conversion
    for female reps *isn’t* “significantly larger” than that for men, we can’t conclude
    that the treatment worked, so we keep running the business as usual. It is standard
    practice to use the concept of statistical significance to have a precise definition
    of what *larger* means.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个相关且可衡量的关键绩效指标来量化测试结果；在本例中，我们选择转化率作为我们的结果。如果女性代表的平均转化率*不明显大于*男性的平均转化率，我们无法得出治疗效果显著的结论，因此我们将继续按照业务惯例运营。使用统计显著性的概念来精确定义*大于*的意义是标准做法。
- en: 'Select the number of customers that will be participating in the test: this
    is the first technical property that must be carefully selected and will be discussed
    next.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择将参与测试的客户数量：这是必须仔细选择并将在下文讨论的第一个技术属性。
- en: Randomly assign the customers to both groups and check that randomization produced
    groups that satisfy the ex-ante indistinguishable property.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机分配客户到两个组，并检查随机化是否产生满足事前不可区分属性的组。
- en: After the test is performed, measure the difference in average outcomes. We
    should take care of the rather technical detail of whether a difference is generated
    by pure chance or not (statistical significance).
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试完成后，测量平均结果的差异。我们应该注意一个相当技术性的细节，即一个差异是由纯粹的偶然产生还是不（统计显著性）。
- en: If randomization was done correctly, we have eliminated the selection bias,
    and the difference in average outcomes provides an estimate of the causal effect.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果随机化执行正确，我们已经消除了选择偏差，并且平均结果的差异提供了因果效应的估计。
- en: Understanding power and size calculations
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解功效和大小计算
- en: Step 3, selecting the number of customers, is what practitioners call *power
    and size calculations*, and unfortunately there are key trade-offs we must face.
    Recall that one common property of statistical estimation is that the larger the
    sample size, the lower the uncertainty we have about our estimate. We can always
    estimate the average outcome for groups of 5, 10, or 1,000 customers assigned
    to the <math alttext="upper B"><mi>B</mi></math> group, but our estimate will
    be more precise for the latter than for the former. From a strictly statistical
    point of view, we prefer having large experiments or tests.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步，选择客户数量，是从业者称之为*功效和大小计算*的内容，不幸的是，我们必须面对关键的权衡。请记住，统计估计的一个普遍特性是，样本量越大，我们对估计的不确定性就越小。我们总是可以估计分配给<math
    alttext="upper B"><mi>B</mi></math>组的5、10或1,000名客户的平均结果，但对于后者而言，我们的估计将比前者更为精确。从严格的统计观点来看，我们更倾向于进行大规模的实验或测试。
- en: From a business perspective, however, testing with large groups may not be desirable.
    First, our assignment must be respected until the test comes to an end, so there
    is the opportunity cost of trying other potentially more profitable treatments,
    or even our control or base scenario. Because of this, it is not uncommon that
    the business stakeholders want to finish the test as quickly as possible. In our
    call center example, it could very much have been the case that conversion rates
    were *lower* with the group of female reps, so during a full day we operated suboptimally,
    which may take an important toll on the business (and our colleagues’ bonuses).
    We simply can’t know at the outset (but a well-designed experiment should include
    some type of analysis of this happening).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从业务角度来看，使用大团体进行测试可能并不理想。首先，我们必须尊重分配直到测试结束，因此存在尝试其他潜在更有利可图的处理控制或基础情景的机会成本。因此，业务利益相关者通常希望尽快完成测试。在我们的呼叫中心示例中，女性代表团的转化率可能低于预期，因此在整个工作日内，我们的运作并不理想，这可能对业务（和我们同事的奖金）造成重大影响。我们在开始时无法知道（但是一个设计良好的实验应该包括对此类事件发生的某种分析）。
- en: 'Because of this trade-off we usually select the *minimum* number of customers
    that satisfies two statistical properties: an experiment should have the right
    statistical size and power so that we can conclude with enough confidence if it
    was a success or not. This takes us to the topic of false positives and false
    negatives.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这种权衡，我们通常选择最小的客户数量，满足两个统计属性：实验应具有正确的统计大小和功效，以便我们可以足够自信地得出实验是否成功的结论。这将引导我们进入假阳性和假阴性的话题。
- en: False positives and false negatives
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 假阳性和假阴性
- en: In our call center example, suppose that contrary to our initial assumption,
    male and female representatives have the exact same conversion efficiency. In
    an ideal scenario we would find no difference between the two cases, but in practice
    this is *always* non-zero, even if small. How do we know if the difference in
    average outcomes is due to random noise or if it is showing a real, but possibly
    small difference? Here’s where statistics enter the story.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的呼叫中心示例中，假设与我们最初的假设相反，男性和女性代表的转化效率完全相同。在理想情况下，我们将在两种情况之间找不到差异，但在实践中，即使很小，这总是*非零的*。我们如何知道平均结果的差异是由于随机噪声，还是显示了一个真实的，但可能很小的差异呢？这就是统计学进入故事的地方。
- en: There is a *false positive* when we mistakenly conclude that there is a difference
    in average outcomes across groups and we therefore conclude that the treatment
    had an effect. We choose the *size of the test* to minimize the probability of
    this happening.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们错误地得出跨组平均结果存在差异的结论时，就会出现*假阳性*，因此我们因此得出了处理有影响的结论。我们选择测试的*大小*以最小化此类事件的发生概率。
- en: On the other hand, it could be that the treatment actually worked but we may
    not be able to detect the effect with enough confidence. This usually happens
    when the number of participants in the experiment is relatively small. The result
    is that we end up with an *underpowered* test. In our call center example, we
    may falsely conclude that representatives’ productivity is the same across genders
    when indeed one has higher conversion rates.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，可能治疗确实起作用，但我们可能无法足够自信地检测到效果。当实验参与者数量相对较少时，通常会发生这种情况。结果是我们得到了一个*功效不足的*测试。在我们的呼叫中心示例中，我们可能错误地得出结论，即性别之间的代表生产力相同，而实际上一个性别的转化率更高。
- en: Statistical Size and Power
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计大小和功效
- en: Somewhat loosely speaking, the *size* of a statistical test is the probability
    of encountering a false positive. The *power* of the test is the probability of
    correctly finding a difference between treatment and control.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微不严格地说，统计检验的*大小*是遇到假阳性的概率。 检验的*功效*是正确找到治疗和对照之间差异的概率。
- en: The first panel in [Figure A-10](#fig2_statistical_power) shows the case of
    an underpowered test. The alternative <math alttext="upper B"><mi>B</mi></math>
    treatment creates 30 additional sales, but because of the small sample sizes,
    this difference is estimated with insufficient precision (as seen by the wide
    and overlapping confidence intervals represented by the vertical lines).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure A-10](#fig2_statistical_power) 的第一个面板显示了一个功效不足的测试情况。 替代物<math alttext="upper
    B"><mi>B</mi></math>增加了30个额外销售，但由于样本量较小，这种差异估计精度不足（由垂直线表示的宽阔且重叠的置信区间可见）。'
- en: The second panel shows the case where the real difference is close to 50 extra
    sales, and we were able to precisely estimate the averages and their differences
    (since confidence intervals are so small that they don’t even look like intervals).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个面板显示了真实差异接近额外50个销售的情况，我们能够精确估计平均值及其差异（由于置信区间非常小，它们甚至看起来不像区间）。
- en: '![Results of underpowered and powered experiments](assets/asai_aa10.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![功效不足和有效实验的结果](assets/asai_aa10.png)'
- en: 'Figure A-10\. The first panel shows the result of an underpowered test: there
    is a difference in the average outcomes for the treated and untreated, but the
    small sample sizes for each group cannot estimate this effect with enough precision;
    the second panel shows the ideal result where there is a difference and we can
    correctly conclude this is the case'
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 A-10\. 第一个面板展示了功效不足测试的结果：在治疗组和对照组的平均结果之间存在差异，但每组的样本量都太小，无法准确估计这种效果；第二个面板展示了理想的结果，显示了差异存在并且我们能够正确得出这一结论。
- en: 'Let’s briefly talk about the costs of false positives and false negatives in
    the context of A/B testing. For this, recall what we wanted to achieve with the
    experiment to begin with: we are currently pulling a lever and want to know if
    an alternative is superior for a given metric that impacts our business. As such,
    there are two possible outcomes: we either continue pulling our A lever, or we
    substitute it with the B alternative. In the case of a false positive, the outcome
    is making a subpar substitution. Similarly, with a false negative we mistakenly
    continue pulling the A lever, which also impacts our results. In this sense both
    are kind of symmetric (in both cases we have an uncertain long-term impact), but
    it is not uncommon to treat them asymmetrically, by setting the probability of
    a false positive at 5% or 10% (size), and the probability of a false negative
    at 20% (one minus the power).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要讨论在A/B测试背景下假阳性和假阴性的成本。 为此，回想一下我们最初想要通过实验实现的目标：我们目前正在拉动一个杠杆，并且想知道替代物是否对影响我们业务的指标更为优越。
    因此，存在两种可能的结果：我们要么继续拉动我们的A杠杆，要么用B替代物代替它。 在假阳性的情况下，结果是做出次优的替代。 类似地，假阴性会误导我们继续拉动A杠杆，这也会影响我们的结果。
    在这种意义上，两者有点对称（在两种情况下，我们都面临不确定的长期影响），但不少人会不对称地处理它们，通过设置假阳性的概率为5%或10%（大小），假阴性的概率为20%（功效的互补）。
- en: There is however the opportunity cost of designing and running the experiment,
    so we’d better run it assuming the best-case scenario that the alternative has
    an effect. That’s why most practitioners tend to fix the size of a test and find
    the minimum sample size that allows us to detect some minimum effect.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，设计和运行实验也存在机会成本，因此我们最好假设最佳情况下替代物有影响。 这就是为什么大多数从业者倾向于确定测试的大小并找到允许我们检测某种最小效果的最小样本量。
- en: '[Example A-1](#calc-ab-sample-test) shows how to calculate the sample size
    for your experiment with Python:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 A-1](#calc-ab-sample-test) 展示了如何使用Python计算实验的样本量：'
- en: Example A-1\. Calculating the sample size for an A/B test
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 A-1\. 计算A/B测试的样本量
- en: '[PRE0]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In practice, we start by setting the power and size of the test and then choose
    an MDE. One way to think about it is that it is the minimum change on our outcome
    metric that makes the experiment worthwhile from a business standpoint. We can
    finally reverse engineer the sample size we need from the formula.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们首先设定测试的功效和大小，然后选择一个MDE。 一种思考方法是，这是对我们的结果指标进行的最小变化，从业务角度来看这个实验是值得的。 最后，我们可以从公式中反向工程出我们需要的样本量。
- en: To see this in practice, suppose that we want to run an A/B test to see if we
    can increase our average customer spend or ticket by way of a price discount.
    In this price elasticity experiment, the treatment group will get the new lower
    price, and the control will keep paying the regular price. Because of those very
    high-spend customers, the variance in monthly spend is $4,500 (standard deviation
    is about $67). As a benchmark we choose standard values for size and power (5%
    and 80%). Finally, our business stakeholders convince us that from their perspective
    it only makes sense to try the new alternative if we find a minimum effect (MDE)
    of 10 dollars (or 15% of one standard deviation). We run our size calculator and
    find that we need at least 1,115 participants in the experiment. Since our contact
    rate is around 2%, we should send emails to around 1115/0.02 = 55.2K customers.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这一实践，假设我们想运行一个A/B测试，看看通过价格优惠是否能增加我们的平均客户消费或票数。在这个价格弹性实验中，治疗组将获得新的更低价格，而对照组将继续支付常规价格。由于这些高消费客户，每月支出的方差为$4,500（标准偏差约为$67）。作为基准，我们选择了标准的尺寸和功率值（5%
    和 80%）。最后，我们的业务利益相关者说服我们，从他们的角度来看，只有在找到至少10美元的最小效应（MDE）（或一个标准偏差的15%）时，尝试新的替代才有意义。我们运行我们的大小计算器，并发现我们需要至少1,115名实验参与者。由于我们的联系率大约为2%，我们应该向大约55.2K名客户发送电子邮件。
- en: Further Reading
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'There are many books on general machine learning. Readers looking for highly
    technical material can always go to Robert Tibshirani’s and Trevor Hastie’s masterpiece
    *Elements of Statistical Learning* (Springer). Together with Daniela Witten and
    Gareth James, they also authored an introductory textbook on building and interpreting
    machine learning models that is less technical and rich on intuition: *Introduction
    to Statistical Learning With Applications in R* (Springer). Kevin Murphy’s *Machine
    Learning: A Probabilistic Perspective* (MIT Press) is great at presenting many
    methods at a technical level as well as providing much-needed Bayesian foundations.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '有许多关于通用机器学习的书籍。寻找高度技术性材料的读者可以随时查阅Robert Tibshirani和Trevor Hastie的杰作*Elements
    of Statistical Learning*（Springer）。他们与Daniela Witten和Gareth James合著了一本介绍性的关于构建和解释机器学习模型的教科书，内容不那么技术性，但富有直觉：*Introduction
    to Statistical Learning With Applications in R*（Springer）。Kevin Murphy的*Machine
    Learning: A Probabilistic Perspective*（MIT Press）在技术层面上展示了许多方法，并提供了必要的贝叶斯基础。'
- en: 'Other books that are more hands-on, and will teach you not only to understand
    but also to implement machine learning with popular open source libraries are
    Matthew Kirk’s *Thoughtful Machine Learning* (O’Reilly) and Joel Grus’s *Data
    Science from Scratch* (O’Reilly). They both provide superb introductions to machine
    learning models, as well as in-depth discussions into some of the methods that
    are commonly used. I also highly recommend Foster Provost and Tom Fawcett’s *Data
    Science for Business* (O’Reilly), a book directed towards business people and
    practitioners alike: they achieve the hard-to-get balance between the technical
    and explanatory. These last three are all part of the highly-recommended O’Reilly
    series on machine learning and data science.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 其他更加实践的书籍，不仅教会你理解，还能使用流行的开源库实施机器学习的有Matthew Kirk的*Thoughtful Machine Learning*（O'Reilly）和Joel
    Grus的*Data Science from Scratch*（O'Reilly）。它们都提供了机器学习模型的出色介绍，并深入讨论了一些常用的方法。我还强烈推荐Foster
    Provost和Tom Fawcett的*Data Science for Business*（O'Reilly），这本书既面向业务人士又面向从业者：它们在技术和解释之间达到了难以置信的平衡。这最后三本都是机器学习和数据科学领域极力推荐的O'Reilly系列书籍之一。
- en: 'Probably the most thorough and general treatment of artificial neural networks
    can be found in *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
    (MIT Press). I also recommend *Deep Learning* by Adam Gibson and Josh Patterson
    (O’Reilly), as well as Francois Chollet’s *Deep Learning With Python* (O’Reilly).
    There are also advanced books for specific topics: Alex Graves’s *Supervised Sequence
    Labelling with Recurrent Neural Networks (Studies in Computational Intelligence)*
    (Springer) provides a thorough treatment of RNNs, and Yoav Goldberg’s *Neural
    Network Methods in Natural Language Processing (Synthesis Lectures on Human Language
    Technologies)* (Morgan & Claypool) is an introduction to learning algorithms in
    natural language applications.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 可能可以在Ian Goodfellow，Yoshua Bengio和Aaron Courville的*Deep Learning*（MIT Press）中找到最详尽和通用的人工神经网络处理方法。我还推荐Adam
    Gibson和Josh Patterson的*Deep Learning*（O'Reilly），以及Francois Chollet的*Deep Learning
    With Python*（O'Reilly）。还有一些针对特定主题的高级书籍：Alex Graves的*Supervised Sequence Labelling
    with Recurrent Neural Networks（Studies in Computational Intelligence）*（Springer）提供了对RNN的深入处理，而Yoav
    Goldberg的*Neural Network Methods in Natural Language Processing（Synthesis Lectures
    on Human Language Technologies）*（Morgan & Claypool）则是自然语言应用中学习算法的介绍。
- en: Two surprisingly refreshing general-public treatments of current AI technologies
    are Terrence Sejnowski’s *The Deep Learning Revolution* (MIT Press) and Jerry
    Kaplan’s *Artificial Intelligence. What Everyone Needs to Know* (Oxford University
    Press). Written by an insider in the revolution, though highly self-referential,
    the former provides a rich chronological account of the development of the techniques
    in the last decades of the last century, as well as how what neuroscientists and
    cognitive scientists know about the workings of the brain motivated many of those
    developments. The latter’s Q&A format may seem too rigid at times, but in the
    end it provides an easy-to-read account of current methods, as well as less common
    topics such as the philosophical underpinnings behind AI.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 两本令人耳目一新的面向公众的当前人工智能技术处理书籍是Terrence Sejnowski的*The Deep Learning Revolution*（MIT
    Press）和Jerry Kaplan的*Artificial Intelligence. What Everyone Needs to Know*（Oxford
    University Press）。前者由革命内部人士编写，虽然高度自我参照，但提供了对上个世纪最后几十年技术发展的丰富时间轴账述，以及神经科学家和认知科学家所知的大脑运作方式如何推动了许多这些发展。后者的问答格式有时可能过于严格，但最终提供了当前方法的易读账述，以及人工智能背后的哲学基础等不太常见的主题。
- en: 'There are many treatments of A/B testing, starting with Dan Siroker and Pete
    Koomen’s *A/B Testing: The Most Powerful Way to Turn Clicks into Customers* (Wiley)
    where you can get a good sense of A/B testing applications. Peter Bruce and Andrew
    Bruce’s *Practical Statistics for Data Scientists* (O’Reilly) provides an accessible
    introduction to statistical foundations, including power and size calculations.
    Carl Andersen’s *Creating a Data-Driven Organization* (O’Reilly), briefly discusses
    some best practices in A/B testing, emphasizing its role in data- and analytics-driven
    organizations. Ron Kohavi (previously at Microsoft and now at Airbnb) has been
    forcefully advancing the use of experimentation in the industry. His recent book,
    coauthored with Diane Tang and Ya Xu, *Trustworthy Online Controlled Experiments:
    A Practical Guide to A/B Testing* (Cambridge University Press) is a great reference.
    Some of this material can be found at [*https://exp-platform.com*](https://exp-platform.com)
    and [*https://oreil.ly/1CPRR*](https://oreil.ly/1CPRR).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'A/B测试有许多处理方法，从Dan Siroker和Pete Koomen的*A/B Testing: The Most Powerful Way to
    Turn Clicks into Customers*（Wiley）开始，您可以对A/B测试的应用有一个很好的理解。Peter Bruce和Andrew Bruce的*Practical
    Statistics for Data Scientists*（O''Reilly）提供了关于统计基础的易于理解的介绍，包括功效和样本大小计算。Carl Andersen的*Creating
    a Data-Driven Organization*（O''Reilly）简要讨论了A/B测试中的一些最佳实践，强调其在数据和分析驱动型组织中的作用。Ron
    Kohavi（曾在微软，现在在Airbnb）一直在积极推动行业中试验的应用。他最近与Diane Tang和Ya Xu合著的书籍*Trustworthy Online
    Controlled Experiments: A Practical Guide to A/B Testing*（剑桥大学出版社）是一个很好的参考资料。部分内容可以在[*https://exp-platform.com*](https://exp-platform.com)和[*https://oreil.ly/1CPRR*](https://oreil.ly/1CPRR)找到。'
- en: You can find material on power calculations in Howard S. Bloom’s *The Core Analytics
    of Randomized Experiments for Social Research*, [available online](https://oreil.ly/zs9gx).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Howard S. Bloom的*The Core Analytics of Randomized Experiments for Social
    Research*中找到关于功效计算的材料，[在线获取](https://oreil.ly/zs9gx)。
- en: ^([1](app01.html#idm46359801798360-marker)) The most commonly used updating
    rule in ML is the *gradient descent* algorithm we showcased in [Chapter 7](ch07.html#ch07_optimization).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](app01.html#idm46359801798360-marker)) 机器学习中最常用的更新规则是我们在[第7章](ch07.html#ch07_optimization)中展示的*梯度下降*算法。
- en: ^([2](app01.html#idm46359801789096-marker)) This is in contrast to business-driven
    segmentation where the analyst provides explicit business logic to group the customers.
    For instance, one group could be males between the ages of 25 and 35 with tenures
    with us longer than 24 months.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](app01.html#idm46359801789096-marker)) 这与以业务为驱动的细分相对，分析师提供明确的业务逻辑来对客户进行分组。例如，一个组可能是男性，年龄在25到35岁之间，在我们这里的任期超过24个月。
- en: ^([3](app01.html#idm46359801777848-marker)) This category is also called *self-supervised*
    learning. See [*https://oreil.ly/yky7W*](https://oreil.ly/yky7W).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](app01.html#idm46359801777848-marker)) 这个类别也被称为*自监督*学习。参见[*https://oreil.ly/yky7W*](https://oreil.ly/yky7W)。
- en: ^([4](app01.html#idm46359801628808-marker)) Averages are extremely sensitive
    to outliers, as opposed to the median, for example. Take three numbers <math alttext="left-parenthesis
    1 comma 2 comma 3 right-parenthesis"><mrow><mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn>
    <mo>,</mo> <mn>3</mn> <mo>)</mo></mrow></math> . The simple average and the median
    are both 2\. Change the dataset to <math alttext="left-parenthesis 1 comma 2 comma
    300 right-parenthesis"><mrow><mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo>
    <mn>300</mn> <mo>)</mo></mrow></math> . The median is still 2, but the average
    is now 101.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](app01.html#idm46359801628808-marker)) 平均数对异常值非常敏感，与中位数相反。例如，取三个数字<math
    alttext="left-parenthesis 1 comma 2 comma 3 right-parenthesis"><mrow><mo>(</mo>
    <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mn>3</mn> <mo>)</mo></mrow></math>。简单平均数和中位数都是2。将数据集改为<math
    alttext="left-parenthesis 1 comma 2 comma 300 right-parenthesis"><mrow><mo>(</mo>
    <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mn>300</mn> <mo>)</mo></mrow></math>。中位数仍然是2，但平均数现在是101。
- en: ^([5](app01.html#idm46359801547704-marker)) Formally, it minimizes the mean
    squared error. That way we treat positive and negative errors symmetrically.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](app01.html#idm46359801547704-marker)) 正式来说，它最小化均方误差。这样我们对正误差和负误差对称处理。
- en: ^([6](app01.html#idm46359801537416-marker)) The method of ordinary least squares
    (OLS) actually has a closed-form solution, meaning that in practice we do not
    have to iterate until converging to a good-enough estimate. There are setups,
    however, where iterative improvements are made by using *gradient descent* that
    sequentially adjusts the weights in such a way that we approach the set of optimal
    parameters ([Chapter 7](ch07.html#ch07_optimization)). This is usually the case
    when we use OLS with big data where we end up distributing the data across several
    nodes or servers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](app01.html#idm46359801537416-marker)) 普通最小二乘法（OLS）实际上有一个封闭形式的解，这意味着在实践中我们不需要迭代直到获得足够好的估计。然而，在使用大数据时，有些情况下会通过*梯度下降*进行迭代改进，依次调整权重，以逼近最优参数集合（[第7章](ch07.html#ch07_optimization)）。这通常是在使用OLS处理大数据时的情况，其中我们最终将数据分布在多个节点或服务器上。
- en: ^([7](app01.html#idm46359801525560-marker)) This is true whenever we include
    a constant, intercept, or bias, <math alttext="alpha 0"><msub><mi>α</mi> <mn>0</mn></msub></math>
    .
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](app01.html#idm46359801525560-marker)) 这是每当我们包括常数、截距或偏差时的情况，<math alttext="alpha
    0"><msub><mi>α</mi> <mn>0</mn></msub></math>。
- en: ^([8](app01.html#idm46359801387880-marker)) In some cases we can still use linear
    regression, say <math alttext="g left-parenthesis z right-parenthesis equals e
    x p left-parenthesis z right-parenthesis"><mrow><mi>g</mi> <mo>(</mo> <mi>z</mi>
    <mo>)</mo> <mo>=</mo> <mi>e</mi> <mi>x</mi> <mi>p</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></math>
    . Using the logarithm transformation takes us back to the linear realm with a
    new, transformed outcome <math alttext="ln left-parenthesis y right-parenthesis"><mrow><mo
    form="prefix">ln</mo> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math> .
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](app01.html#idm46359801387880-marker)) 在某些情况下，我们仍然可以使用线性回归，比如<math alttext="g
    left-parenthesis z right-parenthesis equals e x p left-parenthesis z right-parenthesis"><mrow><mi>g</mi>
    <mo>(</mo> <mi>z</mi> <mo>)</mo> <mo>=</mo> <mi>e</mi> <mi>x</mi> <mi>p</mi> <mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow></math>。使用对数变换将我们带回线性领域，得到新的变换结果<math alttext="ln
    left-parenthesis y right-parenthesis"><mrow><mo form="prefix">ln</mo> <mo>(</mo>
    <mi>y</mi> <mo>)</mo></mrow></math>。
- en: '^([9](app01.html#idm46359801339704-marker)) These are top-5 error rates, meaning
    that we use the top 5 predictions for each image, and check if the real label
    is included in this list. Top-1 error rate is the traditional prediction error:
    algorithms are allowed only one guess per image. Human error rates on this dataset
    have been benchmarked to around 3.6%, meaning that out of 100 images to be classified,
    our top 5 predictions match almost 96 of them.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](app01.html#idm46359801339704-marker)) 这些是前五错误率，意味着我们对每个图像使用前5个预测，并检查真实标签是否包含在此列表中。顶部1错误率是传统的预测错误率：算法只允许每个图像一个猜测。该数据集上的人类错误率已经被基准测试到约为3.6%，这意味着在100张要分类的图像中，我们的前5个预测几乎匹配96张。
- en: ^([10](app01.html#idm46359801337336-marker)) The ImageNet database has more
    than 20,000 categories, but only 1,000 are used in the competition.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](app01.html#idm46359801337336-marker)) ImageNet 数据库包含超过 20,000 个类别，但比赛中只使用了
    1,000 个。
