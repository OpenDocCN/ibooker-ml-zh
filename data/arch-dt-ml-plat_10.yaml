- en: Chapter 10\. AI Application Architecture
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章. AI应用架构
- en: 'In this chapter, you will learn the high-level decisions you should make on
    architecture and frameworks when building an AI and ML application. We start by
    considering what kinds of problems AI/ML is good at addressing and how to develop
    and deploy AI responsibly. Once you have decided that ML is a good fit for a problem,
    you will have to move on to deciding the enterprise approach you take: do you
    buy, adapt, or build? We look at examples of each of these scenarios and the considerations
    if you choose to adopt each of these approaches. If you are building, there are
    several choices of architectures, and the choice depends on the type of problem
    being solved.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解在构建AI和ML应用程序时应该做出的架构和框架的高级决策。我们首先考虑AI/ML擅长解决哪些问题以及如何负责地开发和部署AI。一旦您确定ML适合解决某个问题，您将不得不继续决定您采取的企业方法：您是购买、适应还是构建？我们看看每种情况的例子以及如果选择采用这些方法时的考虑因素。如果您正在构建，有几种架构选择，选择取决于正在解决的问题类型。
- en: This chapter covers AI architecture considerations and decision criteria at
    the application level. The platform on which data scientists and ML engineers
    will develop and deploy these applications is covered in [Chapter 11](ch11.html#architecting_an_ml_platform).
    Do not skip this chapter and dive straight into the technical details in the next
    chapter—as a cloud architect, you will need to advise every application team on
    making the right decision regarding buy, adapt, or build and the choice of AI
    architecture for each application that they build on your platform.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了应用层面上AI架构的考虑因素和决策标准。数据科学家和ML工程师将在[第11章](ch11.html#architecting_an_ml_platform)中讨论开发和部署这些应用程序的平台。不要跳过这一章直接进入下一章的技术细节——作为云架构师，您需要为每个应用团队提供建议，以便在您的平台上为其构建的每个应用选择正确的购买、适应或构建以及AI架构的选择。
- en: Note
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The purpose of this chapter and the next is to show you how to architect an
    ML platform using cloud technologies. Just as we did not cover SQL in the chapter
    on data warehousing, we do not cover TensorFlow in these chapters. If you wish
    to learn how to do ML, we heartily recommend the book [*Hands-on Machine Learning
    with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build
    Intelligent Systems*](https://oreil.ly/62m5z) by Aurélien Géron (O’Reilly).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '本章和接下来的一章的目的是向您展示如何使用云技术架构ML平台。就像我们在数据仓库章节中没有涵盖SQL一样，在这些章节中我们也不涵盖TensorFlow。如果您希望学习如何进行ML，我们诚挚推荐Aurélien
    Géron（O''Reilly）的书籍[*Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow:
    Concepts, Tools, and Techniques to Build Intelligent Systems*](https://oreil.ly/62m5z)。'
- en: Is This an AI/ML Problem?
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这是一个AI/ML问题吗？
- en: To learn what kinds of problems you can solve with ML, let’s start with a few
    fundamentals we briefly touched on in [Chapter 1](ch01.html#modernizing_your_data_platform_an_intro).
    We’ll begin with some definitions, and we’ll then consider the sorts of problems
    where ML as a solution is generally a good fit.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解机器学习可以解决哪些问题，让我们从我们在[第一章](ch01.html#modernizing_your_data_platform_an_intro)中简要提到的一些基础知识开始。我们将从一些定义开始，然后考虑ML作为解决方案通常适合的问题类型。
- en: Subfields of AI
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI的子领域
- en: AI is the field of study that solves problems by getting computers to think
    and act like humans. Historically, there have been several approaches tried in
    AI. One approach is to write code that explicitly tells the computer what to do
    in each potential situation. This is how many robot devices such as manufacturing
    robots and home vacuums work even today. The problem is that writing such rules
    is hard. Another approach is to interview or observe an expert and use their behavior
    to code up the rules. AI models to approve whether patients are accepted into
    trials of experimental medicines work this way. But even experts can’t tell you
    why they do what they do. There is a lot of intuition underlying human decision
    making, and the more someone is an expert, the more intermediate steps they jump.
    The difficulty of these approaches led to what is referred to as an “AI winter,”
    a period of time (loosely 1985–2014) when AI was stuck.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: AI是通过让计算机像人类一样思考和行动来解决问题的研究领域。在AI的历史上，曾经尝试过几种方法。一种方法是编写明确告诉计算机在每种可能情况下该做什么的代码。即使今天，许多机器人设备如制造机器人和家用吸尘器仍然是这样工作的。问题在于编写这些规则很困难。另一种方法是采访或观察专家，并使用他们的行为来编写规则。像批准病人参与实验性药物试验的AI模型就是这样工作的。但是即使是专家也不能告诉你他们为什么这样做。人类决策背后有很多直觉，而且一个人越是专家，他们跳过的中间步骤就越多。这些方法的困难导致了所谓的“AI冬季”，即AI陷入停滞的时期（大致在1985年至2014年之间）。
- en: ML is a subfield of AI (see [Figure 10-1](#relationship_between_aicomma_mlcomma_de))
    that solves AI problems using data instead of custom logic. This makes it particularly
    useful when you can’t articulate the logic or if the logic will be too difficult
    to code up as a computer program. Instead of capturing all the differences between
    nails and screws, for example, an ML system is shown hundreds of nails and told
    they are nails, and hundreds of screws and told they are screws. The ML model
    then figures out how to tell them apart by tuning an internal, very general mathematical
    function. ML is many decades old, but until about 2014 it worked only on structured
    data of the sort that you can store in a database.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ML是AI的一个子领域（参见[图10-1](#relationship_between_aicomma_mlcomma_de)），它使用数据而不是定制逻辑来解决AI问题。当您无法表达逻辑或者逻辑编码为计算机程序过于困难时，ML就显得尤为有用。例如，ML系统不是捕捉钉子和螺钉之间所有的区别，而是向它展示数百个钉子并告诉它它们是钉子，展示数百个螺钉并告诉它它们是螺钉。然后ML模型通过调整一个内部非常通用的数学函数来学会如何区分它们。ML已有数十年历史，但直到大约2014年之前，它只能处理可以存储在数据库中的结构化数据。
- en: '*Deep learning* is a subfield of ML that shot into prominence in the late 2010s.
    It uses highly complex functions (which add “depth” to a pictorial representation
    of the function, hence its name). Deep learning has been successfully used to
    understand unstructured data like speech, images, video, and natural language.
    Deep learning is why you have been seeing smart speakers (Google Assistant, Alexa,
    Siri, etc.), image search (Google Photos), automatic translation, and instant
    video highlights (in sports shows) take off in the past few years. Deep learning
    is powerful, but it requires a lot of data to create the ML model in the first
    place.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习*是机器学习的一个子领域，在2010年代末开始受到广泛关注。它使用高度复杂的函数（这些函数在图形表示中增加了“深度”，因此得名）。深度学习已成功应用于理解语音、图像、视频和自然语言等非结构化数据。深度学习是为什么您在过去几年看到智能音箱（Google
    Assistant、Alexa、Siri等）、图像搜索（Google Photos）、自动翻译和即时视频精彩片段（在体育节目中）如此流行的原因。深度学习非常强大，但在创建ML模型时需要大量的数据。'
- en: '![Relationship between AI, ML, deep learning, and generative AI](assets/adml_1001.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![AI、ML、深度学习和生成AI之间的关系](assets/adml_1001.png)'
- en: Figure 10-1\. Relationship between AI, ML, deep learning, and generative AI
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1\. AI、ML、深度学习和生成AI之间的关系
- en: Generative AI
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成AI
- en: 'Traditionally, ML has involved solving classification and regression problems.
    There is a third form of AI that is becoming increasingly capable: *generative
    AI*. Generative AI is a type of deep learning that shot into prominence in the
    early 2020s. Here, the output of the model is text, image, music, video, etc.
    For example, an LLM like ChatGPT is trained by showing the model large amounts
    of natural language text so that it can predict the most likely next words given
    the preceding text. The LLM is used by having it generate an initial set of words
    given a prompt and then complete the sequence in a likely manner given this initial
    sequence and the prompt. Similarly, image generation models are trained to predict
    pixel values and speech models can generate audio frequency and amplitude values.
    The initial uses of generative AI were in sentence completion (e.g., Smart Compose
    in Gmail) and code completion (e.g., Tabnine, GitHub Copilot), but increasingly
    sophisticated products now use generative AI to generate initial drafts of emails
    and articles in sales, marketing, etc.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，ML涉及解决分类和回归问题。现在，有一种越来越强大的第三种AI形式：*生成AI*。生成AI是一种深度学习，早在2020年代初就引起了人们的关注。在这里，模型的输出可以是文本、图像、音乐、视频等。例如，像ChatGPT这样的LLM通过向模型展示大量的自然语言文本来训练，使其能够根据前文预测最有可能的下一个词语。LLM用于生成一组初始词语，然后根据这些初始词语和提示以可能的方式完成序列。类似地，图像生成模型被训练以预测像素值，语音模型可以生成音频频率和振幅值。生成AI最初的应用是在句子完成（例如Gmail中的智能写作）和代码完成（例如Tabnine、GitHub
    Copilot）方面，但现在，越来越复杂的产品使用生成AI来生成销售、营销等领域的邮件和文章的初稿。
- en: How it works
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'ChatGPT, Bard, LLaMa, etc., are LLMs that have been trained to generate text
    in a variety of styles and for different purposes while maintaining a high level
    of accuracy and detail. At the most basic level, these kinds of ML models have
    been trained to be able to complete sentences. Imagine you start writing something
    like the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT、Bard、LLaMa等都是被训练用于以多种风格和不同目的生成文本的LLM，同时保持高精度和详细程度。在最基本的层面上，这类ML模型已经被训练能够完成句子。想象一下，您开始写如下内容：
- en: We were floating
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们正在漂浮
- en: 'The model will propose some ways to proceed with the sentence. For example:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将提出一些进行句子进展的方式。例如：
- en: '... on the river [80%]'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '... 在河流上 [80%]'
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '... in space [15%]'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '... 在空间中 [15%]'
- en: The language model has been shown a lot of published examples—speeches, articles,
    books, reviews, etc.—which allow it to understand which words are most likely
    to follow others.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型已经展示了大量的出版例子——演讲、文章、书籍、评论等——使其能够理解哪些词语最有可能跟随其他词语。
- en: But what about if you pass to the model a previous phrase that contains the
    word “rocket” or “summer”? The relative probabilities of the words that are likely
    to follow will change. Based on the extensive training dataset, the model seems
    to adapt to the context and will generate a response that seems accurate, detailed,
    and consistent. The LLM has learned, during its training phases, which words or
    phrases to pay attention to. In addition, human feedback is employed in the training
    process for an LLM so that it prefers word continuations that are most pleasing
    to humans.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您将包含“火箭”或“夏季”等词语的先前短语传递给模型，那么后续出现的词语的相对概率将会改变。基于广泛的训练数据集，模型似乎能够适应上下文，并生成看似准确、详细和一致的响应。在其训练阶段，语言模型已经学会了哪些词语或短语需要特别关注。此外，在LLM的训练过程中，还使用人类反馈，以便它更倾向于人类最喜欢的词语延续。
- en: Foundational LLMs like GPT-4 (behind ChatGPT) and PaLM (behind Bard) are trained
    for general tasks that involve common language problems such as text classification,
    question answering, document summarization, and text generation across industries.
    These models provide an interactive way to summarize large amounts of knowledge
    and information. This can be multimodal—that is, in different unstructured formats
    such as text, images, or even videos. These models can achieve satisfactory results
    even with a small amount of domain-specific training data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT-4（ChatGPT背后）和PaLM（Bard背后）这样的基础LLM，是为涉及常见语言问题的通用任务而训练的，包括文本分类、问答、文档摘要和跨行业的文本生成。这些模型提供了一种互动方式来总结大量的知识和信息。这可以是多模态的，即以不同的非结构化格式，如文本、图像，甚至视频。即使只有少量特定领域的训练数据，这些模型也能取得令人满意的结果。
- en: Fine-tuned LLMs go beyond global knowledge and focus on specific tasks and domains.
    For example, an LLM focused on healthcare such as Med-PaLM is trained on specific
    labeled datasets to adapt the model to perform specific tasks within the healthcare
    domain. These labeled datasets are crucial for training the model to accurately
    recognize medical entities, write medical care instructions, and perform other
    healthcare-related tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 细调LLM超越了全球知识的范围，专注于特定的任务和领域。例如，专注于医疗保健的LLM，如Med-PaLM，经过特定的标记数据集训练，以适应在医疗保健领域内执行特定任务的模型。这些标记的数据集对于训练模型准确识别医疗实体、撰写医疗护理说明书以及执行其他与医疗保健相关的任务至关重要。
- en: Strengths and limitations
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优势和局限性
- en: This technology is still in its infancy, and it comes with a lot of promise
    but also several limitations that need to be considered, especially for enterprise
    adoption at scale. Let’s take a look at the most important factors to consider.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此技术仍处于起步阶段，并带来了许多承诺，但也需要考虑到几个限制，特别是在企业规模上的采用。让我们看一下需要考虑的最重要因素。
- en: Do LLMs memorize or generalize?
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM是否记忆或概括？
- en: Although LLMs may appear to be able to handle any type of context because they
    have been trained on a massive dataset with billions of parameters, they are not
    actually able to store all of the digital information available on the planet.
    Because they can’t remember all possible word sequences in context, they instead
    interpolate between probable words and contexts. Then they select similar continuations
    in comparable contexts.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM看起来似乎能够处理任何类型的上下文，因为它们已经在包含数十亿参数的大规模数据集上进行了训练，但它们实际上不能存储地球上所有可用的数字信息。因为它们无法记住所有可能的单词序列在上下文中的位置，所以它们会在可能的单词和上下文之间进行插值。然后在可比较的上下文中选择类似的延续。
- en: Does that mean that an LLM is able to generalize beyond the text it was trained
    on? Debatable. The model may generate text that is consistent with the given context,
    but this does not necessarily mean that it has truly understood the real world.
    More research is needed to determine whether models like this can truly generalize
    beyond the text they are trained on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否意味着LLM能够超越其训练文本进行概括？有争议。模型可能生成与给定上下文一致的文本，但这并不一定意味着它真正理解了真实世界。需要进行更多研究来确定像这样的模型是否能够真正超越它们训练的文本范围。
- en: This area is fast-changing, however. At the time of writing, it is possible
    to get an LLM to make use of external datasets and services to “ground” the LLM
    or tell it to reason. Doubtless, the capability of LLMs will continue to get better.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个领域正在快速变化。在撰写时，可以使LLM利用外部数据集和服务来“基于”LLM或告诉它推理。毫无疑问，LLM的能力将继续变得更好。
- en: LLMs hallucinate
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM幻觉
- en: One of the most significant constraints of current LLMs is that they can “hallucinate,”
    which means that they may produce text that is syntactically correct but illogical
    or untrue. In this case, we say that the model has clearly hallucinated.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当前LLM最显著的限制之一是它们可以“幻觉”，这意味着它们可能生成在语法上正确但逻辑不合或不真实的文本。在这种情况下，我们称该模型显然产生了幻觉。
- en: This is a significant constraint that must be carefully addressed. Given that
    most readers skim over some parts of the text they’re reading, it is possible
    that not all people would be able to identify that there was a hallucination in
    the text. At the time of writing, the amount of human work needed to check for
    accuracy is something to be considered when planning for production adoption of
    this technology.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是必须仔细解决的重要约束条件。考虑到大多数读者会快速浏览他们阅读的某些部分，可能并非所有人都能够识别出文本中存在幻觉的情况。在撰写时，需要考虑到检查准确性所需的人工工作量，这是计划采用这项技术时需要考虑的事项之一。
- en: Human feedback is needed
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 需要人类反馈
- en: To reduce the amount of nonsense generated, an extra step is usually added when
    training a consumer-facing LLM. This is called *reinforcement learning through
    human feedback* (RLHF) and involves training the model to choose between generated
    text documents based on human ratings.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少产生无意义文本的数量，通常在培训面向消费者的LLM时通常会添加额外步骤。这称为通过人类反馈进行的*强化学习*（RLHF），涉及训练模型根据人类评级选择生成的文本文档之间。
- en: In addition, humans help to train the model to create documents in specific
    formats (e.g., emails, blog posts, bullet points, etc.) and guide the model away
    from toxic text.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，人类帮助训练模型创建特定格式的文档（例如电子邮件、博客文章、项目符号等），并引导模型远离有害文本。
- en: Weaknesses
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 弱点
- en: 'Now that you understand how LLMs are trained, it is clear that they will have
    certain limitations:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您了解了LLM的训练方式，显然它们会有一定的局限性：
- en: If any of the input sources is wrong, that wrong continuation is part of the
    probable set of continuations. So the LLM could reproduce misinformation and/or
    outdated concepts.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果任何输入源错误，那个错误的延续将成为可能的延续集的一部分。因此，LLM可能会复制错误信息和/或过时的概念。
- en: The model is better in domains where there is a lot of text that is published
    in digital formats. Therefore, the LLM will be better in domains such as coding
    and politics, but worse in domains such as anthropology.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型在数字格式发布的大量文本存在的领域中表现更佳。因此，在编码和政治等领域，LLM将更为出色，但在人类学等领域则较差。
- en: It is hard to tokenize unusual numbers and names (LLMs use subword tokens).
    Therefore, you should always double-check factual information such as numbers,
    authors of articles, etc., that comes out of an LLM.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以标记不寻常的数字和名称（LLM使用子词标记）。因此，您应始终仔细检查LLM生成的事实信息，如数字、文章作者等。
- en: Like all ML models, LLMs should not be used in situations where deterministic
    outcomes are expected.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像所有ML模型一样，LLM不应在期望确定性结果的情况下使用。
- en: It is important to keep these limitations in mind when building applications
    that rely on generative AI.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建依赖生成式人工智能的应用程序时，牢记这些限制是很重要的。
- en: Use cases
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用案例
- en: 'These limitations do not mean LLMs are not useful. Already, we see various
    good uses of them:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制并不意味着LLM没有用处。我们已经看到它们的各种良好应用：
- en: Domain-specific assistants
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 领域特定助手
- en: The LLM is trained on your organization’s data to answer specific questions
    that would require your employees to read through a large number of documents.
    For example, an LLM can answer questions about internal regulations. This can
    save your employees time and improve efficiency.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LLM是根据您组织的数据进行训练的，以回答需要您的员工阅读大量文档的特定问题。例如，LLM可以回答有关内部规定的问题。这可以节省员工的时间并提高效率。
- en: Code snippets
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段
- en: Generative models can write code snippets or review existing code (like an automatic
    peer programmer) to speed up development or documentation writing. However, the
    code may not do what you want it to do, so you should test and edit the generated
    code in small pieces.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型可以编写代码片段或审查现有代码（如自动对等编程员），以加快开发或文档编写的速度。但是，生成的代码可能不会按您的意愿执行，因此您应该逐步测试和编辑生成的代码。
- en: Document summarization
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 文档摘要
- en: The LLM could help with summarizing long documents that would take ages to read
    to extract relevant information (e.g., financial data on balance sheets). This
    can also be part of making workflows more efficient.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: LLM可以帮助总结长文档，以提取相关信息（例如财务数据的资产负债表）。这也可以是提高工作流效率的一部分。
- en: Content generation
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 内容生成
- en: Content generation at scale—for example, for personalized marketing or sales—is
    unlocked by generative AI.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模内容生成（例如个性化营销或销售）是由生成式人工智能解锁的。
- en: Some production use cases of LLMs include Gmail’s Smart Compose, Jasper AI’s
    ability to create marketing copy, Salesforce’s ability to craft personalized sales
    emails, and GitHub Copilot’s ability to generate code. We are seeing many organizations
    leveraging generative AI to provide their employees better access to their private
    knowledge bases through a natural language question-answering bot. Another widespread
    and successful use case involves using image generation algorithms to improve
    the level of differentiation of marketing ads. However, do not discount the difficulty
    of creating such products where generated text or images solve a human pain point,
    are accurate, and meet human standards.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一些LLM的生产使用案例包括Gmail的智能撰写、Jasper AI的营销文案生成、Salesforce的个性化销售电子邮件编写能力以及GitHub Copilot的代码生成能力。我们看到许多组织正在利用生成式人工智能为员工提供更好的访问私有知识库的途径，通过自然语言问答机器人。另一个广泛和成功的使用案例涉及使用图像生成算法来提高市场广告的差异化水平。然而，不要低估在创建这类产品时遇到的困难，其中生成的文本或图像解决了人类的痛点，准确无误，并符合人类标准。
- en: Problems Fit for ML
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适合机器学习的问题
- en: 'What kind of problems can you solve with ML? The most compelling use cases
    of ML happen when you have many of these conditions in operation:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以用机器学习解决什么样的问题？在许多这些条件同时存在时，ML最具有说服力的应用案例发生：
- en: Repeated decision
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 重复决策
- en: You have a simple “brain” task that has to be done so many times that it becomes
    very expensive or impossible for humans to do it. If you are a medical insurance
    firm and you get thousands of requests per day for reimbursement, it is very expensive
    to have a human look at the reimbursement request and determine if the procedure
    was needed or not. If you can get a computer to identify suspicious-looking claims,
    that will save you a lot of money. Conversely, there is not much of a business
    benefit to automating rare decisions. Yet it is surprising how often data scientists
    get carried away trying to apply ML to a problem that needs to be solved only
    a few times a year.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个简单的“大脑”任务，必须重复执行多次，以至于人类执行起来非常昂贵或不可能。如果您是一家医疗保险公司，每天收到数千份报销请求，让人类查看报销请求并确定是否需要进行该程序非常昂贵。如果您能让计算机识别可疑的索赔，那将为您节省大量资金。相反，对于罕见的决策自动化并没有太多的商业利益。然而，数据科学家却常常陷入试图将ML应用于每年仅需解决几次的问题的情况中，这令人惊讶。
- en: Labeled data
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 标记数据
- en: ML works when you can teach the ML system by showing it examples of correct
    decisions. Anomaly detection where you cluster the data and look for outliers
    is an exception to this. However, the vast majority of ML problems require you
    to have examples that have the correct answer attached.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当您能通过展示正确决策示例来教授ML系统时，ML就会起作用。聚类数据并寻找异常值的异常检测是一个例外。然而，绝大多数ML问题需要您提供具有正确答案的示例。
- en: Fault-tolerant use case
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 容错使用案例
- en: Because no ML algorithm is perfect, you should not use ML in cases where errors
    are fatal to life or property. You will have to consider the implications of the
    ML system being wrong and be willing to live with the consequences. In situations
    where errors have significant impact, having a human in the decision loop or having
    stop-loss provisions is common.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因为没有哪个ML算法是完美的，所以您不应在错误对生命或财产有致命影响的情况下使用ML。您必须考虑ML系统出错的后果，并愿意承担后果。在错误影响重大的情况下，常见的做法是让人类参与决策过程或设立止损条款。
- en: Logic that is hard to articulate
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 难以表达的逻辑
- en: The more complex some logic is to articulate, the better ML is at it. Note that
    it is quite easy to write up the logic of how to calculate the area of a polygon.
    That is not the sort of problem that ML is good for. When it is easy to capture
    the logic, you should use traditional programming methods. Save ML for situations
    where human logic is hard to come by (e.g., calculate the area of a room from
    a photograph).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一些逻辑越难以表达，ML在处理它时就越好。请注意，编写如何计算多边形面积的逻辑是相当容易的。这不是ML擅长的问题类型。当捕捉逻辑容易时，应使用传统编程方法。将ML保留给人类难以掌握的情况（例如，从照片中计算房间面积）。
- en: Unstructured data
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据
- en: Traditional rules tend to work quite well for processing structured data, but
    ML is needed for unstructured data. If you receive an HTTP response consisting
    of what buttons on a website a user clicked on, and you wish to see if they bought
    an item, it is quite simple to see if the Buy button was one of the buttons they
    clicked. On the other hand, if they entered some review text into a form and you
    want to know if the review mentions a shipping issue, you will need to process
    unstructured data (the text of the review).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于处理结构化数据，传统规则通常效果很好，但对于非结构化数据，则需要ML。如果您收到一个HTTP响应，其中包含用户在网站上点击的按钮信息，并且您想知道他们是否购买了某个商品，那么检查购买按钮是否是他们点击的按钮之一就很简单。另一方面，如果他们在表单中输入了一些评论文本，并且您希望知道评论是否提到了运输问题，则需要处理非结构化数据（评论文本）。
- en: Now that you have a better grasp of what ML is and when to use it, let’s examine
    the best way to adopt it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您更好地掌握了ML是什么以及何时使用它，让我们来看看采用它的最佳方式。
- en: Buy, Adapt, or Build?
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 购买、适应还是构建？
- en: Once you decide that you need some AI capability, the question shifts to whether
    this is something you can buy off the shelf or it is something you have to custom
    build. An AI cloud architect increasingly has to choose the areas where buying
    an off-the-shelf capability is the better approach and the areas where it is possible
    to build a better, more differentiated capability. This decision may also be driven
    by the need to prioritize scarce data scientist resources.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定您需要某些AI能力，问题就转向是否可以购买现成的解决方案或者是否必须自定义构建。AI云架构师越来越需要选择在哪些领域购买现成的解决方案更为合适，以及在哪些领域可以构建更好、更具差异化的能力。这个决定可能还受限于优先考虑稀缺的数据科学家资源的需求。
- en: Let’s look at the decision process and some scenarios and capabilities where
    one or the other approach is better.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看决策过程以及某些场景和能力，其中一个或另一个方法更好。
- en: Data Considerations
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据考虑
- en: In general, the quality of an ML model improves with the size and quality of
    the data used to train it. Dramatic ML performance and accuracy are driven by
    improvements in data size. Also, ML systems need to be retrained for new situations.
    For example, if you have a recommendation system for expense categories and you
    want to provide recommendations for hotel accommodations, you can’t use the same
    recommendations model. You have to train it in the second instance on hotels that
    were picked by other employees at the travel destination. So even though the model
    code may be the same, you have to retrain the model with new data for new situations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，机器学习模型的质量会随着用于训练它的数据量和质量的提高而提高。数据规模的增加推动了机器学习性能和准确性的显著改善。此外，机器学习系统需要针对新情况进行重新训练。例如，如果你有一个推荐系统用于费用类别，并且你想为酒店住宿提供推荐，那么你不能使用同样的推荐模型。你必须在第二种情况下训练它，即在出差目的地被其他员工选择的酒店上进行训练。因此，即使模型代码可能相同，你也必须为新情况使用新数据重新训练模型。
- en: 'Taking these two concepts together (you get a better ML model when you have
    more data, and an ML model typically needs to be retrained for a new situation),
    you get a strategy that can tell you whether to buy or build:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 综合考虑这两个概念（当你拥有更多数据时，你会得到一个更好的机器学习模型；而一个机器学习模型通常需要为新情况重新训练），你会得到一个策略，可以告诉你是买还是自建：
- en: Determine if the buyable model is solving the *same problem* that you want to
    solve. Has it been trained on the same input and on similar labels? Let’s say
    you’re trying to do a product search, and the model has been trained on catalog
    images as inputs. But you want to do a product search based on users’ mobile phone
    photographs of the products. The model that was trained on catalog images won’t
    work on your mobile phone photographs, and you’d have to build a new model. But
    let’s say you’re considering a vendor’s translation model that’s been trained
    on speeches in the European Parliament. If you want to translate similar speeches,
    the model works well as it uses the same kind of data.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定可购买模型是否解决了*与你想解决的同样问题*。它是否已经在相同的输入和类似的标签上进行了训练？假设你想进行产品搜索，而模型已经在目录图片上进行了训练。但你想基于用户手机拍摄的产品照片进行产品搜索。那么在目录图片上训练的模型将无法处理你的手机照片，你将不得不建立一个新模型。但假设你正在考虑一个供应商的翻译模型，该模型是在欧洲议会的演讲中进行训练的。如果你想翻译类似的演讲，该模型会很好地工作，因为它使用了相同类型的数据。
- en: Does the vendor have *more data* than you do? If the vendor has trained their
    model on speeches in the European Parliament but you have access to more speech
    data than they have, you should build. If they have more data, then buy their
    model.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 供应商是否拥有比你更多的数据？如果供应商在欧洲议会的演讲上训练了他们的模型，但你比他们拥有更多的演讲数据，那么你应该自建。如果他们拥有更多的数据，那么购买他们的模型就合适。
- en: 'In a nutshell, if the vendor has a solution that solves the problem you want
    to solve, and they have more/better data, then you should buy it. If not, build
    your own. Increasingly, there is a third choice: to adapt a prebuilt model. Let’s
    look at architectural and other considerations in each of these three cases.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，如果供应商有一个解决你想解决问题的解决方案，并且他们有更多/更好的数据，那么你应该购买它。如果没有，那就自建。越来越多的选择是：调整一个预构建模型。让我们分别看看这三种情况中的架构和其他考虑因素。
- en: When to Buy
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时购买
- en: When a credible service provider offers a turnkey service or API that works
    for your problem, you should seriously evaluate purchasing that service. It will
    almost always be less expensive and better maintained than something you can build—remember
    that a popular service will be able to amortize its development budget among hundreds
    of users, whereas you will have to pay for all improvements yourself.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个可信的服务提供商提供适合你的问题的即插即用服务或API时，你应该认真评估购买该服务。它几乎总是比你可以自己构建的东西更便宜和更易维护——请记住，一个流行的服务可以在数百用户中分摊其开发预算，而你必须自己支付所有改进的费用。
- en: Moreover, the fact that there is a marketplace of applications means that the
    capability is not core to your business. Plus, your team can be doing something
    more distinctive and differentiated than implementing something that can be purchased.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，应用市场的存在意味着该能力对你的业务不是核心。另外，你的团队可以做一些比实施可购买的东西更具独特性和差异性的事情。
- en: When alternatives are offered by other clouds or credible third parties, it
    is possible to run a bakeoff on your own workload and choose the one that fits
    best. Buying ML capabilities is one of the places where you don’t have to worry
    about what cloud you use primarily—in many cases, these are simply APIs, and you
    can invoke them regardless of which cloud your application is running on. For
    example, you can run your website on AWS, but it might invoke Google’s Translate
    API on the text of reviews.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当其他云或可信第三方提供了替代方案时，您可以对自己的工作负载进行评估，并选择最适合的解决方案。购买ML能力是一个您无需担心使用哪个云的地方——在许多情况下，这些只是API，无论您的应用程序在哪个云上运行，都可以调用它们。例如，您可以在AWS上运行您的网站，但它可能会调用Google的Translate
    API来处理评论文本。
- en: 'There are three levels of abstraction at which you can buy AI capabilities:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以购买AI能力的三个抽象级别：
- en: SaaS
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: SaaS
- en: Some AI capabilities are made available through turnkey APIs. For example, Retail
    Search from Google Cloud is a turnkey service that can be used by any business
    to provide ML-powered search within its ecommerce site. Amazon Connect is a turnkey
    service that provides contact center capabilities. Azure’s Computer Vision API
    can be invoked by sending it an image and getting back information of what’s in
    the image. All of these are examples of services that are invoked through APIs
    directly used to integrate with your applications. These models are trained on
    diverse datasets and are likely more accurate and more resilient than anything
    you can put together.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一些AI能力通过即插即用的API提供。例如，来自Google Cloud的Retail Search是一个即插即用的服务，任何企业都可以使用它来提供其电子商务网站内的ML支持搜索。Amazon
    Connect是一个即插即用的服务，提供联系中心能力。Azure的计算机视觉API可以通过发送图像来调用，并获取图像中包含的信息。所有这些都是通过API直接用于与您的应用程序集成的服务示例。这些模型经过多种数据集的训练，可能比您可以自行组合的任何东西更准确和更强大。
- en: Building blocks
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 基础构件
- en: Sometimes the capability that can be bought off the shelf is not something that
    constitutes a complete business use case. Nevertheless, you might want to use
    that AI capability as a building block in your applications. For exam­ple, you
    might be able to use the entity extraction or form parser capabilities of Google’s
    DocAI in your NLP pipeline. Azure Computer Vision could be used to prescan images
    for adult content before you ingest those images into your system. Amazon Rekognition
    can be used to identify people in an onboarding system.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，可以购买的能力并不构成完整的业务用例。尽管如此，您可能希望将该AI能力作为应用程序中的基础构件使用。例如，您可以在NLP管道中使用Google DocAI的实体提取或表单解析功能。在将这些图像导入系统之前，Azure计算机视觉可以用于预扫描图像中的成人内容。Amazon
    Rekognition可用于识别入职系统中的人员。
- en: Enterprise applications
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 企业应用程序
- en: Sometimes, the capability is too bespoke to buy as SaaS and too complex to be
    available as a building block. In such cases, you might be able to nevertheless
    purchase the capability as a combination of products and services. The supplier
    has experience implementing the capability in your industry and can reuse quite
    a bit of its earlier work to build the capability for you. The relative mix of
    reusable code to on-site code varies between providers and from use case to use
    case. For example, C3 AI will build an anti-money laundering solution off your
    first-party data but leverage its experience building similar answers for other
    financial firms.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，能力过于定制化，无法作为SaaS购买，也太复杂而无法作为基础构件提供。在这种情况下，您仍然可能购买组合产品和服务的能力。供应商在您的行业中实施该能力的经验，并可以重复使用其较早的工作来为您构建能力。可重用代码与现场代码的相对比例因供应商和用例而异。例如，C3
    AI将根据您的第一方数据构建反洗钱解决方案，但利用其在其他金融公司构建类似解决方案的经验。
- en: The source of such capabilities is not limited to the major cloud providers.
    The marketplace on Google Cloud, AWS, and Azure all have a wide selection of SaaS
    capabilities from a wide variety of vendors. Systems integrators (like Accenture
    and Wipro) and AI application providers (like [C3 AI](https://oreil.ly/SHnU8),
    SpringML, and Brightloom) offer AI solutions to improve customer engagement, pricing,
    digital marketing, revenue intelligence, yield optimization, and so on.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此类能力的来源不仅限于主要的云提供商。Google Cloud、AWS和Azure的市场都有来自各种供应商的广泛选择的SaaS能力。系统集成商（如Accenture和Wipro）和AI应用提供商（如[C3
    AI](https://oreil.ly/SHnU8)、SpringML和Brightloom）提供AI解决方案，以改善客户参与、定价、数字营销、收入智能、产量优化等方面。
- en: What Can You Buy?
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 您可以购买什么？
- en: A cloud architect in a company that is adopting AI has to become familiar with
    vendor choice and vendor management. Knowledge of ML frameworks like TensorFlow
    and MLOps platforms like SageMaker and Vertex AI is not sufficient.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个正在采用人工智能的公司的云架构师必须熟悉供应商选择和供应商管理。了解像TensorFlow这样的ML框架和像SageMaker和Vertex AI这样的MLOps平台的知识是不够的。
- en: With the explosive growth in AI-based startups and the addition of AI practices
    at global and regional systems integrators, the number of capabilities you can
    buy rather than build continues to dramatically expand. Whatever we write in this
    section will be obsolete in months, so we will paint with a broad brush and encour­age
    you to do research, ask for demos, and make a well-informed decision. You could
    save months of effort and dramatically lower the risk of failure by choosing the
    right vendor.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于人工智能的初创公司的爆炸性增长以及全球和地区系统集成商增加AI实践，可以购买而不是构建的能力数量继续大幅扩展。我们在本节写的任何内容几个月后都将过时，因此我们将以大致的概述进行讨论，并鼓励您进行研究、要求演示，并做出明智的决定。通过选择合适的供应商，您可以节省数月的努力，并显著降低失败风险。
- en: 'Here are some capabilities that existed in early 2023:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些2023年初存在的能力：
- en: Better customer service
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的客户服务
- en: There are a number of AI solutions that improve every step of the customer service
    chain, from automatic handling of calls, creating chat transcripts, and improving
    call center agent efficiency to obtaining product insights. The solutions are
    usually omnichannel and can support voice, chat, and email. Generative AI solutions
    exist that can search through your document archives and provide answers to user
    queries.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多AI解决方案可以改善客户服务链的每一个步骤，从自动处理电话、创建聊天记录到提高呼叫中心代理效率和获取产品见解。这些解决方案通常是全渠道的，可以支持语音、聊天和电子邮件。还存在生成式AI解决方案，可以搜索您的文档存档并为用户查询提供答案。
- en: Workflow assistance
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流辅助
- en: Coding assistants can improve software engineering productivity. Workflow copilots
    can streamline many corporate functions such as reviewing legal contracts and
    processing invoices.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 编码助手可以提高软件工程的生产力。工作流合作者可以简化许多企业功能，如审查法律合同和处理发票。
- en: Improving marketing spend
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 提升营销支出
- en: Digital user activity provides a lot of signals, and the impact of digital marketing
    is measurable. This leads to a number of AI solutions around privacy-safe marketing
    analytics, brand measurement, lookalike audiences, and improving campaign performance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数字用户活动提供了大量信号，数字营销的影响是可衡量的。这导致了许多围绕隐私安全营销分析、品牌测量、相似受众和提高活动表现的AI解决方案。
- en: Writing sales and marketing content
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 编写销售和市场营销内容
- en: Generative AI solutions to craft campaigns and help write copy also exist. They
    can also use alternative datasets and first-party data to craft responses to RFPs,
    personalized invoices, sales emails, etc.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 存在生成式AI解决方案来制作广告活动并帮助撰写文案。它们还可以使用替代数据集和第一方数据来为RFP、个性化发票、销售电子邮件等提供响应。
- en: Recommendations
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐
- en: Product and next-best-action recommendations are so widespread that these capabilities
    are available as industry-specific, easy-to-deploy solutions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 产品和下一步行动建议已经广泛存在，这些能力作为行业特定的、易于部署的解决方案提供。
- en: Publicly available or gatherable data
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 公开可用或可收集的数据
- en: Capabilities that are based on publicly available or gatherable data such as
    social media posts, weather, internet images, stock market trades, etc., are often
    available for purchase.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基于公开可用或可收集的数据（如社交媒体帖子、天气、网络图像、股市交易等）的能力通常可供购买。
- en: Retail
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 零售
- en: All sorts of capabilities, from shelf management to omnichannel marketing attribution,
    demand forecasting, and price optimization, are available for purchase.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从货架管理到全渠道营销归因、需求预测和价格优化，各种能力都可以购买。
- en: Manufacturing
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 制造业
- en: While equipment may vary, capabilities like preventive/predictive maintenance,
    yield optimization, and quality control are available and can be customized.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然设备可能各不相同，但预防/预测性维护、产量优化和质量控制等能力可供选择，并可定制。
- en: Supply chain management
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 供应链管理
- en: Many capabilities, from reading customs forms to inventory management and managing
    drivers’ routes, are available for purchase.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从阅读海关表单到库存管理和司机路线管理等许多能力都可以购买。
- en: Healthcare
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗保健
- en: Many stages of pharmaceutical research, detection of many diseases and medical
    conditions, and optimal use of facilities at healthcare providers have corresponding
    capabilities.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 许多阶段的制药研究，检测许多疾病和医疗条件以及医疗提供者设施的最佳利用具有相应的能力。
- en: Media and entertainment
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 媒体和娱乐
- en: Many capabilities such as tracking of a puck in hockey, estimating the likelihood
    of a high-scoring football game, creating thumbnails of images and highlights
    of videos, etc., are already available.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 许多能力，如在冰球比赛中跟踪冰球，估计高得分足球比赛的可能性，创建图像缩略图和视频亮点等，已经可用。
- en: Back-office operations
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 后勤运营
- en: Many back-office operations can be improved by automating the work. There are
    robot process automation (RPA) solutions in everything from insurance application
    processing to classifying invoices. Generative AI solutions exist that can use
    first-party data to automatically fill forms and invoke backend agents through
    APIs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 许多后勤操作可以通过自动化工作来改进。从保险申请处理到分类发票，无所不在的机器人流程自动化（RPA）解决方案。生成AI解决方案存在，可以使用第一方数据自动填写表单并通过API调用后端代理。
- en: Financial services
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 金融服务
- en: Several capabilities like personalization of customer interactions, embedded
    finance solutions, value-at-risk and “what-if” analysis, and real-time fraud analysis
    are available.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 多种能力，如客户互动的个性化，嵌入式金融解决方案，风险价值和“假设”分析，以及实时欺诈分析，都是可用的。
- en: We are avoiding naming example providers of the preceding capabilities because
    there will doubtless be many more options by the time you are reading this. In
    any case, before you commit to building, find out how good the off-the-shelf solutions
    are at addressing your problem. You might be pleasantly surprised.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们避免命名上述能力的示例提供者，因为在您阅读本文时，肯定会有更多选项。无论如何，在决定构建之前，请查明现成解决方案在解决您问题方面的效果如何。您可能会感到惊讶。
- en: How Adapting Works
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适应工作原理
- en: Training a state-of-the-art (SOTA) image classification model requires more
    than 10 million images. Chances are that you don’t have that kind of a large dataset
    for the use case that you are interested in.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 训练最先进的图像分类模型（SOTA）需要超过1000万张图像。您感兴趣的用例可能没有这样大规模的数据集。
- en: The solution is to take a SOTA model trained on an appropriately large dataset
    that is similar to yours (often, this is called a *pretrained model* or *foundational
    model*) and then adapt that SOTA model to fit your data. Your data can be as few
    as 10 images consisting of both positive and negative examples, although the more
    (high-quality) data you have, the better the model will perform.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是采用在与您的类似的适当大规模数据集上训练的SOTA模型（通常称为*预训练模型*或*基础模型*），然后调整该SOTA模型以适应您的数据。您的数据可以仅为10张图像，包括正面和负面示例，尽管您拥有的（高质量）数据越多，模型的表现就越好。
- en: Adapting is an in-between choice, between buying and building. You get the benefits
    of the vendor’s large dataset and the customization afforded by your own data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 适应是在购买和构建之间的中间选择。您既可以享受供应商的大数据集带来的好处，又可以根据自己的数据进行定制。
- en: 'There are two ways to do this adaptation:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以进行这种适应：
- en: Transfer learning
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习
- en: This keeps 99%+ of the original model and tunes the remaining weights on your
    data. Specifically, it keeps the 99% of the model that has learned how to extract
    information from data and then trains the remaining 1% of the model that operates
    on the extracted information to come up with the result.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这保留了原始模型的99%以上，并调整了剩余的权重以适应您的数据。具体来说，它保留了模型中的99%，该模型已经学会如何从数据中提取信息，然后训练剩余的1%的模型，该模型操作提取的信息来得出结果。
- en: Fine-tuning
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 微调
- en: This keeps the entire original model and tunes the entire model on your data
    but keeps the magnitude of the weight updates small enough that the original information
    that was learned is not completely lost. Instead of modifying the weights directly,
    some “low-rank” generative AI fine-tuning methods train an ancillary model that
    adapts the weights of the foundational model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这保留了整个原始模型，并调整了整个模型以适应您的数据，但保持了权重更新的幅度足够小，以便原始学习的信息不会完全丢失。与直接修改权重不同，一些“低秩”生成AI微调方法会训练一个辅助模型，该模型调整基础模型的权重。
- en: You can find pretrained models that are ready for adaptation on sites such as
    [TensorFlow Hub](https://oreil.ly/GRfl5), [PyTorch Hub](https://oreil.ly/wAxQj),
    and [Hugging Face](https://oreil.ly/zJbVe). These tend to be open source models
    for building block capabilities such as image classification, optical character
    recognition, and speech to text. Fine-tune or transfer learn these models with
    your own data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [TensorFlow Hub](https://oreil.ly/GRfl5), [PyTorch Hub](https://oreil.ly/wAxQj),
    和 [Hugging Face](https://oreil.ly/zJbVe) 等网站上找到预训练模型，这些模型通常是用于构建像图像分类、光学字符识别和语音转文本等功能的开源模型。使用自己的数据来进行模型的微调或迁移学习。
- en: Pretrained core LLMs such as OpenAI GPT and PaLM are trained for general purposes
    to solve common language problems across industries. It is possible to fine-tune
    these models for specific tasks, such as creating a checklist for medical procedures.
    However, it is important to recognize the limitations of fine-tuning in the case
    of generative AI. To teach the model new information, it is necessary to retrain
    (not just adapt) the models on that industry’s data. Labeled industry-specific
    datasets are crucial for training the model to accurately recognize medical entities
    or classify medical texts.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的核心 LLM（如 OpenAI GPT 和 PaLM）是为解决跨行业常见语言问题而训练的。可以对这些模型进行微调以执行特定任务，例如创建医疗程序检查单。然而，在生成式
    AI 的情况下，要向模型教授新信息，需要在该行业数据上重新训练（而不仅仅是适应）模型。标记的行业特定数据集对于训练模型以准确识别医疗实体或分类医疗文本至关重要。
- en: Hyperscaler ML platforms provide a fully managed experience for adapting pretrained
    models. For example, AutoML Translate provides a way to adapt Google’s Translate
    API with your own phrase pairs—for example, of industry-specific language. The
    resulting model can do all the translation that Google’s Translate API does but
    also knows about the industry-specific terms that you have added. Similar options
    exist on other hyperscalers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 超大规模机器学习平台提供完全托管的体验，以适应预训练模型。例如，AutoML Translate 提供了一种方法，可以使用自己的短语对自适应谷歌翻译 API，例如行业特定语言。生成的模型可以执行谷歌翻译
    API 所能做的所有翻译，同时了解您添加的行业特定术语。其他超大规模云提供商也提供类似的选项。
- en: 'So the full decision criterion becomes: buy the vendor’s solution if it’s trained
    on the same problem and has access to more data than you do. Then, consider if
    you can adapt (transfer learn from or fine-tune) the vendor’s solution if you
    have unique data of your own to train it to do custom tasks. As with any situation
    where you incorporate externally created artifacts into your software, you will
    have to make sure that the vendors are in compliance with your legal requirements.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，完整的决策标准变成了：如果供应商的解决方案在同一问题上进行了训练，并且可以访问比您更多的数据，则购买供应商的解决方案。然后，考虑如果您有自己的独特数据，是否可以调整（从供应商那里学习迁移或微调）供应商的解决方案，以便执行定制任务。与将外部创建的工件整合到软件中的任何情况一样，您必须确保供应商符合您的法律要求。
- en: Examples of things that you can buy, adapt, or build from cloud providers and
    partners are summarized in [Table 10-1](#example_cloud_capabilities_that_you_can).
    This is an incomplete list even at the time of writing and doubtless will have
    grown by the time you read this. The inclusion of a technology (e.g., Azure ML
    Computer Vision or AWS Lex) in [Table 10-1](#example_cloud_capabilities_that_you_can)
    does not imply any endorsement over similar services available in other clouds
    (AWS Rekognition, GCP Vision API, Azure ML NLP, GCP DocAI, etc.).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从云服务提供商和合作伙伴购买、适应或构建的事例总结在 [Table 10-1](#example_cloud_capabilities_that_you_can)
    中。即使在撰写时这已经是一个不完整的列表，阅读时无疑会增长。在 [Table 10-1](#example_cloud_capabilities_that_you_can)
    中包含技术（例如 Azure ML 计算机视觉或 AWS Lex）并不意味着对其他云中类似服务（例如 AWS Rekognition、GCP Vision
    API、Azure ML NLP、GCP DocAI 等）的认可。
- en: Table 10-1\. Example cloud capabilities that you can buy, adapt, or build
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Table 10-1\. 可以购买、适应或构建的示例云能力
- en: '| Strategy | Example ML capabilities | Why |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 示例 ML 能力 | 原因 |'
- en: '| --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Buy SaaS |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 购买 SaaS |'
- en: Amazon Forecast
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Forecast
- en: Amazon Connect
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Connect
- en: Azure ML Computer Vision
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure ML 计算机视觉
- en: GCP Retail Search
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP 零售搜索
- en: GCP Translate API
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP 翻译 API
- en: Google Enterprise Search
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google 企业搜索
- en: '| Turnkey service or API that can be easily integrated. These models are trained
    on diverse datasets and are likely better than anything you can put together.
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 可以轻松集成的即插即用服务或 API。这些模型是在多样化数据集上训练的，可能比您可以组合的任何东西都要好。 |'
- en: '| Buy building blocks |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 购买构建块 |'
- en: AWS Lex
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Lex
- en: Amazon Polly
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Polly
- en: Hugging Face on AWS
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 上的 Hugging Face
- en: Azure OpenAI GPT
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI GPT
- en: GCP DocAI form parser
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP DocAI 表单解析器
- en: GCP NLP entity extraction
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP NLP 实体提取
- en: '| Use as building blocks in your ML applications. Why reinvent the wheel for
    functionality that is not core to your business? Plus, it is rare that you will
    have enough data to train a better model. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 用作您的 ML 应用程序的构建块。为非核心业务功能重新发明轮子是不必要的吗？此外，您很少会有足够的数据来训练更好的模型。 |'
- en: '| Buy enterprise application |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 购买企业应用程序 |'
- en: AWS Preventive Maintenance
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 预防性维护
- en: Azure Price Optimization
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure 价格优化
- en: GCP Contact Center AI
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP 联系中心 AI
- en: GCP Recommendations AI
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP 推荐 AI
- en: GCP Med-PaLM
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP Med-PaLM
- en: C3 AI Yield Optimization
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C3 AI 收益优化
- en: Symphony Retail
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Symphony Retail
- en: '| Combination of products and services to deploy and configure the application
    to meet your organization’s needs. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 结合产品和服务来部署和配置应用程序，以满足您组织的需求。 |'
- en: '| Adapt |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 适应 |'
- en: AWS Jumpstart
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 快速入门
- en: Azure OpenAI Studio
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure OpenAI Studio
- en: GCP AutoML Vision
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP AutoML 视觉
- en: GCP AutoML Video Intelligence
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP AutoML 视频智能
- en: Vertex AI Fine-tune PaLM
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vertex AI 微调 PaLM
- en: '| Fine-tune AI models on your own tasks. Until you get to incredibly large
    datasets, these are likely to outperform custom models built solely on your data.
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 在自己的任务上微调 AI 模型。在您拥有非常大的数据集之前，这些模型很可能优于仅基于您数据构建的定制模型。 |'
- en: '| Build |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 构建 |'
- en: AWS SageMaker
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS SageMaker
- en: Azure Machine Learning
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure 机器学习
- en: GCP Vertex AI
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP Vertex AI
- en: GCP Dialogflow
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP Dialogflow
- en: '| Managed frameworks to build end-to-end ML models. This is covered in [Chapter 11](ch11.html#architecting_an_ml_platform).
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 管理框架用于构建端到端的 ML 模型。这在 [Chapter 11](ch11.html#architecting_an_ml_platform)
    中有详细介绍。 |'
- en: The strategies in [Table 10-1](#example_cloud_capabilities_that_you_can) are
    tailored to different sets of technical skills. While the first three approaches
    require system-integration capabilities, data scientists are required for the
    last two.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[Table 10-1](#example_cloud_capabilities_that_you_can) 中的策略专为不同技术技能集定制。前三种方法需要系统集成能力，而最后两种则需要数据科学家。'
- en: Next, let’s look at the considerations once you have determined that the AI
    capability you need has to be built by your team.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看一旦确定您需要的 AI 能力必须由您的团队构建时需要考虑的事项。
- en: AI Architectures
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 架构
- en: 'Broadly speaking, there are seven types of AI applications that are currently
    successful:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，目前有七种成功的 AI 应用类型：
- en: Understanding unstructured data
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 理解非结构化数据
- en: For example, you may have a photograph of a piece of equipment and you would
    like the ML model to identify the model number. Understanding the content of images,
    videos, speech, and natural language text fall into this category of applications.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可能有一张设备的照片，并希望 ML 模型能够识别型号编号。理解图像、视频、语音和自然语言文本的内容属于这类应用的范畴。
- en: Generating unstructured data
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 生成非结构化数据
- en: You may receive a set of documents as “discovery” during a lawsuit and may need
    to summarize the information received. Or you may want to create a personalized
    ad based on the last five purchases of the customer, weaving them into a story.
    It is now possible to generate text, images, music, and videos using AI.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能在诉讼中接收一组文件作为“发现”，并可能需要总结收到的信息。或者您可能希望基于客户的最后五次购买创建个性化广告，将它们编织成一个故事。现在可以使用
    AI 生成文本、图像、音乐和视频。
- en: Predicting outcomes
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果
- en: For example, you have information about the basket of items in a shopping cart
    and the purchase history of the shopper, and you’d like to determine whether the
    shopping cart is likely to be abandoned without a purchase happening. Predicting
    the likelihood or magnitude of an event falls into this category of applications.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您了解购物车中物品的信息以及购物者的购买历史，希望确定购物车是否可能在没有发生购买的情况下被放弃。预测事件的发生可能性或程度属于这类应用的范畴。
- en: Forecasting values
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 预测数值
- en: For example, you would like to predict the number of items in a particular category
    that will be sold at a particular store over the next week. Predicting time-varying
    quantities like inventory levels at a future point in time falls into this category.
    The difference between predicting an outcome and forecasting a value is not always
    clear. In predicting an outcome, the other attributes are the primary determinant
    of whether the event will happen. In forecasting, the pattern of change over time
    is the most important factor. There are some problems that you might have to try
    out both ways.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可能希望预测特定类别的商品在未来一周内在特定商店的销售数量。预测未来时间点的库存水平等时间变化数量属于这一类。预测结果与预测值的区别并不总是清晰的。在预测结果中，其他属性是事件发生的主要决定因素。在预测中，时间变化的模式是最重要的因素。有些问题您可能需要尝试两种方法。
- en: Anomaly detection
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测
- en: In many cases, the goal is to identify unusual events or behavior. For example,
    in a gaming scenario, anomaly detection might be employed to identify cases where
    cheating might be happening.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，目标是识别不寻常的事件或行为。例如，在游戏场景中，可以使用异常检测来识别作弊事件。
- en: Personalization
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化
- en: You might want to recommend products or craft experiences based on what the
    user and other users like this user have liked in the past.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望基于用户和其他用户（像这个用户）过去喜欢的内容来推荐产品或制定体验。
- en: Automation
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化
- en: This is where you take a process that is carried out inefficiently today and
    attempt to replace part or all of it using ML.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您采用机器学习来替代部分或全部目前效率低下的流程的地方。
- en: In this section, we will look at canonical architectures and technology choices
    for each of these. All these architectures will be built on top of the platform
    discussed in the next chapter.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分别讨论这些问题的经典架构和技术选择。所有这些架构都将建立在下一章讨论的平台之上。
- en: Understanding Unstructured Data
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解非结构化数据
- en: The capability of deep learning to understand the content of images, videos,
    speech, and natural language text has kept on getting better over time.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习理解图像、视频、语音和自然语言文本内容的能力随着时间的推移而不断提高。
- en: Just a few years ago, the capability of speech recognition models lagged far
    behind that of humans. Now, while not perfect, speech-to-text models are routinely
    used to caption uploaded videos and live speeches. The same sort of improvement
    has been seen in all the other types of unstructured data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 就在几年前，语音识别模型的能力远远落后于人类。现在，虽然不完美，语音转文本模型常用于对上传的视频和现场演讲进行字幕。所有其他类型的非结构化数据也看到了同样的改进。
- en: Deep learning does not require the feature engineering step that is usually
    necessary for traditional ML methods. Because of this, you can build deep learning
    systems in a very generic way. The ML model architecture required to differentiate
    screws from nails will also work if you need to distinguish fractured bones from
    intact ones. Note that we said the same *architecture*, but not the same *model*.
    This is because the model in the first instance will be trained on images of screws
    and nails whereas the model in the second instance will be trained on images of
    broken versus unbroken bones. While the architecture is the same, the weights
    will be different.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习不需要传统机器学习方法通常需要的特征工程步骤。因此，您可以以非常通用的方式构建深度学习系统。用于区分螺钉和钉子的ML模型架构也将适用于需要区分骨折骨头和完整骨头的情况。请注意，我们说的是相同的*架构*，而不是相同的*模型*。这是因为第一个示例中的模型将训练于螺钉和钉子的图像，而第二个示例中的模型将训练于断裂和完整骨头的图像。虽然架构相同，但权重会不同。
- en: The research community has developed and published model architectures for standard
    problems in image, video, speech, and text analysis. Unlike the prebuilt models
    discussed in the section on buying AI, these models have not been trained on very
    large datasets, or if they have, the characteristics of the datasets they have
    been trained on are very different from your data—you will have to train the models
    from scratch using your own data.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 研究界已经为图像、视频、语音和文本分析中的标准问题开发并发布了模型架构。与在购买人工智能部分讨论的预构建模型不同，这些模型未经过大型数据集的训练，或者即使经过了训练，它们所训练的数据集特征与您的数据有很大不同——您将需要使用自己的数据从头开始训练这些模型。
- en: Therefore, when it comes to problems that involve unstructured data, it is usually
    not necessary to design your own model architecture. Just pick a model that is
    SOTA or close to it and train it on your data. All the major cloud providers provide
    easy mechanisms to do this. They usually start with unstructured data held in
    AWS S3, Google Cloud Storage, or Azure Blob Storage. Then use a no-code or low-code
    service such Vertex AI AutoML to train and deploy the model (see [Figure 10-2](#for_unstructured_datacomma_use_no_codec)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在涉及非结构化数据的问题时，通常不必设计自己的模型架构。只需选择一个最先进或接近最先进的模型，并在您的数据上进行训练。所有主要的云服务提供商都提供了便捷的机制来实现这一点。它们通常从存储在
    AWS S3、Google Cloud Storage 或 Azure Blob Storage 中的非结构化数据开始。然后使用无代码或低代码服务，如 Vertex
    AI AutoML 来训练和部署模型（参见 [图 10-2](#for_unstructured_datacomma_use_no_codec)）。
- en: '![For unstructured data, use no-code, end-to-end ML services such as AutoML
    in Google Cloud Vertex AI or AWS SageMaker](assets/adml_1002.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![对于非结构化数据，请使用无代码、端到端的机器学习服务，例如 Google Cloud Vertex AI 中的 AutoML 或 AWS SageMaker](assets/adml_1002.png)'
- en: Figure 10-2\. For unstructured data, use no-code, end-to-end ML services such
    as AutoML in Google Cloud Vertex AI or AWS SageMaker
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 对于非结构化数据，使用无代码、端到端的机器学习服务，如 Google Cloud Vertex AI 中的 AutoML 或 AWS
    SageMaker
- en: The hardest thing might be to supply the AutoML model with *labels* for the
    training data—the label is the correct answer. It might involve human labeling.
    You can use crowd compute services like Amazon Mechanical Turk or SageMaker Ground
    Truth or third-party providers of equivalent services to avoid the drudgery of
    doing the labeling yourself.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最困难的事情可能是为训练数据的 AutoML 模型提供*标签*—标签是正确答案。这可能涉及人工标注。您可以使用类似于 Amazon Mechanical
    Turk、SageMaker Ground Truth 或第三方提供的等效服务的众包计算服务，以避免自己进行标注的单调工作。
- en: Generating Unstructured Data
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成非结构化数据
- en: Generative AI is a type of AI that can create new content, such as text, images,
    music, and video. It does this by learning the structure of existing data (such
    as what English words tend to follow other English words) and then using that
    statistical knowledge to generate new content that is realistic. The architectures
    that work in generative AI are rapidly evolving at the time this book is going
    to press (in October 2023). Doubtless, by the time you are reading this, the choices
    will be broader and/or more streamlined.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能是一种可以创建新内容（如文本、图像、音乐和视频）的人工智能类型。它通过学习现有数据的结构（例如哪些英语单词倾向于跟随其他英语单词）然后利用这些统计知识生成逼真的新内容。在本书出版时（2023年10月），生成式人工智能的架构正在快速发展中。毫无疑问，在您阅读本书时，选择会更加广泛和/或更加精简。
- en: At the time of writing, most use cases of generative AI involve invoking a pretrained
    model offered by an LLM vendor through an API that takes a text prompt as input.
    This is called *zero-shot learning*. For example, images can be created by supplying
    a text prompt to an image generation API such as DALL-E on Azure OpenAI, Imagen
    on GCP, Stable Diffusion on AWS SageMaker Jumpstart, or Midjourney. Similarly,
    you can ask PaLM on Vertex AI, GPT-4 on Azure OpenAI, Cohere on AWS Bedrock, etc.,
    to generate an outline for a new article by sending it a natural language request.
    Code completion and code generation in response to natural language descriptions
    of the code to be generated are supported by the different hyperscalers (GitHub
    Copilot on Azure, Codey on Google Cloud, CodeWhisperer on AWS) and by independents
    such as Tabnine and Replit. Crafting the language of the prompt is important,
    leading to *prompt engineering* as an experimentation and development framework.
    In hybrid architectures, use managed LLMs offered by startups such as Perplexity
    AI and Ryax. Use a microservices architecture to build products that employ zero-shot
    learning, with the LLM being one of the microservices. While prompt engineering
    is good for prototyping, version changes in the models can cause existing prompts
    to break. [Most practitioners guard against this by fine-tuning a model under
    their control for production use cases](https://oreil.ly/sJtXI).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，大多数生成式AI的用例涉及通过接受文本提示作为输入的API调用预训练模型。这被称为*零样本学习*。例如，通过向像Azure OpenAI上的DALL-E、GCP上的Imagen、AWS
    SageMaker Jumpstart上的Stable Diffusion或Midjourney提供文本提示，可以生成图像。同样地，您可以通过向Vertex
    AI上的PaLM、Azure OpenAI上的GPT-4、AWS Bedrock上的Cohere等发送自然语言请求，要求生成一篇新文章的大纲。通过不同的超大规模计算提供商（GitHub
    Copilot在Azure上，Codey在Google Cloud上，CodeWhisperer在AWS上）以及像Tabnine和Replit这样的独立开发者支持，可以根据生成的代码的自然语言描述进行代码补全和代码生成。制定提示语的语言很重要，这导致*提示工程*成为一个实验和开发框架。在混合架构中，使用由Perplexity
    AI和Ryax等初创公司提供的托管LLM。使用微服务架构来构建采用零样本学习的产品，LLM是其中之一。虽然提示工程对于原型设计很有帮助，但模型的版本更改可能导致现有提示失败。[大多数从业者通过在其控制下微调模型以用于生产用例来防范这种情况](https://oreil.ly/sJtXI)。
- en: It is possible to fine-tune LLMs on the public clouds using their platforms.
    Commonly, this is done for instruction tuning (teaching the LLM new tasks) so
    that the LLM can respond to prompts that it doesn’t currently know how to handle
    or that it handles somewhat poorly, or to guard against breaking changes. Fine-tuning
    is done by training the LLM for a few epochs using supervised parameter-efficient
    methods (PEFT or SFT) such as Quantized Low-Rank Adaptation (QLoRA). You can fine-tune
    proprietary models such as OpenAI GPT-4 and Google Cloud PaLM through services
    that they provide and somewhat open LLMs such as Falcon, Dolly, and Llama 2 on
    PaaS ML platforms such as SageMaker, Databricks, and Vertex AI.^([1](ch10.html#ch01fn10))
    Open source model hubs (such as Hugging Face) are increasingly popular as a way
    to abstract out the necessary code. Deploy fine-tuned LLMs as APIs on ML platforms
    such as SageMaker, as you would any other ML model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在公共云平台上使用它们的平台来对LLM进行微调。通常情况下，这是为了指导调优（教LLM新任务），以便LLM可以回应它目前不知道如何处理或处理得不太好的提示，或者防止断裂变化。微调是通过使用监督的参数高效方法（如PEFT或SFT）对LLM进行几个epochs的训练来完成的，例如Quantized
    Low-Rank Adaptation（QLoRA）。您可以通过它们提供的服务对专有模型进行微调，例如OpenAI GPT-4和Google Cloud PaLM，以及在PaaS
    ML平台（如SageMaker、Databricks和Vertex AI）上对开放性LLM（如Falcon、Dolly和Llama 2）进行微调。^([1](ch10.html#ch01fn10))
    开源模型中心（如Hugging Face）越来越受欢迎，作为抽象出必要代码的一种方式。部署微调的LLM作为ML平台上的API，就像您处理任何其他ML模型一样。
- en: Instruction tuning can teach the model new tasks but not new knowledge (at the
    time of writing, that requires retraining the model from scratch on new data).
    To teach LLMs new knowledge cheaply, it is common to use patterns that take advantage
    of an LLM’s ability to solve new tasks from a few examples (*few-shot learning*),
    to use information *stuffed* into the prompt (*retrieval-augmented generation*),
    or to generate structured data that can be passed into an external API (*agents*).
    For example, to answer questions, the embedding of a question or its hypothetical
    answer is searched against a vector database of document chunks to find relevant
    documents, and the LLM is asked to summarize these document chunks to answer the
    question posed (see [Figure 10-3](#architecture_for_retrieval_augmented_ge)).
    Open source abstraction layers (such as LangChain) are becoming popular ways to
    implement these patterns. Architect production use cases in such a way that each
    step of the chain is a container in an ML pipeline. Containerized ML pipelines
    are covered in the next chapter.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 调节指令可以教会模型新的任务，但不能教会新的知识（在撰写本文时，这需要从头开始重新训练模型，使用新数据）。为了便宜地教导LLM学习新知识，通常会利用一些模式，这些模式利用LLM在少量示例（*few-shot
    learning*）中解决新任务的能力，利用嵌入到提示中的信息（*retrieval-augmented generation*），或者生成结构化数据以供传递到外部API（*agents*）。例如，要回答问题，会将问题或其假设答案的嵌入搜索到文档块的向量数据库中，以找到相关文档，并要求LLM总结这些文档块来回答提出的问题（参见
    [图 10-3](#architecture_for_retrieval_augmented_ge)）。开源抽象层（如LangChain）正变得流行，用来实现这些模式。在这样的设计中，每个链条步骤都是ML管道中的一个容器。容器化的ML管道将在下一章讨论。
- en: '![Architecture for retrieval-augmented generation, a pattern for question-answering
    using LLMs](assets/adml_1003.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![检索增强生成架构，用于使用LLM回答问题的模式](assets/adml_1003.png)'
- en: Figure 10-3\. Architecture for retrieval-augmented generation, a pattern for
    question-answering using LLMs
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 检索增强生成架构，用于使用LLM回答问题的模式
- en: Predicting Outcomes
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测结果
- en: The historical data that will serve as training data for predicting outcomes
    usually exists in logs data and transactional databases. For example, if you are
    interested in predicting whether a shopping cart will be abandoned, you have to
    get the history of all website activity—this will be present in logs data. You
    might choose to include information such as the price of the item, discounts in
    effect, and whether the customer actually paid for the item. While some or all
    of this data might also be retrieved from website logs, it might be easier to
    query them from a database. Thus, the historical data also contains the labels
    in this situation (see [Figure 10-4](#outcome_prediction_models_often_involve)).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 将作为预测结果的历史数据通常存在于日志数据和交易数据库中。例如，如果您想预测购物车是否会被放弃，您必须获取所有网站活动的历史记录—这些信息通常存在于日志数据中。您可能选择包括商品价格、优惠信息以及客户是否实际支付商品等信息。虽然这些数据的一部分或全部也可能从网站日志中检索，但从数据库中查询可能更为简单。因此，在这种情况下，历史数据也包含标签（参见
    [图 10-4](#outcome_prediction_models_often_involve)）。
- en: '![Outcome prediction models often involve training the model to predict the
    values of one column in the DWH based on other column values](assets/adml_1004.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![Outcome prediction models often involve training the model to predict the
    values of one column in the DWH based on other column values](assets/adml_1004.png)'
- en: Figure 10-4\. Outcome prediction models often involve training the model to
    predict the values of one column in the DWH based on other column values
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-4\. 预测结果模型通常涉及训练模型，根据DWH中其他列的值预测其中一列的值
- en: Therefore, the architecture (see [Figure 10-5](#architecture_of_trainingcomma_deploying))
    involves using replication mechanisms and connectors to land all the necessary
    data into a DWH. It is then possible to train regression or classification ML
    models on the DWH directly—both Amazon Redshift and Google BigQuery support the
    ability to train models directly from SQL and defer more complex models underneath
    the covers to SageMaker and Vertex AI, respectively. The model, once trained,
    can be exported and deployed into the managed prediction service on both clouds.
    Typically, all the predictions or at least a sample of them are stored back in
    the DWH to support continuous evaluation and drift detection.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该架构（参见图[10-5](#architecture_of_trainingcomma_deploying)）涉及使用复制机制和连接器将所有必要的数据引入数据仓库。然后可以直接在数据仓库上训练回归或分类ML模型——Amazon
    Redshift和Google BigQuery都支持直接从SQL训练模型，并将更复杂的模型推迟到SageMaker和Vertex AI的底层。一旦训练完成，模型可以导出并部署到两个云上的托管预测服务中。通常，所有预测结果或至少其中一部分将存储回数据仓库，以支持持续评估和漂移检测。
- en: '![Architecture of training, deploying, and monitoring an outcome prediction
    model](assets/adml_1005.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![建模、部署和监控结果预测模型的架构](assets/adml_1005.png)'
- en: Figure 10-5\. Architecture of training, deploying, and monitoring an outcome
    prediction model
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5\. 建模、部署和监控结果预测模型的架构
- en: Training is relaunched anytime there is a sufficient amount of new data or if
    continuous evaluation identifies that the model or one of its features has drifted.
    This evaluation is carried out by comparing the predictions against the true values
    that are obtained after the fact. For example, predictions of whether a shopping
    cart will be abandoned can be compared a day later with the carts that were abandoned.
    The evaluation query is carried periodically by means of a scheduler.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 只要有足够的新数据或者连续评估确定模型或其特征已漂移，就会重新启动训练。此评估通过将预测与事后获得的真实值进行比较来进行。例如，可以将预测的购物车是否被放弃与一天后实际放弃的购物车进行比较。评估查询定期通过调度程序执行。
- en: Forecasting Values
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测数值
- en: Most ML models do inference—they “predict” the value of an unobserved value
    based on other factors. For example, an ML model might predict whether a transaction
    is fraudulent based on factors such as the transaction amount and the location
    from which the credit card is being used. Key is that the input factors do not
    include past values of the thing (fraud) to be predicted (see [Figure 10-6](#inference_versus_forecasting)).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数ML模型进行推理——它们基于其他因素“预测”未观察到的值。例如，ML模型可能根据交易金额和信用卡使用地点等因素预测交易是否欺诈。关键在于输入因素不包括要预测的事物（欺诈）的过去值（见图[10-6](#inference_versus_forecasting)）。
- en: In contrast, in a forecasting problem, one of the inputs to the ML model is
    a previous value of the thing to be predicted. For example, if you are trying
    to predict the number of sandwiches that will be sold 3 days from now, one of
    the inputs might be the number of sandwiches that were sold in each of the past
    28 days.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在预测问题中，ML模型的输入之一是要预测的事物的先前值。例如，如果您试图预测未来3天将销售的三明治数量，则其中一个输入可能是过去28天中每天销售的三明治数量。
- en: '![Inference versus time series forecasting](assets/adml_1006.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![推理与时间序列预测](assets/adml_1006.png)'
- en: Figure 10-6\. Inference versus time series forecasting
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6\. 推理与时间序列预测
- en: 'Notice that, in essence, forecasting problems require you to feedback the ground
    truth of past predictions. This real-time feedback loop makes the data engineering
    architecture considerably more complex. One approach is shown in [Figure 10-7](#architecture_for_time_series_analytics).
    Here, training is done on historical data in the DWH, with the data collected
    using windows in SQL. These are queries that partition the data into segments
    that consist of the current row and a certain number of preceding rows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在本质上，预测问题要求您反馈过去预测的实际情况。这种实时反馈循环使得数据工程架构变得更加复杂。一个方法如图[10-7](#architecture_for_time_series_analytics)所示。在这里，训练是在数据仓库中的历史数据上进行的，使用SQL中的窗口收集数据。这些查询将数据分区为包含当前行和一定数量前导行的段：
- en: '[PRE0]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Architecture for time-series analytics with separate batch and streaming
    pipelines](assets/adml_1007.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![具有分离批处理和流处理管道的时间序列分析架构](assets/adml_1007.png)'
- en: Figure 10-7\. Architecture for time-series analytics with separate batch and
    streaming pipelines
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-7\. 具有分离批处理和流处理管道的时间序列分析架构
- en: 'In real time, however, the time windowing needs to be carried out within a
    streaming pipeline. The streaming pipeline pulls the 28 days of data and sends
    it to the forecasting model for prediction. It also takes care of keeping the
    DWH updated so that the next training run happens on up-to-date data. The problem
    with this approach is that you need to maintain two data pipelines: a batch pipeline
    for training based on historical data and a streaming pipeline for forecasting
    based on real-time data.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实时情况下，时间窗口需要在流式处理管道中进行。流处理管道提取过去 28 天的数据并将其发送到预测模型进行预测。它还负责保持数据仓库的更新，以便下一次训练运行基于最新的数据。这种方法的问题在于需要维护两个数据管道：一个基于历史数据进行训练的批处理管道和一个基于实时数据进行预测的流处理管道。
- en: A much better approach is to use a data pipeline framework such as Apache Beam
    that provides the way to execute the same code on both batch and stream datasets.
    This limits the exposure to training-serving skew, which is caused by having different
    preprocessing steps happen in training versus during prediction. Such an architecture
    is shown in [Figure 10-8](#architecture_using_a_data_pipeline_tech).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是使用数据管道框架（如 Apache Beam），它提供了在批处理和流处理数据集上执行相同代码的方式。这限制了由于训练与预测时的不同预处理步骤导致的训练服务偏差。这样的架构如
    [图 10-8](#architecture_using_a_data_pipeline_tech) 所示。
- en: '![Architecture using a data pipeline technology such as Apache Beam that supports
    both batch and stream](assets/adml_1008.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![使用数据管道技术（例如支持批处理和流处理的Apache Beam）的架构](assets/adml_1008.png)'
- en: Figure 10-8\. Architecture using a data pipeline technology such as Apache Beam
    that supports both batch and stream
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-8\. 使用数据管道技术（例如支持批处理和流处理的Apache Beam）的架构
- en: Anomaly Detection
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常检测
- en: In anomaly detection on time-varying data, there are two time windows. Data
    over the longer time window (say, one month) is clustered. Then, data over the
    shorter time window is compared with the clusters. If data in the shorter window
    is unlike the larger data (i.e., if the distance between the new data and the
    clusters is high), an anomaly is flagged.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在时变数据的异常检测中，有两个时间窗口。较长的时间窗口（比如一个月）内的数据被聚类。然后，较短时间窗口内的数据与这些聚类进行比较。如果较短时间窗口内的数据与较大数据（即新数据与聚类之间的距离较大）不同，则会标记异常。
- en: The architecture here is similar to the forecasting case depicted in [Figure 10-8](#architecture_using_a_data_pipeline_tech),
    except there are two time windows that need to be maintained. Make sure that the
    streaming pipeline infrastructure you choose (such as Cloud Dataflow, AWS Kinesis,
    Apache Flink, or Confluent Kafka) is capable of maintaining the length of time
    window that you require.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的架构与 [图 10-8](#architecture_using_a_data_pipeline_tech) 中描绘的预测情况类似，只是需要维护两个时间窗口。确保所选择的流处理管道基础设施（如
    Cloud Dataflow、AWS Kinesis、Apache Flink 或 Confluent Kafka）能够满足您需要的时间窗口长度要求。
- en: In anomaly detection on individual events, the principle is the same except
    that the second (shorter time) window consists of a single event or user session.
    So events over the past month are aggregated to learn what usual events look like.
    Outliers from the distribution are treated as anomalies. For example, if the goal
    is to identify bots in a multiplayer game, the typical behavior of players (time
    between moves, likelihood of repeated moves, likelihood of subsequent moves involving
    different hands, etc.) is collected and aggregated. Any player who deviates significantly
    from the overall distribution is flagged.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在个体事件的异常检测中，原则是相同的，只是第二个（较短时间）窗口由单个事件或用户会话组成。因此，过去一个月内的事件被聚合以学习正常事件的模式。从分布中的离群值被视为异常。例如，如果目标是识别多人游戏中的机器人，将收集和聚合玩家的典型行为（移动间隔时间、重复移动的可能性、后续移动涉及不同手的可能性等）。任何明显偏离整体分布的玩家都会被标记。
- en: Personalization
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 个性化
- en: One of the most common uses of ML is to tailor an experience (such as a game),
    product (such as a music playlist), service (such as a spam filter), or communication
    (such as a sales outbound) on an individual basis. Doing so in the context of
    respecting users’ privacy concerns requires some fundamental guarantees about
    data confidentiality and security, including whether information leaks from one
    user to another.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习最常见的用途之一是根据个体的需求定制体验（如游戏）、产品（如音乐播放列表）、服务（如垃圾邮件过滤器）或通讯（如销售外呼）。在尊重用户隐私的前提下进行此类操作需要对数据保密性和安全性做一些基本保证，包括信息是否会从一个用户泄露到另一个用户。
- en: Such tailoring requires knowledge of the preferences not only of the specific
    individual but also of individuals like the user for which the personalization
    is happening (e.g., next best action for a marketing funnel). The tailoring will
    also try to highlight items similar to items that this individual has liked or
    positively reacted to in the past. Therefore both the demographics of the individual
    (expressed as customer segments, such as “single, age in the 20s, urban, nonsubscriber”)
    and the characteristics of the item (e.g., price, release date, etc.) are key
    features to a personalization ML model (see [Figure 10-9](#architecture_for_personalization_that_i)).
    Given a person-item pair, the personalization model computes a score for how interested
    the individual would be in the item. Starting from a list of candidate items (such
    as all the items meeting the user’s search query or all the ongoing sales promotions),
    the personalization model is thus able to rank the items in a way that captures
    that user’s preferences.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这种定制需要了解不仅特定个体的偏好，还有像进行个性化的用户一样的个体的偏好（例如，营销漏斗中下一步最佳行动）。定制还将尝试突出显示与该个体过去喜欢或积极反应的项目类似的项目。因此，个体的人口统计信息（表现为客户细分，例如“单身，20多岁，城市，非订阅者”）和项目的特征（例如价格，发布日期等）是个性化机器学习模型的关键特征（见[图 10-9](#architecture_for_personalization_that_i)）。给定一个人-项目对，个性化模型计算个体对该项目的兴趣程度得分。从候选项目列表开始（例如所有符合用户搜索查询的项目或所有正在进行的促销活动），个性化模型因此能够按照捕捉用户偏好的方式对项目进行排名。
- en: A good personalization model will also need to handle the *cold-start* problem—if
    you have a new user, you still need to provide a reasonable experience to them,
    and if you have a new item, you still need the model to recommend the item to
    some customers. The cold-start problem is typically handled as a preprocessing
    step that looks for users or items with insufficient history and provides a reasonable
    experience to them (for example, that you recommend the most popular items over
    less popular ones).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的个性化模型还需要处理*冷启动*问题——如果有一个新用户，仍然需要为他们提供一个合理的体验；如果有一个新项目，仍然需要模型向某些客户推荐该项目。冷启动问题通常作为预处理步骤处理，寻找历史不足的用户或项目，并为他们提供一个合理的体验（例如，推荐最流行的项目而不是不太流行的项目）。
- en: '![](assets/adml_1009.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/adml_1009.png)'
- en: Figure 10-9\. Architecture for personalization that involves only a small number
    of potential choices; the deployed model is part of a pipeline that also includes
    handling of cold starts and pruning of recent purchases
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-9\. 仅涉及少量潜在选择的个性化架构；部署的模型是一个包括处理冷启动和修剪最近购买的流水线的一部分。
- en: A personalization model will usually need to do some postprocessing on the recommendations.
    For example, if the user has just seen *The Shawshank Redemption* in your video-streaming
    service, you probably do not want to recommend the same movie again. Postprocessing
    consists of rules such as these to prune the automatically generated list of recommendations.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化模型通常需要对推荐结果进行一些后处理。例如，如果用户刚在您的视频流服务中观看了《肖申克的救赎》，您可能不希望再次推荐同一部电影。后处理包括这类规则，用于修剪自动生成的推荐列表。
- en: The architecture shown in [Figure 10-9](#architecture_for_personalization_that_i)
    is the same as that of outcome prediction (depicted in [Figure 10-5](#architecture_of_trainingcomma_deploying))
    except that there are additional preprocessing and postprocessing filters required.
    Therefore, the model is deployed as a pipeline. The preprocessing step requires
    that a statistical model (e.g., the most popular item) be used to create a model
    that can be used in cold-start situations. The postprocessing step requires filtering
    out recent purchases and requires a connection to the real-time transactional
    database.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-9](#architecture_for_personalization_that_i)所示的架构与结果预测（在[图 10-5](#architecture_of_trainingcomma_deploying)中描述）的架构相同，只是需要额外的预处理和后处理过滤器。因此，该模型被部署为一个流水线。预处理步骤要求使用统计模型（例如，最流行的项目）来创建一个可以在冷启动情况下使用的模型。后处理步骤要求过滤掉最近的购买，并且需要连接到实时事务数据库。'
- en: The architecture in [Figure 10-9](#architecture_for_personalization_that_i)
    breaks down if the number of candidate items is too large. For example, when you
    visit the YouTube homepage, you are presented with a set of recommended videos.
    The candidate space for that list is the entirety of the YouTube catalog. Ranking
    the entire catalog for you at the moment you visit the webpage is not feasible.
    Therefore, when the list of candidates is large, the architecture consists of
    a batch pipeline that precomputes a smaller list of candidate recommendations
    for each user. The frequency at which this batch pipeline runs might vary depending
    on how active the user is, and these precomputed candidates can be supplemented
    with trending topics for an experience that is not stale. This is shown in [Figure 10-10](#when_the_overall_space_of_candidates_is).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-9](#architecture_for_personalization_that_i)中的架构在候选项数量过大时会崩溃。例如，当您访问YouTube首页时，会呈现一组推荐视频。该列表的候选空间是整个YouTube目录。在您访问网页时对整个目录进行排名是不可行的。因此，当候选列表较大时，架构包括一个批处理管道，为每个用户预先计算出较小的候选推荐列表。此批处理管道运行的频率可能根据用户的活跃程度而变化，并且这些预计算的候选项可以通过流行话题进行补充，以确保体验不会过时。这在[图 10-10](#when_the_overall_space_of_candidates_is)中有所体现。
- en: '![When the overall space of candidates is large, a smaller list of candidates
    is precomputed using a scheduled batch pipeline](assets/adml_1010.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![当候选总空间较大时，通过预定的批处理管道预先计算出较小的候选列表](assets/adml_1010.png)'
- en: Figure 10-10\. When the overall space of candidates is large, a smaller list
    of candidates is precomputed using a scheduled batch pipeline
  id: totrans-246
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-10\. 当候选总空间较大时，通过预定的批处理管道预先计算出较小的候选列表。
- en: 'Let’s have a look now at the last type of AI application: automation.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看最后一种AI应用类型：自动化。
- en: Automation
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化
- en: Like personalization, automation of back-office processes requires multiple
    models and pre- and postprocessing filters. Each of the models has to be trained
    on its own data, and the deployed pipeline consists of the ML models invoked as
    a directed acyclic graph (DAG).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 与个性化类似，后台流程的自动化需要多个模型以及前后处理过滤器。每个模型都必须基于自己的数据进行训练，并且部署的管道是作为有向无环图（DAG）调用的ML模型。
- en: The ML models themselves may not all be trained from scratch. Building block
    models (such as a form parser, translation tool, etc.) are common components of
    these automation model pipelines.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ML模型本身可能并非全部从头开始训练。构建块模型（如表单解析器、翻译工具等）是这些自动化模型管道的常见组件。
- en: Typically, in automation pipelines, the model decision on high-confidence predictions
    is automatically carried out, but some provision is made for human oversight,
    as shown in [Figure 10-11](#an_automation_pipeline_consists_of_many). For example,
    a small sample of high-confidence predictions is sent to a human expert for ongoing
    validation. This is termed *human-over-the-loop*. In addition, decisions that
    are costly, low-confidence, or difficult to reverse are also subject to human
    approval. This is termed *human-in-the-loop*. Both are important.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在自动化管道中，模型对高置信度预测的决策是自动执行的，但也考虑到人类监督，如[图 10-11](#an_automation_pipeline_consists_of_many)所示。例如，会将一小部分高置信度预测发送给人类专家进行持续验证。这被称为*人在循环中*。此外，涉及昂贵、低置信度或难以逆转的决策也需要人类批准。这被称为*人在环中*。这两者都非常重要。
- en: Only high-value predictions from any model are fed to the next step of the ML
    pipeline. Low-confidence predictions from any of the models mean that the input
    is sent to a queue for human processing. A key component of automation pipelines
    is labeling—it is essential that the human decision on these low-confidence predictions
    is captured and treated as a label. Once labeled, this data is added to the training
    data and used in subsequent retraining.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 任何模型的高价值预测结果都会被传递到ML管道的下一步骤。如果任何模型的低置信度预测结果，输入会被发送到队列进行人工处理。自动化管道的一个关键组成部分是标记——必须捕获和处理这些低置信度预测的人类决策作为标签。一旦标记，这些数据就会添加到训练数据中，并用于后续的重新训练。
- en: '![An automation pipeline consists of many ML models chained together](assets/adml_1011.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![自动化管道由多个ML模型链式连接而成](assets/adml_1011.png)'
- en: Figure 10-11\. An automation pipeline consists of many ML models chained together
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-11\. 自动化管道由多个ML模型链式连接而成。
- en: 'The ML models are trained and deployed depending on their type (e.g., AutoML,
    prebuilt APIs, or custom models). The set of models is tied into a pipeline and
    orchestrated in a system such as Kubeflow Pipelines for which managed services
    exist in the public cloud. The queue could be any distributed tasks queue: AWS
    Simple Queue Service, Google Cloud Pub/Sub, etc. The training data will be on
    cloud object storage or in a DWH, depending on whether it is unstructured or structured.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其类型（例如 AutoML、预建 API 或定制模型），机器学习模型将被训练和部署。模型集成到管道中，并在诸如 Kubeflow Pipelines
    等系统中进行编排，这些系统在公共云中存在托管服务。队列可以是任何分布式任务队列：AWS Simple Queue Service、Google Cloud
    Pub/Sub 等。训练数据将存储在云对象存储或数据仓库中，具体取决于数据是结构化的还是非结构化的。
- en: Responsible AI
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任的 AI
- en: 'Because ML is such a powerful tool, you have to be careful to not misuse it.
    Be aware of your organization and government policies and regulations. Consider
    this use case, for example: you wish to reduce the time people spend waiting on
    an elevator. So you want to automatically identify the people walking into the
    lobby of the building and use your knowledge of which floor their office is on.
    That way, you can optimally direct the individuals into elevators to minimize
    the number of stops. This use case would violate privacy regulations in many jurisdictions,
    as photographing and tracking people may be forbidden.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习是如此强大的工具，您必须小心不要滥用它。请意识到您的组织和政府的政策和法规。例如，考虑这样一个用例：您希望减少人们等待电梯的时间。因此，您想自动识别走进大厦大堂的人，并利用您对他们的办公室楼层的了解。这样，您可以最优化地引导个人进入电梯，以最小化停止次数。在许多司法管辖区中，这种用例可能违反隐私法规，因为拍摄和追踪人员可能是被禁止的。
- en: Because ML models are never perfect, you have to be careful about using ML in
    situations where decisions are made automatically without any human intervention.
    Zillow, famously, had to [shut down its automated home-buying models](https://oreil.ly/atiBv)
    after it ended up building excess inventory—people whose houses were undervalued
    would not sell to Zillow, but people whose houses were overvalued would rush to
    sell.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习模型永远不完美，您在决策完全自动化且无人干预的情况下使用机器学习时必须小心。著名的 Zillow 在建立过多库存后不得不 [关闭其自动购房模型](https://oreil.ly/atiBv)
    ——房屋被低估的人不会卖给 Zillow，但被高估的人会急于出售。
- en: Note
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For a more detailed coverage of these concepts, we recommend [*Responsible Machine
    Learning*](https://oreil.ly/wJ34A) by Patrick Hall, Navdeep Gill, and Benjamin
    Cox (O’Reilly).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 欲深入了解这些概念，请参考 [*负责任的机器学习*](https://oreil.ly/wJ34A)，作者为 Patrick Hall、Navdeep
    Gill 和 Benjamin Cox（O’Reilly）。
- en: AI Principles
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI 原则
- en: 'Your organization will probably have set up principles for when and where AI
    can be used and not used. As an example, these are [Microsoft’s AI principles](https://oreil.ly/XuW7E):'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 您的组织可能已经为 AI 的使用和不使用设立了原则。例如，这些是 [Microsoft 的 AI 原则](https://oreil.ly/XuW7E)。
- en: Fairness
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性
- en: AI systems should treat all people fairly.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: AI 系统应该公平对待所有人。
- en: Reliability and safety
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠性和安全性
- en: AI systems should perform reliably and safely.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: AI 系统应该可靠且安全运行。
- en: Privacy and security
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私和安全
- en: AI systems should be secure and respect privacy.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: AI 系统应该安全并尊重隐私。
- en: Inclusiveness
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 包容性
- en: AI systems should empower everyone and engage people.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: AI 系统应该赋予每个人力量并与人们互动。
- en: Transparency
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度
- en: AI systems should be understandable.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: AI 系统应该易于理解。
- en: Accountability
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 责任
- en: People should be accountable for AI systems.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 人们应对 AI 系统负责。
- en: Microsoft’s Responsible AI Toolbox is a collection of integrated tools and functionalities
    to support the responsible use of AI by developers. The company also helped create
    a dataset named [ToxiGen](https://oreil.ly/MedGc) to enable ML practitioners to
    detect toxicity against several minority groups.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft 的负责任 AI 工具箱是一组集成工具和功能，旨在支持开发人员负责任地使用 AI。该公司还帮助创建了一个名为 [ToxiGen](https://oreil.ly/MedGc)
    的数据集，以帮助机器学习从业者检测针对多个少数群体的毒性。
- en: 'As another example, here are [Google’s AI principles](https://oreil.ly/6moeJ)
    and how they propel or constrain Google’s use of AI in their own products:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是 [Google 的 AI 原则](https://oreil.ly/6moeJ)，它们如何推动或限制 Google 在其产品中使用 AI：
- en: Be socially beneficial.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 社会效益
- en: For example, Google Maps uses AI to suggest the most [eco-friendly route](https://oreil.ly/-2R2z).
    Google estimates that eco-friendly routing has the potential to prevent more than
    one million tons of carbon emissions per year, an obvious societal benefit.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，谷歌地图利用AI建议最环保的路线。谷歌估计，环保路线有潜力每年减少超过一百万吨碳排放，这显然是社会的一个好处。
- en: Avoid creating or reinforcing bias.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 避免创建或加强偏见。
- en: In February 2021, Google Cloud’s Vision API [stopped attaching gendered labels](https://oreil.ly/RSNwL)
    like “man” or “woman.” The problem was that automatically identifying gender leads
    to use cases that create or perpetuate bias.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年2月，谷歌云的视觉API [停止附加性别标签](https://oreil.ly/RSNwL)如“男性”或“女性”。问题在于自动识别性别会导致创造或延续偏见的用例。
- en: Be built and tested for safety.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为安全而建造和测试。
- en: Waymo, Google’s self-driving car project, [shared its safety framework](https://oreil.ly/_qK3U)
    when opening its fully autonomous ride-hailing service to the public.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Waymo，谷歌的自动驾驶汽车项目，[在向公众开放其完全自动驾驶的乘车服务时分享了其安全框架](https://oreil.ly/_qK3U)。
- en: Be accountable to people.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对人们负责。
- en: For example, even though the Google Nest Learning Thermostat can learn users’
    daily routines and automatically create a temperature schedule, users can turn
    Auto-Schedule off.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，即使谷歌Nest学习恒温器可以学习用户的日常活动并自动创建温度时间表，用户也可以关闭自动调度功能。
- en: Incorporate privacy design principles.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 结合隐私设计原则。
- en: In order for Google Assistant to reduce mistriggering when the device thinks
    it heard users say “Hey Google,” the on-device model is fine-tuned to the user
    of that device. However, this uses federated learning, a privacy-enhancing technology
    that can work without sending users’ raw data to Google servers.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少谷歌助手在设备误听用户说“Hey Google”时的误触发，设备上的模型被调整到与该设备的用户相适应。然而，这使用了联邦学习，这是一种增强隐私的技术，可以在不将用户原始数据发送到谷歌服务器的情况下工作。
- en: Uphold high standards of scientific excellence.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 维护高科学卓越标准。
- en: Google publishes a lot of its work at reputed scientific conferences such as
    [Neu⁠r­IPS](https://nips.cc). By making publishing at peer-reviewed venues a key
    part of the role for engineers working in ML, Google fosters high scientific standards.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在知名科学会议上发表了许多作品，例如[Neu⁠r­IPS](https://nips.cc)。通过使在机器学习领域工作的工程师在同行评议的期刊上发表成果成为其角色的关键部分，谷歌促进了高科学标准。
- en: Be made available for uses that accord with these principles.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 应该提供符合这些原则的用途。
- en: Google makes AI technologies available both as open source (e.g., TensorFlow,
    BERT, etc.) and through Google Cloud (Vertex AI, Retail Search, etc.).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌将AI技术作为开源（例如TensorFlow、BERT等）和通过谷歌云（Vertex AI、Retail Search等）提供。
- en: Do not design or deploy AI if the technology is likely to cause overall harm.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如果技术可能会造成整体伤害，则不要设计或部署AI。
- en: For example, do not design or deploy AI if its primary purpose is as a weapon
    that injures people, does surveillance, or contravenes human rights. In 2020,
    for example, the Google Cloud CEO said that the company had received confirmation
    from the US Customs and Border Patrol that its technology [would not be used for
    immigration enforcement](https://oreil.ly/s9wlB) at the border.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果其主要目的是作为伤害人员的武器、进行监视或违反人权的AI，则不要设计或部署。例如，2020年，谷歌云CEO表示，公司已从美国海关和边境保护局获得确认，其技术[不会用于边境的移民执法](https://oreil.ly/s9wlB)。
- en: While the Google or Microsoft principles may not fit your company’s mission
    and goals (for example, if you work in the defense industry, the last principle
    will definitely not be applicable), verify with your AI governance board before
    embarking on a new AI project and double-check for any recent changes in regulations,
    as they are frequently updated.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然谷歌或微软的原则可能不适合贵公司的使命和目标（例如，如果您在国防行业工作，则最后一个原则肯定不适用），在着手新的AI项目之前，请与您的AI治理委员会核实，并检查最近的法规变化，因为它们经常更新。
- en: ML Fairness
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习公平性
- en: You also have to be careful about the use of historical data in training ML
    models that make irreversible decisions that affect people, because the ML model
    will pick up the biases inherent in the historical data. For example, consider
    whether it is responsible to use an ML model that is based on historical data
    of previous jail sentences to recommend the length of jail time for people convicted
    of a crime today. If prosecutors and judges in real life tend to be prejudiced
    against racial minorities, the [ML model will pick up that bias](https://oreil.ly/VxVdt)—even
    if race is not an explicit input to the ML model, it can get picked up implicitly
    based on where the convicted person lives or the crime they are charged with.
    This is not an easy problem to resolve—pretty much any measure of fairness that
    you can think of (not using protected attributes, achieving classification parity,
    or calibrated outcomes independent of protected attributes) [turns out to be problematic](https://oreil.ly/TmD41).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 您还必须谨慎使用训练ML模型的历史数据，这些模型做出会影响人们的不可逆决策，因为ML模型会吸收历史数据中固有的偏见。例如，考虑到是否负责使用基于以前监禁时长历史数据的ML模型，来推荐今天犯罪被判者的监禁时长。如果现实生活中的检察官和法官倾向于对少数族裔持有偏见，[ML模型会吸收到这种偏见](https://oreil.ly/VxVdt)
    — 即使种族并不是ML模型的显式输入，也可以基于被判人所在地或其被指控的犯罪而隐含地捕捉到。这并不是一个容易解决的问题 — 您可以考虑的几乎所有公平措施（不使用受保护属性，实现分类平等，或者独立于受保护属性的校准结果）[都可能存在问题](https://oreil.ly/TmD41)。
- en: There are a number of tools that are part of cloud ML frameworks to help with
    diagnosing ML fairness issues. For example, it is possible to do sliced evaluations,
    carry out what-if tests, and continuously monitor model performance on protected
    criteria.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 云ML框架中有多种工具可用于帮助诊断ML公平性问题。例如，可以进行切片评估，进行假设测试，并持续监控模型在受保护标准上的表现。
- en: Explainability
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性
- en: Often, stakeholders (end users, regulators) will refuse to adopt an ML model
    unless they get some indication of why the model is making the recommendation
    that it is making. Few doctors will accept an automated X-ray model that simply
    says whether there is a fracture or not without any other explanation. Few regulators
    will take a bank’s assurance that it is not being discriminatory in its loan approvals
    unless the bank can explain the rationale behind its loan decisions. Explainability
    can also be important for explaining to laypeople how the model works and the
    risks inherent to it, to build societal acceptance for technologies such as self-driving
    cars. This also plays out in organizational training, as everyone within the company,
    from marketing to sales to data teams, must understand how the model is making
    recommendations against the data available.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 经常，利益相关者（最终用户、监管者）会拒绝采纳ML模型，除非他们能够得到模型作出推荐的原因的某些指示。很少有医生会接受一个仅仅告诉是否有骨折的自动化X光模型，而没有任何其他解释。很少有监管者会接受银行保证其在贷款批准中不会歧视，除非银行能够解释其贷款决策背后的理由。解释性对于向非专业人士解释模型工作原理及其固有风险，并建立社会对自动驾驶汽车等技术的接受度也很重要。这在组织培训中也是如此，因为公司内的每个人，从营销到销售再到数据团队，都必须理解模型如何根据可用数据做出推荐。
- en: Cloud ML frameworks provide explainable AI (XAI) tools that can help you understand
    how your ML models make decisions. This isn’t a complete step-by-step deconstruction
    of an AI model, which can be close to impossible if you’re attempting to trace
    the millions of parameters used in deep learning algorithms. Instead, XAI aims
    to provide insights into how models work, so human experts are able to understand
    the logic that goes into making a decision.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 云ML框架提供了可解释AI（XAI）工具，可以帮助您理解ML模型如何做出决策。这并不是对AI模型的完整逐步分解，如果尝试追溯深度学习算法中使用的数百万参数几乎是不可能的。相反，XAI旨在提供关于模型工作方式的洞见，使人类专家能够理解做出决策的逻辑。
- en: 'When applied successfully, XAI offers three important benefits:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 成功应用时，XAI提供三个重要好处：
- en: Increases trust in the ML model
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 增强对ML模型的信任
- en: XAI tools can be used to provide clear and understandable explanations of the
    reasoning that led to the model’s output. Say you are using a deep learning model
    to analyze medical images like X-rays; you can use XAI to produce saliency maps
    that highlight the pixels that were used to get the diagnosis.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性工具（XAI）可以用来提供清晰易懂的解释，说明了模型输出的推理过程。假设您正在使用深度学习模型来分析医学图像，如X光片；您可以使用XAI生成显著性地图，突出显示用于进行诊断的像素。
- en: Provides a way to troubleshoot ML
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了一种解决机器学习问题的方法
- en: Explainability in AI can also enable you to debug a model and troubleshoot how
    well a model is working. The What-If Tool allows you to change one of the model
    inputs and see what the model would have done in that case.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: AI 的可解释性还可以帮助您调试模型并排查模型运行情况。What-If工具允许您更改模型输入的一个变量，并查看在那种情况下模型会做出什么样的预测。
- en: Identifies fairness issues
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 识别公平性问题
- en: For example, you might have a model to identify when cars are making illegal
    lefthand turns. The explanation might suggest that the model, instead of focusing
    on cars turning left illegally, is looking to see if there is a pothole. This
    influence could be caused by a skewed dataset that contained a large number of
    images taken on poorly maintained roads, or even real bias, where a ticket might
    be more likely to be given out in an underfunded area of a city.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可能有一个模型用于识别汽车何时进行非法左转。解释可能会建议，与其集中于汽车非法左转，模型更可能是在寻找是否有坑洞。这种影响可能是由一个包含大量在道路维护不良地区拍摄的图像的倾斜数据集引起的，甚至是真实的偏见，即在城市中的资金不足地区可能更有可能被处以罚款。
- en: Cloud ML frameworks provide XAI support during development as part of their
    notebook development experience. For example, What-If Tools are integrated into
    Vertex AI Workbench. They also provide XAI support for deployed models so that
    model predictions can have explanations attached to them. Saliency maps are integrated
    into Vertex AI Predictions. Because the What-If Tool from Google is open source,
    it can be used from notebooks on other clouds as well. Both Azure Machine Learning
    and AWS SageMaker, like Vertex AI Predictions, offer support for explaining predictions.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 云ML框架在开发过程中提供XAI支持，作为笔记本开发体验的一部分。例如，What-If工具已集成到Vertex AI Workbench中。它们还为部署的模型提供XAI支持，以便模型预测可以附带解释。显著性地图已集成到Vertex
    AI Predictions中。由于Google的What-If工具是开源的，因此它也可以从其他云上的笔记本中使用。Azure机器学习和AWS SageMaker与Vertex
    AI Predictions一样，提供了解释预测的支持。
- en: Summary
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter has provided you with an overview of the most relevant things
    you should consider to effectively include AI and ML applications in your data
    platform perimeter. The key takeaways are as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为您提供了一个关于如何有效地将AI和ML应用到您的数据平台边界中的最相关事项的概述。关键要点如下：
- en: AI is the field of study that solves problems by getting computers to think
    and act like humans.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI 是通过使计算机像人类一样思考和行动来解决问题的研究领域。
- en: ML is a subfield of AI that solves AI problems using data instead of custom
    logic.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习是使用数据而不是定制逻辑解决AI问题的AI的一个子领域。
- en: Deep learning is a subfield of ML that has been successfully used to understand
    unstructured data like speech, images, video, and natural language.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子领域，成功应用于理解语音、图像、视频和自然语言等非结构化数据。
- en: Generative AI can be used to create unstructured data.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式AI可以用来创建非结构化数据。
- en: Generative models can be used to generate text, images, music, and video. They
    are commonly invoked as APIs that take text prompts as inputs.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型可以用来生成文本、图像、音乐和视频。它们通常作为API被调用，接受文本提示作为输入。
- en: It is possible to fine-tune large models to make them do tasks for which they
    were not trained. This is usually done in a parameter-efficient way using methods
    such as QLoRA.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以对大型模型进行微调，使它们执行它们原本未经训练的任务。这通常是以QLoRA等方法进行的参数高效方式。
- en: It is possible to use patterns such as retrieval-augmented generation and few-shot
    learning to get a generative model to learn new information.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用检索增强生成和少样本学习等模式来让生成模型学习新信息。
- en: 'The most compelling use cases of ML happen when you have many of the following:
    repeated decisions, labeled data, a fault-tolerant use case, hard-to-articulate
    logic, and unstructured data.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML最引人注目的用例通常出现在以下情况中：重复决策、标记数据、容错使用案例、难以表达的逻辑和非结构化数据。
- en: If the vendor has a solution that solves the problem you want to solve, and
    they have more data, then you should buy it. If not, build your own. The third
    option is to adapt a solution to your data.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果供应商有一个解决您想解决的问题的解决方案，并且他们拥有更多的数据，那么您应该购买它。如果没有，就自己构建。第三个选择是根据您的数据调整解决方案。
- en: 'There are three levels of abstraction at which you can buy AI capabilities:
    SaaS, building blocks, and enterprise applications. Buy SaaS for turnkey services
    or APIs that can be easily integrated. Buy building blocks such as form parsers
    for use in your ML applications. Enterprise applications can be configured to
    meet your organization’s needs.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以购买AI能力的三个抽象级别：SaaS、构建块和企业应用程序。购买SaaS以获取易于集成的即插即用服务或API。购买构建块，例如用于ML应用程序的表单解析器。企业应用程序可以配置以满足您组织的需求。
- en: Adaptation of prebuilt ML models can be done through transfer learning or fine-tuning.
    Until you get to incredibly large datasets, these are likely to outperform custom
    models built solely on your data.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过迁移学习或微调来适应预建的ML模型。在您达到非常大的数据集之前，这些方法可能会胜过仅基于您的数据构建的自定义模型。
- en: Use AWS SageMaker, GCP Vertex AI, and Azure Machine Learning to build end-to-end
    ML models. This is covered in the next chapter.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AWS SageMaker、GCP Vertex AI和Azure Machine Learning构建端到端ML模型。这将在接下来的章节中进行介绍。
- en: When it comes to problems that involve unstructured data, it is usually not
    necessary to design your own model architecture. Use a no-code or low-code service
    such Vertex AI AutoML to train and deploy the model.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理涉及非结构化数据的问题时，通常不需要设计自己的模型架构。可以使用无代码或低代码服务，例如 Vertex AI AutoML 来训练和部署模型。
- en: When predicting outcomes, train ML models within your DWH using SQL. Do batch
    predictions within the DWH itself. For online predictions, deploy the trained
    model to a managed ML service.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预测结果时，使用SQL在您的DWH内训练ML模型。在DWH内执行批量预测。对于在线预测，将训练过的模型部署到托管的ML服务中。
- en: The best approach to forecasting problems is to use a data pipeline framework
    such as Apache Beam that provides the way to execute the same code on both batch
    and stream datasets.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决预测问题的最佳方法是使用数据管道框架，例如Apache Beam，在批处理和流数据集上执行相同代码的方法。
- en: The architecture for anomaly detection is similar to the forecasting case, except
    there are two time windows that need to be maintained.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测的架构与预测案例类似，但需要维护两个时间窗口。
- en: In personalization, the deployed model is part of a pipeline that also includes
    handling of cold starts and pruning of recent purchases. When the overall space
    of candidates is large, a smaller list of candidates is precomputed using a scheduled
    batch pipeline.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在个性化方面，部署的模型是管道的一部分，该管道还包括处理冷启动和修剪最近购买的操作。当候选者的总体空间很大时，将使用定期批处理管道预先计算较小的候选者列表。
- en: An automation pipeline consists of many ML models chained together. It will
    typically include a human over the loop to verify a sample of predictions, a human
    in the loop to make decisions on low-confidence predictions, and a way to capture
    human decisions for subsequent retraining.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化管道由许多串联的ML模型组成。通常包括一个人在循环中验证一部分预测结果，一个人在循环中对低置信度预测结果做出决策，并且有一种方法来捕获人类决策以进行后续重新训练。
- en: Because ML is such a powerful tool, we have to be careful to not misuse it.
    Verify with your AI governance board or legal counsel before embarking on a new
    AI project.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为ML是如此强大的工具，我们必须小心不要滥用它。在启动新的AI项目之前，请与您的AI治理委员会或法律顾问核实。
- en: Sliced evaluations, what-if tests, and continuous monitoring of model performance
    on protected criteria are ways to track ML fairness.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切片评估、假设测试和对受保护标准进行模型性能的持续监控是追踪ML公平性的方式。
- en: Cloud ML frameworks provide XAI tools that offer insights into how models work,
    so human experts are able to understand the logic that goes into making a decision.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云ML框架提供XAI工具，提供有关模型工作原理的洞察，以便人类专家能够理解决策背后的逻辑。
- en: In the next chapter you will learn how to support the development and deployment
    of custom ML models. We will cover the various development stages and the frameworks
    that can support such activity.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习如何支持自定义ML模型的开发和部署。我们将涵盖各种开发阶段和支持这种活动的框架。
- en: ^([1](ch10.html#ch01fn10-marker)) [Researchers](https://oreil.ly/m178J) make
    the point that these LLMs are [not truly open source](https://oreil.ly/4BrtJ).
    They do, however, allow you to fine-tune and deploy them in commercial applications
    without many restrictions.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#ch01fn10-marker)) [研究人员](https://oreil.ly/m178J)指出，这些LLM（大语言模型）[并非真正开源](https://oreil.ly/4BrtJ)。但它们允许您在商业应用中进行精细调整和部署，没有太多限制。
