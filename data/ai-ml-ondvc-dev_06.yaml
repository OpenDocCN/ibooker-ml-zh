- en: Chapter 6\. Computer Vision Apps with ML Kit on iOS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 计算机视觉应用程序与iOS上的ML Kit
- en: '[Chapter 3](ch03.html#introduction_to_ml_kit) gave you an introduction to ML
    Kit and how it could be used to do face detection in a mobile app. In [Chapter 4](ch04.html#computer_vision_apps_with_ml_kit_on_and)
    you then took a look at how to do some more sophisticated scenarios on Android
    devices—image labeling and classification and object detection in both still images
    and video. In this chapter, we’ll see how to use ML Kit for the same scenarios,
    but on iOS using Swift. Let’s start with image labeling and classification.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第 3 章](ch03.html#introduction_to_ml_kit)介绍了ML Kit及其在移动应用程序中进行人脸检测的用途。在[第 4
    章](ch04.html#computer_vision_apps_with_ml_kit_on_and)中，我们还介绍了如何在Android设备上执行更复杂的场景——图像标签和分类，以及静态图像和视频中的对象检测。在本章中，我们将看到如何在iOS中使用ML
    Kit执行相同的场景，使用Swift语言。让我们从图像标签和分类开始。'
- en: Image Labeling and Classification
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像标签和分类
- en: A staple of computer vision is the concept of image classification, where you
    give a computer an image, and the computer will tell you what the image contains.
    At the highest level, you could give it a picture of a dog, like that in [Figure 6-1](#sample_image_for_iphone_image_classific),
    and it will tell you that the image contains a dog.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的一个核心概念是图像分类，您向计算机提供一张图像，计算机会告诉您图像包含的内容。在最高级别上，您可以给它一张狗的图片，比如[图 6-1](#sample_image_for_iphone_image_classific)，它会告诉您图像中包含一只狗。
- en: ML Kit’s image labeling takes this a bit further—and it will give you a list
    of things it “sees” in the image, each with levels of probability. So, for the
    image in [Figure 6-1](#sample_image_for_iphone_image_classific), not only will
    it see a dog, it may also see a pet, a room, a jacket, and more. Building an app
    to do this on iOS is pretty simple, so let’s explore that step by step.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ML Kit的图像标签功能进一步扩展了这一点，它将为您提供一张图片中“看到”的物品列表，每个物品都附带概率级别。因此，对于[图 6-1](#sample_image_for_iphone_image_classific)中的图片，它不仅会看到一只狗，还可能看到一只宠物、一个房间、一件夹克等等。在iOS上构建这样的应用程序非常简单，让我们一步步来探索。
- en: Note
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing, ML Kit’s pods give some issues when running on a Mac
    with the iOS simulator. Apps will still run on devices, and also with the “My
    Mac (designed for iPad)” runtime setting in Xcode.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，ML Kit的pod在运行Mac上的iOS模拟器时可能会出现一些问题。应用程序仍然可以在设备上运行，并且还可以在Xcode中的“My Mac（专为iPad设计）”运行时设置下运行。
- en: '![](assets/aiml_0601.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0601.png)'
- en: Figure 6-1\. Sample image for iPhone image classification
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. iPhone图像分类的示例图片
- en: 'Step 1: Create the App in Xcode'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：在Xcode中创建应用程序
- en: You’ll use Xcode to create the app. Use the App template in Xcode and ensure
    that your interface type is Storyboard and the language is Swift. If you aren’t
    familiar with these steps, refer back to [Chapter 3](ch03.html#introduction_to_ml_kit)
    where we went through them in more detail. Call your app whatever you want, but
    in my case, I used the name MLKitImageClassifier. When you’re done, close Xcode.
    In the next step, you’ll add a pod file, and when you install this, it gives you
    a new file to open Xcode with.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用Xcode创建应用程序。在Xcode中使用应用程序模板，确保您的界面类型为Storyboard，语言为Swift。如果您对这些步骤不熟悉，请参阅[第 3
    章](ch03.html#introduction_to_ml_kit)，我们在其中详细介绍了这些步骤。为应用程序取任何您喜欢的名字，但在我的情况下，我使用了MLKitImageClassifier作为名称。完成后，关闭Xcode。在下一步中，您将添加一个pod文件，安装后它会提供一个新文件以打开Xcode。
- en: 'Step 2: Create the Podfile'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：创建Podfile
- en: This step requires that you have CocoaPods installed on your development box.
    CocoaPods is a dependency management tool that makes it easy for you to add third-party
    libraries to your iOS app. As ML Kit ships from Google, it’s not built-in to Xcode,
    and as a result, you’ll need to add it to any apps as a “pod” from CocoaPods.
    You can find CocoaPods install instructions at [*http://cocoapods.org*](http://cocoapods.org),
    and I’ll give you the code that shows you how to pick the appropriate pod as we
    walk through each example.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤要求您的开发环境中已安装CocoaPods。CocoaPods是一个依赖管理工具，可帮助您轻松向iOS应用程序添加第三方库。由于ML Kit来自Google，并未集成到Xcode中，因此您需要通过CocoaPods将其作为“pod”添加到任何应用程序中。您可以在[*http://cocoapods.org*](http://cocoapods.org)找到CocoaPods的安装说明，并在我们逐步示例中为您提供选择适当pod的代码。
- en: In the directory where you created your project, add a new file. Call it *podfile*
    with no extension. After you save it, your directory structure with your project
    should look like [Figure 6-2](#adding_a_podfile_to_the_project_folder).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建项目的目录中，添加一个新文件。命名为*podfile*，没有扩展名。保存后，您的项目目录结构应如[图 6-2](#adding_a_podfile_to_the_project_folder)所示。
- en: '![](assets/aiml_0602.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 6-2](assets/aiml_0602.png)'
- en: Figure 6-2\. Adding a Podfile to the project folder
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 将 Podfile 添加到项目文件夹中
- en: 'Edit the contents of this file to look like this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑此文件的内容，使其如下所示：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note the line beginning with target. After target, the name of your project
    should be in the quotes, which in the preceding case is `MLKitImageClassifier`,
    but if you used something else, make sure you change the code to fit your project
    name.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以`target`开头的行。在`target`后面，应该用引号括起来你的项目名称，前面的案例中是`MLKitImageClassifier`，但如果你使用了其他名称，请确保修改代码以适应你的项目名称。
- en: Once you’ve done and saved this, use Terminal and navigate to the folder. Type
    the command `**pod install**`. You should see output like [Figure 6-3](#installing_the_image_classifier_cocoapo).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 完成并保存后，请使用Terminal导航到该文件夹。输入命令`**pod install**`。您应该看到类似[图 6-3](#installing_the_image_classifier_cocoapo)的输出。
- en: '![](assets/aiml_0603.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 6-3](assets/aiml_0603.png)'
- en: Figure 6-3\. Installing the image classifier CocoaPod
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 安装图像分类器 CocoaPod
- en: You’ll notice at the end that it asks you to use the *.xcworkspace* file from
    now on. When you created your project in step 1, you would open it in Xcode using
    the *.xcproject* file. Those files cannot handle external libraries that are included
    via pods, but *.xcworkspace* ones can. As such, you should use that file going
    forward. So go ahead and open it now with Xcode!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，注意到末尾要求您从现在开始使用*.xcworkspace*文件。当您在步骤1中创建项目时，您将使用*.xcproject*文件在Xcode中打开它。这些文件无法处理通过pods包含的外部库，但*.xcworkspace*可以。因此，今后应该使用该文件。现在使用Xcode打开它吧！
- en: 'Step 3: Set Up the Storyboard'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：设置Storyboard
- en: The storyboard file called *main.storyboard* will contain the user interface
    for your app. Add a UIImage view to it and set its property to Aspect Fit. Then,
    add a button control, and change its text to “Do Inference.” Finally add a UILabel
    and use the attributes inspector to set its Lines property to “0” to ensure it
    will have multiple lines of text. Resize the label to give it plenty of space.
    When you’re done, your storyboard should look like [Figure 6-4](#creating_the_simple_storyboard).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*main.storyboard*文件将包含您应用程序的用户界面。在其中添加一个UIImage视图，并将其属性设置为Aspect Fit。然后，添加一个按钮控件，并将其文本更改为“Do
    Inference”。最后，添加一个UILabel，并使用属性检查器将其Lines属性设置为“0”，以确保它将具有多行文本。调整标签的大小以确保有足够的空间。完成后，您的Storyboard应该类似于[图
    6-4](#creating_the_simple_storyboard)。'
- en: '![](assets/aiml_0604.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 6-4](assets/aiml_0604.png)'
- en: Figure 6-4\. Creating the simple storyboard
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. 创建简单的Storyboard
- en: Next up you’ll need to create outlets for the image and label controls, and
    an action for the button. You do this by opening a separate window and control-dragging
    the control onto the *ViewController.swift* file. If you aren’t familiar with
    this, please refer back to [Chapter 3](ch03.html#introduction_to_ml_kit) for a
    detailed example.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要为图像和标签控件创建outlet，并为按钮创建一个action。您可以通过打开一个独立的窗口，将控件拖动到*ViewController.swift*文件中来完成此操作。如果您对此不熟悉，请参考[第三章](ch03.html#introduction_to_ml_kit)中的详细示例。
- en: It might be confusing as to when you need outlets and when you need actions,
    so I like to think of it like this. If you want to read or set properties on the
    control, you need an “outlet.” So, for example, we will need to *read* the contents
    of the image view control to pass to ML Kit to have it classified. You will also
    need to *write* to the contents of the Label to render the results. So, as a result,
    they will both need an outlet set up, and the name of that outlet is how you’ll
    refer to these controls in your code. When you need to respond to the user doing
    something with the control—like pressing the button—you’ll need an “action.” When
    you drag the controls onto your code editor, you’ll be given the option to use
    an outlet or action, so use that to create outlets for the UIImageView and the
    Label now, and call them `imageView` and `lblOutput`, respectively. Then create
    an action for the button. You can call this action `doInference`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 或许会感到困惑，不知道何时需要outlet，何时需要action，所以我喜欢这样思考。如果你想要读取或设置控件的属性，则需要一个“outlet”。因此，例如，我们需要*读取*图像视图控件的内容以传递给
    ML Kit 进行分类。您还需要*写入*标签的内容以渲染结果。因此，它们都需要设置一个outlet，并且该outlet的名称是您在代码中引用这些控件的方式。当您需要响应用户对控件进行操作，例如按下按钮时，您将需要一个“action”。当您将控件拖放到代码编辑器时，将提供使用outlet或action的选项，因此现在为UIImageView和Label创建outlet，并分别命名为`imageView`和`lblOutput`。然后为按钮创建一个action。您可以将此操作命名为`doInference`。
- en: 'When you’re done, your *ViewController.swift* file should look like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当您完成时，您的*ViewController.swift*文件应该如下所示：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You’ll edit that next to implement the image classification code. Before you
    can do that, though, you’ll need an image to classify! You can pick any image
    from your filesystem and drag it onto Xcode. Drop it into your project, in the
    same folder as your storyboard. You’ll get a dialog asking you to choose options.
    Keep the defaults here, but ensure that “Add to targets” is set with the checkbox
    beside your project name checked. This will ensure that the image is compiled
    into your app so at runtime it can be loaded.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来你将编辑以实现图像分类代码。不过，在此之前，你需要一个要分类的图像！你可以从文件系统中选择任何图像并将其拖到Xcode中。将其拖放到你的项目中，与你的Storyboard文件位于同一文件夹中。你会得到一个对话框让你选择选项。保持默认设置，但确保“添加到目标”旁边的复选框被选中以确保图像被编译到你的应用程序中，这样在运行时就可以加载它。
- en: 'Step 4: Edit the View Controller Code to Use ML Kit'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步：编辑视图控制器代码以使用ML Kit
- en: Right now your app can’t do anything, the image hasn’t been loaded, and no code
    for handling the button press has been written. Let’s go through building that
    functionality in this section.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的应用程序什么都做不了，图片还没有加载，也没有编写处理按钮按下的代码。让我们在本节中逐步构建该功能。
- en: 'First of all, to load the image, you can add code to the `viewDidLoad` function
    that gets loaded when the app first starts. This is as simple as just setting
    the `.image` property on `imageView`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了加载图像，你可以在应用程序第一次启动时添加到`viewDidLoad`函数中的代码。这只需简单地在`imageView`上设置`.image`属性即可：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you were to launch the app now, the picture would render, but not much else
    would happen. Typically your user would press the button and you would want to
    respond to that action. You already have the action `doInference` set up, so let’s
    have it call a function called `getLabels`, using the image within the `imageView`
    as a parameter:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在启动应用程序，图片会被渲染，但不会发生太多其他事情。通常用户会按下按钮，你需要对该操作做出响应。你已经设置了`doInference`动作，所以让它调用一个名为`getLabels`的函数，使用`imageView`中的图片作为参数：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Xcode will complain because you don’t have this function implemented yet. That’s
    OK—you’ll do that next. The job of this function is to take the contents of the
    image and pass them to ML Kit to get a set of labels back. So, before you can
    write that, you’ll need to ensure that you have referenced the relevant ML Kit
    libraries. At the top of your *ViewController.swift* file, add these lines:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Xcode会抱怨因为你还没有实现这个函数。没关系，你接下来会做这件事。这个函数的作用是获取图像的内容并传递给ML Kit以获取一组标签。因此，在你编写代码之前，确保你已经引用了相关的ML
    Kit库。在你的*ViewController.swift*文件顶部添加这些行：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To use ML Kit for image labeling, you’ll need to do the following tasks:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用ML Kit进行图像标记，您需要完成以下任务：
- en: Convert the image into a VisionImage type.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像转换为VisionImage类型。
- en: Set up options for the image labeler and initialize it with these options.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置图像标签器的选项并使用这些选项进行初始化。
- en: Call the image labeler and catch the asynchronous callback.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用图像标签器并捕获异步回调。
- en: 'So, to first convert the image to a `VisionImage` you’ll write code like this:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先将图像转换为`VisionImage`，你将编写如下代码：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then you’ll initialize the labeler by setting up some options. In this case,
    you can keep it simple and just have the options use a confidence threshold. The
    goal, while it can label many things in an image, is to return the ones above
    a certain probability. So, for example, the image in [Figure 6-1](#sample_image_for_iphone_image_classific),
    despite being a dog, will actually have a little over a 0.4 probability (i.e.,
    40%) that it’s also a cat! So to see this, you can set the confidence threshold
    to 0.4:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过设置一些选项来初始化标签器。在这种情况下，你可以简单地使用置信度阈值的选项。虽然它可以标记图像中的许多物体，但目标是返回超过某个概率的标签。例如，图像在[Figure 6-1](#sample_image_for_iphone_image_classific)中，尽管是一只狗，但实际上有超过0.4的概率（即40%）也是一只猫！因此，你可以将置信度阈值设置为0.4来查看：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that you have a labeler and an image in the desired format, you can pass
    it to the labeler. This is an asynchronous operation, so you won’t lock up your
    user interface while it processes. Instead, you can specify a callback function
    that it will call when the inference is complete. The labeler will return two
    objects: “labels” for the inferences, and “error” if it fails. You can then pass
    these to a function of your own (called `processResult` in this case) to handle
    them:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你有了一个标签器和一个以所需格式的图像，你可以将其传递给标签器。这是一个异步操作，因此在处理过程中不会锁定用户界面。相反，你可以指定一个回调函数，在推理完成时调用它。标签器将返回两个对象：“labels”用于推理结果，“error”如果失败。然后，你可以将它们传递给自己的函数（在这种情况下称为`processResult`）来处理它们：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For convenience, here’s the entire `getLabels func`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为方便起见，这里是整个`getLabels func`：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After the labeler has done its job, it will call the `processResult` function.
    If you’ve entered the preceding code, Xcode is probably complaining because it
    can’t find this function. So, let’s implement that next!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 标签器完成其工作后，将调用`processResult`函数。如果你输入了前面的代码，Xcode 可能会抱怨找不到此函数。因此，让我们接着实现它！
- en: 'The labels collection returned by ML Kit is an array of `ImageLabel` objects.
    So you’ll need your function to use that as the type for the `from` parameter.
    These objects have a `text` property, with the label description (i.e., `cat`)
    and a confidence property containing the probability that the image matches that
    label (i.e., 0.4). So you can iterate through the collection and build a string
    with these values with this code. It will then set the `lblOutput` text to the
    string you constructed. Here’s the complete code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ML Kit 返回的标签集合是一个`ImageLabel`对象的数组。因此，你需要让你的函数使用该类型作为`from`参数。这些对象有一个`text`属性，其中包含标签描述（例如，`cat`），还有一个置信度属性，表示图像匹配该标签的概率（例如，0.4）。因此，你可以遍历该集合并使用以下代码构建包含这些值的字符串。然后，它将把`lblOutput`文本设置为你构建的字符串。以下是完整代码：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: And that’s everything you need! When you run the app and press the button, you’ll
    see something like [Figure 6-5](#running_inference_with_this_app_on_the).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一切就是这样！当你运行应用程序并按下按钮时，你会看到类似于[图 6-5](#running_inference_with_this_app_on_the)的东西。
- en: And you can see that while ML Kit was 99% sure it was looking at a dog, it was
    also 40% sure that it might be a cat!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，虽然 ML Kit 非常确定它在看一只狗，但它也有 40% 的可能性是一只猫！
- en: Understandably this is a super simple app, but hopefully it demonstrates how
    you can use image labeling quickly and easily on iOS in just a few lines of code
    using ML Kit!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 显而易见，这是一个超级简单的应用程序，但希望它展示了如何在 iOS 上仅使用几行代码快速简便地使用 ML Kit 进行图像标记！
- en: '![](assets/aiml_0605.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0605.png)'
- en: Figure 6-5\. Running inference with this app on the dog picture
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. 在狗图片上使用此应用程序进行推断
- en: Object Detection in iOS with ML Kit
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: iOS 中的物体检测与 ML Kit
- en: Next, let’s explore a similar scenario to image classification, where we go
    a step further. Instead of having the device recognize *what* it sees in an image,
    let’s also have it recognize *where* in the image it sees the object, using a
    bounding box to draw that for the user.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索一个类似的场景，像图像分类一样，我们再进一步。不仅让设备识别图像中的*什么*，让它也能识别图像中的*哪里*，使用边界框为用户绘制。
- en: So, as an example, see [Figure 6-6](#object_detector_app_in_ios). The app saw
    this picture and detected three objects within it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看看[图 6-6](#object_detector_app_in_ios)。这个应用程序看到这张图片，并检测到其中的三个对象。
- en: '![](assets/aiml_0606.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0606.png)'
- en: Figure 6-6\. Object detector app in iOS
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. iOS 中的物体检测应用
- en: 'Step 1: Get Started'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1：开始
- en: Creating this app is pretty straightforward. Create a new iOS app as before.
    Give it whatever name you like but ensure that you’re using Swift and storyboards
    in the new project settings dialog. In this case I called my project *MLKitObjectDetector*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这个应用程序非常简单。像以前一样创建一个新的 iOS 应用程序。在新项目设置对话框中使用 Swift 和故事板，给它任何你喜欢的名字。在这种情况下，我将我的项目命名为*MLKitObjectDetector*。
- en: 'In the project directory, you’ll create a Podfile as you did in the previous
    section, but this time you’ll want to specify that you want to use ML Kit’s object
    detection libraries instead of the image labeling ones. Your Podfile should look
    something like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目目录中，你将创建一个 Podfile，就像在前面的部分中所做的那样，但这次你将指定要使用 ML Kit 的物体检测库，而不是图像标签库。你的 Podfile
    应该看起来像这样：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that the target setting should be the name of the project as you created
    it in Xcode (in my case, I used *MLKitObjectDetector*`)` and the pod should be
    *GoogleMLKit/ObjectDetection*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，目标设置应为您在 Xcode 中创建的项目名称（在我的情况下，我使用了 *MLKitObjectDetector*），并且 pod 应为 *GoogleMLKit/ObjectDetection*。
- en: When you’re done, run `**pod install**` to download the required dependencies,
    and you’ll have a new *.xcworkspace* file created for you. Open this to continue.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当完成时，运行 `**pod install**` 下载所需的依赖项，然后将为您创建一个新的 *.xcworkspace* 文件。打开此文件以继续。
- en: 'Step 2: Create Your UI on the Storyboard'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '步骤 2: 在 Storyboard 上创建您的用户界面'
- en: This app is even simpler than the image labeling one in that you’ll have only
    two UI elements—an image that you want to perform object detection on, and on
    which you’ll draw the bounding boxes, and a button that the user will press to
    trigger the object detection. So, add a UIImageView and a button to the storyboard.
    Edit the button to change its text to “Detect Objects.” Your storyboard should
    look something like [Figure 6-7](#the_storyboard_for_the_object_detection) when
    you’re done.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用比图像标注应用更简单，因为它只有两个用户界面元素——一个您希望进行对象检测的图像，并且您将在其上绘制边界框，以及一个用户按下以触发对象检测的按钮。因此，在
    storyboard 中添加一个 UIImageView 和一个按钮。编辑按钮以更改其文本为“检测对象”。当您完成时，您的 storyboard 应该看起来像[图
    6-7](#the_storyboard_for_the_object_detection)中的示例。
- en: '![](assets/aiml_0607.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0607.png)'
- en: Figure 6-7\. The storyboard for the object detection app
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. 对象检测应用的 storyboard
- en: You’ll want to create an outlet for the image view and an action for the button.
    If you’re not familiar with these, I’d recommend going back to [Chapter 3](ch03.html#introduction_to_ml_kit)
    and working through the example there, as well as doing the image labeling one
    earlier in this chapter.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要为图像视图创建一个输出口，并为按钮创建一个操作。如果您对这些不熟悉，我建议您返回到[第三章](ch03.html#introduction_to_ml_kit)，并完成那里的示例，以及本章前面的图像标注示例。
- en: 'When you’re done, your *ViewController.swift* file will have the outlet and
    action defined and should look like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当完成时，您的 *ViewController.swift* 文件将定义了输出口和操作，并应如下所示：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code, I named the outlet for the UIImageView `imageView`, and
    the action for pressing the button `doObjectDetection`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我将 UIImageView 的输出命名为 `imageView`，按钮按下时的操作命名为 `doObjectDetection`。
- en: 'You’ll need to add an image to your project, so that you can load it into the
    UIImageView. In this case I have one called *bird.jpg*, and the code to load it
    in `viewDidLoad()` will look like this:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要向项目中添加一张图像，以便将其加载到 UIImageView 中。在这种情况下，我有一个称为 *bird.jpg* 的图像，并且加载它的代码在 `viewDidLoad()`
    中看起来像这样：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Step 3: Create a Subview for Annotation'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '步骤 3: 创建注释的子视图'
- en: This app draws bounding boxes on top of the image when it gets detected objects
    back from ML Kit. Before we do the inference, let’s make sure that we can draw
    on the image. To do this, you’ll need to have a subview drawn on top of the image.
    This subview will be transparent and will have the rectangles for the bounding
    boxes drawn on it. As it’s on top of the image and transparent except for the
    rectangles, it will appear as if the rectangles are drawn on the image.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当从 ML Kit 收到检测到的对象时，此应用会在图像上绘制边界框。在进行推理之前，让我们确保可以在图像上进行绘制。为此，您需要在图像顶部绘制一个子视图。该子视图将是透明的，并在其上绘制边界框。由于它位于图像顶部并且除了矩形之外是透明的，因此看起来好像矩形是绘制在图像上的。
- en: 'Declare this view in your *ViewController.swift* as a UIView type. With Swift,
    you can specify how to instantiate the view with things like a precondition that
    the view has already loaded, ensuring that this view will load *after* the UIImageView
    containing your picture and making it easier to load it on top of it! See the
    following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *ViewController.swift* 中将此视图声明为 UIView 类型。使用 Swift，您可以指定如何实例化视图，例如，确保视图已经加载，确保此视图将在包含您图片的
    UIImageView 之后加载，使加载更容易！请参见以下示例：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once you’ve declared it, you can now instantiate and configure it within your
    `viewDidLoad` function, adding it as a subview to the `imageView`. You can also
    activate it using `NSLayoutConstraint` to make sure that it matches the dimensions
    of `imageView`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 声明完成后，您现在可以在 `viewDidLoad` 函数中实例化和配置它，将其作为 `imageView` 的子视图添加进去。您还可以使用 `NSLayoutConstraint`
    激活它，以确保它与 `imageView` 的尺寸匹配：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Step 4: Perform the Object Detection'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '步骤 4: 执行对象检测'
- en: 'Before you can use the object detection APIs from ML Kit, you need to include
    them in your code file. You can do this with:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在您可以使用 ML Kit 的对象检测 API 之前，需要将它们包含在您的代码文件中。您可以通过以下方式实现：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, within the action you created for when the user presses the button, add
    this code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在为用户按下按钮创建的操作中，添加以下代码：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Xcode will complain that this function hasn’t yet been created. That’s OK.
    You’ll create it now! Create the `runObjectDetection` function within your *ViewController.swift*
    file:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Xcode 将抱怨这个函数还没有被创建。没关系，现在就来创建它！在您的*ViewController.swift*文件中创建`runObjectDetection`函数：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Similar to the image labeling earlier in the chapter, the process of performing
    object detection with ML Kit is very straightforward:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章前面的图像标注类似，使用 ML Kit 执行对象检测的过程非常简单：
- en: Convert your image into a `VisionImage`.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的图像转换为`VisionImage`。
- en: Create an options object with the options you choose and instantiate an object
    detector with these options.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个包含所选选项的选项对象，并使用这些选项实例化一个对象检测器。
- en: Pass the image to the object detector and catch its response in a callback.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像传递给对象检测器，并在回调中捕获其响应。
- en: 'Let’s walk through how you can do each of these steps within the function you
    just created. First, to convert your image to a `VisionImage`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解如何在刚刚创建的函数中执行每个步骤。首先，将您的图像转换为`VisionImage`：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then create the options object and instantiate an object detector with them:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用这些选项创建选项对象，并实例化一个对象检测器：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Explore the ML Kit documentation for the option types; the ones I have used
    here are very commonly used. `EnableClassification` not only gives you the bounding
    boxes but ML Kit’s classification of the object. The base model (that you’re using
    here) only recognizes five very generic object types like “Fashion Item” or “Food
    Item,” so manage your expectations accordingly! The `EnableMultipleObjects` option,
    when set, as its name suggests, will allow you to detect multiple objects as in
    [Figure 6-6](#object_detector_app_in_ios), where three items—the bird and two
    flowers—were detected and bounding boxes plotted.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 探索 ML Kit 文档以获取选项类型；这里使用的选项非常常见。`EnableClassification`不仅提供边界框，还提供 ML Kit 对对象的分类。基础模型（您在这里使用的模型）只能识别五种非常通用的对象类型，如“时尚物品”或“食品物品”，因此请合理管理您的期望！当设置了`EnableMultipleObjects`选项时，允许检测多个对象，如[图 6-6](#object_detector_app_in_ios)中所示，检测到三个物品——鸟和两朵花，并绘制了边界框。
- en: 'Finally, you’ll pass the image to the object detector to get it to infer the
    labels and bounding boxes for objects it spots within the image. This is an asynchronous
    function, so you’ll need to specify the function to call back when the inference
    is complete. ML Kit will return a `detectedObjects` list and an error object,
    so you can simply pass these to a function that you’ll create next:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您将传递图像给对象检测器，以便它推断图像中检测到的对象的标签和边界框。这是一个异步函数，因此您需要指定在推断完成时回调的函数。ML Kit 将返回一个`detectedObjects`列表和一个错误对象，因此您可以简单地将它们传递给下一个将要创建的函数：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Step 5: Handle the Callback'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 步：处理回调
- en: 'In the previous step we defined the callback function, `processResult`. So
    let’s first create that. It takes `detectedObjects`, which is an array of `Object`s,
    as the `from:` parameter, and an `Error` object as the `error:` parameter. Here’s
    the code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一步中，我们定义了回调函数`processResult`。因此，让我们首先创建它。它以`detectedObjects`（一个`Object`数组）作为`from:`参数，并以`Error`对象作为`error:`参数。以下是代码：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we’ll exit if the `detectedObjects` array is empty, so that we don’t
    waste time trying to draw or update the overlay:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果`detectedObjects`数组为空，我们将退出，以免浪费时间尝试绘制或更新覆盖层：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Next, you’ll iterate through all of the detected objects and draw the bounding
    boxes that were detected.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将遍历所有检测到的对象，并绘制检测到的边界框。
- en: Handling the callback to collect the bounding boxes is a little bit more complicated
    than the image classification scenario you explored earlier in this chapter. This
    is because the image *as rendered on the screen* likely has different dimensions
    than the underlying image. When you pass the image to ML Kit, you are passing
    the entire image, so the bounding boxes you get returned are relative to the image.
    If this doesn’t make sense, consider it this way. Imagine your image is 10,000
    × 10,000 pixels. What gets rendered on your screen might be 600 × 600 pixels.
    When the bounding boxes are returned from ML Kit, they will be relative to the
    image at 10,000 × 10,000, so you will need to transform them to the coordinates
    that are correct for the displayed image.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 处理回调以收集边界框比在本章前面探讨的图像分类场景稍微复杂一些。这是因为屏幕上渲染的图像可能具有与底层图像不同的尺寸。当你将图像传递给ML Kit时，你传递的是整个图像，因此返回的边界框是相对于图像的。如果这看起来没有意义，可以这样考虑。想象一下，你的图像是10000×10000像素。在你的屏幕上呈现的可能是600×600像素。当从ML
    Kit返回边界框时，它们将相对于10000×10000像素的图像，因此你需要将它们转换为正确显示图像的坐标。
- en: So, you’ll first calculate a transformation matrix for the image, which you
    can apply to get a transformed rectangle.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你将首先为图像计算一个变换矩阵，然后可以应用这个矩阵来获得一个变换后的矩形。
- en: 'Here’s the complete code. I won’t go through it in detail, but the main point
    is to get the size of the underlying image and scale it to the size of the image
    as rendered in the UIImage control:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的代码。我不会详细解释，但主要目的是获取底层图像的尺寸，并将其缩放到UIImage控件中渲染的图像大小：
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: So now that you can transform the image, the next thing is to iterate through
    each of the objects, pulling the results and transforming them with it. You’ll
    do this by looping through the detected objects and using their frame property,
    which contains the frame of the bounding box.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以转换图像，接下来的事情是迭代每个对象，提取结果并进行相应的转换。你将通过循环遍历检测到的对象，并使用它们的frame属性来做到这一点，该属性包含了边界框的框架。
- en: 'The loop for this is now simple thanks to the transform matrix you just created:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于你刚刚创建的变换矩阵，这个循环变得很简单：
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Given that you now have a transformed rectangle that matches the rendered image,
    you will next want to draw this rectangle. Earlier you created the annotation
    overlay view that matched the image. So you can draw on it by adding a rectangle
    subview to that overlay. Here’s the code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 给定你现在有一个与渲染图像匹配的转换矩形，接下来你会想要绘制这个矩形。之前你创建了与图像匹配的注释叠加视图。因此，你可以通过向该叠加视图添加一个矩形子视图来进行绘制。这里是代码：
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You don’t have an `addRectangle` function yet, but creating one is now pretty
    straightforward. You simply need to create a new view with the dimensions of the
    rectangle and add it to the specified view. In this case that’s the `annotationOverlayView`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你还没有一个`addRectangle`函数，但现在创建一个非常简单。你只需创建一个具有矩形尺寸的新视图，并将其添加到指定的视图中。在这种情况下，即`annotationOverlayView`。
- en: 'Here’s the code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '[PRE26]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: And that’s it! You’ve now created an app that will recognize elements within
    your image, and give you back bounding boxes for them. Explore with different
    images.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！你现在创建了一个可以识别图像中元素并返回其边界框的应用程序。尝试使用不同的图像进行探索。
- en: The classification that it gave was very limited, but now that you have the
    bounding boxes, you can clip the original image based on the bounding box and
    pass that clip to the image labeler to get a more granular description of what
    it sees! You’ll explore that next.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它给出的分类非常有限，但是现在你有了边界框，你可以根据边界框裁剪原始图像，并将该裁剪区域传递给图像标签器，以获取更细粒度的描述！下面你将探索这一点。
- en: Combining Object Detection with Image Classification
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合对象检测和图像分类
- en: The previous example showed you how you could do object detection and get bounding
    boxes for the objects that were detected in the image. The base model with ML
    Kit can only classify a few classes such as “Fashion good,” “Food good,” “Home
    good,” “Place,” or “Plant.” However it is able to detect distinct objects in an
    image, as we saw in the preceding example, where it spotted the bird and two flowers.
    It just could not *categorize* them as such.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的示例展示了如何进行对象检测，并获取图像中检测到的对象的边界框。ML Kit 的基础模型只能对少数类别进行分类，如“时尚商品”、“美食”、“家居商品”、“地点”或“植物”。然而，它能够检测图像中的不同对象，正如我们在前面的示例中看到的那样，它发现了鸟和两朵花。它只是不能*分类*它们。
- en: If you want to do that, there’s an option in combining object detection with
    image classification. As you already have the bounding boxes for the items in
    the image, you could crop the image to them, and then use image labeling to get
    the details of that subimage.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要这样做，在结合对象检测与图像分类的选项中有一个选项。因为您已经有了图像中项目的边界框，您可以将图像裁剪到它们，然后使用图像标签获取该子图像的详细信息。
- en: 'So, you could update your process results to use the frame property of each
    object to crop the image and load the cropped image into a new `UIImage`, called
    `croppedImage`, like this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您可以更新您的处理结果，使用每个对象的帧属性来裁剪图像，并将裁剪后的图像加载到一个名为`croppedImage`的新`UIImage`中，如下所示：
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The image labeling API uses a `VisionImage` object as you saw earlier in this
    chapter. So, the `UIImage` that represents the cropped image can be converted
    like this:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图像标签API使用一个`VisionImage`对象，正如您在本章前面看到的那样。因此，表示裁剪图像的`UIImage`可以转换如下：
- en: '[PRE28]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You’ll then need to instantiate an image labeler. This code will instantiate
    one with a confidence threshold of 0.8:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要实例化一个图像标签器。以下代码将会实例化一个置信阈值为0.8的标签器：
- en: '[PRE29]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then you’ll just pass the `VisionImage` object to the labeler, and specify
    the callback to handle the results:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您只需将`VisionImage`对象传递给标签器，并指定回调函数来处理结果：
- en: '[PRE30]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Your callback can then process the labels inferred from each object as you did
    earlier in this chapter.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您的回调函数可以处理从每个对象推断出的标签，就像本章早些时候所做的那样。
- en: Object Detection and Tracking in Video
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视频中的对象检测和跟踪
- en: While it’s beyond the scope of this book to demonstrate how live video overlay
    works, the “Chapter6ObjectTracking” sample app in the repo implements it for you.
    It uses CoreVideo from Apple to implement an `AVCaptureVideoPreviewLayer` and
    an `AVCaptureSession`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本书超出了演示实时视频叠加的范围，但在仓库中的“Chapter6ObjectTracking”示例应用程序为您实现了它。它使用来自Apple的CoreVideo来实现一个`AVCaptureVideoPreviewLayer`和一个`AVCaptureSession`。
- en: Capturing is then delegated to an extension of the `ViewController`, which will
    capture frames from an `AVCaptureConnection` derived from the `AVCaptureSession`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉然后委托给`ViewController`的扩展，它将从基于`AVCaptureSession`的`AVCaptureConnection`中捕获帧。
- en: What’s important to show here is how you can use frames from this video, pass
    them to ML Kit, and get object detections back. You can then use these to overlay
    bounding boxes on top of the live video.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要强调的重点是，您可以如何使用此视频的帧，将它们传递给ML Kit，并获取对象检测结果。然后，您可以使用这些结果在实时视频上叠加边界框。
- en: 'When you create an app that uses a live video preview using Apple’s AVFoundation,
    you’ll have a delegate function called `captureOutput` that receives a buffer
    containing details of the frame. This can be used to create a `VisionImage` object,
    which you can then use with the object detection API in ML Kit like this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建一个使用Apple的AVFoundation进行实时视频预览的应用程序时，您将拥有一个名为`captureOutput`的委托函数，它接收包含帧详细信息的缓冲区。这可以用来创建一个`VisionImage`对象，然后可以像这样与ML
    Kit中的对象检测API一起使用：
- en: '[PRE31]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: When you get the captured output from the delegate, it will contain a sample
    buffer. This can then be used to get the image buffer containing a frame.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当您从委托获得捕获的输出时，它将包含一个样本缓冲区。然后可以用来获取包含帧的图像缓冲区。
- en: 'From this, you can convert the buffer to a `VisionImage` type using:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从中，您可以使用以下方法将缓冲区转换为`VisionImage`类型：
- en: '[PRE32]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To track objects in video, you need to disable classification of the objects
    and enable multiple object detection. You can achieve that with:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要在视频中跟踪对象，您需要禁用对象的分类，并启用多对象检测。您可以通过以下方式实现：
- en: '[PRE33]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, given that you have the image, the options, and the dimensions of the
    image, you can call a helper function to do the object detection:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑到您有图像、选项和图像的尺寸，您可以调用一个辅助函数来执行对象检测：
- en: '[PRE34]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The job of this function will be to call ML Kit to get the detected objects
    in the frame, and then calculate bounding boxes for them, displaying those boxes
    with the tracking IDs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数的作用将是调用ML Kit来获取帧中检测到的对象，然后为它们计算边界框，并显示带有跟踪ID的这些框。
- en: 'So, first, to detect the objects:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先要检测对象：
- en: '[PRE35]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, once they’re detected, their bounding boxes will be returned in the `object.frame`.
    This needs to be normalized to be drawn on the preview overlay, and it’s simply
    normalized by dividing its value by the width of the frame:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦它们被检测到，它们的边界框将以`object.frame`返回。这需要被归一化以绘制在预览叠加层上，它通过将其值除以帧的宽度来简单地归一化：
- en: '[PRE36]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preview layer provides a method to turn a normalized rectangle into one
    that matches the preview layer’s coordinate system, so you can add it to the preview
    layer like this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 预览层提供了一种将归一化矩形转换为与预览层坐标系统匹配的方法，因此您可以像这样将其添加到预览层中：
- en: '[PRE37]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: (`UIUtilities` is a class of helper utilities you can find in the repo.) You
    can see it in action on an iPhone in [Figure 6-8](#object_detection_and_tracking_in_a_vide).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: （`UIUtilities`是您可以在存储库中找到的辅助工具类。）您可以在iPhone上看到它在[图 6-8](#object_detection_and_tracking_in_a_vide)中的实际应用。
- en: '![](assets/aiml_0608.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0608.png)'
- en: Figure 6-8\. Object detection and tracking in a video
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-8\. 视频中的物体检测和跟踪
- en: With these tools you should be able to create basic computer vision applications
    using ML Kit on iOS in Swift. In [Chapter 11](ch11.html#using_custom_models_in_ios),
    you’ll see how to use custom models instead of relying on the base models from
    ML Kit!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些工具，您应该能够在iOS上使用Swift创建基本的计算机视觉应用程序，使用ML Kit。在[第 11 章](ch11.html#using_custom_models_in_ios)中，您将看到如何使用自定义模型，而不是依赖于ML
    Kit的基础模型！
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter you saw how to use computer vision algorithms with ML Kit, including
    image labeling and object detection. You saw how you can combine these to extend
    the basic model that ships with ML Kit, so you can classify the contents of the
    bounding boxes returned from object detection. You also saw how you can use object
    detection within a live video and got a sample for how you could do bounding boxes
    on live video! This provides you with the foundation to go forward in building
    apps that use images, either for classification or object detection purposes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您看到了如何使用ML Kit的计算机视觉算法，包括图像标注和物体检测。您看到了如何将这些功能组合起来，扩展ML Kit提供的基本模型，以便对从物体检测返回的边界框的内容进行分类。您还看到了如何在实时视频中使用物体检测，并获得了如何在实时视频上添加边界框的示例！这为您提供了在构建使用图像的应用程序时继续前进的基础，无论是用于分类还是物体检测的目的。
