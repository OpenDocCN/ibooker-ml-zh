- en: Chapter 8\. Feature Selection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 特征选择
- en: We use feature selection to select features that are useful to the model. Irrelevant
    features may have a negative effect on a model. Correlated features can make coefficients
    in regression (or feature importance in tree models) unstable or difficult to
    interpret.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用特征选择来选择对模型有用的特征。无关的特征可能会对模型产生负面影响。相关的特征可能会使回归中的系数（或者树模型中的特征重要性）不稳定或难以解释。
- en: The *curse of dimensionality* is another issue to consider. As you increase
    the number of dimensions of your data, it becomes more sparse. This can make it
    difficult to pull out a signal unless you have more data. Neighbor calculations
    tend to lose their usefulness as more dimensions are added.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*维度诅咒*是另一个需要考虑的问题。随着数据维度的增加，数据变得更加稀疏。这可能使得除非有更多数据，否则很难提取信号。随着维度的增加，邻居计算的效用通常会减弱。'
- en: Also, training time is usually a function of the number of columns (and sometimes
    it is worse than linear). If you can be concise and precise with your columns,
    you can have a better model in less time. We will walk through some examples using
    the `agg_df` dataset from the last chapter. Remember that this is the Titanic
    dataset with some extra columns for cabin information. Because this dataset is
    aggregating numeric values for each cabin, it will show many correlations. Other
    options include PCA and looking at the `.feature_importances_` of a tree classifier.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，训练时间通常是列数的函数（有时甚至比线性更差）。如果能够用简明和准确的列，可以在更短的时间内获得更好的模型。我们将通过上一章节的`agg_df`数据集来演示一些示例。请记住，这是具有一些额外列（用于舱室信息）的泰坦尼克号数据集的聚合数值数据集。因为这个数据集正在聚合每个舱室的数值值，它将展示许多相关性。其他选项包括PCA和查看树分类器的`.feature_importances_`。
- en: Collinear Columns
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共线性列
- en: 'We can use the previously defined `correlated_columns` function or run the
    following code to find columns that have a correlation coefficient of .95 or above:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用之前定义的`correlated_columns`函数或运行以下代码，以查找具有0.95或更高相关系数的列：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The Yellowbrick `Rank2` visualizer, shown previously, will plot a heat map of
    correlations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yellowbrick的`Rank2`可视化器，如前所示，将绘制一个相关性热图。
- en: The [rfpimp package](https://oreil.ly/MsnXc) has a visualization of *multicollinearity*.
    The `plot_dependence_heatmap` function trains a random forest for each numeric
    column from the other columns in a training dataset. The dependence value is the
    R2 score from the out-of-bag (OOB) estimates for predicting that column (see [Figure 8-1](#iddephm)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[rfpimp包](https://oreil.ly/MsnXc)有一个*多重共线性*的可视化。`plot_dependence_heatmap`函数为训练数据集中的每个数值列训练一个随机森林。依赖值是预测该列的袋外（OOB）估计的R2分数（见[图 8-1](#iddephm)）。'
- en: 'The suggested way to use this plot is to find values close to 1\. The label
    on the X axis is the feature that predicts the Y axis label. If a feature predicts
    another, you can remove the predicted feature (the feature on the Y axis). In
    our example, `fare` predicts `pclass`, `sibsp`, `parch`, and `embarked_Q`. We
    should be able to keep `fare` and remove the others and get similar performance:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此图的建议方法是找到接近1的值。X轴上的标签是预测Y轴标签的特征。如果一个特征预测另一个特征，可以移除被预测特征（Y轴上的特征）。在我们的例子中，`fare`预测`pclass`、`sibsp`、`parch`和`embarked_Q`。我们应该能够保留`fare`并移除其他特征，从而获得类似的性能：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Dependence heat map. Pclass, sibsp, parch, and embarked_Q can be predicted
    from fare, so we can remove them.](assets/mlpr_0801.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![依赖热图。Pclass、sibsp、parch和embarked_Q可以从fare中预测，因此我们可以移除它们。](assets/mlpr_0801.png)'
- en: Figure 8-1\. Dependence heat map. Pclass, sibsp, parch, and embarked_Q can be
    predicted from fare, so we can remove them.
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-1\. 依赖热图。Pclass、sibsp、parch和embarked_Q可以从fare中预测，因此我们可以移除它们。
- en: 'Here is code showing that we get a similar score if we remove these columns:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码展示了，如果我们移除这些列，我们可以得到类似的分数：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Lasso Regression
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lasso回归
- en: 'If you use lasso regression, you can set an `alpha` parameter that acts as
    a regularization parameter. As you increase the value, it gives less weight to
    features that are less important. Here we use the `LassoLarsCV` model to iterate
    over various values of alpha and track the feature coefficients (see [Figure 8-2](#idlr1)):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用lasso回归，可以设置一个`alpha`参数作为正则化参数。随着值的增加，对不那么重要的特征给予较小的权重。在这里，我们使用`LassoLarsCV`模型迭代各种alpha值，并跟踪特征系数（见[图 8-2](#idlr1)）：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Coefficients of features as alpha varies during lasso regression.](assets/mlpr_0802.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![随着lasso回归中alpha变化，特征系数的变化。](assets/mlpr_0802.png)'
- en: Figure 8-2\. Coefficients of features as alpha varies during lasso regression.
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. 在 Lasso 回归过程中，随着 alpha 变化，特征的系数。
- en: Recursive Feature Elimination
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归特征消除。
- en: 'Recursive feature elimination will remove the weakest features, then fit a
    model (see [Figure 8-3](#id20)). It does this by passing in a scikit-learn model
    with a `.coef_` or `.feature_importances_` attribute:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 递归特征消除将删除最弱的特征，然后拟合一个模型（参见[图 8-3](#id20)）。它通过传入具有 `.coef_` 或 `.feature_importances_`
    属性的 scikit-learn 模型来实现：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Recursive feature elimination.](assets/mlpr_0803.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![递归特征消除。](assets/mlpr_0803.png)'
- en: Figure 8-3\. Recursive feature elimination.
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 递归特征消除。
- en: We will use recursive feature elimination to find the 10 most important features.
    (In this aggregated dataset we find that we have leaked the survival column!)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用递归特征消除来找出前 10 个最重要的特征。（在这个聚合数据集中，我们发现泄漏了生存列！）
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Mutual Information
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 互信息。
- en: 'Sklearn provides nonparametric tests that will use k-nearest neighbor to determine
    the *mutual information* between features and the target. Mutual information quantifies
    the amount of information gained by observing another variable. The value is zero
    or more. If the value is zero, then there is no relation between them (see [Figure 8-4](#mi1)).
    This number is not bounded and represents the number of *bits* shared between
    the feature and the target:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Sklearn 提供了非参数测试，将使用 k 近邻算法来确定特征与目标之间的*互信息*。互信息量化了观察另一个变量带来的信息增益。该值为零或更高。如果值为零，则它们之间没有关系（参见[图 8-4](#mi1)）。这个数字不受限制，并代表特征与目标之间共享的*比特数*：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Mutual information plot.](assets/mlpr_0804.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![互信息图。](assets/mlpr_0804.png)'
- en: Figure 8-4\. Mutual information plot.
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-4\. 互信息图。
- en: Principal Component Analysis
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析。
- en: Another option for feature selection is to run principal component analysis.
    Once you have the main principal components, examine the features that contribute
    to them the most. These are features that have more variance. Note that this is
    an unsupervised algorithm and doesn’t take `y` into account.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择的另一个选项是运行主成分分析。一旦获得了主要的主成分，就检查对它们贡献最大的特征。这些特征具有更大的方差。请注意，这是一种无监督算法，不考虑 `y`。
- en: See [“PCA”](ch17.html#pca1) for more details.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[“PCA”](ch17.html#pca1)以获取更多细节。
- en: Feature Importance
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征重要性。
- en: Most tree models provide access to a `.feature_importances_` attribute following
    training. A higher importance typically means that there is higher error when
    the feature is removed from the model. See the chapters for the various tree models
    for more details.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数树模型在训练后提供了 `.feature_importances_` 属性。更高的重要性通常意味着在将特征从模型中移除时存在更高的错误。有关各种树模型的详细信息，请参阅各章节。
