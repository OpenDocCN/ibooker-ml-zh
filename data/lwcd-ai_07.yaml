- en: Chapter 7\. Training Custom ML Models in Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。在 Python 中训练自定义 ML 模型
- en: In this chapter, you’ll learn how to build classification models to predict
    customer churn using two popular ML libraries available in Python, scikit-learn
    and Keras. First, you’ll explore and clean your data using Pandas. Then you’ll
    learn how to use scikit-learn to prepare categorical features for training using
    one-hot encoding, train a logistic regression model, understand model performance
    using evaluation metrics, and improve model performance. You’ll learn how to perform
    the same steps using Keras to build a neural network classification model using
    the already prepared data. Along the way, you’ll learn more about performance
    metrics for classification models and how to better understand a confusion matrix
    to better evaluate your classification models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何使用两个流行的 Python 机器学习库，scikit-learn 和 Keras，构建分类模型来预测客户流失。首先，你将使用 Pandas
    探索和清理你的数据。然后，你将学习如何使用 scikit-learn 对分类特征进行独热编码以进行训练，训练逻辑回归模型，使用评估指标了解模型性能，并改善模型性能。你将学习如何使用
    Keras 执行相同的步骤，使用已准备好的数据构建神经网络分类模型。在此过程中，你将进一步了解分类模型的性能指标以及如何更好地理解混淆矩阵来评估你的分类模型。
- en: The dataset being used for this chapter, [the IBM Telco Customer Churn dataset](https://oreil.ly/Rz3r2),
    is a popular dataset for learning how to model customer churn. You should feel
    encouraged to look at other examples of how to work with this dataset to grow
    your knowledge after completing the exercises in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 此章节使用的数据集，[IBM 电信客户流失数据集](https://oreil.ly/Rz3r2)，是一个学习如何建模客户流失的流行数据集。在完成本章练习后，你应该有兴趣查看其他使用该数据集的示例，以增加你的知识。
- en: 'The Business Use Case: Customer Churn Prediction'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务用例：客户流失预测
- en: Your goal in this project will be to predict customer churn for a telecommunications
    company. Customer churn is defined as the *attrition* rate for customers, or in
    other words the rate of customers that choose to stop using services. Telecommunications
    companies often sell their products at a monthly rate or via annual contracts,
    so *churn* here will represent when a customer cancels their subscription or contract
    in the following month.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你在此项目中的目标是预测电信公司的客户流失。客户流失被定义为客户的离职率，或者说选择停止使用服务的客户比率。电信公司通常以每月费率或年度合同销售其产品，因此这里的“流失”表示客户在下个月取消其订阅或合同。
- en: The data is initially supplied in a CSV file, so you will need to spend some
    time loading the data into Pandas before you can explore it and ultimately use
    it to create your ML model using different frameworks. The dataset contains both
    numeric variables and categorical variables, where the variable takes on a value
    from a discrete set of possibilities.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 初始数据以 CSV 文件的形式提供，因此你需要花一些时间将数据加载到 Pandas 中，然后才能探索它，并最终使用它来创建不同框架下的 ML 模型。数据集包含数值变量和分类变量，其中变量从一组离散可能值中取值。
- en: There are 21 columns in the dataset. [Table 7-1](#schema_and_field_value_information)
    gives the column names, data types, and some information about the possible values
    for these columns.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有 21 列。[表 7-1](#schema_and_field_value_information) 提供了这些列的列名、数据类型以及关于这些列可能值的一些信息。
- en: Table 7-1\. Schema and field value information for the customer churn dataset
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-1\. 客户流失数据集的模式和字段值信息
- en: '| Column name | Column type | Notes about field values |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 列类型 | 关于字段值的注释 |'
- en: '| --- | --- | --- |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `customerID` | String | Unique value for every customer |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| `customerID` | String | 每个客户的唯一值 |'
- en: '| `gender` | String | “male” or “female” |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| `gender` | String | “男性”或“女性” |'
- en: '| `SeniorCitizen` | Integer | 1 if customer is a senior citizen, 0 otherwise
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| `SeniorCitizen` | Integer | 如果客户是老年人则为1，否则为0 |'
- en: '| `Partner` | String | Records if the customer has a partner (spouse or domestic
    partner) in the household |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| `Partner` | String | 记录客户是否在家庭中有伴侣（配偶或同居伴侣） |'
- en: '| `Dependents` | String | Records if the customer has dependents in the household
    or not |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| `Dependents` | String | 记录客户家庭中是否有受抚养人 |'
- en: '| `tenure` | Integer | Number of months the customer has used the telco service
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| `tenure` | Integer | 客户使用电信服务的月数 |'
- en: '| `PhoneService` | String | Records if the customer pays for phone service
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| `PhoneService` | String | 记录客户是否支付电话服务费用 |'
- en: '| `MultipleLines` | String | If the customer pays for phone service, do they
    pay for multiple phone lines? |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| `MultipleLines` | String | 如果客户支付电话服务，他们是否支付多条电话线？ |'
- en: '| `InternetService` | String | What type of internet service does the customer
    pay for, if any? |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| `InternetService` | String | 客户是否付费获得何种类型的互联网服务？ |'
- en: '| `OnlineSecurity` | String | Does the customer pay for online security? |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `OnlineSecurity` | String | 客户是否付费获得在线安全服务？ |'
- en: '| `OnlineBackup` | String | Does the customer pay for online backup? |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `OnlineBackup` | String | 客户是否付费获得在线备份服务？ |'
- en: '| `DeviceProtection` | String | Does the customer pay for device protection?
    |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `DeviceProtection` | String | 客户是否购买设备保护？ |'
- en: '| `TechSupport` | String | Does the customer pay for online tech support? |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `TechSupport` | String | 客户是否付费获得在线技术支持？ |'
- en: '| `StreamingTV` | String | Does the customer pay for streaming television?
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| `StreamingTV` | String | 客户是否付费观看流媒体电视？ |'
- en: '| `StreamingMovies` | String | Does the customer pay for streaming movies?
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `StreamingMovies` | String | 客户是否付费观看流媒体电影？ |'
- en: '| `Contract` | String | Does the customer have a contract or do they pay month
    by month? |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `Contract` | String | 客户是否签有合同或者按月支付？ |'
- en: '| `PaperlessBilling` | String | Does the customer use paperless billing? |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `PaperlessBilling` | String | 客户是否使用无纸化账单？ |'
- en: '| `PaymentMethod` | String | What payment method does the customer use? |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| `PaymentMethod` | String | 客户使用何种付款方式？ |'
- en: '| `MonthlyCharges` | Float | Monthly charge for customer services |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| `MonthlyCharges` | Float | 客户服务的月费用 |'
- en: '| `TotalCharges` | Float | Total amount customer has paid over lifetime |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| `TotalCharges` | Float | 客户在其生命周期内支付的总金额 |'
- en: '| `Churn` | String | Did the customer leave the telco service in the following
    month? |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| `Churn` | String | 客户是否在接下来的一个月内离开电信服务？ |'
- en: You will discover that many of the features can be consolidated or omitted for
    training your ML model. However, many of the features will need cleaning and further
    transformation to prepare for the training process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现许多功能可以合并或省略以训练你的 ML 模型。然而，许多功能需要清洗和进一步转换，以准备训练过程。
- en: Choosing Among No-Code, Low-Code, or Custom Code ML Solutions
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在无代码、低代码或定制代码 ML 解决方案之间进行选择
- en: 'Before exploring how to use custom training tools such as scikit-learn, Keras,
    or other options that were discussed in [Chapter 3](ch03.html#machine_learning_libraries_and_framewor),
    it is worth discussing when a custom solution could and should be used over other
    options discussed in this book so far:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索如何使用诸如 scikit-learn、Keras 或本书中讨论的其他选项之前，讨论何时应该使用定制培训工具是值得的，这在 [第 3](ch03.html#machine_learning_libraries_and_framewor)
    章中已经讨论过：
- en: No-code solutions
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 无代码解决方案
- en: These are great in two cases in particular. The first is when you need to build
    an ML model, but do not have any ML expertise. The goal of this book is to give
    you a bit more insight into how to make the right decisions around your data for
    ML, but no-code solutions often exist to simplify the decisions and lessen the
    need for working with more complex solutions. Another place where no-code solutions
    stand out is in rapid prototyping of models. Because no-code solutions, such as
    AutoML solutions, manage steps like feature engineering and hyperparameter tuning
    for the user, this can be an easy way to train a quick benchmark model. Not only
    that, but as shown in Chapters [4](ch04.html#use_automl_to_predict_advertising_media)
    and [5](ch05.html#using_automl_to_detect_fraudulent_trans), it is simple to deploy
    these models using Vertex AI AutoML. In many cases, these no-code solutions can
    be robust enough to use in production immediately. In practice, custom solutions
    can outperform no-code solutions given enough time and effort, but incremental
    gains in model performance can often be outweighed by the time saved in getting
    no-code solutions into production.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在两种特定情况下尤其出色。首先是当您需要构建一个 ML 模型，但没有任何 ML 的专业知识时。本书的目标是为您提供更多关于如何在 ML 数据周围做出正确决策的见解，但无代码解决方案通常存在以简化决策并减少与更复杂解决方案的工作需要。另一个无代码解决方案突出的地方是模型的快速原型设计。因为无代码解决方案（如
    AutoML 解决方案）管理诸如特征工程和超参数调整等步骤，这可以是训练快速基准模型的简便方法。不仅如此，正如第 [4](ch04.html#use_automl_to_predict_advertising_media)
    章和第 [5](ch05.html#using_automl_to_detect_fraudulent_trans) 章所示，通过 Vertex AI AutoML
    部署这些模型也非常简单。在许多情况下，这些无代码解决方案可以立即用于生产环境。在实践中，定制解决方案在足够时间和精力的情况下可能会优于无代码解决方案，但模型性能的增量收益通常会被将无代码解决方案投入生产所节省的时间所抵消。
- en: Low-code solutions
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 低代码解决方案
- en: These are great when you do need some customization and are working with data
    that meets the constraints of the tool you are using. For example, if you are
    working with structured data and the problem type you wish to solve is supported
    by BigQuery ML, then BigQuery ML could be a great choice. The advantage of a low-code
    solution in these cases is that less time needs to be spent on building the model
    and more time can be spent experimenting with your data and tuning your model.
    With many low-code solutions, the model can be productionized either directly
    within the product or via model export and using other tools like Vertex AI.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当你确实需要一些定制化，并且正在处理符合所使用工具约束条件的数据时，这些方案非常适合。例如，如果你正在处理结构化数据，并且希望解决的问题类型得到了BigQuery
    ML的支持，那么BigQuery ML可能是一个很好的选择。在这些情况下，低代码解决方案的优势在于，构建模型所需的时间更少，可以花更多时间对数据进行实验和调整模型。在许多低代码解决方案中，模型可以直接在产品内部进行生产化，或通过模型导出和使用Vertex
    AI等其他工具进行。
- en: Custom code solutions
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义代码解决方案
- en: These are by far the most flexible and are often leveraged by data scientists
    and other AI practitioners who like to build their own custom models. Using ML
    frameworks like TensorFlow, XGBoost, PyTorch, and scikit-learn, you can build
    a model using any type of data and the objective of your choice. In some sense,
    the sky’s the limit in terms of flexibility and deployment options. If you need
    a custom transformation, you can build it. If you need to be able to deploy your
    model as part of a web application, you can do it. Given the right data, expertise,
    and enough time, you can achieve the best results using custom code solutions.
    However, one of the trade-offs is that one needs to spend the time to learn the
    various different tools and techniques for doing this.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些无疑是最灵活的，经常被数据科学家和其他人工智能从业者所利用，他们喜欢构建自己定制的模型。使用像TensorFlow、XGBoost、PyTorch和scikit-learn这样的ML框架，你可以使用任何类型的数据构建模型，并选择你想要的目标。从某种意义上说，灵活性和部署选项几乎没有限制。如果需要自定义转换，可以进行构建。如果需要将模型作为Web应用的一部分进行部署，也可以实现。在拥有正确的数据、专业知识和足够时间的情况下，可以通过定制代码解决方案实现最佳结果。然而，其中一个权衡是需要花时间学习这些不同工具和技术。
- en: Which should you prefer? There is no single correct answer for every possible
    use case. Take into account the time you have to train, tune, and deploy the model.
    Also consider the dataset and the problem objective. Does the no-code or low-code
    solution support your use case? If not, then a custom code solution may be the
    only option. Finally, take into account your own expertise. If you know SQL very
    well but are new to Python, then something like BigQuery ML may be the best choice
    if it supports the problem you are attempting to solve.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该偏向哪一个？对于每种可能的用例，没有单一正确的答案。考虑你有多少时间来训练、调整和部署模型。还要考虑数据集和问题目标。无代码或低代码解决方案是否支持你的用例？如果不支持，那么自定义代码解决方案可能是唯一的选择。最后，考虑你自己的专业知识。如果你非常擅长SQL但对Python还不熟悉，那么像BigQuery
    ML这样如果支持你试图解决的问题，可能是最好的选择。
- en: This book does not aim to make you into an expert at using various different
    custom code ML frameworks. However, the book does take the approach that exposure
    to these tools and some basic knowledge can go a long way toward solving problems
    and collaborating with data scientists and ML engineers. If you are not familiar
    with Python, then Bill Lubanovic’s [*Introducing Python*](https://www.oreilly.com/library/view/introducing-python-2nd/9781492051374)
    (O’Reilly, 2019) is a great resource for getting started. Additionally, if you
    want to dive deeper into the ML frameworks introduced in this chapter, [*Hands-On
    Machine Learning with Scikit-Learn, Keras, and TensorFlow*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632)
    (2nd edition) by Aurélien Géron (O’Reilly, 2022) is a wonderful resource that
    is referenced by data scientists and ML engineers in practice.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不旨在让你成为使用各种不同自定义代码ML框架的专家。然而，本书确实采用了这样一种方法，认为接触这些工具和一些基础知识可以为解决问题和与数据科学家以及ML工程师合作走得更远。如果你对Python不熟悉，那么Bill
    Lubanovic的[*Introducing Python*](https://www.oreilly.com/library/view/introducing-python-2nd/9781492051374)（O'Reilly，2019）是一个很好的入门资源。此外，如果你想深入学习本章介绍的ML框架，Aurélien
    Géron的[*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632)（第二版，O'Reilly，2022）是一个被实际数据科学家和ML工程师引用的精彩资源。
- en: Exploring the Dataset Using Pandas, Matplotlib, and Seaborn
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Pandas、Matplotlib和Seaborn探索数据集
- en: Before you begin learning about scikit-learn and Keras, you should follow the
    workflow discussed in earlier chapters around understanding and preparing data
    for ML. Though you have used Google Colab briefly in earlier chapters to load
    the data from BigQuery into a DataFrame and do some basic visualization, you have
    not gone through the data preparation and model training process completely in
    the Jupyter Notebook environment.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习scikit-learn和Keras之前，您应该遵循之前章节讨论过的围绕理解和准备ML数据的工作流程。尽管您在之前章节中简要使用过Google Colab从BigQuery加载数据到DataFrame并进行了一些基本的可视化，但在Jupyter
    Notebook环境中还没有完全通过数据准备和模型训练过程。
- en: This section revisits how to load data into a Google Colab notebook using Pandas.
    Once the data is loaded into a DataFrame, you will explore, clean, and transform
    the data before creating the datasets that you will use to train your ML model.
    As you have seen in previous chapters, much of the work goes not into training
    the model but into understanding and preparing the training data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重新介绍如何使用Pandas将数据加载到Google Colab笔记本中。将数据加载到DataFrame后，您将探索、清理和转换数据，然后创建用于训练ML模型的数据集。正如您在之前章节中看到的那样，大部分工作不是用来训练模型，而是用来理解和准备训练数据。
- en: All of the code in this section, including some additional examples, is included
    in a Jupyter notebook in the [low-code-ai repo on GitHub](https://oreil.ly/supp-lcai).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的所有代码，包括一些额外的示例，都包含在GitHub上的[低代码AI仓库](https://oreil.ly/supp-lcai)的Jupyter笔记本中。
- en: Loading Data into a Pandas DataFrame in a Google Colab Notebook
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Google Colab笔记本中加载数据到Pandas DataFrame
- en: First, go to [*https://colab.research.google.com*](https://colab.research.google.com)
    and open a new notebook, following the process discussed in [Chapter 2](ch02.html#data_is_the_first_step).
    You may rename this notebook to a more meaningful name by clicking the name as
    shown in [Figure 7-1](#renaming_the_google_colab_notebook_t) and replacing the
    current name with a new name, say, *Customer_Churn_Model.ipynb*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，访问[*https://colab.research.google.com*](https://colab.research.google.com)，并打开一个新的笔记本，按照[第2章](ch02.html#data_is_the_first_step)中讨论的流程进行操作。您可以通过点击名称如[图7-1](#renaming_the_google_colab_notebook_t)所示，并替换当前名称为新名称，例如*Customer_Churn_Model.ipynb*来将此笔记本重命名为更有意义的名称。
- en: '![Renaming the Google Colab notebook to a more meaningful name](assets/lcai_0701.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![将Google Colab笔记本重命名为更有意义的名称](assets/lcai_0701.png)'
- en: Figure 7-1\. Renaming the Google Colab notebook to a more meaningful name.
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 将Google Colab笔记本重命名为更有意义的名称。
- en: 'Now type the following code into the first code block to import the packages
    needed to analyze and visualize the customer churn dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在第一个代码块中键入以下代码以导入分析和可视化客户流失数据集所需的包：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You saw some of these packages before in [Chapter 2](ch02.html#data_is_the_first_step)
    when first exploring the use of Colab notebooks, but some of these will be new
    to you here. The line `import sklearn` imports scikit-learn, a popular ML framework.
    Scikit-learn was first released in 2007 and was built on top of other Python libraries
    such as NumPy and SciPy. It is meant to be an easy-to-use framework for building
    ML models, including linear models, tree-based models, and support vector machines.
    The next line, `import tensorflow as tf`, imports TensorFlow. TensorFlow is a
    high-performance numerical computation library that was designed with the training
    and deployment of deep neural networks in mind. TensorFlow includes Keras, a library
    meant to ease the development of deep neural networks and the corresponding data
    transformations. You will be using Keras later in the chapter to train a neural
    network model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html#data_is_the_first_step)中首次探索使用Colab笔记本时，您已经见过其中一些包，但在这里有些包对您可能是新的。`import
    sklearn` 导入了scikit-learn，这是一个流行的ML框架。Scikit-learn首次发布于2007年，构建在NumPy和SciPy等其他Python库之上。它旨在成为一个易于使用的框架，用于构建包括线性模型、基于树的模型和支持向量机在内的ML模型。接下来的一行，`import
    tensorflow as tf`，导入了TensorFlow。TensorFlow是一个高性能的数值计算库，设计初衷是用于训练和部署深度神经网络。TensorFlow包括Keras，一个旨在简化深度神经网络开发和相应数据转换的库。您将在本章后面使用Keras来训练神经网络模型。
- en: Now execute the cell containing the `import` statements to import the packages.
    To do this, click the Run Cell button on the left side of the cell as shown in
    [Figure 7-2](#the_run_cell_button_is_seen_at_the_top), or press Shift + Enter
    to run the cell.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在执行包含`import`语句的单元格以导入这些包。要做到这一点，点击单元格左侧显示的运行单元格按钮，如[图7-2](#the_run_cell_button_is_seen_at_the_top)所示，或者按Shift
    + Enter来运行单元格。
- en: '![The Run Cell button is seen at the top left of the cell as shown here](assets/lcai_0702.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![在此处顶部左侧看到“运行单元”按钮](assets/lcai_0702.png)'
- en: Figure 7-2\. The Run Cell button is seen at the top left of the cell as shown
    here.
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 在此处顶部左侧看到“运行单元”按钮。
- en: 'You can quickly check that the `import` statements have executed successfully
    by checking the versions of the packages. Every package includes a special attribute,
    `__version__`, which returns the version of the package. Type the following code
    into a new cell, and execute the cell to check your version of the scikit-learn
    and TensorFlow packages:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过检查包的版本来快速验证 `import` 语句是否成功执行。每个包都包含一个特殊的属性 `__version__`，它返回包的版本。在一个新的单元格中输入以下代码，并执行该单元格以检查
    scikit-learn 和 TensorFlow 包的版本：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You should see the versions printed as shown in [Figure 7-3](#printing_the_versions_of_scikit_learn_a).
    Note that your exact version numbers will depend on when you are walking through
    this exercise.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到如 [图 7-3](#printing_the_versions_of_scikit_learn_a) 所示的版本已打印出来。请注意，您的确切版本号将取决于您执行此操作的时间。
- en: '![Printing the versions of scikit-learn and TensorFlow to ensure they were
    imported properly](assets/lcai_0703.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![打印 scikit-learn 和 TensorFlow 的版本，确保它们已正确导入](assets/lcai_0703.png)'
- en: Figure 7-3\. Printing the versions of scikit-learn and TensorFlow to ensure
    they were imported properly.
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 打印 scikit-learn 和 TensorFlow 的版本，确保它们已正确导入。
- en: 'Now you are ready to import your data. Recall that the dataset is stored in
    the CSV format, so you will need to download that data, upload it to your notebook,
    and then import into a Pandas DataFrame, correct? Actually, that is not the case.
    A very nice feature of Pandas is that you can directly import a CSV file into
    a DataFrame from a location on the internet without having to download the file
    first. To do this, type in the following code into a new cell and execute the
    cell:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已准备好导入数据了。回想一下，数据集以 CSV 格式存储，因此您需要下载该数据，将其上传到笔记本中，然后导入到 Pandas DataFrame
    中，对吗？实际上，情况并非如此。Pandas 的一个非常好的功能是，您可以直接从互联网上的位置将 CSV 文件导入到 DataFrame 中，而无需先下载文件。要做到这一点，输入以下代码到一个新的单元格，并执行该单元格：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In general, it is a good idea to look at the first few rows of the DataFrame.
    Use `df_raw.head()` to explore the first few rows of the DataFrame. You can quickly
    scroll through the columns of the data and see at a glance that it seems like
    the types correspond to what was expected. An example of a few of the columns
    is shown in [Table 7-2](#a_few_columns_of_the_first_five_rows_of). Looking at
    the first few rows is a great quick first step, but of course this dataset is
    more than just a few rows and there could be some problems lurking where you cannot
    see them.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，查看 DataFrame 的前几行是一个好主意。使用 `df_raw.head()` 来探索 DataFrame 的前几行。您可以快速滚动数据的列，并一目了然地看到它们似乎与预期相符。表 [7-2](#a_few_columns_of_the_first_five_rows_of) 显示了前五行的部分列示例。查看前几行是一个很好的快速第一步，但当然，这个数据集不仅仅是几行，可能会隐藏一些看不见的问题。
- en: Table 7-2\. A few columns of the first five rows of the DataFrame `df_raw` printed
    using the `head()` method
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-2\. 使用 `head()` 方法打印的 DataFrame `df_raw` 的前五行的部分列
- en: '| `Stream⁠ing​TV` | `Stream⁠ing​Movies` | `Contract` | `Paper⁠less​Billing`
    | `Payment​Me⁠thod` | `MonthlyCharges` | `TotalCharges` | `Churn` |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `流媒体电视` | `流媒体电影` | `合同` | `无纸化账单` | `支付方式` | `月费` | `总费用` | `流失` |'
- en: '| `No` | `No` | `Month-to-month` | `Yes` | `Electronic check` | `29.85` | `29.85`
    | `No` |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `No` | `No` | `按月付费` | `Yes` | `电子支票` | `29.85` | `29.85` | `No` |'
- en: '| `No` | `No` | `One Year` | `No` | `Mailed check` | `56.95` | `1889.5` | `No`
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `No` | `No` | `一年` | `No` | `邮寄支票` | `56.95` | `1889.5` | `No` |'
- en: '| `No` | `No` | `Month-to-Month` | `Yes` | `Mailed check` | `53.85` | `108.15`
    | `Yes` |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `No` | `No` | `按月` | `Yes` | `邮寄支票` | `53.85` | `108.15` | `Yes` |'
- en: '| `No` | `No` | `One year` | `No` | `Bank transfer (automatic)` | `42.30` |
    `1840.75` | `No` |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `No` | `No` | `一年` | `No` | `银行转账（自动）` | `42.30` | `1840.75` | `No` |'
- en: '| `No` | `No` | `Month-to-month` | `Yes` | `Electronic check` | `70.70` | `151.65`
    | `Yes` |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `No` | `No` | `按月付费` | `Yes` | `电子支票` | `70.70` | `151.65` | `Yes` |'
- en: Understanding and Cleaning the Customer Churn Dataset
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和清理客户流失数据集
- en: Now that the data has been loaded into the DataFrame `df_raw`, you can begin
    to explore and understand it. The immediate goal is to get an idea of where there
    could be issues with the data so that you may resolve those issues before moving
    forward. However, you should also be keeping an eye out for the overall distribution
    and other properties of the columns of your DataFrame since this will be important
    when transforming the data later.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据已加载到DataFrame `df_raw`中，您可以开始探索和理解数据。即时目标是了解数据可能存在的问题，以便在继续之前解决这些问题。但是，在转换数据时，您还应该关注DataFrame列的整体分布和其他属性，这在后续操作中将非常重要。
- en: Checking and converting data types
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查和转换数据类型
- en: First you will check that the data types inferred by Pandas match up with what
    was expected from [Table 7-1](#schema_and_field_value_information). Why is this
    useful? It can be an easy way to check for mistyped data, which can often come
    from issues with the data itself. For example, what if there is a string value
    in a column of integers? Pandas will import this column as a string column because
    integers can be cast as strings, but not vice versa in most cases. To check the
    data types for your DataFrame, type **`df_raw.dtypes`** into a new cell and execute
    the cell.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您将检查Pandas推断的数据类型是否与[Table 7-1](#schema_and_field_value_information)中预期的相符。为什么这很有用？这可以是检查误输入数据的简便方法，这种问题通常来自数据本身。例如，如果整数列中存在字符串值，Pandas会将该列导入为字符串列，因为整数可以转换为字符串，但反之通常不行。要检查DataFrame的数据类型，请在新单元格中键入**`df_raw.dtypes`**并执行该单元格。
- en: Note there are no parentheses after `dtypes`. This is because `dtypes` is not
    a function but rather a property of the Pandas DataFrame `df_raw`. Anything that
    was not a floating-point number or an integer was imported as an `object` in the
    DataFrame. This is normal behavior for a Pandas DataFrame. If you look through
    the output more carefully, though, almost every column matches the expected type,
    except for the `TotalCharges` column. You can see the output for the last few
    columns in [Table 7-3](#the_data_types_for_the_last_six_columns) and confirm that
    you see the same thing in your notebook environment.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在`dtypes`后面没有括号。这是因为`dtypes`不是函数，而是Pandas DataFrame `df_raw`的属性。除了`TotalCharges`列之外，Pandas
    DataFrame中导入的任何不是浮点数或整数的内容都被视为`object`类型是正常行为。如果您仔细查看输出，几乎每一列都与预期类型匹配，但`TotalCharges`列除外。您可以在[Table 7-3](#the_data_types_for_the_last_six_columns)中查看最后几列的输出，并确认在您的笔记本环境中是否看到相同的情况。
- en: Table 7-3\. The data types for the last six columns of the `df_raw` DataFrame
    (note that the `TotalCharges` column is not a `float64` as expected)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Table 7-3. `df_raw` DataFrame的最后六列的数据类型（请注意，`TotalCharges`列不是预期的`float64`类型）
- en: '| `Contract` | `object` |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `Contract` | `object` |'
- en: '| `PaperlessBilling` | `object` |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `PaperlessBilling` | `object` |'
- en: '| `PaymentMethod` | `object` |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `PaymentMethod` | `object` |'
- en: '| `MonthlyCharges` | `float64` |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `MonthlyCharges` | `float64` |'
- en: '| `TotalCharges` | `object` |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `TotalCharges` | `object` |'
- en: '| `Churn` | `object` |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `Churn` | `object` |'
- en: This is a good sign that there is something different about the `TotalCharges`
    column than expected. Before moving forward, you should explore this column and
    understand what is happening. Recall that you can work with a single column of
    a Pandas DataFrame using the syntax `df['ColumnName']`, where `df` is the DataFrame
    name and `ColumnName` is the column’s name.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`TotalCharges`列与预期有所不同，这是一个不错的迹象。在继续之前，您应该探索该列并理解发生了什么。请记住，您可以使用语法`df[''ColumnName'']`来处理Pandas
    DataFrame的单个列，其中`df`是DataFrame名称，`ColumnName`是列名。'
- en: 'Begin by getting some high-level statistics about the `TotalCharges` column
    using the `describe()` method. Try to do this without looking at the provided
    code first, but if you need it, the code is right here:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`describe()`方法获取`TotalCharges`列的一些高级统计信息。尝试在查看提供的代码之前完成这一步，但如果需要，这里是代码：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Your output should be the same as the output in [Figure 7-4](#summary_statistics_of_the_totalcharges).
    Since `TotalCharges` is being treated as a categorical variable (in this case
    as a string), you only see the count of elements, the number of unique values,
    the top value in terms of frequency, and the number of times that value appears.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您的输出应与[Figure 7-4](#summary_statistics_of_the_totalcharges)中的输出相同。由于`TotalCharges`被视为分类变量（在这种情况下为字符串），因此您只会看到元素的计数、唯一值的数量、频率最高的顶级值以及该值出现的次数。
- en: '![Summary statistics of the TotalCharges column. Note that the most frequent
    value is a string with a single space.](assets/lcai_0704.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![TotalCharges列的汇总统计信息。注意，最频繁的值是一个只包含一个空格的字符串。](assets/lcai_0704.png)'
- en: Figure 7-4\. Summary statistics of the `TotalCharges` column. Note that the
    most frequent value is a string with a single space.
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4\. `TotalCharges`列的汇总统计信息。注意，最频繁的值是一个只包含一个空格的字符串。
- en: In this case you can see the issue almost immediately. The top value is either
    a blank or empty string, and it appears 11 times. This is likely why Pandas treated
    the `TotalCharges` as an unexpected data type and led you to discover an issue
    with the data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您几乎可以立即看到问题所在。顶部的值要么为空，要么是空字符串，并且出现了11次。这很可能是为什么Pandas将`TotalCharges`视为意外数据类型，并导致您发现数据的问题。
- en: 'When you have missing data, you can ask, “What should be there?” To try to
    understand this, look at the corresponding rows of data in the DataFrame and see
    if there is a pattern for which rows are missing. To do that, you will create
    a *mask* and apply it to the DataFrame. The mask will be a simple statement that
    returns `true` or `false` depending on the input value. In this case, your mask
    will be of the form `mask=(df.raw[''TotalCharges'']=='' '')`. The `==` operator
    checks to see if the value of the `TotalCharges` column is equal to the string
    with a single space. If the value is a string with a single space, the operator
    returns `true`; otherwise it returns `false`. Type the following code into a new
    cell and execute the cell:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据缺失时，您可以问：“应该有什么？”为了尝试理解这一点，查看DataFrame中对应的数据行，看看缺失的行是否有某种模式。为此，您将创建一个*掩码*并将其应用于DataFrame。该掩码将是一个简单的语句，根据输入值返回`true`或`false`。在本例中，您的掩码将采用`mask=(df.raw['TotalCharges']=='
    ')`的形式。`==`运算符用于检查`TotalCharges`列的值是否等于只包含一个空格的字符串。如果值是只包含一个空格的字符串，则运算符返回`true`；否则返回`false`。将以下代码输入到新的单元格中并执行该单元格：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The output of the cell is shown in [Table 7-4](#the_first_few_columns_and_rows_of_the_d).
    Now explore the results of this cell. Do you notice anything that may explain
    why the `TotalCharges` column is blank for these customers? Look at the `tenure`
    column and notice that the value is 0 for each of these customers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 该单元格的输出显示在[表7-4](#the_first_few_columns_and_rows_of_the_d)中。现在探索此单元格的结果。您是否注意到任何可能解释为何这些客户的`TotalCharges`列为空的情况？查看`tenure`列，注意这些客户的值都为0。
- en: Table 7-4\. The first few columns and rows of the DataFrame `df_raw` whose value
    for the `TotalCharges` column is `' '`
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-4\. 数据框`df_raw`的前几列和行，其中`TotalCharges`列的值为`' '`
- en: '| `customerID` | `gender` | `SeniorCitizen` | `Partner` | `Dependents` | `tenure`
    | `PhoneService` |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `customerID` | `gender` | `SeniorCitizen` | `Partner` | `Dependents` | `tenure`
    | `PhoneService` |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| `4472-LVYGI` | `Female` | `0` | `Yes` | `Yes` | `0` | `No` |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `4472-LVYGI` | `女` | `0` | `是` | `是` | `0` | `否` |'
- en: '| `3115-CZMZD` | `Male` | `0` | `No` | `Yes` | `0` | `Yes` |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `3115-CZMZD` | `男` | `0` | `否` | `是` | `0` | `是` |'
- en: '| `5709-LVOEQ` | `Female` | `0` | `Yes` | `Yes` | `0` | `Yes` |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `5709-LVOEQ` | `女` | `0` | `是` | `是` | `0` | `是` |'
- en: '| `4367-NUYAO` | `Male` | `0` | `Yes` | `Yes` | `0` | `Yes` |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `4367-NUYAO` | `男` | `0` | `是` | `是` | `0` | `是` |'
- en: '| `1371-DWPAZ` | `Female` | `0` | `Yes` | `Yes` | `0` | `No` |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `1371-DWPAZ` | `女` | `0` | `是` | `是` | `0` | `否` |'
- en: 'If the tenure is 0, then this is the first month for these customers with the
    telco, and they have not been charged yet. This explains why there is no value
    for `TotalCharges` for these customers. Now verify this hypothesis by using a
    different mask to check the rows with tenure equal to 0\. Try to write the code
    for this cell on your own, but the solution follows in case you need any help:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`tenure`为0，则这对电信公司的新客户来说是他们的第一个月，他们还没有被收费。这就解释了为什么这些客户的`TotalCharges`没有值。现在通过使用另一个掩码来验证这个假设，检查`tenure`等于0的行。尝试自己编写此单元格的代码，但如果需要帮助，可以查看下面的解决方案：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Note that in the code above, you specify a list of columns `[''tenure'',''Total​Char⁠ges'']`.
    Since you were looking purely at the relationship between `tenure` and `TotalCharges`,
    this will make the results easier to parse. All 11 rows with `TotalCharges` equal
    to `'' ''` have the value 0 for the `tenure` column. So, indeed, the relationship
    was as expected. You now know that these odd string values correspond to zero
    `TotalCharges` and can replace the string values with the float `0.0`. The easiest
    way to do this is to use the `df.replace()` method. The syntax for this function
    can take a little bit to parse, so first type the following code into a new cell
    and execute that cell to see the results:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述代码中，你指定了一个列名列表 `['tenure','Total​Char⁠ges']`。由于你仅仅关注 `tenure` 和 `TotalCharges`
    之间的关系，这将使结果更易于解析。所有 `TotalCharges` 等于 `' '` 的11行数据，在 `tenure` 列中的值均为0。因此，实际上，这个关系是符合预期的。现在你知道这些奇怪的字符串值对应于零
    `TotalCharges`，并且可以将这些字符串值替换为浮点数 `0.0`。最简单的方法是使用 `df.replace()` 方法。这个函数的语法可能需要一点时间来解析，所以首先在一个新的单元格中输入以下代码，并执行该单元格来查看结果：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Your results should be the same as the results in [Table 7-5](#the_totalcharges_column_has_been_replac).
    You can now see that the string values for `TotalCharges` from before have now
    been replaced by the float value `0.0`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你的结果应该与 [Table 7-5](#the_totalcharges_column_has_been_replac) 中的结果相同。现在你可以看到，以前
    `TotalCharges` 的字符串值现在已经被替换为浮点数值 `0.0`。
- en: Table 7-5\. The `TotalCharges` column has been replaced with the value `0.0`
    for the rows where the value of `tenure` is `0`
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Table 7-5\. `TotalCharges` 列已被替换为值 `0.0`，对应所有 `tenure` 值为 `0` 的行
- en: '|   | `tenure` | `TotalCharges` |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|   | `tenure` | `TotalCharges` |'
- en: '| --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `488` | `0` | `0.0` |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `488` | `0` | `0.0` |'
- en: '| `753` | `0` | `0.0` |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| `753` | `0` | `0.0` |'
- en: '| `936` | `0` | `0.0` |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `936` | `0` | `0.0` |'
- en: '| `1082` | `0` | `0.0` |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `1082` | `0` | `0.0` |'
- en: '| `1340` | `0` | `0.0` |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `1340` | `0` | `0.0` |'
- en: '| `3331` | `0` | `0.0` |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `3331` | `0` | `0.0` |'
- en: '| `3826` | `0` | `0.0` |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| `3826` | `0` | `0.0` |'
- en: '| `4380` | `0` | `0.0` |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| `4380` | `0` | `0.0` |'
- en: '| `5218` | `0` | `0.0` |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| `5218` | `0` | `0.0` |'
- en: '| `6670` | `0` | `0.0` |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| `6670` | `0` | `0.0` |'
- en: '| `6754` | `0` | `0.0` |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| `6754` | `0` | `0.0` |'
- en: 'With these results in mind, it becomes easier to understand the syntax used
    in the first line of code, `df_raw.replace({''TotalCharges'': {'' '': 0.0}})`.
    The method takes a Python data structure known as a dictionary. *Dictionaries*
    are unordered lists of pairs where the first element of each pair is the name
    of a value, and the second element of each pair is the value itself. In this case,
    the first element is `TotalCharges`, the name of the column where you want to
    replace values. The second element is a dictionary itself, `{'' '':0.0}`. The
    first element of this pair is the value you want to replace, and the second element
    of the pair is the new value you’d like to insert.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '将这些结果牢记心中，更容易理解第一行代码中使用的语法，即 `df_raw.replace({''TotalCharges'': {'' '': 0.0}})`。该方法采用了一种称为字典的Python数据结构。*字典*
    是无序的键值对列表，其中每对的第一个元素是值的名称，第二个元素是值本身。在这种情况下，第一个元素是 `TotalCharges`，即你想要替换值的列名。第二个元素本身也是一个字典，`{''
    '':0.0}`。这对中的第一个元素是你想要替换的值，第二个元素是你想要插入的新值。'
- en: 'Before you explore the summary statistics for the `TotalCharges` column and
    the other numeric columns, be sure Pandas knows that `TotalCharges` is a column
    of `float` values. To do so, type the following code into a new cell and execute
    that cell:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在你探索 `TotalCharges` 列和其他数值列的汇总统计数据之前，请确保 Pandas 知道 `TotalCharges` 是一个 `float`
    值的列。要做到这一点，请在一个新的单元格中输入以下代码，并执行该单元格：
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that the `astype()` method uses similar arguments to the `replace()` method.
    The input is a dictionary where the first element of each pair is the column whose
    data type will be changed, and the second argument (here, `float64`) is the new
    data type for that column. Your output from the cell should be similar to what
    is shown in [Table 7-6](#the_datatypes_of_the_last_four_columns).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`astype()` 方法使用了与 `replace()` 方法类似的参数。输入是一个字典，其中每对的第一个元素是要更改其数据类型的列，第二个参数（这里是
    `float64`）是该列的新数据类型。你从单元格的输出应该类似于 [Table 7-6](#the_datatypes_of_the_last_four_columns)
    中显示的结果。
- en: Table 7-6\. The datatypes of the last four columns of the new DataFrame `df_2`
    portrayed in vertical orientation (the rest of the columns are omitted in this
    figure)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Table 7-6\. 新 DataFrame `df_2` 中最后四列的数据类型，以竖排展示（本图中省略了其余列）
- en: '| `PaymentMethod` | `object` |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| `PaymentMethod` | `object` |'
- en: '| `MonthlyCharges` | `float64` |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| `MonthlyCharges` | `float64` |'
- en: '| `TotalCharges` | `float64` |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| `TotalCharges` | `float64` |'
- en: '| `Churn` | `object` |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| `Churn` | `object` |'
- en: Exploring summary statistics
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索汇总统计信息
- en: 'Now that you have solved the datatype issue you encountered, look at the summary
    statistics of the numeric columns. You saw how to do this back in [Chapter 2](ch02.html#data_is_the_first_step),
    so try to do this without looking at the code first, though the code is below
    in case you need any help, and the results are in [Table 7-7](#summary_statistics_for_the_numeric_colu):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经解决了遇到的数据类型问题，请查看数值列的汇总统计信息。您之前在 [第二章](ch02.html#data_is_the_first_step)
    中看到如何执行此操作，因此请尝试在不查看代码的情况下完成，尽管代码如下以备您需要帮助，并且结果在 [表 7-7](#summary_statistics_for_the_numeric_colu)
    中：
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Table 7-7\. Summary statistics for the numeric columns in the customer churn
    dataset
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-7。客户流失数据集中数值列的汇总统计信息
- en: '|   | `SeniorCitizen` | `tenure` | `MonthlyCharges` | `TotalCharges` |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|   | `老年人` | `任期` | `每月费用` | `总费用` |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **`count`** | `7043` | `7043` | `7043` | `7043` |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **`count`** | `7043` | `7043` | `7043` | `7043` |'
- en: '| **`mean`** | `0.162147` | `32.371149` | `64.761692` | `2279.734304` |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **`mean`** | `0.162147` | `32.371149` | `64.761692` | `2279.734304` |'
- en: '| **`std`** | `0.368612` | `24.559481` | `30.090047` | `2266.79447` |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **`std`** | `0.368612` | `24.559481` | `30.090047` | `2266.79447` |'
- en: '| **`min`** | `0` | `0` | `18.25` | `0` |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **`min`** | `0` | `0` | `18.25` | `0` |'
- en: '| **`25%`** | `0` | `9` | `35.5` | `398.55` |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| **`25%`** | `0` | `9` | `35.5` | `398.55` |'
- en: '| **`50%`** | `0` | `29` | `70.35` | `1394.55` |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| **`50%`** | `0` | `29` | `70.35` | `1394.55` |'
- en: '| **`75%`** | `0` | `55` | `89.85` | `3786.6` |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| **`75%`** | `0` | `55` | `89.85` | `3786.6` |'
- en: '| **`max`** | `1` | `72` | `118.75` | `8684.8` |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| **`max`** | `1` | `72` | `118.75` | `8684.8` |'
- en: At a glance, looking at the results in [Table 7-7](#summary_statistics_for_the_numeric_colu),
    there are no odd values or anything amiss except for maybe `SeniorCitizen`. Recall
    that `SeniorCitizen` has the value of either 0 or 1\. The average (mean) value
    for the `SeniorCitizen` column, 0.162…, then represents the percentage of customers
    that are senior citizens. Though the feature may be better thought of as a categorical
    variable, the fact that it is a binary 0 or 1 means that summary statistics like
    the mean can still give useful information.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一眼看去，在 [表 7-7](#summary_statistics_for_the_numeric_colu) 的结果中，除了可能是 `老年人` 外，没有异常值或其他不正常情况。回想一下，`老年人`
    的值要么是 0 要么是 1。`SeniorCitizen` 列的平均值（均值）为 0.162…，则代表了老年顾客的百分比。尽管该特征可能更适合被视为分类变量，但它是二进制的
    0 或 1，这意味着像均值这样的汇总统计信息仍然可以提供有用的信息。
- en: 'Speaking of categorical features, how can you explore summary statistics for
    these features? The `describe()` method by default only shows numeric features.
    You can have it include statistics for categorical features by using the optional
    keyword argument `include=''object''`. This specifies that you want to include
    only columns of type `object`, which is the default data type for all non-numeric
    columns in Pandas. Include this optional argument in the `describe()` method in
    a new cell and execute the cell. The code is included here in case you need help:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到分类特征，你如何探索这些特征的汇总统计信息？`describe()` 方法默认只显示数值特征。您可以通过使用可选关键字参数 `include='object'`
    来包含分类特征的统计信息。这指定您希望仅包括 `object` 类型的列，这是 Pandas 中所有非数值列的默认数据类型。在 `describe()` 方法中包括此可选参数，并执行单元格。如果您需要帮助，这里包含了代码：
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You will now see the statistics for the categorical features. These summary
    statistics are more simplistic since you are working with discrete values instead
    of numeric values. You can see the number of rows with non-null values or the
    count, the number of unique values, the most frequent value (or one of the most
    frequent values in case of a tie), and the frequency of that value.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在将看到分类特征的统计信息。这些汇总统计信息更简单，因为您正在处理离散值而不是数值。您可以看到具有非空值的行数或计数，唯一值的数量，最常见的值（或在并列情况下的其中之一），以及该值的频率。
- en: For example, consider the `customerID` column. This column has the same number
    of unique values as it does rows. Another way to interpret this information is
    that every single value in this column is unique. You can additionally see that
    by looking at the frequency at which the top value appears.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑 `customerID` 列。该列的唯一值数量与行数相同。解释这些信息的另一种方式是，该列中的每个值都是唯一的。您还可以通过查看顶部值出现的频率来进一步验证这一点。
- en: 'Explore the summary statistics and see what else you notice. Here is a collection
    of some observations that will be helpful moving forward but are by no means a
    complete list of the useful information available from these results:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 探索摘要统计信息并查看其他注意事项。以下是一些观察结果的集合，这些将对未来的工作有所帮助，但并不完全涵盖这些结果中可用信息的列表：
- en: The `gender` and `Partner` columns are fairly well balanced between two different
    values.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gender` 和 `Partner` 列在两个不同值之间相当平衡。'
- en: A large majority of customers have phone service, but almost half of those customers
    do not have multiple lines.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绝大多数客户拥有电话服务，但几乎一半的客户没有多条线路。
- en: Many of the columns have three different possible values. Though you have information
    about the top class, you do not know the distribution of different values at this
    time.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多列有三种不同的可能值。尽管您已了解到顶级类别的信息，但目前还不清楚不同值的分布情况。
- en: The label for our dataset, `Churn,` is somewhat unbalanced with about a 5:2
    ratio of `No` to `Yes` values.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们数据集的标签 `Churn` 的平衡性有些不足，约为 `No` 和 `Yes` 值的 5:2 比率。
- en: All columns, including the numeric columns, have 7,043 elements. There could
    be other missing values similar to what you discovered for `TotalCharges`, but
    there are not any null values.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有列，包括数字列，都有 7,043 个元素。可能存在其他类似于您发现的 `TotalCharges` 的缺失值，但没有任何空值。
- en: Exploring combinations of categorical columns
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索分类列的组合
- en: As you saw in [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg),
    looking at interactions between the different features can often help you understand
    which features are most important and how they interact. However, for that project,
    all of your features were numeric. In this project, most of your features are
    instead categorical. You will explore a method of understanding feature interactions
    in this case by looking at the distribution of different feature value combinations
    across multiple columns.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第 6 章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)中所见，查看不同特征之间的交互通常有助于了解哪些特征最重要以及它们之间的相互作用。然而，在该项目中，您的大多数特征是分类的而不是数值的。在这种情况下，通过查看跨多列的不同特征值组合的分布来探索理解特征交互的方法。
- en: 'First look at the `PhoneService` and `MultipleLines` columns. Common sense
    dictates that a customer cannot have multiple phone lines if they do not have
    phone service. You can confirm that this is true in the dataset by using the `value_counts()`
    method. The `value_counts()` method takes a list of columns in your DataFrame
    as an argument and returns the count of unique value combinations. Type the following
    code into a new cell and execute that cell to return the unique value combinations
    across the `PhoneService` and `MultipleLines` columns:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 首先查看 `PhoneService` 和 `MultipleLines` 列。常识告诉我们，如果客户没有电话服务，他们就不能拥有多条电话线。通过使用
    `value_counts()` 方法，您可以在数据集中确认这一点。`value_counts()` 方法接受一个数据框中的列列表作为参数，并返回唯一值组合的计数。在新单元格中输入以下代码并执行以返回跨
    `PhoneService` 和 `MultipleLines` 列的唯一值组合：
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Your results should be the same as the following results. Note that `MultipleLines`
    has three different values, `No`, `Yes`, and `No phone service`. Unsurprisingly,
    `No phone service` only occurs when the `PhoneService` feature has value `No`.
    This means that the `MultipleLines` feature contains all of the information of
    the `PhoneService` feature. `PhoneService` is redundant, and you will remove this
    feature from your training dataset later.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您的结果应与以下结果相同。请注意，`MultipleLines` 有三种不同的值，`No`、`Yes` 和 `No phone service`。毫不奇怪，当
    `PhoneService` 特征的值为 `No` 时，`No phone service` 只出现一次。这意味着 `MultipleLines` 特征包含了
    `PhoneService` 特征的所有信息。`PhoneService` 是多余的，您将稍后从训练数据集中删除此特征。
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Are other features in your dataset “correlated” in a similar fashion? Unsurprisingly,
    this is indeed the case. As an exercise, write code in a new cell to explore the
    relationship between the `InternetService`, `OnlineSecurity`, `OnlineBackup`,
    `StreamingTV`, and `StreamingMovies`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的其他特征是否以类似的方式“相关”？毫不奇怪，确实如此。作为练习，在新单元格中编写代码来探索 `InternetService`、`OnlineSecurity`、`OnlineBackup`、`StreamingTV`
    和 `StreamingMovies` 之间的关系。
- en: Once again you have some redundancy between feature values, but it’s not as
    clear in this case. When the value of `InternetService` is `No`, the value of
    all of the other columns is `No internet service`. However there are two different
    internet types, `Fiber optic` and `DSL`, and the picture is not as clear in those
    cases whether there is redundancy or not. Though you did not include the columns
    here, the `DeviceProtection` and `TechSupport` columns have the same relationship
    with `InternetService`. You should explore this on your own as well. You will
    consider how to take this information into consideration in the next section when
    transforming features.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 再次出现一些特征值之间的冗余，但在这种情况下并不那么明显。当`InternetService`的值为`No`时，所有其他列的值都为`No internet
    service`。然而，存在两种不同的互联网类型，即`Fiber optic`和`DSL`，在这些情况下图片不那么清晰，不清楚是否存在冗余。虽然您在这里没有包括这些列，但`DeviceProtection`和`TechSupport`列与`InternetService`具有相同的关系。您应该自己探索这一点。在转换特征时，您将考虑如何考虑这些信息。
- en: Note
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Beyond looking at counts of specific value combinations, there exist techniques
    for understanding the correlation between different values of categorical features.
    Two such examples are the chi-square test and Cramer’s V coefficient. The *chi-square
    test* checks for the independence of a dependent and independent categorical variable,
    while *Cramer’s V coefficient* determines the strength of that relationship, similar
    to the Pearson correlation coefficient for numeric variables. For more details,
    you can reference almost any statistics book, such as [*Statistics in a Nutshell*](https://www.oreilly.com/library/view/statistics-in-a/9781449361129/)
    by Sarah Boslaugh (O’Reilly, 2012).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 除了查看特定值组合的计数之外，还存在用于理解分类特征不同值之间关联的技术。其中两个例子是卡方检验和*Cramer's V系数*。*卡方检验*检查因变量和独立分类变量之间的独立性，而*Cramer's
    V系数*确定了该关系的强度，类似于数值变量的Pearson相关系数。有关更多细节，您可以参考几乎任何统计书籍，比如Sarah Boslaugh的[*Statistics
    in a Nutshell*](https://www.oreilly.com/library/view/statistics-in-a/9781449361129/)（O’Reilly,
    2012）。
- en: 'You should also explore the relationship between the categorical features and
    the label `Churn`. For example, consider the `Contract` feature. There are three
    possible values for this feature: `Month-to-month`, `One year`, and `Two year`.
    What does your intuition say about this feature and how it relates to `Churn`?
    You should reasonably expect that longer contract periods lead to churn being
    less likely, at least if the customer is not at the end of the contract period.
    You can use the `value_counts()` method as before to look at this relationship,
    but often it is easier to visually understand relationships than look at a table
    of values. To visualize this, write the following code into a new cell and execute
    that cell:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应探索分类特征与标签`Churn`之间的关系。例如，考虑`Contract`特征。此特征有三个可能的值：`Month-to-month`、`One
    year`和`Two year`。您对此特征与`Churn`的关系有何直觉？您可以合理地期望较长的合同期限会降低流失率，至少如果客户不是处于合同期末的话。您可以像之前一样使用`value_counts()`方法来查看这种关系，但通常更容易通过视觉理解关系而不是查看数值表格。要可视化这一点，请将以下代码写入一个新的单元格并执行该单元格：
- en: '[PRE12]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is actually one very long line of code to be parsed. The parentheses at
    the beginning and end tell Python to treat this as one line of code rather than
    three separate lines. First the `groupby()` function is used to group the values
    by different `Contract` values. You want to look at how `Churn` relates, so you
    select the `Churn` column and then apply the `value_counts` function. Note the
    additional `normalize=True` argument, which will replace the value counts for
    each pair with a percentage rather than a number. The advantage of this is that
    you can see within each value of `Contract` what percentage of customers churned
    versus those that did not instead of comparing counts across uneven groups. The
    `unstack()` function is used to format the table into a more human-readable format
    before you use built-in Pandas plotting capabilities to plot the data. In this
    case, [Figure 7-5](#visualization_of_the_proportion_of_cust) uses a stacked bar
    chart to visually compare the different values for `Contract` quickly.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是一行非常长的代码要进行解析。括号在开头和结尾告诉 Python 将其视为一行代码而不是三行独立的代码。首先使用 `groupby()` 函数按不同的
    `Contract` 值分组。你想要查看 `Churn` 的关系，所以选择 `Churn` 列，然后应用 `value_counts` 函数。注意附加的 `normalize=True`
    参数，它将每对值的计数替换为百分比而不是数字。这样做的好处是，你可以看到在每个 `Contract` 值中，多少客户流失相比于没有流失的客户，而不是比较不均匀组中的计数。在使用内置的
    Pandas 绘图功能之前，使用 `unstack()` 函数将表格格式化为更易读的格式。在这种情况下，[图 7-5](#visualization_of_the_proportion_of_cust)
    使用堆叠条形图快速比较不同 `Contract` 值。
- en: You see that there is a higher percentage of customer churn for month-to-month
    contracts versus one-year or two-year contracts. From the visualization, you see
    that more than 40% of customers on a month-to-month contract canceled their service
    versus around 15% on a one-year contract and less than 5% on a two-year contract.
    This means that the contract type almost certainly will be a useful feature moving
    forward.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到按月付合同的客户流失率比按年付或两年付合同的要高。从可视化中可以看出，超过 40% 的按月付合同客户取消了他们的服务，而按年付约为 15%，按两年付则不到
    5%。这意味着合同类型几乎肯定会成为未来的一个有用特征。
- en: '![Visualization of the proportion of customers that left the telco versus those
    that did not based on the contract type](assets/lcai_0705.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![基于合同类型显示离开电信公司客户比例的可视化](assets/lcai_0705.png)'
- en: Figure 7-5\. Visualization of the proportion of customers that left the telco
    versus those that did not based on the contract type.
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. 基于合同类型，显示离开电信公司客户比例的可视化。
- en: As an exercise, go through this sort of analysis for your other categorical
    features. Note which features have different percentages of churn across the different
    values and those that are more or less the same. This will be helpful in choosing
    features later.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，对你的其他分类特征进行类似的分析。注意哪些特征在不同值之间有不同的流失百分比，哪些则相对更为相似。这将有助于稍后选择特征。
- en: 'When executing similar blocks of code in Python multiple times, it can be more
    efficient to create a function to execute instead. For example, you can create
    a function to create the distribution chart above by using the following code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 Python 中多次执行类似的代码块时，创建一个函数来执行可能更有效。例如，你可以通过以下代码创建一个函数来生成上面的分布图：
- en: '[PRE13]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`def` is the keyword in Python for defining a function, the function name is
    `plot_cat_feature_dist`, and `feature_name` is the input variable. This way, `plot_cat_feature_dist(''Contract'')`
    will generate the same graph as in [Figure 7-5](#visualization_of_the_proportion_of_cust).
    You can then use this function for all of your categorical variables instead.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`def` 是 Python 中定义函数的关键字，函数名为 `plot_cat_feature_dist`，而 `feature_name` 则是输入变量。这样，`plot_cat_feature_dist(''Contract'')`
    将生成与 [图 7-5](#visualization_of_the_proportion_of_cust) 相同的图表。你随后可以对所有的分类变量使用此函数。'
- en: 'Here are some observations you should have made while exploring your categorical
    features:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索分类特征时，你应该注意到以下一些观察结果：
- en: The churn rate was about double for senior citizens versus non-senior citizens.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对老年人与非老年人，流失率大约是两倍。
- en: The values for the `gender`, `StreamingTV`, and `StreamingMovies` features do
    not seem to make a difference in the churn rate.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gender`、`StreamingTV` 和 `StreamingMovies` 特征的值似乎对流失率没有影响。'
- en: The larger the household, the lower the churn rate. In other words, having a
    partner or dependents in the household lowers the churn rate.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 家庭规模越大，流失率越低。换句话说，家庭中有伴侣或者依赖者会降低流失率。
- en: For those with a phone line, having multiple lines increases the churn rate.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于拥有电话线的用户来说，拥有多条电话线会增加流失率。
- en: The `InternetService` feature affects churn rate. Fiber optic internet service
    has a much higher churn rate than DSL. Those without internet service have the
    lowest churn rate.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InternetService`特征会影响流失率。光纤互联网服务的流失率远高于DSL。没有互联网服务的用户流失率最低。'
- en: Internet add-ons (like `OnlineSecurity` and `DeviceProtection`) decrease the
    churn rate.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互联网附加服务（如`OnlineSecurity`和`DeviceProtection`）会降低流失率。
- en: '`PaperlessBilling` increases the churn rate. Most values of `PaymentMethod`
    are the same except for `Electronic Check`, which has a much higher churn rate
    than the others.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PaperlessBilling`会增加流失率。`PaymentMethod`的大多数值相同，除了`Electronic Check`，其流失率远高于其他方式。'
- en: Did you notice anything else? Be sure to make a note of these observations for
    later.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有注意到其他什么？记得做好这些观察以备后用。
- en: Exploring interactions between numeric and categorical columns
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索数值和分类列之间的交互作用。
- en: Before moving on to finally thinking through how you will transform your features,
    you should also explore the relationship between the numeric features and the
    label. Remember that `SeniorCitizen` is really a categorical column since the
    two values represent two discrete classes. The numeric columns that are left are
    `tenure`, `MonthlyCharges`, and `TotalCharges`. The columns would have a simple
    relationship if a customer had paid the same amount every month. That is, `tenure
    × MonthlyCharges = TotalCharges`. You saw this explicitly in the case where `tenure`
    was 0 before.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在最终思考如何转换特征之前，你还应该探索数值特征与标签之间的关系。记住`SeniorCitizen`实际上是一个分类列，因为两个值表示两个离散类别。剩下的数值列是`tenure`、`MonthlyCharges`和`TotalCharges`。如果客户每个月支付相同金额，这些列将有一个简单的关系。也就是说，`tenure
    × MonthlyCharges = TotalCharges`。在`tenure`为0的情况下，你显式地看到了这一点。
- en: How often is this true? Intuitively, and maybe from experience, the monthly
    charges tend to change over time. This can be due to promotional pricing ending,
    but also due to things like changing the services you are paying for. You can
    check this intuition using Pandas functions. Write the following code into a new
    cell and execute that cell to see the summary statistics of a new column comparing
    `tenure × MonthlyCharges` and `TotalCharges:`
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这是多么频繁的情况呢？直觉上，也许是从经验中得知，月费用通常随时间变化。这可能是由于促销价格的结束，也可能是由于例如改变你支付的服务等原因。你可以使用Pandas函数来验证这种直觉。将以下代码写入新的单元格并执行该单元格以查看比较`tenure
    × MonthlyCharges`和`TotalCharges`的新列的摘要统计信息：
- en: '[PRE14]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that you are creating two new columns in your DataFrame `df_2`. The `AvgMonthlyCharge`
    column captures the average monthly charge over the customer’s tenure, and the
    `DiffCharges` column captures the difference between the average monthly charge
    and the current monthly charge. The results are shown below:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在你的DataFrame `df_2`中创建了两个新列。`AvgMonthlyCharge`列记录客户在使用期间的平均月费用，`DiffCharges`列记录平均月费用与当前月费用之间的差异。结果如下所示：
- en: '[PRE15]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'A few observations you should make from these summary statistics: first, note
    that the count is 11 lower than the total number of rows. Why is this? Recall
    that you have 11 rows with zero `tenure`. In Pandas, if you divide by zero, the
    value is recorded as `NaN` instead of throwing an error. Otherwise, note that
    the distribution seems fairly symmetric. The mean value is almost zero, the median
    value is 0, and min and max values are close to being opposites of one another.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些摘要统计中，你应该注意几点：首先，注意到计数比总行数少了11行。为什么呢？回想一下，你有11行的`tenure`值为零。在Pandas中，如果除以零，该值记录为`NaN`而不是抛出错误。另外，注意到分布似乎相当对称。平均值几乎为零，中位数为0，最小值和最大值几乎互为相反数。
- en: 'One way to remove the `NaN` values is to use the `replace()` method instead
    for the undefined values. Use the following code in a new cell to perform this
    task:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 删除`NaN`值的一种方法是使用`replace()`方法替换未定义的值。在新的单元格中使用以下代码执行此任务：
- en: '[PRE16]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The choice of replacing a null value with a zero value is an example of an imputation
    strategy. The process of *imputation* is the process of replacing unknown values
    with substituted values that are reasonable for the problem at hand. Because you
    want to look at the difference between monthly charges and average monthly charges,
    saying that “there is no difference” is a reasonable approach to avoid having
    to throw out possibly useful data. Without imputation here, you would lose all
    rows with zero `tenure`, so your model would not be able to accurately predict
    these cases. With large enough datasets, if the missing data does not focus on
    a single group, often a strategy of omitting that data will be taken. This was
    the approach taken in [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 将空值替换为零值的选择是填充策略的一个示例。*填充* 的过程是用在问题处理上合理的替代值来替换未知值。因为你希望查看月费和平均月费之间的差异，说“没有差异”是一个合理的方法，以避免必须丢弃可能有用的数据。如果不进行填充，你将丢失所有`tenure`为零的行，因此你的模型将无法准确预测这些情况。对于足够大的数据集，如果缺失数据并不集中在一个单一组中，则通常会采取删除数据的策略。这是[第 6 章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)中采取的方法。
- en: 'How does the value of the `DiffCharges` relate to the `Churn` column? The method
    you used to understand the relationship between categorical columns does not quite
    work here since `DiffCharges` is numeric. But you could bucketize the values of
    the `DiffCharges` column and use the approach that was used before. The idea of
    *bucketization* is to break a numeric column into value ranges called *buckets*.
    The numeric feature becomes a categorical feature by asking, “Which bucket does
    this value belong to?” In Pandas, you use the `cut()` function to define buckets
    for a numeric column. You can either provide the number of buckets to use, or
    specify a list of cutoff points. To bucketize the `DiffCharges` column and explore
    its effect on `Churn`, type the following code into a new cell and execute that
    cell:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`DiffCharges`的值如何与`Churn`列相关联？你用于理解分类列之间关系的方法在这里不太适用，因为`DiffCharges`是数值型的。但是你可以将`DiffCharges`的值分桶化，并使用之前使用的方法。*分桶*
    的思想是将数值列分成称为*桶*的值范围。在 Pandas 中，你可以使用`cut()`函数为数值列定义桶。你可以提供要使用的桶的数量，或指定一个切分点列表。要对`DiffCharges`列进行分桶并探索其对`Churn`的影响，请将以下代码键入到新的单元格中并执行该单元格：'
- en: '[PRE17]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The resulting graph (in [Figure 7-6](#the_churn_rate_for_each_bucket_of_the_d))
    shows that the larger the difference (either positive or negative) between `MonthlyCharges`
    and `AvgMonthlyCharge`, the higher the churn rate for the corresponding range
    of values. As an exercise, explore with different numbers of bins and see what
    patterns you notice.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图（见[图 7-6](#the_churn_rate_for_each_bucket_of_the_d)）显示，`MonthlyCharges`与`AvgMonthlyCharge`之间的差异（无论是正还是负）越大，相应数值范围内的流失率就越高。作为练习，尝试不同数量的箱体，并观察你能注意到的模式。
- en: '![The churn rate for each bucket of the DiffCharges column](assets/lcai_0706.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![DiffCharges列每个桶的流失率](assets/lcai_0706.png)'
- en: Figure 7-6\. The churn rate for each bucket of the `DiffCharges` column.
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6. `DiffCharges`列每个桶的流失率。
- en: Notice that the churn rate for each bucket does not follow a nice linear trend.
    That is, the churn rate goes down before it goes back up later, depending on how
    far away the bucket is from the center. In cases such as this, treating bucket
    membership as a categorical variable can be more advantageous for ML than keeping
    the feature as a numeric feature.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 每个桶的流失率并不会呈现出一个漂亮的线性趋势。也就是说，流失率在桶离中心越远时先下降，然后再上升。在这种情况下，将桶成员视为一个分类变量对机器学习而言可能比保持其作为数值特征更有优势。
- en: 'You can also explore the numeric features without any manipulation. For example,
    let’s explore the relationship between `MonthlyCharges` and `Churn` by using the
    following code. The relationship is visualized in [Figure 7-7](#customer_churn_per_bucket_for_monthlyc):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以探索数值特征而不进行任何操作。例如，我们可以使用以下代码探索`MonthlyCharges`和`Churn`之间的关系。关系可在[图 7-7](#customer_churn_per_bucket_for_monthlyc)中可视化：
- en: '[PRE18]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Customer churn per bucket for MonthlyCharges. The churn rate increases as
    the monthly charges increase, with the highest churn rate being for charges between
    85.25 and 118.75.](assets/lcai_0707.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![每月费用桶的客户流失率。随着每月费用的增加，流失率增加，费用在85.25到118.75之间的流失率最高。](assets/lcai_0707.png)'
- en: Figure 7-7\. Customer churn per bucket for `MonthlyCharges`. The churn rate
    increases as the monthly charges increase, with the highest churn rate being for
    charges between 85.25 and 118.75.
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7\. 对于 `MonthlyCharges` 每个桶的客户流失率。随着每月费用的增加，流失率也在增加，85.25 到 118.75 之间的费用具有最高的流失率。
- en: In [Figure 7-7](#customer_churn_per_bucket_for_monthlyc), you can see that the
    churn rate tends to increase as the `MonthlyCharges` value increases. This implies
    that the `MonthlyCharges` column will be useful for predicting churn.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 7-7](#customer_churn_per_bucket_for_monthlyc) 中，您可以看到随着 `MonthlyCharges`
    值的增加，流失率倾向于增加。这意味着 `MonthlyCharges` 列对预测流失将是有用的。
- en: Warning
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Finding the right number of buckets to use for numeric columns can be tricky.
    Too few buckets and you may miss important patterns, but too many buckets and
    the patterns may become very noisy and even misleading. [Figure 7-8](#customer_churn_per_bucket_for_monthlych)
    shows an example for when too many buckets leads to a noisy pattern where it is
    hard to gain insights. Also note that the range of values for each bucket is fairly
    small, so you are capturing a smaller number of customers per bucket.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 找到要用于数值列的正确桶数量可能有些棘手。桶太少可能会错过重要的模式，但桶太多可能会导致模式变得非常嘈杂甚至误导性。[图 7-8](#customer_churn_per_bucket_for_monthlych)
    展示了一个例子，即当桶数过多时，会导致噪声模式，难以获取见解。同时注意，每个桶的值范围相当小，因此每个桶中捕获的客户数量较少。
- en: '![Customer churn per bucket for MonthlyCharges with too many buckets. The pattern
    is lost in the noise, and it is hard to understand the relationship from this
    visualization.](assets/lcai_0708.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![每月费用的客户流失率，桶太多。模式在噪声中丢失，难以从这种可视化中理解关系。](assets/lcai_0708.png)'
- en: Figure 7-8\. Customer churn per bucket for `MonthlyCharges` with too many buckets.
    The pattern is lost in the noise, and it is hard to understand the relationship
    from this visualization.
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-8\. 对于 `MonthlyCharges` 每个桶的客户流失率，桶太多。模式在噪声中丢失，难以从这种可视化中理解关系。
- en: As an exercise, perform this analysis yourself for the `tenure` and `TotalCharges`
    columns. You should see that as `tenure` and `TotalCharges` increase, the churn
    rate decreases. It makes sense that both columns have a similar relationship with
    churn since longer tenures should lead to a larger amount of charges paid over
    the tenure. Using code from previous chapters, check the correlation between these
    two features to see that they are indeed highly correlated, with a correlation
    of about 0.82.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个练习，自行分析 `tenure` 和 `TotalCharges` 列。您应该会看到随着 `tenure` 和 `TotalCharges` 的增加，流失率减少。由于较长的任期应该会导致在任期内支付的费用增加，所以这两列与流失之间有类似的关系是合理的。使用之前章节的代码，检查这两个特征之间的相关性，确保它们确实高度相关，相关系数约为
    0.82。
- en: Transforming Features Using Pandas and Scikit-Learn
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Pandas 和 Scikit-Learn 转换特征
- en: At this point, you have explored the different columns in your dataset, how
    they interact with each other, and specifically how they interact with the label.
    You will now prepare that data for use in custom models. First you will select
    the columns to use for training your ML model. After that, you will transform
    those features into forms more amenable to training. Recall that your features
    must be numeric with meaningful magnitudes. You will take this into account when
    selecting the features for this project.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，您已经探索了数据集中不同的列，它们之间的相互作用，特别是它们与标签的交互作用。现在，您将准备好这些数据以供自定义模型使用。首先，您将选择用于训练机器学习模型的列。之后，您将转换这些特征，使其更适合训练。请记住，您的特征必须是具有有意义的大小的数值。在选择此项目的特征时，您将考虑到这一点。
- en: Feature selection
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征选择
- en: The previous section explored the interaction between the different features
    in the customer churn dataset with the customer churn column `Churn`. You saw
    that a few features were either not predictive—that is, the different values did
    not affect the churn rate—or were redundant with respect to other features. You
    should make a copy of your DataFrame `df_2` and then remove the columns you will
    not be using for training the model. Why make a copy? If you remove the columns
    from `df_2`, then you may have to go back through the code for creating that DataFrame
    to be able to access that data again. Though not explicitly stated, this is why
    the DataFrame `df_2` was created instead of altering the original DataFrame `df_raw`.
    By removing the columns in a copy of the DataFrame, you leave the original data
    accessible in case you have to access it again.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节探索了客户流失数据集中不同特征与客户流失列`Churn`之间的交互作用。您发现一些特征要么不具有预测性——即不同的值不影响流失率——要么与其他特征冗余。您应该复制您的DataFrame
    `df_2`，然后移除您不打算用于训练模型的列。为什么要复制？如果从`df_2`中移除列，那么您可能需要重新查看创建该DataFrame的代码，以便再次访问该数据。虽然没有明确说明，但这就是为什么DataFrame
    `df_2`被创建而不是修改原始DataFrame `df_raw`的原因。通过在DataFrame的副本中删除列，您可以保留原始数据以便在需要时再次访问。
- en: You discovered that the `gender`, `StreamingTV`, and `StreamingMovies` columns
    were not predictive of the label `Churn` in the previous section. Additionally,
    you found that the `PhoneLine` feature was redundant and included in the `MultipleLines`
    feature, so you will want to remove that as well to avoid problems related to
    collinearity. In [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg),
    you learned that collinearity occurs when there are high correlations among predictor
    variables, leading to unreliable and unstable estimates of regression coefficients.
    These problems are magnified when using a linear model above more complex model
    types. One approach to combat this is to only use one column from a set of collinear
    columns.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，您发现`gender`、`StreamingTV`和`StreamingMovies`列对`Churn`标签没有预测性。此外，您还发现`PhoneLine`特征是冗余的，并包含在`MultipleLines`特征中，因此您也希望将其移除，以避免与共线性相关的问题。在[第6章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)中，您了解到当预测变量之间存在高相关性时会发生共线性，导致回归系数的估计不可靠和不稳定。在使用复杂模型类型以上时，这些问题会被放大。对抗这些问题的一种方法是只使用一组共线性列中的一列。
- en: 'The easiest way to drop columns in a Pandas DataFrame is to use the `drop()`
    function. Type the following code into a new cell and execute it to make a copy
    of the Pandas DataFrame and to drop the columns you no longer need:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pandas DataFrame中删除列的最简单方法是使用`drop()`函数。将以下代码输入到新单元格中并执行，以创建Pandas DataFrame的副本并丢弃您不再需要的列：
- en: '[PRE19]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The line `df_3.columns` is included to check to see which columns remain. The
    exact output will differ based on your previous explorations, but as an example
    you may see an output like the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查哪些列保留，包含了`df_3.columns`这一行。具体输出会因您之前的探索而异，但例如，您可能看到如下输出：
- en: '[PRE20]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For the DataFrame columns shown here, `AvgMonthlyCharge`, `DiffCharges`, `DiffBuckets`,
    `MonthlyBuckets`, `TotalBuckets`, and `TenureBuckets` were added. You saw that
    the `DiffBuckets` feature would be a helpful feature and that the `tenure` feature
    was highly related to the `TotalCharges` feature. To prevent problems in terms
    of collinearity, remove the `TotalCharges` feature and all of the additional added
    features except for `DiffBuckets`. The code needed to do this may differ from
    the following code, depending on the exploration you performed:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这里显示的DataFrame列，已添加了`AvgMonthlyCharge`、`DiffCharges`、`DiffBuckets`、`MonthlyBuckets`、`TotalBuckets`和`TenureBuckets`。您发现`DiffBuckets`特征将是一个有用的特征，并且`tenure`特征与`TotalCharges`特征高度相关。为了避免共线性问题，删除`TotalCharges`特征以及除了`DiffBuckets`之外的所有其他添加的特征。执行此操作所需的代码可能与下面的代码不同，具体取决于您执行的探索：
- en: '[PRE21]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, what about the `customerID` column? This column is too granular to
    be of any use in a predictive model. Why is this? Remember that the `customerID`
    column uniquely identifies every row. You risk the model learning to associate
    the value of this feature to the value of `Churn` in a direct relationship, especially
    given the transformations that will follow. This is great for your training dataset,
    but once your model sees a new value for `customerID` for the first time, it will
    not be able to use that value in a meaningful way. For that reason, it is best
    to drop this column for training your model. As an exercise, write the code to
    drop the `customerID` column into a new cell and execute that cell to drop the
    column. Here’s the solution code, but do your best to complete this task without
    looking at it:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`customerID` 列怎么样？此列过于精细化，对预测模型没有任何用处。为什么呢？请记住，`customerID` 列唯一标识每一行。您会冒险使模型学习将此特征的值与
    `Churn` 的值直接关联，特别是在接下来的转换之后。这对于您的训练数据集很好，但一旦您的模型第一次看到 `customerID` 的新值，它将无法以有意义的方式使用该值。因此，在训练模型时最好删除此列。作为练习，编写代码将
    `customerID` 列删除到一个新的单元格，并执行该单元格以删除该列。这里是解决方案代码，但请尽量完成此任务而不查看它：
- en: '[PRE22]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the end, you end up with 15 feature columns and 1 label, `Churn`. The output
    of the final `df_3.dtypes` line is included here for reference:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您将得到 15 个特征列和 1 个标签列 `Churn`。最后一行 `df_3.dtypes` 的输出在此提供作为参考：
- en: '[PRE23]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`DiffBuckets` is a `category` column, rather than an `object`. This is because
    the bucketization process includes additional information, the intervals representing
    the buckets.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`DiffBuckets` 是一个 `category` 列，而不是一个 `object` 列。这是因为桶化过程包括附加信息，即表示桶的间隔。'
- en: Encoding categorical features using scikit-learn
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 对分类特征进行编码
- en: Before beginning the training process, you need to encode your categorical features
    as numeric features. `SeniorCitizen` is a great example of how this could be done.
    Instead of `Yes` and `No` values, the values are encoded as 1 or 0 respectively.
    In essence, this is what you will be doing for your features moving forward using
    scikit-learn.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练过程之前，您需要将分类特征编码为数值特征。`SeniorCitizen` 就是一个很好的例子。不再使用 `Yes` 和 `No` 值，而是分别编码为
    1 或 0。实质上，这就是您将来将要为您的特征执行的操作，使用 scikit-learn。
- en: 'First, note that many of your categorical features are binary features. `Partner`,
    `Dependents`, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`,
    and `PaperlessBilling` are all binary features. Note that for `OnlineSecurity`,
    `OnlineBackup`, `DeviceProtection`, and `TechSupport`, this is not strictly true,
    but the `No internet service` value is already captured by `InternetService`.
    Before encoding your features, replace all instances of `No internet service`
    values in different columns with the value `No`. You can do this by using the
    following code:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请注意，许多分类特征都是二进制特征。`Partner`、`Dependents`、`OnlineSecurity`、`OnlineBackup`、`DeviceProtection`、`TechSupport`
    和 `PaperlessBilling` 都是二进制特征。请注意，对于 `OnlineSecurity`、`OnlineBackup`、`DeviceProtection`
    和 `TechSupport`，这并不严格适用，但 `No internet service` 的值已经由 `InternetService` 捕获。在编码您的特征之前，请使用以下代码将不同列中所有
    `No internet service` 值替换为 `No`：
- en: '[PRE24]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `nunique()` method computes the number of unique values per column. You
    should see in the output for this cell there are two unique values for the `OnlineSecurity`,
    `OnlineBackup`, `DeviceProtection`, and `TechSupport` corresponding to `No` and
    `Yes`. You will keep this DataFrame, `df_prep`, as one that you can come back
    to later for any additional feature engineering.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`nunique()` 方法计算每列的唯一值数量。您应该看到此单元格的输出中，`OnlineSecurity`、`OnlineBackup`、`DeviceProtection`
    和 `TechSupport` 列有两个唯一值，分别对应 `No` 和 `Yes`。您将保留此 DataFrame `df_prep`，以便稍后进行任何额外的特征工程。'
- en: 'Now you are ready to perform one-hot encoding. *One-hot encoding* is a process
    of transforming a categorical feature with independent values to a numeric representation.
    This representation is a list of integers, with one integer for each possible
    feature value. For example, the `InternetService` feature has three possible values:
    `No`, `DSL`, and `Fiber Optic`. The one-hot encoding of these values would be
    `[1,0,0]` , `[0,1,0]`, and `[0,0,1]` respectively. Another way to think of this
    is that we have created a new feature column for every feature value. That is,
    the first column asks, “Is the value of `InternetService` equal to `No`?” If so,
    the value is 1, and if not, the value is 0\. The other two columns correspond
    to the same question, but for values of `DSL` and `Fiber Optic` respectively.
    With this way of thinking of one-hot encoding, often a feature like `Partner`
    with only two values `No` and `Yes` will be encoded as 0 and 1 respectively instead
    of `[1,0]` and `[0,1]`.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经准备好执行独热编码了。*独热编码*是将具有独立值的分类特征转换为数字表示的过程。这个表示是一个整数列表，每个可能的特征值对应一个整数。例如，`InternetService`
    特征有三个可能的值：`No`、`DSL` 和 `Fiber Optic`。这些值的独热编码分别是 `[1,0,0]`、`[0,1,0]` 和 `[0,0,1]`。另一种思考方式是，我们为每个特征值创建了一个新的特征列。也就是说，第一列询问，“`InternetService`
    的值是否等于 `No`？”如果是，值为1，否则为0。其他两列分别对应相同的问题，但针对 `DSL` 和 `Fiber Optic` 的值。通过这种独热编码的思路，通常像
    `Partner` 这样只有两个值 `No` 和 `Yes` 的特征将被编码为0和1，而不是 `[1,0]` 和 `[0,1]`。
- en: 'Scikit-learn includes a preprocessing library with transformers specifically
    for this purpose for your features and labels. For transforming categorical features
    into one-hot encoded features, you will use the `OneHotEncoder` class in scikit-learn.
    The following code is an example of how to one-hot encode the categorical columns
    you are working with in this example:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 包含一个预处理库，专门为您的特征和标签提供转换器。要将分类特征转换为独热编码特征，您将使用 scikit-learn 中的 `OneHotEncoder`
    类。以下代码展示了如何在这个示例中使用 `OneHotEncoder` 对分类列进行独热编码：
- en: '[PRE25]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'It is worth understanding this code line-by-line before moving forward. First
    you import the `OneHotEncoder` class from scikit-learn via `from sklearn.preprocessing
    import OneHotEncoder`. Next you separate the columns into numeric and categorical
    columns. Since `SeniorCitizen` has already been encoded, you can simply include
    it in the numeric columns. After that, the next two lines of code split the DataFrame
    into two separate DataFrames: `X_num` for the numeric features and `X_cat` for
    the categorial features.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前理解这段代码值得一提。首先，通过 `from sklearn.preprocessing import OneHotEncoder` 从 scikit-learn
    中导入 `OneHotEncoder` 类。接下来，将列分为数值和分类列。由于 `SeniorCitizen` 已经被编码，您可以简单地将其包含在数值列中。在此之后，代码的下两行将
    DataFrame 拆分为两个单独的 DataFrame：`X_num` 用于数值特征，`X_cat` 用于分类特征。
- en: Finally, you are ready to use scikit-learn’s `OneHotEncoder`. First you create
    the one-hot encoder via the line `ohe = OneHotEncoder(drop='if_binary')`. The
    argument `drop='if_binary'` will replace a binary feature value with either 0
    or 1 rather than returning the full one-hot encoding.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以开始使用 scikit-learn 的`OneHotEncoder`。首先，通过以下代码创建一个独热编码器：`ohe = OneHotEncoder(drop='if_binary')`。参数`drop='if_binary'`将会把二元特征值替换为0或1，而不是返回完整的独热编码。
- en: 'The final line is where the actual transformation occurs. The `fit_transform`
    function does two different things. The *fit* part of `fit_transform` refers to
    the `OneHotEncoder` learning the different values for the different features and
    the assignment of the one-hot encoded values. This will be important since you
    may want to reverse the process at times and go back to the original values. For
    example, after making a prediction, you want to see what payment method the customer
    was using. You can use the `inverse_transform()` method of `OneHotEncoder` to
    transform the numeric input after encoding back to the original input. For example,
    consider the following two lines of code run in separate cells:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行是实际进行转换的地方。`fit_transform` 函数执行两个不同的操作。`fit_transform` 中的 *fit* 部分指的是 `OneHotEncoder`
    学习不同特征的不同值及其独热编码值的分配。这很重要，因为有时您可能需要反向过程，回到原始值。例如，在进行预测后，您想知道客户使用的支付方式。您可以使用 `OneHotEncoder`
    的 `inverse_transform()` 方法将编码后的数字输入转换回原始输入。例如，考虑在不同单元格中运行以下两行代码：
- en: '[PRE26]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The first line returns this output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行返回以下输出：
- en: '[PRE27]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The second line returns this output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 第二行返回以下输出：
- en: '[PRE28]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Once the `OneHotEncoder` has been fit to the data, you can move back and forth
    between the original values and the encoded values using `transform()` and `inverse_transform()`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`OneHotEncoder`已经适合数据，您可以使用`transform()`和`inverse_transform()`在原始值和编码值之间来回切换。
- en: 'Finally, you need to combine the numeric features and the encoded categorical
    features back into a single object. The one-hot encoded categorical features are
    returned as a NumPy array, so you will need to convert the Pandas DataFrame to
    a NumPy array and concatenate the arrays into a single array. Additionally, you
    will need to create a NumPy array for the label `Churn`. To do this, execute the
    following code in a new cell:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您需要将数值特征和编码的分类特征组合回一个单一对象中。一次热编码的分类特征返回为 NumPy 数组，因此您需要将 Pandas DataFrame
    转换为 NumPy 数组，并将数组连接成一个单一数组。此外，您还需要为标签`Churn`创建一个 NumPy 数组。为此，请在新的单元格中执行以下代码：
- en: '[PRE29]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `concatenate()` function in NumPy takes two arrays and returns a single
    array. `X_num` is a Pandas DataFrame, but the actual values of the Pandas DataFrame
    are stored as a NumPy array. You can access this array by looking at the `values`
    property of the DataFrame. `X_cat_trans` is a special type of NumPy array called
    a sparse array. *Sparse* arrays, arrays where most of the entries are 0, can be
    nice since there are a lot of clever optimizations that can be used to store them
    more efficiently. However, you need the actual corresponding array. You can use
    the `toarray()` method to access that. Finally, you want to concatenate the DataFrames
    “horizontally,” where you are combining the columns side by side, so you need
    to specify the additional argument `axis=1`. Similarly, `axis=0` would correspond
    to stacking the arrays “vertically,” that is, appending one list of rows to the
     other.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 中的`concatenate()`函数接受两个数组并返回一个单一的数组。`X_num`是一个 Pandas DataFrame，但是 Pandas
    DataFrame 的实际值存储为 NumPy 数组。您可以通过查看 DataFrame 的`values`属性来访问这个数组。`X_cat_trans`是一种特殊类型的
    NumPy 数组，称为稀疏数组。*稀疏*数组，即大多数条目为0，可能很好，因为有许多聪明的优化可以用来更有效地存储它们。但是，您需要实际的相应数组。您可以使用`toarray()`方法来访问它。最后，您希望“水平”连接
    DataFrame，即将列并排组合在一起，因此您需要指定额外的参数`axis=1`。类似地，`axis=0`对应于“垂直”堆叠数组，即将一行列表附加到另一个上。
- en: Generalization and data splitting
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 泛化和数据分割
- en: After all the work preparing your dataset, you are finally ready to start training
    your model, correct? Not quite. You need to perform the training-test data split
    to ensure that you can properly evaluate your model.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好数据集之后，您终于可以开始训练模型了，对吗？不完全是。您需要进行训练-测试数据分割，以确保能够正确评估您的模型。
- en: Scikit-learn has a nice helper function for doing this, called `train_test_split`,
    in the `model_selection` module. Though splitting your dataset for training and
    evaluation is something that you have to manage yourself in scikit-learn and other
    custom training frameworks, most (if not all) frameworks have tools in place to
    make the process easier.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了一个很好的辅助函数来完成这个任务，称为`train_test_split`，在`model_selection`模块中。尽管在
    scikit-learn 和其他自定义训练框架中，分割数据集用于训练和评估是你自己需要管理的事情，但大多数（如果不是全部）框架都提供了工具来简化这一过程。
- en: 'To split your dataset into training and test datasets, execute the following
    code in a new cell:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据集分割为训练集和测试集，请在新的单元格中执行以下代码：
- en: '[PRE30]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The first line imports the `train_test_split` function from the `model_selection`
    library in scikit-learn. The second line is where the splitting occurs. `train_test_split`
    takes in a list of arrays that you are splitting (here, your `X` and `y`) and
    the `test_size` to specify the size of the training dataset as a percentage. You
    can optionally also provide a `random_state` so anyone else who executes this
    code will get the same rows in the training and test datasets. Finally, you can
    see the final size of the training dataset with `X_train.shape`. This is an array
    of shape `(5634, 29)`. That is, there are 5,634 examples in the training dataset
    with 29 features after one-hot encoding. This means that the test dataset has
    1,409 examples.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行从 scikit-learn 的`model_selection`库导入`train_test_split`函数。第二行是分割发生的地方。`train_test_split`接受您要分割的数组列表（这里是您的`X`和`y`）和`test_size`以指定训练数据集的大小作为百分比。您还可以提供一个`random_state`，以便执行此代码的任何其他人得到相同的训练和测试数据集行。最后，您可以通过`X_train.shape`看到训练数据集的最终大小。这是一个形状为`(5634,
    29)`的数组。也就是说，经过一次热编码后，训练数据集中有 5,634 个示例和 29 个特征。这意味着测试数据集有 1,409 个示例。
- en: Building a Logistic Regression Model Using Scikit-Learn
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn构建逻辑回归模型
- en: With prepared training and test datasets in hand, you are ready to begin training
    your ML model. This section covers the basics of the model type you will be training,
    logistic regression, and how to begin training models in scikit-learn. After that,
    you will learn about different ways to evaluate and improve your classification
    model. Finally, you will be introduced to *pipelines* in scikit-learn, a way to
    consolidate the different transformations you are performing on your dataset and
    the training algorithm you want to use.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好训练和测试数据集后，你就可以开始训练你的机器学习模型了。本节介绍了你将要训练的模型类型——逻辑回归，以及如何在scikit-learn中开始训练模型。接下来，你将学习不同的方法来评估和改进你的分类模型。最后，你将了解scikit-learn中的*pipelines*，这是一种将你在数据集上执行的不同转换和你想要使用的训练算法整合在一起的方式。
- en: Logistic Regression
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: The goal of a logistic regression model is to predict membership in one of two
    discrete classes. For the sake of simplicity, denote these two classes as <math><mrow><mi>y</mi>
    <mo>=</mo> <mn>1</mn></mrow></math> and <math><mrow><mi>y</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    . You will often think of one of the classes as the “positive” class and the other
    as the “negative” class. Like linear regression, the inputs are a list of numeric
    values, but the output is the predicted probability of the positive class given
    the list of features <math><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>
    . You may see this probability denoted as <math><mrow><mi>P</mi> <mo>(</mo> <mi>y</mi>
    <mo>=</mo> <mn>1</mn> <mo>|</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow></math> . In the case that <math><mrow><mi>y</mi> <mo>=</mo>
    <mn>1</mn></mrow></math> , you want this probability to be as close to 1 as possible,
    whereas for <math><mrow><mi>y</mi> <mo>=</mo> <mn>0</mn></mrow></math> , you want
    this probability to be as close to 0 as possible.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的目标是预测属于两个离散类别中的一个的成员资格。为了简单起见，将这两个类别分别表示为<math><mrow><mi>y</mi> <mo>=</mo>
    <mn>1</mn></mrow></math>和<math><mrow><mi>y</mi> <mo>=</mo> <mn>0</mn></mrow></math>。你经常会把其中一个类别视为“正”类，另一个类别视为“负”类。像线性回归一样，输入是一组数值，但输出是给定特征列表<math><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math>时正类的预测概率。<math><mrow><mi>P</mi>
    <mo>(</mo> <mi>y</mi> <mo>=</mo> <mn>1</mn> <mo>|</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math>。在<math><mrow><mi>y</mi> <mo>=</mo>
    <mn>1</mn></mrow></math>的情况下，你希望这个概率尽可能接近1，而在<math><mrow><mi>y</mi> <mo>=</mo>
    <mn>0</mn></mrow></math>的情况下，你希望这个概率尽可能接近0。
- en: 'How is this probability calculated? Recall that for linear regression you used
    the model <math><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>w</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>w</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo>
    <mo>+</mo> <msub><mi>w</mi> <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>
    for some weights <math><mrow><msub><mi>w</mi> <mn>0</mn></msub> <mo>,</mo> <mo>.</mo>
    <mo>.</mo> <mo>.</mo> <mo>,</mo> <msub><mi>w</mi> <mi>n</mi></msub></mrow></math>
    . For logistic regression, the equation is similar:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如何计算这个概率？回想一下，对于线性回归，你使用了模型<math><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>w</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>w</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>.</mo> <mo>.</mo>
    <mo>.</mo> <mo>+</mo> <msub><mi>w</mi> <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>，其中<math><mrow><msub><mi>w</mi>
    <mn>0</mn></msub> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>,</mo> <msub><mi>w</mi>
    <mi>n</mi></msub></mrow></math>是一些权重。对于逻辑回归，方程式类似：
- en: <math><mrow><mi>g</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mo form="prefix">exp</mo><mo>(</mo><mo>-</mo><mi>f</mi><mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>g</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mo form="prefix">exp</mo><mo>(</mo><mo>-</mo><mi>f</mi><mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow><mo>)</mo></mrow></mfrac></mrow></math>
- en: 'At first glance, this formula may seem scary, but it is worth parsing. <math><mrow><mi>g</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> is the composition of two functions:
    the so-called *logit <math><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>*
    and the *sigmoid (or logistic) function*:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这个公式可能看起来很可怕，但值得解析。<math><mrow><mi>g</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>是两个函数的组合：所谓的*logit<math><mrow><mi>f</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>*和*sigmoid（或logistic）函数*：
- en: <math><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mo form="prefix">exp</mo><mo>(</mo><mo>-</mo><mi>t</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mo form="prefix">exp</mo><mo>(</mo><mo>-</mo><mi>t</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: The sigmoid function (and its variations) shows up in many different fields,
    including ecology, chemistry, economics, statistics, and of course ML. The sigmoid
    function has some nice properties that make it appealing for classification models.
    First, its values range between 0 and 1, allowing for the outputs to be interpreted
    as a probability. In technical terms, this is an example of a cumulative distribution
    function since the values are always increasing as the independent variable increases.
    Second, the *derivative* (rate of change at any given instant) of the function
    is easy to compute. This is important for the sake of gradient descent, making
    training such a model a very reasonable process. A graph of the logistic function
    can be seen in [Figure 7-9](#a_graph_of_the_logistic_functiondot_not).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数（及其变体）出现在许多不同的领域中，包括生态学、化学、经济学、统计学，当然还有机器学习。Sigmoid函数具有一些很好的特性，使其在分类模型中非常有吸引力。首先，其值范围在0到1之间，可以将输出解释为概率。在技术术语中，这是累积分布函数的一个例子，因为值随着独立变量的增加而始终增加。其次，函数的*导数*（在任意给定时刻的变化率）易于计算。这对于梯度下降至关重要，使得训练这样的模型成为一个非常合理的过程。逻辑函数的图形可以在[图 7-9](#a_graph_of_the_logistic_functiondot_not)中看到。
- en: '![A graph of the logistic function. Note the range of values is between 0 and
    1.](assets/lcai_0709.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑函数的图形。注意其值范围在0到1之间。](assets/lcai_0709.png)'
- en: Figure 7-9\. A graph of the logistic function. Note the range of values is between
    0 and 1.
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-9\. 逻辑函数的图形。注意其值范围在0到1之间。
- en: 'Recall, when training a linear regression model, that the goal you used was
    to minimize the mean squared error. For logistic regression, you use a different
    loss function known as the cross-entropy. The *cross-entropy* loss function is
    defined as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练线性回归模型时，请记住，您使用的目标是最小化均方误差。对于逻辑回归，您使用的是一种称为交叉熵的不同损失函数。*交叉熵*损失函数的定义如下：
- en: <math><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>g</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <mfrac><mn>1</mn> <mi>N</mi></mfrac> <mo>∑</mo> <mrow><mo>(</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mi>y</mi> <mo>+</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>g</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>y</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mi>L</mi> <mrow><mo>(</mo> <mi>g</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <mfrac><mn>1</mn> <mi>N</mi></mfrac> <mo>∑</mo> <mrow><mo>(</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mi>y</mi> <mo>+</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>g</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>y</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: Here, the sum is over all examples in the dataset <math><mi>D</mi></math> .
    The argument <math><mi>D</mi></math> is included here as a reminder that the cross-entropy
    depends on the dataset that is being used to compute it, just as much as the model
    that you are evaluating. Either <math><mi>y</mi></math> or <math><mrow><mn>1</mn>
    <mo>-</mo> <mi>y</mi></mrow></math> will be zero, so only one of the terms in
    the sum for each example will be nonzero. The corresponding term <math><mrow><mo
    form="prefix">log</mo> <mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></math> or <math><mrow><mo
    form="prefix">log</mo> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>g</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
    will be what contributes to the loss function. If the model is 100% confident
    about what ends up being the correct answer, then this term will be zero. If not,
    then the value contributed to the loss function increases exponentially as the
    confidence decreases.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，总和是在数据集 <math><mi>D</mi></math> 中的所有示例上。参数 <math><mi>D</mi></math> 在这里被包含，提醒交叉熵取决于用于计算的数据集，就像评估的模型一样。无论是
    <math><mi>y</mi></math> 还是 <math><mrow><mn>1</mn> <mo>-</mo> <mi>y</mi></mrow></math>
    都将为零，因此每个示例的总和中只有一个项是非零的。相应的术语 <math><mrow><mo form="prefix">log</mo> <mo>(</mo>
    <mi>g</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>)</mo></mrow></math> 或 <math><mrow><mo form="prefix">log</mo>
    <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>g</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></math> 将会对损失函数作出贡献。如果模型对最终的正确答案有100%的信心，那么这个项将为零。如果不是，则随着信心的减少，对损失函数的贡献值会呈指数增长。
- en: 'Here’s a concrete example: suppose that <math><mrow><mi>g</mi> <mo>(</mo> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math> = 0.6 and
    <math><mrow><mi>y</mi> <mo>=</mo> <mn>1</mn></mrow></math> . In other words, the
    model gives a 60% chance of the label being 1\. How does this contribute to the
    loss function? Well, for this single term, the only contribution is from <math><mrow><mo
    form="prefix">log</mo> <mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></math> . log(0.6) is about
    equal to –0.222\. In the computation of the loss function itself, the sign is
    ultimately swapped due to the minus sign out in front. However, if <math><mrow><mi>g</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    = 0.4 instead, then log(0.4) is about equal to –0.398\. The further the predicted
    probability <math><mrow><mi>g</mi> <mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> is away from 1 in this case, the larger
    the contribution to the loss function.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个具体的例子：假设 <math><mrow><mi>g</mi> <mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> = 0.6，而 <math><mrow><mi>y</mi> <mo>=</mo>
    <mn>1</mn></mrow></math> 。换句话说，模型给出标签为1的概率为60%。这如何影响损失函数？对于这个单项来说，唯一的贡献来自于 <math><mrow><mo
    form="prefix">log</mo> <mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></math> 。log(0.6) 大约等于 –0.222。在损失函数的计算中，由于前面的负号，符号最终被反转。然而，如果
    <math><mrow><mi>g</mi> <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow></math> = 0.4，则 log(0.4) 大约等于 –0.398。在这种情况下，预测概率 <math><mrow><mi>g</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    离1越远，对损失函数的贡献就越大。
- en: Note
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Cross-entropy first arose as a concept in an area of study known as *information
    theory*. Roughly speaking, cross-entropy measures the amount of information needed
    to represent an event when you assume one probability distribution, but the actual
    probability distribution is different. In the case of logistic regression, the
    assumed probability distribution is from the output of the model, and the actual
    distribution is given by our labels.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵最初是信息论领域的一个概念。粗略地说，交叉熵测量了在假设一个概率分布时表示事件所需的信息量，但实际的概率分布是不同的。在逻辑回归的情况下，假设的概率分布来自模型的输出，而实际的分布由我们的标签给出。
- en: Training and Evaluating a Model in Scikit-Learn
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中训练和评估模型
- en: 'You have prepared your data and have identified a model type, logistic regression,
    that you want to train to predict customer churn. Now you are ready to train your
    first model using scikit-learn. The process for creating and training a model
    is very straightforward in scikit-learn. First, you create the model of the type
    you want to train, and then you train the model. To build and train a logistic
    regression model, type the following code into a new cell and execute that cell:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经准备好了数据，并确定了一种模型类型——逻辑回归，你希望用它来预测客户流失。现在你已经准备好使用scikit-learn来训练你的第一个模型了。在scikit-learn中，创建和训练模型的过程非常简单。首先，你创建你想要训练的模型类型的模型，然后你训练这个模型。要构建和训练一个逻辑回归模型，将以下代码键入一个新单元格并执行该单元格：
- en: '[PRE31]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After executing the cell, you will likely see the following, or a similar,
    message:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 执行完这个单元格后，你可能会看到以下或类似的消息：
- en: '[PRE32]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: What went wrong? By default, scikit-learn will perform gradient descent to try
    to find the best weights for the model for a certain number of iterations. An
    *iteration* in this context means computing gradient descent over the entire dataset.
    The training process will terminate once the improvement in our loss function,
    cross-entropy, becomes very small (by default, <math><msup><mn>10</mn> <mrow><mo>-</mo><mn>4</mn></mrow></msup></math>
    ) between iterations. In your case, your model never hit that threshold. In practice,
    this means that there is still room for the model to improve by training for longer.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么问题？默认情况下，scikit-learn将执行梯度下降来尝试找到模型的最佳权重，用于一定数量的迭代。在这个上下文中，*迭代*意味着在整个数据集上计算梯度下降。一旦我们的损失函数——交叉熵——在迭代之间的改善变得非常小（默认情况下，<math><msup><mn>10</mn>
    <mrow><mo>-</mo><mn>4</mn></mrow></msup></math>），训练过程就会终止。在你的情况下，你的模型从未达到这个阈值。在实践中，这意味着模型仍有改进的空间，可以通过更长时间的训练来实现。
- en: What would cause such an issue, though? There are a few different reasons, but
    one of the most basic issues has to do with feature ranges. If the ranges of values
    for feature ranges differ greatly, gradient descent tends to take more iterations
    to converge to the best solution.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，造成这种问题的原因是什么呢？有几个不同的原因，但其中一个最基本的问题之一与特征范围有关。如果特征值的范围差异很大，梯度下降往往需要更多的迭代才能收敛到最佳解决方案。
- en: One way to think about this is to consider an analogy of rolling a ball down
    from the edge of a bowl. First, suppose that you have a bowl that is perfectly
    round, meaning the top is a perfect circle. If you place a ball on the edge of
    the bowl and give it a little nudge, it immediately rolls down to the bottom point
    of the bowl. What about if the bowl has more of an oval or oblong shape? If you
    give the ball a nudge, it may not roll straight toward the center, but oscillate
    back and forth along the way. In essence, this is exactly what is happening with
    gradient descent. The *curvature*, how far away the “bowl” is from being perfectly
    flat, affects the behavior of gradient descent. When the curvature is a constant—for
    example, when the bowl is perfectly round—then we have the nice behavior discussed
    previously. However, when the curvature is not constant, as in the situation where
    the feature values have different scales, we risk the wobbling behavior discussed
    before.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一种思考这个问题的方式是考虑一个滚球的类比。首先，假设你有一个完全圆形的碗，意味着顶部是一个完美的圆圈。如果你把一个球放在碗的边缘并给它一个小推动，它会立即滚到碗底的最低点。那么如果碗更像是椭圆或椭圆形呢？如果你给球一个推动，它可能不会直接朝向中心滚动，而是沿途来回振荡。本质上，这正是梯度下降的工作原理。*曲率*，也就是“碗”距离完全平坦的程度，影响了梯度下降的行为。当曲率是常数时——例如，当碗是完全圆形时——我们就有了之前讨论过的良好行为。然而，当曲率不是常数时，例如特征值具有不同的比例时，我们就会面临之前讨论过的摇摆行为。
- en: 'How do you address this issue? One approach is to simply train the model for
    more iterations. You can easily do this in scikit-learn by providing your own
    value for the optional `max_iter` argument. This is a fine approach for smaller
    datasets, but once you start working with larger and larger ones, this is no longer
    feasible. A better way of approaching this problem is to rescale the features
    to a standard range, say between 0 and 1\. In scikit-learn, you can use the `MinMaxScaler()`
    transformer to do just that. Type the following code into a new cell and execute
    it to rescale your training dataset and then train a new logistic regression model:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如何解决这个问题？一个方法是简单地增加模型的迭代次数。在 scikit-learn 中，你可以通过提供可选的 `max_iter` 参数来轻松实现这一点。对于较小的数据集，这是一个不错的方法，但一旦开始处理更大的数据集，这种方法就不再可行。更好的方法是将特征重新缩放到标准范围内，例如0到1之间。在
    scikit-learn 中，你可以使用 `MinMaxScaler()` 转换器来完成这一点。在新的代码单元格中输入以下代码并执行，以重新缩放你的训练数据集，然后训练一个新的逻辑回归模型：
- en: '[PRE33]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This time your model should have converged successfully. Instead of having to
    spend more computing time for the model to converge, you were able to simply rescale
    the data to make the training process more efficient. This is a best practice
    in general for training ML models, and not just when using scikit-learn or when
    training a logistic regression model.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这次你的模型应该成功收敛了。与其让模型花费更多计算时间来收敛，你可以简单地重新缩放数据，从而使训练过程更高效。这是训练机器学习模型的一般最佳实践，并不仅限于使用
    scikit-learn 或训练逻辑回归模型时。
- en: Now that your model has been trained, the next step is to evaluate it. In scikit-learn,
    you can use the `score` method of the trained model to do this with your testing
    dataset. The output of the `score` method is the mean accuracy of the model on
    the test dataset expressed as a decimal. Execute a new cell with the code `cls.score(X_test,
    y_test)` to compute the accuracy of your trained model on the test dataset.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，下一步是评估模型。在 scikit-learn 中，你可以使用训练好的模型的 `score` 方法来评估测试数据集的表现。`score`
    方法的输出是模型在测试数据集上的平均准确率，表示为小数。执行新的代码单元格并使用 `cls.score(X_test, y_test)` 来计算模型在测试数据集上的准确率。
- en: The accuracy of your model is likely around 48%, though it may slightly vary
    depending on random states. This does not seem like a good model, as likely you
    could get slightly better results from a random coin flip. However, you may have
    noticed an issue with the approach here. Before training the model, you scaled
    the training features to be between 0 and 1\. But you did not perform the same
    transformations on the testing dataset. Since the model was trained expecting
    a different range of values than was presented for evaluation, the model performed
    poorly. This is a textbook example of *training-serving skew*.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型准确率可能大约在48%左右，尽管具体数值可能会因随机状态而略有不同。这似乎不是一个好模型，因为你可能通过随机抛硬币得到略微更好的结果。但你可能已经注意到这里的问题。在训练模型之前，你将训练特征缩放到0到1之间。但是你没有对测试数据集执行相同的转换。由于模型在评估时预期的值范围与实际呈现的值范围不同，模型表现不佳。这是
    *训练-服务偏差* 的典型例子。
- en: 'You want to perform the same scaling on the test dataset as you did on the
    training dataset. You used the `fit_transform()` method so the `MinMaxScaler()`
    could learn the minimum and maximum value for each feature and scale the values
    to a range of 0 to 1 based on that. You do not want to refit the transformer,
    but you can use the same transformer to transform the testing dataset to prepare
    for evaluation. Do this by using the following code and then compare the difference
    in performance:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望对测试数据集执行与训练数据集相同的缩放。你之前使用了 `fit_transform()` 方法，以便 `MinMaxScaler()` 学习每个特征的最小值和最大值，并根据此范围将值缩放到0到1之间。你不希望重新拟合转换器，但可以使用相同的转换器来对测试数据集进行转换，以便进行评估。通过以下代码完成这一步，然后比较性能差异：
- en: '[PRE34]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The accuracy of your model should now be around 80%. This is much better than
    the accuracy you received before when not scaling the evaluation dataset. The
    important takeaway from this example is that you must perform the same transformations
    at training and evaluation time to be sure you are receiving accurate results.
    This is true at prediction time as well. It is important to document the transformations
    you are performing on your data and leverage tools like pipelines in scikit-learn
    to ensure that these transformations are being applied consistently.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型的准确率现在应该在80%左右。这比您在未对评估数据集进行缩放时获得的准确率要高得多。从这个例子中要得出的重要结论是，您必须在训练和评估时执行相同的转换，以确保获得准确的结果。在预测时也是如此。重要的是要记录您对数据进行的转换，并利用像scikit-learn中的管道这样的工具，以确保这些转换被一致地应用。
- en: Classification Evaluation Metrics
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类评估指标
- en: Accuracy is a simple and easy-to-understand metric that is commonly used for
    classification models. However, there are some possible issues with solely using
    accuracy as a metric. Your model has an accuracy of around 80%, but a model that
    just predicts that there is no customer churn has an accuracy of about 73.5%.
    Eighty percent is better than 73.5%, but the model that predicts no churn would
    have a significantly lower business value compared to a different model that may
    have higher business value. If your goal was to predict customer churn and try
    to prevent customers from leaving, then the “no churn” model will never give any
    actionable insights and effectively has no value.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是用于分类模型的一种简单易懂的指标。然而，仅使用准确率作为指标可能存在一些问题。您的模型准确率约为80%，但只预测没有客户流失的模型准确率约为73.5%。80%比73.5%要好，但预测没有流失的模型将比预测可能有更高商业价值的其他模型具有显著较低的商业价值。如果您的目标是预测客户流失并尝试防止客户离开，那么“无流失”模型永远不会提供任何有价值的见解，实际上没有价值。
- en: As you learned in [Chapter 5](ch05.html#using_automl_to_detect_fraudulent_trans),
    you can use metrics like recall and precision together with accuracy to get a
    clearer picture of your model’s performance. To review, *recall* can be thought
    of as the true positive rate. In this example of customer churn, the recall represents
    the percentage of customers who canceled their account that the model correctly
    predicts. *Precision* can be thought of as the positive predictive power of the
    model. In the case of the customer churn problem, precision represents the percentage
    of the predicted customers canceling their account that indeed do cancel their
    account. If your goal is to proactively contact customers who may cancel their
    accounts, then recall would be a very important metric to consider. You would
    likely be willing to sacrifice a little bit of accuracy or precision to improve
    recall. Of course, this is still something you want to balance as there is a resource
    cost to contacting customers who were not planning to cancel.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第5章](ch05.html#using_automl_to_detect_fraudulent_trans)中学到的，您可以结合准确率、召回率和精确率等指标，更清晰地了解您模型的性能。简而言之，*召回率*可以被视为真正例率。在这个客户流失的例子中，召回率表示模型正确预测了取消账户的客户所占的百分比。*精确率*可以被视为模型的正预测能力。在客户流失问题中，精确率表示模型预测的取消账户中实际上确实取消账户的百分比。如果您的目标是积极联系可能取消账户的客户，那么召回率将是一个非常重要的指标。您可能愿意牺牲一点准确性或精确率来提高召回率。当然，这仍然是需要平衡的，因为联系未计划取消账户的客户也是需要资源成本的。
- en: Take the example of a model that predicts no customer churn. The recall in this
    model is 0 because none of the customers that canceled their accounts are correctly
    identified. However, you would expect that the recall is higher for the model
    that you trained.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 以预测没有客户流失的模型为例。该模型的召回率为0，因为未能正确识别取消账户的任何客户。然而，您预期训练过的模型的召回率会更高。
- en: Remember that the confusion matrix breaks down the predictions into a table
    based on the predicted class and the actual class. A reminder of this is shown
    in [Table 7-8](#confusion_matrix_for_a_general_probl).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，混淆矩阵根据预测类别和实际类别将预测分解成表格。对此的提醒显示在[表格7-8](#confusion_matrix_for_a_general_probl)中。
- en: Table 7-8\. Confusion matrix for a general problem
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7-8\. 一般问题的混淆矩阵
- en: '|   | Predicted positives | Predicted negatives |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|   | 预测正例 | 预测负例 |'
- en: '| --- | --- | --- |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Actual positives | True positives (TP) | False negatives (FN) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 实际正例 | 真正例（TP） | 假负例（FN） |'
- en: '| Actual negatives | False positives (FP) | True negatives (TN) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 实际负样本 | 假阳性（FP） | 真负样本（TN） |'
- en: 'The confusion matrix for the model you trained is easy to compute using scikit-learn.
    To do so, you can use the `confusion_matrix` function from the `sklearn.metrics`
    library. Type the following code into a new cell and execute it to compute the
    confusion matrix for your model:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 您训练的模型的混淆矩阵很容易使用Scikit-Learn进行计算。为此，您可以使用`sklearn.metrics`库中的`confusion_matrix`函数。在新的单元格中键入以下代码并执行，以计算您模型的混淆矩阵：
- en: '[PRE35]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`confusion_matrix` takes three arguments as used here. The first argument is
    the actual labels from your testing dataset, `y_test`. The second is the predicted
    labels from your model. To compute these predictions, you use the `predict` method
    for your model, passing in the transformed test inputs `X_test_scaled`. The last
    argument is optional, but useful. The `labels` argument expects a list of label
    values, here `''Yes''` and `''No''`. This determines the order of the labels in
    the confusion matrix. Your confusion matrix should look similar to the following:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '`confusion_matrix`在此处使用了三个参数。第一个参数是来自您的测试数据集`y_test`的实际标签。第二个是您模型的预测标签。要计算这些预测，您可以使用您的模型的`predict`方法，传入转换后的测试输入`X_test_scaled`。最后一个参数是可选的，但很有用。`labels`参数期望一个标签值列表，这里是`''Yes''`和`''No''`。这确定了混淆矩阵中标签的顺序。您的混淆矩阵应该类似于以下内容：'
- en: '[PRE36]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: What does this mean? Of the examples in your test dataset, 187 + 185 = 372 customers
    canceled their service, and 98 + 939 = 1,037 customers kept their service. Your
    model correctly predicted that 187 customers would cancel (true positives) and
    missed 185 customers who canceled (false negatives). Your model also correctly
    predicted that 939 customers would keep their service (true negatives) but predicted
    that 98 customers who kept their service would cancel instead (false positives).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这是什么意思？在您的测试数据集中，有187 + 185 = 372名顾客取消了他们的服务，而98 + 939 = 1,037名顾客保留了他们的服务。您的模型正确预测了187名顾客会取消（真正例），但错过了185名取消的顾客（假阴性）。您的模型还正确预测了939名顾客会保留他们的服务（真负例），但预测98名实际保留服务的顾客会取消（假阳性）。
- en: 'From this, you can compute precision and recall in two different ways. You
    can compute them by definition, using the information from the confusion matrix,
    or you can leverage the `precision_score` and `recall_score` functions in scikit-learn.
    Use the following code to follow the second approach to compute the precision
    and recall for your model:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 由此，您可以通过两种不同的方式计算精确度和召回率。您可以通过定义使用混淆矩阵中的信息来计算它们，或者您可以利用Scikit-Learn中的`precision_score`和`recall_score`函数。使用以下代码来按照第二种方法计算您模型的精确度和召回率：
- en: '[PRE37]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'A few notes about that code before exploring the results. The first three arguments
    are the same as for the `confusion_matrix` function before. Note that the `labels`
    argument is required for `precision_score` and `recall_score` if you are not using
    indexed labels (such as 0 and 1). You also included an additional argument, `pos_label`,
    which defines the “positive class.” Since recall and precision are metrics pertaining
    to a “positive class,” you need to define which class should be treated as such.
    Your results should be similar to the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索结果之前，请注意该代码的几点。前三个参数与`confusion_matrix`函数中的相同。请注意，如果您不使用索引标签（如0和1），则`precision_score`和`recall_score`需要`labels`参数。您还包括了一个额外的参数`pos_label`，它定义了“正类”。由于召回率和精确度是与“正类”相关的度量，因此您需要定义哪个类应被视为正类。您的结果应该类似于以下内容：
- en: '[PRE38]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In other words, of the customers your model predicted would cancel their account,
    65.6% of those customers actually did cancel. On the other hand, your model only
    captured 50.3% of those customers who actually did cancel. There is still room
    for improvement in your model, but you can clearly see that this model brings
    more business value than a model that cannot detect any churn.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，你的模型预测将要取消账户的顾客中，实际取消的顾客有65.6%。另一方面，你的模型只捕获了实际取消的顾客中的50.3%。你的模型仍有改进的空间，但显然比不能检测到任何客户流失的模型带来更多的业务价值。
- en: Serving Predictions with a Trained Model in Scikit-Learn
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中使用训练模型提供预测
- en: In the previous section, you saw how to serve predictions using the `predict`
    method so you could evaluate your model using various different metrics. One issue
    you encountered naturally in this process was training-serving skew, where in
    your case the data at prediction time was not transformed yet and you received
    inaccurate results. Training-serving skew can arise in different fashions.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，你看到了如何使用`predict`方法提供预测，以便使用不同的度量评估模型。在这个过程中你自然会遇到一个问题，即训练与服务之间的偏差，即在你的情况下，预测时的数据尚未转换，导致结果不准确。训练与服务之间的偏差可能以不同的方式出现。
- en: Another common problem is that the format of the incoming data for predictions
    may be different than the data used for training. In this case, your training
    data was in the CSV format, but maybe the incoming data for predictions is in
    the JSON format, a common data interchange format used in web applications.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见问题是用于预测的传入数据格式可能与训练使用的数据不同。在这种情况下，你的训练数据是CSV格式，但是预测的传入数据可能是JSON格式，这是Web应用中常用的数据交换格式。
- en: When thinking about how to serve predictions in your models, it is important
    to think back through all of the transformations you have done. It can be convenient
    to include these into a single function and include that function with your model
    at prediction time.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑如何在模型中提供预测时，重要的是回顾你所做的所有转换。将这些转换整合到一个单独的函数中，并在预测时与你的模型一起使用，会非常方便。
- en: 'Here are the steps you took to transform your data:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是你转换数据所采取的步骤：
- en: Cleaned the data to ensure that `TotalCharges` was a float.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清洗数据，确保`TotalCharges`为浮点数。
- en: Created a new `DiffBuckets` feature.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`DiffBuckets`特征。
- en: Dropped the `CustomerID`, `gender`, `StreamingTV`, `StreamingMovies`, `PhoneService`,
    and other intermediate columns.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除`CustomerID`、`gender`、`StreamingTV`、`StreamingMovies`、`PhoneService`和其他中间列。
- en: One-hot encoded your categorical features.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对分类特征进行独热编码。
- en: Scaled your numeric features to a range of 0 to 1.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数值特征缩放到0到1的范围内。
- en: 'You would need to perform the same steps when serving predictions. Now you
    will gather all of the preprocessing code into a single function so you can easily
    apply it to new data coming in. Suppose you want to predict whether a specific
    customer will cancel their account at the end of the month. The data has been
    given to you in the JSON format:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供预测时，你需要执行相同的步骤。现在，你将所有预处理代码汇总到一个单独的函数中，以便轻松地将其应用于新进入的数据。假设你想预测某个特定客户在月底是否会取消他们的账户。数据以JSON格式提供给你：
- en: '[PRE39]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You will need to parse this data, perform the transformations listed previously,
    and serve a prediction with your trained model. To parse the data, you can use
    the built-in *json* package in Python. This package has a useful function, `json.loads()`,
    that loads JSON data into a Python dictionary. From there you will be able to
    easily transform the data. Type in the following code, or copy and paste the code
    from the [solution notebook](https://oreil.ly/_XAAH), and execute the cell:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要解析这些数据，执行之前列出的转换，并使用你训练好的模型提供预测。为了解析数据，你可以使用Python中内置的*json*包。该包有一个有用的函数，`json.loads()`，可以将JSON数据加载到Python字典中。从那里，你可以轻松地转换数据。输入以下代码，或者从[solution
    notebook](https://oreil.ly/_XAAH)中复制粘贴代码，并执行该单元格：
- en: '[PRE40]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: At first glance, this may seem like a lot of code, but almost everything here
    is something you have worked through before. The first part may be the part that
    is the most different from before. There you are loading the JSON data using `json.loads()`
    as a dictionary. Since many of your transformations were performed in a Pandas
    DataFrame, it is convenient to load the incoming data into a Pandas DataFrame
    as well. After that, you ensure `TotalCharges` is of type `float64`, and you drop
    the columns you do not need for your model. Next you create the `DiffBuckets`
    feature. Note that for the `pd.cut()` function, you specify the endpoints for
    the bins rather than the number of bins. The cut points are data-dependent, and
    you want to ensure you’re using the same buckets as you did at training time.
    Finally, you will separate out the categorical columns to perform one-hot encoding
    and min-max scaling before serving the prediction.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这可能看起来是很多代码，但几乎所有这些内容你之前都已经处理过。第一部分可能是与以往最不同的部分。在那里，你使用`json.loads()`将JSON数据加载为字典。由于许多转换是在Pandas
    DataFrame中执行的，因此将传入数据加载到Pandas DataFrame中是方便的。之后，确保`TotalCharges`的类型为`float64`，并且删除你的模型不需要的列。接下来，你创建`DiffBuckets`特征。注意，对于`pd.cut()`函数，你指定的是分箱的端点而不是箱子的数量。切割点是依赖于数据的，你希望确保使用与训练时相同的桶。最后，在提供预测之前，你将分离出分类列以执行独热编码和最小-最大缩放。
- en: 'In this case, you will see that this customer is not expected to cancel her
    account. If you wanted to make this code easier to run, you could create a function
    to execute this code. Here is an example of what that would look like:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你会看到这位客户预计不会取消她的账户。如果你想让这段代码更容易运行，你可以创建一个函数来执行这段代码。以下是一个示例：
- en: '[PRE41]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Warning
  id: totrans-321
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: You should only use the `fit_transform` method for transformers like `OneHotEncoder`
    and `MinMaxScaler` at training time. At prediction time, use the `transform` method
    instead to ensure that you are transforming your features in a consistent manner
    to how they were transformed at training time.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，应仅使用诸如`OneHotEncoder`和`MinMaxScaler`的转换器的`fit_transform`方法。在预测时，应改用`transform`方法，以确保以与训练时相同的方式转换特征。
- en: 'Often you will be serving predictions from a different environment than where
    you trained the model. You will need to not just transport the model files, but
    any of the preprocessing code and transformers. You can store objects, like your
    trained model `cls`, using packages like *joblib*. The `dump` method in *joblib*
    serializes the Python object and saves the object to disk, where it can be reloaded
    using the `load` method. For example:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要在与模型训练不同的环境中提供预测时，你需要不仅传输模型文件，还有任何预处理代码和转换器。你可以使用像*joblib*这样的包来存储对象，比如你训练好的模型`cls`。*joblib*中的`dump`方法可以序列化Python对象并将其保存到磁盘，之后可以使用`load`方法重新加载对象。例如：
- en: '[PRE42]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This can be used not just for the model, but for the transformers and other
    objects being used.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅可以用于模型，还可以用于正在使用的转换器和其他对象。
- en: 'Pipelines in Scikit-Learn: An Introduction'
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scikit-Learn中的管道：简介
- en: This section dives into a more advanced topic known as pipelines in scikit-learn.
    Feel free to treat this section as optional on a first reading and return later
    or as needed to learn how to manage transformers in scikit-learn.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 本节深入讨论了scikit-learn中管道的更高级主题。在第一次阅读时，可以将本节视为可选内容，并在需要时返回以学习如何管理scikit-learn中的转换器。
- en: When combining the various different transformations into a single function,
    you likely found the process to be a bit tedious. However, this is a very important
    step to ensure that you are able to avoid training-serving skew. Scikit-learn
    includes a construction known as a `Pipeline` to ease this process. A `Pipeline`
    in scikit-learn contains a sequence of objects where all of the objects are transformers
    (like `OneHot​En⁠coder`) except for the final object, which is a model (like `LinearRegression`).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 当将各种不同的转换合并成一个函数时，你可能觉得这个过程有点乏味。然而，这是一个非常重要的步骤，以确保你能够避免训练-服务偏差。Scikit-learn包含一种称为`Pipeline`的结构来简化这个过程。在scikit-learn中，`Pipeline`包含一个对象序列，其中所有对象都是转换器（如`OneHotEncoder`），除了最后一个对象，它是一个模型（如`LinearRegression`）。
- en: However, some of your processing code involved Pandas operations that did not
    directly involve scikit-learn transformers. How do you include these into a scikit-learn
    pipeline? You can use the `FunctionTransformer()` from scikit-learn in these situations.
    The `FunctionTransformer()` takes a Python function as an argument when defining
    the transformer. When you call `fit_transform()` or `transform()` on that transformer,
    it simply applies the included function to the input and returns the output of
    that function. This is a great way to include NumPy or Pandas processing logic
    into your scikit-learn pipeline.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您的一些处理代码涉及并非直接涉及 scikit-learn 变压器的 Pandas 操作。如何将这些包含到 scikit-learn 管道中？您可以在这些情况下使用
    scikit-learn 的 `FunctionTransformer()`。`FunctionTransformer()` 接受一个 Python 函数作为参数来定义变压器。当您在该变压器上调用
    `fit_transform()` 或 `transform()` 时，它简单地将包含的函数应用于输入并返回该函数的输出。这是将 NumPy 或 Pandas
    处理逻辑包含到 scikit-learn 管道中的好方法。
- en: 'What would the pipeline look like in your case? In essence, you did the following
    steps:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的情况下，管道会是什么样子？实质上，您执行了以下步骤：
- en: Loaded the data into a Pandas DataFrame
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据加载到 Pandas DataFrame 中
- en: Cleaned and prepared the data using Pandas functions
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Pandas 函数清理和准备数据
- en: Split the categorical and numeric columns to be transformed separately
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将要分别转换的分类和数值列进行拆分
- en: One-hot encoded the categorical features and recombined the features
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对分类特征进行了一次独热编码并重新组合了特征
- en: Performed min-max scaling on the dataset before training the model
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练模型之前对数据集进行了最小-最大缩放。
- en: 'The steps here are purposely reorganized slightly compared to the previous
    section to make the transition to a scikit-learn `Pipeline` a little more seamless.
    The first step (loading the DataFrame) would not change here. For the second and
    third steps, you will use a `FunctionTransformer()`. For the fourth step, you
    will need two transformers: the `OneHotEncoder()` you are familiar with from before
    and a new transformer, the `ColumnTransformer()`. The `ColumnTransformer()` allows
    you to apply different transformations on different columns. This is exactly what
    you need for your use case. The `OneHotEncoder()` will be applied to the categorical
    columns, and the `MinMaxScaler()` will be applied to the numeric columns.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的步骤稍微重新组织，以使过渡到 scikit-learn `Pipeline` 更加无缝。第一步（加载 DataFrame）在这里不会改变。对于第二和第三步，您将使用
    `FunctionTransformer()`。对于第四步，您将需要两个变压器：您之前熟悉的 `OneHotEncoder()` 和一个新的变压器，`ColumnTransformer()`。`ColumnTransformer()`
    允许您在不同的列上应用不同的转换。这正是您的使用案例所需要的。`OneHotEncoder()` 将应用于分类列，而 `MinMaxScaler()` 将应用于数值列。
- en: 'First, combine the Pandas preprocessing logic into a single function:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将 Pandas 预处理逻辑合并为一个单独的函数：
- en: '[PRE43]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, include code to specify the numeric and categorical columns:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，包括指定数值和分类列的代码：
- en: '[PRE44]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now define the transformers and the model you plan to use in your pipeline:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在定义在管道中要使用的变压器和模型：
- en: '[PRE45]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'A few notes about the preceding code. The `import` statements for the `FunctionTransformer`
    and `ColumnTransformer` are shown in the first two lines. The `FunctionTransformer`
    takes in a single argument: the Python function containing the transformation
    logic. Since you want to *include* the function and not *call* the function, you
    pass in `transform_fn`, not `transform_fn(df)`. In Python, functions are objects,
    so we can use them as inputs to other functions, as seen here. For the `ColumnTransformer`,
    you pass in a list of ordered triples. The first element of each triple is a name
    that you assign to the transformer, the second element is the transformer itself,
    and the third element is the columns to be transformed.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 关于上述代码的一些说明。`FunctionTransformer` 和 `ColumnTransformer` 的 `import` 语句显示在前两行。`FunctionTransformer`
    接受一个参数：包含转换逻辑的 Python 函数。由于您想要*包含*函数而不是*调用*函数，因此传入 `transform_fn` 而不是 `transform_fn(df)`。在
    Python 中，函数是对象，因此我们可以将它们用作其他函数的输入，就像这里所看到的那样。
- en: 'Now, to define the `Pipeline`, use the following code:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要定义 `Pipeline`，请使用以下代码：
- en: '[PRE46]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The `Pipeline` takes a list of ordered pairs. The first element of each pair
    is the name of that object (transformer or model), and the second element is the
    transformer or model itself. The final `Pipeline` is a nice way to package up
    all of the code, but is there an advantage to doing so other than cleaner code?
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipeline` 接受一个有序对的列表。每对的第一个元素是对象的名称（转换器或模型），第二个元素是转换器或模型本身。最终的 `Pipeline`
    是打包所有代码的好方法，但除了更干净的代码之外，还有其他优势吗？'
- en: The `fit` method of a `Pipeline` will call `fit_transform` on all of the transformers
    and then the `fit` method on the model. The `predict` method will apply each transformer’s
    `transform` methods in order before calling `predict` on the model. In essence,
    you can think of a `Pipeline` as a model with all of the transformations built
    in.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipeline` 的 `fit` 方法会对所有的转换器调用 `fit_transform`，然后对模型调用 `fit` 方法。`predict`
    方法会在调用模型的 `predict` 之前按顺序应用每个转换器的 `transform` 方法。本质上，你可以将 `Pipeline` 视为一个内置了所有转换的模型。'
- en: A final advantage of working with a `Pipeline` is portability. A `Pipeline`
    can be exported using the *pickle* or *joblib* libraries after running the `fit`
    method. This exported pipeline will not only contain the information of the trained
    model, it will contain the fit transformers as well. This is a nice way to transfer
    both transformations and model to a different location to keep consistency for
    serving predictions.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Pipeline` 的最终优势是可移植性。在运行 `fit` 方法后，可以使用 *pickle* 或 *joblib* 库导出 `Pipeline`。这个导出的
    `Pipeline` 不仅包含训练模型的信息，还包含适合的转换器。这是将转换和模型一起传输到其他位置以保持预测一致性的好方法。
- en: As an exercise, finish rewriting your model code to use a `Pipeline`, train
    the model using the `fit` method, then evaluate your model to compute its accuracy,
    precision, and recall.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，完成重写模型代码以使用 `Pipeline`，使用 `fit` 方法训练模型，然后评估模型以计算其准确率、精确度和召回率。
- en: Building a Neural Network Using Keras
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 构建神经网络
- en: You were able to build a logistic regression model using scikit-learn and train
    your first ML model using custom code. In this section you will build another
    type of model using Keras, a framework for easily building custom neural networks
    as part of the larger TensorFlow software development kit (SDK).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 你能够使用 scikit-learn 构建逻辑回归模型，并使用自定义代码训练你的第一个 ML 模型。在本节中，你将使用 Keras 构建另一种类型的模型，Keras
    是一个易于构建自定义神经网络的框架，作为更大的 TensorFlow 软件开发工具包（SDK）的一部分。
- en: Recall that a neural network consists of multiple layers, with each layer having
    some number of neurons, and each neuron having a corresponding activation function.
    In regression models, often the ReLU function is used as the activation function
    for the hidden layers, and no activation function is used for the final output.
    For classification models, the idea is very similar, but you need to convert the
    final output into a probability for the positive class. In logistic regression
    you used the sigmoid function to do this, and it will play the same role in neural
    networks for classification.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，神经网络由多个层组成，每一层有若干个神经元，并且每个神经元有对应的激活函数。在回归模型中，通常ReLU函数用作隐藏层的激活函数，最终输出层则不使用激活函数。对于分类模型，思想非常相似，但你需要将最终输出转换为正类的概率。在逻辑回归中，你使用sigmoid函数来实现这一点，在神经网络中的分类中它也将发挥同样的作用。
- en: Introduction to Keras
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras 简介
- en: TensorFlow was introduced in late 2015 as a free and open source SDK for developing
    ML models. The name *TensorFlow* refers to both tensors and the notion of a flow
    or computation graph. A *tensor* is simply an array with some number of dimensions,
    where the number of dimensions is called the *rank* of the tensor. For example,
    you can think of a line of text as a rank 1 tensor of words or strings. A page
    of text would be a rank 2 tensor since it is an array of lines. A book could be
    a rank 3 tensor, and the analogy goes on. Tensors are a common data structure
    when working with scientific computing and are used in many different contexts.
    The *computation graph* is the set of directions that TensorFlow builds for the
    CPU (or GPU/TPU) to perform the needed computations. Advantages of graph-based
    approaches to computation include optimization techniques that can be applied
    behind the scenes and the ability to easily split up that computation over multiple
    devices.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 于 2015 年底推出，作为一个免费开源的 SDK 用于开发机器学习模型。*TensorFlow* 这个名字指的是张量和计算图的概念。*张量*
    简单地说就是一个带有一定数量维度的数组，其中维度的数量称为张量的*秩*。例如，你可以将一行文本看作是单词或字符串的秩为 1 的张量。一整页文本则是一个秩为
    2 的张量，因为它是一组行的数组。一本书可以是一个秩为 3 的张量，类推。张量在科学计算中是常见的数据结构，在许多不同的上下文中被使用。*计算图* 是 TensorFlow
    为 CPU（或 GPU/TPU）构建的执行所需计算的一组指令。基于图的计算方法的优势包括可以在幕后应用的优化技术以及轻松地将计算分配到多个设备上的能力。
- en: Though TensorFlow has all of these advantages, the original version of TensorFlow
    was known to be difficult to learn due to the approach it took. Over time, new
    libraries and functionalities were added to TensorFlow to make it easier to use
    and more approachable for those new to the framework. In 2019, TensorFlow 2.0
    was released and introduced Keras as the high-level interface of choice for building,
    training, and serving predictions with artificial neural networks. Keras was first
    developed as a Python interface to create neural networks for Theano, a Python
    library defining, optimizing, and efficiently evaluating mathematical expressions
    involving multidimensional arrays. Keras was extended to include support for TensorFlow,
    and with TensorFlow 2.0, Keras is now officially part of the TensorFlow framework.
    Keras is easy to use, and you will be able to use it to create a neural network
    with just a few lines of code.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 TensorFlow 具有所有这些优势，由于其采用的方法，最初版本的 TensorFlow 被认为难以学习。随着时间的推移，向 TensorFlow
    添加了新的库和功能，使其更易于使用，对于那些新手来说更加友好。2019 年，TensorFlow 2.0 发布，引入了 Keras 作为构建、训练和提供人工神经网络预测的高级接口选择。Keras
    最初开发为创建神经网络的 Python 接口，用于 Theano，一个定义、优化和高效评估涉及多维数组的数学表达式的 Python 库。Keras 扩展以支持
    TensorFlow，并且在 TensorFlow 2.0 中，Keras 现在正式成为 TensorFlow 框架的一部分。Keras 易于使用，你可以仅用几行代码创建一个神经网络。
- en: Training a Neural Network Classifier Using Keras
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Keras 训练神经网络分类器
- en: Since the data has already been prepared using scikit-learn and Pandas, you
    will be able to quickly jump into training a new ML model. You will create TensorFlow
    datasets using the Dataset API for your training and test datasets, then you will
    define your neural network model, and finally you will train and evaluate your
    neural network. You will be able to rework the custom function you wrote previously
    for your scikit-learn model to serve predictions for your Keras model as well.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据已经使用 scikit-learn 和 Pandas 准备好了，你可以快速开始训练一个新的机器学习模型。你将使用 Dataset API 创建
    TensorFlow 数据集用于训练和测试数据集，然后定义你的神经网络模型，最后训练和评估你的神经网络。你将能够重新调整你之前为 scikit-learn
    模型编写的自定义函数，以为你的 Keras 模型提供预测服务。
- en: 'Before starting this process, a little additional preprocessing is needed.
    Using the `LogisticRegression` model in scikit-learn, you can build a binary classification
    model to predict two classes, `Yes` and `No`. In Keras, you must use numeric labels
    `1` and `0` instead of string labels as you used before. Luckily, scikit-learn
    includes a `LabelEncoder` transformer for performing this task. Use the following
    code to encode your labels `Yes` and `No` as `1` and `0` respectively:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始这个过程之前，需要进行一些额外的预处理。使用 scikit-learn 中的 `LogisticRegression` 模型，你可以构建一个二分类模型来预测
    `Yes` 和 `No` 两个类别。在 Keras 中，你必须使用数值标签 `1` 和 `0` 而不是之前使用的字符串标签。幸运的是，scikit-learn
    包含一个 `LabelEncoder` 转换器来执行这个任务。使用以下代码将你的标签 `Yes` 和 `No` 编码为 `1` 和 `0`：
- en: '[PRE47]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In the output, you will see that `Yes` is being treated as the positive class,
    or `1`. Note that you can also ensure the order of the labels by fitting the transformer
    on the set `[“No”,”Yes”]` instead and simply transforming `y_train` and `y_test`.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，你会看到`Yes`被视为正类，或者`1`。请注意，你也可以通过在集合`[“No”,”Yes”]`上拟合转换器，然后简单地转换`y_train`和`y_test`来确保标签的顺序。
- en: With the labels properly encoded, now create the TensorFlow datasets. The `tf.data.Dataset`
    API allows you to create data ingestion pipelines to stream data in while training
    your model. Since data is streaming in one batch at a time, the data can be distributed
    across multiple machines. This allows you to train large models on possibly massive
    datasets across multiple different machines. In your case, the dataset does fit
    into memory, but using this API makes it easier to change scale without having
    to change your training code. The Dataset API is easy to use and is considered
    a best practice when using TensorFlow and Keras.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，标签已经正确编码，接下来创建 TensorFlow 数据集。`tf.data.Dataset` API 允许你创建数据摄入流水线，以便在训练模型时流入数据。由于数据是以一批次一次流入的，可以将数据分布在多台机器上。这使得你可以在多台不同的机器上对可能是大规模数据集的大型模型进行训练。在你的情况下，数据集确实适合内存，但是使用此
    API 可以更轻松地改变规模，而无需改变你的训练代码。Dataset API 使用起来很简单，并且在使用 TensorFlow 和 Keras 时被认为是最佳实践。
- en: The common pattern that is used with the `tf.data.Dataset` API is to first create
    a source dataset from your input data. Here the source will be your NumPy arrays
    that you created from your Pandas DataFrames. After that, you will perform any
    transformations you want, using features like the `map()` or `filter()` functions.
    In this case, those transformations have already been completed. The `tf.data.Dataset`
    API will handle sending batches of your data to the training loop and keep the
    training process running seamlessly.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tf.data.Dataset` API 的常见模式是首先从输入数据创建源数据集。在这里，源将是你从 Pandas DataFrames 创建的
    NumPy 数组。之后，你可以执行任何想要的转换，使用`map()`或`filter()`等功能。在本例中，这些转换已经完成。`tf.data.Dataset`
    API 将处理将数据批次发送到训练循环中，并保持训练过程无缝运行。
- en: 'Since you are already working with NumPy arrays, you will be creating your
    `Dataset` using the `from_tensor_slices` method. This method takes a NumPy array
    and treats each “slice” as an example in your training dataset. For example, `X_train`
    is a rank 2 array or a matrix. `from_tensor_slices` will treat each row of that
    matrix as a separate example. Additionally, you can pass in a pair of arrays,
    and Keras will treat the first as examples and the second as labels, which is
    precisely what you want in this scenario. To create your `Dataset` objects, use
    the following code:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你已经在使用 NumPy 数组，你将使用`from_tensor_slices`方法创建你的`Dataset`。此方法接受一个 NumPy 数组，并将该“切片”视为训练数据集中的一个示例。例如，`X_train`是一个秩为2的数组或矩阵。`from_tensor_slices`将该矩阵的每一行视为一个单独的示例。此外，你还可以传入一对数组，Keras
    将第一个数组视为示例，第二个数组视为标签，这正是你在这种情况下想要的。要创建你的`Dataset`对象，请使用以下代码：
- en: '[PRE48]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The only portion of the code that may need some explanation is the `batch()`
    method. Recall that `Dataset` objects send batches of data from your dataset to
    the training loop. The `batch()` method defines the size of those batches. The
    exact right batch size will depend on the dataset and the model type you are working
    with and often takes a bit of tuning to get perfect. As a general rule of thumb,
    the larger your model or example size, the smaller your batch size should be.
    However, the smaller the batch size, the *noisier* the training process may be.
    In other words, gradient descent will take a less direct path toward the optimal
    set of weights, so it could take more compute time to converge to an optimal model.
    Batch size is a perfect example of a hyperparameter that defines the model and
    model training process.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中唯一需要解释的部分可能是`batch()`方法。回想一下，`Dataset`对象会将数据批次从你的数据集发送到训练循环中。`batch()`方法定义了这些批次的大小。确切的批次大小取决于你的数据集和模型类型，并且通常需要进行一些调整以达到最佳效果。作为一个经验法则，你的模型或示例大小越大，批次大小就应该越小。然而，批次大小越小，训练过程可能会*更加嘈杂*。换句话说，梯度下降将不会直接朝着最优权重集合的路径前进，因此可能需要更多的计算时间来收敛到最优模型。批次大小是定义模型和模型训练过程的一个典型超参数。
- en: 'Now that the data is ready for training, you should create your model. Recall
    in BigQuery ML you specified a list of integers. The number of elements in the
    list was the number of hidden units, and the value of each integer is the number
    of neurons in that hidden layer. You will use the same information in Keras, but
    in a slightly different format. The `keras.Sequential` API allows you to give
    a list of the layers you want in your model and then builds a model from that
    information. The layers discussed in the previous chapter are what Keras calls
    `Dense` layers. `Dense` layers are layers where all of the neurons from the previous
    layer connect to every neuron in the next layer, forming the weighted sums you
    saw in [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg). An example
    of a `Dense` layer is shown here:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已准备好进行训练，您应该创建您的模型。回想一下，在 BigQuery ML 中，您指定了一个整数列表。列表中的元素数量是隐藏单元的数量，每个整数的值是该隐藏层中的神经元数。您将在
    Keras 中使用相同的信息，但格式稍有不同。`keras.Sequential` API 允许您提供您模型中要包含的层的列表，然后根据该信息构建模型。前一章中讨论的层是
    Keras 称之为 `Dense` 层的层。`Dense` 层是指所有前一层的神经元连接到下一层的每个神经元，形成了您在 [第 6 章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)
    中看到的加权总和。以下是 `Dense` 层的一个示例：
- en: '[PRE49]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: There are four arguments shown in this example. First is the number of `units`.
    This is simply the number of neurons in this layer. The second argument is the
    `input_shape`. If you are defining the first layer in the model, you need to specify
    the shape of the incoming examples. If this is not the first layer of the model,
    then you can omit this argument, as Keras will receive this information from the
    previous layer. Recall that after one-hot encoding, there were 28 distinct features.
    You can double-check this by looking at the output of `X_train.shape`. The shape
    of `X_train` is (5634, 28). There are 5,634 examples, and each has 28 feature
    values. The notation `(28,)` may seem odd, but it is how Python represents a list
    of one element. In the case that the incoming examples are higher dimensional
    (for example, with image data), then the `input_shape` will be a list of many
    elements. The third argument is `activation`, for defining the activation function.
    For a binary classification model, the final layer will need `1` output and `sigmoid`
    as the activation function. For the hidden layers, the most commonly used activation
    function is the ReLU (or rectified linear unit) function, as discussed in [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg).
    Finally, you can assign your own custom name to the layer with the `name` argument.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例中显示了四个参数。第一个是 `units` 的数量。这只是该层中的神经元数目。第二个参数是 `input_shape`。如果您正在定义模型的第一层，则需要指定传入样本的形状。如果这不是模型的第一层，则可以省略此参数，因为
    Keras 将从前一层接收到这些信息。请记住，在进行独热编码之后，有 28 个不同的特征。您可以通过查看 `X_train.shape` 的输出来再次确认。`X_train`
    的形状是 (5634, 28)。共有 5,634 个样本，每个样本有 28 个特征值。`(28,)` 这种表示法可能看起来有些奇怪，但这是 Python 表示单一元素列表的方式。如果传入的样本具有更高的维度（例如，图像数据），则
    `input_shape` 将是包含多个元素的列表。第三个参数是 `activation`，用于定义激活函数。对于二元分类模型，最终层将需要 `1` 个输出和
    `sigmoid` 作为激活函数。对于隐藏层，最常用的激活函数是 ReLU（或修正线性单元）函数，正如在 [第 6 章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)
    中讨论的那样。最后，您可以使用 `name` 参数为层分配自定义名称。
- en: 'Use the following code to define your neural network in Keras. This code will
    create a neural network with three hidden layers of 64, 32, and 16 neurons respectively:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码在 Keras 中定义您的神经网络。此代码将创建一个包含分别为 64、32 和 16 个神经元的三个隐藏层的神经网络：
- en: '[PRE50]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Now that the model has been defined, it needs to be compiled. This is the process
    that translates the model into TensorFlow operations and configures the model
    for training. Luckily, Keras handles all this with only a few inputs from you.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经定义，需要进行编译。这个过程将模型转换为 TensorFlow 操作，并配置模型进行训练。幸运的是，Keras 只需您提供少量输入即可处理所有这些。
- en: 'Use the following code to compile your model:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码编译您的模型：
- en: '[PRE51]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`keras.losses.BinaryCrossentropy()` is Keras’s implementation of the cross-entropy
    loss function discussed earlier in this chapter. You also included three metrics
    to be included for evaluating the performance of the model. `BinaryAccuracy()`,
    `Precision()`, and `Recall()` are Keras’s implementation of the accuracy, precision,
    and recall metrics, respectively, that you used for your model created in scikit-learn.
    When compiling the model using the `compile()` method, you include the loss function,
    any metrics you want to use, and the optimizer. A deep discussion of optimizers
    is beyond the scope of this book, but the Adam optimizer is considered a good
    default choice for training neural network models. In short, Adam takes the original
    gradient descent algorithm and makes some changes to address some possible pitfalls
    of using gradient descent. For a deeper discussion on optimizers, please see [*Hands-On
    Machine Learning with Scikit-Learn, Keras, and TensorFlow*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)
    by Aurélien Géron (O’Reilly, 2022).'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '`keras.losses.BinaryCrossentropy()` 是 Keras 实现的本章早期讨论过的交叉熵损失函数。您还包括了三个指标来评估模型的性能。`BinaryAccuracy()`、`Precision()`
    和 `Recall()` 分别是 Keras 实现的用于您在 scikit-learn 中创建的模型的准确率、精确度和召回率指标。在使用 `compile()`
    方法编译模型时，您需要包括损失函数、想要使用的任何指标以及优化器。对优化器的深入讨论超出了本书的范围，但 Adam 优化器被认为是训练神经网络模型的良好默认选择。简而言之，Adam
    对原始梯度下降算法进行了一些改进，以解决使用梯度下降时可能遇到的一些问题。关于优化器的更深入讨论，请参阅《*Scikit-Learn、Keras 和 TensorFlow
    实战*》（O''Reilly, 2022）作者 Aurélien Géron 的书籍 [*Hands-On Machine Learning with Scikit-Learn,
    Keras, and TensorFlow*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)。'
- en: 'With the model now defined and compiled, the next step is to train it. In Keras,
    you use the `fit()` method to train a compiled model. You need to specify the
    dataset you want to use for training and evaluation, and you need to specify how
    long you wish to train the model. The following code shows an example of this:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经定义并编译完成，下一步是训练模型。在 Keras 中，您使用 `fit()` 方法来训练一个已编译的模型。您需要指定用于训练和评估的数据集，并且需要指定希望训练模型的时长。以下代码展示了一个示例：
- en: '[PRE52]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The argument `x` corresponds to the training dataset. If you are not using a
    `tf.data.Dataset`, then you will have to specify the labels separately in the
    argument `y`, but your `train_dataset` contains both the features and the labels.
    `validation_data` is similar to the first argument `x` except it is specifically
    for the evaluation dataset. This argument is optional, but it tends to be a good
    idea to monitor how the metrics for training and evaluation evolve side by side.
    Finally, the middle argument, `epochs`, is a measurement of how long the training
    process will last. In ML terms, an *epoch* is an entire pass through your dataset
    for the sake of training. Recall that your dataset is sending the data to the
    training loop in batches of 128, and your training dataset has 5,634 examples.
    After 44 batches (or what are often called *steps*), you will have gone through
    the entire dataset, and thus an epoch. The next epoch will then begin as Keras
    goes back through the dataset once again.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 `x` 对应于训练数据集。如果您不使用 `tf.data.Dataset`，则必须在参数 `y` 中单独指定标签，但是您的 `train_dataset`
    包含特征和标签。`validation_data` 类似于第一个参数 `x`，只是专门用于评估数据集。这个参数是可选的，但是监视训练和评估指标并行演变往往是个好主意。最后，中间参数
    `epochs` 是训练过程将持续的时间测量。在机器学习术语中，一个 *epoch* 是整个数据集的一次完整遍历，用于训练。请记住，您的数据集正在以 128
    的批次发送到训练循环，并且您的训练数据集有 5,634 个示例。经过 44 个批次（或通常称为 *步骤*），您将完成整个数据集的一次遍历，然后下一个 epoch
    将再次开始，因为 Keras 会再次遍历数据集。
- en: How long should the model train for? There is no nice rule of thumb to rely
    on in practice. However, there are a couple of signs you can look out for. First,
    if the model is no longer improving from one epoch to another, that’s a sign that
    the training process has converged. A second sign is that the model is starting
    to perform worse on the evaluation dataset. This is a sign that the model has
    begun overfitting and that you want to stop training it, even if the performance
    continues to improve on the training dataset.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 模型应该训练多长时间呢？在实践中没有一个明确的经验法则。然而，有几个迹象可以依据。首先，如果模型从一个时期到另一个时期不再改善，那就表明训练过程已经收敛。第二个迹象是，模型开始在评估数据集上表现更差。这表明模型已经开始过拟合，您希望停止训练它，即使在训练数据集上的性能继续改善。
- en: 'The `fit()` method also has an optional argument for callbacks. A *callback*
    is a function that can perform actions at various stages of training—for example,
    saving checkpoints of the model every epoch, or stopping training if model performance
    has stalled or is getting worse on the evaluation dataset. The latter example
    is called *early stopping*. Use the following code to train your model, using
    the `keras.callbacks.​Ear⁠lyStopping()` callback function to impose early stopping
    if needed:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()`方法还有一个可选的回调参数。*回调*是在训练的各个阶段执行操作的函数，例如每个周期保存模型的检查点，或者在评估数据集上模型性能停滞或变差时停止训练。后一种例子称为*早期停止*。使用以下代码来训练您的模型，使用`keras.callbacks.EarlyStopping()`回调函数在需要时实施早期停止：'
- en: '[PRE53]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The argument `patience` for `keras.callbacks.EarlyStopping()` specifies that
    you want to wait for five epochs of stalled or worsening performance before stopping
    the training process. The training process, due to various places where randomness
    comes into play, can be noisy. You do not want to terminate training early due
    to noisiness, so the `patience` argument is useful in preventing that. However,
    just in case the model performance is getting worse during that waiting period,
    the `restore_best_weights` argument being set to `True` will return the model
    back to its best performance before terminating the training process. If you have
    not already, run the code above to train your model. This will take a couple of
    minutes in Colab to complete training.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '`keras.callbacks.EarlyStopping()`方法的参数`patience`表示在停止训练过程之前，您希望等待五个周期的停滞或性能恶化。由于训练过程中存在随机性的多个地方，可能会产生噪声。您不希望由于噪声而过早终止训练，因此`patience`参数在防止这种情况下非常有用。然而，以防在等待期间模型性能变差，将`restore_best_weights`参数设置为`True`将使模型恢复到终止训练之前的最佳性能。如果您还没有运行上述代码来训练您的模型，请执行此操作。在Colab中完成训练可能需要几分钟。'
- en: 'While training the model, you will see metrics for every epoch of training.
    The first few metrics are specific to the training dataset, and the second half
    is related to the test datasets. You can also use the `model.evaluate(x=test_dataset)`
    method to evaluate the model. This can be used for the test dataset or for other
    datasets, depending on how you split your dataset for training and evaluation.
    Your output from `model.evaluate()` should look similar to the following:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，您将看到每个训练周期的指标。前几个指标特定于训练数据集，后半部分与测试数据集相关。您还可以使用`model.evaluate(x=test_dataset)`方法来评估模型。这可以用于测试数据集或其他数据集，具体取决于您如何拆分数据集进行训练和评估。从`model.evaluate()`中输出的结果应该类似于以下内容：
- en: '[PRE54]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Good news, you have trained a successful ML model using Keras. Unfortunately,
    in this case the neural network model had slightly lower accuracy, precision,
    and recall. What does this mean? It doesn’t mean that a neural network model is
    automatically worse, but it does mean that likely some further tuning and feature
    engineering will be needed to improve the performance of this model.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息，您已经使用Keras训练了一个成功的ML模型。不幸的是，在这种情况下，神经网络模型的准确性、精确度和召回率略低。这意味着什么？这并不意味着神经网络模型自动更差，但这意味着可能需要进一步调整和特征工程来提高该模型的性能。
- en: Building Custom ML Models on Vertex AI
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Vertex AI上构建自定义ML模型
- en: You have successfully built, trained, and evaluated multiple ML models. You
    used scikit-learn to train a logistic regression model and used Keras to train
    a neural network classifier. In this case, the dataset you were using was fairly
    small, but in practice you may want to train custom code models on large datasets.
    You saw in [Chapter 5](ch05.html#using_automl_to_detect_fraudulent_trans) how
    to train a classification model using Vertex AI AutoML. In this section, you will
    get a brief introduction to training custom code models using Vertex AI Training.
    This section introduces you to more complex topics in Python, such as creating
    packages. You will not dive into the fine details here, but you’ll see enough
    to get you started.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经成功构建、训练和评估了多个ML模型。您使用scikit-learn训练了一个逻辑回归模型，并使用Keras训练了一个神经网络分类器。在这种情况下，您使用的数据集相对较小，但实际上您可能希望在大型数据集上训练自定义代码模型。在第5章中，您看到了如何使用Vertex
    AI AutoML训练分类模型。在本节中，您将简要介绍如何使用Vertex AI训练自定义代码模型。本节向您介绍Python中更复杂的主题，例如创建包。在这里，您不会深入了解细节，但足以让您开始。
- en: 'Vertex AI allows you to train your model in a containerized environment on
    a cluster of machines of your choice. Roughly speaking, you can think of a *container*
    as a computer where the hardware and operating system are abstracted away so that
    a developer can focus purely on the software they want to run. When using standard
    ML frameworks, you can use prebuilt containers. Prebuilt containers are available
    for scikit-learn, TensorFlow, XGBoost, and PyTorch. To use the Vertex AI Training
    service in the Cloud Console, do the following steps:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI允许你在选择的机器集群中的容器化环境中训练你的模型。粗略地说，你可以把*容器*看作是一个计算机，其中硬件和操作系统被抽象化，开发人员可以专注于他们想要运行的软件。当使用标准ML框架时，可以使用预构建的容器。scikit-learn、TensorFlow、XGBoost和PyTorch都提供预构建的容器。要在Cloud
    Console中使用Vertex AI训练服务，执行以下步骤：
- en: Ensure that your dataset is available in Google Cloud Storage, BigQuery, or
    Vertex AI managed datasets.
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你的数据集已经在Google Cloud Storage、BigQuery或Vertex AI托管的数据集中可用。
- en: Gather the Python code into a single script.
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Python代码收集到一个单独的脚本中。
- en: Update your code to save results in a Google Cloud Storage bucket.
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新你的代码以将结果保存在Google Cloud Storage存储桶中。
- en: Create a source code distribution.
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个源代码分发。
- en: Fortunately, the first step has already been completed. The dataset is available
    already in a public Google Cloud Storage bucket. The next step is to gather the
    Python code into a single script. This code includes all of the code that has
    been used to load the data into a Pandas DataFrame, prepare the data for training,
    and build and train an ML model, and you will need to also add code to be sure
    that the model is saved somewhere off of the Vertex AI resources being used to
    train the model.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，第一步已经完成。数据集已经在公共Google Cloud Storage存储桶中可用。接下来的步骤是将Python代码收集到一个单独的脚本中。这段代码包括用于加载数据到Pandas
    DataFrame、准备训练数据、构建和训练ML模型的所有代码，并且你还需要添加代码来确保模型被保存到Vertex AI资源之外的地方。
- en: Warning
  id: totrans-393
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Vertex AI provisions resources for a training job for just the duration of the
    job, and then those resources are torn down. This means you need to be sure to
    save anything locally that you do not want to lose, but it also means that you
    only use the resources that you need.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI为训练作业分配资源，仅在作业运行期间使用这些资源，然后将其销毁。这意味着你需要确保将不想丢失的任何内容保存在本地，但也意味着你只使用你需要的资源。
- en: 'Before moving forward, you will need to create a directory to hold the files
    you will be creating for your Python package. To do this in Colab, run the following
    code in a new cell:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，你需要创建一个目录来保存你将为Python包创建的文件。在Colab中，可以在新单元格中运行以下代码来执行此操作：
- en: '[PRE55]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The exclamation point at the front of the code tells Colab to run this as a
    Linux terminal command. The `mkdir` command creates a new directory, and you are
    calling this new directory *trainer*.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 代码前面的感叹号告诉Colab将其作为Linux终端命令运行。`mkdir`命令创建一个新目录，你将这个新目录命名为*trainer*。
- en: 'The Python code from this chapter has been combined into a single file and
    is available in the [low-code-ai GitHub repo](https://oreil.ly/supp-lcai). You
    should try to build this file yourself as well. You can do this in a notebook
    by using a special cell magic, `%%writefile`. The `%%writefile` cell magic tells
    Colab to write the contents of the cell to a specified file rather than execute
    the code in the cell. The format for the cell will be the following:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的Python代码已经合并到一个单文件中，并且可以在[low-code-ai GitHub仓库](https://oreil.ly/supp-lcai)中找到。你也应该尝试自己构建这个文件。你可以在笔记本中使用特殊的单元格魔法`%%writefile`来完成这个操作。`%%writefile`单元格魔法告诉Colab将单元格的内容写入指定的文件，而不是执行单元格中的代码。单元格的格式如下：
- en: '[PRE56]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Before reading further, you should go ahead and either combine the code from
    your notebook into a single cell to write out to `trainer/trainer.py` using the
    `%%writefile` cell magic, or you should copy the solution at the link above. Note
    that you do not need to include code for visualizations or where you were checking
    the output. You should use the solution to check your work if you are combining
    the Python code yourself into a single file.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续阅读之前，你应该将笔记本中的代码合并到一个单元格中，然后使用`%%writefile`单元格魔法将其写入`trainer/trainer.py`，或者你可以复制上面链接中的解决方案。请注意，你不需要包括用于可视化或检查输出的代码。如果你将Python代码自己合并到单个文件中，可以使用这个解决方案来检查你的工作。
- en: When looking at the solution, the first thing you will notice is that all the
    `import` statements have been moved to the top of the script. This is considered
    a Python best practice, though if the import is done before the module being imported
    is used, it will not be an issue. Toward the end of the file, `print` statements
    have been added for printing out the various different metrics from the model.
    If you do not print the results, they will be lost. Using `print` statements will
    allow you to find these metrics later in the training logs.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 查看解决方案时，您将首先注意到所有的`import`语句都已移到脚本顶部。尽管如果导入在使用之前就已经完成，这被认为是Python的最佳实践，它也不会成为问题。在文件末尾，添加了`print`语句以打印出模型的各种不同指标。如果不打印结果，它们将会丢失。使用`print`语句将允许您在训练日志中稍后找到这些指标。
- en: 'Finally, in the last line of the Python script you will see the use of `joblib.dump()`
    to write the model out to Google Cloud Storage. Note that the reference for Cloud
    Storage is different here: `''gcs/<YOUR-BUCKET-NAME>/sklearn_model/''`. Google
    Cloud Storage buckets are mounted in Vertex AI Training using Cloud Storage FUSE
    and are effectively treated like a file system. You will need to include your
    Cloud Storage bucket name for the bucket you created in [Chapter 4](ch04.html#use_automl_to_predict_advertising_media)
    or create a new bucket.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在Python脚本的最后一行，您将看到使用`joblib.dump()`将模型写入Google Cloud存储的用法。请注意，此处Cloud Storage的引用不同：`'gcs/<YOUR-BUCKET-NAME>/sklearn_model/'`。Google
    Cloud存储桶通过Cloud Storage FUSE挂载，并被有效地视为文件系统。您需要包括您在[第四章](ch04.html#use_automl_to_predict_advertising_media)中创建的或创建新的云存储桶的名称。
- en: 'Now that the script has been written to `trainer\trainer.py`, the next step
    is to create the other files in the package. An easy way to do this is using the
    `%%writefile` cell magic. To have Python recognize a folder as a package, it needs
    an *__init__.py* in the *trainer* directory. This file contains any initialization
    code for the package, but this can be an empty file as well if no such code is
    needed. The other file you will need is a *setup.py* file. The objective of the
    *setup.py* file is to ensure that the package is installed correctly on the machine(s)
    executing the training job and is a standard part of a Python package. That all
    being said, when using the standard prebuilt container for scikit-learn training
    on Vertex AI, most of this process is straightforward and involves mostly boilerplate
    code. To create the *__init__.py* file in the *trainer* directory, run the following
    code in a new cell:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在脚本已经写入到`trainer\trainer.py`，下一步是创建包中的其他文件。一个简单的方法是使用`%%writefile`单元格魔法。为了让Python将一个文件夹识别为一个包，它需要在*trainer*目录中有一个*__init__.py*文件。该文件包含包的任何初始化代码，如果不需要这样的代码，它也可以是一个空文件。您还需要的另一个文件是*setup.py*文件。*setup.py*文件的目标是确保在执行训练作业的机器上正确安装该包，并且它是Python包的标准部分。尽管如此，当使用Vertex
    AI上的scikit-learn训练的标准预构建容器时，大部分流程都是直截了当的，并且主要涉及样板代码。要在*trainer*目录中创建*__init__.py*文件，请在新单元格中运行以下代码：
- en: '[PRE57]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The line `#No initialization needed` is an example of a comment in Python.
    The `#` symbol denotes that Python should not parse the line, but is there for
    human readability. Since there will be no initialization needed for your package,
    Python will simply treat this *__init__.py* file as a blank file. To create your
    *setup.py* file, run the following code in a new cell:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 行`#No initialization needed`是Python中注释的一个示例。`#`符号表示Python不应解析该行，但是这仅供人类可读性。由于您的包不需要初始化，Python将简单地将此*__init__.py*文件视为一个空文件。要创建您的*setup.py*文件，请在新单元格中运行以下代码：
- en: '[PRE58]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The code here is mostly boilerplate code. Within the `setup` function you define
    the package name, the version number, what packages should be installed as part
    of the distribution, and the description of the package. The `find_packages()`
    function automatically detects the packages in your directory for you. The `install_requires=['gcsfs']`
    argument ensures that the *gcsfs* package is installed for using Cloud Storage
    FUSE.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的代码大部分是样板代码。在`setup`函数中，您定义了包名称、版本号、作为分发的一部分应安装的包以及包的描述。`find_packages()`函数会自动检测您目录中的包。`install_requires=['gcsfs']`参数确保安装*gcsfs*包以便使用Cloud
    Storage FUSE。
- en: 'All of the files are in place, so now you can create your package by executing
    the following code in a new cell:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 所有文件都就位了，现在您可以通过在新单元格中执行以下代码来创建您的包：
- en: '[PRE59]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The command is to execute the Python script `./setup.py` with the `sdist` option.
    This creates a Python source distribution with the compression format `tar.gz`.
    Your files, with relevant folders expanded, should look like what is shown in
    [Figure 7-10](#file_structure_after_creating_the_sourc).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 执行Python脚本`./setup.py`的命令，并使用`sdist`选项。这将创建一个带有压缩格式`tar.gz`的Python源代码分发。您的文件，以及相关文件夹扩展后，应如[Figure 7-10](#file_structure_after_creating_the_sourc)所示。
- en: '![File structure after creating the source code distribution for your trainer
    package](assets/lcai_0710.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![为您的训练作业创建源代码分发后的文件结构](assets/lcai_0710.png)'
- en: Figure 7-10\. File structure after creating the source code distribution for
    your trainer package.
  id: totrans-412
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '[Figure 7-10](#file_structure_after_creating_the_sourc)创建您的训练包的源代码分布后的文件结构。'
- en: 'The difficult part of the process has now been completed. Now you should move
    the package to a location on Google Cloud Storage for use on Vertex AI. The easiest
    way to do this is to authorize Colab to access your Google Cloud account and then
    use the `gcloud storage` tool. To authorize Colab to access your Google Cloud
    resources, run the following code in a new cell and follow the prompts:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 现在过程中的困难部分已经完成。现在，您应该将软件包移动到Google Cloud存储的位置，以供在Vertex AI上使用。这样做的最简单方法是授权Colab访问您的Google
    Cloud帐户，然后使用`gcloud storage`工具。要授权Colab访问您的Google Cloud资源，请在新单元格中运行以下代码，并按照提示操作：
- en: '[PRE60]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'After following the prompts, you can then move the file to the Cloud Storage
    bucket of your choice. Run the following code in a new cell, replacing `your-project-id`
    with your project ID and `your-bucket-name` with your bucket name:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在按照提示操作后，您可以将文件移动到您选择的云存储桶中。在新单元格中运行以下代码，将`your-project-id`替换为您的项目ID，将`your-bucket-name`替换为您的存储桶名称：
- en: '[PRE61]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Now everything is in place and you’re ready to train your model. You will submit
    your training job through the Cloud Console. Open a new browser window or tab
    and go to [*console.cloud.google.com*](http://console.cloud.google.com). After
    that, select Vertex AI on the left-hand side menu, and then select Training. If
    you are having problems finding the Training option, see [Figure 7-11](#location_of_the_training_option_in_vert).
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切就绪，您可以开始训练您的模型。您将通过Cloud控制台提交您的培训作业。打开一个新的浏览器窗口或选项卡，然后转到[*console.cloud.google.com*](http://console.cloud.google.com)。然后，在左侧菜单中选择Vertex
    AI，然后选择培训。如果您找不到培训选项，请参见[图 7-11](#location_of_the_training_option_in_vert)。
- en: '![Location of the Training option in Vertex AI](assets/lcai_0711.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![Vertex AI中培训选项的位置](assets/lcai_0711.png)'
- en: Figure 7-11\. Location of the Training option in Vertex AI.
  id: totrans-419
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '[Figure 7-11](#location_of_the_training_option_in_vert)中Vertex AI中培训选项的位置。'
- en: After selecting the Training option, click Create to start creating your new
    training job (see [Figure 7-12](#location_of_the_create_button_in_the_ve)). You
    have a few options to set before you can start the training process. The inputs
    on the different pages are shown in [Figure 7-13](#input_on_the_training_details_page_for)
    through [Figure 7-16](#input_on_the_compute_and_pricing_pagedo), and the inputs
    are shown in [Table 7-9](#inputs_for_your_training_job_in_vertex). Any options
    not mentioned here should be left as the default value. Once you have input all
    of the options on each page, click Continue until the Start Training button is
    available to press.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 选择培训选项后，单击创建以开始创建新的培训作业（参见[图 7-12](#location_of_the_create_button_in_the_ve)）。在开始培训过程之前，您需要设置几个选项。不同页面上的输入如[图 7-13](#input_on_the_training_details_page_for)至[图 7-16](#input_on_the_compute_and_pricing_pagedo)所示，并且这些输入如[表 7-9](#inputs_for_your_training_job_in_vertex)所示。未在此处提到的任何选项都应保留为默认值。一旦在每个页面上输入了所有选项，请继续，直到“开始培训”按钮可用为止。
- en: '![Location of the Create button in the Vertex AI Training console](assets/lcai_0712.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![Vertex AI培训控制台中“创建”按钮的位置](assets/lcai_0712.png)'
- en: Figure 7-12\. Location of the Create button in the Vertex AI Training console.
  id: totrans-422
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '[Figure 7-12](#location_of_the_create_button_in_the_ve)中Vertex AI训练控制台中“创建”按钮的位置。'
- en: '![Input on the Training Details page for your training job](assets/lcai_0713.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![您的培训作业的“培训详情”页面上的输入](assets/lcai_0713.png)'
- en: Figure 7-13\. Input on the Training Details page for your training job.
  id: totrans-424
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '[Figure 7-13](#input_on_the_training_details_page_for)中您的培训作业的“培训详情”页面上的输入。'
- en: '![Input on the Model Details page for your training job](assets/lcai_0714.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![您的培训作业的“模型详情”页面上的输入](assets/lcai_0714.png)'
- en: Figure 7-14\. Input on the Model Details page for your training job.
  id: totrans-426
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '[Figure 7-14](#input_on_the_model_details_page_for)中您的培训作业的“模型详情”页面上的输入。'
- en: '![Input on the “Training container” page for your training job. Be sure to
    replace the bucket with the bucket you are using.](assets/lcai_0715.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![您的培训作业的“培训容器”页面上的输入。请确保将存储桶替换为您正在使用的存储桶。](assets/lcai_0715.png)'
- en: Figure 7-15\. Input on the “Training container” page for your training job.
    Be sure to replace the bucket with the bucket you are using.
  id: totrans-428
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-15\. “训练容器”页面上的输入。确保用您正在使用的存储桶替换这个存储桶。
- en: '![Input on the Compute and Pricing page. Be sure to choose a location close
    to your bucket.](assets/lcai_0716.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![计算和定价页面上的输入。确保选择与您的存储桶接近的位置。](assets/lcai_0716.png)'
- en: Figure 7-16\. Input on the Compute and Pricing page. Be sure to choose a location
    close to your bucket.
  id: totrans-430
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-16\. 计算和定价页面上的输入。确保选择与您的存储桶接近的位置。
- en: Table 7-9\. Inputs for your training job in Vertex AI Training
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-9\. Vertex AI 训练作业的输入
- en: '| Training Method page |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 训练方法页面 |'
- en: '| Dataset | No managed dataset |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 无托管数据集 |'
- en: '| Model training method | Custom training (advanced) |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 模型训练方法 | 自定义训练（高级） |'
- en: '| Model Details page |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 模型详细信息页面 |'
- en: '| Radio buttons | Train new model |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 单选按钮 | 训练新模型 |'
- en: '| Model name | churn |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | churn |'
- en: '| “Training container” page |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| “训练容器”页面 |'
- en: '| Radio buttons | Pre-built container |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 单选按钮 | 预构建容器 |'
- en: '| Model framework | scikit-learn |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 模型框架 | scikit-learn |'
- en: '| Model framework version | 0.23 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 模型框架版本 | 0.23 |'
- en: '| Package location | gs://your-bucket-name/trainer-0.1.tar.gz |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 包位置 | gs://your-bucket-name/trainer-0.1.tar.gz |'
- en: '| Python module | trainer.trainer |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| Python 模块 | trainer.trainer |'
- en: '| Compute and Pricing page |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 计算和定价页面 |'
- en: '| Region | Choose a region close to your bucket location |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| 区域 | 选择与您的存储桶位置接近的区域 |'
- en: '| Machine type | n1-standard-4 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 机器类型 | n1-standard-4 |'
- en: The training pipeline that Vertex AI will create takes around three minutes
    to run. Once it has finished, you can see the logs by going back to the Vertex
    AI link and then Training. Once on that page, click Custom Jobs and then click
    “churn-custom-job” (see [Figure 7-17](#the_custom_jobs_page_with_the_finished)).
    Once on the page for the custom job, you will see a table of information. At the
    bottom of that table, click the link for “View logs.” If you scroll down, you
    will then see the printout of the metrics from the training job. An example of
    the metrics shown in logs is displayed in [Figure 7-18](#metrics_from_the_custom_training_job).
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 创建的训练流水线运行大约需要三分钟。完成后，您可以通过返回 Vertex AI 链接并选择“训练”来查看日志。在该页面上，点击“自定义作业”，然后点击“churn-custom-job”（见
    [图 7-17](#the_custom_jobs_page_with_the_finished)）。进入自定义作业页面后，您将看到一个信息表格。在表格底部，点击“查看日志”链接。如果向下滚动，您将看到训练作业日志中的指标打印输出。日志中显示的指标示例如
    [图 7-18](#metrics_from_the_custom_training_job) 所示。
- en: '![The Custom Jobs page with the finished custom job churn-custom-job](assets/lcai_0717.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![完成的自定义作业“churn-custom-job”在“自定义作业”页面上](assets/lcai_0717.png)'
- en: Figure 7-17\. The Custom Jobs page with the finished custom job `churn-custom-job`.
  id: totrans-449
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-17\. 完成的自定义作业“churn-custom-job”在“自定义作业”页面上。
- en: '![Metrics from the custom training job](assets/lcai_0718.png)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
  zh: '![自定义训练作业的指标](assets/lcai_0718.png)'
- en: Figure 7-18\. Metrics from the custom training job.
  id: totrans-451
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-18\. 自定义训练作业的指标。
- en: You have successfully trained a model using scikit-learn on Vertex AI. Of course,
    this process likely feels like overkill given the amount of time it took to set
    up and the amount of data you were working with. For working with smaller datasets,
    working locally or in a Colab notebook is a reasonable approach. But, as datasets
    become larger and larger, eventually it becomes more advantageous to take advantage
    of larger pools of resources. In Vertex AI, the same basic process will work when
    working with ever-growing datasets. Now your model is stored as a *.joblib* file
    and ready to load wherever needed for serving predictions.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 您已成功在 Vertex AI 上使用 scikit-learn 训练了一个模型。当然，考虑到设置所需的时间和处理的数据量，这个过程可能会感觉有些繁琐。对于处理较小数据集，本地工作或在
    Colab 笔记本中工作是一个合理的方法。但随着数据集变得越来越大，利用更大资源池的优势变得更有利。在 Vertex AI 中，当处理不断增长的数据集时，同样的基本流程也适用。现在，您的模型存储为一个
    *.joblib* 文件，并准备好在需要时加载以进行服务预测。
- en: 'As an exercise, go through this same process with the Keras model. A few hints:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，使用 Keras 模型重复这个过程。几个提示：
- en: Be sure to use a prebuilt container for TensorFlow. You can check the version
    being used in your Colab notebook by running the command `tf.__version__`.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保使用 TensorFlow 的预构建容器。您可以通过在 Colab 笔记本中运行命令 `tf.__version__` 来检查使用的版本。
- en: The TensorFlow prebuilt images include the *sklearn* package, so you can easily
    reuse your preprocessing code.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 预构建镜像包含 *sklearn* 包，因此您可以轻松重用您的预处理代码。
- en: Instead of using `joblib.dump()` to save the model, TensorFlow models include
    a built-in method, `save()`. Use `model.save()` to store your model in Google
    Cloud Storage.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 模型不再使用 `joblib.dump()` 来保存模型，而是包含一个内置方法 `save()`。使用 `model.save()`
    将您的模型存储在 Google Cloud Storage 中。
- en: Finally, for those who went through the optional section on pipelines in scikit-learn,
    package up the code for the `Pipeline` version of the model code and submit that
    for training to Vertex AI Training.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于那些完成了 scikit-learn 中管道（pipelines）可选部分的人，请将模型代码的 `Pipeline` 版本打包并提交到 Vertex
    AI 训练中进行训练。
- en: For further resources, see the official [Vertex AI Custom Training](https://oreil.ly/A127h)
    documentation.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多资源，请参阅官方 [Vertex AI Custom Training](https://oreil.ly/A127h) 文档。
- en: Summary
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter you learned how to build a custom code model to predict customer
    churn for a telco company. You explored two of the most popular ML frameworks
    in scikit-learn and TensorFlow and built simple classification models in each.
    You then learned how to train your model using the Vertex AI Training service
    on Google Cloud. All the topics you covered in this chapter are simply the tip
    of the iceberg and hopefully serve as a stepping stone into deeper knowledge about
    ML. In the next chapter, you will see how to improve your models using techniques
    such as hyperparameter tuning in BigQuery ML and using custom code in Vertex AI.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何构建自定义代码模型，以预测电信公司的客户流失。您探索了 scikit-learn 和 TensorFlow 中两种最流行的 ML
    框架，并在每种框架中构建了简单的分类模型。然后，您学习了如何使用 Google Cloud 上的 Vertex AI 训练服务来训练您的模型。本章涵盖的所有主题仅仅是冰山一角，希望作为进一步深入了解机器学习的基础。在下一章中，您将看到如何通过诸如
    BigQuery ML 中的超参数调整和在 Vertex AI 中使用自定义代码等技术来改进您的模型。
