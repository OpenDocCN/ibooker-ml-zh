- en: Chapter 10\. Infrastructure and Tooling for MLOps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。MLOps 的基础设施和工具
- en: In Chapters [4](ch04.xhtml#training_data) to [6](ch06.xhtml#model_development_and_offline_evaluatio),
    we discussed the logic for developing ML systems. In Chapters [7](ch07.xhtml#model_deployment_and_prediction_service)
    to [9](ch09.xhtml#continual_learning_and_test_in_producti), we discussed the considerations
    for deploying, monitoring, and continually updating an ML system. Up until now,
    we’ve assumed that ML practitioners have access to all the tools and infrastructure
    they need to implement that logic and carry out these considerations. However,
    that assumption is far from being true. Many data scientists have told me that
    they know the right things to do for their ML systems, but they can’t do them
    because their infrastructure isn’t set up in a way that enables them to do so.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.xhtml#training_data)到[第6章](ch06.xhtml#model_development_and_offline_evaluatio)中，我们讨论了开发机器学习系统的逻辑。在[第7章](ch07.xhtml#model_deployment_and_prediction_service)到[第9章](ch09.xhtml#continual_learning_and_test_in_producti)中，我们讨论了部署、监控和持续更新机器学习系统的考虑因素。直到现在，我们假设机器学习从业者能够获取实施这些逻辑和考虑因素所需的所有工具和基础设施。然而，这一假设远非事实。许多数据科学家告诉我，他们知道为其机器学习系统做正确的事情，但由于他们的基础设施没有以使其能够这样做的方式设置，他们无法做到这一点。
- en: ML systems are complex. The more complex a system, the more it can benefit from
    good infrastructure. Infrastructure, when set up right, can help automate processes,
    reducing the need for specialized knowledge and engineering time. This, in turn,
    can speed up the development and delivery of ML applications, reduce the surface
    area for bugs, and enable new use cases. When set up wrong, however, infrastructure
    is painful to use and expensive to replace. In this chapter, we’ll discuss how
    to set up infrastructure right for ML systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统是复杂的。系统越复杂，良好的基础设施就能带来的好处就越多。正确设置的基础设施可以帮助自动化流程，减少对专业知识和工程时间的需求。这反过来可以加快机器学习应用程序的开发和交付，减少错误的表面积，并支持新的使用案例。然而，如果基础设施设置错误，使用起来将非常痛苦且昂贵。在本章中，我们将讨论如何为机器学习系统正确设置基础设施。
- en: Before we dive in, it’s important to note that every company’s infrastructure
    needs are different. The infrastructure required for you depends on the number
    of applications you develop and how specialized the applications are. At one end
    of the spectrum, you have companies that use ML for ad hoc business analytics
    such as to project the number of new users they’ll have next year to present at
    their quarterly planning meeting. These companies probably won’t need to invest
    in any infrastructure—Jupyter Notebooks, Python, and Pandas would be their best
    friends. If you have only one simple ML use case, such as an Android app for object
    detection to show your friends, you probably won’t need any infrastructure either—you
    just need an Android-compatible ML framework like TensorFlow Lite.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论之前，重要的是要注意，每家公司的基础设施需求各不相同。您所需的基础设施取决于您开发的应用程序数量以及应用程序的专业程度。在光谱的一端，有些公司将机器学习用于临时的业务分析，例如预测他们明年新用户的数量以在季度计划会议上展示。这些公司可能不需要投资任何基础设施——Jupyter
    Notebooks、Python 和 Pandas 将成为它们最好的朋友。如果您只有一个简单的机器学习用例，例如用于对象检测的 Android 应用程序以展示给您的朋友，您可能也不需要任何基础设施——您只需要一个兼容
    Android 的机器学习框架，比如 TensorFlow Lite。
- en: At the other end of the spectrum, there are companies that work on applications
    with unique requirements. For example, self-driving cars have unique accuracy
    and latency requirements—the algorithm must be able to respond within milliseconds
    and its accuracy must be near-perfect since a wrong prediction can lead to serious
    accidents. Similarly, Google Search has a unique scale requirement since most
    companies don’t process 63,000 search queries a second, which translates to 234
    million search queries an hour, like Google does.^([1](ch10.xhtml#ch01fn340))
    These companies will likely need to develop their own highly specialized infrastructure.
    Google developed a large part of their internal infrastructure for search; so
    did self-driving car companies like Tesla and Waymo.^([2](ch10.xhtml#ch01fn341))
    It’s common that part of specialized infrastructure is later made public and adopted
    by other companies. For example, Google extended their internal cloud infrastructure
    to the public, resulting in [Google Cloud Platform](https://oreil.ly/0g02L).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一端，有些公司致力于具有独特需求的应用。例如，自动驾驶汽车有独特的准确性和延迟要求——算法必须能在毫秒内响应，并且其准确性必须接近完美，因为错误的预测可能导致严重事故。类似地，谷歌搜索具有独特的规模需求，因为大多数公司不会像谷歌那样处理每秒63,000个搜索查询，这相当于每小时2.34亿个搜索查询。^([1](ch10.xhtml#ch01fn340))
    这些公司可能需要开发自己的高度专业化基础设施。谷歌为搜索开发了大部分内部基础设施；特斯拉和Waymo等自动驾驶汽车公司也是如此。^([2](ch10.xhtml#ch01fn341))
    通常，专业化基础设施的一部分后来会被公开，并被其他公司采纳。例如，谷歌将其内部云基础设施扩展到公众领域，形成了[Google Cloud Platform](https://oreil.ly/0g02L)。
- en: In the middle of the spectrum are the majority of companies, those who use ML
    for multiple common applications—a fraud detection model, a price optimization
    model, a churn prediction model, a recommender system, etc.—at reasonable scale.
    “Reasonable scale” refers to companies that work with data in the order of gigabytes
    and terabytes, instead of petabytes, a day. Their data science team might range
    from 10 to hundreds of engineers.^([3](ch10.xhtml#ch01fn342)) This category might
    include any company from a 20-person startup to a company at Zillow’s scale, but
    not at FAAAM scale.^([4](ch10.xhtml#ch01fn343)) For example, back in 2018, Uber
    was adding tens of terabytes of data a day to their data lake, and Zillow’s biggest
    dataset was bringing in 2 terabytes of uncompressed data a day.^([5](ch10.xhtml#ch01fn344))
    In contrast, even back in 2014, Facebook was generating *4 petabytes* of data
    a day.^([6](ch10.xhtml#ch01fn345))
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在中等规模的公司中，有大多数使用机器学习应用于多个常见应用领域——欺诈检测模型、价格优化模型、客户流失预测模型、推荐系统等——在合理的规模下。所谓的“合理规模”是指那些每天处理以千兆字节和太字节为单位的数据，而不是百拍字节的公司。他们的数据科学团队可能由10至数百名工程师组成。^([3](ch10.xhtml#ch01fn342))
    这个类别可能包括从20人的初创公司到像Zillow这样的公司，但不包括像FAAAM这样的公司。^([4](ch10.xhtml#ch01fn343)) 例如，回顾2018年，Uber每天向他们的数据湖添加数十太字节的数据，而Zillow最大的数据集每天引入2太字节的未压缩数据。^([5](ch10.xhtml#ch01fn344))
    相比之下，即使在2014年，Facebook每天也生成*4百拍字节*的数据。^([6](ch10.xhtml#ch01fn345))
- en: Companies in the middle of the spectrum will likely benefit from generalized
    ML infrastructure that is being increasingly standardized (see [Figure 10-1](#infrastructure_requirements_for_compani)).
    In this book, we’ll focus on the infrastructure for the vast majority of ML applications
    at a reasonable scale.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 中等规模的公司可能会受益于越来越标准化的通用机器学习基础设施（参见[图 10-1](#infrastructure_requirements_for_compani)）。本书将重点讨论绝大多数中等规模机器学习应用的基础设施。
- en: '![](Images/dmls_1001.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1001.png)'
- en: Figure 10-1\. Infrastructure requirements for companies at different production
    scales
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 不同生产规模公司的基础设施需求
- en: 'In order to set up the right infrastructure for your needs, it’s important
    to understand exactly what infrastructure means and what it consists of. According
    to Wikipedia, in the physical world, “infrastructure is the set of fundamental
    facilities and systems that support the sustainable functionality of households
    and firms.”^([7](ch10.xhtml#ch01fn346)) In the ML world, infrastructure is the
    set of fundamental facilities that support the development and maintenance of
    ML systems. What should be considered the “fundamental facilities” varies greatly
    from company to company, as discussed earlier in this chapter. In this section,
    we will examine the following four layers:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为您的需求设置正确的基础设施，理解基础设施的确切含义及其组成非常重要。根据维基百科，在物理世界中，“基础设施是支持家庭和企业可持续功能的基本设施和系统。”^([7](ch10.xhtml#ch01fn346))
    在机器学习世界中，基础设施是支持机器学习系统开发和维护的基本设施集合。什么被认为是“基本设施”在不同公司之间差异很大，正如本章前面讨论的那样。在本节中，我们将讨论以下四个层次：
- en: Storage and compute
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 存储与计算
- en: The storage layer is where data is collected and stored. The compute layer provides
    the compute needed to run your ML workloads such as training a model, computing
    features, generating features, etc.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 存储层是数据收集和存储的地方。计算层提供运行机器学习工作负载所需的计算资源，例如训练模型、计算特征、生成特征等。
- en: Resource management
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 资源管理
- en: Resource management comprises tools to schedule and orchestrate your workloads
    to make the most out of your available compute resources. Examples of tools in
    this category include Airflow, Kubeflow, and Metaflow.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 资源管理包括调度和编排工具，以充分利用您可用的计算资源。这一类工具的例子包括Airflow、Kubeflow和Metaflow。
- en: ML platform
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ML平台
- en: This provides tools to aid the development of ML applications such as model
    stores, feature stores, and monitoring tools. Examples of tools in this category
    include SageMaker and MLflow.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具提供了帮助开发机器学习应用程序的工具，例如模型存储、特征存储和监控工具。这一类工具的例子包括SageMaker和MLflow。
- en: Development environment
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 开发环境
- en: This is usually referred to as the dev environment; it is where code is written
    and experiments are run. Code needs to be versioned and tested. Experiments need
    to be tracked.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为开发环境；这是编写代码和运行实验的地方。代码需要进行版本管理和测试。实验需要进行跟踪。
- en: These four different layers are shown in [Figure 10-2](#different_layers_of_infrastructure_for).
    Data and compute are the essential resources needed for any ML project, and thus
    the *storage and compute layer* forms the infrastructural foundation for any company
    that wants to apply ML. This layer is also the most abstract to a data scientist.
    We’ll discuss this layer first because these resources are the easiest to explain.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个不同的层次显示在[图10-2](#different_layers_of_infrastructure_for)中。数据和计算是任何机器学习项目所需的基本资源，因此*存储和计算层*构成了任何希望应用机器学习的公司的基础设施基础。对数据科学家来说，这一层次也是最抽象的。我们将首先讨论这一层，因为这些资源最容易解释。
- en: '![](Images/dmls_1002.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1002.png)'
- en: Figure 10-2\. Different layers of infrastructure for ML
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2\. 机器学习基础设施的不同层次
- en: The dev environment is what data scientists have to interact with daily, and
    therefore, it is the least abstract to them. We’ll discuss this category next,
    then we’ll discuss resource management, a contentious topic among data scientists—people
    are still debating whether a data scientist needs to know about this layer or
    not. Because “ML platform” is a relatively new concept with its different components
    still maturing, we’ll discuss this category last, after we’ve familiarized ourselves
    with all other categories. An ML platform requires up-front investment from a
    company, but if it’s done right, it can make the life of data scientists across
    business use cases at that company so much easier.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 开发环境是数据科学家每天必须与之交互的环境，因此对他们来说是最具体的。我们接下来将讨论这一类别，然后我们将讨论资源管理，这是数据科学家中争议较大的话题——人们仍在争论数据科学家是否需要了解这一层。因为“ML平台”是一个相对新的概念，其不同的组件仍在成熟阶段，所以我们将在熟悉了所有其他类别之后，最后讨论这一类别。ML平台需要公司的前期投资，但如果做得正确，它可以大大简化公司各业务用例中数据科学家的工作。
- en: Even if two companies have the exact same infrastructure needs, their resulting
    infrastructure might look different depending on their approaches to build versus
    buy decisions—i.e., what they want to build in-house versus what they want to
    outsource to other companies. We’ll discuss the build versus buy decisions in
    the last part of this chapter, where we’ll also discuss the hope for standardized
    and unified abstractions for ML infrastructure.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 即使两家公司有完全相同的基础设施需求，根据它们在建设与购买决策方面的方法不同，它们得到的基础设施可能看起来会有所不同。我们将在本章的最后部分讨论建设与购买决策，同时也会讨论ML基础设施的标准化和统一抽象的希望。
- en: Let’s dive in!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入讨论吧！
- en: Storage and Compute
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储与计算
- en: ML systems work with a lot of data, and this data needs to be stored somewhere.
    The *storage layer* is where data is collected and stored. At its simplest form,
    the storage layer can be a hard drive disk (HDD) or a solid state disk (SSD).
    The storage layer can be in one place, e.g., you might have all your data in Amazon
    S3 or in Snowflake, or spread out over multiple locations.^([8](ch10.xhtml#ch01fn347))
    Your storage layer can be on-prem in a private data center or on the cloud. In
    the past, companies might have tried to manage their own storage layer. However,
    in the last decade, the storage layer has been mostly commoditized and moved to
    the cloud. Data storage has become so cheap that most companies just store all
    the data they have without the cost.^([9](ch10.xhtml#ch01fn348)) We’ve covered
    the data layer intensively in [Chapter 3](ch03.xhtml#data_engineering_fundamentals),
    so in this chapter, we’ll focus on the compute layer.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ML系统处理大量数据，这些数据需要存储在某个地方。*存储层* 是数据收集和存储的地方。在其最简单的形式下，存储层可以是硬盘驱动器（HDD）或固态硬盘（SSD）。存储层可以在一个地方，例如，你可能把所有数据存储在Amazon
    S3或Snowflake中，或分布在多个位置。^([8](ch10.xhtml#ch01fn347)) 你的存储层可以是在私有数据中心的本地环境中，也可以是云上的。过去，公司可能试图管理自己的存储层。然而，在过去的十年里，存储层大多数已经被商品化并移到了云端。数据存储变得如此便宜，以至于大多数公司都会存储所有他们拥有的数据而不计成本。^([9](ch10.xhtml#ch01fn348))
    我们在[第三章](ch03.xhtml#data_engineering_fundamentals)中已经深入探讨了数据层，因此在本章中，我们将专注于计算层。
- en: The *compute layer* refers to all the compute resources a company has access
    to and the mechanism to determine how these resources can be used. The amount
    of compute resources available determines the scalability of your workloads. You
    can think of the compute layer as the engine to execute your jobs. At its simplest
    form, the compute layer can just be a single CPU core or a GPU core that does
    all your computation. Its most common form is cloud compute managed by a cloud
    provider such as AWS Elastic Compute Cloud (EC2) or GCP.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*计算层* 指的是公司可以访问的所有计算资源及其使用这些资源的机制。可用的计算资源量决定了工作负载的可扩展性。你可以把计算层看作是执行作业的引擎。在其最简单的形式下，计算层可以仅是一个单独的CPU核心或GPU核心来执行所有的计算任务。它最常见的形式是由云提供商管理的云计算，如AWS
    Elastic Compute Cloud (EC2)或GCP。'
- en: The compute layer can usually be sliced into smaller compute units to be used
    concurrently. For example, a CPU core might support two concurrent threads; each
    thread is used as a compute unit to execute its own job. Or multiple CPU cores
    might be joined together to form a larger compute unit to execute a larger job.
    A compute unit can be created for a specific short-lived job such as an AWS Step
    Function or a GCP Cloud Run—the unit will be eliminated after the job finishes.
    A compute unit can also be created to be more “permanent,” aka without being tied
    to a job, like a virtual machine. A more permanent compute unit is sometimes called
    an “instance.”
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 计算层通常可以分割成较小的计算单元以供并发使用。例如，一个CPU核心可以支持两个并发线程；每个线程作为计算单元执行自己的作业。或者多个CPU核心可以组合在一起形成一个更大的计算单元来执行更大的作业。可以为特定的短暂作业（例如AWS
    Step Function或GCP Cloud Run）创建一个计算单元——该单元在作业完成后将被清除。也可以创建一个更“永久”的计算单元，例如虚拟机，不绑定于任何作业。更“永久”的计算单元有时被称为“实例”。
- en: However, the compute layer doesn’t always use threads or cores as compute units.
    There are compute layers that abstract away the notions of cores and use other
    units of computation. For example, computation engines like Spark and Ray use
    “job” as their unit, and Kubernetes uses “pod,” a wrapper around containers, as
    its smallest deployable unit. While you can have multiple containers in a pod,
    you can’t independently start or stop different containers in the same pod.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，计算层并不总是使用线程或核心作为计算单元。有些计算层抽象了核心的概念，并使用其他的计算单元。例如，像 Spark 和 Ray 这样的计算引擎使用“作业”作为它们的计算单元，而
    Kubernetes 使用“Pod”作为最小的可部署单元，它是容器的一种封装。虽然你可以在一个 Pod 中有多个容器，但你不能独立地启动或停止同一个 Pod
    中的不同容器。
- en: 'To execute a job, you first need to load the required data into your compute
    unit’s memory, then execute the required operations—addition, multiplication,
    division, convolution, etc.—on that data. For example, to add two arrays, you
    will first need to load these two arrays into memory, and then perform addition
    on the two arrays. If the compute unit doesn’t have enough memory to load these
    two arrays, the operation will be impossible without an algorithm to handle out-of-memory
    computation. Therefore, a compute unit is mainly characterized by two metrics:
    how much memory it has and how fast it runs an operation.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行一个作业，你首先需要将所需的数据加载到计算单元的内存中，然后执行所需的操作——如加法、乘法、除法、卷积等。例如，要对两个数组进行加法，你首先需要将这两个数组加载到内存中，然后在这两个数组上执行加法运算。如果计算单元的内存不足以加载这两个数组，那么在没有处理内存溢出的算法的情况下，这个操作将是不可能的。因此，一个计算单元主要通过两个度量来进行特征化：它的内存容量以及它运行操作的速度。
- en: 'The memory metric can be specified using units like GB, and it’s generally
    straightforward to evaluate: a compute unit with 8 GB of memory can handle more
    data in memory than a compute unit with only 2 GB, and it is generally more expensive.^([10](ch10.xhtml#ch01fn349))
    Some companies care not only how much memory a compute unit has but also how fast
    it is to load data in and out of memory, so some cloud providers advertise their
    instances as having “high bandwidth memory” or specify their instances’ I/O bandwidth.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 内存度量可以使用 GB 等单位来指定，通常很容易评估：一个具有 8 GB 内存的计算单元可以在内存中处理比只有 2 GB 内存的计算单元更多的数据，并且通常更昂贵。一些公司不仅关心计算单元的内存容量，还关心加载数据进出内存的速度，因此一些云服务提供商宣传他们的实例具有“高带宽内存”或指定他们实例的
    I/O 带宽。
- en: The operation speed is more contentious. The most common metric is FLOPS—floating
    point operations per second. As the name suggests, this metric denotes the number
    of float point operations a compute unit can run per second. You might see a hardware
    vendor advertising that their GPUs or TPUs or IPUs (intelligence processing units)
    have teraFLOPS (one trillion FLOPS) or another massive number of FLOPS.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 运算速度是一个更具争议性的问题。最常见的度量标准是 FLOPS（每秒浮点运算次数）。顾名思义，这个度量标准表示计算单元每秒可以运行的浮点运算次数。你可能会看到硬件供应商宣传他们的
    GPU、TPU 或 IPU（智能处理单元）拥有 TeraFLOPS（一万亿 FLOPS）或其他大量 FLOPS 的数据。
- en: However, this metric is contentious because, first, companies that measure this
    metric might have different ideas on what is counted as an operation. For example,
    if a machine fuses two operations into one and executes this fused operation,^([11](ch10.xhtml#ch01fn350))
    does this count as one operation or two? Second, just because a compute unit is
    capable of doing a trillion FLOPS doesn’t mean you’ll be able to execute your
    job at the speed of a trillion FLOPS. The ratio of the number of FLOPS a job can
    run to the number of FLOPs a compute unit is capable of handling is called utilization.^([12](ch10.xhtml#custom_ch10n1))
    If an instance is capable of doing a million FLOPs and your job runs with 0.3
    million FLOPS, that’s a 30% utilization rate. Of course, you’d want to have your
    utilization rate as high as possible. However, it’s near impossible to achieve
    100% utilization rate. Depending on the hardware backend and the application,
    the utilization rate of 50% might be considered good or bad. Utilization also
    depends on how fast you can load data into memory to perform the next operations—hence
    the importance of I/O bandwidth.^([13](ch10.xhtml#ch01fn351))
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个度量标准是有争议的，首先，衡量此度量标准的公司可能对什么算作一个操作有不同的理解。例如，如果一台机器将两个操作融合成一个并执行这个融合操作，^([11](ch10.xhtml#ch01fn350))
    这算作一个操作还是两个操作？其次，仅仅因为一个计算单元能够执行一万亿次 FLOPS，并不意味着你能以一万亿 FLOPS 的速度执行你的作业。作业可以运行的
    FLOPS 数量与计算单元能够处理的 FLOPS 数量的比率被称为利用率。^([12](ch10.xhtml#custom_ch10n1)) 如果一个实例能够做一百万次
    FLOPS，而你的作业以 0.3 百万次 FLOPS 运行，那就是 30% 的利用率。当然，你希望将利用率尽可能提高，但是几乎不可能达到 100% 的利用率。根据硬件后端和应用程序，50%
    的利用率可能被认为是好的或者是坏的。利用率还取决于你能够多快地将数据加载到内存中以执行下一个操作——这也解释了 I/O 带宽的重要性。^([13](ch10.xhtml#ch01fn351))
- en: When evaluating a new compute unit, it’s important to evaluate how long it will
    take this compute unit to do common workloads. For example, [MLPerf](https://oreil.ly/XuVka)
    is a popular benchmark for hardware vendors to measure their hardware performance
    by showing how long it will take their hardware to train a ResNet-50 model on
    the ImageNet dataset or use a BERT-large model to generate predictions for the
    SQuAD dataset.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估新的计算单元时，评估这个计算单元完成常见工作负载所需的时间是很重要的。例如，[MLPerf](https://oreil.ly/XuVka) 是一个流行的基准测试，用于硬件供应商衡量其硬件性能，展示其硬件在
    ImageNet 数据集上训练 ResNet-50 模型或使用 BERT-large 模型生成 SQuAD 数据集预测所需的时间。
- en: Because thinking about FLOPS is not very useful, to make things easier, when
    evaluating compute performance, many people just look into the number of cores
    a compute unit has. So you might use an instance with 4 CPU cores and 8 GB of
    memory. Keep in mind that AWS uses the concept of vCPU, which stands for virtual
    CPU and which, for practical purposes, can be thought of as half a physical core.^([14](ch10.xhtml#ch01fn352))
    You can see the number of cores and memory offered by some AWS EC2 and GCP instances
    in [Figure 10-3](#examples_of_gpu_and_tpu_instances_avail).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 想到 FLOPS 并不是很有用，为了简化事情，在评估计算性能时，很多人只关注计算单元的核心数。因此，你可能会选择一个有 4 个 CPU 核心和 8 GB
    内存的实例。请记住，AWS 使用 vCPU 的概念，代表虚拟 CPU，对于实际目的而言，可以视为半个物理核心。^([14](ch10.xhtml#ch01fn352))
    你可以查看一些 AWS EC2 和 GCP 实例提供的核心数和内存情况，请参见 [Figure 10-3](#examples_of_gpu_and_tpu_instances_avail)。
- en: '![](Images/dmls_1003.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1003.png)'
- en: 'Figure 10-3\. Examples of GPU and TPU instances available on AWS and GCP as
    of February 2022\. Source: Screenshots of AWS and GCP websites'
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. AWS 和 GCP 上可用的 GPU 和 TPU 实例示例（截至 2022 年 2 月）。来源：AWS 和 GCP 网站的截图
- en: Public Cloud Versus Private Data Centers
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公共云与私有数据中心
- en: Like data storage, the compute layer is largely commoditized. This means that
    instead of setting up their own data centers for storage and compute, companies
    can pay cloud providers like AWS and Azure for the exact amount of compute they
    use. Cloud compute makes it extremely easy for companies to start building without
    having to worry about the compute layer. It’s especially appealing to companies
    that have variable-sized workloads. Imagine if your workloads need 1,000 CPU cores
    one day of the year and only 10 CPU cores the rest of the year. If you build your
    own data centers, you’ll need to pay for 1,000 CPU cores up front. With cloud
    compute, you only need to pay for 1,000 CPU cores one day of the year and 10 CPU
    cores the rest of the year. It’s convenient to be able to just add more compute
    or shut down instances as needed—most cloud providers even do that automatically
    for you—reducing engineering operational overhead. This is especially useful in
    ML as data science workloads are bursty. Data scientists tend to run experiments
    a lot for a few weeks during development, which requires a surge of compute power.
    Later on, during production, the workload is more consistent.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据存储类似，计算层主要是商品化的。这意味着公司可以支付像AWS和Azure这样的云服务提供商，按照他们实际使用的计算量付费，而不是为存储和计算设立自己的数据中心。云计算使得公司能够轻松开始构建而无需担心计算层。对于那些工作负载大小可变的公司来说尤为吸引。想象一下，如果你的工作负载一年中有一天需要1000个CPU核心，而其余时间只需要10个CPU核心。如果建立自己的数据中心，你需要一开始就支付1000个CPU核心的费用。而使用云计算，你只需在一年中的一天支付1000个CPU核心的费用，其余时间支付10个CPU核心的费用。能够根据需要添加更多计算资源或关闭实例非常便利，大多数云提供商甚至可以自动执行这些操作，从而减少工程运营的开销。这在机器学习中特别有用，因为数据科学工作负载是爆发性的。在开发过程中，数据科学家倾向于连续几周运行实验，这需要大量计算能力。在生产阶段，工作负载则更加稳定。
- en: Keep in mind that cloud compute is elastic but not magical. It doesn’t actually
    have infinite compute. Most cloud providers offer [limits](https://oreil.ly/TzUOv)
    on the compute resources you can use at a time. Some, but not all, of these limits
    can be raised through petitions. For example, as of writing this book, AWS EC2’s
    largest instance is [X1e](https://oreil.ly/29lsT) with 128 vCPUs and almost 4
    TB of memory.^([15](ch10.xhtml#ch01fn353)) Having a lot of compute resources doesn’t
    mean that it’s always easy to use them, especially if you have to work with spot
    instances to save cost.^([16](ch10.xhtml#ch01fn354))
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，云计算是弹性的，但并非神奇。它实际上并没有无限的计算能力。大多数云服务提供商在同时使用计算资源方面都设有[限制](https://oreil.ly/TzUOv)。其中一些限制可以通过申请来解除。例如，截至撰写本书时，AWS
    EC2的最大实例是[X1e](https://oreil.ly/29lsT)，拥有128个vCPU和将近4TB内存。^([15](ch10.xhtml#ch01fn353))
    拥有大量计算资源并不意味着始终能轻松使用它们，尤其是在需要利用抢占式实例节省成本的情况下。^([16](ch10.xhtml#ch01fn354))
- en: Due to the cloud’s elasticity and ease of use, more and more companies are choosing
    to pay for the cloud over building and maintaining their own storage and compute
    layer. Synergy Research Group’s research shows that in 2020, “enterprise spending
    on cloud infrastructure services [grew] by 35% to reach almost $130 billion” while
    “enterprise spending on data [centers] dropped by 6% to under $90 billion,”^([17](ch10.xhtml#ch01fn355))
    as shown in [Figure 10-4](#in_twozerotwozerocomma_enterprise_spend).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于云计算的弹性和易用性，越来越多的公司选择支付云服务费用，而不是建设和维护自己的存储和计算层。Synergy Research Group的研究显示，2020年，“企业在云基础设施服务上的支出增长了35%，达到了近1300亿美元”，而“数据中心的企业支出下降了6%，不足900亿美元”，如[图10-4](#in_twozerotwozerocomma_enterprise_spend)所示。^([17](ch10.xhtml#ch01fn355))
- en: '![](Images/dmls_1004.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1004.png)'
- en: 'Figure 10-4\. In 2020, enterprise spending on cloud infrastructure services
    grew by 35% while spending on data centers dropped by 6%. Source: Adapted from
    an image by Synergy Research Group'
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4。2020年，企业在云基础设施服务上的支出增长了35%，而在数据中心的支出下降了6%。资料来源：根据Synergy Research Group的图片调整。
- en: While leveraging the cloud tends to give companies higher returns than building
    their own storage and compute layers early on, this becomes less defensible as
    a company grows. Based on disclosed cloud infrastructure spending by public software
    companies, the venture capital firm a16z shows that cloud spending accounts for
    approximately 50% cost of revenue of these companies.^([18](ch10.xhtml#ch01fn356))
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管利用云计算在公司初期比自建存储和计算层带来更高的回报，但随着公司规模的增长，这种优势变得不那么具有防御性。根据公开软件公司披露的云基础设施支出，风险投资公司a16z指出，云计算支出约占这些公司**营业成本的50%**。^([18](ch10.xhtml#ch01fn356))
- en: The high cost of the cloud has prompted companies to start moving their workloads
    back to their own data centers, a process called “cloud repatriation.” [Dropbox’s
    S-1 filing in 2018](https://oreil.ly/zRm9j) shows that the company was able to
    save $75M over the two years prior to IPO due to their infrastructure optimization
    overhaul—a large chunk of it consisted of moving their workloads from public cloud
    to their own data centers. Is the high cost of cloud unique to Dropbox because
    Dropbox is in the data storage business? Not quite. In the aforementioned analysis,
    a16z estimated that “across 50 of the top public software companies currently
    utilizing cloud infrastructure, an estimated $100B of market value is being lost
    among them due to cloud impact on margins—relative to running the infrastructure
    themselves.”^([19](ch10.xhtml#ch01fn357))
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 云端的高成本促使公司开始将其工作负载迁移回自己的数据中心，这个过程被称为“云端收归”。[Dropbox 在 2018 年的 S-1 文件](https://oreil.ly/zRm9j)
    显示，该公司通过基础设施优化大规模改革，节省了 IPO 前两年的 7500 万美元——其中大部分是将其工作负载从公共云迁移到自己的数据中心。云端的高成本是否是独特于
    Dropbox，因为 Dropbox 是数据存储业务？并非完全如此。在上述分析中，a16z 估计，“在当前利用云基础设施的 50 家顶级公共软件公司中，由于云端对利润率的影响，它们的市值损失达到了约
    1000 亿美元。”^([19](ch10.xhtml#ch01fn357))
- en: 'While getting started with the cloud is easy, moving away from the cloud is
    hard. Cloud repatriation requires nontrivial up-front investment in both commodities
    and engineering effort. More and more companies are following a hybrid approach:
    keeping most of their workloads on the cloud but slowly increasing their investment
    in data centers.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管开始使用云端服务很容易，但是远离云端却很困难。云端收归需要在商品和工程工作上做出重大的前期投资。越来越多的公司正在采取混合方法：将大部分工作负载保留在云端，但逐渐增加在数据中心的投资。
- en: Development Environment
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发环境
- en: 'The dev environment is where ML engineers write code, run experiments, and
    interact with the production environment where champion models are deployed and
    challenger models evaluated. The dev environment consists of the following components:
    IDE (integrated development environment), versioning, and CI/CD.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 开发环境是机器学习工程师编写代码、运行实验并与部署冠军模型和评估挑战者模型的生产环境进行交互的地方。开发环境包括以下组件：集成开发环境（IDE）、版本控制和持续集成/持续交付（CI/CD）。
- en: If you’re a data scientist or ML engineer who writes code daily, you’re probably
    very familiar with all these tools and might wonder what there is to say about
    them. In my experience, outside of a handful of tech companies, the dev environment
    is severely underrated and underinvested in at most companies. According to Ville
    Tuulos in his book *Effective Data Science Infrastructure*, “you would be surprised
    to know how many companies have well-tuned, scalable production infrastructure
    but the question of how the code is developed, debugged, and tested in the first
    place is solved in an ad-hoc manner.”^([22](ch10.xhtml#idm46868206610032))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一名每天编写代码的数据科学家或机器学习工程师，你可能对所有这些工具非常熟悉，可能会想知道还有什么可以说的。根据 Ville Tuulos 在他的书
    *Effective Data Science Infrastructure* 中的说法，“你会惊讶地知道有多少公司拥有良好调优、可扩展的生产基础设施，但在开发、调试和测试代码的问题上却采取了临时性的解决方案。”^([22](ch10.xhtml#idm46868206610032))
- en: He suggested that “if you have time to set up only one piece of infrastructure
    well, make it the development environment for data scientists.” Because the dev
    environment is where engineers work, improvements in the dev environment translate
    directly into improvements in engineering productivity.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 他建议，“如果你只有时间来好好设置一个基础设施，那就让它成为数据科学家的开发环境。” 因为开发环境是工程师工作的地方，开发环境的改进直接转化为工程生产力的提升。
- en: In this section, we’ll first cover different components of the dev environment,
    then we’ll discuss the standardization of the dev environment before we discuss
    how to bring your changes from the dev environment to the production environment
    with containers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先将介绍开发环境的不同组件，然后讨论开发环境的标准化，最后再讨论如何通过容器将您的更改从开发环境带入生产环境。
- en: Dev Environment Setup
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发环境设置
- en: The dev environment should be set up to contain all the tools that can make
    it easier for engineers to do their job. It should also consist of tools for *versioning*.
    As of this writing, companies use an ad hoc set of tools to version their ML workflows,
    such as Git to version control code, DVC to version data, Weights & Biases or
    Comet.ml to track experiments during development, and MLflow to track artifacts
    of models when deploying them. Claypot AI is working on a platform that can help
    you version and track all your ML workflows in one place. Versioning is important
    for any software engineering projects, but even more so for ML projects because
    of both the sheer number of things you can change (code, parameters, the data
    itself, etc.) and the need to keep track of prior runs to reproduce later on.
    We’ve covered this in the section [“Experiment Tracking and Versioning”](ch06.xhtml#experiment_tracking_and_versioning).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 开发环境应该设置包含所有可以使工程师工作更轻松的工具。它还应包括用于*版本控制*的工具。截至撰写本文时，公司使用一套特定的工具来对其ML工作流进行版本控制，例如使用Git进行代码版本控制，使用DVC对数据进行版本控制，使用Weights
    & Biases或Comet.ml来跟踪开发过程中的实验，以及使用MLflow来跟踪模型部署时的工件。Claypot AI正在开发一个平台，可以帮助您在一个地方版本化和跟踪所有的ML工作流。对于任何软件工程项目来说，版本控制都非常重要，但对于ML项目来说更是如此，因为您可以改变的东西（代码、参数、数据本身等）数量庞大，并且需要跟踪以复制后续运行的先前运行。我们在章节[“实验跟踪和版本控制”](ch06.xhtml#experiment_tracking_and_versioning)中已经讨论过这一点。
- en: The dev environment should also be set up with a *CI/CD* test suite to test
    your code before pushing it to the staging or production environment. Examples
    of tools to orchestrate your CI/CD test suite are GitHub Actions and CircleCI.
    Because CI/CD is a software engineering concern, it’s beyond the scope of this
    book.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 开发环境还应该设置一个*CI/CD*测试套件，以在推送到暂存或生产环境之前测试您的代码。用于编排您的CI/CD测试套件的工具示例包括GitHub Actions和CircleCI。因为CI/CD是一个软件工程问题，它超出了本书的范围。
- en: 'In this section, we’ll focus on the place where engineers write code: the IDE.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将专注于工程师编写代码的地方：IDE。
- en: IDE
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IDE
- en: The *IDE* is the editor where you write your code. IDEs tend to support multiple
    programming languages. IDEs can be native apps like VS Code or Vim. IDEs can be
    browser-based, which means they run in browsers, such as AWS Cloud9.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*IDE* 是您编写代码的编辑器。IDE通常支持多种编程语言。IDE可以是像VS Code或Vim这样的本地应用程序。IDE也可以是基于浏览器的，这意味着它们在浏览器中运行，比如AWS
    Cloud9。'
- en: Many data scientists write code not just in IDEs but also in notebooks like
    Jupyter Notebooks and Google Colab.^([23](ch10.xhtml#ch01fn360)) Notebooks are
    more than just places to write code. You can include arbitrary artifacts such
    as images, plots, data in nice tabular formats, etc., which makes notebooks very
    useful for exploratory data analysis and analyzing model training results.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据科学家不仅在IDE中编写代码，还在Jupyter Notebooks和Google Colab这样的笔记本中编写代码。^([23](ch10.xhtml#ch01fn360))
    笔记本不仅仅是编写代码的地方。您可以包含任意的工件，如图像、图表、以及漂亮的表格格式的数据等，这使得笔记本在探索性数据分析和分析模型训练结果方面非常有用。
- en: 'Notebooks have a nice property: they are stateful—they can retain states after
    runs. If your program fails halfway through, you can rerun from the failed step
    instead of having to run the program from the beginning. This is especially helpful
    when you have to deal with large datasets that might take a long time to load.
    With notebooks, you only need to load your data once—notebooks can retain this
    data in memory—instead of having to load it each time you want to run your code.
    As shown in [Figure 10-5](#in_jupyter_notebookscomma_if_step_four), if your code
    fails at step 4 in a notebook, you’ll only need to rerun step 4 instead of from
    the beginning of your program.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本有一个很好的特性：它们是有状态的——在运行后可以保留状态。如果您的程序中途失败，您可以重新从失败的步骤运行，而不必重新从头运行程序。这在处理可能需要长时间加载的大型数据集时特别有帮助。使用笔记本，您只需加载数据一次——笔记本可以在内存中保留这些数据——而不必每次运行代码时都加载它。如图[10-5](#in_jupyter_notebookscomma_if_step_four)，如果您在笔记本的步骤4中代码失败，您只需重新运行步骤4，而不是从程序开始处重新运行。
- en: '![](Images/dmls_1005.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1005.png)'
- en: Figure 10-5\. In Jupyter Notebooks, if step 4 fails, you only need to run step
    4 again, instead of having to run steps 1 to 4 again
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5\. 在Jupyter Notebooks中，如果步骤4失败了，您只需再次运行步骤4，而不是必须重新运行步骤1到4。
- en: Note that this statefulness can be a double-edged sword, as it allows you to
    execute your cells out of order. For example, in a normal script, cell 4 must
    run after cell 3 and cell 3 must run after cell 2\. However, in notebooks, you
    can run cell 2, 3, then 4 or cell 4, 3, then 2\. This makes notebook reproducibility
    harder unless your notebook comes with an instruction on the order in which to
    run your cells. This difficulty is captured in a joke by Chris Albon (see [Figure 10-6](#notebooksapostrophe_statefulness_allows)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种状态性可能是一把双刃剑，因为它允许您无序执行单元格。例如，在普通脚本中，单元格4必须在单元格3之后运行，而单元格3必须在单元格2之后运行。然而，在笔记本中，您可以按顺序运行单元格2、3、然后4，或者单元格4、3、然后2。这使得笔记本的可重现性更加困难，除非您的笔记本附带运行单元格顺序的说明。克里斯·奥尔本通过一个笑话捕捉到了这种困难（参见[图10-6](#notebooksapostrophe_statefulness_allows)）。
- en: '![](Images/dmls_1006.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1006.png)'
- en: Figure 10-6\. Notebooks’ statefulness allows you to execute cells out of order,
    making it hard to reproduce a notebook
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6\. 笔记本的状态性允许您无序执行单元格，使得重现笔记本变得困难
- en: 'Because notebooks are so useful for data exploration and experiments, notebooks
    have become an indispensable tool for data scientists and ML. Some companies have
    made notebooks the center of their data science infrastructure. In their seminal
    article, “Beyond Interactive: Notebook Innovation at Netflix,” Netflix included
    a list of infrastructure tools that can be used to make notebooks even more powerful.^([24](ch10.xhtml#ch01fn361))
    The list includes:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于笔记本在数据探索和实验中非常有用，它们已经成为数据科学家和机器学习不可或缺的工具。一些公司已经将笔记本作为其数据科学基础设施的核心。在他们的重要文章《超越互动：Netflix的笔记本创新》中，Netflix包括了一系列基础设施工具，可以用来使笔记本更加强大。^([24](ch10.xhtml#ch01fn361))
    列表包括：
- en: '[Papermill](https://oreil.ly/569ot)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[Papermill](https://oreil.ly/569ot)'
- en: For spawning multiple notebooks with different parameter sets—such as when you
    want to run different experiments with different sets of parameters and execute
    them concurrently. It can also help summarize metrics from a collection of notebooks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成具有不同参数集的多个笔记本——例如，当您希望使用不同参数集运行不同实验并并行执行时。它还可以帮助总结一系列笔记本的指标。
- en: '[Commuter](https://oreil.ly/dFlYV)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[Commuter](https://oreil.ly/dFlYV)'
- en: A notebook hub for viewing, finding, and sharing notebooks within an organization.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一个笔记本中心，用于在组织内查看、查找和共享笔记本。
- en: Another interesting project aimed at improving the notebook experience is [nbdev](https://nbdev.fast.ai),
    a library on top of Jupyter Notebooks that encourages you to write documentation
    and tests in the same place.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个旨在改善笔记本体验的有趣项目是[nbdev](https://nbdev.fast.ai)，这是一个建立在Jupyter Notebooks之上的库，鼓励您在同一地方编写文档和测试。
- en: Standardizing Dev Environments
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化开发环境
- en: The first thing about the dev environment is that it should be standardized,
    if not company-wide, then at least team-wide. We’ll go over a story to understand
    what it means to have the dev environment standardized and why that is needed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 关于开发环境的第一件事是，它应该是标准化的，如果不是公司范围内，至少应该是团队范围内。我们将讲述一个故事来理解什么是标准化开发环境以及为什么需要它。
- en: In the early days of our startup, we each worked from our own computer. We had
    a bash file that a new team member could run to create a new virtual environment—in
    our case, we use conda for virtual environments—and install the required packages
    needed to run our code. The list of the required packages was the good old *requirements.txt*
    that we kept adding to as we started using a new package. Sometimes, one of us
    got lazy and we just added a package name (e.g., `torch`) without specifying which
    version of the package it was (e.g., `torch==1.10.0+cpu`). Occasionally, a new
    pull request would run well on my computer but not another coworker’s computer,^([25](ch10.xhtml#ch01fn362))
    and we usually quickly figured out that it was because we used different versions
    of the same package. We resolved to always specify the package name together with
    the package version when adding a new package to the *requirements.txt*, and that
    removed a lot of unnecessary headaches.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创业初期，每个人都从自己的电脑上工作。我们有一个bash文件，新团队成员可以运行它来创建一个新的虚拟环境——在我们的情况下，我们使用conda来创建虚拟环境——并安装运行我们代码所需的包。所需包的列表是我们不断添加的旧*requirements.txt*。有时候，我们中的某个人会懒惰，只是添加了一个包的名称（例如`torch`），而没有指定包的版本（例如`torch==1.10.0+cpu`）。偶尔，一个新的拉取请求在我的电脑上运行良好，但在另一个同事的电脑上却不行，^([25](ch10.xhtml#ch01fn362))
    我们通常很快就发现，这是因为我们使用了不同版本的同一个包。我们决定，每次向*requirements.txt*添加新包时，总是同时指定包名称和包版本，这样就消除了很多不必要的头疼。
- en: One day, we ran into this weird bug that only happened during some runs and
    not others. I asked my coworker to look into it, but he wasn’t able to reproduce
    the bug. I told him that the bug only happened some of the time, so he might have
    to run the code around 20 times just to be sure. He ran the code 20 times and
    still found nothing. We compared our packages and everything matched. After a
    few hours of hair-pulling frustration, we discovered that it was a concurrency
    issue that is only an issue for Python version 3.8 or earlier. I had Python 3.8
    and my coworker had Python 3.9, so he didn’t see the bug. We resolved to have
    everyone on the same Python version, and that removed some more headaches.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有一天，我们遇到了一个奇怪的bug，只在某些运行时出现而不在其他运行时出现。我让我的同事去调查，但他无法重现这个bug。我告诉他这个bug只会偶尔出现，所以他可能需要运行代码大约20次来确认。他运行了20次代码，仍然没有发现任何问题。我们比较了我们的包，一切都匹配。经过几个小时的令人头痛的挫折后，我们发现这是一个并发问题，只有在Python版本3.8或更早版本中才会出现问题。我使用的是Python
    3.8，而我的同事使用的是Python 3.9，所以他没有看到这个bug。我们决定让所有人都使用相同的Python版本，这消除了更多的头痛。
- en: Then one day, my coworker got a new laptop. It was a MacBook with the then new
    M1 chip. He tried to follow our setup steps on this new laptop but ran into difficulty.
    It was because the M1 chip was new, and some of the tools we used, including Docker,
    weren’t working well with M1 chips yet. After seeing him struggling with setting
    the environment up for a day, we decided to move to a cloud dev environment. This
    means that we still standardize the virtual environment and tools and packages,
    but now everyone uses the virtual environment and tools and packages on the same
    type of machine too, provided by a cloud provider.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后有一天，我的同事换了一台新笔记本电脑。那是一台搭载当时新款M1芯片的MacBook。他试图在这台新笔记本电脑上按照我们的设置步骤进行设置，但遇到了困难。那是因为M1芯片是新的，而我们使用的一些工具，包括Docker，与M1芯片的兼容性还不够好。在看到他一整天都在为设置环境而苦苦挣扎后，我们决定转向云开发环境。这意味着我们仍然标准化虚拟环境、工具和包，但现在每个人都使用由云提供商提供的相同类型的虚拟环境、工具和包。
- en: When using a cloud dev environment, you can use a cloud dev environment that
    also comes with a cloud IDE like [AWS Cloud9](https://oreil.ly/xFEZx) (which has
    no built-in notebooks) and [Amazon SageMaker Studio](https://oreil.ly/m1yFZ) (which
    comes with hosted JupyterLab). As of writing this book, Amazon SageMaker Studio
    seems more widely used than Cloud9\. However, most engineers I know who use cloud
    IDEs do so by installing IDEs of their choice, like Vim, on their cloud instances.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用云开发环境时，您可以选择具备云IDE的云开发环境，如[AWS Cloud9](https://oreil.ly/xFEZx)（不含内置笔记本）和[Amazon
    SageMaker Studio](https://oreil.ly/m1yFZ)（带有托管的JupyterLab）。截至撰写本书时，Amazon SageMaker
    Studio似乎比Cloud9更广泛使用。然而，我认识的大多数使用云IDE的工程师是通过在他们的云实例上安装喜爱的IDE（如Vim）来使用的。
- en: A much more popular option is to use a cloud dev environment with a local IDE.
    For example, you can use VS Code installed on your computer and connect the local
    IDE to the cloud environment using a secure protocol like Secure Shell (SSH).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 更受欢迎的选项是使用具备本地集成开发环境的云开发环境。例如，您可以在计算机上安装VS Code，并通过安全协议如SSH将本地IDE连接到云环境。
- en: While it’s generally agreed upon that tools and packages should be standardized,
    some companies are hesitant to standardize IDEs. Engineers can get emotionally
    attached to IDEs, and some have gone to great length to defend their IDE of choice,^([26](ch10.xhtml#ch01fn363))
    so it’ll be hard forcing everyone to use the same IDE. However, over the years,
    some IDEs have emerged to be the most popular. Among them, VS Code is a good choice
    since it allows easy integration with cloud dev instances.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一般认为工具和包应该标准化，但一些公司对标准化IDE持怀疑态度。工程师可能会对IDE产生情感依恋，并有人竭力捍卫他们所选择的IDE^([26](ch10.xhtml#ch01fn363))，因此强迫每个人使用同一种IDE是很难的。然而，多年来，一些IDE已经成为最受欢迎的选择。其中，VS
    Code是一个不错的选择，因为它可以轻松集成云开发实例。
- en: At our startup, we chose [GitHub Codespaces](https://oreil.ly/bQdUW) as our
    cloud dev environment, but an AWS EC2 or a GCP instance that you can SSH into
    is also a good option. Before moving to cloud environments, like many other companies,
    we were worried about the cost—what if we forgot to shut down our instances when
    not in use and they kept charging us money? However, this worry has gone away
    for two reasons. First, tools like GitHub Codespaces automatically shut down your
    instance after 30 minutes of inactivity. Second, some instances are pretty cheap.
    For example, an AWS instance with 4 vCPUs and 8 GB of memory costs around $0.1/hour,
    which comes to approximately $73/month if you never shut it down. Because engineering
    time is expensive, if a cloud dev environment can help you save a few hours of
    engineering time a month, it’s worth it for many companies.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的创业公司，我们选择了[GitHub Codespaces](https://oreil.ly/bQdUW)作为我们的云开发环境，但AWS EC2或可以通过SSH访问的GCP实例也是一个不错的选择。在转移到云环境之前，和许多其他公司一样，我们担心成本——如果我们忘记在不使用时关闭实例会怎么样？然而，这种担忧因为两个原因而消失了。首先，像GitHub
    Codespaces这样的工具在30分钟不活动后会自动关闭你的实例。其次，有些实例价格非常便宜。例如，一个具有4个vCPU和8GB内存的AWS实例每小时约$0.1，如果你从不关闭它，一个月的费用大约为$73。因为工程时间很宝贵，如果云开发环境可以帮助你每月节省几小时的工程时间，对许多公司来说是值得的。
- en: Moving from local dev environments to cloud dev environments has many other
    benefits. First, it makes IT support so much easier—imagine having to support
    1,000 different local machines instead of having to support only one type of cloud
    instance. Second, it’s convenient for remote work—you can just SSH into your dev
    environment wherever you go from any computer. Third, cloud dev environments can
    help with security. For example, if an employee’s laptop is stolen, you can just
    revoke access to cloud instances from that laptop to prevent the thief from accessing
    your codebase and proprietary information. Of course, some companies might not
    be able to move to cloud dev environments also because of security concerns. For
    example, they aren’t allowed to have their code or data on the cloud.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从本地开发环境迁移到云开发环境还有很多其他好处。首先，它极大地简化了IT支持——想象一下，要支持1000台不同的本地机器，而不是只需要支持一种云实例。其次，它对于远程工作非常方便——你可以从任何计算机上通过SSH访问你的开发环境。第三，云开发环境可以帮助提升安全性。例如，如果某位员工的笔记本被盗，你可以立即取消该笔记本对云实例的访问权限，以防止窃贼访问你的代码库和专有信息。当然，一些公司可能由于安全顾虑而无法转移到云开发环境。例如，他们不允许将他们的代码或数据存储在云上。
- en: The fourth benefit, which I would argue is the biggest benefit for companies
    that do production on the cloud, is that having your dev environment on the cloud
    reduces the gap between the dev environment and the production environment. If
    your production environment is in the cloud, bringing your dev environment to
    the cloud is only natural.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个好处，我认为对于在云上进行生产的公司来说是最大的好处，就是将开发环境放在云端可以缩小开发环境和生产环境之间的差距。如果你的生产环境在云端，将开发环境移到云端就是理所当然的事情。
- en: Occasionally, a company has to move their dev environments to the cloud not
    only because of the benefits, but also out of necessity. For the use cases where
    data can’t be downloaded or stored on a local machine, the only way to access
    it is via a notebook in the cloud (SageMaker Studio) that can read the data from
    S3, provided it has the right permissions.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 偶尔，公司不得不将他们的开发环境转移到云端，不仅因为它的好处，更是因为必要性。对于那些数据无法下载或存储在本地机器上的用例，唯一的访问途径是通过可以从S3读取数据的云笔记本（SageMaker
    Studio），前提是它具备适当的权限。
- en: Of course, cloud dev environments might not work for every company due to cost,
    security, or other concerns. Setting up cloud dev environments also requires some
    initial investments, and you might need to educate your data scientists on cloud
    hygiene, including establishing secure connections to the cloud, security compliance,
    or avoiding wasteful cloud usage. However, standardization of dev environments
    might make your data scientists’ lives easier and save you money in the long run.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，并非每家公司都适合使用云开发环境，可能是因为成本、安全性或其他原因。设置云开发环境也需要一些初期投资，你可能需要教育你的数据科学家关于云卫生学，包括建立安全连接到云、安全合规性或避免浪费的云使用等方面。然而，标准化开发环境可能会让你的数据科学家生活更轻松，并在长远节省资金。
- en: 'From Dev to Prod: Containers'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从开发到生产：容器
- en: During development, you might usually work with a fixed number of machines or
    instances (usually one) because your workloads don’t fluctuate a lot—your model
    doesn’t suddenly change from serving only 1,000 requests an hour to 1 million
    requests an hour.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，您可能通常会使用固定数量的机器或实例（通常是一个），因为您的工作负载不会波动很大——您的模型不会突然从每小时仅处理 1,000 个请求变成每小时处理
    1 百万个请求。
- en: A production service, on the other hand, might be spread out on multiple instances.
    The number of instances changes from time to time depending on the incoming workloads,
    which can be unpredictable at times. For example, a celebrity tweets about your
    fledgling app and suddenly your traffic spikes 10x. You will have to turn on new
    instances as needed, and these instances will need to be set up with required
    tools and packages to execute your workloads.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生产服务可能分布在多个实例上。实例数量根据传入的工作负载不时地变化，有时可能是不可预测的。例如，一位名人在推特上发表了关于您新兴应用程序的推文，突然间您的流量增加了
    10 倍。您将需要根据需要打开新实例，并且这些实例需要设置所需的工具和包以执行您的工作负载。
- en: Previously, you’d have to spin up and shut down instances yourself, but most
    public cloud providers have taken care of the autoscaling part. However, you still
    have to worry about setting up new instances.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，您必须自己启动和关闭实例，但大多数公共云提供商已经负责自动缩放部分。但是，您仍然需要担心设置新实例。
- en: When you consistently work with the same instance, you can install dependencies
    once and use them whenever you use this instance. In production, if you dynamically
    allocate instances as needed, your environment is inherently stateless. When a
    new instance is allocated for your workload, you’ll need to install dependencies
    using a list of predefined instructions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当您始终使用同一个实例时，您可以一次安装依赖项，并在每次使用此实例时重复使用它们。在生产环境中，如果根据需要动态分配实例，您的环境本质上是无状态的。当为您的工作负载分配一个新实例时，您将需要使用预定义指令列表安装依赖项。
- en: 'A question arises: how do you re-create an environment on any new instance?
    Container technology—of which Docker is the most popular—is designed to answer
    this question. With Docker, you create a Dockerfile with step-by-step instructions
    on how to re-create an environment in which your model can run: install this package,
    download this pretrained model, set environment variables, navigate into a folder,
    etc. These instructions allow hardware anywhere to run your code.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题是：如何在任何新实例上重新创建环境？容器技术——其中 Docker 是最流行的——就是为了回答这个问题而设计的。使用 Docker，您可以创建一个
    Dockerfile，其中包含逐步说明如何重新创建能够运行您的模型的环境的指令：安装这个包，下载这个预训练模型，设置环境变量，进入文件夹等等。这些指令使得任何地方的硬件都能运行您的代码。
- en: Two key concepts in Docker are image and container. Running all the instructions
    in a Dockerfile gives you a Docker image. If you run this Docker image, you get
    back a Docker container. You can think of a Dockerfile as the recipe to construct
    a mold, which is a Docker image. From this mold, you can create multiple running
    instances; each is a Docker container.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 中的两个关键概念是镜像和容器。运行 Dockerfile 中的所有指令会生成一个 Docker 镜像。如果运行此 Docker 镜像，则会得到一个
    Docker 容器。您可以将 Dockerfile 视为构建模具的配方，而这个模具就是 Docker 镜像。从这个模具中，您可以创建多个运行实例，每个实例都是一个
    Docker 容器。
- en: You can build a Docker image either from scratch or from another Docker image.
    For example, NVIDIA might provide a Docker image that contains TensorFlow and
    all necessary libraries to optimize TensorFlow for GPUs. If you want to build
    an application that runs TensorFlow on GPUs, it’s not a bad idea to use this Docker
    image as your base and install dependencies specific to your application on top
    of this base image.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从头开始构建 Docker 镜像，也可以从另一个 Docker 镜像开始构建。例如，NVIDIA 可能提供一个包含 TensorFlow 和所有必要库以优化
    TensorFlow 用于 GPU 的 Docker 镜像。如果您想构建一个在 GPU 上运行 TensorFlow 的应用程序，使用这个 Docker 镜像作为基础，并在此基础镜像之上安装特定于您的应用程序的依赖项，是一个不错的选择。
- en: A container registry is where you can share a Docker image or find an image
    created by other people to be shared publicly or only with people inside their
    organizations. Common container registries include Docker Hub and AWS ECR (Elastic
    Container Registry).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 容器注册表是您可以共享 Docker 镜像或查找其他人创建并公开或仅在其组织内分享的镜像的地方。常见的容器注册表包括 Docker Hub 和 AWS
    ECR（Elastic Container Registry）。
- en: Here’s an example of a simple Dockerfile that runs the following instructions.
    The example is to show how Dockerfiles work in general, and might not be executable.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的 Dockerfile 示例，展示了以下指令的运行方式。此示例旨在概述 Dockerfile 的工作原理，并非可执行示例。
- en: Download the latest PyTorch base image.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载最新的 PyTorch 基础镜像。
- en: Clone NVIDIA’s apex repository on GitHub, navigate to the newly created *apex*
    folder, and install apex.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GitHub上克隆NVIDIA的apex仓库，导航到新创建的*apex*文件夹，并安装apex。
- en: Set *fancy-nlp-project* to be the working directory.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*fancy-nlp-project*设为工作目录。
- en: Clone Hugging Face’s transformers repository on GitHub, navigate to the newly
    created *transformers* folder, and install transformers.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GitHub上克隆Hugging Face的transformers仓库，导航到新创建的*transformers*文件夹，并安装transformers。
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If your application does anything interesting, you will probably need more than
    one container. Consider the case where your project consists of the featurizing
    code that is fast to run but requires a lot of memory, and the model training
    code that is slow to run but requires less memory. If you run both parts of the
    code on the same GPU instances, you’ll need GPU instances with high memory, which
    can be very expensive. Instead, you can run your featurizing code on CPU instances
    and the model training code on GPU instances. This means you’ll need one container
    for featurizing and another container for training.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用程序有趣的功能，你可能需要多个容器。考虑这样一个情况，你的项目包含快速运行但需要大量内存的特征化代码，以及运行速度较慢但需要较少内存的模型训练代码。如果你在同一GPU实例上运行代码的两个部分，你将需要具有高内存的GPU实例，这可能非常昂贵。相反，你可以在CPU实例上运行特征化代码，并在GPU实例上运行模型训练代码。这意味着你需要一个用于特征化和另一个用于训练的容器。
- en: Different containers might also be necessary when different steps in your pipeline
    have conflicting dependencies, such as your featurizer code requires NumPy 0.8
    but your model requires NumPy 1.0.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的流水线中的不同步骤有冲突的依赖关系时，可能需要不同的容器，例如，你的特征提取代码需要NumPy 0.8，但你的模型需要NumPy 1.0。
- en: If you have 100 microservices and each microservice requires its own container,
    you might have 100 containers running at the same time. Manually building, running,
    allocating resources for, and stopping 100 containers might be a painful chore.
    A tool to help you manage multiple containers is called container orchestration.
    Docker Compose is a lightweight container orchestrator that can manage containers
    on a single host.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有100个微服务，每个微服务都需要自己的容器，你可能会同时运行100个容器。手动构建、运行、分配资源和停止100个容器可能是一项痛苦的工作。一个帮助你管理多个容器的工具称为容器编排。Docker
    Compose是一种轻量级容器编排器，可以在单个主机上管理容器。
- en: However, each of your containers might run on its own host, and this is where
    Docker Compose is at its limits. Kubernetes (K8s) is a tool for exactly that.
    K8s creates a network for containers to communicate and share resources. It can
    help you spin up containers on more instances when you need more compute/memory
    as well as shutting down containers when you no longer need them, and it helps
    maintain high availability for your system.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每个容器可能在自己的主机上运行，这正是Docker Compose的极限所在。Kubernetes（K8s）正是解决这个问题的工具。K8s为容器创建了一个通信和共享资源的网络。它可以帮助你在需要更多计算/内存时在更多实例上启动容器，并在不再需要时关闭容器，同时帮助维持系统的高可用性。
- en: K8s was one of the fastest-growing technologies in the 2010s. Since its inception
    in 2014, it’s become ubiquitous in production systems today. Jeremy Jordan has
    a great [introduction to K8s](https://oreil.ly/QLAC3) for readers interested in
    learning more. However, K8s is not the most data-scientist-friendly tool, and
    there have been many discussions on how to move data science workloads away from
    it.^([27](ch10.xhtml#ch01fn364)) We’ll go more into K8s in the next section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes（K8s）是2010年代增长最快的技术之一。自2014年推出以来，它已经普及到今天的生产系统中。Jeremy Jordan为那些有兴趣了解更多的读者提供了一个关于[K8s的介绍](https://oreil.ly/QLAC3)。然而，K8s并不是最适合数据科学家的工具，关于如何将数据科学工作负载迁移出K8s已经进行了多次讨论。^([27](ch10.xhtml#ch01fn364))我们将在下一节更深入地讨论K8s。
- en: Resource Management
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源管理
- en: In the pre-cloud world (and even today in companies that maintain their own
    data centers), storage and compute were finite. Resource management back then
    centered around how to make the most out of limited resources. Increasing resources
    for one application could mean decreasing resources for other applications, and
    complex logic was required to maximize resource utilization, even if that meant
    requiring more engineering time.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在云计算出现之前（甚至是今天仍然在自己维护数据中心的公司），存储和计算资源是有限的。当时的资源管理围绕如何充分利用有限资源展开。增加一个应用程序的资源可能意味着减少其他应用程序的资源，并且需要复杂的逻辑来最大化资源利用，即使这意味着需要更多的工程时间。
- en: However, in the cloud world where storage and compute resources are much more
    elastic, the concern has shifted from how to maximize resource utilization to
    how to use resources cost-effectively. Adding more resources to an application
    doesn’t mean decreasing resources for other applications, which significantly
    simplifies the allocation challenge. Many companies are OK with adding more resources
    to an application as long as the added cost is justified by the return, e.g.,
    extra revenue or saved engineering time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在云计算世界中，存储和计算资源更具弹性，关注点已从如何最大化资源利用转向如何成本有效地使用资源。向应用程序添加更多资源并不意味着减少其他应用程序的资源，这显著简化了分配挑战。许多公司可以接受向应用程序添加更多资源，只要增加的成本能够通过回报（例如额外的收入或节省的工程时间）来证明是合理的。
- en: In the vast majority of the world, where engineers’ time is more valuable than
    compute time, companies are OK using more resources if this means it can help
    their engineers become more productive. This means that it might make sense for
    companies to invest in automating their workloads, which might make using resources
    less efficient than manually planning their workloads, but free their engineers
    to focus on work with higher returns. Often, if a problem can be solved by either
    using more non-human resources (e.g., throwing more compute at it) or using more
    human resources (e.g., requiring more engineering time to redesign), the first
    solution might be preferred.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数地区，工程师的时间比计算时间更宝贵，只要能帮助工程师提高生产力，公司通常可以接受使用更多资源。这意味着对公司来说，投资于自动化其工作负载可能是有意义的，尽管这可能使得使用资源的效率低于手动规划工作负载，但可以使工程师有更高的回报。通常，如果一个问题可以通过使用更多非人力资源（例如投入更多计算资源）或使用更多人力资源（例如需要更多工程时间进行重新设计）来解决，第一种解决方案可能更可取。
- en: In this section, we’ll discuss how to manage resources for ML workflows. We’ll
    focus on cloud-based resources; however, the discussed ideas can also be applicable
    for private data centers.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何管理ML工作流的资源。我们将重点放在基于云的资源上；然而，讨论的思想也可以适用于私有数据中心。
- en: Cron, Schedulers, and Orchestrators
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cron、调度器和编排器
- en: 'There are two key characteristics of ML workflows that influence their resource
    management: repetitiveness and dependencies.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ML工作流的两个关键特征影响其资源管理：重复性和依赖性。
- en: In this book, we’ve discussed at length how developing ML systems is an iterative
    process. Similarly, ML workloads are rarely one-time operations but something
    repetitive. For example, you might train a model every week or generate a new
    batch of predictions every four hours. These repetitive processes can be scheduled
    and orchestrated to run smoothly and cost-effectively using available resources.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经详细讨论了如何开发ML系统是一个迭代过程。同样，ML工作负载很少是一次性操作，而是重复的。例如，您可能每周训练一个模型或每四个小时生成一批新的预测。可以通过可用资源顺利且成本有效地安排和编排这些重复过程。
- en: 'Scheduling repetitive jobs to run at fixed times is exactly what *cron* does.
    This is also all that cron does: run a script at a predetermined time and tell
    you whether the job succeeds or fails. It doesn’t care about the dependencies
    between the jobs it runs—you can run job A after job B with cron but you can’t
    schedule anything complicated like run B if A succeeds and run C if A fails.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 定期安排重复作业在固定时间运行正是*cron*所做的。这也是cron的全部功能：在预定时间运行脚本，并告诉你作业是否成功或失败。它不关心运行的作业之间的依赖关系——你可以用cron在作业B之后运行作业A，但不能安排像在A成功后运行B，A失败后运行C这样复杂的事情。
- en: 'This leads us to the second characteristic: dependencies. Steps in an ML workflow
    might have complex *dependency* relationships with each other. For example, an
    ML workflow might consist of the following steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到第二个特征：依赖性。ML工作流程中的步骤可能彼此之间具有复杂的*依赖关系*。例如，ML工作流可能包括以下步骤：
- en: Pull last week’s data from data warehouses.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据仓库中提取上周的数据。
- en: Extract features from this pulled data.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这些提取的数据中提取特征。
- en: Train two models, A and B, on the extracted features.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对提取的特征训练两个模型A和B。
- en: Compare A and B on the test set.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上比较A和B。
- en: Deploy A if A is better; otherwise deploy B.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果A更好，则部署A；否则部署B。
- en: 'Each step depends on the success of the previous step. Step 5 is what we call
    conditional dependency: the action for this step depends on the outcome of the
    previous step. The order of execution and dependencies among these steps can be
    represented using a graph, as shown in [Figure 10-7](#a_graph_that_shows_the_order_of_executi).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步都依赖于前一步的成功。第五步是我们所说的条件依赖：这一步的操作取决于前一步的结果。这些步骤的执行顺序和依赖关系可以用图形表示，如[图 10-7](#a_graph_that_shows_the_order_of_executi)所示。
- en: '![](Images/dmls_1007.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1007.png)'
- en: Figure 10-7\. A graph that shows the order of execution of a simple ML workflow,
    which is essentially a DAG (directed acyclic graph)
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. 一个显示简单ML工作流程执行顺序的图表，本质上是一个DAG（有向无环图）。
- en: 'Many readers might recognize that [Figure 10-7](#a_graph_that_shows_the_order_of_executi)
    is a DAG: directed acyclic graph. It has to be directed to express the dependencies
    among steps. It can’t contain cycles because, if it does, the job will just keep
    on running forever. DAG is a common way to represent computing workflows in general,
    not just ML workflows. Most workflow management tools require you to specify your
    workflows in a form of DAGs.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 许多读者可能会认识到[图 10-7](#a_graph_that_shows_the_order_of_executi)是一个DAG：有向无环图。它必须是有向的以表达步骤之间的依赖关系。它不能包含循环，因为如果有循环，作业将永远运行下去。DAG
    是一种通用的表示计算工作流的方式，不仅适用于机器学习工作流。大多数工作流管理工具要求您以DAG的形式指定工作流程。
- en: '*Schedulers* are cron programs that can handle dependencies. It takes in the
    DAG of a workflow and schedules each step accordingly. You can even schedule to
    start a job based on an event-based trigger, e.g., start a job whenever an event
    X happens. Schedulers also allow you to specify what to do if a job fails or succeeds,
    e.g., if it fails, how many times to retry before giving up.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*调度程序*是可以处理依赖关系的cron程序。它接受工作流的DAG并相应地安排每个步骤的调度。您甚至可以根据基于事件的触发器安排启动作业，例如，每当事件X发生时启动作业。调度程序还允许您指定作业失败或成功时的操作，例如，如果作业失败，重试多少次后放弃。'
- en: Schedulers tend to leverage queues to keep track of jobs. Jobs can be queued,
    prioritized, and allocated resources needed to execute. This means that schedulers
    need to be aware of the resources available and the resources needed to run each
    job—the resources needed are either specified as options when you schedule a job
    or estimated by the scheduler. For instance, if a job requires 8 GB of memory
    and two CPUs, the scheduler needs to find among the resources it manages an instance
    with 8 GB of memory and two CPUs and wait until the instance is not executing
    other jobs to run this job on the instance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 调度程序倾向于利用队列来跟踪作业。作业可以被排队、优先级排序并分配所需的执行资源。这意味着调度程序需要了解可用资源以及每个作业运行所需的资源——在调度作业时，需要指定这些资源选项或由调度程序估算。例如，如果一个作业需要
    8 GB 的内存和两个CPU，调度程序需要在管理的资源中找到一个具有 8 GB 内存和两个CPU的实例，并等待直到该实例不在执行其他作业时运行此作业。
- en: 'Here’s an example of how to schedule a job with the popular scheduler Slurm,
    where you specify the job name, the time when the job needs to be executed, and
    the amount of memory and CPUs to be allocated for the job:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何使用流行的调度程序 Slurm 调度作业的示例，其中您可以指定作业名称、执行作业所需的时间以及为作业分配的内存和CPU数量：
- en: '[PRE1]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Schedulers should also optimize for resource utilization since they have information
    on resources available, jobs to run, and resources needed for each job to run.
    However, the number of resources specified by users is not always correct. For
    example, I might estimate, and therefore specify, that a job needs 4 GB of memory,
    but this job only needs 3 GB of memory or needs 4 GB of memory at peak and only
    1–2 GB of memory otherwise. Sophisticated schedulers like Google’s Borg estimate
    how many resources a job will actually need and reclaim unused resources for other
    jobs,^([28](ch10.xhtml#ch01fn365)) further optimizing resource utilization.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于调度程序掌握了可用资源、要运行的作业以及每个作业运行所需的资源信息，因此调度程序还应优化资源利用。然而，用户指定的资源数量并不总是正确的。例如，我可能估计并因此指定一个作业需要
    4 GB 的内存，但实际上这个作业只需要 3 GB 内存，或者在峰值时需要 4 GB 内存，其他时间只需要 1–2 GB 内存。像谷歌的 Borg 这样的高级调度程序估计作业实际需要的资源量，并回收未使用的资源供其他作业使用，进一步优化资源利用。^([28](ch10.xhtml#ch01fn365))
- en: Designing a general-purpose scheduler is hard, since this scheduler will need
    to be able to manage almost any number of concurrent machines and workflows. If
    your scheduler is down, every single workflow that this scheduler touches will
    be interrupted.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要设计一个通用的调度器是很难的，因为这个调度器需要能够管理几乎任意数量的并发机器和工作流程。如果你的调度器出现问题，那么它触及的每一个工作流程都会被中断。
- en: If schedulers are concerned with *when* to run jobs and what resources are needed
    to run those jobs, orchestrators are concerned with *where* to get those resources.
    Schedulers deal with job-type abstractions such as DAGs, priority queues, user-level
    quotas (i.e., the maximum number of instances a user can use at a given time),
    etc. Orchestrators deal with lower-level abstractions like machines, instances,
    clusters, service-level grouping, replication, etc. If the orchestrator notices
    that there are more jobs than the pool of available instances, it can increase
    the number of instances in the available instance pool. We say that it “provisions”
    more computers to handle the workload. Schedulers are often used for periodical
    jobs, whereas orchestrators are often used for services where you have a long-running
    server that responds to requests.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果调度器关注于何时运行作业以及运行这些作业所需的资源，编排器关注于从何处获取这些资源。调度器处理作业类型的抽象，如 DAGs、优先级队列、用户级配额（即用户在给定时间内可以使用的最大实例数）等。编排器处理较低级别的抽象，如机器、实例、集群、服务级别分组、复制等。如果编排器注意到作业比可用实例池中的实例更多，它可以增加可用实例池中的实例数。我们说它“配置”更多计算机来处理工作负载。调度器通常用于周期性作业，而编排器通常用于具有长时间运行服务器以响应请求的服务。
- en: 'The most well-known orchestrator today is undoubtedly Kubernetes, the container
    orchestrator we discussed in the section [“From Dev to Prod: Containers”](#from_dev_to_prod_containers).
    K8s can be used on-prem (even on your laptop via minikube). However, I’ve never
    met anyone who enjoys setting up their own K8s clusters, so most companies use
    K8s as a hosted service managed by their cloud providers, such as AWS’s Elastic
    Kubernetes Service (EKS) or Google Kubernetes Engine (GKE).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当今最著名的编排器无疑是 Kubernetes，即我们在“从开发到生产：容器”部分讨论过的容器编排器。K8s 可以在本地使用（甚至可以通过 minikube
    在您的笔记本电脑上使用）。然而，我从未遇到过喜欢设置自己 K8s 集群的人，所以大多数公司使用由云提供商管理的托管服务，如 AWS 的 Elastic Kubernetes
    Service (EKS) 或 Google 的 Kubernetes Engine (GKE)。
- en: Many people use schedulers and orchestrators interchangeably because schedulers
    usually run on top of orchestrators. Schedulers like Slurm and Google’s Borg have
    some orchestrating capacity, and orchestrators like HashiCorp Nomad and K8s come
    with some scheduling capacity. But you can have separate schedulers and orchestrators,
    such as running Spark’s job scheduler on top of Kubernetes or AWS Batch scheduler
    on top of EKS. Orchestrators such as HashiCorp Nomad and data science–specific
    orchestrators including Airflow, Argo, Prefect, and Dagster have their own schedulers.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人将调度器和编排器交替使用，因为调度器通常在编排器的顶部运行。像 Slurm 和 Google 的 Borg 这样的调度器具有一定的编排能力，而像
    HashiCorp 的 Nomad 和 K8s 这样的编排器则带有一定的调度能力。但是你可以拥有单独的调度器和编排器，例如在 Kubernetes 上运行
    Spark 的作业调度器或在 EKS 上运行 AWS Batch 调度器。像 HashiCorp 的 Nomad 和专门用于数据科学的编排器，包括 Airflow、Argo、Prefect
    和 Dagster，都有它们自己的调度器。
- en: Data Science Workflow Management
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据科学工作流管理
- en: We’ve discussed the differences between schedulers and orchestrators and how
    they can be used to execute workflows in general. Readers familiar with workflow
    management tools aimed especially at data science like Airflow, Argo, Prefect,
    Kubeflow, Metaflow, etc. might wonder where they fit in this scheduler versus
    orchestrator discussion. We’ll go into this topic here.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了调度器和编排器之间的区别，以及它们如何用于通用的工作流程执行。熟悉数据科学特定工具如Airflow、Argo、Prefect、Kubeflow、Metaflow等的读者可能会想知道它们在调度器与编排器讨论中的定位。我们将在这里详细讨论这个话题。
- en: In its simplest form, workflow management tools manage workflows. They generally
    allow you to specify your workflows as DAGs, similar to the one in [Figure 10-7](#a_graph_that_shows_the_order_of_executi).
    A workflow might consist of a featurizing step, a model training step, and an
    evaluation step. Workflows can be defined using either code (Python) or configuration
    files (YAML). Each step in a workflow is called a task.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的形式下，工作流管理工具管理工作流程。它们通常允许您将工作流程指定为 DAGs，类似于 [图 10-7](#a_graph_that_shows_the_order_of_executi)
    中的图表。一个工作流可能包括特征提取步骤、模型训练步骤和评估步骤。工作流可以使用代码（Python）或配置文件（YAML）定义。工作流中的每个步骤称为任务。
- en: Almost all workflow management tools come with some schedulers, and therefore,
    you can think of them as schedulers that, instead of focusing on individual jobs,
    focus on the workflow as a whole. Once a workflow is defined, the underlying scheduler
    usually works with an orchestrator to allocate resources to run the workflow,
    as shown in [Figure 10-8](#after_a_workflow_is_definedcomma_the_ta).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的工作流管理工具都带有一些调度器，因此，你可以将它们视为调度器，而不是专注于单个作业，而是专注于整个工作流。一旦定义了工作流，底层调度器通常与编排器一起工作，分配资源来运行工作流，如图10-8所示。
- en: '![](Images/dmls_1008.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1008.png)'
- en: Figure 10-8\. After a workflow is defined, the tasks in this workflow are scheduled
    and orchestrated
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-8。在定义工作流之后，这个工作流中的任务被调度和编排。
- en: 'There are many articles online comparing different data science workflow management
    tools. In this section, we’ll go over five of the most common tools: Airflow,
    Argo, Prefect, Kubeflow, and Metaflow. This section isn’t meant to be a comprehensive
    comparison of those tools, but to give you an idea of different features a workflow
    management tool might need.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在网上有很多文章比较不同的数据科学工作流管理工具。在本节中，我们将介绍五种最常见的工具：Airflow、Argo、Prefect、Kubeflow和Metaflow。本节不旨在全面比较这些工具，而是为你介绍工作流管理工具可能需要的不同功能。
- en: 'Originally developed at Airbnb and released in 2014, Airflow is one of the
    earliest workflow orchestrators. It’s an amazing task scheduler that comes with
    a huge library of operators that makes it easy to use Airflow with different cloud
    providers, databases, storage options, and so on. Airflow is a champion of the
    [“configuration as code”](https://oreil.ly/aNVdq) principle. Its creators believed
    that data workflows are complex and should be defined using code (Python) instead
    of YAML or other declarative language. Here’s an example of an Airflow workflow,
    drawn from the platform’s [GitHub repository](https://oreil.ly/Ubgf1):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow最初是在Airbnb开发，并于2014年发布，是最早的工作流编排器之一。它是一个了不起的任务调度器，配有大量的操作器库，可以轻松地在不同的云提供商、数据库、存储选项等中使用Airflow。Airflow是“配置即代码”原则的支持者。它的创作者认为数据工作流程复杂，应该使用代码（Python）而不是YAML或其他声明性语言来定义。这里有一个Airflow工作流的示例，来自平台的[GitHub仓库](https://oreil.ly/Ubgf1)：
- en: '[PRE2]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: However, because Airflow was created earlier than most other tools, it had no
    tool to learn lessons from and suffers from many drawbacks, as discussed in detail
    in [a blog post by Uber Engineering](https://oreil.ly/U7gkM). Here, we’ll go over
    only three to give you an idea.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于Airflow比大多数其他工具更早创建，它没有可以借鉴的工具，因此遭受了许多缺点，详细讨论见[Uber工程博客文章](https://oreil.ly/U7gkM)。在这里，我们将仅介绍三个缺点，为你提供一个概念。
- en: First, Airflow is monolithic, which means it packages the entire workflow into
    one container. If two different steps in your workflow have different requirements,
    you can, in theory, create different containers for them using Airflow’s [`DockerOperator`](https://oreil.ly/NwVFF),
    but it’s not that easy to do so.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Airflow是单片的，这意味着它将整个工作流打包到一个容器中。如果你的工作流中的两个不同步骤有不同的要求，在理论上，你可以使用Airflow的[`DockerOperator`](https://oreil.ly/NwVFF)为它们创建不同的容器，但实际上并不那么容易。
- en: Second, Airflow’s DAGs are not parameterized, which means you can’t pass parameters
    into your workflows. So if you want to run the same model with different learning
    rates, you’ll have to create different workflows.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，Airflow的DAGs没有参数化，这意味着你不能向工作流传递参数。因此，如果你想用不同的学习率运行相同的模型，你必须创建不同的工作流。
- en: Third, Airflow’s DAGs are static, which means it can’t automatically create
    new steps at runtime as needed. Imagine you’re reading from a database and you
    want to create a step to process each record in the database (e.g., to make a
    prediction), but you don’t know in advance how many records there are in the database.
    Airflow won’t be able to handle that.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，Airflow的DAGs是静态的，这意味着它不能在运行时根据需要自动创建新步骤。想象一下你从数据库中读取数据，并且想要创建一个处理数据库中每条记录的步骤（例如进行预测），但是你事先不知道数据库中有多少条记录。Airflow将无法处理这种情况。
- en: The next generation of workflow orchestrators (Argo, Prefect) were created to
    address different drawbacks of Airflow.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 下一代工作流编排器（Argo、Prefect）的创建是为了解决Airflow的不同缺点。
- en: Prefect’s CEO, Jeremiah Lowin, was a core contributor of Airflow. Their early
    marketing campaign drew [intense comparison](https://oreil.ly/E19Pg) between Prefect
    and Airflow. Prefect’s workflows are parameterized and dynamic, a vast improvement
    compared to Airflow. It also follows the “configuration as code” principle so
    workflows are defined in Python.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Prefect 的 CEO，Jeremiah Lowin，曾是 Airflow 的核心贡献者。他们早期的营销活动引起了 [激烈的比较](https://oreil.ly/E19Pg)
    between Prefect and Airflow。Prefect 的工作流程是参数化和动态的，相比于 Airflow 有了很大的改进。它还遵循“配置即代码”的原则，因此工作流程是用
    Python 定义的。
- en: However, like Airflow, containerized steps aren’t the first priority of Prefect.
    You can run each step in a container, but you’ll still have to deal with Dockerfiles
    and register your docker with your workflows in Prefect.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与 Airflow 一样，Prefect 并不把容器化步骤作为首要任务。你可以在每个步骤中运行容器，但仍然需要处理 Dockerfile，并在 Prefect
    中注册你的 Docker 镜像与工作流程。
- en: 'Argo addresses the container problem. Every step in an Argo workflow is run
    in its own container. However, Argo’s workflows are defined in YAML, which allows
    you to define each step and its requirements in the same file. The following code
    sample, drawn from the [Argo GitHub repository](https://oreil.ly/Su1XX), demonstrates
    how to create a workflow to show a coin flip:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Argo 解决了容器问题。Argo 工作流中的每个步骤都在其自己的容器中运行。然而，Argo 的工作流是用 YAML 定义的，这使你能够在同一个文件中定义每个步骤及其要求。下面的代码示例，来自于
    [Argo GitHub 仓库](https://oreil.ly/Su1XX)，演示了如何创建一个显示硬币翻转的工作流程：
- en: '[PRE3]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The main drawback of Argo, other than its messy YAML files, is that it can only
    run on K8s clusters, which are only available in production. If you want to test
    the same workflow locally, you’ll have to use minikube to simulate a K8s on your
    laptop, which can get messy.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Argo 的主要缺点，除了其混乱的 YAML 文件之外，是它只能在生产中运行的 K8s 集群上运行。如果你想在本地测试相同的工作流程，你将不得不使用 minikube
    在你的笔记本电脑上模拟一个 K8s，这可能会变得混乱。
- en: Enter Kubeflow and Metaflow, the two tools that aim to help you run the workflow
    in both dev and prod environments by abstracting away infrastructure boilerplate
    code usually needed to run Airflow or Argo. They promise to give data scientists
    access to the full compute power of the prod environment from local notebooks,
    which effectively allows data scientists to use the same code in both dev and
    prod environments.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 Kubeflow 和 Metaflow，这两个工具旨在通过抽象掉通常需要运行 Airflow 或 Argo 的基础设施样板代码，帮助你在开发和生产环境中运行工作流程。它们承诺让数据科学家从本地笔记本电脑中访问生产环境的完整计算能力，从而有效地使数据科学家能够在开发和生产环境中使用相同的代码。
- en: Even though both tools have some scheduling capacity, they are meant to be used
    with a bona fide scheduler and orchestrator. One component of Kubeflow is Kubeflow
    Pipelines, which is built on top of Argo, and it’s meant to be used on top of
    K8s. Metaflow can be used with AWS Batch or K8s.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两个工具都具有一定的调度能力，但它们都旨在与真正的调度器和编排器一起使用。Kubeflow 的一个组成部分是 Kubeflow Pipelines，它是建立在
    Argo 之上的，并且旨在在 K8s 上使用。Metaflow 可以与 AWS Batch 或 K8s 一起使用。
- en: Both tools are fully parameterized and dynamic. Currently, Kubeflow is the more
    popular one. However, from a user experience perspective, Metaflow is superior,
    in my opinion. In Kubeflow, while you can define your workflow in Python, you
    still have to write a Dockerfile and a YAML file to specify the specs of each
    component (e.g., process data, train, deploy) before you can stitch them together
    in a Python workflow. Basically, Kubeflow helps you abstract away other tools’
    boilerplate by making you write Kubeflow boilerplate.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个工具都是完全参数化和动态的。目前，Kubeflow 更受欢迎。然而，在用户体验方面，我认为 Metaflow 更优秀。在 Kubeflow 中，虽然你可以用
    Python 定义工作流程，但在能够在 Python 工作流程中将它们组合在一起之前，你仍然需要编写 Dockerfile 和 YAML 文件来指定每个组件（例如，处理数据、训练、部署）的规格。基本上，Kubeflow
    帮助你通过让你编写 Kubeflow 标准化代码来抽象出其他工具的样板代码。
- en: In Metaflow, you can use a Python decorator `@conda` to specify the requirements
    for each step—required libraries, memory and compute requirements—and Metaflow
    will automatically create a container with all these requirements to execute the
    step. You save on Dockerfiles or YAML files.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Metaflow 中，你可以使用 Python 装饰器 `@conda` 来指定每个步骤的要求——包括必需的库、内存和计算要求——Metaflow
    将自动创建一个包含所有这些要求的容器来执行该步骤。你可以节省 Dockerfile 或 YAML 文件。
- en: Metaflow allows you to work seamlessly with both dev and prod environments from
    the same notebook/script. You can run experiments with small datasets on local
    machines, and when you’re ready to run with the large dataset on the cloud, simply
    add `@batch` decorator to execute it on [AWS Batch](https://aws.amazon.com/batch).
    You can even run different steps in the same workflow in different environments.
    For example, if a step requires a small memory footprint, it can run on your local
    machine. But if the next step requires a large memory footprint, you can just
    add `@batch` to execute it on the cloud.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow允许您从同一个笔记本/脚本无缝地在开发和生产环境中工作。您可以在本地机器上使用小数据集运行实验，当您准备在云上使用大数据集时，只需添加`@batch`装饰器即可在[AWS
    Batch](https://aws.amazon.com/batch)上执行。您甚至可以在同一个工作流中的不同环境中运行不同的步骤。例如，如果一个步骤需要小内存占用，它可以在您的本地机器上运行。但如果下一个步骤需要大内存占用，您只需添加`@batch`即可在云上执行。
- en: '[PRE4]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ML Platform
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习平台
- en: The manager of the ML platform team at a major streaming company told me the
    story of how his team got started. He originally joined the company to work on
    their recommender systems. To deploy their recommender systems, they needed to
    build out tools such as feature management, model management, monitoring, etc.
    Last year, his company realized that these same tools could be used by other ML
    applications, not just recommender systems. They created a new team, the ML platform
    team, with the goal of providing shared infrastructure across ML applications.
    Because the recommender system team had the most mature tool, their tools were
    adopted by other teams, and some members from the recommender system team were
    asked to join the new ML platform team.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一家主要流媒体公司的机器学习平台团队经理告诉我他们团队如何起步的故事。他最初加入公司是为了负责推荐系统。为了部署推荐系统，他们需要构建特征管理、模型管理、监控等工具。去年，他的公司意识到这些相同的工具不仅可以用于推荐系统，还可以用于其他机器学习应用程序。他们成立了一个新团队，机器学习平台团队，旨在为机器学习应用程序提供共享基础设施。由于推荐系统团队的工具最为成熟，其他团队也采纳了他们的工具，并且从推荐系统团队中选了一些成员加入了新的机器学习平台团队。
- en: This story represents a growing trend since early 2020\. As each company finds
    uses for ML in more and more applications, there’s more to be gained by leveraging
    the same set of tools for multiple applications instead of supporting a separate
    set of tools for each application. This shared set of tools for ML deployment
    makes up the ML platform.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 自2020年初以来，这个故事代表了一个增长趋势。随着每家公司在越来越多的应用中使用机器学习，通过利用同一套工具来支持多个应用程序，而不是为每个应用程序支持单独的工具集，可以获得更多收益。这些用于机器学习部署的共享工具集构成了机器学习平台。
- en: Because ML platforms are relatively new, what exactly constitutes an ML platform
    varies from company to company. Even within the same company, it’s an ongoing
    discussion. Here, I’ll focus on the components that I most often see in ML platforms,
    which include model development, model store, and feature store.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因为机器学习平台相对较新，每家公司对什么构成机器学习平台存在不同看法。甚至在同一家公司内部，这是一个持续讨论的话题。在这里，我将重点关注我经常在机器学习平台中看到的组件，包括模型开发、模型存储和特征存储。
- en: 'Evaluating a tool for each of these categories depends on your use case. However,
    here are two general aspects you might want to keep in mind:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个类别的工具进行评估取决于您的用例。但是，以下是您可能想要牢记的两个一般方面：
- en: Whether the tool works with your cloud provider or allows you to use it on your
    own data center
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 无论工具是否与您的云服务提供商兼容，或者允许您在自己的数据中心使用它
- en: You’ll need to run and serve your models from a compute layer, and usually tools
    only support integration with a handful of cloud providers. Nobody likes having
    to adopt a new cloud provider for another tool.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要从计算层运行和提供模型，并且通常工具仅支持与少数云提供商的集成。没有人喜欢因为另一个工具而不得不采用新的云提供商。
- en: Whether it’s open source or a managed service
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是开源还是托管服务
- en: If it’s open source, you can host it yourself and have to worry less about data
    security and privacy. However, self-hosting means extra engineering time required
    to maintain it. If it’s managed service, your models and likely some of your data
    will be on its service, which might not work for certain regulations. Some managed
    services work with virtual private clouds, which allows you to deploy your machines
    in your own cloud clusters, helping with compliance. We’ll discuss this more in
    the section [“Build Versus Buy”](#build_versus_buy).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是开源的，您可以自行托管，减少对数据安全和隐私的担忧。然而，自行托管意味着需要额外的工程时间来维护。如果是托管服务，您的模型及可能的一些数据将位于其服务上，这可能不适用于某些法规。一些托管服务与虚拟私有云配合使用，这允许您在自己的云集群中部署您的机器，有助于遵守法规。我们将在[“构建还是购买”](#build_versus_buy)部分进一步讨论这一点。
- en: 'Let’s start with the first component: model deployment.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个组件开始：模型部署。
- en: Model Deployment
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型部署
- en: 'Once a model is trained (and hopefully tested), you want to make its predictive
    capability accessible to users. In [Chapter 7](ch07.xhtml#model_deployment_and_prediction_service),
    we talked at length on how a model can serve its predictions: online or batch
    prediction. We also discussed how the simplest way to deploy a model is to push
    your model and its dependencies to a location accessible in production then expose
    your model as an endpoint to your users. If you do online prediction, this endpoint
    will provoke your model to generate a prediction. If you do batch prediction,
    this endpoint will fetch a precomputed prediction.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成（并希望已进行测试），您希望使其预测能力对用户可访问。在[第7章](ch07.xhtml#model_deployment_and_prediction_service)中，我们详细讨论了模型如何提供其预测的能力：在线或批量预测。我们还讨论了将模型部署为端点的最简单方法是将模型及其依赖项推送到生产环境中，并向用户公开其模型作为端点的方式。如果进行在线预测，此端点将促使模型生成预测。如果进行批量预测，此端点将获取预先计算的预测结果。
- en: 'A deployment service can help with both pushing your models and their dependencies
    to production and exposing your models as endpoints. Since deploying is the name
    of the game, deployment is the most mature among all ML platform components, and
    many tools exist for this. All major cloud providers offer tools for deployment:
    AWS with [SageMaker](https://oreil.ly/S7IR4), GCP with [Vertex AI](https://oreil.ly/JNnGr),
    Azure with [Azure ML](https://oreil.ly/7deF1), Alibaba with [Machine Learning
    Studio](https://oreil.ly/jzQfg), and so on. There are also a myriad of startups
    that offer model deployment tools such as [MLflow Models](https://oreil.ly/tUJz9),
    [Seldon](https://www.seldon.io), [Cortex](https://oreil.ly/UpnsA), [Ray Serve](https://oreil.ly/WNEL5),
    and so on.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 部署服务可以帮助将您的模型及其依赖项推送到生产环境，并将您的模型作为端点公开。由于部署是游戏的名字，部署是所有 ML 平台组件中最成熟的，有许多工具可以实现这一点。所有主要的云提供商都提供了部署工具：AWS
    提供[SageMaker](https://oreil.ly/S7IR4)，GCP 提供[Vertex AI](https://oreil.ly/JNnGr)，Azure
    提供[Azure ML](https://oreil.ly/7deF1)，阿里巴巴提供[Machine Learning Studio](https://oreil.ly/jzQfg)，等等。此外，还有许多初创公司提供模型部署工具，如[MLflow
    Models](https://oreil.ly/tUJz9)，[Seldon](https://www.seldon.io)，[Cortex](https://oreil.ly/UpnsA)，[Ray
    Serve](https://oreil.ly/WNEL5)，等等。
- en: When looking into a deployment tool, it’s important to consider how easy it
    is to do both online prediction and batch prediction with the tool. While it’s
    usually straightforward to do online prediction at a smaller scale with most deployment
    services, doing batch prediction is usually trickier.^([29](ch10.xhtml#ch01fn366))
    Some tools allow you to batch requests together for online prediction, which is
    different from batch prediction. Many companies have separate deployment pipelines
    for online prediction and batch prediction. For example, they might use Seldon
    for online prediction but leverage Databricks for batch prediction.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择部署工具时，考虑如何轻松地进行在线预测和批量预测是很重要的。虽然使用大多数部署服务在较小规模上进行在线预测通常很简单，但通常批量预测比较棘手。^([29](ch10.xhtml#ch01fn366))
    一些工具允许您将请求批处理到一起进行在线预测，这与批量预测不同。许多公司有单独的部署管道用于在线预测和批量预测。例如，他们可能使用Seldon进行在线预测，但利用Databricks进行批量预测。
- en: An open problem with model deployment is how to ensure the quality of a model
    before it’s deployed. In [Chapter 9](ch09.xhtml#continual_learning_and_test_in_producti),
    we talked about different techniques for test in production such as shadow deployment,
    canary release, A/B testing, and so on. When choosing a deployment service, you
    might want to check whether this service makes it easy for you to perform the
    tests that you want.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署的一个悬而未决的问题是如何在部署之前确保模型的质量。在第 [9章](ch09.xhtml#continual_learning_and_test_in_producti)
    中，我们讨论了测试在生产中的不同技术，如影子部署、金丝雀发布、A/B 测试等。在选择部署服务时，您可能希望检查该服务是否能够轻松进行您想要的测试。
- en: Model Store
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型存储
- en: Many companies dismiss model stores because they sound simple. In the section
    [“Model Deployment”](#model_deployment), we talked about how, to deploy a model,
    you have to package your model and upload it to a location accessible in production.
    Model store suggests that it stores models—you can do so by uploading your models
    to storage like S3\. However, it’s not quite that simple. Imagine now that your
    model’s performance dropped for a group of inputs. The person who was alerted
    to the problem is a DevOps engineer, who, after looking into the problem, decided
    that she needed to inform the data scientist who created this model. But there
    might be 20 data scientists in the company; who should she ping?
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司忽视模型存储，因为它们听起来很简单。在 [“模型部署”](#model_deployment) 部分，我们讨论了如何部署模型，您需要将模型打包并上传到生产环境可访问的位置。模型存储建议存储模型——您可以通过上传您的模型到像
    S3 这样的存储来实现。然而，事情并不是那么简单。现在想象一下，您的模型对一组输入的性能下降了。收到问题警报的人是一位 DevOps 工程师，她调查后决定需要通知创建该模型的数据科学家。但公司可能有
    20 位数据科学家；她应该通知谁呢？
- en: 'Imagine now that the right data scientist is looped in. The data scientist
    first wants to reproduce the problems locally. She still has the notebook she
    used to generate this model and the final model, so she starts the notebook and
    uses the model with the problematic sets of inputs. To her surprise, the outputs
    the model produces locally are different from the outputs produced in production.
    Many things could have caused this discrepancy; here are just a few examples:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设正确的数据科学家已经加入了。数据科学家首先希望在本地重现这些问题。她仍然拥有用于生成此模型的笔记本和最终模型，所以她启动笔记本并使用具有问题集的模型。令她惊讶的是，本地生成的模型输出与生产中生成的输出不同。这种差异可能是由许多原因引起的，以下仅列举几个例子：
- en: The model being used in production right now is not the same model that she
    has locally. Perhaps she uploaded the wrong model binary to production?
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前在生产中使用的模型与她在本地拥有的模型不同。也许她将错误的模型二进制文件上传到了生产环境？
- en: The model being used in production is correct, but the list of features used
    is wrong. Perhaps she forgot to rebuild the code locally before pushing it to
    production?
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前在生产中使用的模型是正确的，但使用的特征列表是错误的。也许她在推送到生产之前忘记重新构建本地代码了？
- en: The model is correct, the feature list is correct, but the featurization code
    is outdated.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是正确的，特征列表是正确的，但特征处理代码已过时。
- en: The model is correct, the feature list is correct, the featurization code is
    correct, but something is wrong with the data processing pipeline.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是正确的，特征列表是正确的，特征处理代码也是正确的，但数据处理管道出了问题。
- en: Without knowing the cause of the problem, it’ll be very difficult to fix it.
    In this simple example, we assume that the data scientist responsible still has
    access to the code used to generate the model. What if that data scientist no
    longer has access to that notebook, or she has already quit or is on vacation?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在不知道问题原因的情况下，修复问题将非常困难。在这个简单的例子中，我们假设负责的数据科学家仍然可以访问用于生成模型的代码。如果这位数据科学家不再拥有那本笔记本的访问权限，或者她已经离职或者在度假呢？
- en: Many companies have realized that storing the model alone in blob storage isn’t
    enough. To help with debugging and maintenance, it’s important to track as much
    information associated with a model as possible. Here are eight types of artifacts
    that you might want to store. Note that many artifacts mentioned here are information
    that should be included in the model card, as discussed in the section [“Create
    model cards”](ch11.xhtml#create_model_cards).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司意识到，仅仅将模型存储在 Blob 存储中是不够的。为了帮助调试和维护，尽可能跟踪与模型相关的信息是很重要的。以下是您可能希望存储的八种类型的工件。请注意，这里提到的许多工件是应包含在模型卡中的信息，如第
    [“创建模型卡”](ch11.xhtml#create_model_cards) 节所讨论的。
- en: Model definition
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 模型定义
- en: This is the information needed to create the shape of the model, e.g., what
    loss function it uses. If it’s a neural network, this includes how many hidden
    layers it has and how many parameters are in each layer.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建模型形状所需的信息，例如使用的损失函数。如果是神经网络，这包括隐藏层的数量以及每个层中的参数数量。
- en: Model parameters
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数
- en: These are the actual values of the parameters of your model. These values are
    then combined with the model’s shape to re-create a model that can be used to
    make predictions. Some frameworks allow you to export both the parameters and
    the model definition together.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是您模型参数的实际值。然后，这些值与模型的形状结合起来，重新创建一个可用于预测的模型。某些框架允许您同时导出参数和模型定义。
- en: Featurize and predict functions
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取和预测函数
- en: Given a prediction request, how do you extract features and input these features
    into the model to get back a prediction? The featurize and predict functions provide
    the instruction to do so. These functions are usually wrapped in endpoints.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 给定预测请求，如何提取特征并将这些特征输入模型以获得预测？特征提取和预测函数提供了执行此操作的指令。这些函数通常包装在端点中。
- en: Dependencies
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖项
- en: The dependencies—e.g., Python version, Python packages—needed to run your model
    are usually packaged together into a container.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖项，例如 Python 版本、Python 包，通常被打包到一个容器中以运行您的模型。
- en: Data
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 数据
- en: The data used to train this model might be pointers to the location where the
    data is stored or the name/version of your data. If you use tools like DVC to
    version your data, this can be the DVC commit that generated the data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练此模型的数据可能是指向存储数据的位置或数据的名称/版本的指针。如果使用诸如 DVC 等工具对数据进行版本控制，这可以是生成数据的 DVC 提交。
- en: Model generation code
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 模型生成代码
- en: 'This is the code that specifies how your model was created, such as:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是指定您的模型创建方式的代码，例如：
- en: What frameworks it used
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用的框架
- en: How it was trained
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练方式
- en: The details on how the train/valid/test splits were created
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建训练/验证/测试分割的详细信息
- en: The number of experiments run
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验运行次数
- en: The range of hyperparameters considered
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑的超参数范围
- en: The actual set of hyperparameters that final model used
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终模型使用的实际超参数集
- en: Very often, data scientists generate models by writing code in notebooks. Companies
    with more mature pipelines make their data scientists commit the model generation
    code into their Git repos on GitHub or GitLab. However, in many companies, this
    process is ad hoc, and data scientists don’t even check in their notebooks. If
    the data scientist responsible for the model loses the notebook or quits or goes
    on vacation, there’s no way to map a model in production to the code that generated
    it for debugging or maintenance.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 很多时候，数据科学家通过在笔记本中编写代码来生成模型。具有更成熟流水线的公司要求其数据科学家将模型生成代码提交到 GitHub 或 GitLab 上的
    Git 仓库中。然而，在许多公司中，这个过程是临时的，数据科学家甚至不会提交他们的笔记本。如果负责模型的数据科学家丢失笔记本、离职或休假，那么没有办法将生产中的模型映射到生成它的代码，用于调试或维护。
- en: Experiment artifacts
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果
- en: These are the artifacts generated during the model development process, as discussed
    in the section [“Experiment Tracking and Versioning”](ch06.xhtml#experiment_tracking_and_versioning).
    These artifacts can be graphs like the loss curve. These artifacts can be raw
    numbers like the model’s performance on the test set.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在模型开发过程中生成的工件，如“实验跟踪和版本控制”部分中讨论的内容。这些工件可以是像损失曲线这样的图形，也可以是像测试集上模型性能这样的原始数字。
- en: Tags
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 标签
- en: This includes tags to help with model discovery and filtering, such as owner
    (the person or the team who is the owner of this model) or task (the business
    problem this model solves, like fraud detection).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括帮助模型发现和过滤的标签，例如所有者（拥有此模型的人或团队）或任务（此模型解决的业务问题，如欺诈检测）。
- en: Most companies store a subset, but not all, of these artifacts. The artifacts
    a company stores might not be in the same place but scattered. For example, model
    definitions and model parameters might be in S3\. Containers that contain dependencies
    might be in ECS (Elastic Container Service). Data might be in Snowflake. Experiment
    artifacts might be in Weights & Biases. Featurize and prediction functions might
    be in AWS Lambda. Some data scientists might manually keep track of these locations
    in, say, a README, but this file can be easily lost.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数公司存储一个子集，但并非所有工件。公司存储的工件可能不在同一位置，而是分散的。例如，模型定义和模型参数可能在 S3 中。包含依赖项的容器可能在 ECS（弹性容器服务）中。数据可能在
    Snowflake 中。实验工件可能在 Weights & Biases 中。特征化和预测函数可能在 AWS Lambda 中。一些数据科学家可能会手动跟踪这些位置，比如在
    README 中，但这个文件很容易丢失。
- en: A model store that can store sufficient general use cases is far from being
    a solved problem. As of writing this book, MLflow is undoubtedly the most popular
    model store that isn’t associated with a major cloud provider. Yet three out of
    the six top MLflow questions on Stack Overflow are about storing and accessing
    artifacts in MLflow, as shown in [Figure 10-9](#mlflow_is_the_most_popular_model_storec).
    Model stores are due for a makeover, and I hope that in the near future a startup
    will step up and solve this.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 模型存储器，能够存储足够的一般使用情况，远未解决。截至本书撰写时，MLflow 显然是最流行的模型存储器，不与主要云提供商关联。然而，在 Stack Overflow
    上，六个最热门的 MLflow 问题中有三个是关于在 MLflow 中存储和访问工件的，如[图 10-9](#mlflow_is_the_most_popular_model_storec)所示。模型存储器需要进行改造，我希望在不久的将来，一家初创公司能够站出来解决这个问题。
- en: '![](Images/dmls_1009.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1009.png)'
- en: 'Figure 10-9\. MLflow is the most popular model store, yet it’s far from solving
    the artifact problem. Three out of the six top MLflow questions on Stack Overflow
    are about storing and accessing artifacts in MLflow. Source: Screenshot of Stack
    Overflow page'
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-9\. MLflow 是最流行的模型存储器，但远未解决工件问题。在 Stack Overflow 上，六个最热门的 MLflow 问题中有三个是关于在
    MLflow 中存储和访问工件的。来源：从 Stack Overflow 页面截图
- en: Because of the lack of a good model store solution, companies like Stitch Fix
    resolve to build their own model store. [Figure 10-10](#artifacts_that_stitch_fixapostrophes_mo)
    shows the artifacts that Stitch Fix’s model store tracks. When a model is uploaded
    to their model store, this model comes with the link to the serialized model,
    the dependencies needed to run the model (Python environment), the Git commit
    where the model code generation is created (Git information), tags (to at least
    specify the team that owns the model), etc.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 因缺乏良好的模型存储解决方案，像 Stitch Fix 这样的公司决定自行构建他们自己的模型存储。[图 10-10](#artifacts_that_stitch_fixapostrophes_mo)展示了
    Stitch Fix 模型存储跟踪的工件。当模型上传到他们的模型存储时，此模型附带链接到序列化模型的链接，运行模型所需的依赖项（Python 环境），创建模型代码生成的
    Git 提交（Git 信息），标签（至少指定拥有模型的团队）等信息。
- en: '![](Images/dmls_1010.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dmls_1010.png)'
- en: 'Figure 10-10\. Artifacts that Stitch Fix’s model store tracks. Source: Adapted
    from a slide by Stefan Krawczyk for [CS 329S (Stanford)](https://oreil.ly/zWQM9).'
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-10\. Stitch Fix 模型存储跟踪的工件。来源：根据 Stefan Krawczyk 为[CS 329S（斯坦福大学）](https://oreil.ly/zWQM9)制作的幻灯片修改
- en: Feature Store
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征存储
- en: '“Feature store” is an increasingly loaded term that can be used by different
    people to refer to very different things. There have been many attempts by ML
    practitioners to define what features a feature store should have.^([30](ch10.xhtml#ch01fn367))
    At its core, there are three main problems that a feature store can help address:
    feature management, feature transformation, and feature consistency. A feature
    store solution might address one or a combination of these problems:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: “特征存储”是一个越来越具有多重含义的术语，可以被不同的人用来指代完全不同的事物。机器学习实践者已经多次尝试定义特征存储应具备的特性。^([30](ch10.xhtml#ch01fn367))
    在其核心，特征存储可以帮助解决三个主要问题：特征管理、特征转换和特征一致性。特征存储解决方案可能解决其中一个或几个问题：
- en: Feature management
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 特征管理
- en: A company might have multiple ML models, each model using a lot of features.
    Back in 2017, Uber had about 10,000 features across teams!^([31](ch10.xhtml#ch01fn368))
    It’s often the case that features used for one model can be useful for another
    model. For example, team A might have a model to predict how likely a user will
    churn, and team B has a model to predict how likely a free user will convert into
    a paid user. There are many features that these two models can share. If team
    A discovers that feature X is super useful, team B might be able to leverage that
    too.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一家公司可能拥有多个机器学习模型，每个模型使用大量特征。例如，Uber在2017年拥有大约10,000个特征跨团队使用！^([31](ch10.xhtml#ch01fn368))通常情况下，用于一个模型的特征可能对另一个模型也有用。例如，A团队可能有一个模型来预测用户流失的可能性，而B团队则有一个模型来预测免费用户转化为付费用户的可能性。这些模型可以共享许多特征。如果A团队发现特征X非常有用，B团队可能也能利用它。
- en: A feature store can help teams share and discover features, as well as manage
    roles and sharing settings for each feature. For example, you might not want everyone
    in the company to have access to sensitive financial information of either the
    company or its users. In this capacity, a feature store can be thought of as a
    feature catalog. Examples of tools for feature management are [Amundsen](https://oreil.ly/Cm5Xe)
    (developed at Lyft) and [DataHub](https://oreil.ly/ApXeL) (developed at LinkedIn).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储可以帮助团队共享和发现特征，以及管理每个特征的角色和共享设置。例如，你可能不希望公司中的每个人都能访问公司或用户的敏感财务信息。在这种情况下，特征存储可以被视为特征目录。特征管理工具的示例包括[Amundsen](https://oreil.ly/Cm5Xe)（在Lyft开发）和[DataHub](https://oreil.ly/ApXeL)（在LinkedIn开发）。
- en: Feature computation^([32](ch10.xhtml#ch01fn369))
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 特征计算^([32](ch10.xhtml#ch01fn369))
- en: 'Feature engineering logic, after being defined, needs to be computed. For example,
    the feature logic might be: use the average meal preparation time from yesterday.
    The computation part involves actually looking into your data and computing this
    average.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程逻辑在定义后需要计算。例如，特征逻辑可能是：使用昨天的平均餐饮准备时间。计算部分涉及实际查看数据并计算这个平均值。
- en: In the previous point, we discussed how multiple models might share a feature.
    If the computation of this feature isn’t too expensive, it might be acceptable
    computing this feature each time it is required by a model. However, if the computation
    is expensive, you might want to execute it only once the first time it is required,
    then store it for feature uses.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一点中，我们讨论了多个模型可能共享一个特征的情况。如果这个特征的计算不太昂贵，每次模型需要时计算一次可能是可接受的。但是，如果计算很昂贵，你可能希望仅在首次需要时执行计算，然后存储以备后续使用。
- en: A feature store can help with both performing feature computation and storing
    the results of this computation. In this capacity, a feature store acts like a
    data warehouse.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储可以帮助进行特征计算和存储计算结果。在这种情况下，特征存储类似于数据仓库。
- en: Feature consistency
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 特征一致性
- en: 'In [Chapter 7](ch07.xhtml#model_deployment_and_prediction_service), we talked
    about the problem of having two separate pipelines for the same model: the training
    pipeline extracts batch features from historical data and the inference pipeline
    extracts streaming features. During development, data scientists might define
    features and create models using Python. Production code, however, might be written
    in another language, such as Java or C, for performance.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.xhtml#model_deployment_and_prediction_service)中，我们讨论了同一模型存在两个独立管道的问题：训练管道从历史数据中提取批量特征，而推断管道提取流式特征。在开发过程中，数据科学家可能会使用Python定义特征并创建模型。然而，生产代码可能会用另一种语言编写，如Java或C，以提高性能。
- en: This means that feature definitions written in Python during development might
    need to be converted into the languages used in production. So you have to write
    the same features twice, once for training and once for inference. First, it’s
    annoying and time-consuming. Second, it creates extra surface for bugs since one
    or more features in production might differ from their counterparts in training,
    causing weird model behaviors.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在开发过程中用Python编写的特征定义可能需要转换为生产中使用的语言。因此，你必须两次编写相同的特征，一次用于训练，一次用于推断。首先，这很烦人且耗时。其次，它增加了bug的可能性，因为生产中的一个或多个特征可能与训练中的不同，导致奇怪的模型行为。
- en: A key selling point of modern feature stores is that they unify the logic for
    both batch features and streaming features, ensuring the consistency between features
    during training and features during inference.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现代特征存储的一个关键卖点是它们统一了批处理特征和流处理特征的逻辑，确保训练期间特征与推断期间特征的一致性。
- en: Feature store is a newer category that only started taking off around 2020\.
    While it’s generally agreed that feature stores should manage feature definitions
    and ensure feature consistency, their exact capacities vary from vendor to vendor.
    Some feature stores only manage feature definitions without computing features
    from data; some feature stores do both. Some feature stores also do feature validation,
    i.e., detecting when a feature doesn’t conform to a predefined schema, and some
    feature stores leave that aspect to a monitoring tool.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储是一个较新的类别，大约在2020年开始受到关注。虽然普遍认为特征存储应管理特征定义并确保特征一致性，但各供应商的具体能力各不相同。一些特征存储仅管理特征定义而不从数据计算特征；一些特征存储两者都做。有些特征存储还进行特征验证，即检测特征是否符合预定义的模式，而有些则将这一方面留给监控工具。
- en: As of writing this book, the most popular open source feature store is Feast.
    However, Feast’s strength is in batch features, not streaming features. Tecton
    is a fully managed feature store that promises to be able to handle both batch
    features and online features, but their actual traction is slow because they require
    deep integration. Platforms like SageMaker and Databricks also offer their own
    interpretations of feature stores. Out of 95 companies I surveyed in January 2022,
    only around 40% of them use a feature store. Out of those who use a feature store,
    half of them build their own feature store.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，最流行的开源特征存储是Feast。然而，Feast的优势在于批处理特征，而不是流处理特征。Tecton是一个完全托管的特征存储，承诺能够处理批处理特征和在线特征，但它们的实际吸引力较低，因为需要深度集成。像SageMaker和Databricks这样的平台也提供它们自己对特征存储的解决方案。在2022年1月我调查的95家公司中，只有约40%使用特征存储。在使用特征存储的公司中，有一半是建立了自己的特征存储。
- en: Build Versus Buy
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自建与购买
- en: At the beginning of this chapter, we discussed how difficult it is to set up
    the right infrastructure for your ML needs. What infrastructure you need depends
    on the applications you have and the scale at which you run these applications.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开头，我们讨论了为满足您的机器学习需求设置正确基础设施的困难程度。您所需的基础设施取决于您拥有的应用程序及其运行规模。
- en: How much you need to invest into infrastructure also depends on what you want
    to build in-house and what you want to buy. For example, if you want to use fully
    managed Databricks clusters, you probably need only one engineer. However, if
    you want to host your own Spark Elastic MapReduce clusters, you might need five
    more people.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要投入多少资金到基础设施中，也取决于您想在内部建造什么和您想购买什么。例如，如果您想使用完全托管的Databricks集群，可能只需要一名工程师。然而，如果您想托管自己的Spark
    Elastic MapReduce集群，可能需要额外五个人。
- en: 'At one extreme, you can outsource all your ML use cases to a company that provides
    ML applications end-to-end, and then perhaps the only piece of infrastructure
    you need is for data movement: moving your data from your applications to your
    vendor, and moving predictions from that vendor back to your users. The rest of
    your infrastructure is managed by your vendor.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在极端情况下，您可以将所有机器学习用例外包给一家提供端到端ML应用的公司，也许您唯一需要的基础设施就是数据移动：将数据从您的应用程序移动到供应商那里，将预测结果从供应商那里移回给用户。您的其余基础设施由您的供应商管理。
- en: At the other extreme, if you’re a company that handles sensitive data that prevents
    you from using services managed by another company, you might need to build and
    maintain all your infrastructure in-house, even having your own data centers.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个极端，如果您是一家处理敏感数据的公司，无法使用另一家公司管理的服务，您可能需要在内部建立和维护所有基础设施，甚至拥有自己的数据中心。
- en: Most companies, however, are in neither of these extremes. If you work for one
    of these companies, you’ll likely have some components managed by other companies
    and some components developed in-house. For example, your compute might be managed
    by AWS EC2 and your data warehouse managed by Snowflake, but you have your own
    feature store and your own monitoring dashboards.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数公司都不处于这两个极端之间。如果您在这些公司之一工作，您可能会有一些由其他公司管理的组件和一些内部开发的组件。例如，您的计算资源可能由AWS
    EC2管理，数据仓库可能由Snowflake管理，但您有自己的特征存储和监控仪表板。
- en: 'Your build versus buy decisions depend on many factors. Here, we’ll discuss
    three common ones that I often encounter when talking with heads of infrastructures
    on how they evaluate these decisions:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 您的自建还是购买决策取决于许多因素。在这里，我们将讨论三个我在与基础设施负责人交流时经常遇到的常见因素，以及他们如何评估这些决策：
- en: The stage your company is at
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 公司所处的阶段
- en: In the beginning, you might want to leverage vendor solutions to get started
    as quickly as possible so that you can focus your limited resources on the core
    offerings of your product. As your use cases grow, however, vendor costs might
    become exorbitant and it might be cheaper for you to invest in your own solution.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，您可能希望利用供应商解决方案尽快启动，以便将有限的资源集中投入到产品的核心功能上。然而，随着使用案例的增加，供应商的成本可能变得过高，对您而言，投资自己的解决方案可能更便宜。
- en: What you believe to be the focus or the competitive advantages of your company
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为公司的重点或竞争优势是什么？
- en: 'Stefan Krawczyk, manager of the ML platform team at Stitch Fix, explained to
    me his build versus buy decision: “If it’s something we want to be really good
    at, we’ll manage that in-house. If not, we’ll use a vendor.” For the vast majority
    of companies outside the technology sector—e.g., companies in retail, banking,
    manufacturing—ML infrastructure isn’t their focus, so they tend to bias toward
    buying. When I talk to these companies, they prefer managed services, even point
    solutions (e.g., solutions that solve a business problem for them, like a demand
    forecasting service). For many tech companies where technology is their competitive
    advantage, and whose strong engineering teams prefer to have control over their
    stacks, they tend to bias toward building. If they use a managed service, they
    might prefer that service to be modular and customizable, so that they can plug
    and play with any component.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Stitch Fix的ML平台团队经理Stefan Krawczyk向我解释了他在“自建还是购买”决策上的看法：“如果是我们想在某个领域真正擅长的事情，我们会自行管理。如果不是，我们会使用供应商的解决方案。”对于大多数非技术行业的公司——例如零售、银行、制造业的公司来说，ML基础设施并非他们的重点，因此他们倾向于购买。当我与这些公司交流时，他们更喜欢托管服务，甚至是针对特定问题的解决方案（例如需求预测服务）。对于许多技术公司来说，技术是他们的竞争优势，他们强大的工程团队更倾向于自主控制技术堆栈，因此他们倾向于自行构建。如果他们使用托管服务，他们可能更希望该服务是模块化和可定制的，以便可以随时插拔任何组件。
- en: The maturity of the available tools
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 可用工具的成熟度
- en: For example, your team might decide that you need a model store, and you’d have
    preferred to use a vendor, but there’s no vendor mature enough for your needs,
    so you have to build your own feature store, perhaps on top of an open source
    solution.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您的团队可能决定需要一个模型存储库，本来希望使用供应商的解决方案，但没有供应商能够满足您的需求，因此您不得不建立自己的特征存储，或许是在开源解决方案的基础上。
- en: This is what happens in the early days of ML adoption in the industry. Companies
    that are early adopters, i.e., big tech companies, build out their own infrastructure
    because there are no solutions mature enough for their needs. This leads to the
    situation where every company’s infrastructure is different. A few years later,
    solution offerings mature. However, these offerings find it difficult to sell
    to big tech companies because it’s impossible to create a solution that works
    with the majority of custom infrastructure.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在工业界早期采用机器学习时发生的情况。早期采用者，即大型科技公司，因为没有足够成熟的解决方案来满足他们的需求，因此建立了自己的基础设施。这导致了每家公司的基础设施各不相同的情况。几年后，解决方案的提供变得更加成熟。然而，这些解决方案发现很难向大型科技公司销售，因为不可能创建一个能够与大多数定制基础设施兼容的解决方案。
- en: As we’re building out Claypot AI, other founders have actually advised us to
    avoid selling to big tech companies because, if we do, we’ll get sucked into what
    they call “integration hell”—spending more time integrating our solution with
    custom infrastructure instead of building out our core features. They advised
    us to focus on startups with much cleaner slates to build on.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们建设Claypot AI的过程中，其他创始人实际上建议我们避免向大型科技公司销售，因为如果这样做，我们将陷入他们所谓的“集成地狱”——花费更多时间将我们的解决方案与定制基础设施集成，而不是构建我们的核心功能。他们建议我们专注于那些有更清晰发展路径的初创企业。
- en: 'Some people think that building is cheaper than buying, which is not necessarily
    the case. Building means that you’ll have to bring on more engineers to build
    and maintain your own infrastructure. It can also come with future cost: the cost
    of innovation. In-house, custom infrastructure makes it hard to adopt new technologies
    available because of the integration issues.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人认为建设比购买更便宜，但情况并非总是如此。建设意味着你将不得不雇用更多工程师来建设和维护自己的基础设施。这也可能伴随着未来的成本：创新的成本。内部定制的基础设施使得很难采纳新技术，因为会存在集成问题。
- en: The build versus buy decisions are complex, highly context-dependent, and likely
    what heads of infrastructure spend much time mulling over. Erik Bernhardsson,
    ex-CTO of Better.com, said in a tweet that “one of the most important jobs of
    a CTO is vendor/product selection and the importance of this keeps going up rapidly
    every year since the infrastructure space grows so fast.”^([33](ch10.xhtml#ch01fn370))
    There’s no way that a small section can address all its nuances. But I hope that
    this section provides you with some pointers to start the discussion.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 建设与购买的决策是复杂的，高度依赖上下文，并且可能是基础设施负责人经常深思熟虑的问题。Better.com的前CTO埃里克·伯恩哈德森在一条推文中表示：“CTO的最重要工作之一是供应商/产品选择，由于基础设施空间增长如此迅速，其重要性每年都在迅速上升。”^([33](ch10.xhtml#ch01fn370))
    没有办法在一个小节中解决所有细微差别。但我希望本节为您提供了一些开始讨论的指引。
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: If you’ve stayed with me until now, I hope you agree that bringing ML models
    to production is an infrastructural problem. To enable data scientists to develop
    and deploy ML models, it’s crucial to have the right tools and infrastructure
    set up.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您一直与我同行至今，我希望您同意将机器学习模型引入生产环境是一个基础设施问题。为了使数据科学家能够开发和部署机器学习模型，建立正确的工具和基础设施是至关重要的。
- en: In this chapter, we covered different layers of infrastructure needed for ML
    systems. We started from the storage and compute layer, which provides vital resources
    for any engineering project that requires intensive data and compute resources
    like ML projects. The storage and compute layer is heavily commoditized, which
    means that most companies pay cloud services for the exact amount of storage and
    compute they use instead of setting up their own data centers. However, while
    cloud providers make it easy for a company to get started, their cost becomes
    prohibitive as this company grows, and more and more large companies are looking
    into repatriating from the cloud to private data centers.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了机器学习系统所需的不同基础设施层。我们从存储和计算层开始，这为需要大量数据和计算资源的任何工程项目提供了重要资源，例如机器学习项目。存储和计算层已经非常商品化，这意味着大多数公司支付云服务，按照他们使用的存储和计算资源量来付费，而不是建立自己的数据中心。然而，尽管云服务提供商让公司能够轻松入门，但随着公司的增长，它们的成本变得不可承受，越来越多的大公司正在考虑从云服务回迁到私有数据中心。
- en: We then continued on to discuss the development environment where data scientists
    write code and interact with the production environment. Because the dev environment
    is where engineers spend most of their time, improvements in the dev environment
    translate directly into improvements in productivity. One of the first things
    a company can do to improve the dev environment is to standardize the dev environment
    for data scientists and ML engineers working on the same team. We discussed in
    this chapter why standardization is recommended and how to do so.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续讨论了开发环境，在这里数据科学家编写代码并与生产环境进行交互。因为开发环境是工程师们花费大部分时间的地方，开发环境的改进直接转化为生产力的提升。公司可以做的第一件事情之一是为在同一团队工作的数据科学家和机器学习工程师标准化开发环境。在本章中，我们讨论了为什么推荐标准化以及如何实施。
- en: 'We then discussed an infrastructural topic whose relevance to data scientists
    has been debated heavily in the last few years: resource management. Resource
    management is important to data science workflows, but the question is whether
    data scientists should be expected to handle it. In this section, we traced the
    evolution of resource management tools from cron to schedulers to orchestrators.
    We also discussed why ML workflows are different from other software engineering
    workflows and why they need their own workflow management tools. We compared various
    workflow management tools such as Airflow, Argo, and Metaflow.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后讨论了一个基础设施主题，近年来数据科学家们对其重要性有着激烈的争论：资源管理。资源管理对于数据科学工作流程至关重要，但问题是是否应该期望数据科学家来处理它。在本节中，我们追溯了从
    cron 到调度程序再到编排器的资源管理工具的演变。我们还讨论了为什么 ML 工作流与其他软件工程工作流不同，以及为什么它们需要自己的工作流管理工具。我们比较了各种工作流管理工具，如
    Airflow、Argo 和 Metaflow。
- en: 'ML platform is a team that has emerged recently as ML adoption matures. Since
    it’s an emerging concept, there are still disagreements on what an ML platform
    should consist of. We chose to focus on the three sets of tools that are essential
    for most ML platforms: deployment, model store, and feature store. We skipped
    monitoring of the ML platform since it’s already covered in [Chapter 8](ch08.xhtml#data_distribution_shifts_and_monitoring).'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ML 平台是一个随着 ML 采纳成熟而出现的团队。由于它是一个新兴概念，对于 ML 平台应包含什么仍存在争议。我们选择关注对大多数 ML 平台至关重要的三组工具：部署、模型存储和特征存储。由于监控
    ML 平台已经在 [第 8 章](ch08.xhtml#data_distribution_shifts_and_monitoring) 中有所涵盖，我们跳过了对
    ML 平台的监控。
- en: 'When working on infrastructure, a question constantly haunts engineering managers
    and CTOs alike: build or buy? We ended this chapter with a few discussion points
    that I hope can provide you or your team with sufficient context to make those
    difficult decisions.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行基础设施工作时，一个问题不断困扰着工程经理和首席技术官：是自建还是购买？我们在本章末尾提出了一些讨论观点，希望能为您或您的团队提供足够的背景来做出这些困难的决策。
- en: ^([1](ch10.xhtml#ch01fn340-marker)) Kunal Shah, “This Is What Makes SEO Important
    for Every Business,” *Entrepreneur India*, May 11, 2020, [*https://oreil.ly/teQlX*](https://oreil.ly/teQlX).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.xhtml#ch01fn340-marker)) Kunal Shah，《这就是为什么 SEO 对每个企业都重要》，*Entrepreneur
    India*，2020 年 5 月 11 日，[*https://oreil.ly/teQlX*](https://oreil.ly/teQlX)。
- en: ^([2](ch10.xhtml#ch01fn341-marker)) For a sneak peek into Tesla’s compute infrastructure
    for ML, I highly recommend watching the recording of Tesla AI Day 2021 on [YouTube](https://oreil.ly/etH9C).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.xhtml#ch01fn341-marker)) 如果你想深入了解特斯拉的 ML 计算基础设施，强烈推荐观看特斯拉 2021 年
    AI Day 的录像，[YouTube](https://oreil.ly/etH9C)。
- en: '^([3](ch10.xhtml#ch01fn342-marker)) The definition for “reasonable scale” was
    inspired by Jacopo Tagliabue in his paper “You Do Not Need a Bigger Boat: Recommendations
    at Reasonable Scale in a (Mostly) Serverless and Open Stack,” *arXiv*, July 15,
    2021, [*https://oreil.ly/YNRZQ*](https://oreil.ly/YNRZQ). For more discussion
    on reasonable scale, see [“ML and MLOps at a Reasonable Scale”](https://oreil.ly/goPrb)
    by Ciro Greco (October 2021).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '^([3](ch10.xhtml#ch01fn342-marker)) “合理规模”一词的定义灵感来自 Jacopo Tagliabue 在其论文《You
    Do Not Need a Bigger Boat: 在（大部分）无服务器和开放堆栈中的合理规模推荐》中，*arXiv*，2021 年 7 月 15 日，[*https://oreil.ly/YNRZQ*](https://oreil.ly/YNRZQ)。有关合理规模的更多讨论，请参阅
    Ciro Greco 的文章《ML 和 MLOps 在合理规模下》（2021 年 10 月）[“ML and MLOps at a Reasonable Scale”](https://oreil.ly/goPrb)。'
- en: ^([4](ch10.xhtml#ch01fn343-marker)) FAAAM is short for Facebook, Apple, Amazon,
    Alphabet, Microsoft.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch10.xhtml#ch01fn343-marker)) FAAAM 是 Facebook、Apple、Amazon、Alphabet、Microsoft
    的简称。
- en: '^([5](ch10.xhtml#ch01fn344-marker)) Reza Shiftehfar, “Uber’s Big Data Platform:
    100+ Petabytes with Minute Latency,” *Uber Engineering*, October 17, 2018, [*https://oreil.ly/6Ykd3*](https://oreil.ly/6Ykd3);
    Kaushik Krishnamurthi, “Building a Big Data Pipeline to Process Clickstream Data,”
    Zillow, April 6, 2018, [*https://oreil.ly/SGmNe*](https://oreil.ly/SGmNe).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch10.xhtml#ch01fn344-marker)) Reza Shiftehfar，《Uber 的大数据平台：100+PB 的数据与分钟级延迟》，*Uber
    Engineering*，2018 年 10 月 17 日，[*https://oreil.ly/6Ykd3*](https://oreil.ly/6Ykd3)；Kaushik
    Krishnamurthi，《构建处理点击流数据的大数据流水线》，Zillow，2018 年 4 月 6 日，[*https://oreil.ly/SGmNe*](https://oreil.ly/SGmNe)。
- en: ^([6](ch10.xhtml#ch01fn345-marker)) Nathan Bronson and Janet Wiener, “Facebook’s
    Top Open Data Problems,” Meta, October 21, 2014, [*https://oreil.ly/p6QjX*](https://oreil.ly/p6QjX).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch10.xhtml#ch01fn345-marker)) Nathan Bronson 和 Janet Wiener，《Facebook
    的顶级开放数据问题》，Meta，2014 年 10 月 21 日，[*https://oreil.ly/p6QjX*](https://oreil.ly/p6QjX)。
- en: ^([7](ch10.xhtml#ch01fn346-marker)) Wikipedia, s.v. “Infrastructure,” [*https://oreil.ly/YaIk8*](https://oreil.ly/YaIk8).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch10.xhtml#ch01fn346-marker)) Wikipedia，见“基础设施”，[*https://oreil.ly/YaIk8*](https://oreil.ly/YaIk8)。
- en: ^([8](ch10.xhtml#ch01fn347-marker)) I’ve seen a company whose data is spread
    over Amazon Redshift and GCP BigQuery, and their engineers are not very happy
    about it.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch10.xhtml#ch01fn347-marker)) 我见过一家公司，他们的数据分布在Amazon Redshift和GCP BigQuery上，他们的工程师对此并不是很满意。
- en: ^([9](ch10.xhtml#ch01fn348-marker)) We only discuss data storage here since
    we’ve discussed data systems in [Chapter 2](ch02.xhtml#introduction_to_machine_learning_system).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch10.xhtml#ch01fn348-marker)) 我们这里只讨论数据存储，因为我们已经在[第2章](ch02.xhtml#introduction_to_machine_learning_system)中讨论了数据系统。
- en: ^([10](ch10.xhtml#ch01fn349-marker)) As of writing this book, an ML workload
    typically requires between 4 GB and 8 GB of memory; 16 GB of memory is enough
    to handle most ML workloads.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch10.xhtml#ch01fn349-marker)) 在编写本书时，ML工作负载通常需要4 GB到8 GB的内存；16 GB的内存足以处理大多数ML工作负载。
- en: ^([11](ch10.xhtml#ch01fn350-marker)) See operation fusion in the section [“Model
    optimization”](ch07.xhtml#model_optimization).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch10.xhtml#ch01fn350-marker)) 参见第7章的“模型优化”部分中的操作融合。
- en: ^([12](ch10.xhtml#custom_ch10n1-marker)) “What Is FLOP/s and Is It a Good Measure
    of Performance?,” Stack Overflow, last updated October 7, 2020, [*https://oreil.ly/M8jPP*](https://oreil.ly/M8jPP).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch10.xhtml#custom_ch10n1-marker)) “什么是FLOP/s，它是否是性能的好衡量标准？”，Stack Overflow，最近更新于2020年10月7日，[*https://oreil.ly/M8jPP*](https://oreil.ly/M8jPP)。
- en: ^([13](ch10.xhtml#ch01fn351-marker)) For readers interested in FLOPS and bandwidth
    and how to optimize them for deep learning models, I recommend the post [“Making
    Deep Learning Go Brrrr From First Principles”](https://oreil.ly/zvVFB) (He 2022).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch10.xhtml#ch01fn351-marker)) 对于对FLOPS和带宽以及如何为深度学习模型进行优化感兴趣的读者，我推荐阅读文章
    [“从第一原理让深度学习飙升”](https://oreil.ly/zvVFB)（He 2022）。
- en: ^([14](ch10.xhtml#ch01fn352-marker)) According to Amazon, “EC2 instances support
    multithreading, which enables multiple threads to run concurrently on a single
    CPU core. Each thread is represented as a virtual CPU (vCPU) on the instance.
    An instance has a default number of CPU cores, which varies according to instance
    type. For example, an m5.xlarge instance type has two CPU cores and two threads
    per core by default—four vCPUs in total” (“Optimize CPU Options,” Amazon Web Services,
    last accessed April 2020, [*https://oreil.ly/eeOtd*](https://oreil.ly/eeOtd)).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch10.xhtml#ch01fn352-marker)) 根据亚马逊，“EC2实例支持多线程，允许多个线程在单个CPU核心上并发运行。每个线程在实例上被表示为一个虚拟CPU（vCPU）。一个实例具有默认数量的CPU核心，根据实例类型而变化。例如，m5.xlarge实例类型默认有两个CPU核心和每核心两个线程——总共四个vCPU”（“优化CPU选项”，Amazon
    Web Services，最后访问于2020年4月，[*https://oreil.ly/eeOtd*](https://oreil.ly/eeOtd)）。
- en: ^([15](ch10.xhtml#ch01fn353-marker)) Which costs $26.688/hour.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch10.xhtml#ch01fn353-marker)) 这成本为每小时$26.688。
- en: ^([16](ch10.xhtml#ch01fn354-marker)) On-demand instances are instances that
    are available when you request them. Spot instances are instances that are available
    when nobody else is using them. Cloud providers tend to offer spot instances at
    a discount compared to on-demand instances.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch10.xhtml#ch01fn354-marker)) 按需实例是在请求时可用的实例。Spot实例是在没有其他人使用时可用的实例。与按需实例相比，云提供商倾向于以折扣价格提供Spot实例。
- en: ^([17](ch10.xhtml#ch01fn355-marker)) Synergy Research Group, “2020—The Year
    That Cloud Service Revenues Finally Dwarfed Enterprise Spending on Data Centers,”
    March 18, 2021, [*https://oreil.ly/uPx94*](https://oreil.ly/uPx94).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch10.xhtml#ch01fn355-marker)) Synergy Research Group，“2020年——云服务收入终于超过企业数据中心支出”，2021年3月18日，[*https://oreil.ly/uPx94*](https://oreil.ly/uPx94)。
- en: ^([18](ch10.xhtml#ch01fn356-marker)) Sarah Wang and Martin Casado, “The Cost
    of Cloud, a Trillion Dollar Paradox,” a16z, [*https://oreil.ly/3nWU3*](https://oreil.ly/3nWU3).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch10.xhtml#ch01fn356-marker)) Sarah Wang和Martin Casado，“云的成本，一个万亿美元的悖论”，a16z，[*https://oreil.ly/3nWU3*](https://oreil.ly/3nWU3)。
- en: ^([19](ch10.xhtml#ch01fn357-marker)) Wang and Casado, “The Cost of Cloud.”
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch10.xhtml#ch01fn357-marker)) Wang和Casado，“云的成本”。
- en: ^([20](ch10.xhtml#ch01fn358-marker)) Laurence Goasduff, “Why Organizations Choose
    a Multicloud Strategy,” Gartner, May 7, 2019, [*https://oreil.ly/ZiqzQ*](https://oreil.ly/ZiqzQ).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch10.xhtml#ch01fn358-marker)) Laurence Goasduff，“为什么组织选择多云策略”，Gartner，2019年5月7日，[*https://oreil.ly/ZiqzQ*](https://oreil.ly/ZiqzQ)。
- en: ^([21](ch10.xhtml#ch01fn359-marker)) Goasduff, “Why Organizations Choose a Multicloud
    Strategy.”
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch10.xhtml#ch01fn359-marker)) Goasduff，“为什么组织选择多云策略”。
- en: ^([22](ch10.xhtml#idm46868206610032-marker)) Ville Tuulos, *Effective Data Science
    Infrastructure* (Manning, 2022).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch10.xhtml#idm46868206610032-marker)) Ville Tuulos，《高效数据科学基础设施》（Manning，2022）。
- en: ^([23](ch10.xhtml#ch01fn360-marker)) As of writing this book, Google Colab even
    offers [free GPUs](https://oreil.ly/9ij7E) for their users.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch10.xhtml#ch01fn360-marker)) 在编写本书时，Google Colab 甚至为其用户提供[免费的 GPU](https://oreil.ly/9ij7E)。
- en: '^([24](ch10.xhtml#ch01fn361-marker)) Michelle Ufford, M. Pacer, Matthew Seal,
    and Kyle Kelley, “Beyond Interactive: Notebook Innovation at Netflix,” *Netflix
    Technology Blog*, August 16, 2018, [*https://oreil.ly/EHvAe*](https://oreil.ly/EHvAe).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch10.xhtml#ch01fn361-marker)) Michelle Ufford, M. Pacer, Matthew Seal
    和 Kyle Kelley，“Netflix 的笔记本创新”，*Netflix 技术博客*，2018年8月16日，[*https://oreil.ly/EHvAe*](https://oreil.ly/EHvAe)。
- en: ^([25](ch10.xhtml#ch01fn362-marker)) For the uninitiated, a new pull request
    can be understood as a new piece of code being added to the codebase.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch10.xhtml#ch01fn362-marker)) 对于初学者来说，新的拉取请求可以理解为向代码库添加的新代码片段。
- en: ^([26](ch10.xhtml#ch01fn363-marker)) See [editor war](https://oreil.ly/OOkqJ),
    the decade-long, heated debate on Vim versus Emacs.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch10.xhtml#ch01fn363-marker)) 参见[编辑器之争](https://oreil.ly/OOkqJ)，这是关于
    Vim 与 Emacs 的长达十年的激烈辩论。
- en: '^([27](ch10.xhtml#ch01fn364-marker)) Chip Huyen, “Why Data Scientists Shouldn’t
    Need to Know Kubernetes,” September 13, 2021, [*https://huyenchip.com/2021/09/13/data-science-infrastructure.html*](https://huyenchip.com/2021/09/13/data-science-infrastructure.html);
    Neil Conway and David Hershey, “Data Scientists Don’t Care About Kubernetes,”
    Determined AI, November 30, 2020, [*https://oreil.ly/FFDQW*](https://oreil.ly/FFDQW);
    I Am Developer on Twitter (@iamdevloper): “I barely understand my own feelings
    how am I supposed to understand kubernetes,” June 26, 2021, [*https://oreil.ly/T2eQE*](https://oreil.ly/T2eQE).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch10.xhtml#ch01fn364-marker)) Chip Huyen，“为什么数据科学家不需要了解 Kubernetes”，2021年9月13日，[*https://huyenchip.com/2021/09/13/data-science-infrastructure.html*](https://huyenchip.com/2021/09/13/data-science-infrastructure.html)；Neil
    Conway 和 David Hershey，“数据科学家不关心 Kubernetes”，Determined AI，2020年11月30日，[*https://oreil.ly/FFDQW*](https://oreil.ly/FFDQW)；I
    Am Developer 在 Twitter 上 (@iamdevloper)：“我几乎不了解自己的感觉，怎么可能理解 Kubernetes”，2021年6月26日，[*https://oreil.ly/T2eQE*](https://oreil.ly/T2eQE)。
- en: '^([28](ch10.xhtml#ch01fn365-marker)) Abhishek Verma, Luis Pedrosa, Madhukar
    Korupolu, David Oppenheimer, Eric Tune, and John Wilkes, “Large-Scale Cluster
    Management at Google with Borg,” *EuroSys ’15: Proceedings of the Tenth European
    Conference on Computer Systems* (April 2015): 18, [*https://oreil.ly/9TeTM*](https://oreil.ly/9TeTM).'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '^([28](ch10.xhtml#ch01fn365-marker)) Abhishek Verma, Luis Pedrosa, Madhukar
    Korupolu, David Oppenheimer, Eric Tune, and John Wilkes，“Google 的大规模集群管理与 Borg”，*EuroSys
    ’15: 第十届欧洲计算机系统会议*（2015年4月）：18，[*https://oreil.ly/9TeTM*](https://oreil.ly/9TeTM)。'
- en: ^([29](ch10.xhtml#ch01fn366-marker)) When doing online prediction at a smaller
    scale, you can just hit an endpoint with payloads and get back predictions. Batch
    prediction requires setting up batch jobs and storing predictions.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch10.xhtml#ch01fn366-marker)) 在较小规模进行在线预测时，您只需命中端点并获取预测结果。批量预测需要设置批处理作业并存储预测结果。
- en: ^([30](ch10.xhtml#ch01fn367-marker)) Neal Lathia, “Building a Feature Store,”
    December 5, 2020, [*https://oreil.ly/DgsvA*](https://oreil.ly/DgsvA); Jordan Volz,
    “Why You Need a Feature Store,” *Continual*, September 28, 2021, [*https://oreil.ly/kQPMb*](https://oreil.ly/kQPMb);
    Mike Del Balso, “What Is a Feature Store?” *Tecton*, October 20, 2020, [*https://oreil.ly/pzy0I*](https://oreil.ly/pzy0I).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ^([30](ch10.xhtml#ch01fn367-marker)) Neal Lathia，“构建特征存储”，2020年12月5日，[*https://oreil.ly/DgsvA*](https://oreil.ly/DgsvA)；Jordan
    Volz，“为什么您需要特征存储”，*Continual*，2021年9月28日，[*https://oreil.ly/kQPMb*](https://oreil.ly/kQPMb)；Mike
    Del Balso，“什么是特征存储？”*Tecton*，2020年10月20日，[*https://oreil.ly/pzy0I*](https://oreil.ly/pzy0I)。
- en: '^([31](ch10.xhtml#ch01fn368-marker)) Jeremy Hermann and Mike Del Balso, “Meet
    Michelangelo: Uber’s Machine Learning Platform,” *Uber Engineering*, September
    5, 2017, [*https://oreil.ly/XteNy*](https://oreil.ly/XteNy).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '^([31](ch10.xhtml#ch01fn368-marker)) Jeremy Hermann 和 Mike Del Balso，“Meet
    Michelangelo: Uber 的机器学习平台”，*Uber 工程*，2017年9月5日，[*https://oreil.ly/XteNy*](https://oreil.ly/XteNy)。'
- en: ^([32](ch10.xhtml#ch01fn369-marker)) Some people use the term “feature transformation.”
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ^([32](ch10.xhtml#ch01fn369-marker)) 有些人使用术语“特征转换”。
- en: ^([33](ch10.xhtml#ch01fn370-marker)) Erik Bernhardsson on Twitter (@bernhardsson),
    September 29, 2021, [*https://oreil.ly/GnxOH*](https://oreil.ly/GnxOH).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ^([33](ch10.xhtml#ch01fn370-marker)) Erik Bernhardsson 在 Twitter 上 (@bernhardsson)，2021年9月29日，[*https://oreil.ly/GnxOH*](https://oreil.ly/GnxOH)。
