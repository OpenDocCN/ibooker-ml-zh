- en: Chapter 9\. Imbalanced Classes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。不平衡类别
- en: If you are classifying data, and the classes are not relatively balanced in
    size, the bias toward more popular classes can carry over into your model. For
    example, if you have 1 positive case and 99 negative cases, you can get 99% accuracy
    simply by classifying everything as negative. There are various options for dealing
    with *imbalanced classes*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在对数据进行分类，并且类的大小不相对平衡，那么对更流行的类别的偏向可能会在模型中体现出来。例如，如果您有1个正例和99个负例，您可以通过将所有内容分类为负例获得99％的准确率。处理*不平衡类别*的各种选项。
- en: Use a Different Metric
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用不同的度量标准
- en: One hint is to use a measure other than accuracy (AUC is a good choice) for
    calibrating models. Precision and recall are also better options when the target
    sizes are different. However, there are other options to consider as well.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一个提示是使用除准确率之外的度量（AUC是一个不错的选择）来校准模型。当目标大小不同时，精确度和召回率也是更好的选择。但是，还有其他考虑的选项。
- en: Tree-based Algorithms and Ensembles
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于树的算法和集成方法
- en: Tree-based models may perform better depending on the distribution of the smaller
    class. If they tend to be clustered, they can be classified easier.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的模型可能根据较小类的分布表现更好。如果它们倾向于聚集，它们可以更容易地被分类。
- en: Ensemble methods can further aid in pulling out the minority classes. Bagging
    and boosting are options found in tree models like random forests and Extreme
    Gradient Boosting (XGBoost).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法可以进一步帮助提取少数类。装袋和提升是树模型（如随机森林和极端梯度增强（XGBoost））中的选项。
- en: Penalize Models
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 惩罚模型
- en: Many scikit-learn classification models support the `class_weight` parameter.
    Setting this to `'balanced'` will attempt to regularize minority classes and incentivize
    the model to classify them correctly. Alternatively, you can grid search and specify
    the weight options by passing in a dictionary mapping class to weight (give higher
    weight to smaller classes).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 许多scikit-learn分类模型支持`class_weight`参数。将其设置为`'balanced'`将尝试正则化少数类，并激励模型正确分类它们。或者，您可以进行网格搜索，并通过传递将类映射到权重的字典来指定权重选项（给较小类更高的权重）。
- en: The [XGBoost](https://xgboost.readthedocs.io) library has the `max_delta_step`
    parameter, which can be set from 1 to 10 to make the update step more conservative.
    It also has the `scale_pos_weight` parameter that sets the ratio of negative to
    positive samples (for binary classes). Also, the `eval_metric` should be set to
    `'auc'` rather than the default value of `'error'` for classification.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[XGBoost](https://xgboost.readthedocs.io)库有`max_delta_step`参数，可以设置为1到10，使更新步骤更加保守。它还有`scale_pos_weight`参数，用于设置负样本到正样本的比例（针对二元类）。此外，对于分类，`eval_metric`应设置为`''auc''`而不是默认值`''error''`。'
- en: The KNN model has a `weights` parameter that can bias neighbors that are closer.
    If the minority class samples are close together, setting this parameter to `'distance'`
    may improve performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: KNN模型有一个`weights`参数，可以偏向邻近的邻居。如果少数类样本靠近在一起，将此参数设置为`'distance'`可能会提高性能。
- en: Upsampling Minority
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上采样少数类
- en: 'You can upsample the minority class in a couple of ways. Here is an sklearn
    implementation:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过几种方式增加少数类。以下是一个sklearn的实现：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can also use the imbalanced-learn library to randomly sample with replacement:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用imbalanced-learn库进行随机替换的采样：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Generate Minority Data
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成少数类数据
- en: The imbalanced-learn library can also generate new samples of minority classes
    with both the Synthetic Minority Over-sampling Technique (SMOTE) and Adaptive
    Synthetic (ADASYN) sampling approach algorithms. SMOTE works by choosing one of
    its k-nearest neighbors, connecting a line to one of them, and choosing a point
    along that line. ADASYN is similar to SMOTE, but generates more samples from those
    that are harder to learn. The classes in imbanced-learn are named `over_sampling.SMOTE`
    and `over_sampling.ADASYN`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: imbalanced-learn库还可以使用Synthetic Minority Over-sampling Technique（SMOTE）和Adaptive
    Synthetic（ADASYN）采样方法生成少数类的新样本。SMOTE通过选择其k个最近邻之一，连接到其中之一，并沿着该线选择一个点来工作。ADASYN与SMOTE类似，但会从更难学习的样本生成更多样本。imbanced-learn中的类名为`over_sampling.SMOTE`和`over_sampling.ADASYN`。
- en: Downsampling Majority
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下采样多数类
- en: 'Another method to balance classes is to downsample majority classes. Here is
    an sklearn example:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡类别的另一种方法是对多数类进行下采样。以下是一个sklearn的例子：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Don’t use replacement when downsampling.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行下采样时不要使用替换。
- en: 'The imbalanced-learn library also implements various downsampling algorithms:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: imbalanced-learn库还实现了各种下采样算法：
- en: '`ClusterCentroids`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterCentroids`'
- en: This class uses K-means to synthesize data with the centroids.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此类使用 K-means 来合成具有质心的数据。
- en: '`RandomUnderSampler`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomUnderSampler`'
- en: This class randomly selects samples.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此类随机选择样本。
- en: '`NearMiss`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`NearMiss`'
- en: This class uses nearest neighbors to downsample.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此类使用最近邻来进行下采样。
- en: '`TomekLink`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`TomekLink`'
- en: This class downsamples by removing samples that are close to each other.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此类通过移除彼此接近的样本来进行下采样。
- en: '`EditedNearestNeighbours`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`EditedNearestNeighbours`'
- en: This class removes samples that have neighbors that are either not in the majority
    or all of the same class.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此类移除具有邻居不在大多数或完全相同类别中的样本。
- en: '`RepeatedNearestNeighbours`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`RepeatedNearestNeighbours`'
- en: This class repeatedly calls the `EditedNearestNeighbours`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此类重复调用 `EditedNearestNeighbours`。
- en: '`AllKNN`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`AllKNN`'
- en: This class is similar but increases the number of nearest neighbors during the
    iterations of downsampling.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此类类似，但在下采样迭代期间增加了最近邻居的数量。
- en: '`CondensedNearestNeighbour`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`CondensedNearestNeighbour`'
- en: This class picks one sample of the class to be downsampled, then iterates through
    the other samples of the class, and if KNN doesn’t misclassify, it adds that sample.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此类选择要下采样的类的一个样本，然后迭代该类的其他样本，如果 KNN 不会误分类，则将该样本添加进去。
- en: '`OneSidedSelection`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneSidedSelection`'
- en: This classremoves noisy samples.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此类移除噪声样本。
- en: '`NeighbourhoodCleaningRule`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`NeighbourhoodCleaningRule`'
- en: This class uses `EditedNearestNeighbours` results and applies KNN to it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此类使用 `EditedNearestNeighbours` 的结果，并对其应用 KNN。
- en: '`InstanceHardnessThreshold`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`InstanceHardnessThreshold`'
- en: This class trains a model, then removes samples with low probabilities.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此类训练模型，然后移除概率低的样本。
- en: All of these classes support the `.fit_sample` method.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些类都支持 `.fit_sample` 方法。
- en: Upsampling Then Downsampling
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先上采样再下采样
- en: The imbalanced-learn library implements `SMOTEENN` and `SMOTETomek`, which both
    upsample and then apply downsampling to clean up the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: imbalanced-learn 库实现了 `SMOTEENN` 和 `SMOTETomek`，它们都是先上采样然后再应用下采样来清理数据。
