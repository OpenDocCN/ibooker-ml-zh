- en: 'Chapter 3\. Processes: Taming the Wild West of Machine Learning Workflows'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章。流程：驯服机器学习工作流的荒野
- en: “AI is in this critical moment where humankind is trying to decide whether this
    technology is good for us or not.”
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “AI正处于这个关键时刻，人类正在决定这项技术对我们是否有益。”
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Been Kim
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 被金（Been Kim）
- en: 'Despite its long-term promise, ML is likely overhyped today just like other
    forms of AI have been in the past (see, for example, the [first](https://oreil.ly/KWrci)
    and [second](https://oreil.ly/K3rgr) AI winters). Hype, cavalier attitudes, and
    lax regulatory oversight in the US have led to sloppy ML system implementations
    that frequently cause discrimination and privacy harms. Yet, we know that, at
    its core, ML is software. To help avoid failures in the future, all the documentation,
    testing, managing, and monitoring that organizations do with their existing software
    assets should be done with their ML projects, too. And that’s just the beginning.
    Organizations also have to consider the specific risks for ML: discrimination,
    privacy harms, security vulnerabilities, drift toward failure, and unstable results.
    After introducing these primary drivers of AI incidents and proposing some lower-level
    process solutions, this chapter touches on the emergent issues of legal liability
    and compliance. We then offer higher-level risk mitigation proposals related to
    model governance, AI incident response plans, organizational ML principles, and
    corporate social responsibility (CSR). While this chapter focuses on ways organizations
    can update their processes to better address special risk considerations for ML,
    remember that ML needs basic software governance as well.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习的长期前景令人期待，但今天的炒作可能与过去其他形式的人工智能一样被过度炒作（例如，请参见[第一](https://oreil.ly/KWrci)和[第二](https://oreil.ly/K3rgr)次AI寒冬）。美国的炒作、草率态度和松散的监管导致了粗糙的机器学习系统实施，这些系统经常引发歧视和隐私危害。然而，我们知道，从根本上讲，机器学习是软件。为了帮助避免未来的失败，组织在现有软件资产上所做的所有文档、测试、管理和监控工作，也应该用于他们的机器学习项目。而这只是开始。组织还必须考虑机器学习的特定风险：歧视、隐私危害、安全漏洞、向失败漂移和不稳定的结果。在介绍这些AI事件的主要驱动因素并提出一些较低级别的流程解决方案后，本章还触及了法律责任和合规性的新兴问题。然后我们提出了与模型治理、AI事件响应计划、组织机器学习原则和企业社会责任（CSR）相关的更高级别的风险缓解建议。尽管本章侧重于组织如何更新其流程以更好地应对机器学习的特殊风险考虑，但请记住，机器学习也需要基本的软件治理。
- en: Discrimination In, Discrimination Out
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 歧视输入，歧视输出
- en: 'We hear about many discriminatory algorithms these days, but discrimination
    tends to enter ML systems most often through poor experimental design or biased,
    unrepresentative, or mislabeled training data. This is a crucial process concern
    because business goals often define an ML model’s inherent experiment, and training
    data is usually collected or purchased as part of some broader organizational
    mechanism. When an organization is designing an ML system or selecting data for
    an ML project, discrimination can enter into the system in [many ways](https://oreil.ly/jaOT3),
    including:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们经常听到许多具有歧视性的算法，但歧视往往是通过糟糕的实验设计或偏见、不典型或错误标记的训练数据最常进入机器学习系统的方式。这是一个关键的过程问题，因为业务目标通常会定义机器学习模型固有的实验，并且训练数据通常是作为某种更广泛的组织机制的一部分收集或购买的。当一个组织正在设计机器学习系统或为机器学习项目选择数据时，歧视可以通过[多种方式](https://oreil.ly/jaOT3)进入系统，包括：
- en: Problem framing (e.g., association or label bias)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 问题框架（例如，关联或标签偏见）
- en: 'In ML, we essentially use a dataset to ask the question: is X predictive of
    y? Sometimes simply asking this question can set up a discriminatory premise.
    For instance, predicting criminal risk (y) based on facial characteristics (X),
    or using individual healthcare costs (y) as an inherently biased substitute for
    healthcare needs. Said another way, just because you have access to data on different
    topics, doesn’t mean that ML can link the two topics without introducing or perpetuating
    discrimination.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们基本上使用数据集来提出问题：X是否预测y？有时仅仅提出这个问题就可能建立一个歧视性的前提。例如，基于面部特征（X）预测犯罪风险（y），或者使用个人医疗费用（y）作为医疗需求的固有有偏替代品。换句话说，仅仅因为你可以访问不同主题的数据，并不意味着机器学习可以在不引入或持续歧视的情况下将这两个主题联系起来。
- en: Labeling or annotation (e.g., exclusion, sampling, reporting, label, or nonresponse
    bias)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 标记或注释（例如，排除、采样、报告、标签或非响应偏见）
- en: Data is often cleaned or preprocessed before it ever reaches an ML algorithm.
    These processes, if done without care, can introduce discrimination. For example,
    switching race to a numeric code, misinterpreting a coded value for a particular
    demographic group, or mislabeling sound or images due to unconscious human bias
    are just a few ways discrimination can seep into data cleaning or preprocessing.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在传递给机器学习算法之前通常会经过清理或预处理。如果处理不当，这些过程可能会引入歧视。例如，将种族转换为数值代码，误解特定人口群体的编码值，或由于无意识的人类偏见而错误地标记声音或图像，这些都是歧视可能渗入数据清理或预处理的几种方式。
- en: Unrepresentative data (e.g., selection or coverage bias)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 非代表性数据（例如，选择性或覆盖偏差）
- en: ML models require highly representative training data. Consider training a facial
    recognition classifier on face images collected in one country, for example, the
    US, and then applying it in another country, like Japan or Kenya. The chances
    are that the model will be less accurate for the people it learned less about
    during training. This is yet another way ML can be discriminatory.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型需要高度代表性的训练数据。例如，考虑在一个国家（如美国）收集的面部图像训练面部识别分类器，然后将其应用于另一个国家（如日本或肯尼亚）。在训练期间学习较少信息的人群，模型的准确性可能会降低。这是机器学习可能具有歧视性的另一种方式。
- en: Accurate data that is correlated to demographic group membership (e.g., historical
    or prejudice bias)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 准确的数据与人口统计群体成员资格相关联（例如，历史或偏见偏差）
- en: Data like traditional credit scores are accurate predictors of credit default,
    but due to long-running systemic inequalities in the US, some minorities have
    [lower average credit scores](https://oreil.ly/dYUl8) than Whites or Asians. It’s
    not necessarily wrong to use a traditional credit score in an ML system, but you
    have to be aware that data like this encodes information about demographic groups
    into your ML model and may lead to discriminatory ML system outcomes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 像传统信用评分这样的数据是信用违约的准确预测因子，但由于美国长期存在的系统性不平等，一些少数族裔的信用评分平均低于白人或亚裔。在机器学习系统中使用传统信用评分并不一定错误，但必须意识到，这类数据将关于人口统计群体的信息编码到您的机器学习模型中，并可能导致歧视性机器学习系统的结果。
- en: Accurate data that encodes discrimination (e.g., historical or prejudice bias)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 准确的数据编码包含歧视（例如，历史或偏见偏差）
- en: For example, data sampled from police or court records may be highly accurate,
    but it likely also contains historical and current racism. Models simply make
    decisions from what they learn in training data, so this kind of data should only
    be used with the utmost care in ML systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，从警察或法院记录中抽样的数据可能非常准确，但很可能也包含历史和当前的种族主义。模型仅仅从训练数据中学习做出决策，因此这类数据在机器学习系统中必须非常小心地使用。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: These topics are often discussed under the heading *ML fairness*. Of course,
    fairness has proven devilishly tricky to define mathematically, and the concept
    of [fairness](https://oreil.ly/vdpTC) is subject to varying political, cultural,
    and ethical interpretations. We use the term *discrimination* more frequently
    in this text due to its seemingly more narrow and clearly negative interpretation.^([1](ch03.xhtml#idm46137004264152))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主题通常在“机器学习公平性”下讨论。当然，从数学上定义公平性已被证明非常棘手，而“公平性”的概念受政治、文化和伦理解释的影响也不同。在本文中，由于其看似更狭窄和明确的负面解释，我们更频繁地使用“歧视”这个术语。^([1](ch03.xhtml#idm46137004264152))
- en: 'Once discriminatory data enters into an ML system, you can bet discriminatory
    predictions will quickly follow. A real difference between ML and human decision
    making is speed. ML systems can make decisions about a lot of people, very quickly.
    Moreover, the complexity of ML models can make finding discrimination more difficult
    than in traditional linear models. All of this can add up to a disastrous AI incident,
    like the one described in a recent [*Science* article](https://oreil.ly/3TcF4),
    where a major US insurer unintentionally used an allegedly discriminatory algorithm
    to allocate healthcare resources for perhaps millions of patients. Such discrimination
    can cause significant harm to consumers and regulatory and reputational problems
    for organizations. Luckily, you can test for many types of discrimination in ML
    system outputs before deployment. The major ways that discrimination can manifest
    in the output of ML systems are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦歧视性数据进入机器学习系统，歧视性预测很快就会出现。机器学习与人类决策之间的真正差异在于速度。机器学习系统可以非常快速地对大量人做出决策。此外，机器学习模型的复杂性可能会使在传统线性模型中发现歧视变得更加困难。所有这些因素加起来可能导致灾难性的人工智能事件，就像最近《科学》文章中描述的那种情况，一家美国主要保险公司无意中使用了一种据称具有歧视性的算法来分配医疗资源，可能涉及数百万患者。这种歧视可能对消费者造成重大伤害，并对组织的监管和声誉造成问题。幸运的是，在部署之前，您可以测试机器学习系统输出中许多类型的歧视。机器学习系统输出中歧视可能表现出的主要方式包括：
- en: Explicit discrimination
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 明示性歧视
- en: Demographic group membership, or direct proxies, are used directly in an ML
    system, resulting in unfavorable outcomes for historically disadvantaged groups.
    This is sometimes called “disparate treatment,” and it’s often an illegal business
    practice.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 人口统计群体成员身份或直接代理人被直接用于机器学习系统中，导致历史上处于劣势的群体遭受不利结果。这有时被称为“差异对待”，通常是非法的商业做法。
- en: Group outcome disparities
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 群体结果差异
- en: Learning different correlations between demographic groups and favorable model
    outcomes, resulting in disproportionately unfavorable outcomes for historically
    disadvantaged groups. This is sometimes called “disparate impact,” and it can
    also be illegal in some settings.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 学习不同的人口统计群体与有利的模型结果之间的相关性，导致历史上处于劣势的群体遭受不成比例的不利结果。这有时被称为“不同影响”，在某些情况下也可能是非法的。
- en: Group accuracy disparities
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 群体准确性差异
- en: Exhibiting different accuracies across demographic groups, especially when an
    ML system is less accurate for historically disadvantaged groups. Sometimes known
    as “differential validity,” this type of discrimination is also of interest to
    certain regulators in the US.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习系统对历史上处于劣势的群体不太准确时，尤其是在人口统计群体之间展示不同准确度时。有时被称为“差异有效性”，这种歧视也引起了美国某些监管机构的关注。
- en: Individual disparities
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 个体差异
- en: An ML system treats similarly situated individuals that differ only in terms
    of demographic group membership differently in terms of outcomes or accuracy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器学习系统在处理类似情况下的个体时，仅根据人口统计群体成员身份的差异而在结果或准确性上有所不同。
- en: Because of all the different vectors for discrimination to infect an ML system,
    it’s always safest to test for these types of discrimination and attempt to remediate
    any discovered discrimination. [Chapter 4](ch04.xhtml#technology_engineering_machine_learning_for_human_trust_and_understanding)
    will provide more details on testing ML systems for discrimination and how to
    address discovered discrimination.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于歧视可以影响机器学习系统的不同向量，因此始终最安全地测试这些类型的歧视，并尝试纠正发现的任何歧视。[第四章](ch04.xhtml#technology_engineering_machine_learning_for_human_trust_and_understanding)将详细介绍如何测试机器学习系统中的歧视以及如何解决发现的歧视问题。
- en: Algorithmic Discrimination and US Regulations
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 美国法规中的算法歧视
- en: Before moving onto data privacy and security, it’s important to mention laws
    and regulations on the books today that already address AI, ML, and discrimination.
    Given the recent rush of headlines on AI and ML discrimination, you could be forgiven
    for thinking this is a new problem. But that’s not true. Discrimination in testing
    and decision making has likely always been around and it has been studied for
    decades. So long, in fact, that specific discrimination tests and remediation
    tactics have become almost enshrined in US law and regulations. The key here is
    for practitioners to understand when those laws and regulations apply and when
    you might be more free to pursue your choice of discrimination testing and remediation
    strategies outside of regulatory frameworks. For practitioners in industry verticals
    like consumer credit, healthcare, employment, or education (and maybe others),
    testing for discrimination with a shiny new open-source package, instead of the
    legally required methods, could get your organization into a lot of trouble. Much
    like the data privacy and security topics in the next section, antidiscrimination
    regulatory and compliance concerns are a good reason to include legal personnel
    in ML projects.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入数据隐私和安全之前，有必要提到当前已有的涉及人工智能（AI）、机器学习（ML）和歧视的法律法规。最近关于AI和ML歧视的头条新闻涌现，你可能会误以为这是一个新问题。但事实并非如此。在测试和决策中的歧视问题可能一直存在，并且已经研究了数十年。事实上，特定的歧视测试和补救策略已经几乎成为美国法律和法规的一部分。关键在于从业者要理解这些法律和法规何时适用，以及何时可以更自由地进行歧视测试和补救策略，超出法规框架之外。对于像消费信贷、医疗保健、就业或教育等行业的从业者（可能还有其他行业），如果使用一个闪亮新的开源包进行歧视测试，而不是使用法律要求的方法，可能会给你的组织带来很多麻烦。就像接下来章节中的数据隐私和安全主题一样，反歧视的法规和合规问题是在ML项目中纳入法律人员的一个很好理由。
- en: Data Privacy and Security
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据隐私和安全
- en: 'Successful ML implementations require training data, and typically, lots of
    it. This means that data privacy and security concerns are relevant to ML workflows
    that consume and generate data. ML engineers and managers should acquaint themselves
    with the basics of their organization’s security and privacy policies, in addition
    to the major features of applicable privacy and security regulations. While there
    is not yet a nationwide uniform regulation for data privacy in the US, the combination
    of data security requirements, the EU GDPR, the CCPA, and many industry-specific,
    local, or emerging laws make the US a [heavily regulated region](https://oreil.ly/f8lWK)
    for data privacy and security. For ML practitioners, key concepts for data privacy
    to keep in mind include:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的ML实施需要训练数据，通常需要大量的数据。这意味着与消费和生成数据相关的ML工作流程需要关注数据隐私和安全问题。ML工程师和管理者应该了解他们组织安全和隐私政策的基本内容，以及适用的主要隐私和安全法规的主要特点。尽管在美国尚无全国性统一的数据隐私法规，但数据安全要求、欧盟GDPR、加州CCPA以及许多行业特定、地方性或新兴法律的结合，使美国成为数据隐私和安全方面的[重度监管地区](https://oreil.ly/f8lWK)。对于ML从业者来说，数据隐私的关键概念包括：
- en: Consent for use
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用同意
- en: Though sometimes burdensome for consumers, most current privacy laws put forward
    notions of consumer consent for data usage. Training ML systems or adding new
    ML system features without the appropriate consideration for consent could cause
    big problems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有时对消费者来说可能是负担，大多数当前的隐私法律提出了数据使用的消费者同意的概念。在没有适当考虑同意的情况下训练ML系统或添加新的ML系统功能，可能会引发重大问题。
- en: Legal basis for data collection
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集的法律基础
- en: 'The GDPR lays out six valid reasons that consumer personal data can be collected
    and used: consumer consent, contractual obligation, legal obligation, public interest
    (i.e., public or governmental tasks), vital interest (i.e., to save the consumer’s
    life), or legitimate business interests (e.g., appropriate marketing activities).
    ML training data should probably be collected for one of these reasons too, or
    an ML system could raise unpleasant questions down the road.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: GDPR明确列出了六种合法收集和使用消费者个人数据的理由：消费者同意、合同义务、法律义务、公共利益（例如公共或政府任务）、生命安全（例如拯救消费者生命）或合法商业利益（例如适当的营销活动）。ML训练数据很可能也应该基于这些理由之一收集，否则ML系统可能会在未来引发不愉快的问题。
- en: Alignment with privacy policy
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与隐私政策的一致性
- en: Many organizations have privacy policies, and if you or your ML system violate
    that privacy policy, it may cause regulatory or reputational headaches for your
    organization.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织都有隐私政策，如果您或您的机器学习系统违反了该隐私政策，可能会给您的组织带来监管或声誉方面的麻烦。
- en: Anonymization requirements
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 匿名化要求
- en: Laws such as HIPAA and GDPR include data anonymization requirements applicable
    to ML training data. If you’re working with personal or sensitive data in an ML
    project, it should probably be anonymized (even though true anonymization has
    proven difficult in practice).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 像HIPAA和GDPR这样的法律包含适用于机器学习培训数据的数据匿名化要求。如果在机器学习项目中处理个人或敏感数据，可能需要对其进行匿名化处理（尽管在实践中真正的匿名化被证明很困难）。
- en: Retention requirements and limitations
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 保留要求和限制
- en: Sensitive consumer data often comes with conditions on how long it must be stored
    or when it must be destroyed. These requirements and constraints should likely
    be considerations for data selection and generation in ML systems.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感消费者数据通常伴随着存储期限或销毁时间的条件。这些要求和限制可能会成为机器学习系统中数据选择和生成的考虑因素。
- en: 'From a data security perspective, goals and failures are usually defined in
    terms of the confidentiality, integrity, and availability (CIA) triad. The CIA
    triad can be briefly summarized as: data should only be available to authorized
    users, data should be correct and up-to-date, and data should be promptly available
    when needed. If one of these tenets is broken, this is usually a security incident.
    To avoid incidents involving ML-related data, these basic best practices can be
    helpful:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据安全的角度来看，目标和失败通常以机密性、完整性和可用性（CIA）三元组来定义。CIA三元组可以简要总结为：数据只能供授权用户使用，数据应正确且及时更新，需要时数据应能迅速获得。如果其中一项原则被违反，通常会发生安全事件。为避免涉及机器学习相关数据的事件，以下基本最佳实践可能会有所帮助：
- en: Access control
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 访问控制
- en: Limit access to training data, particularly personal or sensitive data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 限制对培训数据的访问权限，特别是个人或敏感数据。
- en: Authentication
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 身份验证
- en: For those with access to ML-related data, require strong passwords or other
    authentication to access training data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于访问机器学习相关数据的人员，要求使用强密码或其他身份验证方式来访问培训数据。
- en: User permissions
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 用户权限
- en: Review and update user permissions frequently. Use the concept of “least privilege”
    in which all personnel receive the lowest possible access level. Strictly limit
    the number of administrative or “root” users.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 定期审查和更新用户权限。采用“最小权限”概念，确保所有人员获得最低可能的访问级别。严格限制管理或“根”用户的数量。
- en: Remote access
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 远程访问
- en: Limit and monitor remote access to ML-related data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 限制和监控与机器学习相关数据的远程访问。
- en: Third parties
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第三方
- en: Verify that third-party data providers and consumers follow reasonable security
    standards.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 确保第三方数据提供者和消费者遵循合理的安全标准。
- en: Physical media
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 物理介质
- en: Protect documents, thumb drives, backup media, and other portable data sources.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 保护文件、闪存驱动器、备份介质和其他可移动数据源。
- en: The above principles and more may fall under the [FTC’s reasonable security
    purview](https://oreil.ly/AELq3). This means that violations can be a big deal
    for your organization. Moreover, privacy and security breaches must often be reported
    to the proper authorities. Of course, breach reporting and other incident response
    steps should be part of written incident response plans. These plans should be
    followed when incidents occur, and they should be reassessed and updated frequently.
    Later in [Chapter 3](#processes_taming_the_wild_west_of_machine_learning_workflows),
    we’ll address AI incident response specifically. Next, we’ll go over dangerous
    new vectors for attackers to extract ML-training data and the problems it can
    cause.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上述原则及更多内容可能属于[FTC合理安全监管范围](https://oreil.ly/AELq3)。这意味着对您的组织来说，违规可能是个大问题。此外，隐私和安全违规通常需要向适当的管理机构报告。当然，违规报告和其他事件响应步骤应包括在书面事件响应计划中。发生事件时应遵循这些计划，并定期重新评估和更新。稍后在[第三章](#processes_taming_the_wild_west_of_machine_learning_workflows)中，我们将具体讨论AI事件响应。接下来，我们将介绍攻击者提取机器学习培训数据的危险新向量及其可能引发的问题。
- en: Machine Learning Security
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习安全
- en: If “[t]he worst enemy of security is complexity,” according to [Bruce Schneier](https://oreil.ly/Jpfwe),
    ML may be innately insecure. Other researchers have also released numerous studies
    describing and confirming [specific security vulnerabilities](https://oreil.ly/bi__1)
    for ML systems. And we’re now beginning to see how real-world attacks occur, like
    [Islamic State operatives blurring their logos](https://oreil.ly/btXxC) in online
    content to evade social media filters. Since organizations often take measures
    to secure valuable software and data assets, ML systems should be no different.
    Beyond specific incident response plans, several additional information security
    processes should be applied to ML systems. These include security audits, bug
    bounties, and red teaming.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果"[安全的最大敌人是复杂性](https://oreil.ly/Jpfwe)，" 根据[Bruce Schneier](https://oreil.ly/Jpfwe)，ML可能天生不安全。其他研究人员还发布了大量描述和证实[ML系统的具体安全漏洞](https://oreil.ly/bi__1)的研究。现在我们开始看到现实世界中的攻击是如何发生的，比如[伊斯兰国的操作人员在在线内容中模糊其标志](https://oreil.ly/btXxC)以逃避社交媒体过滤器。由于组织通常会采取措施保护宝贵的软件和数据资产，ML系统也应该不例外。除了具体的事件响应计划外，还应该将几种额外的信息安全流程应用于ML系统，包括安全审核、漏洞赏金和红队攻击。
- en: 'The primary security threats for ML today appear to be:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当今ML的主要安全威胁似乎是：
- en: Insider manipulation of ML-system training data or software
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部人员操纵ML系统的训练数据或软件
- en: Manipulation of ML-system outcomes by external adversaries
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部对ML系统结果的操控
- en: Extraction of ML-system logic or training data by external adversaries
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部对ML系统逻辑或训练数据的提取
- en: Trojans buried in third-party ML software, models, data, or other artifacts
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入第三方ML软件、模型、数据或其他工件中的特洛伊木马
- en: For mission-critical, or otherwise high stakes, deployments of ML, systems should
    be audited for at least these known vulnerabilities. Audits can be conducted internally
    or by specialist teams in what’s known as *red teaming*, as is done by [Facebook](https://oreil.ly/NgXLg).
    Bug bounties, or when organizations offer monetary rewards to the public for finding
    vulnerabilities, are another practice from general information security that should
    probably also be applied to ML systems. Moreover, audits, red teaming, and bug
    bounties need not be limited to security concerns alone. These types of processes
    can also be used to spot other ML-system problems, such as discrimination or instability,
    and spot them before they explode into AI incidents.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些使命关键或者具有高风险的ML部署，系统应至少审核这些已知的漏洞。审核可以由内部进行，也可以由专业团队进行，这就是所谓的*红队攻击*，就像[Facebook](https://oreil.ly/NgXLg)所做的那样。漏洞赏金，即组织向公众提供金钱奖励以发现漏洞，是另一种来自一般信息安全的实践，可能也应用于ML系统。此外，审核、红队攻击和漏洞赏金不仅仅局限于安全问题。这些类型的过程还可以用来发现其他ML系统问题，比如歧视或不稳定性，并在其发展为AI事件之前予以发现。
- en: Legality and Compliance
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合法性与合规性
- en: ML can create a lot of value for organizations. But given the real discrimination,
    privacy, and security concerns, among others, it can also bring on serious reputational
    damage or legal liabilities. These include causing your organization to be slapped
    with a lawsuit or to run afoul of local, federal, or international regulations.
    Difficult questions about compliance and legality often slam the brakes on end-stage
    ML products and projects because oversight personnel are seldom involved in the
    build stages of ML endeavors. Moreover, ML, like many other powerful commercial
    technologies that came before it, is likely to be highly regulated in the future.
    With increasing international regulation, US government regulatory agencies, such
    as CFPB, FINRA, and FTC making announcements about ML guidance, and state regulators
    announcing various ML discrimination investigations, now is a good time to consider
    your ML systems in the current and evolving ML legal and compliance landscape.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ML可以为组织创造大量价值。但考虑到真正的歧视、隐私和安全问题等，它也可能带来严重的声誉损害或法律责任。这些问题包括导致您的组织被起诉或违反地方、联邦或国际法规。合规性和法律问题常常使得ML产品和项目在最终阶段遭遇阻碍，因为监管人员很少参与ML事业的建设阶段。此外，像许多先前的强大商业技术一样，ML未来可能会受到高度监管。随着国际监管的增加，美国政府的监管机构，如CFPB、FINRA和FTC发布有关ML指导的公告，以及州监管机构发布各种ML歧视调查的公告，现在是考虑您的ML系统在当前和不断发展的ML法律和合规性环境中的良好时机。
- en: As mentioned previously, some regulations can impact your ML system today, especially
    in healthcare, financial services, and employment. Under laws and regulations
    like ECOA, FCRA, FHA, SR 11-7, and under EEOC guidelines, ML systems are generally
    expected to be mathematically sound and stable, exhibit minimal discrimination,
    and be explainable. Outside these verticals, your ML system could still be held
    to local anti-discrimination laws, reasonable security standards, unfair and deceptive
    practice (UDAP) laws, and to scrutiny related to its terms of service or warranty
    (or lack thereof). Today, violations of these laws and regulations can result
    in regulatory fines, litigation costs, reputational damage for your organization,
    and harm to consumers. Moreover, government agencies have telegraphed increased
    future ML regulation, or, outside of the US, started to implement such regulations.
    For a more detailed list of US and international ML guidance documents, see the
    [Awesome Machine Learning Interpretability](https://oreil.ly/TIe_U) metalist.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，某些法规今天可能会对你的机器学习系统产生影响，特别是在医疗保健、金融服务和就业领域。根据ECOA、FCRA、FHA、SR 11-7等法律和法规，以及根据EEOC指南，机器学习系统通常被期望是数学上健全和稳定的，表现出最小的歧视，并且具有解释性。在这些垂直领域之外，你的机器学习系统仍可能受到当地的反歧视法、合理的安全标准、不公平和欺骗行为（UDAP）法律以及与其服务条款或保证（或其缺乏）相关的审查的影响。今天，违反这些法律和法规可能会导致你的组织面临监管罚款、诉讼成本、声誉损害以及对消费者造成伤害。此外，政府机构已经预示未来对机器学习的监管将会增加，或者在美国以外开始实施此类法规。关于美国和国际机器学习指导文件的更详细清单，请参阅[Awesome
    Machine Learning Interpretability](https://oreil.ly/TIe_U)元目录。
- en: 'As of today, US government agencies are generally advising that your ML be
    documented, explainable, managed, monitored, and minimally discriminatory. Of
    course, we’re not lawyers or regulators, and it’s not our place to give legal
    advice or determine what is compliant with regulations. So, take a look at the
    documents highlighted below to see for yourself what some US regulators and agencies
    are saying about ML:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 截至今天，美国政府机构一般建议你的机器学习应当是有文档记录的、可以解释的、被管理的、被监控的，并且具有最小的歧视性。当然，我们不是律师或监管机构，不应该提供法律建议或确定什么符合法规。因此，请查看下面突出显示的文件，自行了解一些美国监管机构和机构对机器学习的看法：
- en: '[*Artificial Intelligence (AI) in the Securities Industry*](https://oreil.ly/3kWpK)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*证券行业中的人工智能（AI）*](https://oreil.ly/3kWpK)'
- en: '[*A Primer on Artificial Intelligence in Securities Markets*](https://oreil.ly/8AyeM)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*证券市场中人工智能入门*](https://oreil.ly/8AyeM)'
- en: '[*Innovation spotlight: Providing adverse action notices when using AI/ML models*](https://oreil.ly/-A7PQ)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*创新聚焦：使用AI/ML模型时提供不利行动通知*](https://oreil.ly/-A7PQ)'
- en: '[“Office of Management and Budget Draft Guidance for Regulation of Artificial
    Intelligence Applications”](https://oreil.ly/WcgLJ)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“管理与预算办公室关于人工智能应用监管的草案指导”](https://oreil.ly/WcgLJ)'
- en: '[*Using Artificial Intelligence and Algorithms*](https://oreil.ly/DMp1p)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*使用人工智能和算法*](https://oreil.ly/DMp1p)'
- en: If you’re feeling overwhelmed by the combination of ML, law, and regulation,
    there are current examples on which to pattern your organization’s future ML endeavors.
    Organizations that operate under current ML-related regulations, or that already
    learned their lesson about playing fast and loose with ML, often ascribe to a
    practice known as model governance. The next section will attempt to summarize
    practices from that field.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对机器学习、法律和监管的结合感到不知所措，这里有一些当前的例子，可以作为你的组织未来机器学习努力的模式。那些在当前与机器学习相关的法规下运作，或者已经吸取了在机器学习上玩快和松的教训的组织，通常会遵循一种被称为模型治理的实践。接下来的章节将试图总结该领域的实践。
- en: Model Governance
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型治理
- en: 'The decision to move into the world of ML is not a simple undertaking and smart
    leaders can be left asking, “how can I mitigate the risks for my organization?”
    Luckily, there are mature model governance practices crafted by [government agencies](https://oreil.ly/380Gy)
    and [private companies](https://oreil.ly/myISS) that your organization can use
    to get started. This section will highlight some of the governance structures
    and processes your organization can employ to ensure fairness, accountability,
    and transparency in your ML functions. This discussion is split into three major
    sections: model monitoring, model documentation, and organizational concerns.
    We’ll wrap up the discussion of model governance with some brief advice for practitioners
    looking for just the bare bones needed to get started on basic model governance.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要进入机器学习的世界并不是简单的事情，聪明的领导者可能会问：“我如何减轻组织面临的风险呢？”幸运的是，一些成熟的模型治理实践由[政府机构](https://oreil.ly/380Gy)和[私营公司](https://oreil.ly/myISS)精心制定，您的组织可以利用这些实践来开始。本节将重点介绍您的组织可以采用的一些治理结构和流程，以确保机器学习功能的公平性、问责性和透明度。本讨论分为三个主要部分：模型监控、模型文档化和组织关注事项。我们将通过一些简短的建议来结束模型治理的讨论，这些建议适用于寻求开始基本模型治理所需的基本要素的从业者。
- en: Model Monitoring
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型监控
- en: Model monitoring is a stage in the ML lifecycle that involves keeping tabs on
    your ML system while it is making predictions or decisions on new, live data.
    There’s lots to be aware of when monitoring ML models. First and foremost is model
    decay. Model decay is a common failure mode for ML systems. It happens when the
    characteristics of live data coming into an ML system drift away from those of
    the training data, making the underlying ML model less accurate. Model drift is
    most often described in terms of decreasing model accuracy, but can also affect
    the fairness or security of ML systems. Model drift is typically detected by monitoring
    the statistical properties of model inputs and predictions and comparing them
    to those recorded at training time. For fairness and security, monitoring could
    involve real-time discrimination testing and ongoing red-teaming or security audits
    of deployed ML systems, respectively. Anytime a significant drift is detected,
    system stakeholders should be alerted. To address accuracy drift, ML systems are
    typically retrained with new data when drift is detected, or at frequent intervals
    to avoid drift altogether. Addressing drifts in fairness or security is a more
    novel pursuit and standard practices are not yet established. However, the discrimintion
    testing and remediation and security countermeasures discussed elsewhere in this
    report could also be helpful in this regard.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控是机器学习生命周期中的一个阶段，涉及在模型基于新的实时数据进行预测或决策时对其进行监视。在监控机器学习模型时，有很多需要注意的地方。首先是模型衰退。模型衰退是机器学习系统的常见故障模式。当实时数据的特征与训练数据的特征偏离时，会导致基础机器学习模型的准确性下降。模型漂移通常描述为模型准确性下降，但也可能影响机器学习系统的公平性或安全性。通常通过监控模型输入和预测的统计特性，并将其与训练时记录的统计数据进行比较来检测模型漂移。对于公平性和安全性，监控可能涉及实时歧视测试以及对已部署的机器学习系统进行持续的红队测试或安全审计。一旦检测到重大漂移，应立即通知系统的利益相关者。为了解决准确性漂移，通常会在检测到漂移时使用新数据对机器学习系统进行重新训练，或者在频繁的时间间隔内进行重新训练，以避免漂移。解决公平性或安全性方面的漂移是一项较新的探索，尚未建立标准做法。但是，本报告中讨论的歧视测试和红队测试以及安全对策也可能在这方面提供帮助。
- en: Another major topic in model monitoring is anomaly detection. Strange input
    or output values from an ML system can be indicative of stability problems or
    security and privacy vulnerabilities. It’s possible to use statistics, ML, and
    business rules to monitor anomalous behavior in both inputs and outputs, and across
    an entire ML system. Just like when model drift is detected, system stakeholders
    must be made aware of anomalous ML system inputs and outputs. Two additional and
    worrisome scenarios for which to monitor are error propagation and feedback loops.
    Error propagation refers to problems in the output of some data or ML system leading
    to worsening errors in the consuming ML system or in subsequent downstream processes.
    Feedback loops can occur in applications like predictive policing or algorithmic
    trading, leading to serious external risks. Whenever an ML system can affect the
    real-world, and then have that outcome act as an input to the ML system again,
    feedback loops and AI incidents can ensue.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控中的另一个主要话题是异常检测。ML系统的奇怪输入或输出值可能表明稳定性问题或安全性和隐私漏洞。可以使用统计、ML和业务规则来监控输入和输出的异常行为，以及整个ML系统。就像检测到模型漂移时一样，必须让系统相关方了解ML系统输入和输出的异常情况。还有两种额外且令人担忧的监测场景是错误传播和反馈环路。错误传播是指某些数据或ML系统输出中的问题导致消费ML系统或后续下游流程中错误恶化。反馈环路可能发生在预测性执法或算法交易等应用中，可能导致严重的外部风险。每当ML系统能够影响现实世界，并且其结果作为ML系统再次输入时，就可能发生反馈环路和AI事件。
- en: Monitoring your ML in production environments is extremely important as this
    can quickly catch degradation in accuracy or changes in the fairness, security,
    and stability characteristics of your system—before they become an AI incident.
    To get the most out of model monitoring, you’ll need to know what your system
    looked like at training time and who to call when things go wrong. The perfect
    place for those details is in model documentation, which we’ll discuss next.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中监控您的ML是非常重要的，因为这可以迅速捕捉精度下降或系统公正性、安全性和稳定性特征变化之前的情况，防止它们成为AI事件。为了充分利用模型监控，您需要了解在训练时系统的状态以及在出现问题时该联系谁。这些详细信息的最佳存放地点是模型文档，接下来我们将讨论它。
- en: Model Documentation
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型文档
- en: 'All organizational predictive models should be inventoried and documented.
    When done correctly, model documentation should provide all pertinent technical,
    business, and personnel information about a model, enabling detailed human review,
    maintenance continuity, and some degree of incident response. Moreover, in some
    industries, model documentation is already a [regulatory requirement](https://oreil.ly/90qH8).
    The main drawback of model documentation is that it is tedious and time consuming,
    sometimes taking longer to write the documentation than to train the ML model
    itself. One answer to this problem was provided by Google research in their recent
    model cards and datasheet work. Model cards and datasheets provide quick, summary
    information about the [models](https://oreil.ly/Fqgug) and [data](https://oreil.ly/pzVBW)
    used in ML systems, respectively. Another promising answer has started to emerge
    in the commercial analytics market: [automatic model documentation](https://oreil.ly/NvMpq).
    Purchasing or building ML software that creates model documents along with your
    ML-model training can be a great solution for ML teams looking to document their
    models and save time on resource-intensive model governance activities. Of course,
    even if model documentation is generated automatically, humans must still read
    the documentation and raise concerns when necessary.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所有组织的预测模型都应进行清单和文档化。如果做得正确，模型文档应提供有关模型的所有相关技术、业务和人员信息，支持详细的人工审查、维护连续性以及某种程度的事件响应。此外，在一些行业中，模型文档已经是法规要求的一部分。模型文档的主要缺点是繁琐和耗时，有时编写文档的时间甚至比训练ML模型本身还要长。Google研究在他们最近的模型卡和数据表工作中提供了解决此问题的一种方案。模型卡和数据表分别提供关于ML系统中使用的模型和数据的快速摘要信息。在商业分析市场上，另一个有前景的解决方案已经开始出现：自动模型文档化。购买或构建能够在ML模型训练时同时创建模型文档的ML软件，对于希望为其模型文档化工作节省时间的ML团队来说是一个很好的解决方案。当然，即使模型文档是自动生成的，人类仍然必须阅读文档，并在必要时提出关注。
- en: Hierarchy and Teams for Model Governance
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型治理的层次结构和团队
- en: To guarantee models are designed, deployed, and managed responsibly, properly
    defining the organization that will execute these responsibilities is crucial.
    In [Figure 3-1](#fig_1__a_basic_overview_of_a_proposed_organizational_st) we propose
    a simple structure for organizations looking to build ML into their operational
    processes. This chart uses the acronym *D&A*, a common industry shorthand for
    *data and analytics groups*, especially in years past.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保模型的设计、部署和管理负责任，正确定义将执行这些职责的组织结构至关重要。在 [图 3-1](#fig_1__a_basic_overview_of_a_proposed_organizational_st)
    中，我们提出了一种适合希望将机器学习融入其运营流程的组织简单结构。此图使用了“D&A”这一行业常用缩写，特别是在过去几年中。
- en: '![  A basic overview of a proposed organizational structure for data and analytics  D
    A  groups. Figure courtesy of Ben Cox and H2O.ai.](Images/reml_0301.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![提议的数据与分析（D&A）团队组织结构的基本概述。图由Ben Cox和H2O.ai提供。](Images/reml_0301.png)'
- en: Figure 3-1\. Basic overview of a proposed organizational structure for data
    and analytics (D&A) groups (courtesy of Ben Cox and H2O.ai).
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. 提议的数据与分析（D&A）团队组织结构的基本概述（由Ben Cox和H2O.ai提供）。
- en: 'One early problem in D&A adoption was that companies leaned into one of two
    less than ideal structures:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据与分析采纳的早期问题之一是公司倾向于不完美的两种结构之一：
- en: D&A as a unique group within every line of business (horizontal)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 数据与分析作为每个业务线内的独特群体（横向）。
- en: D&A resources in each business unit spend too much time building redundant tools
    that exist in other areas of the firm, and because the teams are disparate, the
    organization is not taking advantage of cross-functional synergies and often create
    duplicate customer or transaction data that hinders operations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 每个业务单元的数据与分析资源花费过多时间构建其他部门已有的冗余工具，因为团队分散，组织未能利用跨职能协同效应，并经常产生妨碍运营的重复客户或交易数据。
- en: D&A as a single line of business (LOB) reporting out to all other LOBs (vertical)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 数据与分析作为所有其他业务线（纵向）的报告部门（LOB）。
- en: A silo effect forces the D&A unit to spend too much time correcting disconnects
    between respective lines of business and responding to ad hoc requests to spend
    time on new value-driving opportunities. Also, friction can develop between incumbent
    areas of ML expertise, like credit risk and the silo.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 系统性隔离效应迫使数据与分析部门花费过多时间纠正各业务线之间的脱节，并响应临时要求，以便专注于新的驱动价值的机会。此外，各领域的机器学习专业知识（例如信用风险）与系统性隔离之间可能存在摩擦。
- en: Learning from these lessons by having analytics, data science, and ML functions
    operate at the cross section of different groups within an organization can minimize
    classic data management obstacles and technical debt through increased transparency
    and collaboration. Additionally, the cross-sectional option includes a centralized
    AI and ML Center of Excellence, a specific unit focused on the highest value or
    novel ML pursuits across organizations and capabilities.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过让分析、数据科学和机器学习职能在组织内不同组之间的交叉点运作，可以最小化经典的数据管理障碍和技术债务，通过增加透明度和协作。此外，交叉部门选项还包括一个集中的人工智能和机器学习卓越中心，专注于跨组织和能力的最高价值或新型机器学习追求。
- en: '[Figure 3-2](#fig_2__a_proposed_ml_model_governance_reporting_organiz) puts
    forward an idealized operating architecture for a contemporary D&A team that’s
    ready to tackle ML adoption responsibly. In [Figure 3-2](#fig_2__a_proposed_ml_model_governance_reporting_organiz),
    we can see that technical functions roll up to a single accountable executive
    while also serving all LOBs.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-2](#fig_2__a_proposed_ml_model_governance_reporting_organiz) 提出了一个现代化数据与分析团队的理想操作架构，以负责任地采纳机器学习。在
    [图 3-2](#fig_2__a_proposed_ml_model_governance_reporting_organiz) 中，我们可以看到技术职能向单个负责的执行官汇报，同时也为所有业务线提供服务。'
- en: '![A proposed ML model governance reporting organizational hierarchy. Figure
    courtesy of Ben Cox and H2O.ai.](Images/reml_0302.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![提议的机器学习模型治理报告组织层次结构。图由Ben Cox和H2O.ai提供。](Images/reml_0302.png)'
- en: Figure 3-2\. A proposed ML model governance reporting organizational hierarchy
    (courtesy of Ben Cox and H2O.ai).
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. 提出的机器学习模型治理报告组织层次结构（由Ben Cox和H2O.ai提供）。
- en: Finally, [Figure 3-3](#fig_3__proposed_model_governance_workflow_and_organizat)
    illustrates how technical, responsible ML functions, defined in [Figure 1-2](ch01.xhtml#fig_2_a_responsible_machine_learning_workflow_diagram_a),
    fit into the larger organizational hierarchy proposed in [Figure 3-2](#fig_2__a_proposed_ml_model_governance_reporting_organiz).
    As is common in model governance, data science and analytics teams train ML models,
    while other groups act as defense lines to challenge, validate, and monitor those
    models.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，[图 3-3](#fig_3__proposed_model_governance_workflow_and_organizat) 展示了技术上负责的机器学习功能如何融入[图 1-2](ch01.xhtml#fig_2_a_responsible_machine_learning_workflow_diagram_a)
    中定义的更大的组织层次结构，该结构提出在[图 3-2](#fig_2__a_proposed_ml_model_governance_reporting_organiz)
    中。正如在模型治理中普遍存在的那样，数据科学和分析团队训练机器学习模型，而其他团队则作为防线挑战、验证和监视这些模型。
- en: '![Proposed model governance workflow and organizational responsibility architecture.
    Figure courtesy of Ben Cox and H2O.ai.](Images/reml_0303.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![建议的模型治理工作流程和组织责任架构。图由Ben Cox和H2O.ai提供。](Images/reml_0303.png)'
- en: Figure 3-3\. Proposed model governance workflow and organizational responsibility
    architecture (courtesy of Ben Cox and H2O.ai).
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. 建议的模型治理工作流程和组织责任架构（由Ben Cox和H2O.ai提供）。
- en: Model Governance for Beginners
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初学者的模型治理
- en: 'If you’re at a small or young organization, you may need just the bare bones
    of model governance. Two of the most crucial model governance processes are model
    documentation and model monitoring:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在一家小型或年轻的组织中，您可能只需要最基本的模型治理。两个最关键的模型治理流程是模型文档和模型监控：
- en: Basic model documentation
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型文档
- en: Model documentation should contain the who, what, when, where, and how for any
    personnel, hardware, data, or algorithms used in an ML system. These documents
    should enable new employees to understand how an ML system works so they can take
    over maintenance, or they should facilitate third parties in performing detailed
    investigations of the model in case of failures and attacks. And yes, these documents
    can be very long.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 模型文档应包含任何人员、硬件、数据或算法在机器学习系统中的使用情况的“谁、什么、何时、何地、以及如何”。这些文档应使新员工能够理解机器学习系统的工作原理，以便他们可以接管维护，或者应在失败和攻击发生时促使第三方进行详细调查。是的，这些文档可能会非常长。
- en: Basic model monitoring
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型监控
- en: Even if you never touch your ML system’s code, its outputs can change with the
    new data it’s encountering. This rarely happens in a way that’s beneficial for
    ML systems, and sadly, most ML systems are destined to drift toward failure. Hence,
    ML systems must be monitored. Typically, monitoring is used to watch the inputs
    or outputs of an ML system change over time, particularly for model accuracy.
    However, it can’t hurt to monitor for drift in fairness or security characteristics
    as well.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您从未接触过您的机器学习系统的代码，它的输出也会随着其遇到的新数据而变化。这种情况很少对机器学习系统有利，而且可悲的是，大多数机器学习系统注定会朝着失败的方向漂移。因此，机器学习系统必须进行监控。通常，监控用于观察机器学习系统的输入或输出随时间的变化，特别是模型的准确性。但是，对公平性或安全特性的漂移进行监控也无妨。
- en: If your organization is not in a place to devote large amounts of resources
    to model governance, whole-hearted investment in these two practices, plus preparing
    for AI incidents as discussed below, can take you a long way toward mitigating
    ML risks.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的组织没有足够的资源投入模型治理，那么全心全意投入以下两项实践，以及按下面讨论的方式准备AI事故，可以大大减少机器学习风险。
- en: AI Incident Response
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 事故响应
- en: Like nearly all of the commercial technologies that came before it, ML systems
    fail and can be attacked. To date, there have been over 1,000 public reports of
    such incidents. Even our most secure, regulated, and monitored commercial technologies,
    like airliners and nuclear reactors, experience attacks and failures. Given that
    very few organizations are auditing and monitoring ML systems with the same rigor,
    and that [regulatory interest in ML systems is on the rise](https://oreil.ly/QVC4t),
    we’ll probably hear more about AI incidents in the next few years. Furthermore,
    when a technology is important to an organization’s mission, it’s not uncommon
    to have built-in redundancy and incident response plans. ML systems should be
    no different. Having a plan in place for ML system failures or attacks can be
    the difference between a glitch in system behavior and a serious AI incident with
    negative consequences for both the organization and the public.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前面的几乎所有商业技术一样，ML系统会失败并且可能受到攻击。迄今为止，已经有超过1,000起此类事件的公开报告。即使是我们最安全、受监管和监控的商业技术，如客机和核反应堆，也会遭受攻击和故障。鉴于很少有组织像同样严格审计和监控ML系统，而且[ML系统的监管兴趣正在上升](https://oreil.ly/QVC4t)，我们可能在未来几年听到更多关于AI事件的消息。此外，当一项技术对组织的使命至关重要时，内置冗余和事件响应计划并不罕见。ML系统也应该如此。对ML系统故障或攻击制定计划可以成为区分系统行为故障和对组织及公众产生负面后果的严重AI事件之间的区别。
- en: The stress and confusion of an active AI incident can make incident response
    difficult. Who has the authority to respond? Who has the budget? What are the
    commercial consequences of turning off an ML system? These basic questions and
    many more are why AI incident response requires advanced planning. The idea of
    being prepared for problems is not new for computer systems, or even for predictive
    modeling. Respected institutions such as [SANS](https://oreil.ly/fzJos) and [NIST](https://oreil.ly/2Y1bp)
    already publish computer security incident response plans. Model governance practices
    typically include inventories of ML systems with detailed documentation designed,
    among other goals, to help respond to ML system failures. While conventional incident
    response plans and model governance are great places to start mitigating AI incident
    risks, neither are a perfect fit for AI incident response. Many conventional incident
    response plans do not yet address specialized ML attacks, and model governance
    often does not explicitly address incident response or ML security. To see a sample
    AI incident response plan that builds off both traditional incident response and
    model governance, and that incorporates the necessary specifics for ML, check
    out the free and open [*Sample AI Incident Checklist*](https://oreil.ly/YLe0g).
    And don’t wait until it’s too late to make your own AI incident response plan!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在活跃的AI事件中，压力和混乱会使事件响应变得困难。谁有权作出响应？谁有预算？关闭ML系统的商业后果是什么？这些基本问题及更多问题是为什么AI事件响应需要提前规划的原因。为计算机系统或甚至预测建模做好准备的想法并不新鲜。像[SANS](https://oreil.ly/fzJos)和[NIST](https://oreil.ly/2Y1bp)这样的尊重机构已经发布了计算机安全事件响应计划。模型治理实践通常包括ML系统清单及详细文档，旨在帮助应对ML系统故障等目标之一。虽然传统的事件响应计划和模型治理是减少AI事件风险的好方法，但两者都不完全适用于AI事件响应。许多传统的事件响应计划尚未涵盖专门的ML攻击，而模型治理通常并未明确涉及事件响应或ML安全性。要查看一个以传统事件响应和模型治理为基础，并结合ML所需特定细节的样本AI事件响应计划，请参阅免费开放的[*样本AI事件清单*](https://oreil.ly/YLe0g)。不要等到为时已晚才制定自己的AI事件响应计划！
- en: Organizational Machine Learning Principles
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机构化机器学习原则
- en: If your organization is working with ML, organizational ML principles are critical
    for many reasons. One primary reason is that ML systems present opportunities
    to scale, automate, and defer liability for harm. When you blend these capabilities
    with entrenched corporate or government power structures, history tells us bad
    things can happen. Actionable, public ML principles provide at least one mechanism
    to hold accountable those seeking to combine organizational and ML power for nefarious
    purposes. There’s also the issue of ML openly designed to cause harm. Organizational
    ML principles can serve as a guiding compass for difficult decisions related to
    this topic. Imagine if your company’s ML systems were performing so well that
    they attracted attention from military customers. Are you willing to sell into
    a situation where ML could be used to kill people at scale? Having some idea of
    how you will handle these situations before they arise can help your organization
    make the best decision. And whatever that decision is, it’s probably better that
    it is based on previously agreed upon, responsible, human-centered ML tenets than
    on heat-of-the-moment inclinations. Another advantage of organizational ML principles
    is that they can educate nontechnical employees on ML risks. What if a group in
    your organization that was new to ML deployed a black-box image classifier without
    discrimination testing? If the organization had an ML principle that stated, “All
    ML products that could be used on humans will be tested for discrimination,” perhaps
    someone somewhere in your organization would be more likely to catch the oversight
    before release.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的组织正在使用机器学习，组织机器学习原则出于许多原因至关重要。其中一个主要原因是，机器学习系统提供了扩展、自动化和推迟责任的机会。当您将这些能力与根深蒂固的企业或政府权力结构相结合时，历史告诉我们会发生不好的事情。具体可行的公开机器学习原则至少提供了一种机制，以便追究那些试图将组织和机器学习力量结合用于不良目的的人的责任。还有一个问题是，公开设计用于造成伤害的机器学习。组织机器学习原则可以作为困难决策的指南。想象一下，如果您公司的机器学习系统运行得非常出色，以至于吸引了军方客户的关注。您是否愿意在可能使用机器学习大规模杀人的情况下出售？在此类情况发生之前对如何处理这些情况有一些想法可能有助于您的组织做出最佳决策。无论决策是什么，基于事先达成的负责任、以人为中心的机器学习原则可能比基于当时的冲动倾向更好。组织机器学习原则的另一个好处是，它们可以教育非技术员工了解机器学习的风险。想象一下，如果您组织中一个对机器学习不熟悉的团队部署了一个没有进行歧视测试的黑箱图像分类器会发生什么？如果组织中有一项机器学习原则规定：“所有可能用于人类的机器学习产品都将进行歧视测试”，那么也许组织中的某个地方会更有可能在发布之前发现这个疏忽。
- en: Need some example principles to get started? AlgorithmWatch keeps a [catalog](https://oreil.ly/XdwyI)
    of many corporate and government ML and AI principles. Nearly all these principles
    appear well meaning and serviceable. However, what is often missing is the nuts-and-bolts
    roadmap of how to implement them. So, if you do create ML principles, keep in
    mind that a common pratfall to avoid is ineffectiveness. It’s easy to make organizational
    ML principles so high-level or abstract that they cannot be implemented or enforced.
    AlgorithmWatch recently published a [report](https://oreil.ly/soU7W) stating that
    only 10 of 160 reviewed sets of principles were enforceable. So, getting technologists,
    along with ethics, legal, oversight, and leadership perspectives into organizational
    ML principles is probably a best practice. We’ll reference a set of example ML
    principles, published as part of a publicly traded bank annual report, in the
    next section on the broader topic of CSR.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一些示例原则来开始吗？AlgorithmWatch 维护了一个[目录](https://oreil.ly/XdwyI)，其中包含许多企业和政府的机器学习和人工智能原则。几乎所有这些原则看起来都是出于善意且可操作的。然而，通常缺少的是如何实施这些原则的具体路线图。因此，如果你制定了机器学习原则，请记住要避免的一个常见陷阱是无效性。很容易制定出过于高层或抽象的组织机器学习原则，以至于无法实施或强制执行。AlgorithmWatch
    最近发表的[报告](https://oreil.ly/soU7W)指出，在审查的160套原则中，只有10套是可执行的。因此，将技术人员与伦理、法律、监督和领导层的视角融入组织机器学习原则可能是最佳实践。在下一节中，我们将引用作为公开交易银行年度报告的一部分发布的一组示例机器学习原则，以探讨更广泛的企业社会责任话题。
- en: Corporate Social Responsibility and External Risks
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 企业社会责任和外部风险
- en: 'It is no secret that companies, both private and public, have experienced pressure
    to prioritize ethical and sustainable business practices in recent years. When
    thinking about the implications of this as it pertains to ML, we must note that
    ML sits at the intersection of multiple critical areas of concern for workers,
    consumers, and the public: automation, perpetuating discrimination and inequality,
    privacy harms, lack of accountability, and more. For this very reason, a focus
    on responsible deployment of ML and rigorous internal accountability should be
    a pillar of success for organizations adopting ML. This section will briefly touch
    on the importance of CSR for ML. It will also quickly discuss the mitigation of
    broad and general risks to organizations, consumers, and the public, i.e., external
    risks, that organizations can create through the irresponsible use of ML.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，无论是私营还是公共公司，近年来都面临着优先考虑道德和可持续商业实践的压力。在思考这对ML的影响时，我们必须注意，ML处于工人、消费者和公众关注的多个关键领域的交汇点：自动化、歧视和不平等的持续、隐私伤害、缺乏问责制等等。正因为如此，对ML的负责任部署和严格的内部问责制应该是采用ML的组织成功的支柱。本节将简要介绍ML的企业社会责任的重要性。它还将快速讨论减轻组织、消费者和公众的广泛和一般风险，即通过不负责任使用ML而造成的外部风险。
- en: Corporate Social Responsibility
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 企业社会责任
- en: 'The news and technology media are replete with examples of why organizations
    should consider social responsibility when adopting ML. Let’s consider the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻和技术媒体充满了为何组织在采用机器学习时应考虑社会责任的例子。让我们考虑以下内容：
- en: According to [Forbes](https://oreil.ly/OA3Tv), “81 percent of Millennials expect
    companies to make a public commitment to good corporate citizenship.”
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据[Forbes](https://oreil.ly/OA3Tv)，“81%的千禧一代期望公司公开承诺良好的企业公民责任。”
- en: According to [Cone Communications](https://oreil.ly/Y1aMz), “75 percent of millennials
    would take a pay cut to work for a socially responsible company.”
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据[Cone Communications](https://oreil.ly/Y1aMz)，“75%的千禧一代愿意为了在社会上负责任的公司工作而减薪。”
- en: According to [Crunchbase News](https://oreil.ly/QJv3l), “More investors recognize
    that making money and making a positive impact on the world doesn’t have to be
    mutually exclusive.”
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据[Crunchbase News](https://oreil.ly/QJv3l)，“越来越多的投资者认识到，赚钱和对世界产生积极影响并不是互斥的。”
- en: According to [Capgemini](https://oreil.ly/hfZZX), “62% of consumers will place
    higher trust in the company if they perceive AI-enabled interactions as ethical.”
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据[Capgemini](https://oreil.ly/hfZZX)，“62%的消费者将更信任公司，如果他们认为启用AI的互动是道德的。”
- en: As indicated in previous sections, responsible ML can minimize the long-term
    fines associated with regulatory noncompliance or costs stemming from large legal
    settlements. But, as the quotes here hint at, responsible ML and CSR processes
    do not act solely to protect brand reputation or as a shield from regulatory retribution.
    They can also impact an organization’s ability to access customers, capital, funding,
    and talent, and impact employee satisfaction and retention. For a prime example
    of a company combining responsible ML with CSR reasonably early on in their AI
    transformation journey, see the Regions Bank 2019 [*Annual Review and Environmental,
    Social and Governance Report*](https://oreil.ly/d-82z).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所述，负责任的ML可以最小化因法规不合规或大额法律和解导致的长期罚款。但正如这里的引用所暗示的那样，负责任的ML和CSR流程不仅仅是保护品牌声誉或作为免受法规惩罚的屏障。它们还可以影响组织获取客户、资本、资金和人才的能力，并影响员工的满意度和留任率。一个典型的例子是，一家公司在其AI转型早期合理结合了负责任的ML与企业社会责任，参见Regions
    Bank 2019年[年度审查和环境、社会与治理报告](https://oreil.ly/d-82z)。
- en: Mitigating External Risks
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减轻外部风险
- en: For many companies, answering the question, “Which of my customers get hurt
    if a model breaks?” is not straightforward. Suppose a company is systemically
    important, poses significant health risks to surrounding areas, or sells a product
    or service with network effects. In that case, they must be aware that their IT
    systems can cause problems beyond their own revenue and customers. One of the
    most prominent examples of this kind of serious hazard is the 2007–2008 Global
    Financial Crisis, where major financial institutions recklessly created synthetic
    financial instruments that mathematically disguised the risks entailed in these
    products. Ultimately when the dominos fell, the final result was a massive global
    recession that wreaked havoc in financial and real estate markets and created
    severe and lasting damage across much of the world economy.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多公司来说，回答“如果模型出现问题，我的哪些客户会受到伤害？”并不简单。假设一家公司具有系统重要性，对周围地区造成重大健康风险，或者销售具有网络效应的产品或服务，那么他们必须意识到他们的IT系统可能会引发超出其自身收入和客户范围的问题。这类严重危害的最突出例子之一是2007-2008年全球金融危机，当时主要金融机构鲁莽地创建了数学上掩盖了这些产品风险的合成金融工具。最终，当多米诺骨牌倒下时，结果是造成全球范围内的大规模经济衰退，严重影响金融和房地产市场，并对世界经济的大部分造成了严重和持久的损害。
- en: In ML, we’ve just started to see these kinds of broad external risks emerge.
    For instance, in the US, health insurance companies sometimes use algorithms to
    allocate resources for entire insured populations—hundreds of millions of people.
    Unfortunately, at least one such algorithm was found to [harm the healthcare](https://oreil.ly/KWWmR)
    of large numbers of Black people. Another example of external algorithmic risk
    is the 2016 British Pound flash crash, in which algorithmic trading is suspected
    of crashing currency markets. Given even the possibility of such large-scale incidents,
    it seems evident that ML and algorithmic decision making must be treated more
    like commercial air travel or nuclear energy. In these verticals, operators are
    not only accountable for their successes and failures within the company, but
    they can also be held accountable for the impacts their decisions have on customers
    and tangential third parties. Perhaps one thing to keep in mind is that, in the
    already regulated verticals of ML, organizations and accountable executives can
    be penalized for their failures. Regardless of the presence of serious regulatory
    oversight in your industry, if you’re doing big things with ML, it is probably
    a good time to start thinking about ML’s external risks. To help your organization
    start tracking these kinds of ML risks, check out the high-level document created
    by the Future of Privacy Forum and bnh.ai, entitled [Ten Questions on AI Risk](https://oreil.ly/qjOgW).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）领域，我们刚开始看到这些广泛的外部风险出现。例如，在美国，健康保险公司有时使用算法来分配整个被保人群体的资源——数亿人口。不幸的是，至少有一个这样的算法被发现对大量黑人群体的[医疗保健造成伤害](https://oreil.ly/KWWmR)。另一个外部算法风险的例子是2016年英镑闪崩事件，据称算法交易导致货币市场崩溃。考虑到这类大规模事件的可能性，显然，机器学习和算法决策必须被视为商业航空或核能一样重要。在这些行业中，操作者不仅对公司内部的成功和失败负责，还必须对其决策对客户和相关第三方造成的影响负责。或许有一点需要记住的是，尽管ML领域已经有严格的监管垂直领域，但组织和负责的高管们可能会因其失败而受到惩罚。无论你的行业是否有严格的监管监督，如果你正在进行大规模的ML项目，现在可能是开始考虑ML外部风险的好时机。要帮助你的组织开始追踪这类ML风险，可以查阅由未来隐私论坛和bnh.ai制作的高级文档，名为[Ten
    Questions on AI Risk](https://oreil.ly/qjOgW)。
- en: '^([1](ch03.xhtml#idm46137004264152-marker)) See: [*https://oreil.ly/K4HMW*](https://oreil.ly/K4HMW);
    for a more broadly accepted academic treatment of the subject see: [*https://fairmlbook.org*](https://fairmlbook.org).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.xhtml#idm46137004264152-marker)) 参见：[*https://oreil.ly/K4HMW*](https://oreil.ly/K4HMW)；有关该主题更广泛接受的学术处理，请参见：[*https://fairmlbook.org*](https://fairmlbook.org)。
