- en: Chapter 8\. Improving Custom Model Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 改进自定义模型性能
- en: In Chapters [6](ch06.html#using_bigquery_ml_to_train_a_linear_reg) and [7](ch07.html#training_custom_ml_models_in_python),
    you learned how to prepare data and build custom models using SQL, BigQuery ML,
    and Python using scikit-learn and Keras. You will revisit those tools in this
    chapter with an eye toward additional feature engineering and hyperparameter tuning.
    In contrast to previous chapters, you will start with prepared data and an already
    trained model and work to improve from there. If you are confused when exploring
    the code for the previously built models or the user interface for BigQuery, please
    revisit the discussions in Chapters [6](ch06.html#using_bigquery_ml_to_train_a_linear_reg)
    and [7](ch07.html#training_custom_ml_models_in_python).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[6](ch06.html#using_bigquery_ml_to_train_a_linear_reg)章和第[7](ch07.html#training_custom_ml_models_in_python)章中，您学习了如何准备数据，并使用SQL、BigQuery
    ML和Python构建自定义模型。在本章中，您将重温这些工具，关注额外的特征工程和超参数调整。与之前的章节不同，您将从准备好的数据和已训练的模型开始，并努力进一步改进。如果您在探索先前构建的模型的代码或BigQuery的用户界面时感到困惑，请重新查看第[6](ch06.html#using_bigquery_ml_to_train_a_linear_reg)章和第[7](ch07.html#training_custom_ml_models_in_python)章中的讨论内容。
- en: 'The Business Use Case: Used Car Auction Prices'
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务使用案例：二手车拍卖价格
- en: Your goal in this project will be to improve the performance of an ML model
    trained to predict the auction price of used cars. The initial model is a linear
    regression model built in scikit-learn and does not quite meet your business goals.
    You will ultimately explore using tools in scikit-learn, Keras, and BigQuery ML
    to improve your model performance via feature engineering and hyperparameter tuning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的目标是改进一个用于预测二手车拍卖价格的机器学习模型的性能。初始模型是一个在scikit-learn中构建的线性回归模型，但并没有完全达到您的业务目标。最终，您将探索使用scikit-learn、Keras和BigQuery
    ML中的工具，通过特征工程和超参数调整来提高模型的性能。
- en: The dataset used for training the linear regression model has been supplied
    to you as CSV files. These datasets have been cleaned (missing and incorrect values
    have been remedied appropriately), and the code that was used to build the scikit-learn
    linear regression model has also been provided. Your teammate who trained the
    linear regression model has shared some notes with you on model performance and
    their initial explorations into using Keras for training the ML model. Your colleague
    has also shared the data split that they used to train and evaluate their model.
    They created a separate test dataset that has not been used yet that you will
    be able to use to validate your final model performance. Your task will be to
    explore the use of feature engineering to improve the feature set for the model
    and to leverage hyperparameter tuning to ensure that the best model architecture
    is being used. You will see how to perform these tasks in scikit-learn, Keras,
    and BigQuery ML.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练线性回归模型的数据集已经作为CSV文件提供给您。这些数据集已经进行了清理（已适当修复了缺失和不正确的值），并且提供了用于构建scikit-learn线性回归模型的代码。您的队友训练了线性回归模型，并与您分享了一些关于模型性能以及他们初步尝试使用Keras训练ML模型的笔记。您的同事还分享了用于训练和评估模型的数据分割。他们创建了一个独立的测试数据集，尚未使用，您将可以用它来验证最终模型的性能。您的任务将是探索使用特征工程来改进模型的特征集，并利用超参数调整来确保使用最佳模型架构。您将学习如何在scikit-learn、Keras和BigQuery
    ML中执行这些任务。
- en: In the wholesale car sales industry, one of the leading indicators of wholesale
    prices is the [Manheim Market Report (MMR)](https://www.manheim.com). MMR pricing
    calculations are based on over 10 million sales transactions for the previous
    13 months. In your dataset, you have access to pricing calculations for the car
    sales that have been shared when the data was initially pulled. However, you are
    not certain if you will have access to this information in the future. For that
    reason, you have been asked to avoid using this feature in your explorations.
    The business goal that has been shared with you is to have an RMSE in the sales
    price of $2,000 or less without using the MMR feature. You will start by using
    the notebook provided by your colleague to load the data and replicate the model
    training they performed.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在批发汽车销售行业中，批发价格的一个主要指标是[Manheim市场报告（MMR）](https://www.manheim.com)。 MMR定价计算基于过去13个月的1000万多次销售交易。
    在您的数据集中，您可以访问数据最初提取时共享的汽车销售定价计算。 但是，您不确定将来是否还能访问此信息。 因此，您被要求在探索中避免使用此功能。 与您共享的业务目标是在不使用MMR功能的情况下，使销售价格的RMSE达到2000美元或更低。
    您将首先使用同事提供的笔记本加载数据并复制他们执行的模型训练。
- en: There are 13 columns in the dataset. [Table 8-1](#schema_and_field_value_informatio)
    gives the column names, data types, and some information about the possible values
    for these columns.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有13列。 [表 8-1](#schema_and_field_value_informatio) 给出了列名、数据类型以及这些列可能值的一些信息。
- en: Table 8-1\. Schema and field value information for the car sales dataset
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-1\. 汽车销售数据集的模式和字段值信息
- en: '| Column name | Column type | Notes about field values |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 列类型 | 关于字段值的注释 |'
- en: '| --- | --- | --- |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `year` | Integer | Year vehicle was manufactured |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| `year` | Integer | 车辆制造年份 |'
- en: '| `make` | String | Brand of the vehicle |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| `make` | String | 车辆品牌 |'
- en: '| `model` | String | Specific version or variation of a vehicle make |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| `model` | String | 车辆品牌的具体版本或变种 |'
- en: '| `trim` | String | Specific version or variation of a vehicle model |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| `trim` | String | 车辆型号的具体版本或变种 |'
- en: '| `body` | String | Body style of vehicle (e.g., sedan) |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| `body` | String | 车辆的车身风格（例如轿车） |'
- en: '| `transmission` | String | Automatic or manual transmission |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| `transmission` | String | 自动或手动变速器 |'
- en: '| `state` | String | State in which the car will be sold |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| `state` | String | 车辆将要出售的州 |'
- en: '| `condition` | Float | Condition of the car as rated from 0 to 5 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| `condition` | Float | 车辆评级的条件，从0到5 |'
- en: '| `odometer` | Integer | Odometer reading at time of sale |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| `odometer` | Integer | 销售时的里程表读数 |'
- en: '| `color` | String | Vehicle color |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `color` | String | 车辆颜色 |'
- en: '| `interior` | String | Interior color |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `interior` | String | 内饰颜色 |'
- en: '| `mmr` | Float | Manheim Market Report pricing |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `mmr` | Float | Manheim市场报告的定价 |'
- en: '| `sellingprice` | Float | Actual selling price for vehicle (label) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `sellingprice` | Float | 车辆的实际销售价格（标签） |'
- en: Model Improvement in Scikit-Learn
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中改进模型
- en: In this section you will work to improve the linear regression model in scikit-learn
    that was shared with you by your teammate. You will first quickly explore the
    data, the preprocessing pipeline, and the model itself in scikit-learn. Then you
    will explore the features more carefully and see how you can improve the model
    performance using both new and familiar feature engineering techniques. Finally,
    you will leverage hyperparameter tuning to ensure that you are optimally creating
    the new features for your specific problem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将努力改进您的同事与您分享的scikit-learn中的线性回归模型。 您将首先快速探索数据、预处理管道和scikit-learn中的模型本身。
    然后，您将仔细探索特征，看看如何使用新的和熟悉的特征工程技术来提高模型性能。 最后，您将利用超参数调整来确保您在为您的特定问题优化地创建新特征。
- en: Loading the Notebook with the Preexisting Model
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载带有现有模型的笔记本
- en: First, go to [*https://colab.research.google.com*](https://colab.research.google.com).
    You will load a notebook from the [low-code-ai repository](https://oreil.ly/supp-lcai)
    directly rather than create a new notebook. Click the GitHub button and type in
    the URL for the low-code-ai GitHub repo, [*https://github.com/maabel0712/low-code-ai*](https://github.com/maabel0712/low-code-ai),
    under the prompt “Enter a GitHub URL or search by organization or user.” [Figure 8-1](#connecting_to_github_in_google_colab_to)
    shows what it should look like.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，前往[*https://colab.research.google.com*](https://colab.research.google.com)。您将直接从[low-code-ai
    repository](https://oreil.ly/supp-lcai)加载笔记本，而不是创建一个新的笔记本。点击GitHub按钮，并在提示“输入GitHub
    URL或按组织或用户搜索”下输入low-code-ai GitHub repo的URL，[*https://github.com/maabel0712/low-code-ai*](https://github.com/maabel0712/low-code-ai)，如[图8-1](#connecting_to_github_in_google_colab_to)所示。
- en: '![Connecting to GitHub in Google Colab to open a notebook directly](assets/lcai_0801.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![在Google Colab中连接到GitHub直接打开笔记本](assets/lcai_0801.png)'
- en: Figure 8-1\. Connecting to GitHub in Google Colab to open a notebook directly.
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. 在Google Colab中连接到GitHub直接打开笔记本。
- en: Hit Enter (or click the magnifying glass) to search the repo for notebooks.
    Scroll down until you see `chapter_8/sklearn_model.ipynb` and click the “Open
    notebook in new tab” button on the far right. This will open up the `sklearn_model.ipynb`
    notebook in a new browser tab.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 按Enter（或点击放大镜图标）在repo中搜索笔记本。向下滚动直到看到`chapter_8/sklearn_model.ipynb`，然后点击最右侧的“在新标签页中打开笔记本”按钮。这将在新的浏览器标签页中打开`sklearn_model.ipynb`笔记本。
- en: The code for loading the vehicle auction sales data, preparing the data for
    training, training the ML model, and evaluating the ML model is already included
    in the notebook. In this chapter, you will not go through this code in the level
    of detail that you did in [Chapter 7](ch07.html#training_custom_ml_models_in_python),
    but you will spend some time reviewing the code before beginning the process of
    model improvement.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 加载车辆拍卖销售数据的代码，准备训练数据，训练ML模型以及评估ML模型的步骤已经包含在笔记本中。在本章中，您不会像在[第7章](ch07.html#training_custom_ml_models_in_python)中那样详细地讨论这些代码，但在开始模型改进过程之前，您将花一些时间审查这些代码。
- en: Loading the Datasets and the Training-Validation-Test Data Split
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据集和训练-验证-测试数据拆分
- en: 'First, execute the cell to load the training, validation, and test datasets
    into corresponding DataFrames:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先执行单元格以加载训练、验证和测试数据集到相应的DataFrames中：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code should be mostly familiar from previous chapters, but the `wget` bash
    command may be new to you. `wget` simply downloads the file at the given URL into
    the local file directory. The `-q` flag suppresses the logs from the `wget` command.
    When loading the data into DataFrames, note that the file location starts with
    `./`. The `.` is a shorthand for “the current working directory,” which is where
    the `wget` command will download the three files you specified.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码在之前的章节中应该大部分都很熟悉，但`wget` bash命令可能对您来说是新的。`wget`简单地将给定URL处的文件下载到本地文件目录中。`-q`标志抑制了`wget`命令的日志输出。在将数据加载到DataFrames时，请注意文件位置以`./`开头。`.`是“当前工作目录”的简写，这也是`wget`命令将下载您指定的三个文件的位置。
- en: 'After loading the datasets into their respective DataFrames, quickly ensure
    that the data is what you expect by using the `head()` method on each DataFrame.
    The first few columns of output from the `train_df.head()` method are shown below:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据集加载到各自的DataFrames中之后，通过对每个DataFrame使用`head()`方法快速确认数据是否符合预期。`train_df.head()`方法输出的前几列如下所示：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The very first column is the index in your DataFrame `train_df`, but where is
    the second column, `Unnamed:0`, coming from? This column is an unnamed column
    in the CSV files that you are loading to create your DataFrames, and it was not
    mentioned in [Table 8-1](#schema_and_field_value_informatio). Most likely this
    was the index from a previous DataFrame that got carried over by mistake and not
    an important feature. You will see in the next section that your colleague dropped
    this column as part of preprocessing the data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的DataFrame `train_df` 中，第一列是索引，但第二列`Unnamed:0`是从哪里来的？这列是CSV文件中未命名的列，您加载以创建DataFrames时没有提到它，最有可能是前一个DataFrame中因误操作而保留的索引，并不是重要特征。在下一节中，您将看到您的同事在数据预处理中删除了这一列。
- en: Note
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Note that you could also use Pandas to read directly from the file location
    in Google Cloud Storage as you did in [Chapter 7](ch07.html#training_custom_ml_models_in_python).
    The advantage of using the `wget` command is that you will now have your own local
    copy of the data. Which approach is the most advantageous depends on your workflow
    and how you are manipulating the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您也可以像在[第七章](ch07.html#training_custom_ml_models_in_python)中所做的那样，使用Pandas直接从Google
    Cloud Storage中的文件位置读取。使用`wget`命令的优点是您现在拥有数据的本地副本。哪种方法更有利取决于您的工作流程以及如何操作数据。
- en: 'Before moving forward, recall that in the previous chapters you used two datasets,
    a training dataset and a test dataset, for evaluating the model after training.
    Now there are three datasets: a training dataset, a validation (or evaluation)
    dataset, and a test dataset. Why are there three datasets? The training dataset
    is of course used for training the model, and the validation dataset is used for
    evaluating the model. In this project, you are going to be comparing many different
    models to each other. You will use the training dataset to train each model and
    then evaluate the models using the validation dataset. The “final model” will
    be the model whose performance was the best on the validation dataset. However,
    the final model you choose may be biased toward the validation dataset since you
    specifically choose the model that performed the best on the validation dataset.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请回忆一下在前几章中，您使用了两个数据集，一个训练数据集和一个测试数据集，用于训练后评估模型。现在有三个数据集：一个训练数据集，一个验证（或评估）数据集和一个测试数据集。为什么会有三个数据集呢？训练数据集当然用于训练模型，验证数据集用于评估模型。在这个项目中，您将比较许多不同的模型。您将使用训练数据集来训练每个模型，然后使用验证数据集来评估模型。"最终模型"将是在验证数据集上表现最佳的模型。然而，您选择的最终模型可能会偏向于验证数据集，因为您明确选择了在验证数据集上表现最佳的模型。
- en: The role of the test dataset is to have a final independent dataset to verify
    the final model’s performance. If the final model has similar performance on the
    test dataset as it does to the validation data, then that model is ready to use.
    In the case that the model has significantly worse performance on the test dataset,
    then you know you have a problem before you take the model and use it in your
    workloads.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集的作用是有一个最终独立的数据集来验证最终模型的性能。如果最终模型在测试数据集上的表现与验证数据相似，那么该模型就可以使用。如果模型在测试数据集上的性能显著较差，那么在将模型用于工作负载之前，您就知道存在问题。
- en: One way to help avoid this scenario is to ensure that your training, validation,
    and test datasets have similar data distributions. As an exercise, explore the
    datasets using the `describe(include='all')` method and see that the three datasets
    have similar distributions up to a few outliers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助避免这种情况的一种方法是确保您的训练、验证和测试数据集具有类似的数据分布。作为一个练习，使用`describe(include='all')`方法来探索数据集，看看这三个数据集是否具有相似的分布，直到一些离群值为止。
- en: Exploring the Scikit-Learn Linear Regression Model
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索Scikit-Learn线性回归模型
- en: 'Now go to the next cell in the notebook. This cell contains the code to prepare
    the data, train the ML model, and evaluate the model all using scikit-learn. There
    will not be a careful walkthrough of all the code in this section, as the concept
    of a scikit-learn pipeline was covered in [Chapter 7](ch07.html#training_custom_ml_models_in_python).
    However, a quick overview with some additional notes will be discussed along the
    way. First consider the data processing portion of code after the import statements:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转到笔记本中的下一个单元格。此单元格包含准备数据、训练ML模型和评估模型的代码，所有这些都使用scikit-learn完成。在本节中不会仔细讲解所有代码，因为在[第七章](ch07.html#training_custom_ml_models_in_python)中已经介绍了scikit-learn流水线的概念。然而，在路上会讨论一个快速概述和一些额外的注释。首先考虑导入语句之后的数据处理部分代码：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'First, you are splitting the DataFrame into a separate column of labels (`sellingprice`)
    and the rest of the feature columns as a single DataFrame. Then, you are dropping
    the `Unnamed: 0` and `mmr` columns of your training, validation, and test datasets.
    This is being done by defining a function `drop_columns` and applying that function
    using a `FunctionTransformer`. Note that there’s a new parameter in defining the
    `FunctionTransformer`. The `kw_args` parameter takes in values for arguments in
    the chosen function beyond the first. For `preproc_cols`, the first argument is
    the DataFrame we wish to drop columns from, and that will be provided in the pipeline.
    The second parameter is the list of columns we wish to drop, and that will be
    passed in as the corresponding value of a dictionary for the key `columns`.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，您将DataFrame分割为标签列（`sellingprice`）和其余特征列作为单独的DataFrame。然后，您在训练、验证和测试数据集中删除`Unnamed:
    0`和`mmr`列。这是通过定义一个`drop_columns`函数并使用`FunctionTransformer`应用该函数来完成的。请注意，在定义`FunctionTransformer`时有一个新的参数。`kw_args`参数接受超出第一个参数的选择函数参数的值。对于`preproc_cols`，第一个参数是我们希望从中删除列的DataFrame，这将在管道中提供。第二个参数是我们希望删除的列的列表，这将作为字典键`columns`的相应值传递。'
- en: 'The `Unnamed: 0` column may seem odd, but as discussed before this is likely
    an artifact of how the data was shuffled using the Pandas DataFrame method `sample()`,
    which kept the original index as a new column. Within the context of your problem,
    this column has no relation to the target and has been discarded. The `mmr` column
    is unsurprisingly highly correlated with the target `sellingprice`, but given
    that you have been instructed to avoid using that feature, it is being dropped
    as well.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`Unnamed: 0`列可能看起来有些奇怪，但正如之前讨论的那样，这很可能是使用Pandas DataFrame方法`sample()`对数据进行洗牌时保留原始索引作为新列的结果。在您问题的背景下，这列与目标没有关系，因此被丢弃。`mmr`列与目标`sellingprice`高度相关，但由于您已被指示避免使用该特征，因此也被丢弃。'
- en: 'Otherwise, the rest of the preceding code will seem familiar [“Pipelines in
    Scikit-Learn: An Introduction”](ch07.html#pipelines_in_scikit_learn_an_introducti)
    in [Chapter 7](ch07.html#training_custom_ml_models_in_python). You are splitting
    the columns into numeric columns and categorical columns (`numeric_columns` and
    `categorical_​col⁠umns`, respectively) and then using a `ColumnTransformer()`
    to apply different transformations to the different sets of columns. The `OneHotEncoder()`
    will be used for the categorical columns, and a `MinMaxScaler()` will be used
    for the numeric columns.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，前述代码的其余部分将会很熟悉[“Scikit-Learn中的管道：介绍”](ch07.html#pipelines_in_scikit_learn_an_introducti)在[第7章](ch07.html#training_custom_ml_models_in_python)。您将列分为数值列和分类列（分别为`numeric_columns`和`categorical_columns`），然后使用`ColumnTransformer()`对不同的列集应用不同的转换。对于分类列，将使用`OneHotEncoder()`，对于数值列将使用`MinMaxScaler()`。
- en: 'Now consider the rest of the code in the cell where you define the model and
    the pipeline:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑在定义模型和管道的单元格中的其余代码：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The model that is being used here is a linear regression model. You are also
    using a `Pipeline` object to define the preprocessing and the model together as
    a sequence of steps. First the `preproc_cols FunctionTransformer()` will be applied
    to the DataFrame, then `col_transformer ColumnTransformer()` will be used to apply
    the appropriate transformation to columns depending on their type. Finally, the
    linear regression model will be trained as the last part of the pipeline when
    `pipeline.fit` is called. The last line fits the transformers and trains the model
    together. The fit transformers are stored as part of the pipeline at prediction
    time. After training the model, you will see a graphical representation of the
    pipeline, as shown in [Figure 8-2](#graphical_representation_of_the_scikit). If
    you wish, you can expand this to see more details and confirm that they align
    with your expectations from the code.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的模型是线性回归模型。同时，您还使用了一个`Pipeline`对象来定义预处理和模型作为一系列步骤。首先，将会对DataFrame应用`preproc_cols
    FunctionTransformer()`，然后使用`col_transformer ColumnTransformer()`根据列的类型应用适当的转换。最后，在调用`pipeline.fit`时，线性回归模型将作为管道的最后一部分进行训练。最后一行同时拟合转换器并训练模型。在预测时，拟合的转换器将作为管道的一部分存储。训练模型后，您将看到管道的图形表示，如[图 8-2](#graphical_representation_of_the_scikit)所示。如果希望，您可以展开查看更多细节，并确认这些细节与代码的预期一致。
- en: '![Graphical representation of the scikit-learn Pipeline used to train your
    model](assets/lcai_0802.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![用于训练模型的scikit-learn Pipeline的图形表示](assets/lcai_0802.png)'
- en: Figure 8-2\. Graphical representation of the scikit-learn `Pipeline` used to
    train your model.
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. scikit-learn `Pipeline`用于训练模型的图形表示。
- en: 'Now that the model has been trained in your notebook environment, you can evaluate
    the model. Recall that when using scikit-learn pipelines, you can use the `score()`
    method in the same way you would for any model object. You can also import other
    metrics such as RMSE and MAE. Run the following code in the following cell to
    look at evaluation metrics for your model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经在您的笔记本环境中训练好了，您可以评估模型。回想一下，当使用scikit-learn管道时，您可以像对待任何模型对象一样使用`score()`方法。您还可以导入其他指标，如RMSE和MAE。在下一个单元格中运行以下代码查看模型的评估指标：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You will see that the R² score is about 0.876\. This means that, roughly speaking,
    your features describe 87.6% of the variability in the label. You will also see
    that the RMSE is about 3,384.60 and the MAE is about 2,044.26\. Recall that your
    business goal was to have an RMSE of less than $2,000\. As expected, based on
    the communication from your colleague, the model does not meet those needs, but
    now you have everything ready to work on improving the model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到R²分数约为0.876。这意味着您的特征大致描述了标签变量的87.6%的变异性。您还将看到RMSE约为3,384.60，MAE约为2,044.26。回想一下，您的业务目标是使RMSE低于$2,000。根据同事的传达，模型未能达到这些需求，但现在您已经准备好改进模型了。
- en: Note
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In general, when collaborating across a team, you want to avoid any randomness
    from splitting and the training process when comparing results. Otherwise, you
    may be misled by comparing different model results being trained in different
    environments. In this case, the actual data split was shared with you rather than
    the code. This is often preferable since a random shuffle and split will often
    be used. You can set a random seed to make that split deterministic or save the
    corresponding training, validation, and test datasets, as was done here. Additionally,
    this is something to consider when initializing and training models as well.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在团队合作时，您希望在比较结果时避免任何从分割和训练过程中产生的随机性。否则，您可能会被在不同环境中训练的不同模型结果误导。在这种情况下，实际的数据拆分已与您共享，而不是代码。这通常是可取的，因为通常会使用随机洗牌和拆分。您可以设置随机种子以使拆分确定性，或保存相应的训练、验证和测试数据集，就像这里做的那样。此外，在初始化和训练模型时也要考虑这一点。
- en: Feature Engineering and Improving the Preprocessing Pipeline
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程和改进预处理流水线
- en: Often, well-chosen and carefully created features, even with simple model architectures,
    can lead to very strong results. Just simply making your model more complicated
    is not always the right approach. More complex models will need more data to successfully
    train the model and take more compute power to train and ultimately tune the hyperparameters.
    Even looking for easy fixes, such as looking out for strange values and removing
    unrelated features, can lead to significant model improvement.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，选择得当并精心创建的特征，即使使用简单的模型架构，也可能导致非常强大的结果。仅仅是让模型变得更复杂并不总是正确的方法。更复杂的模型将需要更多的数据来成功训练模型，并且需要更多的计算资源来训练和最终调整超参数。甚至寻找简单的修复方法，例如查找异常值并删除无关的特征，也可能导致显著的模型改进。
- en: Looking for easy improvements
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找简单的改进
- en: Your colleague did a careful analysis of the original dataset and communicated
    to you that they removed all null values and columns (such as the VIN) that were
    in a one-to-one relationship with the label. Now it is a good idea to explore
    the dataset to see if anything else could be improved. To do this, run the command
    `train_df.describe()` in your notebook environment in a new cell if you have not
    already done so. A sample of the expected output is shown in [Table 8-2](#partial_output_of_train_dfdotdescribele).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您的同事对原始数据集进行了仔细分析，并向您传达了他们已删除所有空值和与标签呈一对一关系的列（例如VIN）。现在是探索数据集以查看是否还有其他可以改进的地方的好时机。如果您尚未这样做，请在笔记本环境中的新单元格中运行命令`train_df.describe()`。预期输出的示例在[表8-2](#partial_output_of_train_dfdotdescribele)中显示。
- en: Table 8-2\. Partial output of `train_df.describe()`
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表8-2\. `train_df.describe()`的部分输出
- en: '|   | `year` | `condition` | `odometer` | `mmr` | `sellingprice` |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|   | `year` | `condition` | `odometer` | `mmr` | `sellingprice` |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| `count` | `385000.000000` | `385000.000000` | `385000.000000` | `385000.000000`
    | `385000.000000` |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `count` | `385000.000000` | `385000.000000` | `385000.000000` | `385000.000000`
    | `385000.000000` |'
- en: '| **`mean`** | `2010.120177` | `3.339402` | `67732.957974` | `13695.356558`
    | `13544.324018` |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **`mean`** | `2010.120177` | `3.339402` | `67732.957974` | `13695.356558`
    | `13544.324018` |'
- en: '| **`std`** | `3.879672` | `1.117698` | `52521.619238` | `9525.243974` | `9599.953488`
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **`std`** | `3.879672` | `1.117698` | `52521.619238` | `9525.243974` | `9599.953488`
    |'
- en: '| **`min`** | `1990.000000` | `-1.000000` | `1.000000` | `25.000000` | `1.000000`
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| **`min`** | `1990.000000` | `-1.000000` | `1.000000` | `25.000000` | `1.000000`
    |'
- en: '| **`25%`** | `2008.000000` | `2.700000` | `28494.000000` | `7200.000000` |
    `7000.000000` |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **`25%`** | `2008.000000` | `2.700000` | `28494.000000` | `7200.000000` |
    `7000.000000` |'
- en: '| **`50%`** | `2012.000000` | `3.600000` | `52122.000000` | `12200.000000`
    | `12100.000000` |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **`50%`** | `2012.000000` | `3.600000` | `52122.000000` | `12200.000000`
    | `12100.000000` |'
- en: '| **`75%`** | `2013.000000` | `4.200000` | `98188.000000` | `18150.000000`
    | `18000.000000` |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| **`75%`** | `2013.000000` | `4.200000` | `98188.000000` | `18150.000000`
    | `18000.000000` |'
- en: '| **`max`** | `2015.000000` | `5.000000` | `999999.000000` | `182000.000000`
    | `183000.000000` |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **`max`** | `2015.000000` | `5.000000` | `999999.000000` | `182000.000000`
    | `183000.000000` |'
- en: 'Recall that you are dropping the `Unnamed: 0` and the `mmr` columns in the
    preprocessing pipeline, so you do not need to worry about those columns in your
    analysis. Nothing seems out of place in the `year` column at a glance; the model
    year of the cars range between 1990 and 2015, with the data distribution skewed
    toward newer cars. You should notice something odd about the `condition` column.
    You have a minimum value of `-1.0` in the `condition` column. This likely means
    that a magic number slipped by your colleague. When analyzing many columns across
    a dataset, it is easy to sometimes miss something simple like this. This is a
    good reason an extra pair of eyes is always valuable.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '请记住，在预处理管道中删除 `Unnamed: 0` 和 `mmr` 列，所以在分析中无需担心这些列。乍一看 `year` 列中似乎没有什么异常；汽车的年份在1990年到2015年之间，数据分布偏向更新的车辆。但是
    `condition` 列似乎有些奇怪。在 `condition` 列中有一个最小值为 `-1.0`。这很可能意味着你的同事在处理时遗漏了一个魔数。在分析数据集中的多列时，有时会错过像这样简单的问题。这正是为什么额外的一双眼睛总是有价值的原因。'
- en: Since `condition` is a floating-point number, we cannot easily just treat `-1.0`
    as a separate value without doing some transformation. You have a few options.
    If you expect that the selling price has a linear relationship with the `condition`
    value, then you could create a new feature `condition_recorded` as a binary `0`
    or `1` value and replace instances of `-1.0` with `0.0` so that those values are
    treated differently than normal `condition` values. However, as you may have experienced
    with other rating systems, often the effect of ratings is not linear. An easy
    way to address this is to bucketize the values and then one-hot encode the corresponding
    bucket membership. This way, the case of no rating will be treated entirely differently
    than cases of other ratings (say, between 2 and 3), and you can tune the number
    of buckets to see which gives your model the best performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `condition` 是一个浮点数，我们不能简单地将 `-1.0` 视为一个单独的值进行处理。你有几个选择。如果你认为销售价格与 `condition`
    值有线性关系，那么你可以创建一个新特征 `condition_recorded`，作为二进制 `0` 或 `1` 值，并将 `-1.0` 的实例替换为 `0.0`，这样这些值就会与普通的
    `condition` 值不同。然而，正如你可能在其他评分系统中经历过的那样，评分的效果通常并非线性的。解决这个问题的简单方法是对值进行分桶，然后对应的桶进行独热编码。这样，没有评分的情况将与其他评分的情况（例如
    2 到 3 之间）完全不同，你可以调整桶的数量以找到最适合你的模型性能的设置。
- en: 'To take the second approach, create a new cell in the notebook and add the
    following code, but do not run the code yet:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要采用第二种方法，在笔记本中创建一个新的单元格，并添加以下代码，但暂时不要运行代码：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This code is mostly the same as what was shared with you before, but take the
    time to spot a few changes. The `KBinsDiscretizer` transformer has been added;
    this transformer is the tool in scikit-learn that can be used to bucketize data.
    Note that the transformers are now defined on separate lines rather than in the
    `ColumnTransformer` as before. This is done for increased readability, but also
    for *modularity*, the ability to split up code more easily as you continue to
    improve your model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与之前分享的大部分相同，但请注意几处更改。`KBinsDiscretizer` 转换器已经添加；这个转换器是 scikit-learn 中用于数据分桶的工具。请注意，现在转换器是在单独的行上定义而不是像以前在
    `ColumnTransformer` 中。这样做增加了可读性，同时也增强了*模块化*，即在继续改进模型时更容易分割代码。
- en: 'In the last of these lines is where the `KBinsDiscretizer` is defined. The
    `n_bins` argument is set to `10` for 10 different buckets, the `encode` argument
    tells the transformer to perform one-hot encoding, and the `strategy`, `uniform`,
    tells the transformer to split the buckets evenly. This way, `-1.0` will be in
    its own bucket separate from the other ranges. Finish defining the pipeline using
    the following code, and run the cell to train your new model:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些行的最后定义了`KBinsDiscretizer`。`n_bins`参数设置为`10`，表示10个不同的桶，`encode`参数告诉转换器执行独热编码，而`strategy`参数`uniform`告诉转换器均匀地分割这些桶。这样，`-1.0`将会独立于其他范围在其自己的桶中。完成使用以下代码定义管道，并运行单元格以训练您的新模型：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can evaluate the new model by executing the code you used before in a new
    cell:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在新的单元格中执行之前使用的代码来评估新模型：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The change did lead to a small increase in the performance of the model. The
    RMSE dropped from about 3,384.60 to 3,313.63\. Though it is a small improvement
    in this case, in many cases catching issues like this can lead to a large improvement
    in the model performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这一变更确实导致了模型性能的轻微提升。RMSE从约3,384.60降至3,313.63。尽管在这种情况下提升不大，但在许多情况下，捕捉到这样的问题可以大幅提升模型性能。
- en: 'As you were evaluating the model, you likely noticed a warning message in the
    results:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型时，您可能注意到了结果中的警告消息：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: What does this warning actually mean? Here columns 2 and 3 correspond to `trim`
    and `body`. This warning means that there are values for these columns in the
    validation dataset that are not in the training dataset. Checking for skew between
    the datasets, such as different values appearing in the training and validation
    datasets, is an important step of understanding your data in preparation for training.
    However, it is very possible that an issue you were not expecting could appear
    while training and evaluating models, so it is useful to know what to look out
    for.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个警告实际上是什么意思？这里的第2列和第3列对应于`trim`和`body`。这个警告意味着在验证数据集中，这些列对应的值在训练数据集中不存在。检查数据集之间的偏差，例如训练和验证数据集中出现不同值的情况，是理解数据准备训练的重要步骤。然而，在训练和评估模型时可能会出现意外问题，因此了解需要注意的事项是非常有用的。
- en: 'You can quickly check, using the following code, how many values for the `trim`
    column appear exactly once:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，您可以快速检查`trim`列中出现的仅一次的值有多少个：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You will see that there are 124 values that are unique in the training dataset
    for the `trim` column. Likewise, in the validation dataset you can see there are
    273 values that are unique. It seems as if your colleague may have caught on to
    this and addressed it in their `OneHotEncoder` definition. They included the `han⁠dle_​unknown='infre⁠quent_​if_exist'`
    parameter and value. `handle_unknown` defines the behavior that is followed when
    an unknown value appears at prediction time, and the `'infrequent_if_exist'` value
    will assign unknown features to an infrequent category if it exists. To create
    an “infrequent” category, you can set the `min_frequency` argument. This again
    is something that can be tuned.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到在训练数据集中有124个在`trim`列中是唯一的值。同样，在验证数据集中，您可以看到有273个唯一值。看起来您的同事可能已经意识到了这一点，并在他们的`OneHotEncoder`定义中加以解决。他们包括了`han⁠dle_​unknown='infre⁠quent_​if_exist'`参数和值。`handle_unknown`定义了在预测时出现未知值时遵循的行为，而`'infrequent_if_exist'`值将未知特征分配给一个少见的类别（如果存在）。要创建一个“少见”类别，您可以设置`min_frequency`参数。这也是可以调整的内容。
- en: Setting the `min_frequency` too high will mean that many categories will have
    the same contribution to output of the model, lowering the usefulness of the feature.
    On the other hand, if the `min_frequency` is set too low, then you can run across
    the issue of having a large number of features that only appear once or the issue
    you have already seen where it is difficult to get a proper distribution of feature
    values between datasets.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 将`min_frequency`设置得太高将导致许多类别对模型输出的贡献相同，降低特征的有用性。另一方面，如果`min_frequency`设置得太低，则可能会出现许多仅出现一次的特征或者您已经看到的在数据集之间难以获得正确分布的特征值的问题。
- en: Set the `min_frequency` to 1 and then rerun the training code to see if there
    is any difference in performance. You will see that the performance only changed
    very slightly this time. In essence, you said that you treat all categories that
    appear less than 1 time (or 0 times) in the training set as the same. It may make
    sense to increase the `min_frequency` so that you treat all infrequent variables
    as the same category, the `infrequent` category. You will explore this later when
    performing hyperparameter tuning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 将`min_frequency`设置为1，然后重新运行训练代码，看看性能是否有所不同。你会发现，这一次性能只有微小的变化。实质上，你说你将所有在训练集中出现少于1次（或0次）的类别视为相同。也许增加`min_frequency`是有道理的，这样你就可以将所有不频繁的变量视为同一类别，即“不频繁”类别。在进行超参数调优时，你将稍后探索这一点。
- en: Feature crosses
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征交叉
- en: Consider the `model` and `trim` features for a moment. Often you think of these
    features together rather than separately, correct? When you say “I bought a Honda
    CR-V,” that does not completely describe the car. There can be many different
    *trims* or packages for the car. For a 2023 Honda CR-V, for example, three of
    the trims are “LX,” “EX,” and “EX-L.” It is entirely possible that the same names
    can be used across different vehicles as well. For example, the 2023 Honda Pilot
    also has the “LX” and “EX-L” trims. Therefore, the value of the `trim` variable
    also does not tell the entire story either. You need the value of both features
    to be able to identify the vehicle.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细考虑一下`model`和`trim`特征。通常情况下，你会将这些特征一起考虑，而不是分开考虑，对吧？当你说“我买了一辆本田CR-V”时，这并不能完全描述这辆车。车上可能有多种不同的*trim*或包装。例如，对于2023年的本田CR-V，有三种trim型号：“LX”，“EX”和“EX-L”。同样的名字也可能会用于不同的车型。例如，2023年的本田Pilot也有“LX”和“EX-L”这两种trim。因此，`trim`变量的值也不能完全描述整个情况。你需要这两个特征的值才能识别出车辆。
- en: However, in your model, you treat `model` and `trim` as completely separate
    variables. Recall, when using one-hot encoding you create a binary feature for
    each feature value, and a linear regression model will assign a weight to each
    of these binary features. The `trim` value of LX will have its own weight associated
    with it, independent of the value of the `model` variable since you are using
    one-hot encoding. That is, the `trim` feature value of “LX” will be treated the
    same regardless if the `model` is “Pilot” or “CR-V.” It makes sense to still consider
    the `make` feature separately since certain makes tend to be more expensive than
    others.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在你的模型中，你将`model`和`trim`视为完全独立的变量。回想一下，使用一位有效编码时，你为每个特征值创建一个二进制特征，并且线性回归模型将为每个这些二进制特征分配一个权重。因为你使用了一位有效编码，所以`trim`值为LX将有其自己的权重，与`model`变量的值无关。也就是说，“LX”这个`trim`特征值在`model`是“Pilot”还是“CR-V”时都会被同等对待。考虑到某些制造商往往比其他制造商更昂贵，因此仍然有必要单独考虑`make`特征。
- en: How do you capture both feature values as a pair? One way of approaching this
    is by using something known as a *feature cross*. A feature cross is a synthetic
    feature formed by concatenating two or more features. One way to intuitively think
    about this is that you are considering the value of both variables at the same
    time, rather than separately.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如何捕捉两个特征值作为一对？一种方法是使用所谓的*特征交叉*。特征交叉是通过连接两个或更多特征形成的合成特征。直观地说，你考虑的是同时考虑两个变量的值，而不是分开考虑。
- en: How does this work for categorical features? Recall that the feature value corresponding
    to one-hot encoding is a binary 0 or 1\. The idea of a feature cross in this case
    would be that the crossed feature value would be 1 if the *pair* of features have
    the corresponding values, but 0 otherwise. For example, take “CR-V LX” as the
    `model` and `trim`. The value for the “CR-V” feature (under one-hot encoding)
    would be 1, and the value for the “LX” feature would be 1\. So the value for the
    “CR-V LX” feature for the feature cross of `model` and `trim` would be 1\. However,
    the value for the “Pilot LX” feature would be 0 since the “Pilot” feature has
    a value of 0 in this example.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这对分类特征如何工作？回想一下，与一位有效编码对应的特征值是二进制的0或1。在这种情况下，特征交叉的想法是，交叉特征值将在对应值的特征对存在时为1，否则为0。例如，以“CR-V
    LX”作为`model`和`trim`。在一位有效编码下，“CR-V”特征的值将为1，而“LX”特征的值也将为1。因此，`model`和`trim`的特征交叉“CR-V
    LX”的值将为1。然而，“Pilot LX”的特征交叉值将为0，因为在这个例子中，“Pilot”特征的值为0。
- en: This seems like a simple feature to create and use, and when you used AutoML
    in Chapters [4](ch04.html#use_automl_to_predict_advertising_media) and [5](ch05.html#using_automl_to_detect_fraudulent_trans)
    it created these sorts of features (and more) for you in its process of finding
    the best model for your dataset. However, feature crosses can be extremely powerful
    features even in simple linear regression models. Can you think of other pairs
    of features that may benefit from using a feature cross?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是一个创建和使用的简单特征，当您在第[4章](ch04.html#use_automl_to_predict_advertising_media)和第[5章](ch05.html#using_automl_to_detect_fraudulent_trans)中使用AutoML时，它会在找到最适合您数据集的模型的过程中为您创建这些特征（以及更多）。然而，即使在简单的线性回归模型中，特征交叉也可以是极其强大的特征。您能想到其他可以受益于特征交叉的特征对吗？
- en: 'To see this in action, first replace the code for the `preproc_cols` function
    with the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到其效果，请首先将`preproc_cols`函数的代码替换为以下内容：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Consider the first two lines of this function. You are creating a new column,
    `model_trim`, in your DataFrame. This new column is formed by concatenating the
    value of the `model` and the `trim` column. So, the value of the `model_trim`
    column will depend on both the model and the trim of the car. The second line
    converts the corresponding string into all lowercase. This is a good practice
    to ensure that random differences in capitalization do not lead to different feature
    values. `color` and `interior` are another good example of features that have
    a relationship that can be represented well by a feature cross, so the third and
    fourth line implement the same ideas.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑此函数的前两行。您正在DataFrame中创建一个新列`model_trim`。这个新列是通过连接`model`列和`trim`列的值形成的。因此，`model_trim`列的值将取决于车型和车辆修剪。第二行将相应的字符串转换为全部小写。这是一个很好的做法，可以确保大小写的随机差异不会导致不同的特征值。`color`和`interior`是另一个很好的例子，它们之间的关系可以通过特征交叉很好地表示，因此第三行和第四行实现了相同的思想。
- en: 'Finally, you need to be sure that the new feature cross columns are being one-hot
    encoded just as the other categorical variables; to do this, update the list `categorical_columns`
    with the new feature names. Your final list should look like this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您需要确保新的特征交叉列正如其他分类变量一样进行了独热编码；为此，请更新列表`categorical_columns`以包含新的特征名称。您的最终列表应如下所示：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now execute the model code with the preceding changes and reevaluate the performance
    of the model. The complete code is available in the solution notebook if you get
    stuck. You should see that the RMSE is now about 3,122.14\. By adding feature
    crosses, you were able to decrease the RMSE by about 2% and get even closer to
    your ultimate goal.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在执行具有上述更改的模型代码，并重新评估模型的性能。如果您遇到困难，完整的代码在解决方案笔记本中可供查看。您应该能够看到RMSE现在约为3,122.14。通过添加特征交叉，您能够将RMSE降低约2％，并更接近您的最终目标。
- en: As an exercise, before moving on to the next section, explore other features
    that could be bucketized and crossed with other features. As a goal, see if you
    can get the RMSE for your model under 3,000 before moving on to the next section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，在进入下一节之前，探索其他可以进行桶装并与其他特征进行交叉的特征。作为目标，在进入下一节之前，看看能否将您模型的RMSE降到3,000以下。
- en: Hyperparameter Tuning
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调优
- en: In  the previous section you included new and useful features to lower the RMSE
    for your model. You may not have quite reached your goal of $2,000 RMSE yet, but
    you have made good progress. The next process that you will explore is known as
    *hyperparameter tuning*. Recall that a *hyperparameter* is a variable that is
    not updated during training but defines things such as the model architecture
    (such as number of hidden layers or neurons per hidden layer for neural networks),
    how features are engineered (such as how many buckets), and how the training process
    is executed (such as the learning rate or batch size). When you bucketized the
    `condition` feature, you selected a number of buckets. But how do you know what
    the optimal number of buckets would be? The process of hyperparameter tuning aims
    to answer these sorts of questions for you.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，您为模型添加了新的有用功能，以降低RMSE。也许您还没有完全达到$2,000 RMSE的目标，但您已经取得了良好的进展。接下来您将探索的下一个过程被称为*超参数调优*。请记住，*超参数*是在训练过程中不更新的变量，但定义了模型架构（如神经网络中隐藏层的数量或每个隐藏层的神经元数）、特征工程的方式（如多少个桶）以及训练过程的执行方式（如学习率或批量大小）。当您对`condition`特征进行桶装时，您选择了一些桶的数量。但是您如何知道最佳的桶数量是多少呢？超参数调优的过程旨在帮助您回答这些问题。
- en: Hyperparameter tuning strategies
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数调优策略
- en: 'There are three main strategies that are commonly used for hyperparameter tuning:
    grid search, random search, and Bayesian search. For all three, the first step
    is the same. First you select a range of candidate values for the hyperparameters
    you wish to tune. You can choose a range of values or a discrete set of values
    depending on the hyperparameter you want to tune. For example, if you want to
    tune the optimizer learning rate, you could set a range such as <math><mrow><mo>[</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>]</mo></mrow></math> for the candidate range.
    In your case, you bucketized the `condition` feature and set 6 as the number of
    buckets. This was really an arbitrary choice, and there may be a better choice.
    For example, you could set the candidate range between 5 and 15.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通常用于超参数调优的三种主要策略是：网格搜索、随机搜索和贝叶斯搜索。对于这三种方法，第一步是相同的。首先，您选择要调整的超参数的候选值范围。您可以根据要调整的超参数选择一个值范围或一个离散的值集合。例如，如果要调整优化器的学习率，您可以设置一个如<math><mrow><mo>[</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>]</mo></mrow></math>的范围作为候选范围。在您的情况下，您对`condition`特征进行了分桶处理，并将桶数设置为6。这实际上是一个任意选择，并且可能存在更好的选择。例如，您可以将候选范围设置在5到15之间。
- en: If you choose too low a number of buckets, you are treating large ranges of
    condition values the same in the model. For example, with two buckets, all condition
    values between `3.0` and `5.0` could be in the same bucket. On the other hand,
    if you have too many buckets, then you risk overfitting, as you will have a small
    number of examples per bucket that could be memorized by the model. With all of
    this in mind, 5 to 15 seems like a reasonable candidate range.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择的桶数太少，则在模型中处理大范围的条件值时将其视为相同。例如，使用两个桶，所有在`3.0`和`5.0`之间的条件值都可能位于同一个桶中。另一方面，如果您的桶数过多，则会有过拟合的风险，因为每个桶中的样本数可能会被模型记住。综上所述，5到15似乎是一个合理的候选范围。
- en: Once you have set the candidate ranges for the hyperparameters you wish to tune,
    the next step is to choose a tuning method. The *grid search* method is very simply
    a “try everything and see what works the best” approach. For example, suppose
    you are tuning for two hyperparameters. The first has 4 candidate values and the
    second has 3 candidate values, so there are 12 combinations of hyperparameters
    to check. A visual representation of this is shown in [Figure 8-3](#visual_representation_of_the_grid_searc).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您为希望调整的超参数设置了候选范围，下一步就是选择调整方法。*网格搜索*方法非常简单，即“尝试一切，找出最有效的方法”。例如，假设您要调整两个超参数。第一个有4个候选值，第二个有3个候选值，因此有12种超参数组合要检查。这在[图8-3](#visual_representation_of_the_grid_searc)中有可视化表示。
- en: '![Visual representation of the grid search method with two hyperparameters
    tuned](assets/lcai_0803.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![调整了两个超参数的网格搜索方法的可视化表示](assets/lcai_0803.png)'
- en: Figure 8-3\. Visual representation of the grid search method with two hyperparameters
    tuned.
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3\. 网格搜索方法的可视化表示，其中调整了两个超参数。
- en: To select the best set of hyperparameters, you train a model for each set of
    hyperparameters using the training dataset and evaluate the models using the validation
    dataset. In this case, you know for sure that you have the best hyperparameters
    (in the candidate ranges) since you tried every possible value.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择最佳的超参数集，您需要使用训练数据集为每组超参数训练一个模型，并使用验证数据集评估模型。在这种情况下，您确信已经找到了最佳的超参数（在候选范围内），因为您尝试了每一个可能的值。
- en: The downside of the grid search method should be apparent at this point. If
    you want to work with a couple of hyperparameters each with a small range of candidate
    values, then there are a reasonable number of models to train. However, if you
    want to tune a large number of hyperparameters with a large number of candidate
    values each, then this can quickly become infeasible. For example, if you have
    four hyperparameters to tune with 10 candidate values each, then there are 10,000
    candidate models to train.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在网格搜索方法的缺点应该显而易见了。如果您想调整几个超参数，每个超参数的候选值范围都很小，那么可以接受的模型数量就不多。然而，如果您想调整大量超参数，并且每个超参数都有大量候选值，那么这很快就会变得不可行。例如，如果您有四个超参数要调整，每个超参数有10个候选值，那么就有10,000个候选模型需要训练。
- en: There are two alternate approaches that are often used. The *random search*
    approach is a partial search strategy that randomly selects some preset number
    of the candidate models. Those models are trained and compared. The advantage
    of this approach is that you can control how much time and effort is taken in
    searching through the set of candidate models, but the downside is that you may
    miss the best model in the search space because you got unlucky in the random
    selection process.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 经常使用两种备选方法。*随机搜索*方法是一种部分搜索策略，随机选择预设数量的候选模型。这些模型将被训练和比较。这种方法的优点是您可以控制在搜索候选模型集合时花费多少时间和精力，但缺点是您可能因为在随机选择过程中不幸而错过了搜索空间中的最佳模型。
- en: The third approach is *Bayesian search* or *optimization*, which is a more intelligent
    approach to a partial search. The full details of the approach are beyond the
    scope of this book, but the core idea is fairly simple. You randomly train a small
    number of the candidate models as a starting point. Based on the evaluation metrics
    of those initial models, the Bayesian optimization algorithm chooses the next
    set of candidate models in the search space. These are the candidates, based on
    the earlier models, that are expected to have the best evaluation metrics. This
    process continues for some number of steps that are set up front. The next set
    of candidate models are chosen based on the previous candidates’ performance.
    Though this has the same downside as random search in terms of not exhausting
    the search space, the upside is that the search is a “more intelligent” search
    than random search.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法是*贝叶斯搜索*或*优化*，这是一种更智能的部分搜索方法。该方法的详细细节超出了本书的范围，但核心思想相当简单。首先随机训练少量候选模型作为起始点。根据这些初始模型的评估指标，贝叶斯优化算法选择搜索空间中的下一组候选模型。这些候选模型是基于之前模型的评估指标，预计具有最佳的评估指标。这个过程将在预先设定的步骤中继续进行。下一组候选模型将基于先前候选模型的性能选择。虽然在搜索空间耗尽方面与随机搜索相同的劣势，但好处是这种搜索比随机搜索更“智能”。
- en: Hyperparameter tuning in scikit-learn
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中的超参数调优
- en: 'In scikit-learn, both the grid search and random search strategies are easy
    to implement. In the remainder of this section, you will implement a variant of
    the grid search strategy to find a better set of hyperparameters for your model.
    First, add a couple of new transformers to bucketize the `odometer` and `year`
    columns, and remove the list of numeric columns since those will now be bucketized
    as categorical columns. Also, include the new `KBinsDiscretizer` transforms in
    the `ColumnTransformer`. For convenience, the corresponding code is included here:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，网格搜索和随机搜索策略都很容易实现。在本节的其余部分中，您将实现网格搜索策略的一种变体，以找到模型的更好超参数集。首先，添加一对新的转换器来分桶`odometer`和`year`列，并移除数值列的列表，因为这些现在将被分桶为分类列。还要在`ColumnTransformer`中包含新的`KBinsDiscretizer`转换器。为方便起见，这里包括相应的代码：
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You will tune the following four hyperparameters: the number of buckets for
    the odometer, condition, and the year columns, and the minimum frequency for the
    `OneHotEncoder` transformer for a feature to not be encoded as `infrequent`. In
    scikit-learn, you need to define the candidate ranges as a dictionary of values.
    Since you are using a `pipeline` for your model and transformations, the syntax
    may look a little odd at first glance. The code for this case is the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您将调整以下四个超参数：odometer、condition 和 year 列的桶数，以及`OneHotEncoder`转换器的最小频率，以便不将某些特征编码为`infrequent`。在
    scikit-learn 中，您需要将候选范围定义为值的字典。因为您正在使用`pipeline`进行模型和转换，所以语法可能乍看起来有点奇怪。此案例的代码如下：
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The dictionary has pairs of the form `''hyperparameter_name'' : candidate_range`.
    The hyperparameter range may look odd here at first glance, but it is not too
    bad to parse. For example, the first hyperparameter has the name `col_transformer__bucket_cond__n_bins`.
    This corresponds to the `n_bins` value for the `bucket_cond` transformer, which
    is part of the `col_transformer`. The corresponding candidate range is a list
    of possible values for the `n_bins` parameter of `bucket_cond`. `range(8,13)`
    is a convenient syntax for the list `[8,9,10,11,12]`. Note that the second endpoint
    `13` is not included in the list. For the `min_frequency` hyperparameter, the
    candidate range is `range(1,6)`.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '字典中有形如 `''hyperparameter_name'' : candidate_range` 的成对数据。超参数范围乍看起来可能有些奇怪，但解析起来并不困难。例如，第一个超参数的名称是
    `col_transformer__bucket_cond__n_bins`。这对应于 `col_transformer` 的一部分 `bucket_cond`
    转换器的 `n_bins` 值。相应的候选范围是 `bucket_cond` 的 `n_bins` 参数的可能取值列表。`range(8,13)` 是列表
    `[8,9,10,11,12]` 的一种便捷写法。注意，列表中不包括第二个端点 `13`。对于 `min_frequency` 超参数，候选范围是 `range(1,6)`。'
- en: Now that the candidate ranges have been defined, you need to define the strategy
    and then train the corresponding models—in this case, 625 candidate models with
    the different choices of hyperparameters that you have defined. This is not an
    excessively large number of models to train, but it will likely take at least
    an hour or so to fully train them all. Scikit-learn offers a variant of the grid
    search strategy known as a *halving grid search*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在候选范围已经定义好，你需要定义策略，然后训练相应的模型——在本例中，有625个候选模型，使用你定义的不同超参数选择。这不是一个过多的模型训练数量，但可能需要至少一个小时或更长时间来完全训练它们。Scikit-learn提供了一种名为*减半网格搜索*的网格搜索策略的变体。
- en: To perform a halving grid search, first train all candidate models, but only
    for a small portion of the training data. Based on the evaluation of those models,
    you keep some portion of the candidate model pool. The name implies that you keep
    half, but you can reduce the number of models more aggressively if you wish. After
    you remove models from the candidate pool, you then train the remaining candidate
    models using more of the training data. You repeat the overall process until you
    have chosen the best candidate model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行减半网格搜索，首先对所有候选模型进行训练，但仅使用少量的训练数据。根据这些模型的评估结果，保留候选模型池中的一部分。名称暗示你保留一半，但如果愿意，你可以更积极地减少模型数量。在从候选池中移除模型之后，然后使用更多的训练数据来训练剩余的候选模型。重复整个过程，直到选择出最佳候选模型。
- en: It is entirely possible that models that perform well on a subset of the data
    will not perform as well on the entire dataset and be eliminated in later rounds
    of the process. Additionally, it’s possible that a model that performed poorly
    on a small subset of the dataset would have actually performed very well on the
    entire training dataset. That candidate model could be discarded before you see
    the improvement in the model using more data. In general, any method you use other
    than grid search has some risk along these lines, but halving grid search tends
    to be more effective than random search in finding the best candidate model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 完全有可能，表现良好的模型在部分数据上表现良好，但在整个数据集上表现不佳，并且可能在后续的过程中被淘汰。此外，一个在小数据子集上表现不佳的模型可能在整个训练数据集上表现非常好。在你看到模型利用更多数据改善之前，该候选模型可能已被丢弃。一般来说，除了网格搜索之外的任何方法在这些方面都有一定风险，但是减半网格搜索倾向于比随机搜索更有效地找到最佳候选模型。
- en: 'To implement halving grid search in scikit-learn takes just a few lines of
    code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中实现减半网格搜索只需几行代码：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: At the time of writing, the halving grid search strategy is experimental in
    scikit-learn, so it has to be enabled with the first line. The second line imports
    the `HalvingGridSearchCV` class for performing halving grid search. The third
    line is where we create the object that will be performing the halving grid search.
    The first argument is the model or pipeline that you wish to use and the second
    argument is the `grid_params` dictionary you defined earlier. The keyword argument
    `cv` refers to a resampling method known as cross-validation. Briefly, `cv` corresponds
    to the number of trials per candidate model using different splits of the training
    dataset into a smaller training set and evaluation dataset. Higher `cv` values
    will lead to more precise evaluation metrics but take more time to process. The
    `verbose` argument takes an integer from `0` to `3`. The higher the number, the
    more information will be output during the tuning process. Finally, you have to
    set the metric that you are trying to optimize in the tuning process. Since we
    are trying to optimize RMSE, we use the `neg_root_mean_squared_error` score.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，halving grid search 策略在 scikit-learn 中是实验性的，因此必须通过第一行启用它。第二行导入`HalvingGridSearchCV`类以执行
    halving grid search。第三行是我们创建将执行 halving grid search 的对象的地方。第一个参数是您希望使用的模型或流水线，第二个参数是您之前定义的`grid_params`字典。关键字参数`cv`指的是一种称为交叉验证的重采样方法。简而言之，`cv`对应于每个候选模型使用训练数据集的不同拆分来进行试验的数量。较高的`cv`值将导致更精确的评估指标，但处理时间更长。`verbose`参数接受从`0`到`3`的整数。数字越高，在调整过程中输出的信息越多。最后，您必须设置您在调整过程中尝试优化的度量标准。由于我们试图优化RMSE，因此我们使用`neg_root_mean_squared_error`得分。
- en: Note
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You may wonder why we use the negative root mean squared error for the `scoring`
    argument. In statistical modeling, a score function should increase when a model
    improves. On the other hand, a loss function should decrease when the model improves.
    Hyperparameter tuning methods in scikit-learn are set to use score functions.
    Fortunately, we can get a score function by taking the negative RMSE instead.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道为什么我们在`scoring`参数中使用负的均方根误差。在统计建模中，得分函数应在模型改进时增加。另一方面，损失函数应在模型改进时减少。scikit-learn中的超参数调整方法设置为使用得分函数。幸运的是，我们可以通过取负的RMSE来得到一个得分函数。
- en: 'Now you are ready to perform hyperparameter tuning. Add the following code
    after the definition of the `grid_search` object, and execute the code cell to
    perform hyperparameter tuning to find the best candidate model:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好进行超参数调整。在定义`grid_search`对象之后添加以下代码，并执行代码单元以执行超参数调整以找到最佳候选模型：
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The second line that was added will print the hyperparameters for the best
    candidate model. The hyperparameter tuning process will take 35–40 minutes in
    Google Colab. You should see a similar result to the following, though the exact
    output may differ based on randomness in the sampling process:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 添加的第二行将打印最佳候选模型的超参数。在Google Colab中，超参数调整过程将需要35-40分钟。您应该看到类似以下的结果，尽管确切的输出可能会根据采样过程中的随机性而有所不同：
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can check the best model’s performance on our validation dataset so you
    can compare performance with the earlier models. Execute the following code in
    a new cell to output the evaluation metrics (RMSE) for the best model from the
    grid search:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在我们的验证数据集上检查最佳模型的性能，以便与早期模型的性能进行比较。在新单元格中执行以下代码以输出来自网格搜索最佳模型的评估指标（RMSE）：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that when you call the `predict()` method on the `grid_search` method,
    it calls the `predict()` method on the best model from the grid search. The RMSE
    on the best model was 2,915.02\. Compared with where you started, with an RMSE
    over 3,300, this is a significant improvement using both feature engineering and
    hyperparameter tuning. As an exercise, continue experimenting to see if you can
    find new features and tune any new hyperparameters that come up to see if you
    can improve the model further.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当您在`grid_search`方法上调用`predict()`方法时，它会调用网格搜索中最佳模型的`predict()`方法。最佳模型的RMSE为2,915.02。与起始值超过3,300的RMSE相比，这是通过特征工程和超参数调整显著的改进。作为练习，继续尝试实验，看看是否可以找到新的特征并调整任何新出现的超参数，以查看是否可以进一步改进模型。
- en: Finally, once you believe you have the best model that you can get, you should
    evaluate the model on the testing dataset. However, in this chapter, you will
    still explore new model architectures using Keras, so you should hold off for
    now on performing this  step.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦您认为已经得到了最佳模型，您应该在测试数据集上评估模型。然而，在本章中，您仍将探索使用Keras的新模型架构，因此暂时不要执行这一步骤。
- en: Model Improvement in Keras
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的模型改进
- en: This section explores different neural network model architectures for your
    car auction selling price problem using Keras. You will not go back through the
    feature selection and engineering conversation from before, but you will be introduced
    to Keras preprocessing layers as an analogue to the transformers you used in scikit-learn.
    After re-creating the feature engineering part of the scikit-learn pipeline, you
    will learn how to perform hyperparameter tuning using the Keras Tuner package
    and the Bayesian optimization method discussed in the previous section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了在Keras中为您的汽车拍卖销售价格问题使用不同的神经网络模型架构。您不会回顾之前的特征选择和工程对话，但将介绍Keras预处理层作为您在scikit-learn中使用的转换器的类比。重新创建scikit-learn管道的特征工程部分后，您将学习如何使用Keras
    Tuner包和前一节讨论的贝叶斯优化方法进行超参数调整。
- en: Introduction to Preprocessing Layers in Keras
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras中的预处理层介绍
- en: Keras preprocessing layers allow for you to easily build your data preprocessing
    functions into the model function in the same way that you created a pipeline
    in scikit-learn. You saw in [Chapter 7](ch07.html#training_custom_ml_models_in_python)
    and the previous section how convenient it was to include the preprocessing logic
    into the model itself. Though you did not export the model in the previous section,
    you can easily export the entire trained pipeline using the *joblib* library as
    you did in [Chapter 7](ch07.html#training_custom_ml_models_in_python).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Keras预处理层允许您将数据预处理功能轻松地构建到模型函数中，就像您在scikit-learn中创建管道一样。在[第7章](ch07.html#training_custom_ml_models_in_python)和前一节中，您看到将预处理逻辑包含到模型本身中是多么方便。尽管在前一节中您没有导出模型，但您可以像在[第7章](ch07.html#training_custom_ml_models_in_python)中使用*joblib*库一样轻松导出整个训练过程中的管道。
- en: Think back through the transformations you performed on the dataset in the previous
    section. You one-hot encoded the categorical features, bucketized the numeric
    features, and created feature crosses. Before starting to build the model in Keras,
    it is good to understand the preprocessing layers that you will be using.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请回顾您在前一节中对数据集执行的转换。您进行了分类特征的独热编码，对数值特征进行了分桶，并创建了特征交叉。在开始在Keras中构建模型之前，了解您将使用的预处理层是很重要的。
- en: The `Discretization` layer is used in Keras to bucketize numerical features
    just as the `KBinsDiscretizer` transformer was used in scikit-learn. You can provide
    the endpoints of the buckets or use the `adapt()` method to have Keras choose
    the endpoints based on the data and a specified number of bins. When using the
    `adapt()` method, you must specify the dataset that you wish to use. The range
    of values in this dataset is what will be used to choose the bucket boundaries.
    Typically, you should use your training dataset or a representative sample of
    your training dataset for the `adapt()` method.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`Discretization` 层在Keras中用于像scikit-learn中的 `KBinsDiscretizer` 转换器一样对数值特征进行分桶。您可以提供桶的端点或使用
    `adapt()` 方法，让Keras基于数据和指定的桶数量选择端点。使用 `adapt()` 方法时，您必须指定希望使用的数据集。此数据集中的值范围将用于选择桶的边界。通常情况下，您应该使用训练数据集或其代表性样本进行
    `adapt()` 方法。'
- en: Note
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The result of the `adapt()` method is a list of boundary points corresponding
    to the number of bins you chose. The left-most boundary point is actually the
    right endpoint for a bin and the right-most boundary point is actually the left
    endpoint for another bin.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`adapt()` 方法的结果是一个边界点列表，对应于您选择的桶的数量。最左边的边界点实际上是一个桶的右端点，而最右边的边界点实际上是另一个桶的左端点。'
- en: For example, if you set the number of bins to be four and receive the boundary
    points `[0.0, 1.0, 2.0]` then the actual bins are `(-inf, 0.0)`, `[0.0, 1.0)`,
    `[1.0, 2.0)`, and `[2.0, +inf)`. In other words, all values less than 0 will belong
    to the first bin and all values greater than 2.0 will belong to the last bin.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果将桶的数量设置为四个并收到边界点 `[0.0, 1.0, 2.0]`，那么实际的桶是 `(-inf, 0.0)`、`[0.0, 1.0)`、`[1.0,
    2.0)` 和 `[2.0, +inf)`。换句话说，所有小于0的值将属于第一个桶，所有大于2.0的值将属于最后一个桶。
- en: Another preprocessing layer that corresponds to the transformations being done
    in scikit-learn is the `StringLookup` layer. The `StringLookup` layer is used
    to encode categorical columns with string values. You can encode the values in
    different manners, but you will use one-hot encoding for your model. Another option
    is to encode the columns as integers and then perform one-hot encoding, or other
    possible transformations, in later layers.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与scikit-learn中正在进行的转换对应的预处理层是`StringLookup`层。`StringLookup`层用于对具有字符串值的分类列进行编码。您可以以不同的方式编码值，但是您将在模型中使用独热编码。另一个选项是将列编码为整数，然后在后续层中执行独热编码或其他可能的转换。
- en: 'Finally, you also performed feature crossing when preprocessing your features
    in scikit-learn. In scikit-learn, this was a somewhat manual process: you concatenated
    the strings corresponding to the values for each feature, then one-hot encoded
    the concatenated values. In Keras, there is a preprocessing layer that handles
    feature crosses, known as the `HashedCrossing` layer. The `HashedCrossing` layer
    takes two categorical features and creates the feature cross for you.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在预处理scikit-learn中特征时，还执行了特征交叉。在scikit-learn中，这是一个相对手动的过程：您将每个特征的值对应的字符串串联起来，然后对串联的值进行独热编码。在Keras中，有一个处理特征交叉的预处理层，称为`HashedCrossing`层。`HashedCrossing`层接受两个分类特征，并为您创建特征交叉。
- en: There are many more useful preprocessing layers to explore. For details about
    additional layers, see the [“Working with Preprocessing Layers”](https://oreil.ly/K6TLx)
    guide in the TensorFlow documentation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多有用的预处理层可以探索。有关更多层的详细信息，请参阅TensorFlow文档中的[“使用预处理层”](https://oreil.ly/K6TLx)指南。
- en: Creating the Dataset and Preprocessing Layers for Your Model
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建模型的数据集和预处理层
- en: Now you will re-create the preprocessing pipeline that you created in scikit-learn
    so you can explore new model architectures in Keras.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将重新创建在scikit-learn中创建的预处理管道，以便可以在Keras中探索新的模型架构。
- en: Return to [*https://colab.research.google.com*](https://colab.research.google.com).
    Open a new notebook and name the notebook *keras_model.ipynb*. You will be adding
    code to this notebook throughout the next few sections, but if you are stuck,
    there is a solution notebook in the *chapter8* directory also called [*keras_model.ipynb*](https://oreil.ly/AVf5n).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 返回至[*https://colab.research.google.com*](https://colab.research.google.com)。打开一个新笔记本并命名笔记本为*keras_model.ipynb*。您将在接下来的几个部分中向此笔记本添加代码，但如果遇到困难，*chapter8*目录中也有一个名为[*keras_model.ipynb*](https://oreil.ly/AVf5n)的解决方案笔记本。
- en: 'First, import the training and validation datasets into DataFrames as you did
    before with scikit-learn. Also split the datasets into a DataFrame of features
    and series of labels. If you need help doing so, here is the solution code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，像以前在scikit-learn中一样，将训练和验证数据集导入到DataFrames中。还将数据集拆分为特征的DataFrame和标签的系列。如果需要帮助，这里是解决方案代码：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You will need to prepare the input features in Keras. When using different
    preprocessing layers on different features, you cannot use the Sequential API
    that you leveraged in [Chapter 7](ch07.html#training_custom_ml_models_in_python)
    to build a neural network in Keras. The alternative API, the Functional API, is
    very similar to use with a slightly different syntax. To use the Functional API,
    you need to start by creating an `Input` for each input feature. First, copy the
    following code to a new cell in your notebook and execute the cell:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在Keras中准备输入特征。当在不同特征上使用不同的预处理层时，无法像您在[第7章](ch07.html#training_custom_ml_models_in_python)中使用的Sequential
    API一样在Keras中构建神经网络。替代API是Functional API，使用起来非常类似，只是语法略有不同。要使用Functional API，您需要首先为每个输入特征创建一个`Input`。首先，将以下代码复制到笔记本中的新单元格中，并执行该单元格：
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Before moving forward, take a moment to parse this code. First you import the
    preprocessing layers you will be leveraging in later steps. Then you split the
    list of columns into numeric and categorical columns (`num_cols` and `cat_cols`
    respectively) as you did in scikit-learn. Then you create an empty dictionary
    for the `Input` layers. You then create an `Input` for each feature. The `for
    col in cat_cols` statement means that the following code will be executed for
    every column in the `cat_cols` list. `tf.keras.Input` is the full name for `Input`
    layers. The first argument, `shape`, states that for each example, each feature
    will be just a single value. You set the name of each `Input` to the corresponding
    column name in `cat_cols`. Finally, you set the data type (`dtype`) to `tf.string`,
    which is the implementation of the string data type in TensorFlow. The concepts
    are the same for the `num_cols` column except that the data type is set to `tf.int64`,
    TensorFlow’s implementation of 64-bit integers.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请花点时间解析这段代码。首先，导入你稍后将利用的预处理层。然后，像在 scikit-learn 中那样，将列列表分为数值列和分类列（分别为
    `num_cols` 和 `cat_cols`）。然后，创建一个空字典用于 `Input` 层。接着，为每个特征创建一个 `Input`。`for col
    in cat_cols` 语句意味着以下代码将针对 `cat_cols` 列表中的每一列执行。`tf.keras.Input` 是 `Input` 层的完整名称。第一个参数
    `shape` 表明，对于每个示例，每个特征将只是一个单一值。你将每个 `Input` 的名称设置为 `cat_cols` 中对应的列名。最后，设置数据类型
    (`dtype`) 为 `tf.string`，这是 TensorFlow 中字符串数据类型的实现。对于 `num_cols` 列，概念相同，只是数据类型设置为
    `tf.int64`，即 TensorFlow 中 64 位整数的实现。
- en: 'Now that the `Input` layers have been created, you are ready to create the
    preprocessing layers. First, one-hot encode each categorical column using the
    following code:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `Input` 层已创建完成，你可以开始创建预处理层。首先，使用以下代码对每个分类列进行 one-hot 编码：
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: First you create an empty dictionary to hold the layers you will use for preprocessing.
    Then you create a `StringLookup` layer for every categorical column, and then
    you set the `output_mode` to `'one_hot'` so that the output is one-hot encoded.
    You then use the `adapt()` method on the corresponding column in the training
    dataset to learn the vocabulary for one-hot encoding. Note that if an unknown
    value appears when transforming data in the model, it will be assigned an unknown
    value `'[UNK]'`. The behavior of how unknown values are treated can be set. Finally,
    you specify the input for the column at training and prediction time and store
    that in the dictionary `preproc_layers`. You should reference the documentation
    for [StringLookup](https://oreil.ly/cGDlm) for more details.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个空字典来保存预处理中将使用的层。然后，为每个分类列创建一个 `StringLookup` 层，并将 `output_mode` 设置为 `'one_hot'`，以便进行
    one-hot 编码输出。然后，使用训练数据集中相应列上的 `adapt()` 方法来学习 one-hot 编码的词汇表。请注意，如果在模型中转换数据时出现未知值，将分配一个未知值
    `'[UNK]'`。可以设置未知值处理的行为。最后，在训练和预测时为列指定输入，并将其存储在字典 `preproc_layers` 中。有关更多详细信息，请参考
    [StringLookup 文档](https://oreil.ly/cGDlm)。
- en: 'Next up are the `Discretization` layers for bucketizing your numeric columns.
    Use the following code to create the `Discretization` preprocessing layers:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是用于分桶数值列的 `Discretization` 层。使用以下代码创建 `Discretization` 预处理层：
- en: '[PRE21]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The idea here is similar to before, where for each numeric column you create
    a `Discretization` layer. Each layer (for now) will split the data into 10 buckets
    and then one-hot encode the bucket membership. Finally, you use the `adapt()`
    method to fit the bucketization to the individual columns.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法与之前类似，对于每个数值列，你创建一个 `Discretization` 层。每个层（暂时）将数据分为 10 个桶，然后进行桶成员的 one-hot
    编码。最后，使用 `adapt()` 方法将桶化适应于各个列。
- en: 'The last type of feature engineering you performed with your scikit-learn model
    was feature crossed. Now, re-create these features using `HashedCrossing` layers
    using the following code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 scikit-learn 模型执行的最后一种特征工程类型是特征交叉。现在，使用以下代码重新创建这些特征，使用 `HashedCrossing` 层：
- en: '[PRE22]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that the `HashedCrossing` column is slightly different from performing
    a feature cross as we did in scikit-learn. A *hashing* function is a special type
    of function that takes a string input and returns an integer in return. The output
    is deterministic—that is, you always get the same output integer when inputting
    the same string, but the outputs are distributed in such a way it is nearly impossible
    to predict. The `HashedCrossing` column takes the output of the hashing function
    and uses that to choose a bin to place the corresponding element into. For the
    `model_trim` layer, there are 1,000 bins, and for the `color_interior` layer,
    there are 400 bins.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`HashedCrossing`列与我们在scikit-learn中执行特征交叉略有不同。*哈希*函数是一种特殊类型的函数，它接受字符串输入并返回一个整数。输出是确定性的，即当输入相同的字符串时，始终会得到相同的输出整数，但输出以一种几乎不可能预测的方式分布。`HashedCrossing`列接收哈希函数的输出，并使用它选择一个桶来放置相应的元素。对于`model_trim`层，有1,000个桶，对于`color_interior`层，有400个桶。
- en: 'Where do these numbers come from? Well, there are over one million different
    model and trim combinations possible from the values that come up in our dataset.
    Likewise, there are about 300 combinations of color and interior values that are
    possible. Since the distribution of values is effectively random, possibly multiple
    values could end up in the same bin. Overestimating the number of bins helps to
    lower the likelihood of that scenario. There is a trade-off, though: each bin
    corresponds to a feature in your model, and this corresponds to multiple weights
    depending on how many neurons there are in the first hidden layer. This trade-off
    is the reason that we chose 1,000 bins for the `model_trim` feature rather than
    including over one million bins. This trade-off also makes the number of bins
    a great candidate for hyperparameter tuning.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字从何而来？嗯，我们的数据集中可能有超过一百万种不同的模型和修剪组合。同样，颜色和内饰值可能有大约300种组合。由于值的分布是有效随机的，可能会有多个值最终落入同一个桶中。过度估计桶的数量有助于降低这种情况发生的可能性。不过，存在一个权衡：每个桶对应于模型中的一个特征，并且这取决于第一个隐藏层中有多少神经元的多个权重。这种权衡是我们选择在`model_trim`特征中使用1,000个桶而不是包括一百万个桶的原因。这种权衡也使得桶的数量成为超参数调优的一个很好的选择。
- en: Building a Neural Network Model
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立神经网络模型
- en: 'Now that you have created the preprocessing layers, it is time to put everything
    together. The first thing you should do is concatenate all of the preprocessing
    layers into a single layer for input into the neural network. Use the following
    code to perform this task:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经创建了预处理层，是时候将所有内容组合在一起了。首先要做的是将所有预处理层连接成一个单独的层，以输入到神经网络中。使用以下代码执行此任务：
- en: '[PRE23]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This code is fairly straightforward: you create a `Concatenate` layer and then
    give it a list of input layers. Since you have been creating all of your preprocessing
    layers in a dictionary, you simply need to extract the values of the dictionary.
    `prepared_layer` is a length `3903` tensor, taking into account all of the possible
    feature values for the one-hot encoded and bucketized features. The second line
    reshapes `prepared_layer` into a rank 2 tensor, which is expected in the Functional
    API for the next layer.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码非常直观：您创建一个`Concatenate`层，然后给它一个输入层列表。由于您已经在字典中创建了所有预处理层，因此只需提取字典的值即可。`prepared_layer`是一个长度为`3903`的张量，考虑了所有可能的特征值，用于独热编码和桶化特征。第二行将`prepared_layer`重塑为二阶张量，这是Functional
    API中下一层所期望的。
- en: 'With all of your inputs as a single layer, the rest of the process of building
    a model is more or less the same as in [Chapter 7](ch07.html#training_custom_ml_models_in_python).
    There is a minor difference with the Functional API in Keras, but this is easier
    to explain after seeing the code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有输入作为一个单独的层后，构建模型的其余过程与[第七章](ch07.html#training_custom_ml_models_in_python)中基本相同。在Keras的Functional
    API中有一点点不同，但在看过代码之后很容易解释：
- en: '[PRE24]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The first line creates a new layer, `hid_1`, which is a dense layer with 16
    neurons and ReLU activation. In the Functional API, you have to specify an input
    for each layer just as you would for a function. In this case, this will be the
    `prepared_layer` from before. Next, you define a second layer, `hid_2`, with the
    same parameters as the first hidden layer, but with `hid_1` as the input layer.
    Finally, you define the output layer, `output`, as a dense layer with a single
    output neuron and no activation function. Recall that for regression models, your
    output should be a single number, the predicted value.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行创建了一个新的层`hid_1`，它是一个具有16个神经元和ReLU激活的密集层。在Functional API中，您必须为每个层指定一个输入，就像为函数一样。在本例中，这将是之前的`prepared_layer`。接下来，您定义第二层`hid_2`，具有与第一隐藏层相同的参数，但以`hid_1`作为输入层。最后，您将输出层定义为一个具有单个输出神经元且没有激活函数的密集层。请记住，对于回归模型，您的输出应该是一个单一的预测值。
- en: 'You need to now create the `Model` object. You do this by using `tf.keras.Model`
    and specifying the inputs for the model (`inputs` that you defined earlier) and
    the output for the model (the `output` layer). From here, the process is the same
    as it was in [Chapter 7](ch07.html#training_custom_ml_models_in_python), with
    a few minor differences. Use the following code to compile and train the model:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您需要创建`Model`对象。您可以使用`tf.keras.Model`来做到这一点，并指定模型的输入（之前定义的`inputs`）和模型的输出（`output`层）。从这里开始，过程与[第7章](ch07.html#training_custom_ml_models_in_python)中的过程基本相同，有一些细微的差别。使用以下代码来编译和训练模型：
- en: '[PRE25]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: First you compile the model, setting the optimizer to be the Adam optimizer
    and the loss function to be the mean squared error, or MSE. Next you create the
    `tf.Datasets` for training and validation from the corresponding DataFrames. You
    set the batch size to `100` for training and `1000` for validation. To train the
    model, you use the `fit()` method as before.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要编译模型，将优化器设置为Adam优化器，损失函数设置为均方误差（MSE）。接下来，您从相应的DataFrames创建用于训练和验证的`tf.Datasets`。将批量大小设置为`100`用于训练和`1000`用于验证。要训练模型，您像以前一样使用`fit()`方法。
- en: Your model performance may vary slightly depending on the randomness involved
    with initializing and training a neural network, but you should see an MSE of
    around $10,719,103 after training completes, which translates to an RMSE of $3,274\.
    The performance is similar to your model’s performance in scikit-learn before
    hyperparameter tuning. Note that your MSE may differ due to randomness in how
    the neural network was initialized. The choice of neural network architecture
    was arbitrary, though, so there is likely still further room for improvement.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型性能可能会因初始化和训练神经网络时涉及的随机性而有所不同，但在训练完成后，您应该看到约$10,719,103的MSE，这相当于$3,274的RMSE。性能与在scikit-learn中进行超参数调整之前的模型性能类似。请注意，由于神经网络初始化的随机性可能会导致您的MSE有所不同。神经网络架构的选择是任意的，因此可能仍有改进的空间。
- en: Hyperparameter Tuning in Keras
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Keras中进行超参数调整
- en: Now that you have a working model in Keras, it is time to work on improving
    it. When building Keras models, you can use the Keras Tuner package to easily
    perform hyperparameter tuning.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您在Keras中有一个可工作的模型，是时候开始改进它了。在构建Keras模型时，您可以使用Keras Tuner包轻松进行超参数调整。
- en: 'Google Colab does not include the Keras Tuner package by default, but it is
    easy to install. `pip` (a recursive acronym for Pip Installs Packages) is a package-management
    tool for Python to install and manage packages. The `pip install` command will
    allow you to download and install packages from the Python Package Index or PyPI.
    Run the following command in a new cell to install the Keras Tuner package:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab默认不包含Keras Tuner包，但安装起来很容易。`pip`（递归缩写，意为Pip Installs Packages）是Python的包管理工具，用于安装和管理包。`pip
    install`命令允许您从Python Package Index或PyPI下载和安装包。在新的单元格中运行以下命令以安装Keras Tuner包：
- en: '[PRE26]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`pip` is a command-line tool, so as before you use the `!` line magic to run
    the line as a bash command. The `-q` flag suppresses most of the output from the
    install to avoid cluttering up the notebook environment. Now that Keras Tuner
    is installed, you can start to alter your model code to prepare it for hyperparameter
    tuning.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip`是一个命令行工具，因此您像以前一样使用`!`行魔术将该行作为bash命令运行。`-q`标志抑制了安装过程中大部分输出，以避免在笔记本环境中产生混乱。现在Keras
    Tuner已安装完成，您可以开始修改模型代码以准备进行超参数调整。'
- en: 'When using Keras Tuner you need to create a function (which you will call `build_model`)
    that takes hyperparameters as inputs and returns your compiled model. For every
    candidate model, this function will be executed with different hyperparameters
    to create the model for training. As you noticed earlier, it takes a few minutes
    to perform the `adapt()` method for all of your preprocessing layers, so ideally
    you will have this code outside of the `build_model` function. Use the following
    code to create the `build_model` function for Keras Tuner:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras Tuner时，需要创建一个函数（称为`build_model`），该函数以超参数作为输入，并返回已编译的模型。对于每个候选模型，此函数将使用不同的超参数执行，以创建用于训练的模型。正如您之前注意到的，对所有预处理层执行`adapt()`方法需要几分钟的时间，因此理想情况下，您将将此代码放在`build_model`函数之外。使用以下代码为Keras
    Tuner创建`build_model`函数：
- en: '[PRE27]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'First you import the `keras_tuner` package and the `partial` function, both
    of which will be used later. Next you define an “intermediate” function: `_build_model_fn`.
    The underscore at the front of the function name is a Python convention that this
    is a function that is not meant to be used directly. Note that this function has
    two arguments, `hp` and `prepared_layer`. The `hp` argument will be provided by
    Keras Tuner, and the `prepared_layer` argument will correspond to the layer of
    the same name you created earlier.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入`keras_tuner`包和`partial`函数，两者稍后将被使用。接下来，您定义了一个“中间”函数：`_build_model_fn`。函数名称开头的下划线是Python的约定，表示这是一个不应直接使用的函数。请注意，此函数有两个参数，`hp`和`prepared_layer`。`hp`参数将由Keras
    Tuner提供，`prepared_layer`参数将对应您之前创建的同名层。
- en: The line `units_1 = hp.Int('units_1', min_value=8, max_value=64, step=4)` is
    an example of how to define a hyperparameter using Keras Tuner. `hp.Int` defines
    an integer-valued hyperparameter. You can also define floating-point hyperparameters
    (`hp.Float`), Boolean hyperparameters (`hp.Boolean`), or choose from a list of
    possible values (`hp.Choice`). For more details, see the [Keras Tuner documentation](https://oreil.ly/ZnCKe).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`units_1 = hp.Int(''units_1'', min_value=8, max_value=64, step=4)`这一行展示了如何使用Keras
    Tuner定义超参数的示例。`hp.Int`定义了一个整数值超参数。您还可以定义浮点数超参数（`hp.Float`）、布尔型超参数（`hp.Boolean`）或从可能值列表中选择（`hp.Choice`）。有关更多详细信息，请参阅[Keras
    Tuner文档](https://oreil.ly/ZnCKe)。'
- en: 'In the case of integer hyperparameters, you set a minimum value, a maximum
    value, and a step size. So in this case, the possible values would be 8, 12, 16,
    20, ..., 64\. In the preceding code, you create three hyperparameters: `units_1`,
    `units_2`, and `units_3`. Next you define the three hidden layers for your model.
    Note that for each hidden layer, the number of neurons is replaced with the `hp.Int`
    objects that were defined. Otherwise, the process is similar to the code you used
    for building and compiling a model. The `_build_model_fn` function returns the
    compiled model as the output.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于整数超参数，您设置最小值、最大值和步长。因此，在本例中，可能的值为8、12、16、20、...、64。在前面的代码中，您创建了三个超参数：`units_1`、`units_2`和`units_3`。接下来，为模型定义三个隐藏层。请注意，对于每个隐藏层，神经元的数量被替换为之前定义的`hp.Int`对象。否则，该过程与您用于构建和编译模型的代码类似。`_build_model_fn`函数将编译后的模型作为输出返回。
- en: The `build_model` function needs to take only `hp` as an argument for use with
    Keras Tuner. This is where the `partial` function comes into play. The `partial`
    function allows you to create a new function from an old function, but with certain
    fixed arguments already plugged into the original function. `partial(_build_model_fn,
    prepared_layer=prepared_layer)` takes the function `_build_model_fn` and creates
    a new function where your `prepared_layer` layer is always plugged in for the
    corresponding argument.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`build_model`函数只需接受`hp`作为参数，以供Keras Tuner使用。这就是`partial`函数的作用所在。`partial`函数允许您从旧函数创建新函数，但某些固定参数已经插入到原始函数中。`partial(_build_model_fn,
    prepared_layer=prepared_layer)`接受`_build_model_fn`函数，并创建一个新函数，其中您的`prepared_layer`层始终插入到相应的参数中。'
- en: 'Now that the `build_model` function has been created, next create the tuner
    that will manage the hyperparameter tuning process. Use the following code to
    create the `tuner` object and perform the hyperparameter search:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经创建了`build_model`函数，接下来创建调整器，用于管理超参数调优过程。使用以下代码创建`tuner`对象，并执行超参数搜索：
- en: '[PRE28]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `tuner` is an example of a `Tuner` in Keras Tuner using Bayesian optimization
    to optimize hyperparameters. You create an `Objective` to define the goal of the
    tuning process. In this case, you want to minimize the loss (MSE) of the validation
    dataset, so you set `val_loss` as the goal and `direction` as `min` to specify
    that you wish to minimize the `val_loss`. You also set a maximum number of trials
    or candidate models to be trained.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`tuner`是Keras Tuner中使用贝叶斯优化来优化超参数的一个示例`Tuner`。您可以创建一个`Objective`来定义调整过程的目标。在本例中，您希望最小化验证数据集的损失（MSE），因此将`val_loss`设置为目标，并将`direction`设置为`min`以指定您希望最小化`val_loss`。您还设置了最大试验或候选模型的数量。'
- en: To perform the tuning process, you use the `search()` method on `tuner`. You
    specify the training dataset, the number of epochs to train the candidate models
    for, the verbosity (how much detail you want from 0 to 3), and the validation
    dataset. Note that the number of epochs is fairly small here since you are training
    many models. Often, but not always, you can understand which models will perform
    better after only a few epochs of training, without having to train them until
    convergence. Your output and results should look similar to those in [Figure 8-4](#an_example_of_the_output_from_the_hyper).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行调整过程，您可以在`tuner`上使用`search()`方法。您需要指定训练数据集、训练候选模型的周期数、详细程度（从0到3的程度）和验证数据集。请注意，这里的周期数相对较少，因为您正在训练许多模型。通常情况下，您可以在仅进行几个周期的训练后就了解哪些模型将表现更好，而不必将它们训练至收敛。您的输出和结果应与[图8-4](#an_example_of_the_output_from_the_hyper)中的类似。
- en: '![An example of the output from the hyperparameter tuning process using Keras
    Tuner](assets/lcai_0804.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras Tuner进行超参数调整过程的输出示例](assets/lcai_0804.png)'
- en: Figure 8-4\. An example of the output from the hyperparameter tuning process
    using Keras Tuner.
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4\. 使用Keras Tuner进行超参数调整过程的输出示例。
- en: 'Your exact results will vary depending on some randomness in the process, but
    likely your results will have the best model’s `val_loss` around $6,000,000, which
    corresponds to an RMSE of $2,470\. This is an improvement over the previous model’s
    results, even after only five  epochs. You should now train this best candidate
    model for longer to see if you can get even better results. To do this, you need
    to be able to retrieve the best hyperparameters. Execute the following code in
    a new cell to find the hyperparameters from the best candidate model:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于过程中的某些随机性，你的确切结果会有所不同，但可能最佳模型的`val_loss`约为$6,000,000，对应RMSE为$2,470。这比之前的模型结果有所改进，甚至仅经过五次周期后也是如此。现在，你应该继续训练这个最佳候选模型，看看是否能获得更好的结果。为此，你需要能够检索最佳超参数。在新单元格中执行以下代码以找到最佳候选模型的超参数：
- en: '[PRE29]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The best hyperparameters found once again can vary from run to run due to randomness
    in the process. For the run being discussed in this chapter, the best values for
    `units_1`, `units_2`, and `units_3` were 52, 64, and 32 respectively.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于过程中的随机性，每次找到的最佳超参数可能会有所不同。在本章讨论的运行中，`units_1`、`units_2`和`units_3`的最佳值分别为52、64和32。
- en: 'To make things easier, Keras Tuner includes the `tuner.hypermodel.build()`
    method where we can provide the best hyperparameters, and it will pass those values
    to the `build_model` method to re-create our best candidate model. Use the following
    code to do just that, create an early stopping callback, and train the best model
    until the `val_loss` stops improving:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化操作，Keras Tuner包括了`tuner.hypermodel.build()`方法，我们可以在其中提供最佳超参数，它将这些值传递给`build_model`方法以重新创建我们的最佳候选模型。使用以下代码来实现这一点，创建一个早期停止的回调，并训练最佳模型直到`val_loss`不再改善：
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: After training the model, the validation RMSE has decreased even further, to
    under 2,000\. You finally have a model that meets your initial goals! As an exercise,
    implement hyperparameter tuning for other hyperparameters such as the number of
    bins for the `HashedCrossing` layers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后，验证RMSE进一步降低，降至2000以下。最终你得到了一个符合初衷的模型！作为练习，实现其他超参数的超参数调整，例如`HashedCrossing`层的箱数。
- en: However, we chose the model based on performance on the validation dataset,
    so we could have happened to choose a model that was just simply biased toward
    that dataset. This is where the test dataset comes in. The test dataset has not
    been used at any point during the model training process, so it is the closest
    thing that we have to data “in the wild” or that your model would see in production.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们选择的模型基于验证数据集的性能，所以我们可能只是简单地选择了一个偏向于该数据集的模型。这就是测试数据集的用武之地。测试数据集在模型训练过程中从未被使用过，因此它是我们最接近“野外”数据或您的模型在生产中可能看到的数据。
- en: 'Since we have chosen our final model, we can use the test dataset as a final
    verification of performance. To do so, use the following code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经选择了最终的模型，我们可以使用测试数据集作为性能的最终验证。为此，请使用以下代码：
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How did your model perform on the test dataset? If the performance was similar,
    then you are in great shape and ready to pass the model along to be deployed.
    Otherwise, you may need to recombine the datasets, do a new training-validation-test
    data split, and start the process over from the beginning. In the process of doing
    so, be sure to ensure that your training, validation, and test datasets have similar
    distributions of examples. In practice, different distributions in the different
    datasets is a very common reason to see a large drop in performance when evaluating
    on the test dataset.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型在测试数据集上表现如何？如果表现相似，那么您的模型表现良好，准备好进行部署。否则，您可能需要重新组合数据集，进行新的训练-验证-测试数据集拆分，并从头开始整个过程。在这个过程中，请确保您的训练、验证和测试数据集具有相似的示例分布。实际上，不同数据集之间的不同分布是在测试数据集评估时看到性能大幅下降的非常常见原因。
- en: Having to start over can be frustrating, but once the independence of the test
    dataset is compromised by using it to make a decision, this is the best approach
    if you need to continue to improve your model performance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要继续改进模型的性能，重新开始可能会令人沮丧，但一旦测试数据集的独立性被使用来做出决策所破坏，这是最佳的方法。
- en: Hyperparameter Tuning in BigQuery ML
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BigQuery ML 中的超参数调优
- en: In this section you will revisit the model you created in scikit-learn and Keras
    in BigQuery ML. You will load the car auction price datasets you were using before
    into BigQuery, explore feature engineering in BigQuery ML, and train a neural
    network model. Finally, you will learn how to perform hyperparameter tuning in
    BigQuery ML.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，您将重新访问在 BigQuery ML 中创建的 scikit-learn 和 Keras 模型。您将把之前使用的汽车拍卖价格数据集加载到
    BigQuery 中，探索 BigQuery ML 中的特征工程，并训练一个神经网络模型。最后，您将学习如何在 BigQuery ML 中进行超参数调优。
- en: You will not be doing a full review of the concepts of BigQuery and BigQuery
    ML again in this chapter, so please reference [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg)
    for additional details on certain tasks being performed in this chapter.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不会再对 BigQuery 和 BigQuery ML 的概念进行全面审查，请参考[第6章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)了解本章中执行的某些任务的详细信息。
- en: Loading and Transforming Car Auction Data
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和转换汽车拍卖数据
- en: First go to the [Google Cloud Console](http://console.cloud.google.com) and
    navigate to BigQuery (either using the side menu or the search bar). In the Explorer
    to the right of your project ID, click the View Actions button, which is represented
    by three vertical dots to the right of your project ID. Then click “Create dataset.”
    A reminder of the location of these items in the UI is shown in [Figure 8-5](#the_location_of_the_view_actions_button).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，转到[Google Cloud 控制台](http://console.cloud.google.com)，然后导航到 BigQuery（可以使用侧边菜单或搜索栏）。在项目
    ID 的右侧的“资源管理器”中，单击“查看操作”按钮，该按钮由项目 ID 右侧的三个垂直点表示。然后单击“创建数据集”。UI 中这些项目的位置提示如图8-5所示。
- en: '![The location of the View Actions button and “Create dataset” action in the
    BigQuery UI](assets/lcai_0805.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![BigQuery UI 中查看操作按钮和“创建数据集”操作的位置](assets/lcai_0805.png)'
- en: Figure 8-5\. The location of the View Actions button and “Create dataset” action
    in the BigQuery UI.
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5. BigQuery UI 中查看操作按钮和“创建数据集”操作的位置。
- en: Create a new dataset, `car_sales_prices`, in the US region. Once the dataset
    is created, you can use the View Actions button beside your dataset to create
    a BigQuery table. Select the dataset, click “View actions,” then select “Create
    table.” Create three tables, with one for each of the three datasets, using the
    information in [Table 8-3](#options_for_the_three_tables_to_be_crea). Note that
    you will need to replace the `*<dataset>*` part of the “Select file from GCS bucket
    or use a URI pattern” and “Table” fields with `train` , `valid` , and `test` for
    the three different datasets.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国地区创建一个名为 `car_sales_prices` 的新数据集。创建数据集后，您可以使用数据集旁边的“查看操作”按钮创建一个 BigQuery
    表。选择数据集，单击“查看操作”，然后选择“创建表格”。根据 [表 8-3](#options_for_the_three_tables_to_be_crea)
    中的信息创建三个表，每个表对应一个数据集。请注意，您需要将“从 GCS 存储桶中选择文件或使用 URI 模式”和“表格”字段中的 `*<dataset>*`
    部分替换为 `train`、`valid` 和 `test`，以适应三个不同的数据集。
- en: Table 8-3\. Options for the three tables to be created
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-3\. 要创建的三个表的选项
- en: '| Field | Value |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 字段 | 值 |'
- en: '| --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Create table from | Google Cloud Storage |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 从中创建表格 | Google Cloud Storage |'
- en: '| Select file from GCS bucket or use a URI pattern | low-code-ai-book/car_prices_*<dataset>*.csv
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 从 GCS 存储桶中选择文件或使用 URI 模式 | low-code-ai-book/car_prices_*<dataset>*.csv |'
- en: '| File format | CSV |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 文件格式 | CSV |'
- en: '| Table | car_prices_*<dataset>* |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 表格 | car_prices_*<dataset>* |'
- en: '| Schema | Auto detect |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | 自动检测 |'
- en: Before beginning to build the model, you need to replicate the transformations
    that you performed in scikit-learn and Keras. First, recall that you performed
    one-hot encoding on the categorical columns. Remember that in BigQuery ML, all
    string-valued columns are automatically one-hot encoded, so there will be nothing
    you need to do for those columns.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始构建模型之前，您需要复制在 scikit-learn 和 Keras 中执行的转换操作。首先，回想一下，您对分类列执行了独热编码。请记住，在 BigQuery
    ML 中，所有字符串值列都会自动进行独热编码，因此对于这些列，您无需进行任何操作。
- en: What about the numeric columns that you bucketized? BigQuery ML provides two
    functions for bucketizing numeric features. First is the `ML.BUCKETIZE` function,
    which takes in two arguments. The first argument is the column you wish to bucketize,
    and the second is a list of bucket endpoints which you provide. Note that you
    need to know what buckets you wish to use up front.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 那么您如何处理桶化的数值列呢？BigQuery ML 提供了两个用于桶化数值特征的函数。首先是 `ML.BUCKETIZE` 函数，它接受两个参数。第一个参数是您希望进行桶化的列，第二个参数是您提供的桶端点的列表。请注意，您需要事先知道希望使用的桶。
- en: There is also the `ML.QUANTILE_BUCKETIZE` function. This function also takes
    two arguments. The first argument is again the column you wish to bucketize, but
    the second column will now be the number of buckets you wish to split the data
    into. `ML.QUANTILE_BUCKETIZE` will split the data into quantile-based buckets
    based on the number of buckets you specify. For example, if you specify four buckets,
    the first quartile (25% of data) will be placed into the first bucket, the second
    quartile into the second bucket, and so on. The actual output of these functions
    will be of the form `bin_n` for data placed into bucket `n`, and then BigQuery
    ML will one-hot encode this column just like any other string column.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 还有 `ML.QUANTILE_BUCKETIZE` 函数。该函数同样需要两个参数。第一个参数再次是您希望进行分桶的列，但第二个列现在是您希望将数据分成的桶的数量。
    `ML.QUANTILE_BUCKETIZE` 将根据您指定的桶数将数据分成基于分位数的桶。例如，如果您指定四个桶，那么第一个四分位数（数据的25%）将被放入第一个桶中，第二四分位数将被放入第二个桶中，依此类推。这些函数的实际输出将采用
    `bin_n` 的形式，用于放入第 `n` 个桶的数据，然后 BigQuery ML 将像对待任何其他字符串列一样对此列进行独热编码。
- en: The final transformation you performed was a feature cross. The function that
    implements feature crosses in BigQuery ML is the `ML.FEATURE_CROSS` function.
    This function takes in a `STRUCT` of feature columns and returns feature crosses
    of those columns. If you provide just a pair of columns, then it will return the
    feature cross of those two columns. If you provide three columns, then you will
    receive three feature crosses, one for each possible pair.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 您执行的最终转换是特征交叉。在 BigQuery ML 中实现特征交叉的函数是 `ML.FEATURE_CROSS`。该函数接受一个特征列的 `STRUCT`
    并返回这些列的特征交叉。如果您只提供一对列，那么它将返回这两列的特征交叉。如果您提供三列，则将收到三个特征交叉，分别对应每对可能的列。
- en: 'The syntax for `ML.FEATURE_CROSS` may seem a little odd at first:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`ML.FEATURE_CROSS` 的语法起初可能看起来有些奇怪：'
- en: '[PRE32]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `STRUCT` keyword is needed to create a `STRUCT`, which is an ordered list
    of columns possibly of different types. Without this keyword, you will receive
    an error from this line of code.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`STRUCT`关键字用于创建`STRUCT`，这是可能包含不同类型列的有序列。如果没有这个关键字，您将从这行代码中收到错误信息。
- en: 'Now you are ready to preprocess your data. Write and execute the following
    SQL query in the BigQuery console to perform the desired transformations:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以预处理您的数据。在BigQuery控制台中编写并执行以下SQL查询，执行所需的转换：
- en: '[PRE33]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `SELECT * EXCEPT(...)` statement returns all columns in the table except
    for the ones listed. Here `int64_field_0` is the name for the `Unnamed: 0` column
    from before. You want to also remove the `mmr` column since you were not going
    to be able to use it for training. Finally, you did not use the numeric values
    for `odometer`, `year`, and `condition` before, as you had bucketized those features,
    so you will not return those features in the results.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`SELECT * EXCEPT(...)`语句返回表中除了列出的列之外的所有列。这里的`int64_field_0`是之前的`Unnamed: 0`列的名称。您还想删除`mmr`列，因为您将不会将其用于训练。最后，您之前没有使用`odometer`、`year`和`condition`的数值，因为您已经对这些特征进行了桶化，所以在结果中不会返回这些特征。'
- en: Next, you bucketize the `odometer`, `year`, and `condition` columns using `ML.QUANTILE_BUCKETIZE`
    and 10 buckets. The `OVER()` clause at the end allows you to split up the data
    into different sets (based on the inside of the `OVER` statement) and then bucketize
    into quantiles. Here, you simply bucketize into quantiles without any additional
    splitting.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您使用`ML.QUANTILE_BUCKETIZE`和10个桶对`odometer`、`year`和`condition`列进行桶化。最后的`OVER()`子句允许您将数据分割成不同的集合（基于`OVER`语句内部），然后进行分位数桶化。在这里，您只是进行分位数桶化，没有额外的分割。
- en: Finally, you implement the feature crosses with `ML.FEATURE_CROSS`. For this
    example, you have the `LIMIT 10` statement, so you can look at just the first
    few rows of data. An example of what the results could look like is shown in [Table 8-4](#the_output_from_the_preprocessing_queri).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以使用`ML.FEATURE_CROSS`实现特征交叉。对于本例，您有`LIMIT 10`语句，因此可以查看数据的前几行。显示的结果示例可参见[表 8-4](#the_output_from_the_preprocessing_queri)。
- en: Table 8-4\. The output from the preprocessing queries for the `ML.QUANTILE_BUCKETIZE`
    and `ML.FEATURE_CROSS` transformations
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-4\. ML.QUANTILE_BUCKETIZE 和 ML.FEATURE_CROSS 转换的预处理查询输出
- en: '| `odo_bucket` | `year_bucket` | `cond_bucket` | `make_model` | `color_interior`
    |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `odo_bucket` | `year_bucket` | `cond_bucket` | `make_model` | `color_interior`
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| `bin_10` | `bin_1` | `bin_1` | `Nissan_300ZX` | `red_red` |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `bin_10` | `bin_1` | `bin_1` | `Nissan_300ZX` | `red_red` |'
- en: '| `bin_7` | `bin_1` | `bin_1` | `Chevrolet_Corvette` | `red_—` |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `bin_7` | `bin_1` | `bin_1` | `Chevrolet_Corvette` | `red_—` |'
- en: '| `bin_10` | `bin_1` | `bin_2` | `Lexus_LS 400` | `silver_silver` |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `bin_10` | `bin_1` | `bin_2` | `Lexus_LS 400` | `silver_silver` |'
- en: '| `bin_10` | `bin_1` | `bin_4` | `Jeep_Cherokee` | `white_gray` |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `bin_10` | `bin_1` | `bin_4` | `Jeep_Cherokee` | `white_gray` |'
- en: '| `bin_9` | `bin_1` | `bin_2` | `Mazda_MX-5 Miata` | `red_blue` |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `bin_9` | `bin_1` | `bin_2` | `Mazda_MX-5 Miata` | `red_blue` |'
- en: '| `bin_10` | `bin_1` | `bin_2` | `Honda_Accord` | `blue_—` |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `bin_10` | `bin_1` | `bin_2` | `Honda_Accord` | `blue_—` |'
- en: Note that for the bucketized columns, the output is `bin_n` as expected. Also,
    the output for the feature cross columns has the form `value1_value2`. These concatenated
    values will be one-hot encoded by BigQuery ML, taking a very similar approach
    to what you did in scikit-learn earlier in the chapter.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于桶化的列，输出是预期的`bin_n`形式。此外，特征交叉列的输出形式为`value1_value2`。这些串联值将由BigQuery ML进行独热编码，与您在本章早些时候在scikit-learn中所做的非常相似。
- en: Training a Linear Regression Model and Using the TRANSFORM Clause
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练线性回归模型并使用TRANSFORM子句
- en: Now you are ready to train a linear regression model using the preceding query
    you wrote for preprocessing the data. Note that if you transform the data using
    that query, save the results, and then train the model using the new table, everything
    works just fine. However, you must perform the same transformations at prediction
    time. This becomes very tricky when you do not know exactly how the one-hot encoding
    was done nor do you have the bucket endpoints for the bucketization.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用前面编写的查询来训练线性回归模型，以预处理数据。请注意，如果您使用该查询转换数据，保存结果，然后使用新表训练模型，一切都能正常工作。然而，在预测时，您必须执行相同的转换。当您不知道独热编码的确切方式或者桶化的分桶端点时，这变得非常棘手。
- en: 'BigQuery ML provides the `TRANSFORM` clause to enable you to build these transformations
    into the model. The overall structure of the `CREATE MODEL` statement is as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery ML 提供了`TRANSFORM`子句，使您能够将这些转换集成到模型中。`CREATE MODEL`语句的整体结构如下：
- en: '[PRE34]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `<transformation_sql>` is the `SELECT` part of the preceding query where
    you specified the columns you wanted to use and the transformations on those columns.
    Write and execute the following SQL statement to train a linear regression model
    using your transformations in a `TRANSFORM` clause:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`<transformation_sql>`是前面查询的`SELECT`部分，其中指定了要使用的列及其上的转换。编写并执行以下 SQL 语句，使用`TRANSFORM`子句训练线性回归模型：'
- en: '[PRE35]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This query should seem mostly familiar from what you did before in [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg)
    up to a few changes. First, the `TRANSFORM` clause is included to build the transformation
    logic into the model so that it can be referenced at inference time. When you
    call `ML.PREDICT` to serve predictions, the `TRANSFORM` clause will be executed
    on the input table before being passed to the model for predictions. This means
    that things like the bucket endpoints will be included in the model itself now.
    In scikit-learn and Keras, you used pipelines and preprocessing layers, respectively,
    to manage this process.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询应该在大部分情况下都与您之前在[第 6 章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)中所做的类似，只有一些变化。首先，包含了`TRANSFORM`子句以将转换逻辑构建到模型中，以便在推理时引用。当调用`ML.PREDICT`来提供预测时，`TRANSFORM`子句将在输入表上执行，然后传递给模型进行预测。这意味着像桶端点这样的事物现在将被包含在模型本身中。在
    scikit-learn 和 Keras 中，您使用管道和预处理层来管理此过程。
- en: 'The other thing that you may have noticed is that there is a new option. The
    `data_split_method` option dictates how the data will be split for training and
    validation. Since you already have a separate validation dataset, the `NO_SPLIT`
    option is employed to use the entire training dataset for training. You can evaluate
    your trained model using your validation dataset using the following SQL statement:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到的另一件事是，出现了一个新选项。`data_split_method`选项决定了数据在训练和验证中的拆分方式。由于你已经有了一个单独的验证数据集，所以采用`NO_SPLIT`选项来使用整个训练数据集进行训练。你可以使用以下
    SQL 语句对已训练的模型进行验证：
- en: '[PRE36]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Since you used the RMSE for evaluation before, you will use it again here for
    consistency. Your RMSE here could be fairly high, and possibly over $8,000\. You
    can check the RMSE for the training set as well by running the following query:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您之前使用过 RMSE 进行评估，因此在这里为保持一致性，您将再次使用它。这里的 RMSE 可能会相当高，可能超过 $8,000\. 您可以通过运行以下查询来检查训练集的
    RMSE：
- en: '[PRE37]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The RMSE on the training dataset will be closer to $3,000 and what you expected
    from your scikit-learn model before. This is a classic example of overfitting,
    but where is this coming from? The feature crosses involve a very large number
    of possible values, thus leading to a very large number of features for the model.
    You can compute the number of features coming from the feature crosses by running
    the following query:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集上的 RMSE 将接近于 $3,000，并且与您之前的 scikit-learn 模型预期一致。这是过拟合的一个典型例子，但是它是从哪里来的呢？特征交叉涉及大量可能的值，因此导致模型的特征数量非常多。您可以通过运行以下查询计算来自特征交叉的特征数量：
- en: '[PRE38]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You will see that there are 770,000 different feature values from the feature
    crosses. Such a large number of features compared to the number of examples can
    lead to overfitting very easily. In the next section, you will learn how regularization
    techniques can address overfitting with large numbers of features.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到特征交叉中存在 770,000 个不同的特征值。与示例数相比，这么多的特征数量很容易导致过拟合。在接下来的部分，您将了解到正则化技术如何处理大量特征的过拟合问题。
- en: 'Finally, you can predict using your model in the same way as before in [Chapter 6](ch06.html#using_bigquery_ml_to_train_a_linear_reg):'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以像在[第 6 章](ch06.html#using_bigquery_ml_to_train_a_linear_reg)中之前一样使用模型进行预测：
- en: '[PRE39]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You can train a deep neural network regression model in a very similar manner
    by simply changing the options, as you can see in the following SQL statement:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过简单更改选项来训练深度神经网络回归模型，如下所示的 SQL 语句所示：
- en: '[PRE40]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Configure a Hyperparameter Tuning Job in BigQuery ML
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 BigQuery ML 中配置超参数调优作业
- en: Once you have written the code to train a model, then you only need to make
    a few small alterations to begin hyperparameter tuning. First you will need to
    include a new option, `num_trials`. This option sets the number of different models
    that will be trained during the hyperparameter tuning process. You can optionally
    also set a value for the `num_parallel_trials` option. This will allow you to
    run multiple trials in parallel at the same time. The total number of resources
    used to train all the models will be the same, but being able to run multiple
    models in parallel will make it take less time overall. However, there is a trade-off
    when using Bayesian optimization as implemented in BigQuery ML and Vertex AI in
    general. The more parallel trials you run, the fewer iterations you go through
    until you get to the maximum number of trials, and in general Bayesian optimization
    learns from each iteration.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 编写训练模型的代码后，只需进行少量修改即可开始超参数调整。首先，您需要包含一个新的选项 `num_trials`。此选项设置在超参数调整过程中将训练的不同模型数量。您还可以选择为
    `num_parallel_trials` 选项设置一个值。这将允许您同时并行运行多个试验。用于训练所有模型的总资源数量将保持不变，但能够同时运行多个模型将总体花费的时间缩短。然而，使用
    BigQuery ML 和 Vertex AI 中实现的贝叶斯优化时存在权衡。您运行的并行试验越多，直到达到最大试验次数为止，贝叶斯优化学习的迭代次数就越少。
- en: After setting the `num_trials` option, the next step is to set up your hyperparameters.
    In BigQuery ML, only certain hyperparameters can be tuned. For deep neural network
    (DNN) models, you can tune the `batch_size`, `dropout`, `hidden_units`, `learn_rate`,
    `optimizer`, `l1_reg`, `l2_reg`, and `activation_fn`. You will focus on `dropout`,
    `l1_reg`, and `hidden_units` here, but you should explore other hyperparameters
    as an exercise.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置了 `num_trials` 选项之后，下一步是设置超参数。在 BigQuery ML 中，只能调整某些超参数。对于深度神经网络（DNN）模型，您可以调整
    `batch_size`、`dropout`、`hidden_units`、`learn_rate`、`optimizer`、`l1_reg`、`l2_reg`
    和 `activation_fn`。在这里，您将专注于 `dropout`、`l1_reg` 和 `hidden_units`，但您可以作为练习探索其他超参数。
- en: Regularization
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: You are familiar with `hidden_units` from earlier examples. But what about `dropout`
    and `l1_reg`? *Dropout* is a type of regularization technique. In general, regularization
    techniques are used to reduce the risk of overfitting for a model. *Overfitting*
    occurs when the model performs much better on the training dataset than evaluation
    datasets. This often happens because the model “memorizes” the dataset and starts
    to miss the general patterns that are needed to perform well on other datasets.
    One main way of reducing this risk of overfitting is to lower the model’s complexity.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 您在之前的示例中熟悉了 `hidden_units`。但 `dropout` 和 `l1_reg` 呢？*Dropout* 是一种正则化技术。通常，正则化技术用于减少模型过拟合的风险。*过拟合*
    是指模型在训练数据集上的表现远远优于评估数据集。这通常发生因为模型“记住”了数据集，开始错过在其他数据集上表现良好所需的一般模式。减少过拟合风险的主要方式之一是降低模型的复杂性。
- en: 'L1 and L2 regularization are commonly the first regularization techniques that
    ML practitioners learn about. Suppose you have a loss function <math><mrow><mi>L</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow></math> . Recall
    that the goal of the training process is to minimize this loss function. The idea
    of L1/L2 regularization is to add an additional term to the loss function to force
    the learning algorithm to balance minimizing the original loss function and the
    new “penalty term.” Let <math><msub><mi>W</mi> <mn>2</mn></msub></math> represent
    the sum of the squares of all of the weights in your model. For L2 regularization,
    the new loss function looks like the following:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化通常是机器学习实践者首先学习的正则化技术。假设您有损失函数 <math><mrow><mi>L</mi> <mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow></math>。回想一下，训练过程的目标是最小化这个损失函数。L1/L2 正则化的想法是在损失函数中添加一个额外的项，以强制学习算法在最小化原始损失函数和新的“惩罚项”之间取得平衡。让
    <math><msub><mi>W</mi> <mn>2</mn></msub></math> 表示模型中所有权重的平方和。对于 L2 正则化，新的损失函数如下所示：
- en: <math><mrow><msub><mi>L</mi> <mrow><mi>r</mi><mi>e</mi><mi>g</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>L</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <mo>*</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>.</mo></mrow></math>
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi>L</mi> <mrow><mi>r</mi><mi>e</mi><mi>g</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>L</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <mo>*</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>.</mo></mrow></math>
- en: 'The rough idea is that for a model to become more complex, the values of the
    weights need to become larger to have more of an effect on the outcome. This new
    loss function balances the original loss function and the “complexity” of the
    model as measured by <math><msub><mi>W</mi> <mn>2</mn></msub></math> . <math><mi>λ</mi></math>
    is known as the *regularization rate *, which controls how much the original loss
    function is weighed against the complexity of the model. The higher the value
    of <math><mi>λ</mi></math> , the more complexity is punished in the training process.
    Similarly for L1 regularization, the term <math><msub><mi>W</mi> <mn>2</mn></msub></math>
    is replaced by the sum of the absolute values of all of the weights <math><msub><mi>W</mi>
    <mn>1</mn></msub></math> . These types of regularization can be used together
    in what is known as elastic net regularization. The corresponding loss function
    for elastic net regularization is as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 大致的思路是，为了使模型变得更复杂，权重值需要变得更大，以对结果产生更大的影响。这个新的损失函数平衡了原始损失函数和模型复杂性（由<math><msub><mi>W</mi>
    <mn>2</mn></msub></math>度量）之间的关系。<math><mi>λ</mi></math>被称为*正则化率*，它控制原始损失函数与模型复杂性之间的权衡程度。<math><mi>λ</mi></math>的值越高，在训练过程中惩罚模型的复杂性就越严重。类似于L1正则化，L2正则化中的<math><msub><mi>W</mi>
    <mn>2</mn></msub></math>被所有权重的绝对值之和<math><msub><mi>W</mi> <mn>1</mn></msub></math>所取代。这些正则化类型可以结合在一起，称为弹性网络正则化。弹性网络正则化的相应损失函数如下：
- en: <math><mrow><msub><mi>L</mi> <mrow><mi>e</mi><mi>n</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>L</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>λ</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>W</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>λ</mi>
    <mn>2</mn></msub> <mo>×</mo> <msub><mi>W</mi> <mn>2</mn></msub></mrow></math>
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msub><mi>L</mi> <mrow><mi>e</mi><mi>n</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>L</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>λ</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>W</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>λ</mi>
    <mn>2</mn></msub> <mo>×</mo> <msub><mi>W</mi> <mn>2</mn></msub></mrow></math>
- en: Note that <math><msub><mi>λ</mi> <mn>1</mn></msub></math> and <math><msub><mi>λ</mi>
    <mn>2</mn></msub></math> are different constants controlling the influence of
    L1 and L2 regularization separately.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，<math><msub><mi>λ</mi> <mn>1</mn></msub></math>和<math><msub><mi>λ</mi> <mn>2</mn></msub></math>是分别控制L1和L2正则化影响的不同常数。
- en: You now know what the definitions of L1 and L2 regularization are, but what
    effect do they actually have on the model? The mathematics behind this is beyond
    the scope of this book, though is not too complex, and the ultimate effects are
    easy to describe. L2 regularization tends to try to push weights to smaller values.
    L1 regularization tends to push weights that are not influential on the model’s
    performance to zero. This can be very valuable when you have a large number of
    sparse features. An example of this scenario is when you are creating feature
    crosses of two features with a large number of values. This is exactly the scenario
    you encountered in the linear regression model trained in BigQuery ML. In general,
    when using feature crosses, it is usually a good idea to include L1 regularization.
    The regularization parameter(s) control how aggressive the push on the values
    of the weights are during the training process.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道了L1和L2正则化的定义，但它们对模型的实际影响是什么？这方面的数学超出了本书的范围，虽然并不太复杂，但最终的效果很容易描述。L2正则化倾向于将权重推向较小的值。L1正则化倾向于将对模型性能不重要的权重推向零。当你有大量稀疏特征时，这可能非常有价值。例如，当你创建具有大量值的两个特征的特征交叉时。这正是你在BigQuery
    ML中训练的线性回归模型中遇到的情况。通常，在使用特征交叉时，包括L1正则化通常是个好主意。正则化参数控制训练过程中权重值推动的侵略性。
- en: Dropout is a different kind of regularization in the sense that it is applied
    to the model itself during the training process and not to the loss function.
    The idea of dropout on neural networks is that a certain percentage of neurons
    are “turned off” for each batch of data. By *turned off* here we mean that the
    corresponding weighted sums for certain neurons in hidden layers are set to zero
    for that specific batch of data. The goal of using a technique like dropout is
    to hinder the model’s complexity during training time. This keeps the model from
    becoming too complex while still letting the model learn more about the data.
    However, during prediction time, no dropout is used so that you have full access
    to the model’s power.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种不同类型的正则化，因为它是在模型本身在训练过程中应用的，而不是在损失函数上。在神经网络中使用dropout的想法是，每批数据中有一定百分比的神经元被“关闭”。这里所说的“关闭”是指在特定数据批次中，隐藏层中某些神经元的加权总和被设置为零。使用dropout这样的技术的目标是在训练时阻碍模型的复杂性。这样可以避免模型变得过于复杂，同时仍然让模型更多地学习数据。然而，在预测时，不会使用dropout，这样可以充分利用模型的能力。
- en: Note
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Over the past decade, researchers have found that using dropout at prediction
    time as well can be beneficial.^([1](ch08.html#ch01fn3)) This can be used as a
    way to represent a model’s uncertainty for both classification and regression
    tasks and to make a model’s predictions nondeterministic.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，研究人员发现，在预测时使用**dropout**同样是有益的^([1](ch08.html#ch01fn3))。这可以用作表示模型不确定性的一种方式，适用于分类和回归任务，并使模型的预测变得非确定性。
- en: Using hyperparameter tuning in the CREATE MODEL statement
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在**CREATE MODEL**语句中使用超参数调优。
- en: 'Now that you understand a little bit about regularization, it is time to set
    up hyperparameter tuning in BigQuery ML. First consider the following SQL statement:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对正则化有了一些了解，是时候在BigQuery ML中设置超参数调优了。首先考虑以下SQL语句：
- en: '[PRE41]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The statement for creating a hyperparameter tuning job is very similar to what
    you used before, with some key differences for the sake of hyperparameter tuning.
    First, notice the `hidden_units` option. Instead of just having a single list
    of hidden units, instead there is the `hparam_candidates` function. This function
    takes a list of structs with the corresponding hyperparameter tuning values and
    passes them along to the model during the tuning process. Here you are having
    the model decide the best architecture between three possibilities. The first
    is a neural network with 64 neurons in the first hidden layer, 32 in the second
    layer, and 16 in the third layer. The second option has two hidden layers with
    32 and 16 neurons each, respectively. Finally, the last option has a single hidden
    layer with 32 neurons. Also, you are searching for the best `l1_reg` and `dropout`
    by using an `hparam_range`. `hparam_range` is used to find the best value in a
    range of floating-point values. For example, here the range for `dropout` is between
    0 and 0.8 for the percentage of neurons in hidden layers affected by dropout at
    training time.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 创建超参数调优作业的语句与之前使用的非常相似，但为了进行超参数调优，有一些关键区别。首先，请注意`hidden_units`选项。不再仅仅是一个隐藏层单元的列表，而是有了`hparam_candidates`函数。此函数接受一个包含相应超参数调优值的结构体列表，并在调优过程中将它们传递给模型。在这里，你让模型在三种可能性之间决定最佳架构。第一种是具有64个神经元的神经网络，第二层有32个神经元，第三层有16个神经元。第二个选项有两个隐藏层，分别有32个和16个神经元。最后一个选项只有一个隐藏层，有32个神经元。此外，你正在使用`hparam_range`来搜索最佳的`l1_reg`和`dropout`值。`hparam_range`用于在浮点值范围内查找最佳值。例如，在这里，`dropout`的范围在0到0.8之间，表示在训练时影响隐藏层神经元的百分比。
- en: Finally, there are a couple of new options that need to be set before beginning
    the training. First, the `num_trials`, which was mentioned before, and the `hparam_​tun⁠ing_objectives`.
    You want to optimize the RMSE, so set the `hparam_​tun⁠ing_objectives` to be `mean_squared_error`.
    Go ahead, if you have not already, and start the tuning process. This tuning process
    will take around an hour to complete.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在开始训练之前，有几个需要设置的新选项。首先是`num_trials`，前面提到过，以及`hparam_​tun⁠ing_objectives`。你希望优化RMSE，所以将`hparam_​tun⁠ing_objectives`设置为`mean_squared_error`。如果还没有开始，现在可以开始调优过程。这个调优过程大约需要一个小时来完成。
- en: Note
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In the query for the hyperparameter tuning job, you have to specify the optimizer
    being used with the `optimizer='adagrad'` option. The default optimizer, `adam`,
    does not support L1 regularization. For more details, please see the BigQuery
    ML documentation for [creating DNN models](https://oreil.ly/6hZQN).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在超参数调优作业的查询中，您必须使用 `optimizer='adagrad'` 选项指定正在使用的优化器。默认优化器 `adam` 不支持 L1 正则化。有关更多详细信息，请参阅
    BigQuery ML 文档中关于 [创建 DNN 模型](https://oreil.ly/6hZQN) 的内容。
- en: 'Once the training process has completed, you can explore the trial results
    and chosen hyperparameters by executing the following query:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练过程完成，您可以通过执行以下查询来探索试验结果和选择的超参数：
- en: '[PRE42]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: An example of what your output should look like is shown in [Table 8-5](#the_results_of_the_trial_info_query_for).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 你的输出示例如 [表 8-5](#the_results_of_the_trial_info_query_for) 所示。
- en: Table 8-5\. The results of the trial info query for the five best trials—note
    the chosen hyperparameters and trial metrics (the exact values in your output
    will differ from what is shown here; some column names were condensed for readability)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-5\. 五个最佳试验的试验信息查询结果—注意所选的超参数和试验指标（您的输出中的确切值将与此处显示的不同；一些列名为了可读性而被压缩）
- en: '| `trial_id` | `l1_reg` | `hidden_units` | `dropout` | `mean_squared_error`
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| `trial_id` | `l1_reg` | `hidden_units` | `dropout` | `mean_squared_error`
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 10 | 1.0 | 64 | 0.0 | 194784591.6 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 1.0 | 64 | 0.0 | 194784591.6 |'
- en: '| 32 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 32 |'
- en: '| 16 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 16 |'
- en: '| 8 | 0.00031591034078693391 | 64 | 0.0 | 213445602.34905455 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.00031591034078693391 | 64 | 0.0 | 213445602.34905455 |'
- en: '| 32 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 32 |'
- en: '| 16 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 16 |'
- en: '| 9 | 1.0 | 64 | 0.25599690406708309 | 218611976.60226983 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 1.0 | 64 | 0.25599690406708309 | 218611976.60226983 |'
- en: '| 32 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 32 |'
- en: '| 16 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 16 |'
- en: 'If you use `ML.PREDICT` with `car_sales_prices.dnn_hp_car_model` as the model
    of choice, BigQuery will automatically use the best trial by default:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择 `car_sales_prices.dnn_hp_car_model` 作为模型，使用 `ML.PREDICT`，BigQuery 将默认使用最佳试验：
- en: '[PRE43]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Options for Hyperparameter Tuning Large Models
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型模型超参数调整选项
- en: The frameworks and techniques discussed in this chapter are wonderful for datasets
    and models that are not too large. However, using scikit-learn and Keras on local
    machines or Colab notebooks for very large datasets and models could take a long
    time, or even be impossible due to memory and processing constraints. Training
    and tuning large models is an art of its own, and there are tools available on
    public cloud providers to make this much easier. This book does not do a deep
    dive into these products, as they tend to be much more involved from the custom
    code development point of view, but simply lists some options and references for
    those who are interested.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的框架和技术非常适合数据集和模型不太大的情况。然而，在本地机器或 Colab 笔记本上使用 scikit-learn 和 Keras 处理非常大的数据集和模型可能需要很长时间，甚至可能由于内存和处理约束而无法完成。训练和调整大型模型是一门艺术，并且有公共云提供商提供的工具可以使这一过程变得更加容易。本书不会对这些产品进行深入介绍，因为从自定义代码开发的角度来看，这些产品通常更为复杂，但只是列出了一些选项和参考资料，供有兴趣的人参考。
- en: Vertex AI Training and Tuning
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vertex AI 训练和调优
- en: In [Chapter 7](ch07.html#training_custom_ml_models_in_python) you saw how you
    could package up a Python module for training a scikit-learn model and submit
    it to Vertex AI Training. The same could be done for the scikit-learn or Keras
    code in this chapter for hyperparameter tuning.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 7 章](ch07.html#training_custom_ml_models_in_python) 中，您看到了如何将一个 Python
    模块打包为训练 scikit-learn 模型，并提交给 Vertex AI 训练。在本章中，用于超参数调优的 scikit-learn 或 Keras 代码也可以做同样的处理。
- en: Vertex AI also offers a hyperparameter tuning service as part of Vertex AI Training.
    This uses the `cloudml-hypertune` Python to report metrics to Vertex AI from various
    different trials, which can be executed in different clusters using Vertex AI
    Training. Like Keras Tuner, Vertex AI uses Bayesian optimization to find the best
    hyperparameters for your model.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 还提供作为 Vertex AI 训练一部分的超参数调整服务。这使用 `cloudml-hypertune` Python 从不同的试验向
    Vertex AI 报告指标，可以在不同的集群中使用 Vertex AI 训练执行。与 Keras 调优器类似，Vertex AI 使用贝叶斯优化来找到您模型的最佳超参数。
- en: For more details on how to use this service, please see the [Vertex AI documentation](https://oreil.ly/qcLDL).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何使用此服务的更多详细信息，请参阅[Vertex AI 文档](https://oreil.ly/qcLDL)。
- en: Automatic Model Tuning with Amazon SageMaker
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker 自动模型调整
- en: Amazon SageMaker includes an automatic model tuning service ([SageMaker AMT](https://oreil.ly/boB_A))
    for performing hyperparameter tuning. You can use SageMaker AMT with built-in
    algorithms, custom algorithms, or SageMaker prebuilt containers for ML frameworks
    such as scikit-learn, TensorFlow, and PyTorch.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 包括一个自动模型调整服务（[SageMaker AMT](https://oreil.ly/boB_A)），用于进行超参数调整。你可以使用
    SageMaker AMT 配合内置算法、自定义算法或 SageMaker 预构建的 ML 框架容器，如 scikit-learn、TensorFlow 和
    PyTorch。
- en: For more details, see the [SageMaker AMT documentation](https://oreil.ly/boB_A).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多详情，请参阅 [SageMaker AMT 文档](https://oreil.ly/boB_A)。
- en: Azure Machine Learning
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure 机器学习
- en: Azure Machine Learning includes hyperparameter tuning as part of the Python
    client library and command-line interface. Like the other options mentioned, you
    can provide your own custom model written in the framework of your choice, make
    the hyperparameters for the model arguments for a function creating the model,
    specify the hyperparameter search space, and specify a job configuration to submit
    to run a hyperparameter sweep job on Azure Machine Learning. For more information,
    see the [Azure Machine Learning documentation](https://oreil.ly/vrKRr).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 机器学习包括超参数调整作为 Python 客户端库和命令行界面的一部分。与前述选项类似，你可以提供自己选择框架中编写的自定义模型，将模型的超参数作为函数创建模型的参数，指定超参数搜索空间，并指定一个作业配置来提交在
    Azure 机器学习上运行超参数扫描作业。更多信息，请参阅 [Azure 机器学习文档](https://oreil.ly/vrKRr)。
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you took a custom code model built by a colleague and improved
    it using feature engineering and hyperparameter tuning. You leveraged new transformers
    in scikit-learn and performed a grid search to hyperparameter tune the original
    linear regression model. You learned how to perform the same feature engineering
    in Keras using preprocessing layers and perform hyperparameter tuning using Keras
    Tuner for your neural network model in Keras. Finally, you learned how to perform
    these same tasks in BigQuery ML using SQL.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你接手了同事构建的自定义代码模型，并通过特征工程和超参数调整进行了改进。你利用了 scikit-learn 中的新转换器，并进行了网格搜索以调整原始的线性回归模型。你学会了如何在
    Keras 中使用预处理层进行相同的特征工程，并使用 Keras Tuner 对 Keras 中的神经网络模型进行超参数调整。最后，你还学会了如何在 BigQuery
    ML 中使用 SQL 执行这些相同的任务。
- en: This chapter and the previous chapter on custom code models hopefully have given
    you a taste of what is available for building ML models. No-code and low-code
    solutions are at the very least a great starting point and very well may get you
    to your goal without having to write custom code. But you do not need to be a
    data scientist to explore with custom code, nor does it involve writing hundreds
    and hundreds of lines of code.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和前一章关于自定义代码模型的内容，希望能为你展示构建 ML 模型的可能性。无代码和低代码解决方案至少是一个很好的起点，而且很可能可以帮助你达到目标，而无需编写自定义代码。但是，你并不需要成为数据科学家来探索自定义代码，也不必编写成百上千行的代码。
- en: In the next and final chapter, you will learn about some next steps you can
    take if you want to go deeper into ML. You have already developed a very powerful
    toolkit throughout this book, but the field is ever growing, and a lot of the
    new tools and developments are available to more than just researchers in the
    field.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的最后一章中，你将了解如果想要更深入地学习 ML，可以采取的一些下一步措施。在本书中，你已经建立了一个非常强大的工具包，但是这个领域正在不断发展，很多新工具和发展已经不仅仅是研究人员的专利。
- en: '^([1](ch08.html#ch01fn3-marker)) For example, see Y. Gal and Z. Ghahramani,
    “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning”
    (Proceedings of the 33rd International Conference on Machine Learning, 2016).'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch08.html#ch01fn3-marker)) 例如，参见 Y. Gal 和 Z. Ghahramani, “Dropout as
    a Bayesian Approximation: Representing Model Uncertainty in Deep Learning”（第33届国际机器学习会议论文集，2016年）。'
