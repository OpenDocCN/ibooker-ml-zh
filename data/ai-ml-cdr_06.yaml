- en: Chapter 4\. Using Public Datasets with TensorFlow Datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。使用TensorFlow Datasets进行公共数据集
- en: In the first chapters of this book you trained models using a variety of data,
    from the Fashion MNIST dataset that is conveniently bundled with Keras to the
    image-based Horses or Humans and Dogs vs. Cats datasets, which were available
    as ZIP files that you had to download and preprocess. You’ve probably already
    realized that there are lots of different ways of getting the data with which
    to train a model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，你使用了各种数据来训练模型，从与Keras捆绑的时尚MNIST数据集，到基于图像的Horses or Humans和Dogs vs.
    Cats数据集，后者作为ZIP文件提供，你需要下载并预处理。你可能已经意识到，获取数据来训练模型有很多不同的方式。
- en: However, many public datasets require you to learn lots of different domain-specific
    skills before you begin to consider your model architecture. The goal behind TensorFlow
    Datasets (TFDS) is to expose datasets in a way that’s easy to consume, where all
    the preprocessing steps of acquiring the data and getting it into TensorFlow-friendly
    APIs are done for you.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多公共数据集在你开始考虑模型架构之前，需要你掌握许多不同的领域特定技能。TensorFlow Datasets（TFDS）的目标是以易于消费的方式暴露数据集，其中包括获取数据和将其转换为TensorFlow友好API的所有预处理步骤。
- en: 'You’ve already seen a little of this idea with how Keras handled Fashion MNIST
    back in Chapters [1](ch01.xhtml#introduction_to_tensorflow) and [2](ch02.xhtml#introduction_to_computer_vision).
    As a recap, all you had to do to get the data was this:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经在第[1](ch01.xhtml#introduction_to_tensorflow)章和第[2](ch02.xhtml#introduction_to_computer_vision)章中稍微了解了Keras如何处理Fashion
    MNIST的这个想法。回顾一下，你只需要这样做就能获取数据：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'TFDS builds on this idea, but greatly expands not only the number of datasets
    available but the diversity of dataset types. The [list of available datasets](https://oreil.ly/zL7zq)
    is growing all the time, in categories such as:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: TFDS基于这一理念，不仅极大地扩展了可用数据集的数量，还增加了数据集类型的多样性。[可用数据集列表](https://oreil.ly/zL7zq)不断增长，涵盖以下类别：
- en: Audio
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 音频
- en: Speech and music data
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 语音和音乐数据
- en: Image
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图像
- en: From simple learning datasets like Horses or Humans up to advanced research
    datasets for uses such as diabetic retinopathy detection
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从简单的学习数据集（如Horses or Humans）到用于糖尿病视网膜病变检测等高级研究数据集
- en: Object detection
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测
- en: COCO, Open Images, and more
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: COCO、Open Images等
- en: Structured data
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据
- en: Titanic survivors, Amazon reviews, and more
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 泰坦尼克号幸存者、亚马逊评论等
- en: Summarization
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: News from CNN and the *Daily Mail*, scientific papers, wikiHow, and more
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: CNN和*每日邮报*的新闻、科学论文、wikiHow等
- en: Text
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 文本
- en: IMDb reviews, natural language questions, and more
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: IMDb评论、自然语言问题等
- en: Translate
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译
- en: Various translation training datasets
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 各种翻译训练数据集
- en: Video
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 视频
- en: Moving MNIST, Starcraft, and more
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Moving MNIST、Starcraft等
- en: Note
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: TensorFlow Datasets is a separate install from TensorFlow, so be sure to install
    it before trying out any samples! If you are using Google Colab, it’s already
    preinstalled.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Datasets与TensorFlow是分开安装的，请务必在尝试任何样本之前安装它！如果您使用Google Colab，它已经预安装。
- en: This chapter will introduce you to TFDS and how you can use it to greatly simplify
    the training process. We’ll explore the underlying TFRecord structure and how
    it can provide commonality regardless of the type of the underlying data. You’ll
    also learn about the Extract-Transform-Load (ETL) pattern using TFDS, which can
    be used to train models with huge amounts of data efficiently.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍TFDS以及如何使用它来大大简化训练过程。我们将探讨基础的TFRecord结构以及它如何提供无论底层数据类型如何都通用的功能。您还将了解如何使用TFDS进行提取-转换-加载（ETL）模式，这可以有效地训练大量数据的模型。
- en: Getting Started with TFDS
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用TFDS
- en: Let’s go through some simple examples of how to use TFDS to illustrate how it
    gives us a standard interface to our data, regardless of data type.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些简单的示例来演示如何使用TFDS，以说明它如何为我们提供数据的标准接口，无论数据类型如何。
- en: 'If you need to install it, you can do so with a `pip` command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要安装，可以使用`pip`命令：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once it’s installed, you can use it to get access to a dataset with `tfds.load`,
    passing it the name of the desired dataset. For example, if you want to use Fashion
    MNIST, you can use code like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，您可以使用`tfds.load`访问数据集，只需传递所需数据集的名称。例如，如果您想使用Fashion MNIST，可以使用以下代码：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Be sure to inspect the data type that you get in return from the `tfds.load`
    command—the output from printing the items will be the different splits that are
    natively available in the data. In this case it’s a dictionary containing two
    strings, `test` and `train`. These are the available splits.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 确保检查从 `tfds.load` 命令返回的数据类型——打印项目的输出将是数据中本地可用的不同分割。在这种情况下，它是一个包含两个字符串 `test`
    和 `train` 的字典。这些是可用的分割。
- en: 'If you want to load these splits into a dataset containing the actual data,
    you can simply specify the split you want in the `tfds.load` command, like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想将这些分割加载到包含实际数据的数据集中，您可以简单地在 `tfds.load` 命令中指定您想要的分割，就像这样：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this instance, you’ll see that the output is a `DatasetAdapter`, which you
    can iterate through to inspect the data. One nice feature of this adapter is you
    can simply call `take(1)` to get the first record. Let’s do that to inspect what
    the data looks like:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，您将看到输出是一个 `DatasetAdapter`，您可以通过迭代来检查数据。这个适配器的一个很好的特性是，您可以简单地调用 `take(1)`
    来获取第一条记录。让我们这样做来检查数据的样子：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output from the first `print` will show that the type of item in each record
    is a dictionary. When we print the keys to that we’ll see that in this image set
    the types are `image` and `label`. So, if we want to inspect a value in the dataset,
    we can do something like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个 `print` 的输出将显示每条记录中项目的类型是一个字典。当我们打印这些键时，我们将看到在这个图像集中类型是 `image` 和 `label`。因此，如果我们想要检查数据集中的一个值，我们可以做如下操作：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You’ll see the output for the image is a 28 × 28 array of values (in a `tf.Tensor`)
    from 0–255 representing the pixel intensity. The label will be output as `tf.Tensor(2,
    shape=(), dtype=int64)`, indicating that this image is class 2 in the dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到图像的输出是一个 28 × 28 的值数组（在 `tf.Tensor` 中）从 0 到 255，表示像素强度。标签将以 `tf.Tensor(2,
    shape=(), dtype=int64)` 的形式输出，表明这个图像在数据集中属于类别 2。
- en: 'Data about the dataset is also available using the `with_info` parameter when
    loading the dataset, like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载数据集时，还可以使用 `with_info` 参数获取关于数据集的信息，如下所示：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Printing the info will give you details about the contents of the dataset.
    For example, for Fashion MNIST, you’ll see output like this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 打印信息将提供有关数据集内容的详细信息。例如，对于 Fashion MNIST，您将看到类似于以下输出：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Within this you can see details such as the splits (as demonstrated earlier)
    and the features within the dataset, as well as extra information like the citation,
    description, and dataset version.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，您可以查看诸如分割（如前所示）和数据集中的特征以及额外信息（如引用、描述和数据集版本）的详细信息。
- en: Using TFDS with Keras Models
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TFDS 与 Keras 模型
- en: 'In [Chapter 2](ch02.xhtml#introduction_to_computer_vision) you saw how to create
    a simple computer vision model using TensorFlow and Keras, with the built-in datasets
    from Keras (including Fashion MNIST), using simple code like this:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 2 章](ch02.xhtml#introduction_to_computer_vision) 中，您看到如何使用 TensorFlow 和
    Keras 创建一个简单的计算机视觉模型，使用了来自 Keras 的内置数据集（包括 Fashion MNIST），并且使用了如下简单的代码：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When using TFDS the code is very similar, but with some minor changes. The
    Keras datasets gave us `ndarray` types that worked natively in `model.fit`, but
    with TFDS we’ll need to do a little conversion work:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 TFDS 时，代码非常相似，但有一些小的改变。Keras 数据集给了我们在 `model.fit` 中可以直接使用的 `ndarray` 类型，但是在
    TFDS 中，我们需要做一些转换工作：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this case we use `tfds.load`, passing it `fashion_mnist` as the desired dataset.
    We know that it has train and test splits, so passing these in an array will return
    us an array of dataset adapters with the images and labels in them. Using `tfds.as_numpy`
    in the call to `tfds.load` causes them to be returned as Numpy arrays. Specifying
    `batch_size=-1` gives us *all* of the data, and `as_supervised=True` ensures we
    get tuples of (input, label) returned.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用 `tfds.load`，将 `fashion_mnist` 作为所需数据集传递。我们知道它有训练和测试分割，因此将它们作为数组传递将返回包含图像和标签的数据集适配器数组。在调用
    `tfds.load` 中使用 `tfds.as_numpy` 会将它们作为 Numpy 数组返回。指定 `batch_size=-1` 给我们返回*所有*数据，而
    `as_supervised=True` 确保我们得到返回的元组（输入、标签）。
- en: Once we’ve done that, we have pretty much the same format of data that was available
    in the Keras datasets, with one modification—the shape in TFDS is (28, 28, 1),
    whereas in the Keras datasets it was (28, 28).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了这些更改，我们基本上就获得了与 Keras 数据集中相同的数据格式，只有一点不同——TFDS 中的形状是 (28, 28, 1)，而在 Keras
    数据集中是 (28, 28)。
- en: 'This means the code needs to change a little to specify that the input data
    shape is (28, 28, 1) instead of (28, 28):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着代码需要做一些更改，以明确指定输入数据的形状为 (28, 28, 1)，而不是 (28, 28)：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'For a more complex example, you can take a look at the Horses or Humans dataset
    used in [Chapter 3](ch03.xhtml#going_beyond_the_basics_detecting_featu). This
    is also available in TFDS. Here’s the complete code to train a model with it:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的例子，你可以查看在[第三章](ch03.xhtml#going_beyond_the_basics_detecting_featu)中使用的
    Horses or Humans 数据集。这也可以在 TFDS 中找到。这是用它来训练模型的完整代码：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can see, it’s pretty straightforward: simply call `tfds.load`, passing
    it the split that you want (in this case `train`), and use that in the model.
    The data is batched and shuffled to make training more effective.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这非常简单：只需调用 `tfds.load`，传递你想要的分割（在本例中为 `train`），然后在模型中使用它。数据被批处理和洗牌以使训练更有效。
- en: 'The Horses or Humans dataset is split into training and test sets, so if you
    want to do validation of your model while training, you can do so by loading a
    separate validation set from TFDS like this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Horses or Humans 数据集被分成了训练集和测试集，因此如果你想在训练时验证模型，你可以像这样从 TFDS 加载一个单独的验证集：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You’ll need to batch it, the same as you did for the training set. For example:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要对其进行分批处理，就像对训练集做的那样。例如：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, when training, you specify the validation data as these batches. You
    have to explicitly set the number of validation steps to use per epoch too, or
    TensorFlow will throw an error. If you’re not sure, just set it to `1` like this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在训练时，将验证数据指定为这些批次。你还必须显式地设置每个 epoch 使用的验证步数，否则 TensorFlow 将抛出错误。如果不确定，就像这样设置为
    `1`：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Loading Specific Versions
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载特定版本
- en: All datasets stored in TFDS use a *MAJOR.MINOR.PATCH* numbering system. The
    guarantees of this system are as follows. If *PATCH* is updated, then the data
    returned by a call is identical, but the underlying organization may have changed.
    Any changes should be invisible to developers. If *MINOR* is updated, then the
    data is still unchanged, with the exception that there may be additional features
    in each record (nonbreaking changes). Also, for any particular slice (see [“Using
    Custom Splits”](#using_custom_splits)) the data will be the same, so records aren’t
    reordered. If *MAJOR* is updated, then there may be changes in the format of the
    records and their placement, so that particular slices may return different values.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所有存储在 TFDS 中的数据集都使用 *MAJOR.MINOR.PATCH* 编号系统。此系统的保证如下。如果更新 *PATCH*，则调用返回的数据相同，但底层组织可能已更改。任何更改对开发者来说都是不可见的。如果更新
    *MINOR*，则数据仍然不变，除了每条记录可能有额外的特征（非破坏性更改）。另外，对于任何特定的切片（见[“使用自定义拆分”](#using_custom_splits)），数据将保持不变，因此记录不会重新排序。如果更新
    *MAJOR*，则记录的格式及其位置可能会有所更改，因此特定的切片可能会返回不同的值。
- en: 'When you inspect datasets, you will see when there are different versions available—for
    example, this is the case for the [`cnn_dailymail` dataset](https://oreil.ly/673CJ).
    If you don’t want the default one, which at time of writing was 3.0.0, and instead
    want an earlier one, such as 1.0.0, you can simply load it like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当你检查数据集时，你会看到不同版本的可用性，例如，[`cnn_dailymail` 数据集](https://oreil.ly/673CJ) 就是一个例子。如果你不想使用默认的版本，在撰写本文时版本为
    3.0.0，而是想使用早期的版本，比如 1.0.0，你可以像这样加载：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that if you are using Colab, it’s always a good idea to check the version
    of TFDS that it uses. At time of writing, Colab was preconfigured for TFDS 2.0,
    but there are some bugs in loading datasets (including the `cnn_dailymail` one)
    that have been fixed in TFDS 2.1 and later, so be sure to use one of those versions,
    or at least install them into Colab, instead of relying on the built-in default.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果你在使用 Colab，检查它所使用的 TFDS 版本总是一个好主意。在撰写本文时，Colab 预配置为 TFDS 2.0，但加载数据集时（包括
    `cnn_dailymail` 数据集）可能会出现一些错误，在 TFDS 2.1 及更高版本中已修复了这些问题，因此请务必使用其中一个版本，或者至少安装它们到
    Colab，而不要依赖内置的默认设置。
- en: Using Mapping Functions for Augmentation
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用映射函数进行增强
- en: In [Chapter 3](ch03.xhtml#going_beyond_the_basics_detecting_featu) you saw the
    useful augmentation tools that were available when using an `ImageDataGenerator`
    to provide the training data for your model. You may be wondering how you might
    achieve the same when using TFDS, as you aren’t flowing the images from a subdirectory
    like before. The best way to achieve this—or indeed any other form of transformation—is
    to use a mapping function on the data adapter. Let’s take a look at how to do
    that.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.xhtml#going_beyond_the_basics_detecting_featu)中，你看到了使用 `ImageDataGenerator`
    提供模型训练数据时可用的有用增强工具。也许你会想知道在使用 TFDS 时如何实现同样的效果，因为你不再像以前那样从子目录流动图像。实现这一点——或者任何其他形式的转换——的最佳方式是在数据适配器上使用映射函数。让我们看看如何做到这一点。
- en: 'Earlier, with our Horses or Humans data, we simply loaded the data from TFDS
    and created batches for it like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们通过TFDS加载了Horses or Humans数据，并像这样为其创建了批次：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To do transforms and have them mapped to the dataset, you can create a *mapping
    function*. This is just standard Python code. For example, suppose you create
    a function called `augmentimages` and have it do some image augmentation, like
    this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行变换并将其映射到数据集，你可以创建一个*映射函数*。这只是标准的Python代码。例如，假设你创建了一个名为`augmentimages`的函数，并让它进行一些图像增强，就像这样：
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can then map this to the data to create a new dataset called `train`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以将其映射到数据上，创建一个名为`train`的新数据集：
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then when you create the batches, do this from `train` instead of from `data`,
    like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在创建批次时，从`train`而不是`data`中进行，像这样操作：
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You can see in the `augmentimages` function that there is a random flip left
    or right of the image, done using `tf.image.random_flip_left_right(image)`. There
    are lots of functions in the `tf.image` library that you can use for augmentation;
    see the [documentation](https://oreil.ly/H5LZh) for details.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到在`augmentimages`函数中有一个图像左右随机翻转的操作，使用的是`tf.image.random_flip_left_right(image)`。`tf.image`库中有许多函数可用于增强；详细信息请参阅[文档](https://oreil.ly/H5LZh)。
- en: Using TensorFlow Addons
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow插件
- en: The [TensorFlow Addons](https://oreil.ly/iwDv9) library contains even more functions
    that you can use. Some of the functions in the `ImageDataGenerator` augmentation
    (such as `rotate`) can only be found there, so it’s a good idea to check it out.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[TensorFlow插件](https://oreil.ly/iwDv9)库包含更多你可以使用的函数。某些函数在`ImageDataGenerator`增强中（例如`rotate`）只能在这里找到，因此查看一下是个好主意。'
- en: 'Using TensorFlow Addons is pretty easy—you simply install the library with:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow插件非常简单——你只需用以下命令安装该库：
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once that’s done, you can mix the addons into your mapping function. Here’s
    an example where the `rotate` addon is used in the mapping function from earlier:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，你可以将插件混合到你的映射函数中。以下是一个例子，展示了从先前映射函数中使用`rotate`插件的情况：
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Using Custom Splits
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义分片
- en: Up to this point, all of the data you’ve been using to build models has been
    presplit into training and test sets for you. For example, with Fashion MNIST
    you had 60,000 and 10,000 records, respectively. But what if you don’t want to
    use those splits? What if you want to split the data yourself according to your
    own needs? That’s one of the aspects of TFDS that’s really powerful—it comes complete
    with an API that gives you fine, granular control over how you split your data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你用来构建模型的所有数据都已经预先分为训练集和测试集。例如，对于Fashion MNIST，分别有60,000条和10,000条记录。但如果你不想使用这些分割怎么办？如果你想根据自己的需要分割数据怎么办？这就是TFDS的一个非常强大的方面之一——它配备了一个API，可以精细地控制你如何分割数据。
- en: 'You’ve actually seen it already when loading data like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当你像这样加载数据时，你已经见过它：
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Note that the `split` parameter is a string, and in this case you’re asking
    for the `train` split, which happens to be the entire dataset. If you’re familiar
    with [Python slice notation](https://oreil.ly/Enqzq), you can use that as well.
    This notation can be summarized as defining your desired slices within square
    brackets like this: `[<start>: <stop>: <step>]`. It’s quite a sophisticated syntax
    giving you great flexibility.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '注意`split`参数是一个字符串，在这种情况下，你要求的是`train`分片，这恰好是整个数据集。如果你熟悉[Python切片表示法](https://oreil.ly/Enqzq)，你也可以使用它。这种表示法可以总结为在方括号内定义你所需的切片，如下所示：`[<start>:
    <stop>: <step>]`。这是一种非常复杂的语法，为你提供了极大的灵活性。'
- en: 'For example, if you want the first 10,000 records of `train` to be your training
    data, you can omit `<start>` and just call for `train[:10000]` (a useful mnemonic
    is to read the leading colon as “the first,” so this would read “train the first
    10,000 records”):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想要`train`的前10,000条记录作为训练数据，你可以省略`<start>`，只需调用`train[:10000]`（一个有用的助记法是将前导冒号读作“第一个”，因此这将读作“训练前10,000条记录”）：
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can also use `%` to specify the split. For example, if you want the first
    20% of the records to be used for training, you could use `:20%` like this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用`%`来指定分片。例如，如果你想要使用前20%的记录进行训练，可以像这样使用`:20%`：
- en: '[PRE24]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You could even get a little crazy and combine splits. That is, if you want
    your training data to be a combination of the first and last thousand records,
    you could do the following (where `-1000:` means “the last 1,000 records” and
    `:1000` means “the first 1,000 records”):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以有些疯狂，将这些分割方法结合起来。也就是说，如果你希望你的训练数据是前一千条记录和最后一千条记录的组合，你可以这样做（其中 `-1000:`
    表示“最后1,000条记录”，`:1000` 表示“前1,000条记录”）：
- en: '[PRE25]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The Dogs vs. Cats dataset doesn’t have fixed training, test, and validation
    splits, but, with TFDS, creating your own is simple. Suppose you want the split
    to be 80%, 10%, 10%. You could create the three sets like this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Dogs vs. Cats数据集没有固定的训练、测试和验证分割，但是使用TFDS，创建自己的数据集非常简单。假设你希望分割为80%，10%，10%。你可以像这样创建三个集合：
- en: '[PRE26]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Once you have them, you can use them as you would any named split.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了它们，你可以像任何命名分割一样使用它们。
- en: 'One caveat is that because the datasets that are returned can’t be interrogated
    for length, it’s often difficult to check that you have split the original set
    correctly. To see how many records you have in a split, you have to iterate through
    the whole set and count them one by one. Here’s the code to do that for the training
    set you just created:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一个注意事项是，由于返回的数据集不能被用来查询其长度，通常很难检查你是否正确分割了原始集合。要查看分割中有多少条记录，你必须遍历整个集合并逐个计数。这里是你刚刚创建的训练集的代码：
- en: '[PRE27]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This can be a slow process, so be sure to use it only when you’re debugging!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个缓慢的过程，所以只有在调试时才使用它！
- en: Understanding TFRecord
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解TFRecord
- en: When you’re using TFDS, your data is downloaded and cached to disk so that you
    don’t need to download it each time you use it. TFDS uses the TFRecord format
    for caching. If you watch closely as it’s downloading the data you’ll see this—for
    example, [Figure 4-1](#downloading_the_cnn_dailymail_dataset_a) shows how the
    `cnn_dailymail` dataset is downloaded, shuffled, and written to a TFRecord file.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用TFDS时，你的数据被下载并缓存在磁盘上，这样每次使用时就不需要重新下载。TFDS使用TFRecord格式进行缓存。如果你仔细观察下载数据的过程，你将看到这一点——例如，[图4-1](#downloading_the_cnn_dailymail_dataset_a)展示了如何下载、洗牌并将`cnn_dailymail`数据集写入TFRecord文件。
- en: '![Downloading the cnn_dailymail dataset as a TFRecord file](Images/aiml_0401.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![作为TFRecord文件下载cnn_dailymail数据集](Images/aiml_0401.png)'
- en: Figure 4-1\. Downloading the cnn_dailymail dataset as a TFRecord file
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1\. 作为TFRecord文件下载cnn_dailymail数据集
- en: This is the preferred format in TensorFlow for storing and retrieving large
    amounts of data. It’s a very simple file structure, read sequentially for better
    performance. On disk the file is pretty straightforward, with each record consisting
    of an integer indicating the length of the record, a cyclic redundancy check (CRC)
    of that, a byte array of the data, and a CRC of that byte array. The records are
    concatenated into the file and then sharded in the case of large datasets.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是TensorFlow中存储和检索大量数据的首选格式。它是一个非常简单的文件结构，顺序读取以获得更好的性能。在磁盘上，文件的结构非常直接，每个记录由一个表示记录长度的整数、其循环冗余检查（CRC）、数据的字节数组和该字节数组的CRC组成。记录被连接成文件，然后在大型数据集的情况下进行分片。
- en: For example, [Figure 4-2](#inspecting_the_tfrecords_for_cnn_dailym) shows how
    the training set from `cnn_dailymail` is sharded into 16 files after download.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[图4-2](#inspecting_the_tfrecords_for_cnn_dailym)展示了从`cnn_dailymail`下载后如何将训练集分片成16个文件。
- en: 'To take a look at a simpler example, download the MNIST dataset and print its
    info:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看一个更简单的例子，下载MNIST数据集并打印其信息：
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Within the info you’ll see that its features are stored like this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息中，你将看到其特征存储如下：
- en: '[PRE29]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Similar to the CNN/DailyMail example, the file is downloaded to */root/tensorflow_datasets/mnist/<version>/files*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于CNN/DailyMail的例子，该文件下载到*/root/tensorflow_datasets/mnist/<version>/files*。
- en: 'You can load the raw records as a `TFRecordDataset` like this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像这样加载原始记录作为`TFRecordDataset`：
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that your filename location may be different depending on your operating
    system.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，根据您的操作系统，文件名的位置可能会有所不同。
- en: '![Inspecting the TFRecords for cnn_dailymail](Images/aiml_0402.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![检查cnn_dailymail的TFRecords](Images/aiml_0402.png)'
- en: Figure 4-2\. Inspecting the TFRecords for cnn_dailymail
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. 检查cnn_dailymail的TFRecords
- en: 'This will print out the raw contents of the record, like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出记录的原始内容，就像这样：
- en: '[PRE31]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'It’s a long string containing the details of the record, along with checksums,
    etc. But if we already know the features, we can create a feature description
    and use this to parse the data. Here’s the code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含记录详细信息及校验和等内容的长字符串。但如果我们已经了解了特征，我们可以创建一个特征描述，并用它来解析数据。以下是代码：
- en: '[PRE32]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output of this is a little friendlier! First of all, you can see that the
    image is a `Tensor`, and that it contains a PNG. PNG is a compressed image format
    with a header defined by `IHDR` and the image data between `IDAT` and `IEND`.
    If you look closely, you can see them in the byte stream. There’s also the label,
    stored as an `int` and containing the value `2`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出更加友好！首先，您可以看到图像是一个`Tensor`，它包含一个PNG格式的图像。PNG是一种压缩图像格式，其头部由`IHDR`定义，图像数据位于`IDAT`和`IEND`之间。如果仔细观察字节流，您也可以看到它们。还有存储为`int`类型并包含值`2`的标签：
- en: '[PRE33]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: At this point you can read the raw TFRecord and decode it as a PNG using a PNG
    decoder library like Pillow.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您可以读取原始的TFRecord并使用像Pillow这样的PNG解码器库将其解码为PNG。
- en: The ETL Process for Managing Data in TensorFlow
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中管理数据的ETL过程
- en: ETL is the core pattern that TensorFlow uses for training, regardless of scale.
    We’ve been exploring small-scale, single-computer model building in this book,
    but the same technology can be used for large-scale training across multiple machines
    with massive datasets.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ETL是TensorFlow在训练中使用的核心模式，无论规模如何。在本书中，我们一直在探索小规模、单机模型构建，但相同的技术也可以用于跨多台机器进行大规模训练，使用海量数据集。
- en: The *Extract phase* of the ETL process is when the raw data is loaded from wherever
    it is stored and prepared in a way that can be transformed. The *Transform* phase
    is when the data is manipulated in a way that makes it suitable or improved for
    training. For example, batching, image augmentation, mapping to feature columns,
    and other such logic applied to the data can be considered part of this phase.
    The *Load* phase is when the data is loaded into the neural network for training.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*提取阶段* 是ETL过程的一部分，当原始数据从存储位置加载并准备好可以转换的方式时。*转换* 阶段是当数据以适合或改进用于训练的方式进行操作时。例如，批处理、图像增强、映射到特征列等逻辑可以被视为此阶段的一部分。*加载*
    阶段是当数据加载到神经网络进行训练时。'
- en: 'Consider the full code to train the Horses or Humans classifier, shown here.
    I’ve added comments to show where the Extract, Transform, and Load phases take
    place:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到训练马匹或人类分类器的完整代码，如下所示。我已经添加了注释，显示了提取、转换和加载阶段的位置：
- en: '[PRE34]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Using this process can make your data pipelines less susceptible to changes
    in the data and the underlying schema. When you use TFDS to extract data, the
    same underlying structure is used regardless of whether the data is small enough
    to fit in memory, or so large that it cannot be contained even on a simple machine.
    The `tf.data` APIs for transformation are also consistent, so you can use similar
    ones regardless of the underlying data source. And, of course, once it’s transformed,
    the process of loading the data is also consistent whether you are training on
    a single CPU, a GPU, a cluster of GPUs, or even pods of TPUs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个过程可以使您的数据管道对数据和底层架构的变化更不易受到影响。当您使用TFDS提取数据时，不管数据是小到可以放入内存，还是大到简单的机器无法容纳，都会使用相同的底层结构。`tf.data`转换的API也是一致的，因此您可以使用类似的API，无论底层数据源如何。当然，一旦转换完成，加载数据的过程也是一致的，无论您是在单个CPU、GPU、GPU集群甚至TPU
    Pod上进行训练。
- en: How you load the data, however, can have a huge impact on your training speed.
    Let’s take a look at that next.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如何加载数据会对您的训练速度产生巨大影响。让我们接下来看看这一点。
- en: Optimizing the Load Phase
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化加载阶段
- en: Let’s take a closer look at the Extract-Transform-Load process when training
    a model. We can consider the extraction and transformation of the data to be possible
    on any processor, including a CPU. In fact, the code used in these phases to perform
    tasks like downloading data, unzipping it, and going through it record by record
    and processing them is not what GPUs or TPUs are built for, so this code will
    likely execute on the CPU anyway. When it comes to training, however, you can
    get great benefits from a GPU or TPU, so it makes sense to use one for this phase
    if possible. Thus, in the situation where a GPU or TPU is available to you, you
    should ideally split the workload between the CPU and the GPU/TPU, with Extract
    and Transform taking place on the CPU, and Load taking place on the GPU/TPU.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看一下在训练模型时的提取-转换-加载过程。我们可以考虑数据的提取和转换可以在任何处理器上完成，包括 CPU。实际上，在这些阶段使用的代码执行任务，如下载数据、解压缩以及逐条记录地处理它们，不是
    GPU 或 TPU 的用途，因此这些代码可能最终还是在 CPU 上执行。然而，当涉及到训练时，你可以从 GPU 或 TPU 中获得很大的好处，因此如果可能的话，在这个阶段使用
    GPU 或 TPU 是有意义的。因此，在你可以使用 GPU 或 TPU 的情况下，最好将工作负载分配到 CPU 和 GPU/TPU 之间，提取和转换在 CPU
    上进行，加载在 GPU/TPU 上进行。
- en: Suppose you’re working with a large dataset. Assuming it’s so large that you
    have to prepare the data (i.e., do the extraction and transformation) in batches,
    you’ll end up with a situation like that shown in [Figure 4-3](#training_on_a_cpusolidusgpu).
    While the first batch is being prepared, the GPU/TPU is idle. When that batch
    is ready it can be sent to the GPU/TPU for training, but now the CPU is idle until
    the training is done, when it can start preparing the second batch. There’s a
    lot of idle time here, so we can see that there’s room for optimization.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在处理一个大型数据集。假设数据集如此之大，以至于你必须分批准备数据（即进行提取和转换），你将会得到类似于在 [图 4-3](#training_on_a_cpusolidusgpu)
    中展示的情况。在准备第一批数据时，GPU/TPU 是空闲的。当第一批数据准备好后，可以将其发送到 GPU/TPU 进行训练，但此时 CPU 则处于空闲状态，直到训练完成，然后才能开始准备第二批数据。这里存在大量的空闲时间，因此我们可以看到这里有优化的空间。
- en: '![Training on a CPU/GPU](Images/aiml_0403.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![在 CPU/GPU 上训练](Images/aiml_0403.png)'
- en: Figure 4-3\. Training on a CPU/GPU
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 在 CPU/GPU 上训练
- en: The logical solution is to do the work in parallel, preparing and training side
    by side. This process is called *pipelining* and is illustrated in [Figure 4-4](#pipelining).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑解决方案是并行进行工作，准备和训练并行进行。这个过程称为*管道化*，并且在 [图 4-4](#pipelining) 中有所示。
- en: '![Pipelining](Images/aiml_0404.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![管道化](Images/aiml_0404.png)'
- en: Figure 4-4\. Pipelining
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 管道化
- en: In this case, while the CPU prepares the first batch the GPU/TPU again has nothing
    to work on, so it’s idle. When the first batch is done, the GPU/TPU can start
    training—but in parallel with this, the CPU will prepare the second batch. Of
    course, the time it takes to train batch *n* – 1 and prepare batch *n* won’t always
    be the same. If the training time is faster, you’ll have periods of idle time
    on the GPU/TPU. If it’s slower, you’ll have periods of idle time on the CPU. Choosing
    the correct batch size can help you optimize here—and as GPU/TPU time is likely
    more expensive, you’ll probably want to reduce its idle time as much as possible.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，当 CPU 准备第一批数据时，GPU/TPU 再次没有工作可做，因此它是空闲的。当第一批数据准备好后，GPU/TPU 可以开始训练，但与此同时，CPU
    将准备第二批数据。当然，训练批次 *n* – 1 和准备批次 *n* 所需的时间不会总是相同。如果训练时间更快，您将在 GPU/TPU 上有空闲时间。如果更慢，则
    CPU 将有空闲时间。选择正确的批次大小可以帮助您在这里进行优化——而且由于 GPU/TPU 的时间可能更昂贵，您可能希望尽可能减少其空闲时间。
- en: 'You probably noticed when we moved from simple datasets like Fashion MNIST
    in Keras to using the TFDS versions that you had to batch them before you could
    train. This is why: the pipelining model is in place so that regardless of how
    large your dataset is, you’ll continue to use a consistent pattern for ETL on
    it.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从使用 Keras 中的简单数据集（如时尚 MNIST）转到使用 TFDS 版本时，您可能注意到必须在训练之前对它们进行批处理。这就是为什么：管道化模型被设计成无论数据集有多大，您都将继续使用其上的
    ETL 的一致模式。
- en: Parallelizing ETL to Improve Training Performance
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行化 ETL 以提高训练性能
- en: TensorFlow gives you all the APIs you need to parallelize the Extract and Transform
    process. Let’s explore what they look like using Dogs vs. Cats and the underlying
    TFRecord structures.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 为您提供了所有并行化提取和转换过程所需的 API。让我们使用狗 vs. 猫和底层的 TFRecord 结构来探索它们的样子。
- en: 'First, you use `tfds.load` to get the dataset:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您使用 `tfds.load` 来获取数据集：
- en: '[PRE35]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: If you want to use the underlying TFRecords, you’ll need to access the raw files
    that were downloaded. As the dataset is large, it’s sharded across a number of
    files (eight, in version 4.0.0).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用底层的 TFRecords，你需要访问下载的原始文件。由于数据集很大，在版本 4.0.0 中分成了多个文件（8 个）。
- en: 'You can create a list of these files and use `tf.Data.Dataset.list_files` to
    load them:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以创建这些文件的列表，并使用 `tf.Data.Dataset.list_files` 加载它们：
- en: '[PRE36]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Once you have the files, they can be loaded into a dataset using `files.interleave`
    like this:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了这些文件，可以使用 `files.interleave` 将它们加载到数据集中，就像这样：
- en: '[PRE37]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: There are a few new concepts here, so let’s take a moment to explore them.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些新概念，让我们花点时间来探索它们。
- en: The `cycle_length` parameter specifies the number of input elements that are
    processed concurrently. So, in a moment you’ll see the mapping function that decodes
    the records as they’re loaded from disk. Because `cycle_length` is set to `4`,
    this process will be handling four records at a time. If you don’t specify this
    value, then it will be derived from the number of available CPU cores.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`cycle_length` 参数指定同时处理的输入元素数量。因此，马上你将看到从磁盘加载时解码记录的映射函数。因为 `cycle_length` 设置为
    `4`，所以此过程将同时处理四条记录。如果不指定此值，则将根据可用 CPU 核心数来推导。'
- en: The `num_parallel_calls` parameter, when set, will specify the number of parallel
    calls to execute. Using `tf.data.experimental.AUTOTUNE`, as is done here, will
    make your code more portable because the value is set dynamically, based on the
    available CPUs. When combined with `cycle_length`, you’re setting the maximum
    degree of parallelism. So, for example, if `num_parallel_calls` is set to `6`
    after autotuning and `cycle_length` is `4`, you’ll have six separate threads,
    each loading four records at a time.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当设置 `num_parallel_calls` 参数时，将指定要执行的并行调用数。在这里使用 `tf.data.experimental.AUTOTUNE`
    会使你的代码更具可移植性，因为该值是动态设置的，根据可用的 CPU。与 `cycle_length` 结合使用时，你正在设置最大并行度。因此，例如，如果在自动调整后将
    `num_parallel_calls` 设置为 `6`，`cycle_length` 设置为 `4`，那么将有六个单独的线程，每个线程一次加载四条记录。
- en: 'Now that the Extract process is parallelized, let’s explore parallelizing the
    transformation of the data. First, create the mapping function that loads the
    raw TFRecord and converts it to usable content—for example, decoding a JPEG image
    into an image buffer:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在提取过程已经并行化，让我们来探索数据转换的并行化。首先，创建加载原始 TFRecord 并将其转换为可用内容的映射函数，例如将 JPEG 图像解码为图像缓冲区：
- en: '[PRE38]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As you can see, this is a typical mapping function without any specific work
    done to make it work in parallel. That will be done when we call the mapping function.
    Here’s how to do that:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这是一个典型的映射函数，没有做任何特定的工作来并行执行。在调用映射函数时，将会执行这些工作。这是如何做到的：
- en: '[PRE39]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: First, if you don’t want to autotune, you can use the `multiprocessing` library
    to get a count of your CPUs. Then, when you call the mapping function, you just
    pass this as the number of parallel calls that you want to make. It’s really as
    simple as that.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果你不想自动调整，可以使用 `multiprocessing` 库来获取你的 CPU 数量。然后，在调用映射函数时，将这个数字作为你想要进行的并行调用的数量传递进去。就是这么简单。
- en: The `cache` method will cache the dataset in memory. If you have a lot of RAM
    available this is a really useful speedup. Trying this in Colab with Dogs vs.
    Cats will likely crash your VM due to the dataset not fitting in RAM. After that,
    if available, the Colab infrastructure will give you a new, higher-RAM machine.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`cache` 方法将在内存中缓存数据集。如果你有大量的 RAM 可用，这将是一个非常有用的加速。尝试在 Colab 中使用 Dogs vs. Cats
    很可能会因为数据集不适合内存而导致虚拟机崩溃。此后，如果可用，Colab 基础设施将为你提供一个新的、更高 RAM 的机器。'
- en: 'Loading and training can also be parallelized. As well as shuffling and batching
    the data, you can prefetch based on the number of CPU cores that are available.
    Here’s the code:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 加载和训练也可以并行化。除了对数据进行洗牌和分批处理外，你还可以根据可用的 CPU 核心数量进行预取。以下是代码：
- en: '[PRE40]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Once your training set is all parallelized, you can train the model as before:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的训练集全部并行化，就可以像以前一样训练模型：
- en: '[PRE41]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: When I tried this in Google Colab, I found that this extra code to parallelize
    the ETL process reduced the training time to about 40 seconds per epoch, as opposed
    to 75 seconds without it. These simple changes cut my training time almost in
    half!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在 Google Colab 中尝试时，我发现这些额外的并行化 ETL 过程的代码将训练时间缩短到每个时期约 40 秒，而没有这些代码则为 75 秒。这些简单的更改几乎使我的训练时间减少了一半！
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced TensorFlow Datasets, a library that gives you access
    to a huge range of datasets, from small learning ones up to full-scale datasets
    used in research. You saw how they use a common API and common format to help
    reduce the amount of code you have to write to get access to data. You also saw
    how to use the ETL process, which is at the heart of the design of TFDS, and in
    particular we explored parallelizing the extraction, transformation, and loading
    of data to improve training performance. In the next chapter you’ll take what
    you’ve learned and start applying it to natural language processing problems.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 TensorFlow Datasets，这是一个库，为您提供了从小型学习数据集到用于研究的大规模数据集的访问权限。您看到它们如何使用通用的
    API 和通用格式来帮助减少您编写的代码量，以获取数据访问权限。您还学习了如何使用 ETL 过程，这是 TFDS 设计的核心，并特别探讨了并行化数据的提取、转换和加载，以改善训练性能。在下一章中，您将把学到的知识应用到自然语言处理问题中。
