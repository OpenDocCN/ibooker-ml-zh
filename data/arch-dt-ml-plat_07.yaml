- en: Chapter 7\. Converging to a Lakehouse
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。走向湖仓
- en: 'As you know by this point, there are two main approaches that organizations
    can take when designing their data platform: following a data lake or a DWH paradigm.
    Both approaches come with pros and cons, but the question is: is it possible to
    make both technologies coexist to have a convergent architecture? In this chapter
    we will explore this topic, starting with a brief motivation for this idea, and
    then we will analyze two broad variants of the convergent architecture—known as
    the *lakehouse architecture*—and help you decide how to choose between them.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你现在所知，组织在设计其数据平台时可以采取两种主要方法：遵循数据湖或数据仓库范式。这两种方法各有利弊，但问题是：是否可能使这两种技术共存以实现收敛架构？在本章中，我们将探讨这个主题，从这个想法的简要动机开始，然后分析收敛架构的两个广泛变体——即所谓的*湖仓架构*——并帮助您决定如何在它们之间做出选择。
- en: The lakehouse concept is becoming increasingly popular because it allows for
    a more flexible and scalable way to store and analyze structured, semistructured,
    and unstructured data at scale. A lakehouse can handle the entire lifecycle of
    structured and unstructured data, combining the best of the data lake and DWH
    approaches you learned in the previous two chapters in a governed manner. At the
    end of this chapter we will describe how to evolve toward the lakehouse architecture.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 湖仓（Lakehouse）概念因其在规模化处理结构化、半结构化和非结构化数据方面的灵活性和可扩展性而日益流行。湖仓可以以一种治理的方式处理整个结构化和非结构化数据的生命周期，结合了前两章中学到的数据湖和数据仓库方法的优点。本章末尾，我们将描述如何向湖仓架构演进。
- en: The Need for a Unique Architecture
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要独特架构
- en: Data lakes and DWHs emerged to meet the needs of different users. An organization
    with both types of users is faced with an unappealing choice.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖和数据仓库（Data Warehouse, DWH）应运而生，以满足不同用户的需求。一个同时拥有这两种类型用户的组织面临一个不那么吸引人的选择。
- en: User Personas
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户角色
- en: As you have learned in the previous chapters, some key differences between a
    data lake and a DWH are related to the type of data that can be ingested and the
    ability to land unprocessed (raw) data into a common location. Therefore, the
    typical user of these two paradigms is different.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在之前的章节中所学到的，数据湖和数据仓库之间一些关键差异与能够摄取的数据类型及将未处理（原始）数据落入通用位置的能力有关。因此，这两种范式的典型用户是不同的。
- en: Traditional DWH users are BI analysts who are closer to the business, focusing
    on deriving insights from data. Data is traditionally prepared by the ETL tools
    based on the requirements of the data analysts. These users are typically using
    the data to answer questions. They tend to be proficient in SQL.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的数据仓库用户是商业智能（BI）分析师，更接近业务，专注于从数据中提取洞察。数据通常由ETL工具根据数据分析师的要求准备。这些用户通常使用数据来回答问题。他们通常精通SQL。
- en: Data lake users, in addition to analysts, include data engineers and data scientists.
    They are closer to the raw data, with the tools and capabilities to explore and
    mine the data. They not only transform the data to make it accessible by the business
    (i.e., data that can be transferred to the DWHs) but also experiment with it and
    use it to train their ML models and for AI processing. These users not only find
    answers in the data, but they also find the questions that are relevant for the
    business and prepare the data to make it useful to other users. They tend to be
    proficient in a coding language such as Python, Java, or Scala.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖的用户除了分析师外，还包括数据工程师和数据科学家。他们更接近原始数据，拥有探索和挖掘数据的工具和能力。他们不仅仅是将数据转换为业务可访问的形式（即可传输到数据仓库的数据），还会对数据进行实验，并用于训练机器学习模型和进行人工智能处理。这些用户不仅仅在数据中找到答案，还能找到对业务相关的问题，并准备数据使其对其他用户有用。他们通常精通Python、Java或Scala等编程语言。
- en: 'Antipattern: Disconnected Systems'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反模式：断开的系统
- en: 'As a result of these different needs, we often see different IT departments
    or teams managing the DWH and the data lake. However, this split approach has
    an opportunity cost: organizations spend their resources on operational aspects
    rather than focusing on business insights. As such, they cannot allocate resources
    to focus on the key business drivers or on challenges that would allow them to
    gain a competitive edge.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些不同的需求，我们经常看到不同的IT部门或团队管理数据仓库和数据湖。然而，这种分裂式方法有一个机会成本：组织将资源花费在运营方面，而不是专注于业务洞察。因此，他们无法将资源分配到专注于关键业务驱动因素或允许他们获得竞争优势的挑战上。
- en: Additionally, maintaining two separate systems, both of which have the same
    end goal of providing actionable insights from data, can cause data quality and
    consistency problems. The extra effort required to transform data from one system
    to use it alongside data from the other may put off end users completely. This
    can also lead to *data puddles* across the enterprise, which are datasets stored
    on individual’s machines, causing both a security risk and inefficient use of
    data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，维护两个分离的系统，它们都有提供从数据中获取可操作洞见的相同最终目标，可能会导致数据质量和一致性问题。从一个系统转换数据以便与另一个系统中的数据一起使用所需的额外努力可能完全阻止最终用户。这也可能导致整个企业存在*数据水坑*，即存储在个人机器上的数据集，既造成安全风险又浪费数据效率。
- en: 'Antipattern: Duplicated Data'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反模式：重复数据
- en: Why not simply connect the two systems with data being synced by the platform
    team? You may end up with the architecture represented in [Figure 7-1](#data_lake_and_dwh_coexistence),
    where the data lake and the DWH coexist to enable self-service analytics and ML
    environments.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不简单地通过平台团队同步连接这两个系统？你可能最终会得到如[Figue 7-1](#data_lake_and_dwh_coexistence)所示的架构，数据湖和数据仓库共存，以支持自助式分析和ML环境。
- en: Note that in this architecture the data, which is *centrally managed* by the
    platform, is the one that resides within the data lake. All the other data (e.g.,
    DWH, BI/reporting tools, notebooks, etc.) can be considered a duplicated/transformed
    version of the original data that from one side helps with getting better performance
    and facilitating analysis (e.g., materialized views), but, as you have already
    seen in the previous chapters, it generates a lot of drawbacks because it leads
    to the generation of data silos.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在此架构中，由平台集中管理的数据就是驻留在数据湖中的数据。所有其他数据（例如数据仓库、BI/报告工具、笔记本等）可以被视为原始数据的重复/转换版本，一方面有助于提高性能和便于分析（例如物化视图），但正如前几章所见，这也会带来许多缺点，因为它导致数据孤立现象。
- en: '![Data lake and DWH coexistence](assets/adml_0701.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![数据湖与数据仓库共存](assets/adml_0701.png)'
- en: Figure 7-1\. Data lake and DWH coexistence
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 数据湖与数据仓库共存
- en: 'Let’s have a look at the main building blocks of the data journey:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看数据旅程的主要构建模块：
- en: Data from various sources (batch and/or streaming) is *collected* and *trans⁠formed/​wrangled/prepared*
    (even on the fly) to be *stored* in the data lake, where it will pass by all the
    different stages (bronze, silver, and gold). Since the beginning, the data is
    subject to security, data quality, and data governance checks.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自各种来源的数据（批处理和/或流式）被*收集*并*转换/整理/准备*（甚至实时处理），以便*存储*在数据湖中，数据会经过不同的阶段（铜、银和金）。从一开始，数据就会经历安全性、数据质量和数据治理检查。
- en: Data can now be *processed* via the *analytical engine* solutions (e.g., Spark)
    that interact with SQL clients, batch reporting, and notebooks.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在数据可以通过分析引擎解决方案（例如Spark）进行*处理*，与SQL客户端、批量报告和笔记本交互。
- en: The data in parallel can be transformed and ingested into business-specific
    segments in the DWH, the data marts, to handle BI workloads. Maintaining data
    marts is a complex and costly activity because it requires a lot of ETL engineering
    and it is challenging to keep all the needed data up to date. And this has to
    be done in a perpetual manner. Furthermore, all the data governance activities
    have to be repeated in the DWH.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，数据可以并行转换并纳入数据仓库中的业务特定段，即数据集市，以处理BI工作负载。维护数据集市是一项复杂且昂贵的活动，因为它需要大量的ETL工程，并且保持所有必要的数据更新是具有挑战性的。而且，这必须是一个持续不断的过程。此外，所有数据治理活动必须在数据仓库中重复执行。
- en: The DWH powers the BI tools and SQL clients, but sometimes it is not able to
    give the adequate level of performance requested by the final users, so people
    tend to download the data, create local copies, and work with that.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据仓库驱动BI工具和SQL客户端，但有时无法提供最终用户要求的充分性能水平，因此人们倾向于下载数据，创建本地副本并进行处理。
- en: ML solutions (e.g., scikit-learn, TensorFlow, Keras), generally handled via
    notebooks, can leverage the data coming from the data lake, but at the same time
    they can be connected with the DWH and the analytical engine.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ML解决方案（例如scikit-learn、TensorFlow、Keras），通常通过笔记本处理，可以利用来自数据湖的数据，同时可以与数据仓库和分析引擎连接。
- en: 'An architecture like this typically includes a series of drawbacks:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的架构通常包括一系列缺点：
- en: Proliferation of the data
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的扩散
- en: Data exists in various forms and, potentially, even outside the boundaries of
    the platform (i.e., local laptops), which may cause *security and compliance risks*,
    *data freshness issues,* or *data versioning issues*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以各种形式存在，甚至可能在平台边界之外（即本地笔记本电脑），这可能会引起*安全合规风险*，*数据新鲜度问题*或*数据版本问题*。
- en: Slowdown in time to market
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 时间到市场放缓
- en: Organizations may spend up to two weeks implementing a minor change to a report
    for management because they must first request that data engineers modify the
    ETLs in order to access the data necessary to complete the task.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 组织可能需要花费多达两周的时间来实施对管理报告的较小更改，因为他们必须首先请求数据工程师修改ETL以访问完成任务所需的数据。
- en: Limitation in data size
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据大小的限制
- en: Users may not have access to all the data they need to carry out their analysis
    because, for example, data marts in the DWH could include just a subset of the
    necessary info due to performance needs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可能无法访问他们进行分析所需的所有数据，因为例如，数据仓库中的数据集市可能仅包含由于性能需求而不全面的信息子集。
- en: Infrastructure and operational cost
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施和运营成本
- en: These could increase due to the complexity of ETLs that need to be put in place.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要实施的ETL的复杂性，这些可能会增加。
- en: It’s bad practice to have duplicated/transformed data like this, and you should
    minimize it as much as possible. Ideally, you will get rid of duplicates and self-managed
    data with the goal of having all the data in a platform-managed mode that is available
    to all of the actors for analysis (i.e., on the right side of the architecture
    diagram depicted in [Figure 7-1](#data_lake_and_dwh_coexistence)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有重复/转换的数据是不良实践，您应该尽可能将其最小化。理想情况下，您将消除重复数据和自管理数据，以便将所有数据置于平台管理模式中，所有参与者均可进行分析（即在[图7-1](#data_lake_and_dwh_coexistence)中所示的架构图的右侧）。
- en: 'At this point you may ask yourself why organizations are leveraging this kind
    of architecture. The reason for this is pretty straightforward: because, as recently
    as 2018, this was the only way to give to the many different users all the solutions
    needed to meet their needs.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 到此时，您可能会问自己为什么组织要利用这种架构。这个原因非常明显：因为直到2018年，这是为了给许多不同的用户提供满足其需求的所有解决方案的唯一途径。
- en: Technology evolves and, especially thanks to the advent of the cloud, new solutions
    have been developed to bring better possibilities in terms of data handling. Let’s
    have a look at how we can improve on this combined architecture.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 技术不断发展，特别是得益于云的出现，已经开发出新的解决方案，以提供更好的数据处理可能性。让我们看看如何改进这种组合架构。
- en: Converged Architecture
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 融合架构
- en: By this point, it’s clear that a full convergence between data lake and DWH
    can help final users get the most from their platform and hence from their data.
    But what is the architecture for such an end state, and what is the path to get
    there?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到此时，显而易见，数据湖与数据仓库的完全融合可以帮助最终用户充分利用其平台，从而充分利用其数据。但是，这种最终状态的架构是什么，以及达到这种状态的路径是什么？
- en: Two Forms
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两种形式
- en: The key factor in the converged lakehouse architecture is that the DWH and data
    lake share a common storage. Two different compute engines—a SQL engine (for DWH
    use cases) and a distributed programming engine (for data lake use cases)—read
    and process the data without moving it around.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 汇合湖架构的关键因素在于数据仓库（DWH）和数据湖共享共同存储。两种不同的计算引擎——SQL引擎（用于数据仓库用例）和分布式编程引擎（用于数据湖用例）——读取和处理数据而无需移动数据。
- en: 'This common storage can take one of two forms:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此共同存储可以采用两种形式之一：
- en: The data is stored in an open source format (Parquet, Avro, etc.) on cloud storage,
    and you can leverage Spark to process it. At the same time, you can use Spark
    SQL to provide interactive querying capability (the Databricks solution). In addition,
    DWH technologies (e.g., BigQuery, Athena, DuckDB, Snowflake, etc.) can support
    you in running SQL directly on the datasets without any data copying or movement.
    You can use open source formats combined with a technology such as Delta Lake
    or Apache Iceberg that improves the underlying performance.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储在云存储中以开源格式（Parquet，Avro等），您可以利用Spark来处理它。同时，您可以使用Spark SQL提供交互式查询功能（Databricks解决方案）。此外，数据仓库技术（例如BigQuery，Athena，DuckDB，Snowflake等）可以支持您直接在数据集上运行SQL，无需任何数据复制或移动。您可以结合使用开源格式和Delta
    Lake或Apache Iceberg这样的技术来提升底层性能。
- en: Alternatively, the common storage can be a highly optimized DWH format (e.g.,
    the native format within BigQuery, Snowflake, etc.) where you can of course natively
    leverage SQL. In addition, these DWHs support the use of compute engines such
    as Spark directly on the native data.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，通用存储可以是高度优化的DWH格式（例如BigQuery、Snowflake等内置格式），在这些DWH上，您当然可以原生地利用SQL。此外，这些DWH支持直接在原生数据上使用诸如Spark之类的计算引擎。
- en: Both of these are compromises. Just because the other type of workload is supported
    does not mean that it is of equal performance. A data lake is slower/costlier
    on SQL workloads than a data warehouse. And a data warehouse is slower/costlier
    on Spark and ML workloads than a data lake.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者都是妥协。支持其他类型的工作负载并不意味着其性能相等。在SQL工作负载上，数据湖比数据仓库更慢/更昂贵。而在Spark和ML工作负载上，数据仓库比数据湖更慢/更昂贵。
- en: Choose based on user skills
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据用户技能选择
- en: We recommend making the choice between the two forms based on who your primary
    users are. A SQL engine operating on cloud storage (the first option) loses many
    of the optimizations that make DWHs interactive and suitable for ad hoc querying.
    A data lake running SQL (the second option) loses the schema-free data processing
    capability that makes data lakes so flexible. So choose the form of hybrid architecture
    based on which type of user and workload you want to support very well and which
    one you want to support in a compromised way.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议根据主要用户类型进行选择。在云存储上运行SQL引擎（第一种选择）会失去许多使DWH互动并适合即席查询的优化。运行SQL的数据湖（第二种选择）会失去使数据湖如此灵活的无模式数据处理能力。因此，请根据要很好地支持的用户类型和工作负载以及要以折衷方式支持的用户类型选择混合架构的形式。
- en: At a very high level, the first option is best for users who are proficient
    programmers. Build a lakehouse with the data lake (i.e., cloud storage) as your
    storage if the majority of your users are programmers who do a lot of data wrangling
    in code, such as to write ETL pipelines and to train ML models. The main advantage
    of data lakes is that they permit flexible data processing by programmers. The
    second option is best for users who want to interact with the data to gain insights
    from it. Build a lakehouse with the DWH as your storage if the majority of your
    users are analysts rather than programmers, by which we mean that they can either
    write SQL or use a dashboard tool (like Tableau, Power BI, or Looker) that will
    generate SQL. The main advantage of DWHs is that they permit self-service, ad
    hoc querying by business users.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常高的层面上，第一种选择适合熟练的程序员用户。如果你的大多数用户是那些在代码中进行大量数据处理（例如编写ETL管道和训练ML模型）的程序员，那么建立一个以数据湖（即云存储）为存储的湖屋是最好的选择。数据湖的主要优势在于它允许程序员进行灵活的数据处理。第二种选择适合希望与数据交互以从中获取洞察的用户。如果你的大多数用户是分析师而不是程序员，也就是说他们可以编写SQL或使用类似Tableau、Power
    BI或Looker这样的仪表板工具来生成SQL，那么建立一个以DWH作为存储的湖屋是最好的选择。数据仓库的主要优势在于它允许商业用户进行自助式、即席查询。
- en: Complete evaluation criteria
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完成评估标准
- en: What we have discussed so far is just a preliminary attempt to identify the
    right approach for your organization. Other factors include the volume of data
    to be ingested, whether you need streaming, how much of your data is structured
    or semistructured, and whether you work in a regulated industry with heavy data
    governance needs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只是初步尝试确定您组织的正确方法。其他因素包括要摄入的数据量、是否需要流处理、数据中有多少是结构化或半结构化的，以及您是否在需要严格数据治理的受监管行业工作。
- en: You should list all the features of both data lake and DWH approaches discussed
    in Chapters [5](ch05.html#architecting_a_data_lake) and [6](ch06.html#innovating_with_an_enterprise_data_ware)
    and produce an evaluation matrix, assigning a score from 0 to 5 (0 being less
    important, 5 being mandatory) to each element. This will help you to have a more
    cohesive understanding of your needs and make the right choice for your lakehouse
    approach.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该列出在第[5](ch05.html#architecting_a_data_lake)章和第[6](ch06.html#innovating_with_an_enterprise_data_ware)章讨论的数据湖和DWH方法的所有特性，并制作一个评估矩阵，为每个元素分配从0到5的分数（0表示不重要，5表示必须）。这将帮助您更全面地理解您的需求，并为您的湖屋方案做出正确选择。
- en: Now that you have a better understanding of the different alternatives you can
    use to implement your lakehouse, let’s take a closer look at both options to better
    understand how they work behind the scenes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您对实施湖屋的不同选择有了更好的理解，让我们更近距离地了解这两个选项，以更好地理解它们在幕后的工作原理。
- en: Lakehouse on Cloud Storage
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云存储上的湖仓
- en: Separation of compute and storage provides the perfect habitat for a convergence
    of the data lake and DWH. The cloud provider is able to enable distributed applications
    for interactive queries by bringing compute to the storage. This allows the cloud
    provider to allocate storage and compute independently to an organization’s jobs,
    something that is hard to do on premises where machines have to be procured months
    in advance. The amount of data and the compute required to analyze it can scale
    dynamically from warm pools of compute clusters. When storage is decoupled from
    the compute, you can utilize it for many different use cases. The same storage
    that was once file-based data lakes for structured data can serve as storage and
    data for the DWH. This key convergence enables you to store data once, utilizing
    views to prepare the data for each specific use case.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 计算与存储的分离为数据湖与DWH的融合提供了理想的环境。云服务提供商能够通过将计算带到存储中为交互式查询启用分布式应用程序。这允许云服务提供商独立地为组织的作业分配存储和计算资源，这在本地环境中很难做到，因为机器需要提前数月采购。数据量和分析所需的计算能够从热池的计算集群动态扩展。当存储与计算解耦时，您可以将其用于许多不同的用例。曾经作为结构化数据文件型数据湖的同一存储现在可以作为DWH的存储和数据。这种关键的融合使您可以仅存储一次数据，利用视图为每个特定用例准备数据。
- en: Let’s have a look at the foundational elements of such architecture, how to
    get there, and why it is something that can prepare you for future needs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这种架构的基础要素，如何达到这个目标，以及为什么这是一种能够为未来需求做好准备的东西。
- en: Reference architecture
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考架构
- en: We are utilizing cloud storage (AWS S3, Google Cloud Storage, or Azure Blob
    Storage) for both the data used by a Spark cluster and the DWH that serves BI
    reporting. This enables Spark code that data lake teams spent years perfecting
    to be run on ephemeral compute clusters that connect to always-available storage.
    It allows the compute to move to the data, rather than the data to have to be
    shuffled among local disks (as it would with HDFS). The high throughput of cloud
    storage unlocks better speed and performance. Both Databricks and the hyperscalers
    offer Spark clusters as a managed service, further abstracting the required management
    of the infrastructure.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用云存储（如AWS S3、Google Cloud Storage或Azure Blob Storage）同时为Spark集群使用的数据和为BI报告服务的DWH。这使得数据湖团队花费多年完善的Spark代码可以在连接到始终可用存储的短暂计算集群上运行。它允许计算移到数据上，而不是数据必须在本地磁盘之间进行洗牌（如使用HDFS时）。云存储的高吞吐量提升了速度和性能。Databricks和超大规模云服务提供商都提供Spark集群作为托管服务，进一步抽象化了基础设施的管理要求。
- en: Convergence of the data lake and DWH (see [Figure 7-2](#data_lakehouse_reference_architectureem))
    is about simplifying and unifying the ingestion and storage of the data, and leveraging
    the correct computing framework for a given problem. For structured and semistructured
    data, writing all of the data as it is streamed into tables using a CDC tool enables
    users to use simple SQL to transform the data and build logical views to query
    the information in a way that aligns with business use cases. Because views are
    heavily leveraged in this pattern, there can be column elimination, partitioning,
    and logic to optimize the speed of the queries while maintaining a historical
    ledger of the data streamed into the tables.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖与DWH（见[图 7-2](#data_lakehouse_reference_architectureem)）的融合是为了简化和统一数据的摄取与存储，并利用适合特定问题的正确计算框架。对于结构化和半结构化数据，使用CDC工具将所有数据流入表格，用户可以使用简单的SQL转换数据，并构建逻辑视图以按照业务用例查询信息。由于视图在这种模式中被广泛利用，可以进行列消除、分区和逻辑以优化查询速度，同时保持流入表格的数据的历史分类帐。
- en: '![Data lakehouse reference architecture—cloud storage approach](assets/adml_0702.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![数据湖仓库参考架构 — 云存储方法](assets/adml_0702.png)'
- en: Figure 7-2\. Data lakehouse reference architecture—cloud storage approach
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 数据湖仓库参考架构 — 云存储方法
- en: Conversely, data that is ingested via a batch pipeline can use a similar approach
    by which all of the data is written to a table and SQL is used to create views
    with the most recent version of each record. Like the streaming data, a historical
    ledger is maintained in the raw tables, allowing data scientists to use all of
    the data for building and testing ML models. In this architecture, users can leverage
    scheduled queries or an event-based Lambda architecture for data ingestion.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，通过批处理管道摄取的数据可以使用类似的方法，所有数据都写入表格，并使用 SQL 创建具有每条记录最新版本的视图。与流数据类似，原始表中维护历史账本，允许数据科学家使用所有数据构建和测试
    ML 模型。在这种架构中，用户可以利用定期查询或基于事件的 Lambda 架构进行数据摄入。
- en: In contrast to the architecture in [Figure 7-1](#data_lake_and_dwh_coexistence),
    which has two different engines (i.e., the analytical engine and the inner DWH
    SQL engine), the reference architecture in [Figure 7-2](#data_lakehouse_reference_architectureem)
    has a single analytical engine that can access both data lake and DWH data. This
    is important to note because it allows for a more streamlined and efficient data
    analysis process.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [图 7-1](#data_lake_and_dwh_coexistence) 中具有两个不同引擎（即分析引擎和内部 DWH SQL 引擎）的架构相比，[图 7-2](#data_lakehouse_reference_architectureem)
    中的参考架构具有单一的分析引擎，可以访问数据湖和数据仓库数据。这一点很重要，因为它可以实现更流畅高效的数据分析过程。
- en: The main goal of data lakehouse solutions like Dremio and Databricks is to enable
    high-performance data processing while also supporting analytics and BI directly
    on the data lake storage. They usually provide a semantic layer that allows you
    to abstract the underlying data and provide to the analytical engine a view on
    the data that enables fast query processing without the need to implement data
    marts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖仓库解决方案，如 Dremio 和 Databricks，主要目标是在数据湖存储上直接支持高性能数据处理、分析和商业智能。它们通常提供语义层，允许您抽象底层数据，并向分析引擎提供数据视图，实现快速查询处理，无需实现数据集市。
- en: The *Production/Gold* layer of the project (see [Chapter 5](ch05.html#architecting_a_data_lake))
    can be the business-driven views or materialized tables that are governed and
    purpose-built. The underlying logic and storage provide access to end users and
    applications alike, allowing you to use the converged data platform for Hadoop,
    Spark, analytics, and ML.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的 *生产/黄金* 层（参见 [第 5 章](ch05.html#architecting_a_data_lake)）可以是经过管理和专门构建的业务驱动视图或物化表格。底层逻辑和存储为终端用户和应用程序提供访问，允许您在
    Hadoop、Spark、分析和 ML 上使用融合数据平台。
- en: It no longer matters if the data is stored within the DWH or within the freely
    floating cloud bucket. Behind the scenes it is the similar distributed storage
    architecture, but data is structured differently. For example, data lakes would
    move the data from HDFS to the same object storage outside the cluster. This is
    the same as what cloud EDW would use as the backbone of its storage system. As
    a result, data is easily accessible and managed by both data lake and DWH architectures
    in one place. Therefore, organizations can now apply governance rules around data
    residing in the lake and the same data accessed by the DWH. Thanks to that, you
    can break down silos not just by putting data into a central repository but also
    by enabling processing and query engines to move to wherever that data is, as
    described in [Figure 7-3](#dwhcomma_data_lakecomma_and_data_lakeho). This leads
    to DWH and data lake convergence, allowing them to use the same metadata store
    and governance and enabling data engineers, data scientists, and data analysts
    to work together in collaboration rather than in siloed systems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据存储在数据仓库（DWH）内部还是自由浮动的云存储桶中已不再重要。在幕后，它是相似的分布式存储架构，但数据结构不同。例如，数据湖将数据从 HDFS
    移动到集群外的同一对象存储中。这与云环境数据仓库（EDW）作为存储系统骨干的使用方式相同。因此，数据可以在一个地方轻松访问和管理，同时由数据湖和数据仓库架构管理。因此，组织现在可以应用数据湖内数据和数据仓库访问的治理规则。由此，您不仅可以通过将数据放入中央存储库来打破信息孤岛，还可以通过使处理和查询引擎能够移动到数据所在的任何位置来实现。如
    [图 7-3](#dwhcomma_data_lakecomma_and_data_lakeho) 所述。这导致了数据仓库和数据湖的融合，使它们能够使用相同的元数据存储和治理，并使数据工程师、数据科学家和数据分析师能够共同协作，而不是在信息孤岛系统中。
- en: Data engineers, data scientists, and even business users will be able to per­form
    self-serving queries on any kind of data they may have, leveraging one single
    point of access. And even in terms of development, the work will be easier because
    there is only one system to interact with instead of a plethora of different tools.
    Another point in favor of this approach is the centralization of security and
    governance that is made stronger via the elimination of self-managed copies of
    the data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师、数据科学家甚至业务用户都能够在可能拥有的任何数据类型上执行自服务查询，利用一个单一的访问点。甚至在开发方面，工作也将更容易，因为只需与一个系统互动，而不是众多不同的工具。这种方法的另一个优点是通过消除自我管理数据副本来加强安全和治理的集中化。
- en: '![DWH, data lake, and data lakehouse-on-storage approaches](assets/adml_0703.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![DWH、数据湖和数据湖仓库存储方法](assets/adml_0703.png)'
- en: Figure 7-3\. DWH, data lake, and data-lakehouse-on-storage approaches
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. DWH、数据湖和基于存储的数据湖仓库方法
- en: Migration
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移
- en: Moving from the architecture in [Figure 7-1](#data_lake_and_dwh_coexistence)
    to the one in [Figure 7-2](#data_lakehouse_reference_architectureem) is an iterative
    process that keeps the data lake but migrates the data warehouse. Start by introducing
    a single analytical engine that is connected to the data lake storage, to take
    care of all the data processing, as represented in [Figure 7-4](#journey_to_data_lakehouseem_dashcentral).
    Then, migrate away from the DWH SQL engine through various substeps, following
    the process described in [Chapter 4](ch04.html#a_migration_framework).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图 7-1](#data_lake_and_dwh_coexistence)中的架构到[图 7-2](#data_lakehouse_reference_architectureem)中的架构是一个迭代过程，保留了数据湖但迁移了数据仓库。首先引入一个连接到数据湖存储的单一分析引擎，来处理所有数据处理工作，如[图
    7-4](#journey_to_data_lakehouseem_dashcentral)所示。然后，通过各种子步骤逐步迁移DWH SQL引擎，遵循[第四章](ch04.html#a_migration_framework)中描述的过程。
- en: 'Since this process can take several repetitions, it is important to start by
    identifying a quick win use case (as covered in [Chapter 4](ch04.html#a_migration_framework))
    that could demonstrate the benefits of the new solution to create consensus within
    the organization. From there it will be possible to continue expanding the footprint
    of the new solution until it becomes the “de facto” analytical engine of the entire
    platform. A very good place to start is generally the world of interactive queries
    for data exploration: users can interact with the new engine performing data analysis
    on a variety of data that spans across the data lake and DWH. They can carry out
    queries directly with the new tool or via the integration with BI and reporting
    solutions. Once the benefits of the change become tangible, the old DWH engine
    can be slowly decommissioned to save costs, reduce complexity, and limit the need
    to download the data for offline elaboration, as the users will now have access,
    based on the organizations’ data governance rules, to all the data of the company.
    Of course, brand-new workloads should only be implemented leveraging the new analytical
    engine.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个过程可能需要多次重复，因此首先要识别一个快速获胜的用例（如[第四章](ch04.html#a_migration_framework)中所述），这可以展示新解决方案的好处，以在组织内部形成共识。从那里开始，将可能继续扩展新解决方案的足迹，直到它成为整个平台的“事实上的”分析引擎。一个非常好的开始通常是交互式查询的数据探索世界：用户可以通过新引擎与数据湖和DWH跨越各种数据进行数据分析交互。他们可以直接使用新工具进行查询，也可以通过与BI和报告解决方案集成来执行查询。一旦变革的好处变得具体化，旧的DWH引擎就可以逐步停用以节省成本、减少复杂性，并且限制因离线数据加工而需下载数据的需求，因为根据组织的数据治理规则，用户现在将有权访问公司的所有数据。当然，新的工作负载应该只通过利用新的分析引擎来实现。
- en: '![Journey to data lakehouse—central analytical engine](assets/adml_0704.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![数据湖仓库之旅——中央分析引擎](assets/adml_0704.png)'
- en: Figure 7-4\. Journey to data lakehouse—central analytical engine
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-4\. 数据湖仓库之旅——中央分析引擎
- en: Future proofing
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 未来证明
- en: 'Once fully deployed, the interaction with datasets sitting mainly in the data
    lake and in the DWH (*only the curated ones*) will be bidirectional: that means
    that the analytical engine will be able to read and write directly on the underlying
    storage systems that are usually storing the data in an open format like Apache
    Parquet or Apache Avro. This is a great benefit because the underlying technology
    can be seen as a consistent and common medium across different types of storage
    systems. The analytical engine will be flexible enough to use either a schema-on-read
    approach (that is typical of the data lake pattern) or a more structured approach
    like the SQL-based data system. Another important benefit that becomes out of
    the box when adopting a lakehouse architecture is the ease of streaming adoption.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完全部署，与数据湖和数据仓库（*仅限策划的数据*）中的数据集的交互将是双向的：这意味着分析引擎能够直接在通常以开放格式如Apache Parquet或Apache
    Avro存储数据的底层存储系统上读取和写入。这是一个巨大的好处，因为底层技术可以被视为跨不同类型存储系统的一致和共同媒介。分析引擎将足够灵活，可以采用类似数据湖模式的按需模式读取（schema-on-read）或像基于SQL的更结构化的方法。采用湖仓架构时的另一个重要好处是易于流式数据采纳。
- en: As you will see in the next chapter, real-time access to the data is becoming
    increasingly important for every kind of business, and the fact that you can treat
    streaming data in the same way as standard stored data is surely something that
    can bring additional value. In fact, in this architecture, streaming data pipelines
    can read and write data in real time, knowing that the underlying technology (e.g.,
    Apache Iceberg, Apache Hudi, or Delta Lake—please refer to [“The Evolution of
    Data Lake with Apache Iceberg, Apache Hudi, and Delta Lake”](ch05.html#the_evolution_of_data_lake_with_apache))
    can ensure full ACID transactions, bringing consistency within the system and
    always handling the most recent and up-to-date data. You can use these technologies
    in either a proprietary (e.g., Databricks) or an open source manner on EMR, Dataproc,
    etc. The final decision of which is the right choice for your organization is
    up to you. The key factors to consider are flexibility, maintenance, and support.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在下一章看到的，对数据的实时访问对于各种类型的业务越来越重要，而你能够将流数据像处理标准存储数据一样对待，这无疑能带来额外的价值。事实上，在这种架构中，流数据管道能够实时读写数据，知道底层技术（例如Apache
    Iceberg、Apache Hudi或Delta Lake，请参考[“Apache Iceberg、Apache Hudi和Delta Lake的数据湖演变”](ch05.html#the_evolution_of_data_lake_with_apache)）能确保完整的ACID事务，在系统内带来一致性，并且始终处理最新和最新的数据。你可以在EMR、Dataproc等平台上以专有（例如Databricks）或开源方式使用这些技术。选择哪种适合你的组织的最终决定取决于你。要考虑的关键因素是灵活性、维护和支持。
- en: SQL-First Lakehouse
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SQL-First Lakehouse
- en: The main goal of SQL-first lakehouse solutions is to enable high-performance
    analytics and BI while also supporting flexible data processing with Spark directly
    on the DWH storage. An advantage of this architecture is that business users can
    carry out orchestration and ML.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: SQL-First湖仓解决方案的主要目标是在数据仓库存储上直接使用Spark实现高性能分析和BI，同时支持灵活的数据处理。这种架构的优势在于业务用户可以进行编排和机器学习。
- en: Let us examine the fundamental elements of such architecture, how to achieve
    it, and why it can be beneficial to you in the future.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检视这种架构的基本要素，如何实现它，以及为什么将来它对你会有益。
- en: Reference architecture
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考架构
- en: 'Using a DWH as a data lake of course requires that the DWH solutions are able
    to handle not only standard SQL queries on tables but also native integration
    with Spark-based environments, ML capabilities, and streaming features. Modern
    DWHs like BigQuery, Athena, Synapse, and Snowflake support these capabilities
    to varying extents that will, no doubt, have improved by the time you are reading
    this. In [Figure 7-5](#data_lakehousesolidussql_first_approach) you can see a
    reference architecture where the DWH acts as a central analytical engine point
    for the platform. As you can see from the diagram, the data flows from the original
    sources (in various forms and velocity) through both the data lake and the native
    DWH storage. From here you can identify four main storage areas:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当将数据仓库作为数据湖使用时，需要确保数据仓库解决方案不仅能处理表格上的标准SQL查询，还能与基于Spark的环境、ML功能和流处理功能进行本地集成。像BigQuery、Athena、Synapse和Snowflake这样的现代数据仓库在不同程度上支持这些功能，这些功能在您阅读本文时肯定已经得到了改进。在[图 7-5](#data_lakehousesolidussql_first_approach)中，您可以看到一个参考架构，其中数据仓库充当平台的中央分析引擎点。从图中可以看出，数据从原始来源（以各种形式和速度）流经数据湖和本地数据仓库存储。从这里，您可以识别出四个主要的存储区域：
- en: The data lake storage that is equal to what we have seen in the previous section
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据湖存储与前一节中所见相同
- en: 'The DWH storage split in three dimensions:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仓库存储分为三个维度：
- en: Raw
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 原始
- en: The data coming as-is from the various sources (batch or streaming)
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 来自各种来源（批处理或流处理）的原始数据。
- en: Enriched
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增强的
- en: Raw data with a first layer of transformation
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具有第一层转换的原始数据
- en: Curated
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精心策划的
- en: Enriched data ready for final transformations
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准备进行最终转换的增强数据
- en: Spark (ETL) or SQL (ELT) can carry out all the transformations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Spark（ETL）或SQL（ELT）可以执行所有的转换。
- en: '![Data lakehouse/SQL-first approach](assets/adml_0705.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![数据湖仓库/SQL优先方法](assets/adml_0705.png)'
- en: Figure 7-5\. Data lakehouse—SQL-first approach
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. 数据湖仓库—SQL优先方法
- en: In this approach to building a lakehouse, using SQL is the preferred approach
    to handling and transforming your data. A SQL-first approach leverages the highly
    optimized, interactive querying capabilities of the DWH to keep costs down and
    opens data-driven decision making to business users.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建数据湖仓库的这种方法中，使用SQL是处理和转换数据的首选方法。SQL优先方法利用数据仓库的高度优化的交互式查询能力来降低成本，并将数据驱动的决策扩展给业务用户。
- en: 'When you need to tackle more advanced data processing, you may be able to leverage
    a structured programming language like Spark. This is especially true when working
    with ML algorithms, be it structured data (e.g., boosted tree regression/classification)
    or unstructured data (e.g., NLP). This is due to the fact that these kinds of
    operations are not flexible to implement in SQL (inflexible, but not impossible:
    BigQuery and Redshift SQL have some ML functions today at the time of writing,
    and other DWHs will no doubt support them shortly). Utilizing Spark also allows
    you to avoid having to rewrite legacy data processing jobs that may have been
    written in Spark in SQL.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要处理更高级的数据处理时，您可能可以利用像Spark这样的结构化编程语言。特别是在处理ML算法时，无论是结构化数据（例如提升树回归/分类）还是非结构化数据（例如NLP），这一点尤为明显。这是因为这些操作在SQL中实现起来不够灵活（虽然不灵活，但并非不可能：BigQuery和Redshift
    SQL在撰写本文时已经具备了一些ML功能，其他数据仓库很快也会支持）。利用Spark还可以避免重新编写可能已在Spark中编写的遗留数据处理作业。
- en: Modern DWH solutions come with the ability to execute Python, Spark, and even
    serverless functions directly from their engines and, most importantly, without
    moving or copying data outside the data store where it resides. BigQuery, for
    example, achieves this by providing the ability to write stored procedures in
    Spark, executed in an ad hoc Serverless Spark environment, that can be called
    in a SQL statement. Snowflake provides Snowpark, a built-in Python engine that
    is capable of running Spark (and other programming languages). Athena achieves
    this by operating off standard-format files in formats such as Parquet so that
    the same data can be read from managed Hadoop environments like EMR. Also, data
    lake technologies such as Databricks support directly reading and writing to the
    native warehouse storage through optimized bulk APIs such as the BigQuery Storage
    API.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现代数据仓库解决方案具备直接从其引擎执行Python、Spark甚至无服务器函数的能力，而且最重要的是，无需将数据移动或复制到数据存储之外。例如，BigQuery通过提供在Spark中编写存储过程、在即席Serverless
    Spark环境中执行这些过程，并在SQL语句中调用它们的能力来实现这一点。Snowflake提供Snowpark，一个内置的Python引擎，能够运行Spark（及其他编程语言）。Athena通过使用Parquet等标准格式文件在管理的Hadoop环境（如EMR）中操作，从而实现了这一点。此外，数据湖技术（如Databricks）通过优化的批量API（如BigQuery
    Storage API）直接读取和写入本地仓库存储。
- en: One of the key advantages of the data lakehouse paradigm is that it has urged
    vendors (and the industry in general) to develop increasingly powerful DWHs to
    make several complex solutions very easy to use for a broad set of users. This
    is particularly true for ML. Business users can train and deploy ML models leveraging
    standard SQL code, and they can leverage open source solutions, like dbt, to easily
    automate and productionize data preparation. It is pretty common nowadays to see
    business analysts who are, for example, able to autonomously forecast, with a
    high level of accuracy, customer demand to put in place efficient inventory management
    or to predict the best price based on historical data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖仓库范式的关键优势之一是，它促使供应商（以及整个行业）开发越来越强大的数据仓库，使多种复杂解决方案对广泛用户群体变得易于使用。这在机器学习领域尤为明显。业务用户可以利用标准
    SQL 代码训练和部署机器学习模型，并可以利用开源解决方案（如 dbt）轻松自动化和生产化数据准备。如今，很常见看到例如能够独立预测客户需求并实现高精度的商业分析师，以便有效进行库存管理或根据历史数据预测最佳价格。
- en: 'While the SQL support for ML is great, the majority of the tasks related to
    ML technology are in the hands of data engineers and data scientists who tend
    to be more keen to leverage Python frameworks they are more familiar with. Especially
    when dealing with the development of deep learning models on unstructured data,
    you need to manipulate massive datasets leveraging open source languages to train
    models that have to be served to final users. This is why the native integration
    among the various data sources is playing a pivotal role: the ability to leverage
    the Spark support is a key feature of the paradigm that gives a high level of
    freedom to users.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SQL对机器学习的支持很棒，但与ML技术相关的大多数任务都掌握在更倾向于利用他们更熟悉的Python框架的数据工程师和数据科学家手中。特别是在处理非结构化数据上开发深度学习模型时，您需要利用开源语言处理海量数据集，以训练必须提供给最终用户的模型。这就是为什么各种数据源之间的本地集成发挥着关键作用：利用Spark支持的能力是给用户提供高自由度的范式的一个关键特性。
- en: 'With the proliferation of the development of ML models, another category of
    operations that the platform has to handle arose: ML operations (MLOps). In MLOps,
    users want to keep track of the data versioning, data lineage (especially important
    for audit purposes), and model updates leveraging the same approach adopted in
    the development of software (i.e., DevOps). Solutions like Databricks MLflow,
    Google Vertex AI, or Amazon SageMaker are natively connected with modern DWH,
    giving final users a unified experience when dealing with the ML model lifecycle.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型开发的普及，平台还必须处理另一类运营：机器学习运维（MLOps）。在MLOps中，用户希望通过与软件开发相同的方法（即DevOps）来跟踪数据版本、数据血缘（特别是出于审计目的）和模型更新。诸如Databricks
    MLflow、Google Vertex AI或Amazon SageMaker等解决方案与现代数据仓库天然连接，使最终用户在处理机器学习模型生命周期时有统一的体验。
- en: Migration
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迁移
- en: 'As with the lakehouse-on-cloud-storage approach, the transition to a SQL-first
    lakehouse is an iterative process that requires several steps to be fully operational,
    as represented in [Figure 7-6](#journey_to_data_lakehouseem_dashsql_fir). The
    first step is to enable data ingestion directly into the DWH: the data sources
    have to be directly connected not only with the data lake but especially with
    the three types of DWH storage (i.e., raw, enriched, and curated). Once the data
    is fully ingested into the central data repository, both in batch and in streaming
    mode, it is time to cut out external analytical engines, elevating the SQL engine
    of the DWH as the main one. Here it is pivotal to pay attention to how you’re
    going to move data lake workloads into the DWH, leveraging the built-in programming
    language module (i.e., Python or Spark engines/connectors) to operate directly
    on the native storage format. The ML workloads, which initially are still handled
    outside the DWH, will be insourced as the last iteration of the process.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于云存储的湖仓库方法类似，向 SQL 优先的湖仓库转型是一个迭代过程，需要多个步骤才能完全投入运行，如[图 7-6](#journey_to_data_lakehouseem_dashsql_fir)
    所示。第一步是直接将数据摄入到 DWH 中：数据源不仅要直接连接到数据湖，尤其是要连接到三种 DWH 存储（即原始、丰富和策划）。一旦数据完全被批处理和流式处理摄入到中央数据存储库中，就是时候剔除外部分析引擎，将
    DWH 的 SQL 引擎提升为主要引擎。在这里，关键是注意如何将数据湖的工作负载转移到 DWH 中，利用内置的编程语言模块（如 Python 或 Spark
    引擎/连接器）直接在原生存储格式上操作。最后一个迭代过程中，初始时仍在 DWH 外部处理的 ML 工作负载将被内部化处理。
- en: '![Journey to data lakehouse—SQL-first approach](assets/adml_0706.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![走向数据湖仓库——以 SQL 为先的方法](assets/adml_0706.png)'
- en: Figure 7-6\. Journey to data lakehouse—SQL-first approach
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. 走向数据湖仓库——以 SQL 为先的方法
- en: During the phases of the migration, following the SQL-first approach, organizations
    will discover that a huge number of pipelines will have to be written in SQL.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移的各个阶段中，遵循 SQL 优先的方法，组织将发现大量数据管道需要用 SQL 编写。
- en: In this architecture, it is more performant to implement data processing in
    SQL (although ETL in Spark remains a possibility). This often requires a new type
    of skill within your organization, termed *analytics engineering*. You may be
    able to easily find workers skilled in analytics engineering within your organization
    (refer to [“Data Analysis–Driven Organization”](ch03.html#data_analysis_driven_organization))
    since SQL is a widespread language (more so than Spark or other programming languages)
    and can be learned rapidly. In fact, this is a point in favor of democratization
    because adopting this paradigm will make data access easier for a huge number
    of employees.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，通过 SQL 实现数据处理更为高效（尽管 Spark 中的 ETL 仍然是一种可能性）。这通常需要组织内一种新的技能类型，称为*分析工程*。您可能很容易在组织内找到具备分析工程技能的员工（参考[“数据分析驱动组织”](ch03.html#data_analysis_driven_organization)），因为
    SQL 是一种广泛使用的语言（比 Spark 或其他编程语言更为广泛），可以迅速学习。事实上，这是民主化的一个优点，因为采用这种范式将使大量员工更容易访问数据。
- en: Analytics engineers will be responsible for much of the data enrichment and
    curation, and they will be able to bring in their domain knowledge and require
    only SQL skills. The different types of datasets (raw, enriched, and curated)
    will likely each be utilized by different types of users. In general, most end
    users, analytical teams, and applications will utilize the curated data, while
    data engineers and data scientists will probably prefer to access the raw/data
    lake and enriched data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析工程师将负责大部分数据丰富和策划工作，并且他们能够运用他们的领域知识，只需具备 SQL 技能。不同类型的数据集（原始数据、丰富数据和策划数据）可能会被不同类型的用户使用。总体而言，大多数最终用户、分析团队和应用程序将利用策划数据，而数据工程师和数据科学家可能更倾向于访问原始数据或数据湖和丰富数据。
- en: 'In this scenario it is important to note that streaming and ML capabilities
    are included in the analytical engine: this means that they are natively available
    to everyone, even to people who are not advanced in writing code in TensorFlow,
    PyTorch, Keras, or scikit-learn. This is a great achievement in enabling democratization
    of data and tool access to employees within the organization, and it will enable
    more and more users to achieve more with their data. Finally, it is important
    to note that data governance is handled in a federated and transversal way, creating
    a unified and centralized place to manage, monitor, and govern the data, making
    it accessible to a variety of users and tools.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，重要的是注意到流式处理和机器学习能力包含在分析引擎中：这意味着它们对每个人都是本地可用的，即使是那些在使用TensorFlow、PyTorch、Keras或scikit-learn编写代码方面不那么高级的人也是如此。这是在促进数据民主化和工具访问方面取得的重大成就，使得组织内的员工能够更多地利用他们的数据。最后，需要注意的是数据治理以联邦和横向的方式进行处理，创建了一个统一和集中的管理、监控和治理数据的场所，使其能够被多种用户和工具访问。
- en: Future proofing
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 未来保障
- en: Once the migration has been completed and the organization has developed an
    architecture as outlined in [Figure 7-6](#journey_to_data_lakehouseem_dashsql_fir),
    the majority of interactions will be SQL based. It is possible to leverage other
    programming languages for use cases where SQL is difficult or off-the-shelf libraries
    are available, making the non-SQL option more attractive.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦迁移完成，并且组织已经根据[图7-6](#journey_to_data_lakehouseem_dashsql_fir)中的架构进行了开发，大部分交互将基于SQL。在SQL困难或可用现成库的用例中，可以利用其他编程语言，使非SQL选项更具吸引力。
- en: The benefit of a SQL-first lakehouse architecture is huge because it provides
    a single place for data storage and democratizes access to it via a standard and
    widely known programming language (SQL) that is supported by a large number of
    tools used by business users. The major benefit, compared with the cloud storage–based
    data lakehouse, is exactly this ability to bring a broader set of users closer
    to the data and allow them to create innovative solutions (i.e., ML)—the more
    people who have the ability to employ the data (in a governed way), the more innovation
    your organization will see.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 采用SQL优先湖仓架构的好处非常大，因为它提供了一个统一的数据存储位置，并通过一个广泛使用的标准编程语言（SQL）——由许多商业用户使用的工具支持——来民主化访问数据。与基于云存储的数据湖仓相比，其主要优势在于能够让更广泛的用户接近数据并允许他们创建创新解决方案（即机器学习）——能够使用数据的人越多（以受控的方式），你的组织就会看到更多创新。
- en: The Benefits of Convergence
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 融合的好处
- en: 'If you are a startup or in the fortunate situation of doing greenfield development,
    start with a pure data lake or a pure DWH, depending on your use case and skill
    set (see [Chapter 3](ch03.html#designing_your_data_team)). For everyone else,
    we recommend the lakehouse architecture. Regardless of whether you choose a lakehouse-on-storage
    or a SQL-first lakehouse, choosing a lakehouse architecture provides the following
    benefits:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是创业公司或者幸运地进行绿地开发，根据你的使用案例和技能集，可以选择纯数据湖或纯数据仓库（见[第3章](ch03.html#designing_your_data_team)）。对于其他所有人，我们建议采用湖仓架构。无论你选择湖仓存储还是SQL优先湖仓，选择湖仓架构都带来以下好处：
- en: Time to market
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 上市时间
- en: You can ingest and use data straightaway, whether from batch or real-time data
    sources. Rather than employing complex ETL pipelines to process data, data is
    “staged” either in a messaging bus or through object storage. Then it is transformed
    within the converged DWH/data lakes, enabling users to act as the data is received.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接摄取和使用数据，无论是来自批处理还是实时数据源。不再使用复杂的ETL管道处理数据，数据直接“分阶段”存储在消息总线或对象存储中，然后在融合的数据仓库/数据湖中进行转换，使用户能够在收到数据时立即行动。
- en: Reduced risk
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 减少风险
- en: You can continue leveraging existing tools and applications without rewriting
    them. This reduces the risk and costs associated with change.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续利用现有的工具和应用程序而无需重写它们。这降低了变更带来的风险和成本。
- en: Predictive analytics
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分析
- en: Moving away from the traditional view of data marts and data mining to real-time
    decision making using fresh data increases business value. This is only possible
    because the governance and strictness around DWHs have come down, reducing barriers
    to entry.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从传统的数据集市和数据挖掘的观点转向使用新鲜数据进行实时决策，增加了业务价值。这只有因为围绕数据仓库的治理和严格要求已经降低，减少了进入门槛，才变得可能。
- en: Data sharing
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据共享
- en: The converged environment is now the one-stop shop for all the types of users
    (i.e., data analyst, data engineer, and data scientist) you may have. They can
    all access the same managed environment, getting access to different stages of
    data when they need it. At the same time, different roles can have access to the
    same data through different layers, and this is governed by platform-wide access
    rights. This not only increases the data governance but also allows simpler access
    management and auditing throughout the data ecosystem.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 收敛环境现在是所有用户类型（即数据分析师、数据工程师和数据科学家）的一站式购物平台。他们可以在需要时访问同一管理环境，获取不同数据阶段的访问权限。同时，不同角色可以通过不同层次访问相同数据，这由平台范围内的访问权限管理。这不仅增加了数据治理，还简化了数据生态系统中的访问管理和审计。
- en: ACID transactions
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ACID事务
- en: In a typical DWH, the data integrity is maintained, and multiple users reading
    and writing the data see a consistent copy of the data. Although ACID is a key
    feature in the majority of the databases, traditionally it has been rather difficult
    to provide the same guarantees when it comes to traditional HDFS-based data lakes.
    There are schemes such as Delta Lake and Apache Iceberg that try to maintain ACID
    semantics (refer to [“The Evolution of Data Lake with Apache Iceberg, Apache Hudi,
    and Delta Lake”](ch05.html#the_evolution_of_data_lake_with_apache)); they store
    a transaction log with the aim of keeping track of all the commits made to a data
    source.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的数据仓库中，保持数据完整性，多个用户读写数据时看到的是一致的数据副本。虽然ACID是大多数数据库的关键特性，但传统上在基于传统HDFS的数据湖中提供相同的保证相当困难。有诸如Delta
    Lake和Apache Iceberg之类的方案试图保持ACID语义（参见[“Apache Iceberg、Apache Hudi和Delta Lake的数据湖演进”](ch05.html#the_evolution_of_data_lake_with_apache)）。它们存储事务日志，旨在跟踪对数据源的所有提交。
- en: Multimodal data support
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态数据支持
- en: Semistructured and structured data are key differentiators with the DWHs and
    data lakes. Semistructured data has some organizational properties such as semantic
    tags or metadata to make it easier to organize, but data still does not conform
    to a strict schema. In the converged world, this is accommodated with extended
    semistructured data support. On the other hand, for unstructured use cases, data
    lakes are still required apart from edge cases.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 半结构化和结构化数据是数据仓库和数据湖的关键区别。半结构化数据具有一些组织属性，如语义标签或元数据，使其更容易组织，但数据仍不符合严格的模式。在收敛的世界中，通过扩展的半结构化数据支持可以容纳这些数据。另一方面，对于非结构化的用例，仍然需要数据湖，除了边缘情况。
- en: Unified environment
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 统一环境
- en: Traditionally, different tools and environments, usually orchestrated by ETLs,
    manage data capture, ingest, storage, processing, and serving. In addition, processing
    frameworks such as Spark, Storm, Beam, etc., provide built-in ETL templates to
    enable organizations to build ETL pipelines. However, with capable cloud EDWs
    and integrated cloud tools, this pipeline is now all handled by a single environment.
    ELT does most of the traditional ETL tasks such as cleanse, dedupe, join, and
    enrich. This is made possible at different stages of the data lake implementation
    within the DWH. Furthermore, with the support of core DWHs, you can have access
    through a united environment to concepts such as stored procedures, scripting,
    and materialized views.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，不同的工具和环境通常由ETL协调管理数据捕获、摄入、存储、处理和服务。此外，如Spark、Storm、Beam等处理框架提供内置ETL模板，使组织能够构建ETL管道。然而，通过功能强大的云数据仓库和集成云工具，现在可以通过单一环境处理所有这些管道。ELT执行大部分传统ETL任务，如数据清理、去重、连接和增强。这在DWH中的数据湖实施的不同阶段变得可能。此外，借助核心数据仓库的支持，您可以通过统一环境访问存储过程、脚本和物化视图等概念。
- en: Schema and governance
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 架构和治理
- en: In reality, business requirements and challenges evolve over time. As a result,
    the associated data changes and accumulates, either by adapting to new data or
    by introducing new dimensions. As the data changes, applying data quality rules
    becomes more challenging and requires schema enforcement and evolution. Furthermore,
    PII data governance becomes more important as new data sources are added. There
    needs to be a data governance solution allowing organizations to have a holistic
    view of their data environment. In addition, it is paramount to have the ability
    to identify and mask PII data for different purposes and personas.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，业务需求和挑战随时间而演变。因此，相关数据会随之改变和累积，无论是通过适应新数据还是引入新维度。随着数据的变化，应用数据质量规则变得更具挑战性，并需要模式强制执行和演进。此外，随着新数据源的添加，PII数据治理变得更加重要。组织需要一个数据治理解决方案，能够全面了解其数据环境。此外，对于不同目的和角色，识别和掩码PII数据至关重要。
- en: Streaming analytics
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理分析
- en: Real-time analytics enables immediate responses, and there would be specific
    use cases where an extremely low-latency anomaly detection application is required
    to run. In other words, business requirements would be such that it has to be
    acted upon as the data arrives on the fly. Processing this type of data or application
    requires transformation done outside of the warehouse.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 实时分析能够实现即时响应，存在特定的用例需要运行极低延迟的异常检测应用程序。换言之，业务需求要求在数据即时到达时立即采取行动。处理此类数据或应用程序需要在数据仓库之外完成转换。
- en: Having a single system to manage simplifies the enterprise data infrastructure
    and allows users to work more efficiently.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个管理单一系统简化了企业数据基础设施，并允许用户更高效地工作。
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter we focused on describing how you can mix data lake and DWH
    technology in a brand-new mixed architecture called a lakehouse. We presented
    the two most common architectures and how to get there. The key takeaways are
    as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于描述如何在一种全新的混合架构——湖屋中混合数据湖和数据仓库技术。我们介绍了两种最常见的架构以及如何实现这一目标。主要收获如下：
- en: 'There are two main approaches that organizations have followed in designing
    their data platform: a data lake or a DWH paradigm.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设计其数据平台时，组织通常遵循两种主要方法：数据湖或数据仓库范式。
- en: 'Both approaches come with pros and cons, but there’s a new option: convergence
    in the form of a lakehouse that allows you to have the best of both worlds.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两种方法各有利弊，但有一个新的选择：湖屋形式的融合，使您能够兼顾两者的优点。
- en: There are two possible approaches to take when opting for a lakehouse. Choose
    between them based on the primary skill set of your developers.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择湖屋时，有两种可能的方法。根据开发人员的主要技能集选择其中一种。
- en: One approach to a lakehouse is to converge the data lake and DWH to use the
    same data in cloud storage. The Spark jobs will be changed from using HDFS to
    read from the cloud. A Spark-based SQL engine is used. Over time, this will lead
    to the decommission of the DWH engine.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种湖屋的方法是将数据湖和数据仓库合并，使用云存储中的相同数据。Spark作业将从云中读取，而不再使用HDFS。使用基于Spark的SQL引擎。随着时间的推移，这将导致数据仓库引擎的停用。
- en: The second lakehouse option is to move data lake workloads into the DWH and
    use built-in Python engines or Spark connectors to operate directly on the native
    storage format.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个湖屋选项是将数据湖工作负载移至数据仓库，并使用内置的Python引擎或Spark连接器直接操作原生存储格式。
- en: 'There are several benefits of the lakehouse approach: decreased time to market,
    ability to easily implement predictive analytics and to break down silos and ETL
    pipelines, schema and governance, and streaming analytics are just a few.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 湖屋方法有几个好处：缩短上市时间，轻松实施预测分析并打破孤立和ETL管道，模式和治理以及流处理分析仅是其中几个。
- en: At this point, we have covered the backbone data platform architectures (data
    lake, data warehouse, and data lakehouse). In the next two chapters, you will
    learn how to incorporate streaming and edge/hybrid requirements into this architecture.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们已经涵盖了主干数据平台架构（数据湖、数据仓库和数据湖屋）。在接下来的两章中，您将学习如何将流处理和边缘/混合要求纳入此架构。
