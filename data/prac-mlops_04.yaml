- en: Chapter 4\. Continuous Delivery for Machine Learning Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：机器学习模型的持续交付
- en: By Alfredo Deza
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：Alfredo Deza
- en: Is it really the sad truth that natural philosophy (what we now call science)
    has so far separated off from its origins that it has left behind only papyrologists—people
    who take paper in, put paper out, and while reading and writing assiduously, earnestly
    avoid the tangible? Do they consider direct contact with data to be of negative
    value? Are they, like some redneck in the novel *Tobacco Road*, actually proud
    of their ignorance?
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自然哲学（我们现在称为科学）真的已经远离其起源了吗？它只留下了纸质学家——那些接受纸质信息、输出纸质信息的人，他们在阅读和写作时刻苦心避免与现实接触？他们是否认为直接接触数据是有害的？他们是否像小说《烟草之路》中的某些乡巴佬一样，对自己的无知感到自豪？
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dr. Joseph Bogen
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Dr. Joseph Bogen
- en: As a professional athlete, I was often dealing with injuries. Injuries have
    all kinds of severity levels. Sometimes it would be something minor, like a mild
    contracture on my left hamstring after intense hurdle workouts. Other times it
    would be more serious, like insufferable lower back pain. High-performance athletes
    cannot afford to have days off in the middle of the season. If the plan is to
    work out seven days a week, it is critical to go through those seven days. Missing
    out a day has serious repercussions that can diminish (or entirely wipe out) the
    workouts until that point. Workouts are like pushing a wheelbarrow uphill, and
    missing a workout means stepping to the side letting the wheelbarrow ride downhill.
    The repercussion of that action is that you will have to go back and pick up that
    wheelbarrow to push it up again. You cannot miss out on workouts.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名职业运动员，我经常面对受伤问题。受伤有各种严重程度。有时可能只是一些轻微的问题，比如剧烈跨栏训练后左腿肌肉轻微紧缩。其他时候可能更严重，比如难以忍受的下背痛。高水平运动员在赛季中间不能有休息日。如果计划是每周七天锻炼，那么坚持这七天是至关重要的。错过一天会带来严重后果，可能会使直到那时的训练成果减弱（甚至完全消失）。锻炼就像推着一辆上坡的手推车，错过一次锻炼就意味着让手推车沿着山下滑行。这样做的后果是你需要回去重新推上山。你不能错过锻炼。
- en: If you are injured and you cannot work out, getting back up in full shape as
    soon as possible is a priority *as important as finding alternative workouts*.
    That means that if your hamstring hurts and you can’t run, see if you can go to
    the pool and keep up the cardio plan. Hill repeats are not possible tomorrow because
    you broke a toe? Then try hopping on the bike to tackle those same hills. Injuries
    require a war strategy; giving up and quitting is not an option, but if you must
    retreat, then retreating *the least* *possible* is considered first. If we cannot
    fire cannons, let’s bring the cavalry. There is always an option, and creativity
    is as important as trying to recover fully.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你受伤了，无法进行锻炼，那么尽快以最佳状态恢复上场是首要任务，就像找到替代锻炼一样重要。这意味着如果你的腿筋受伤了，无法跑步，那么看看是否可以去游泳池，保持有氧计划。明天无法进行上坡训练，因为你的脚趾头受伤了？那就试试骑自行车去应对同样的坡道。受伤需要战略规划；放弃不是选项，但如果必须撤退，那么首先考虑的是尽量减少撤退的可能性。如果我们不能开炮，那就派骑兵。总会有办法，创造力和完全康复同样重要。
- en: 'Recovery requires strategy as well, but more than strategy, it requires constant
    evaluation. Since you keep working out as much as possible with an injury, it
    is essential to evaluate if the injury is getting worse. If you get on the bike
    to compensate because you can’t run, you must be hyper-aware if the bike is making
    the injury worse. The constant evaluation for injuries is a rather simplistic
    algorithm:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复需要策略，但更重要的是持续评估。由于你在受伤的情况下尽可能多地进行锻炼，评估伤势是否加重是至关重要的。如果你因为无法跑步而骑自行车，你必须非常警觉，看看自行车是否加重了伤势。对于受伤的持续评估是一个相当简单的算法：
- en: First thing every day, assess if the injury is the same, worse, or better than
    the day before.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每天第一件事，评估伤势是否比前一天更糟，还是更好。
- en: If it is worse, then make changes to avoid the previous workouts or alter them.
    Those may be harming recovery.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果情况变得更糟，那么要进行调整以避免之前的锻炼或者改变它们。这些可能会妨碍恢复。
- en: If it is the same, compare the injury against last week or last month even.
    Ask the question *“Am I feeling the same, worse, or better than last week?”*
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果情况没有改变，将伤势与上周或上个月进行比较。问自己：“我是否感觉比上周更糟、更好还是一样？”
- en: Finally, if you feel better, that strongly reinforces that the current strategy
    is working, and you should continue until fully recovered.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，如果你感觉好些了，那将极大地强化当前策略正在起作用的事实，并且你应该继续，直到完全康复。
- en: 'With some injuries, I had to evaluate at a higher frequency (rather than waiting
    until the next morning). The result of constant evaluation was the key to recovery.
    In some cases, I had to evaluate if a specific action was hurting me. One time
    I broke a toe (slammed it into the corner of a bookshelf), and I immediately strategized:
    can I walk? Do I feel pain if I run? The answer to all of these was a resounding
    yes. I tried going swimming that afternoon. For the next few weeks, I would constantly
    check if walking was possible without pain. Pain is not a foe. It is the indicator
    that helps you decide to keep doing what you are doing or stop and rethink the
    current strategy.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些受伤，我不得不更频繁地评估（而不是等到第二天早上）。持续评估的结果是恢复的关键。在某些情况下，我不得不评估特定行动是否对我有害。有一次我摔断了脚趾（撞在书架的角落上），我立即制定了策略：我能走路吗？如果跑步会感到疼痛吗？对于所有这些问题，答案都是肯定的。那天下午我试着去游泳。接下来的几周，我不断检查是否能够在没有疼痛的情况下行走。疼痛不是敌人。它是帮助你决定是继续做你正在做的事情，还是停下来重新思考当前策略的指示器。
- en: Constant evaluation, making changes and adapting to the feedback, and applying
    new strategies to achieve success is exactly what continuous integration (CI)
    and continuous delivery (CD) are about. Even today, where information about robust
    deployment strategies is easily available, you often encounter businesses without
    tests or a poor testing strategy to ensure a product is ready for a new release
    or even releases that take weeks (and months!). I recall trying to cut a new release
    of a major open source project, and there were times it would take close to a
    week. Even worse, the Quality Assurance (QA) lead would send emails to every team
    lead and ask them if they felt ready for a release or wanted more changes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不断评估、做出改变并根据反馈进行调整，以及应用新策略以实现成功，这正是持续集成（CI）和持续交付（CD）的核心内容。即使在如今，强大的部署策略信息易得，仍然经常遇到没有测试或测试策略薄弱的企业，以确保产品能够在新版本发布时准备就绪，甚至发布需要数周（甚至数月）的情况。我还记得，尝试发布一个重要的开源项目新版本时，有时需要接近一周的时间。更糟糕的是，质量保证（QA）负责人会给每位团队负责人发送电子邮件，询问他们是否准备好发布或者是否需要进行更多改动。
- en: Sending emails around and waiting for different replies is not a straightforward
    way to release software. It is prone to error and highly inconsistent. The feedback
    loop that CI/CD platforms and steps grant you and your team is invaluable. If
    you find a problem, you must automate it away and make it not-a-problem for the
    next release. Constant evaluation, just like injuries with high-performing athletes,
    is a core pillar of DevOps and absolutely critical for successful machine learning
    operationalization.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 发送电子邮件并等待不同回复并不是发布软件的简单方法。它容易出错且高度不一致。CI/CD平台和步骤赋予你和你的团队的反馈循环是无价的。如果发现问题，你必须自动化解决，并确保在下一次发布中不再成为问题。持续评估，就像高水平运动员受伤一样，是DevOps的核心支柱，对于成功的机器学习运营至关重要。
- en: I like the description of continuous as persistence or recurrence of a process.
    CI/CD are usually mentioned together when talking about the system that builds,
    verifies, and deploys artifacts. In this chapter, I will detail what a robust
    process looks like and how you can enable various strategies to implement (or
    improve) a pipeline to ship models into production.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢将持续描述为一个过程的持久性或再现性的描述。在谈论构建、验证和部署工件的系统时，通常会一起提到CI/CD。在本章中，我将详细介绍一个强大流程的外观，以及如何实施（或改进）流水线来将模型投入生产。
- en: Packaging for ML Models
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型的打包
- en: It wasn’t that long ago I heard about packaging ML models for the first time.
    If you’ve never heard about packaging models before, it’s OK—this is all fairly
    recent, and *packaging* here doesn’t mean some special type of operating system
    package like an RPM (Red Hat Package Manager) or DEB (Debian Package) file with
    special directives for bundling and distribution. This all means getting a model
    into a container to take advantage of containerized processes to help sharing,
    distributing, and easy deployment. I’ve already described containerization in
    detail in [“Containers”](ch03.xhtml#Section-containers) and why it makes sense
    to use them for operationalizing machine learning versus using other strategies
    like virtual machines, but it is worth reiterating that the ability to quickly
    try out a model from a container regardless of the operating system is a dream
    scenario come true.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不久前，我第一次听说打包 ML 模型的事情。如果你以前从未听说过打包模型，没关系——这都是最近的事情，这里的*打包*并不意味着一些特殊类型的操作系统包，比如带有用于捆绑和分发的特殊指令的
    RPM（Red Hat Package Manager）或 DEB（Debian Package）文件。这一切意味着将模型放入容器中，以利用容器化进程来帮助共享、分发和轻松部署。我已经在[“容器”](ch03.xhtml#Section-containers)中详细描述了容器化，并解释了为什么在操作化机器学习时使用它们比使用其他策略如虚拟机更有意义，但值得重申的是，能够快速从容器中尝试模型，而不受操作系统影响，是一个梦想般的场景。
- en: 'There are three characteristics of packaging ML models into containers that
    are significant to go over:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将 ML 模型打包到容器中有三个重要的特性需要讨论：
- en: As long as a container runtime is installed, it is effortless to run a container
    locally.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只要安装了容器运行时，本地运行容器就很简单。
- en: There are plenty of options to deploy a container in the cloud, with the ability
    to scale up or down as needed.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有很多选项可以在云中部署容器，根据需要进行横向或纵向扩展。
- en: Others can quickly try it out with ease and interact with the container.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他人可以轻松尝试并与容器进行交互。
- en: The benefits of these characteristics are that maintainability becomes less
    complicated, and debugging a nonperformant model locally (or in a cloud offering
    even) can be as simple as a few commands in a terminal. The more complicated the
    deployment strategy is, the more difficult it will be to troubleshoot and investigate
    potential issues.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性的好处在于维护变得不那么复杂，即使在本地或云服务中调试性能不佳的模型，也只需在终端输入几个命令即可。部署策略越复杂，故障排除和问题调查就会越困难。
- en: 'For this section, I will use an ONNX model and package it within a container
    that serves a Flask app that performs the prediction. I will use the [RoBERTa-SequenceClassification](https://oreil.ly/DbzOM)
    ONNX model, which is very well documented. After creating a new Git repository,
    the first step is to figure out the dependencies needed. After creating the Git
    repository, start by adding the following *requirements.txt* file:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，我将使用一个 ONNX 模型，并将其打包到一个容器中，用于提供执行预测的 Flask 应用。我将使用[RoBERTa-SequenceClassification](https://oreil.ly/DbzOM)
    ONNX 模型，该模型有很好的文档支持。创建新的 Git 仓库后，第一步是确定所需的依赖项。创建 Git 仓库后，首先添加以下*requirements.txt*文件：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, create a Dockerfile that installs everything in the container:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个 Dockerfile，在容器中安装所有内容：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The Dockerfile copies the requirements file, creates a *webapp* directory,
    and copies the application code into a single *app.py* file. Create the *webapp/app.py*
    file to perform the sentiment analysis. Start by adding the imports and everything
    needed to create an ONNX runtime session:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Dockerfile 复制了要求文件，创建了一个*webapp*目录，并将应用程序代码复制到一个名为*app.py*的文件中。创建*webapp/app.py*文件来执行情感分析。首先添加导入和创建
    ONNX 运行时会话所需的所有内容：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This first part of the file creates the Flask application, defines the tokenizer
    to use with the model, and finally, it initializes an ONNX runtime session that
    requires passing a path to the model. There are quite a few imports that aren’t
    used yet. You will make use of those next when adding the Flask route to enable
    the live inferencing:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的第一部分创建了 Flask 应用程序，定义了与模型一起使用的分词器，最后初始化了一个需要传递模型路径的 ONNX 运行时会话。有几个未使用的导入将在添加
    Flask 路由以启用实时推理时使用：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `predict()` function is a Flask route that enables the */predict* URL when
    the application is running. The function only allows `POST` HTTP methods. There
    is no description of the sample inputs and outputs yet because one critical part
    of the application is missing: the ONNX model does not exist yet. Download the
    [RoBERTa-SequenceClassification](https://oreil.ly/Pjvit) ONNX model locally, and
    place it at the root of the project. This is how the final project structure should
    look:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()` 函数是 Flask 路由，当应用程序运行时，启用 */predict* URL。该函数仅允许 `POST` HTTP 方法。尚未描述样本输入和输出，因为应用程序的一个关键部分尚未完成：ONNX
    模型尚不存在。本地下载 [RoBERTa-SequenceClassification](https://oreil.ly/Pjvit) ONNX 模型，并将其放置在项目的根目录。这是最终项目结构的样子：'
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'One last thing missing before building the container is that there is no instruction
    to copy the model into the container. The *app.py* file requires the model *roberta-sequence-classification-9.onnx*
    to exist in the */webapp* directory. Update the *Dockerfile* to reflect that:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建容器之前，最后一件事情是没有指令将模型复制到容器中。*app.py* 文件要求模型 *roberta-sequence-classification-9.onnx*
    存在于 */webapp* 目录中。更新 *Dockerfile* 反映这一点：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now the project has everything needed so you can build the container and run
    the application. Before building the container, let’s double-check everything
    works. Create a new virtual environment, activate it, and install all the dependencies:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在项目已经具备一切所需，因此可以构建容器并运行应用程序。在构建容器之前，让我们再次检查一切是否正常工作。创建一个新的虚拟环境，激活它，并安装所有依赖项：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The ONNX model exists at the root of the project, but the application wants
    it in the */webapp* directory, so move it inside that directory so that the Flask
    app doesn’t complain (this extra step is not needed when the container runs):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX 模型存在于项目的根目录，但应用程序希望它位于 */webapp* 目录中，因此将其移动到该目录中，以避免 Flask 应用程序抱怨（当容器运行时不需要此额外步骤）：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now run the application locally by invoking the *app.py* file with Python:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在通过使用 Python 调用 *app.py* 文件在本地运行应用程序：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, the application is ready to consume HTTP requests. So far, I’ve not shown
    what the expected inputs are. These are going to be JSON-formatted requests with
    JSON responses. Use the *curl* program to send a sample payload to detect sentiment:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，应用程序已准备好接收 HTTP 请求。到目前为止，我还没有展示预期的输入是什么。这些将是 JSON 格式的请求和 JSON 格式的响应。使用 *curl*
    程序发送一个示例负载以检测情感：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The JSON request is an array with a single string, and the response is a JSON
    object with a “positive” key that indicates the sentiment of the sentence. Now
    that you’ve verified that the application runs and that the live prediction is
    functioning properly, it is time to create the container locally to verify all
    works there. Create the container, and tag it with something meaningful:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 请求是一个包含单个字符串的数组，响应是一个具有指示句子情感的“positive”键的 JSON 对象。现在您已经验证应用程序正在运行，并且实时预测功能正常工作，是时候在本地创建容器以验证所有工作情况了。创建容器，并标记它为有意义的东西：
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now run the container locally to interact with it in the same way as when running
    the application directly with Python. Remember to map the ports of the container
    to the localhost:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在本地运行容器，以与直接使用 Python 运行应用程序时相同的方式进行交互。记得将容器的端口映射到本地主机：
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Send an HTTP request in the same way as before. You can use the *curl* program
    again:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以与之前相同的方式发送 HTTP 请求。您可以再次使用 *curl* 程序：
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We’ve gone through many steps to package a model and get it inside a container.
    Some of these steps might seem overwhelming, but challenging processes are a perfect
    opportunity to automate and leverage continuous delivery patterns. In the next
    section, I’ll automate all of this using continuous delivery and publishing this
    container to a container registry that anyone can consume.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经历了许多步骤来打包模型并将其放入容器中。其中一些步骤可能显得令人不知所措，但具有挑战性的过程正是自动化和利用持续交付模式的绝佳机会。在下一节中，我将使用持续交付来自动化所有这些，并将此容器发布到任何人都可以消费的容器注册表中。
- en: Infrastructure as Code for Continuous Delivery of ML Models
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于 ML 模型持续交付的基础设施即代码
- en: Recently at work, I saw that a few test container images existed in a public
    repository, which were widely used by the test infrastructure. Having images hosted
    in a container registry (like Docker Hub) is already a great step in the right
    direction for repeatable builds and reliable tests. I encountered a problem with
    one of the libraries used in a container that needed an update, so I searched
    for the files used to create these test containers. They were nowhere to be found.
    At some point, an engineer built these locally and uploaded the images to the
    registry. This presented a big problem because I couldn’t make a simple change
    to the image since the files needed to build the image were lost.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在工作中，我发现公共仓库中存在一些测试容器映像，这些映像被测试基础设施广泛使用。在容器注册表（如Docker Hub）中托管映像已经是朝着可重复构建和可靠测试的正确方向迈出了一大步。我遇到了一个问题，其中一个测试容器中使用的库需要更新，因此我搜索用于创建这些测试容器的文件。但我找不到它们。某个工程师曾在本地构建这些映像并上传到注册表中。这带来了一个大问题，因为我无法简单地更改映像，因为用于构建映像的文件已丢失。
- en: Experienced container developers can find a way to get most (if not all) files
    to rebuild the container, but that is beside the point. A step forward in this
    problematic situation is to create automation that can automatically build these
    containers from known source files, including the *Dockerfile*. Rebuilding or
    solving the problem to update the container and re-upload to the registry is like
    finding candles and flashlights in a blackout, instead of having a generator that
    starts automatically as soon as the power goes away. Be highly analytical when
    situations like the one I just described happens. Rather than pointing fingers
    and blaming others, use these as an opportunity to enhance the process with automation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 经验丰富的容器开发者可以找到一种方法，获取大多数（如果不是全部）文件以重建容器，但这并不是重点。在这种问题情境中迈出的一步是创建能够自动从已知源文件（包括*Dockerfile*）构建这些容器的自动化。重新构建或解决更新容器并重新上传到注册表的问题，就像在停电时找到蜡烛和手电筒，而不是拥有在电力中断时自动启动的发电机一样。当发生类似我刚描述的情况时，请保持高度分析性。与其互相指责和责怪他人，不如将其视为提升过程中自动化的机会。
- en: 'The same problem happens in machine learning. We tend to grow easily accustomed
    to things being manual (and complex!), but there is always an opportunity to automate.
    This section will not go over all the steps needed in containerization again (already
    covered in [“Containers”](ch03.xhtml#Section-containers)), but I will go into
    the details needed to automate everything. Let’s assume we’re in a similar situation
    to the one I just described and that someone has created a container with a model
    that lives in Docker Hub. Nobody knows how the trained model got into the container;
    there are no docs, and updates are needed. Let’s add a slight complexity: the
    model is not in any repository to be found, but it lives in Azure as a registered
    model. Let’s get some automation going to solve this problem.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中也存在同样的问题。我们往往很容易习惯于手动操作（和复杂的！），但始终存在自动化的机会。本节不会再重复所有容器化所需的步骤（已在[“容器”](ch03.xhtml#Section-containers)中涵盖），但我将详细介绍自动化所需的细节。让我们假设我们处于与我刚描述的类似情况，并且有人已经创建了一个包含在Docker
    Hub中的模型的容器。没有人知道训练模型是如何进入容器的；没有文档，需要更新。让我们增加一些复杂性：模型不在任何可找到的仓库中，而是作为注册模型存在于Azure中。让我们进行一些自动化操作来解决这个问题。
- en: Warning
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It might be tempting to add models into a GitHub repository. Although this is
    certainly possible, GitHub has (at the time of this writing) a hard file limit
    of 100 MB. If the model you are trying to package is close to that size, you might
    not be able to add it to the repository. Further, Git (the version control system)
    is not meant to handle versioning of binary files and has the side-effect of creating
    huge repositories because of this.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会有诱惑将模型添加到GitHub仓库中。尽管这确实是可能的，但GitHub（截至本文写作时）有着100 MB的硬文件大小限制。如果您尝试打包的模型接近这个大小，可能无法将其添加到仓库中。此外，Git（版本控制系统）并不适合处理二进制文件的版本控制，这会导致仓库因此而变得庞大。
- en: In the current problem scenario, the model is available on the Azure ML platform
    and previously registered. I didn’t have one already, so I quickly registered
    [RoBERTa-SequenceClassification](https://oreil.ly/oNgCD) using Azure ML Studio.
    Click the Models section and then “Register model” as shown in [Figure 4-1](#Figure-4-1).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的问题场景中，模型在 Azure ML 平台上是可用的并且先前已注册。我之前没有一个，因此我迅速使用 Azure ML Studio 注册了 [RoBERTa-SequenceClassification](https://oreil.ly/oNgCD)。点击“模型”部分，然后如
    [图 4-1](#Figure-4-1) 所示，“注册模型”。
- en: '![pmlo 0401](Images/pmlo_0401.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0401](Images/pmlo_0401.png)'
- en: Figure 4-1\. Azure model registering menu
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. Azure 模型注册菜单
- en: Fill out the form shown in [Figure 4-2](#Figure-4-2) with the necessary details.
    In my case, I downloaded the model locally and need to upload it using the “Upload
    file” field.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 填写 [图 4-2](#Figure-4-2) 中显示的表单，并附上必要的细节。在我的情况下，我将模型下载到本地，需要使用“上传文件”字段上传它。
- en: '![pmlo 0402](Images/pmlo_0402.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0402](Images/pmlo_0402.png)'
- en: Figure 4-2\. Azure model registering form
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. Azure 模型注册表单
- en: Note
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you want to know more about registering a model in Azure, I cover how to
    do that with the Python SDK in [“Registering Models”](ch08.xhtml#Section-Registering-Models).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果想了解如何在 Azure 中注册模型的更多信息，我在 [“注册模型”](ch08.xhtml#Section-Registering-Models)
    中使用 Python SDK 讲解了如何做。
- en: 'Now that the pretrained model is in Azure let’s reuse the same project from
    [“Packaging for ML Models”](#Section-packaging-models). All the heavy lifting
    to perform the (local) live inferencing is done, so create a new GitHub repository
    and add the project contents *except* for the ONNX model. Remember, there is a
    size limit for files in GitHub, so it isn’t possible to add the ONNX model into
    the GitHub repo. Create a *.gitigore* file to ignore the model and prevent adding
    it by mistake:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在预训练模型已经在 Azure 中，让我们重用 [“打包 ML 模型”](#Section-packaging-models) 中相同的项目。所有执行（本地）实时推理的重活都已完成，因此创建一个新的
    GitHub 仓库并添加项目内容，*除了* ONNX 模型。请记住，GitHub 中的文件有大小限制，因此无法将 ONNX 模型添加到 GitHub 仓库中。创建一个
    *.gitignore* 文件来忽略模型，防止误添加：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After pushing the contents of the Git repository without the ONNX model, we
    are ready to start automating the model creation and delivery. To do this, we
    will use GitHub Actions, which allows us to create a continuous delivery workflow
    in a YAML file that gets triggered when configurable conditions are met. The idea
    is that whenever the repository has a change in the main branch, the platform
    will pull the registered model from Azure, create the container, and lastly, it
    will push it to a container registry. Start by creating a *.github/workflows/*
    directory at the root of your project, and then add a *main.yml* that looks like
    this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在将不包含 ONNX 模型的 Git 仓库内容推送后，我们已准备好开始自动化模型的创建和交付。为此，我们将使用 GitHub Actions，在触发满足可配置条件的情况下，从
    Azure 获取已注册的模型，创建容器，最后将其推送到容器注册表。首先，在项目的根目录下创建一个 *.github/workflows/* 目录，然后添加一个
    *main.yml* 文件，内容如下：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The configuration so far doesn’t do anything other than defining the action.
    You can define any number of jobs, and in this case, we define a *build* job that
    will put everything together. Append the following to the *main.yml* file you
    previously created:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止的配置除了定义操作外什么也没做。您可以定义任意数量的作业，在这种情况下，我们定义一个 *build* 作业来整合所有内容。将以下内容附加到您之前创建的
    *main.yml* 文件中：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The build job has many steps. In this case, each step has a distinct task, which
    is an excellent way to separate failure domains. If everything were in a single
    script, it would be more difficult to grasp potential issues. The first step is
    to check out the repository when the action triggers. Next, since the ONNX model
    doesn’t exist locally, we need to retrieve it from Azure, so we must authenticate
    using the Azure action. After authentication, the *az* tool is made available,
    and you must attach the folder for your workspace and group. Finally, the job
    can retrieve the model by its ID.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 构建作业有许多步骤。在这种情况下，每个步骤都有一个明确的任务，这是分离失败域的一个很好方法。如果所有内容都在单个脚本中，那么找出潜在问题将更加困难。第一步是当操作触发时检出仓库。接下来，由于
    ONNX 模型在本地不存在，我们需要从 Azure 检索它，因此必须使用 Azure 动作进行身份验证。身份验证后，*az* 工具就可用了，您必须附加工作区和组的文件夹。最后，作业可以通过其
    ID 检索模型。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Some steps in the YAML file have a `uses` directive, which identifies what external
    action (for example `actions/checkout`) and at what version. Versions can be branches
    or published tags of a repository. In the case of `checkout` it is the `v2` tag.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 文件中的某些步骤具有 `uses` 指令，该指令标识了外部操作（例如 `actions/checkout`）及其版本。版本可以是存储库的分支或发布的标签。对于
    `checkout` 来说，是 `v2` 标签。
- en: Once all those steps complete, the RoBERTa-Sequence model should be at the root
    of the project, enabling the next steps to build the container properly.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有这些步骤完成，RoBERTa-Sequence 模型应该位于项目的根目录，从而使下一步能够正确构建容器。
- en: The workflow file is using `AZURE_CREDENTIALS`. These are used with a special
    syntax that allows the workflow to retrieve secrets configured for the repository.
    These credentials are the service principal information. If you aren’t familiar
    with a service principal, this is covered in the [“Authentication”](ch08.xhtml#Section-azure-authentication).
    You will need the service principal’s configuration that has access to the resources
    in the workspace and group where the model lives. Add the secret on your GitHub
    repository by going to Settings, then Secrets, and finally clicking the “New repository
    secret” link. [Figure 4-3](#Figure-4-3) shows the form you will be presented when
    adding a new secret.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流文件使用 `AZURE_CREDENTIALS`。这些与一个特殊语法一起使用，允许工作流检索为存储库配置的密钥。这些凭据是服务主体信息。如果您对服务主体不熟悉，可以在
    [“认证”](ch08.xhtml#Section-azure-authentication) 中找到相关信息。您需要配置有权访问模型所在的工作区和组的服务主体配置。通过转到设置，然后选择“秘密”，最后点击“新存储库秘密”链接，在
    GitHub 存储库上添加密钥。[图 4-3](#Figure-4-3) 显示了您在添加新密钥时会看到的表单。
- en: '![pmlo 0403](Images/pmlo_0403.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0403](Images/pmlo_0403.png)'
- en: Figure 4-3\. Add secret
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 添加密钥
- en: Commit and push your changes to your repository and then head to the Actions
    tab. A new run is immediately scheduled and should start running in a few seconds.
    After a few minutes, everything should’ve completed. In my case, [Figure 4-4](#Figure-4-4)
    shows it takes close to four minutes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将更改提交并推送到您的存储库，然后转到操作选项卡。新的运行会立即计划，并应在几秒钟内开始运行。几分钟后，所有事情应该已经完成。在我的情况下，[图 4-4](#Figure-4-4)显示需要接近四分钟。
- en: '![pmlo 0404](Images/pmlo_0404.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0404](Images/pmlo_0404.png)'
- en: Figure 4-4\. GitHub action success
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. GitHub 动作成功
- en: There are now quite a few moving parts to accomplish a successful job run. When
    designing a new set of steps (or pipelines, as I’ll cover in the next section),
    a good idea is to enumerate the steps and identify greedy steps. These *greedy
    steps* are steps that are trying to do too much and have lots of responsibility.
    At first glance, it is hard to identify any step that might be problematic. The
    process of maintaining a CI/CD job includes refining responsibilities of steps
    and adapting them accordingly.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，完成一个成功的作业运行需要考虑多个移动部件。在设计一组新步骤（或管道，如我将在下一节中介绍的），一个好主意是列出这些步骤并确定贪婪步骤。这些 *贪婪步骤*
    尝试做得太多，并且有很多责任。乍一看，很难识别可能存在问题的任何步骤。维护 CI/CD 作业的过程包括精细化步骤的责任并相应地调整它们。
- en: Once the steps are identified, you can break them down into smaller steps, which
    will help you understand the responsibility of each part faster. A faster understanding
    means easier debugging, and although it’s not immediately apparent, you will benefit
    from making this a habit.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了步骤，您可以将它们分解为更小的步骤，这将帮助您更快地理解每个部分的责任。更快的理解意味着更容易调试，尽管这并不立即明显，但养成这种习惯会给您带来好处。
- en: 'These are the steps we have for packaging the RoBERTa-Sequence model:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们打包 RoBERTa-Sequence 模型的步骤：
- en: Check out the current branch of the repository.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检出存储库的当前分支。
- en: Authenticate to Azure Cloud.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 身份验证到 Azure 云。
- en: Configure auto-install of Azure CLI extensions.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置自动安装 Azure CLI 扩展。
- en: Attach the folder to interact with the workspace.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件夹附加到与工作区交互。
- en: Download the ONNX model.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 ONNX 模型。
- en: Build the container for the current repo.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建当前存储库的容器。
- en: 'There is one final item missing, though, and that is to publish the container
    after it builds. Different container registries will require different options
    here, but most do support GitHub Actions, which is refreshing. Docker Hub is straightforward,
    and all it requires is to create a token and then save it as a GitHub project
    secret, along with your Docker Hub username. Once that is in place, adapt the
    workflow file to include the authentication step before building:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，还有一个最后缺少的项目，那就是在构建后发布容器。不同的容器注册表在此处需要不同的选项，但大多数都支持GitHub Actions，这是令人耳目一新的。Docker
    Hub非常简单，只需要创建一个令牌，然后将其保存为GitHub项目密钥，以及您的Docker Hub用户名。一旦设置完成，调整工作流文件以包括在构建之前的身份验证步骤：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Lastly, update the build step to use `push: true`.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，更新构建步骤以使用`push: true`。'
- en: Recently, GitHub has released a container registry offering as well, and its
    integration with GitHub Actions is straightforward. The same Docker steps can
    be used with minor changes and creating a PAT (Personal Access Token). Start by
    creating a PAT by going to your GitHub account settings, clicking Developer Settings,
    and finally “Personal access” tokens. Once that page loads, click “Generate new
    token.” Give it a meaningful description in the Note section, and ensure that
    the token has permissions to write packages as I do in [Figure 4-5](#Figure-4-5).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，GitHub也发布了一个容器注册表提供，并且它与GitHub Actions的集成非常直接。相同的Docker步骤可以在进行轻微更改并创建PAT（个人访问令牌）后使用。首先，通过转到您的GitHub账户设置，点击“开发者设置”，最后点击“个人访问”令牌来创建PAT。一旦页面加载完成，点击“生成新的令牌”。在备注部分给它一个有意义的描述，并确保该令牌具有适当的权限，如我在[图 4-5](#Figure-4-5)中所做的那样。
- en: '![pmlo 0405](Images/pmlo_0405.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0405](Images/pmlo_0405.png)'
- en: Figure 4-5\. GitHub Personal Access Token
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. GitHub个人访问令牌
- en: 'Once you are done, a new page is presented with the actual token. This is the
    only time you will see the token in plain text, so make sure you copy it now.
    Next, go to the repository where the container code lives, and create a new repository
    secret, just like you did with the Azure service principal credentials. Name the
    new secret *GH_REGISTRY* and paste the contents of the PAT created in the previous
    step. Now you are ready to update the Docker steps to publish the package using
    the new token and GitHub’s container registry:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，会显示一个新页面，并呈现实际的令牌。这是您在明文中看到令牌的唯一时刻，因此确保现在复制它。接下来，转到包含容器代码的存储库，并创建一个新的存储库密钥，就像您在Azure服务主体凭据中所做的那样。将新密钥命名为*GH_REGISTRY*，并粘贴在前一步骤中创建的PAT的内容。现在，您已准备好更新Docker步骤，以使用新令牌和GitHub的容器注册表发布包：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In my case, *alfredodeza* is my GitHub account, so I can tag with it along with
    the *flask-roberta* name of the repository. These will need to match according
    to your account and repository. After pushing the changes to the main branch (or
    after merging if you made a pull request), the job will trigger. The model should
    get pulled in from Azure, packaged within the container, and finally published
    as a GitHub Package in its container registry offering, looking similar to [Figure 4-6](#Figure-4-6).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，*alfredodeza* 是我的GitHub账号，所以我可以标记它以及*flask-roberta*仓库的名称。这些需要与您的账号和仓库相匹配。将更改推送到主分支（或合并拉取请求后），作业将被触发。模型应从Azure中拉取，打包到容器中，并最终作为GitHub包发布到其容器注册表中，看起来类似于[图 4-6](#Figure-4-6)。
- en: '![pmlo 0406](Images/pmlo_0406.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0406](Images/pmlo_0406.png)'
- en: Figure 4-6\. GitHub Package container
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. GitHub包容器
- en: 'Now that the container is packaging and distributing the ONNX model in a fully
    automated fashion by leveraging GitHub’s CI/CD offering and container registry,
    we have solved the problematic scenario I assumed at the beginning of the chapter:
    a model needs to get packaged in a container, but the container files are not
    available. In this way, you are providing clarity to others and to the process
    itself. It is segmented into small steps, and it allows any updates to be done
    to the container. Finally, the steps publish the container to a selected registry.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，容器正在通过利用GitHub的CI/CD提供和容器注册表以完全自动化的方式打包和分发ONNX模型，我们解决了本章开始时我假设的问题情景：模型需要打包到容器中，但容器文件不可用。通过这种方式，您为其他人和过程本身提供了清晰度。它被分成了小步骤，允许对容器进行任何更新。最后，这些步骤将容器发布到选择的注册表中。
- en: You can accomplish quite a few other things with CI/CD environments besides
    packaging and publishing a container. CI/CD platforms are the foundation for automation
    and reliable results. In the next section, I go into other ideas that work well
    regardless of the platform. By being aware of general patterns available in other
    platforms, you can take advantage of those features without worrying about the
    implementations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 除了打包和发布容器外，CI/CD 环境还可以完成很多其他任务。CI/CD 平台是自动化和可靠结果的基础。在下一节中，我将介绍一些在任何平台上都适用的好的想法。通过了解其他平台中可用的一般模式，你可以利用这些功能，而不必担心实现细节。
- en: Using Cloud Pipelines
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用云流水线
- en: The first time I heard about pipelines, I thought of them as more advanced than
    the typical scripting pattern (a procedural set of instructions representing a
    build). But pipelines aren’t advanced concepts at all. If you’ve dealt with shell
    scripts in any continuous integration platform, then a pipeline will seem straightforward
    to use. A pipeline is nothing more than a set of steps (or instructions) that
    can achieve a specific objective like publishing a model into a production environment
    when run. For example, a pipeline with three steps to train a model can be as
    simple as [Figure 4-7](#Figure-4-7).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次听说流水线时，我认为它们比典型的脚本模式（代表构建的一组过程化指令）更高级。但事实上，流水线并不是什么高级概念。如果你在任何持续集成平台中处理过
    shell 脚本，那么使用流水线将会非常直观。流水线只不过是一组步骤（或指令），可以在运行时实现特定目标，例如，在生产环境中发布模型。例如，一个包含三个步骤来训练模型的流水线可以像图
    4-7 中所示那样简单。
- en: '![pmlo 0407](Images/pmlo_0407.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0407](Images/pmlo_0407.png)'
- en: Figure 4-7\. Simple pipeline
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 简单流水线
- en: You could represent the same pipeline as a shell script that does all three
    things at once. There are multiple benefits with a pipeline that separates concerns.
    When each step has a specific responsibility (or concern), it is easier to grasp.
    If a single-step pipeline that retrieves the data, validates it, and trains the
    model is failing, it isn’t immediately clear why that might fail. Indeed you can
    dive into the details, look at logs, and check the actual error. If you separate
    the pipeline into three steps and the *train model* step is failing, you can narrow
    the failure’s scope and get to a possible resolution faster.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将同样的流水线表示为一个 shell 脚本，它可以同时执行所有三个操作。使用流水线将关注点分离有多个好处。当每个步骤都有特定的责任（或关注点）时，更容易理解。如果一个单步流水线用于获取数据、验证数据并训练模型失败了，那么失败的原因并不明显。确实，你可以深入细节，查看日志，并检查实际错误。如果你将流水线分成三个步骤，并且“训练模型”步骤失败了，你可以缩小失败范围，并更快地找到可能的解决方案。
- en: Tip
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'One general recommendation that you can apply to the many aspects of operationalizing
    machine learning is to consider making any operation more straightforward for
    a future failure situation. Avoid being tempted to go fast and get a pipeline
    (like in this case) deployed and running in a single step because it is easier.
    Take the time to reason about what would make it easier for you (and others) to
    build ML infrastructure. When a failure does happen, and you identify problematic
    aspects, go back to the implementation and improve it. You can apply the concepts
    of CI/CD to improvement: continuous evaluation and improvement of processes is
    a sound strategy for robust environments.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的建议是，你可以将其应用到机器学习的多个方面中，考虑让任何操作在未来失败的情况下更加简单化。避免急于快速部署和运行一个流水线（就像在这种情况下一样），因为这样做更容易。花些时间思考，什么会让你（和其他人）更容易构建机器学习基础设施。当失败发生时，如果你识别出问题的方面，回到实现并进行改进。你可以应用
    CI/CD 的概念来进行改进：持续评估和改进流程是一个健全的策略，适用于稳健的环境。
- en: Cloud pipelines are no different from any continuous integration platform out
    there except that they are hosted or managed by a cloud provider.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 云流水线与任何其他持续集成平台并无不同，只是它们由云提供商托管或管理。
- en: 'Some definitions of CI/CD pipelines you can encounter try to define elements
    or parts of a pipeline rigidly. In reality, I think that the parts of the pipeline
    should be loosely defined and not constrained by definitions. RedHat [has a nice
    explanation of pipelines](https://oreil.ly/rlJUx) that describes five common elements:
    build, test, release, deploy, and validate. These elements are mostly for mix-and-match,
    not to strictly include them in the pipeline. For example, if the model you are
    building doesn’t need to get deployed, then there is no need to pursue a deploy
    step at all. Similarly, if your workflow requires extracting and preprocessing
    data, you need to implement it as another step.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有些CI/CD流水线的定义可能会试图严格定义流水线的元素或部分。实际上，我认为流水线的部分应该松散定义，不受定义的限制。RedHat在[这里](https://oreil.ly/rlJUx)有一个很好的解释，描述了五个常见元素：构建、测试、发布、部署和验证。这些元素主要用于混合和匹配，而不是严格包含在流水线中。例如，如果您正在构建的模型不需要部署，那么根本不需要执行部署步骤。同样地，如果您的工作流程需要提取和预处理数据，您需要将其实现为另一个步骤。
- en: Now that you are aware that a pipeline is basically the same as a CI/CD platform
    with several steps, it should be straightforward to apply machine learning operations
    to an actionable pipeline. [Figure 4-8](#Figure-4-8) shows a rather simplistic
    assumed pipeline, but this can involve several other steps as well, and like I’ve
    mentioned, these elements can be mixed and matched together to any number of operations
    and steps.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解到流水线基本上与具有多个步骤的CI/CD平台相同，将机器学习操作应用于可操作的流水线应该是直接的。[图 4-8](#Figure-4-8)显示了一个相当简单的假定流水线，但这也可以涉及其他几个步骤，正如我提到的那样，这些元素可以混合和匹配以执行任意数量的操作和步骤。
- en: '![pmlo 0408](Images/pmlo_0408.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0408](Images/pmlo_0408.png)'
- en: Figure 4-8\. Involved pipeline
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 涉及的流水线
- en: AWS SageMaker does an outstanding job of providing examples that are ready to
    use out of the box for crafting involved pipelines that include everything you
    need to run several steps. SageMaker is a specialized machine learning platform
    that goes beyond offering steps in a pipeline to accomplish a goal like publishing
    a model. Since it is specialized for machine learning, you are exposed to features
    that are particularly important for getting models into production. Those features
    don’t exist in other common platforms like GitHub Actions, or if they do, they
    aren’t as well thought out because the primary goal of platforms like GitHub Actions
    or Jenkins isn’t to train machine learning models but rather be as generic as
    possible to accommodate for most common use cases.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SageMaker出色地提供了准备好使用的示例，用于创建涉及的流水线，其中包括运行多个步骤所需的所有内容。SageMaker是一个专门的机器学习平台，不仅提供流水线中的步骤来完成发布模型等目标。由于它专门用于机器学习，您可以接触到对于将模型投入生产非常重要的功能。这些功能在其他常见平台如GitHub
    Actions中并不存在，或者如果存在的话，它们的考虑可能不如此周到，因为GitHub Actions或Jenkins等平台的主要目标不是训练机器学习模型，而是尽可能通用，以适应大多数常见用例。
- en: Another crucial problem that is somewhat hard to solve is that specialized machines
    for training (for example, GPU-intensive tasks) are just not available or hard
    to configure in a generic pipeline offering.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相当难以解决的关键问题是，专门用于训练的机器（例如，GPU密集型任务）在通用流水线提供中要么根本不可用，要么难以配置。
- en: Open SageMaker Studio and head over to the Components and Registries section
    on the left sidebar and select Projects. Several SageMaker project templates show
    up to choose from, as shown in [Figure 4-9](#Figure-4-9).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 打开SageMaker Studio并转到左侧边栏的“组件和注册表”部分，然后选择“项目”。显示多个SageMaker项目模板供您选择，如[图 4-9](#Figure-4-9)所示。
- en: '![pmlo 0409](Images/pmlo_0409.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0409](Images/pmlo_0409.png)'
- en: Figure 4-9\. SageMaker templates
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-9\. SageMaker模板
- en: Note
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Although the examples are meant to get you started, and Jupyter Notebooks are
    provided, they are great for learning more about the steps involved and how to
    change and adapt them to your specific needs. After creating a pipeline instance
    in SageMaker, training, and finally registering the model, you can browse through
    the parameters for the pipeline, like in [Figure 4-10](#Figure-4-10).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些示例旨在帮助您入门，并提供了Jupyter Notebooks，但它们非常适合了解涉及的步骤以及如何更改和适应它们以满足您的特定需求。在SageMaker中创建流水线实例后，训练并最终注册模型，您可以浏览流水线的参数，就像在[图 4-10](#Figure-4-10)中展示的那样。
- en: '![pmlo 0410](Images/pmlo_0410.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0410](Images/pmlo_0410.png)'
- en: Figure 4-10\. Pipeline parameters
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. 流水线参数
- en: Another crucial part of the pipeline that shows all the steps involved is also
    available, as shown in [Figure 4-11](#Figure-4-11).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个展示所有步骤的流水线的关键部分也是可用的，如 [图 4-11](#Figure-4-11) 所示。
- en: '![pmlo 0411](Images/pmlo_0411.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0411](Images/pmlo_0411.png)'
- en: Figure 4-11\. SageMaker pipeline
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-11\. SageMaker 流水线
- en: As you can see, preparing data, training, evaluating, and registering a model
    are all part of the pipeline. The main objective is to register the model to deploy
    it later for live inferencing after packaging. Not all the steps need to be captured
    in this particular pipeline, either. You can craft other pipelines that can run
    whenever there is a newly registered model available. That way, that pipeline
    is not tied to a particular model, but rather, you can reuse it for any other
    model that gets trained successfully and registered. Reusability of components
    and automation is another critical component of DevOps that works well when applied
    to MLOps.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，准备数据、训练、评估和注册模型都是流水线的一部分。主要目标是注册模型，以便稍后在打包后进行实时推断部署。并非所有步骤都需要在这个特定的流水线中捕获。您可以创建其他流水线，每当有新注册的模型可用时运行。这样，该流水线就不是针对特定模型的，而是可以重复使用于任何成功训练和注册的模型。在应用于
    MLOps 时，组件的可重用性和自动化是 DevOps 的另一个关键组成部分。
- en: Now that pipelines are demystified, we can see certain enhancements that can
    make them more robust by manually controlling rolling out models or even switching
    inferencing from one model to another.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在流水线被揭秘，我们可以看到一些增强可以使它们更加健壮，通过手动控制模型的逐步部署，甚至切换推断从一个模型到另一个模型。
- en: Controlled Rollout of Models
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型的控制性逐步部署
- en: There are a few concepts from web service deployments that map nicely into strategies
    for deploying models into production environments, like creating several instances
    of a live inferencing application for scalability and progressively switching
    from an older to a newer model. Before going into some of the details that encompass
    the control part of rolling out models into production, it is worth describing
    the strategies where these concepts come into play.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些从 Web 服务部署中映射到将模型部署到生产环境中的策略的概念，例如为可扩展性创建几个实例的实时推断应用程序，并逐步从旧模型切换到新模型。在深入讨论涵盖部署模型控制部分的细节之前，值得描述一下这些概念可以发挥作用的策略。
- en: 'I’ll discuss two of these strategies in detail in this section. Although these
    strategies are similar, they have particular behavior that you can take advantage
    of when deploying:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在本节详细讨论这两种策略。虽然这些策略相似，但它们有特定的行为，您在部署时可以利用：
- en: Blue-green deployment
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: Canary deployment
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金丝雀部署
- en: A blue-green deployment is a strategy that gets a new version into the staging
    environment identical to production. Sometimes this staging environment is the
    same as production, but traffic is routed differently (or separately). Without
    going into details, Kubernetes is a platform that allows for this type of deployment
    with ease, since you can have the two versions in the same Kubernetes cluster,
    but routing the traffic to a separate address for the newer (“blue”) version while
    production traffic is still going into the older (“green”). The reason for this
    separation is that it allows further testing and assurance that the new model
    is working as expected. Once this verification is complete and certain conditions
    are satisfactory, you modify the configuration to switch traffic from the current
    model to the new one.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署是一种策略，将新版本部署到与生产环境完全相同的暂存环境中。有时，此暂存环境与生产环境相同，但流量路由不同（或分开）。不详细介绍，Kubernetes
    是一个允许进行这种类型部署的平台，因为您可以在同一个 Kubernetes 集群中拥有两个版本，但将流量路由到新版本的不同地址（“蓝色”），同时生产流量仍然进入较旧版本（“绿色”）。分离的原因在于它允许进一步测试和确保新模型按预期工作。一旦验证完成并且满足某些条件，您可以修改配置以将流量从当前模型切换到新模型。
- en: There are some issues with blue-green deployments, primarily associated with
    how complicated it can be to replicate production environments. Again, this is
    one of those situations where Kubernetes is a perfect fit since the cluster can
    accommodate the same application with different versions with ease.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署存在一些问题，主要与复制生产环境的复杂性有关。再次强调，这是 Kubernetes 完美适配的情况之一，因为集群可以轻松容纳同一应用的不同版本。
- en: A canary deployment strategy is a bit more involved and somewhat riskier. Depending
    on your level of confidence and the ability to progressively change configuration
    based on constraints, it is a sound way to send models into production. In this
    case, traffic is routed progressively to the newer model *at the same time the
    previous model is serving predictions*. So the two versions are live and processing
    requests simultaneously, but doing them in different ratios. The reason for this
    percentage-based rollout is that you can enable metrics and other checks to capture
    problems in real time, allowing you to roll back immediately if conditions are
    unfavorable.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署策略稍微复杂，并且有一定的风险性。根据你的信心水平和根据约束逐步改变配置的能力，这是将模型发送到生产的合理方式。在这种情况下，流量逐步路由到新模型，*同时之前的模型正在提供预测*。因此，两个版本都是活动的并且同时处理请求，但是以不同的比例进行。选择百分比增长的原因是你可以启用指标和其他检查来实时捕捉问题，如果条件不利，可以立即回滚。
- en: For example, assume that a new model with better accuracy and no noted drift
    is ready to get into production. After several instances of this new version are
    available to start receiving traffic, make a configuration change to send 10%
    of all traffic to the new version. While traffic starts to get routed, you notice
    a dismal amount of errors from responses. The HTTP 500 errors indicate that the
    application has an internal error. After some investigation, it shows that one
    of the Python dependencies that do the inferencing is trying to import a module
    that has been moved, causing an exception. If the application receives one hundred
    requests per minute, only ten of those would’ve experienced the error condition.
    After noticing the errors, you quickly change the configuration to send all traffic
    to the older version currently deployed. This operation is also referred to as
    a *rollback*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一个新的精度更高且没有漂移的模型准备投入生产。在几个新版本实例可用以开始接收流量之后，进行配置更改以将所有流量的10%发送到新版本。当流量开始路由时，你会注意到响应中存在大量的错误。HTTP
    500错误表明应用程序存在内部错误。经过一些调查，发现进行推理的一个Python依赖项尝试导入已移动的模块，导致异常。如果应用程序每分钟接收一百个请求，则只有十个请求会遇到错误条件。在注意到错误后，你迅速将配置更改为将所有流量发送到当前部署的旧版本。这个操作也被称为*回滚*。
- en: 'Most cloud providers have the ability to do a controlled rollout of models
    for these strategies. Although this is not a fully functional example, the Azure
    Python SDK can define the percentage of traffic for a newer version when deploying:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数云提供商都可以针对这些策略进行受控发布模型的能力。虽然这不是一个完全功能的示例，但Azure Python SDK在部署时可以定义新版本的流量百分比：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The tricky part is that a canary deployment’s objective is to progressively
    increase until the `traffic_percentile` is at 100%. The increase has to happen
    alongside meeting constraints about application healthiness and minimal (or zero)
    error rates.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 难点在于金丝雀部署的目标是逐步增加，直到`traffic_percentile`达到100%。增加必须同时满足应用程序健康和最小（或零）错误率的约束条件。
- en: Monitoring, logging, and detailed metrics of production models (aside from model
    performance) are absolutely critical for a robust deployment strategy. I consider
    them crucial for deployment, but they are a core pillar of the robust DevOps practices
    covered in [Chapter 6](ch06.xhtml#Chapter6). Besides monitoring, logging, and
    metrics that have their own chapter, there are other interesting things to check
    for continuous delivery. In the next section, we will see a few that make sense
    and increase the confidence of deploying a model into production.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 生产模型的监控、日志记录和详细指标（除了模型性能）对于强大的部署策略至关重要。我认为它们对部署至关重要，但它们是覆盖在[第6章](ch06.xhtml#Chapter6)中的强大DevOps实践的核心支柱。除了拥有自己章节的监控、日志记录和指标外，还有其他有趣的内容可以用来进行持续交付的检查。在接下来的部分中，我们将看到一些有意义的内容，并增加将模型部署到生产环境的信心。
- en: Testing Techniques for Model Deployment
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型部署的测试技术
- en: 'So far, the container built in this chapter works great and does exactly what
    we need: from some HTTP requests with a carefully crafted message in a JSON body,
    a JSON response predicts the sentiment. A seasoned machine learning engineer might
    have put accuracy and drift detection (covered in detail in [Chapter 6](ch06.xhtml#Chapter6))
    in place before getting to the model packaging stage. Let’s assume that’s already
    the case and concentrate on other helpful tests you can perform before deploying
    a model into production.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章构建的容器工作得非常好，确切地做了我们需要的事情：从一些带有精心制作的消息的HTTP请求中，JSON响应预测了情感。一位经验丰富的机器学习工程师可能在进入模型打包阶段之前就已经放置了准确性和漂移检测（在[第6章](ch06.xhtml#Chapter6)中详细讨论）。让我们假设已经这样，并集中精力在部署模型到生产之前可以执行的其他有用测试上。
- en: 'When you send an HTTP request to the container to produce a prediction, several
    software layers need to go through from start to end. At a high level, these are
    critical:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当您向容器发送HTTP请求以生成预测时，从开始到结束需要通过几个软件层。在高层次上，这些是关键的：
- en: Client sends an HTTP request, with a JSON body, in the form of an array with
    a single string.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端发送HTTP请求，带有JSON体，格式为单个字符串数组。
- en: A specific HTTP PORT (*5000*) and endpoint (*predict*) have to exist and get
    routed to.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 必须存在特定的HTTP端口（*5000*）和端点（*predict*），并进行路由。
- en: The Python Flask application has to receive the JSON payload and load it into
    native Python.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Python Flask应用程序必须接收JSON负载并将其加载到本机Python中。
- en: The ONNX runtime needs to consume the string and produce a prediction.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ONNX运行时需要消耗字符串并生成预测。
- en: A JSON response with an HTTP 200 response needs to contain the boolean value
    of the prediction.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个带有HTTP 200响应的JSON响应需要包含预测的布尔值。
- en: Every single one of these high-level steps can (and should) be tested.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 每一个这些高级步骤都可以（而且应该）进行测试。
- en: Automated checks
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化检查
- en: 'While putting together the container for this chapter, I got into some problems
    with the *onnxruntime* Python module: the documentation doesn’t pin (an exact
    version number) the version, which caused the latest version to get installed,
    which needed different arguments as input. The accuracy of the model was good,
    and I was not able to detect a significant drift. And yet, I deployed the model
    only to find it fully broken once requests were consumed.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在为本章组装容器时，我遇到了*onnxruntime* Python模块的一些问题：文档没有固定（确切的版本号），导致安装了最新版本，需要不同的输入参数。模型的准确性很好，但我无法检测到显著的漂移。然而，一旦请求被消耗，我部署的模型就完全破损了。
- en: With time, applications become better and more resilient. Another engineer might
    add error handling to respond with an error message when invalid inputs get detected,
    and perhaps with an HTTP response with an appropriate HTTP error code along with
    a nice error message that the client can understand. You must test out these types
    of additions and behaviors before allowing a model to ship into production.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，应用程序变得更加优秀和更加弹性。另一位工程师可能会添加错误处理以在检测到无效输入时响应错误消息，并可能使用适当的HTTP错误代码以及客户端可以理解的漂亮错误消息的HTTP响应。在允许模型进入生产之前，您必须测试这些类型的增加和行为。
- en: 'Sometimes there will be no HTTP error condition and no Python tracebacks either.
    What would happen if I made a change like the following to the JSON response:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 有时不会出现HTTP错误条件，也不会出现Python的回溯。如果我对JSON响应进行如下更改会发生什么：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Without looking back at the previous sections, can you tell the difference?
    The change would go unnoticed. The canary deployment strategy would go to 100%
    without any errors detected. The machine learning engineer would be happy with
    high accuracy and no drift. And yet, this change has completely broken the effectiveness
    of the model. If you haven’t caught the difference, that is OK. I encounter these
    types of problems all the time, and they can take me hours sometimes to detect
    the problem: instead of `false` (a boolean value), it is using `"false"` (a string).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在不回顾之前章节的情况下，你能分辨出差异吗？这种变化将不会被注意到。金丝雀部署策略将达到100%而没有检测到任何错误。机器学习工程师将对高准确率和无漂移感到满意。然而，这种变化完全破坏了模型的效果。如果你没有察觉到差异，那没关系。我经常遇到这些类型的问题，有时候可能需要花几个小时来检测问题：它使用的不是`false`（布尔值），而是`"false"`（字符串）。
- en: None of these checks should ever be manual; manual verification should be kept
    to a minimum. Automation should be a high priority, and the suggestions I’ve so
    far made can all be added as part of the pipeline. These checks can be generalized
    to other models for reuse, but at a high level, they can run in parallel as shown
    in [Figure 4-12](#Figure-4-12).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这些检查任何时候都不应该是手动的；手动验证应该尽量减少。自动化应该是一个高优先级的任务，到目前为止我提出的建议都可以作为流水线的一部分添加进去。这些检查可以泛化到其他模型以供重复使用，但在高层次上，它们可以像[图 4-12](#Figure-4-12)所示的方式并行运行。
- en: '![pmlo 0412](Images/pmlo_0412.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![pmlo 0412](Images/pmlo_0412.png)'
- en: Figure 4-12\. Automated checks
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-12\. 自动化检查
- en: Linting
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码检查
- en: 'Beyond some of the functional checks I mention, like sending HTTP requests,
    there are other checks closer to the code in the Flask app that are far simpler
    to implement, like using a linter ([I recommend Flake8 for Python](https://oreil.ly/MMs0C)).
    It would be best to automate all these checks to prevent getting into trouble
    when the production release needs to happen. Regardless of the development environment
    you are in, I *strongly* recommend enabling a linter for writing code. While creating
    the Flask application, I found errors as I adapted the code to work with HTTP
    requests. Here is a short example of the linter’s output:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我提到的一些功能性检查，比如发送HTTP请求，还有更接近Flask应用程序中代码的其他检查，比如使用一个代码检查工具（[我推荐Python使用Flake8](https://oreil.ly/MMs0C)）。最好自动化所有这些检查，以避免在生产发布时陷入麻烦。无论你处于什么样的开发环境，我*强烈*建议启用代码的代码检查工具。在创建Flask应用程序时，我在适应HTTP请求时发现了错误。这里是代码检查工具输出的一个简短示例：
- en: '[PRE20]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Undefined names break applications. In this case, I forgot to import the `RobertaTokenizer`
    from the `transformers` module. As soon as I realized this, I added the import
    and fixed it. This didn’t take me more than a few seconds.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 未定义的名称会破坏应用程序。在这种情况下，我忘记从`transformers`模块导入`RobertaTokenizer`。当我意识到这一点后，我添加了导入并修复了它。这不会花费我超过几秒钟的时间。
- en: In fact, the earliest you can detect these problems, the better. When talking
    about security in software, it is typical to hear “software supply chain,” where
    the *chain* is all the steps from development to shipping code into production.
    And in this chain of events, there is a constant push to *shift left*. If you
    see these steps as one big chain, the leftmost link is the developer creating
    and updating the software, and the end of the chain (the farthest to the right)
    is the released product, where the end-consumer can interact with it.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你能越早发现这些问题，效果越好。谈到软件安全时，通常会听到“软件供应链”，其中*链*就是从开发到将代码发布到生产的所有步骤。在这一系列事件中，有一个持续不断的推动向*左移*的趋势。如果你把这些步骤看作一个大链条，最左边的链接是开发者创建和更新软件，而链条的末端（最右边的）是已发布的产品，最终用户可以与之交互。
- en: The earlier you can shift left the error detection, the better. This is because
    it is cheaper and faster than waiting all the way until it is in production when
    a rollback needs to happen.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 尽早左移错误检测，效果越好。这是因为这比等待到生产阶段再进行回滚更便宜和更快。
- en: Continuous improvement
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持续改进
- en: 'A couple of years ago, I was the release manager of a large open source software.
    The software was so complicated to release that it would take me anywhere from
    two days up to a whole week. It was tough to make improvements as I was responsible
    for other systems as well. One time, while trying to get a release out, following
    the many different steps to publish the packages, a core developer asked me to
    get in one last change. Instead of saying *“No”* right away, I asked: *“Has this
    change been tested already?”*'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，我曾是一个大型开源软件的发布经理。这个软件的发布过程非常复杂，发布的时间从两天到整整一周不等。由于我还负责其他系统，所以很难做出改进。有一次，在试图发布时，按照发布包的许多不同步骤，一个核心开发者要求我最后再加一个改动。我没有立刻说*“不行”*，而是问道：*“这个改动已经测试过了吗？”*
- en: 'The response was completely unexpected: *“Don’t be ridiculous, Alfredo, this
    is a one-line change, and it is a documentation comment in a function. We really
    need this change to be part of the release.”* The push to get the change in came
    all the way from the top, and I had to budge. I added the last-minute change and
    cut the release.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 回应完全出乎意料：*“别荒谬，阿尔弗雷多，这只是一个一行的改动，而且是一个函数的文档注释。我们真的需要这个改动成为发布的一部分。”* 推动这个改动来自最高层，我不得不让步。我添加了这个临时改动并发布了版本。
- en: The very first thing the next morning, we came back to users (and most importantly,
    customers) all complaining that the latest release was completely broken. It would
    install, but it would not run at all. The culprit was the one-line change that,
    although it was a comment within a function, was being parsed by other code. There
    was an unexpected syntax in that comment, so it prevented the application from
    starting up. The story is not meant to chastise the developer. He didn’t know
    better. The whole process was a learning moment for everyone involved, and it
    was now clear how expensive this one-line change was.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 第二天早晨回来后，用户（尤其是客户）都抱怨最新版本完全不可用。虽然它安装了，但根本无法运行。罪魁祸首是一行代码的变更，尽管它只是一个函数内的注释，却被其他代码解析了。该注释中有一个意外的语法，导致应用程序无法启动。这个故事并不是要责备开发者。他并不知道更好的方法。整个过程对所有参与者来说都是一个学习时刻，现在大家都清楚这个一行代码变更有多昂贵。
- en: There was a set of disruptive events that followed. Aside from restarting the
    release process, the testing phase for the one change took another (extra) day.
    Lastly, I had to *retire* the released packages and redo the repositories so new
    users would get the previous version.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 随后出现了一系列破坏性事件。除了重新启动发布流程外，对一个变更的测试阶段又花费了额外的一天时间。最后，我不得不*废弃*已发布的包，并重新设置存储库，以便新用户获取先前的版本。
- en: It was beyond costly. The number of people involved and the high impact made
    this an excellent opportunity to assert that this should not be allowed again—even
    if it is a one-line change. The earlier the detection, the least impact it will
    have, and the cheaper it is to fix.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个极为昂贵的事情。参与其中的人数和高影响力使得这是一个绝佳的机会，以明确表示这种情况不应再次发生——即使只是一行代码的更改。越早检测到，影响就越小，修复的成本也更低。
- en: Conclusion
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Continuous delivery and the practice of constant feedback is crucial for a robust
    workflow. As this chapter proves, there is a lot of value in automation and continuous
    improvement of the feedback loop. Packaging containers, along with pipelines and
    CI/CD platforms in general, are meant to make it easier to add more checks and
    verifications, which are intended to increase the confidence of shipping models
    into production.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 持续交付和不断反馈的实践对于强大的工作流至关重要。正如本章所证明的那样，自动化和持续改进反馈循环的价值巨大。打包容器以及流水线和 CI/CD 平台的一般功能，旨在使添加更多检查和验证变得更加容易，这些都旨在增加模型交付到生产环境的信心。
- en: 'Shipping models into production is the number one objective, but doing so with
    very high confidence, in a resilient set of steps, is what you should strive for.
    Your task does not end once the processes are in place. You must keep finding
    ways to thank yourself later by asking the question: what can I add today to make
    my life easier if this process fails? Finally, I would strongly recommend creating
    these workflows in a way that makes it easy to add more checks and verifications.
    If it is hard, no one will want to touch it, defeating the purpose of a robust
    pipeline to ship models into production.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型发布到生产环境是首要目标，但要在非常高的信心水平下完成这一过程，以及保证步骤的韧性，是您应该努力达到的。当流程一旦建立，您的任务并未结束。您必须不断寻找方法，通过问自己这样一个问题来感谢自己：今天我能添加什么来让生活更轻松，如果这个流程失败了？最后，我强烈建议以一种易于添加更多检查和验证的方式创建这些工作流程。如果难以实现，没有人会愿意碰它，从而打败了将模型交付到生产环境的强大流水线的初衷。
- en: Now that you have a good grasp of delivering models and what the automation
    looks like, we will dive into AutoML and Kaizen in the next chapter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经很好地掌握了模型交付及其自动化的情况，我们将在下一章深入探讨 AutoML 和 Kaizen。
- en: Exercises
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Create your own Flask application in a container, publish it to a GitHub repository,
    document it thoroughly, and add GitHub Actions to ensure it builds correctly.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在容器中创建您自己的 Flask 应用程序，将其发布到 GitHub 存储库，并进行详细文档化，同时添加 GitHub Actions 以确保其正确构建。
- en: Make changes to the ONNX container so that it pushes to Docker Hub instead of
    GitHub Packages.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改 ONNX 容器以便将其推送至 Docker Hub 而不是 GitHub Packages。
- en: Modify a SageMaker pipeline, so it prompts you before registering the model
    after training it.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改 SageMaker 流水线，使其在训练后在注册模型之前提示您。
- en: Using the Azure SDK, create a Jupyter notebook that will increase the percentile
    of traffic going to a container.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Azure SDK，创建一个 Jupyter 笔记本，以增加流量流向容器的百分位。
- en: Critical Thinking Discussion Questions
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批判性思维讨论问题
- en: Name at least four critical checks you can add to verify a packaged model in
    a container is built correctly.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少命名四项关键检查，以验证容器中打包的模型是否正确构建。
- en: What are the differences between canary and blue-green deployments? Which one
    do you prefer? Why?
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金丝雀部署（canary deployment）和蓝绿部署（blue-green deployment）之间有什么区别？你更喜欢哪一种？为什么？
- en: Why are cloud pipelines useful versus using GitHub Actions? Name at least three
    differences.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么与使用 GitHub Actions 相比，云管道（cloud pipelines）更有用？至少列举三个不同之处。
- en: What does *packaging a container* mean? Why is it useful?
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*打包容器* 的意思是什么？为什么这很有用？'
- en: What are three characteristics of package machine learning models?
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型打包的三个特征是什么？
