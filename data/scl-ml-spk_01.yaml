- en: Chapter 1\. Distributed Machine Learning Terminology and Concepts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。分布式机器学习术语和概念
- en: Remember when data scientists ran their machine learning algorithms on datasets
    that fit in a laptop’s memory? Or generated their own data? It wasn’t because
    of a lack of data in the world; we had already entered the Zettabyte Era.^([1](ch01.xhtml#ch01fn1))
    For many, the data was there, but it was locked in the production systems that
    created, captured, copied, and processed data at a massive scale. Data scientists
    knew that gaining access to it would allow them to produce better, more profound
    machine learning models. But this wasn’t the only problem—what about computation?
    In many cases, data scientists didn’t have access to sufficient computation power
    or tools to support running machine learning algorithms on large datasets. Because
    of this, they had to sample their data and work with CSV or text files.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 记得以前数据科学家在笔记本电脑内存中运行他们的机器学习算法吗？或者生成他们自己的数据？这并不是因为世界上缺乏数据；我们已经进入了赛博字节时代。^([1](ch01.xhtml#ch01fn1))
    对于许多人来说，数据是存在的，但它被锁在生产系统中，这些系统以大规模创建、捕获、复制和处理数据。数据科学家知道，获得这些数据将使他们能够产生更好、更深刻的机器学习模型。但这不是唯一的问题——计算呢？在许多情况下，数据科学家没有足够的计算能力或工具来支持在大数据集上运行机器学习算法。因此，他们不得不对数据进行抽样，并使用
    CSV 或文本文件进行工作。
- en: When the public cloud revolution occurred around 2016–2017, we could finally
    get hold of that desired computation capacity. All we needed was a credit card
    in one hand and a computer mouse in the other. One click of a button and boom,
    hundreds of machines were available to us! But we still lacked the proper open
    source tools to process huge amounts of data. There was a need for distributed
    compute and for automated tools with a healthy ecosystem.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当公共云革命在2016年至2017年左右爆发时，我们终于能够获得所需的计算能力。我们只需要一张信用卡和一只鼠标。按下一个按钮，哇哦，数百台机器都可以使用了！但是，我们仍然缺乏适当的开源工具来处理大量数据。需要分布式计算和具有健全生态系统的自动化工具。
- en: The growth in *digitalization*, where businesses use digital technologies to
    change their business model and create new revenue streams and value-producing
    opportunities, increased data scientists’ frustration. Digitalization led to larger
    amounts of data being available, but data scientists couldn’t work with that data
    fast enough because they didn’t have the tools. The tedious process of waiting
    for days to try out one machine learning algorithm or to get a sample of production
    data blocked many from reaching their full potential. The need to improve and
    automate grew.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数字化增长，即企业利用数字技术改变其业务模式并创造新的收入流和价值产生机会，增加了数据科学家的挫折感。数字化导致更多的数据可用，但由于缺乏工具，数据科学家无法快速处理这些数据。长时间等待尝试一个机器学习算法或获取生产数据样本的繁琐过程阻碍了许多人实现其全部潜力。改进和自动化的需求日益增加。
- en: Small companies saw how larger ones had demonstrated a positive impact on their
    business by providing automated, personalized solutions to their customers, improving
    sentiment and boosting revenue. From a fantasy, machine learning became a hot
    commodity. Companies understood that to take advantage of it they would need more
    tools, and dedicated teams to build those tools in house, which in turn increased
    the demand for engineers to build reliable, scalable, accelerated, and high-performance
    tools to support machine learning workloads.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 小公司看到更大公司通过为客户提供自动化、个性化解决方案对其业务产生了积极影响，改善了情感并增加了收入。从幻想到现实，机器学习成为了一种热门商品。公司们意识到，要利用它，他们需要更多的工具，并且需要内部团队来构建这些工具，这反过来增加了对工程师的需求，以构建可靠、可扩展、加速和高性能的工具来支持机器学习工作负载。
- en: Netflix, the world-leading internet television network that streams hundreds
    of millions of hours of content daily, has stated that it uses machine learning
    pervasively, across all aspects of the business. This includes recommending personalized
    content for customers, optimizing the movie and show production processes in the
    Netflix studios, optimizing video and audio encoding, and improving advertising
    spending and ad creativity to reach new potential customers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Netflix，全球领先的互联网电视网络，每天流传数亿小时的内容，表示它在业务的各个方面广泛使用机器学习。这包括为客户推荐个性化内容，在Netflix工作室优化电影和节目制作过程，优化视频和音频编码，以及改进广告支出和广告创意，以触及新的潜在客户。
- en: Machine learning has found applications in a broad array of industries, however,
    not just in technology-focused businesses. Data scientists and analytics teams
    at the multinational oil and gas company Shell plc leverage machine learning over
    large-scale datasets to support the business with insights regarding product opportunities
    and process optimizations and to test the effectiveness of different courses of
    action. One example is their inventory prediction model, which runs [over 10,000
    simulations](https://oreil.ly/pgrJZ) across all parts and facilities, predicting
    demand and improving stocking. Shell also uses machine learning to power a recommendation
    engine for its customer loyalty program, Go+, which offers personalized offers
    and rewards to individual customers. This approach provides Shell with an enhanced
    engagement model that helps retain customers by catering to their specific needs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习不仅在技术型企业中找到了应用，而且在广泛的行业中都有应用。Shell plc跨国石油和天然气公司的数据科学家和分析团队利用大规模数据集上的机器学习来支持业务，提供关于产品机会和流程优化的见解，并测试不同行动方案的有效性。其中一个例子是他们的库存预测模型，该模型在所有零件和设施上运行了[超过10,000次模拟](https://oreil.ly/pgrJZ)，预测需求并改善库存。Shell还利用机器学习为其客户忠诚计划Go+提供推荐引擎，为个体客户提供个性化的优惠和奖励。这种方法为Shell提供了一个增强的参与模型，通过满足客户的特定需求来帮助保留客户。
- en: Other industries use machine learning for fraud detection, recommendation systems,
    patient diagnosis, and more. Take a look at [Figure 1-1](#some_of_the_many_uses_of_machine_learni)
    to get an idea of how you may be able to drive innovation using machine learning
    in your industry.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 其他行业利用机器学习进行欺诈检测、推荐系统、患者诊断等。查看[图1-1](#some_of_the_many_uses_of_machine_learni)可以了解如何在您的行业中利用机器学习推动创新。
- en: As these examples suggest, the ability to use large datasets to create solutions
    with proven business impact has been eye-opening for many companies looking to
    grow their business and improve revenue.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这些例子所示，利用大数据集来创建具有实证业务影响力的解决方案，对许多寻求业务增长和提高收入的公司来说是一种启发。
- en: '![](assets/smls_0101.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0101.png)'
- en: Figure 1-1\. Some of the many uses of machine learning across industries
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-1\. 机器学习在各行业中的众多应用示例
- en: The computer science and engineering research communities have helped greatly
    in enabling scalable machine learning. In recent years, academic researchers have
    conducted hundreds if not thousands of studies on using machine learning, distributed
    computation, and databases and building smarter, more efficient algorithms to
    support distributed machine learning. As a result, general-purpose distributed
    platforms have emerged, such as the immensely popular Apache Spark. Apache Spark
    provides a scalable, general-purpose engine for analytics and machine learning
    workloads. At the same time, the teams behind various machine learning libraries
    built to support workloads on a single machine are tirelessly adding backend support
    for executing in a distributed setting. To list a few examples, additional capabilities
    to support distributed machine learning have been added to Google’s TensorFlow,
    which simplifies deep neural network workloads, and Facebook’s PyTorch, used for
    computer vision and natural language processing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学和工程研究社区在实现可扩展机器学习方面发挥了重要作用。近年来，学术研究人员已经进行了数百甚至数千项关于使用机器学习、分布式计算和数据库以及构建更智能、更高效算法的研究。因此，通用的分布式平台应运而生，比如极其流行的Apache
    Spark。Apache Spark提供了一个可扩展的通用引擎，用于分析和机器学习工作负载。与此同时，为了在分布式环境中执行工作负载，各种支持单机器工作负载的机器学习库团队不断增加了后端支持能力。举几个例子，Google的TensorFlow已经增加了支持分布式机器学习的额外功能，用于简化深度神经网络工作负载，而Facebook的PyTorch用于计算机视觉和自然语言处理，也在不断增强其支持分布式机器学习的后端支持能力。
- en: 'Throughout this book, we’ll focus on using Apache Spark, and I will show you
    how to bridge from it into distributed machine learning with TensorFlow and PyTorch.
    The book concludes with a discussion of machine learning deployment patterns in
    [Chapter 10](ch10.xhtml#deployment_patterns_for_machine_learnin). To get you started,
    this chapter provides an introduction to the fundamental concepts, terminology,
    and building blocks of distributed machine learning. We will cover the basics
    of the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的整个过程中，我们将专注于使用Apache Spark，并向您展示如何通过它进入基于TensorFlow和PyTorch的分布式机器学习。本书最后讨论了机器学习部署模式在[第十章](ch10.xhtml#deployment_patterns_for_machine_learnin)。为了让您快速入门，本章介绍了分布式机器学习的基本概念、术语和构建模块。我们将涵盖以下基础知识：
- en: The machine learning workflow
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习工作流程
- en: Spark MLlib
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark MLlib
- en: Distributed computing
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式计算
- en: Distributed systems
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式系统
- en: Familiar with those concepts? We’ll shift to an introduction to Spark and PySpark
    in [Chapter 2](ch02.xhtml#introduction_to_spark_and_pyspark) and managing the
    machine learning lifecycle in [Chapter 3](ch03.xhtml#managing_the_ml_experiment_lifecycle_wi).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些概念熟悉了吗？我们将在[第二章](ch02.xhtml#introduction_to_spark_and_pyspark)中介绍Spark和PySpark，以及在[第三章](ch03.xhtml#managing_the_ml_experiment_lifecycle_wi)中管理机器学习生命周期。
- en: Excited? Let’s dive in!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 兴奋了吗？让我们开始吧！
- en: The Stages of the Machine Learning Workflow
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作流程的阶段
- en: 'Many applications today are driven by machine learning, using machine learning
    models to answer questions such as: How can my application automatically adapt
    itself to the customer’s needs? How can I automate this tedious process to enable
    my employees to do more with their time? How can I make sense of my pile of data
    without spending the whole year going over it? However, as data practitioners,
    we have just one question to answer: How can we enable the process to answer those
    questions?'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如今许多应用程序是由机器学习驱动的，使用机器学习模型来回答诸如：如何让我的应用程序自动适应客户的需求？如何自动化这个繁琐的流程，让我的员工能够更有效地利用他们的时间？如何在不花费整年时间的情况下理解我的数据堆？然而，作为数据从业者，我们只需回答一个问题：如何使整个过程能够回答这些问题？
- en: The short answer is machine learning. A more comprehensive response is the machine
    learning workflow.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的答案是机器学习。更全面的回答是机器学习工作流程。
- en: The machine learning workflow comprises a set of stages that help us reach the
    goal of having a machine learning model running in production solving a business
    problem. What is a machine learning model? Good question! A machine learning model
    is the output of a machine learning algorithm. From now on, we will refer to it
    simply as a model. The automation of this workflow is referred to as the machine
    learning pipeline. To improve the accuracy of the model, the workflow is iterative.
    This allows us to exercise complete control over the model—including automation,
    monitoring, and deployment—and its output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作流程包括一系列阶段，帮助我们实现将机器学习模型投入生产并解决业务问题的目标。什么是机器学习模型？好问题！机器学习模型是机器学习算法的输出。从现在开始，我们将简称其为模型。这个工作流程的自动化被称为机器学习管道。为了提高模型的准确性，工作流程是迭代的。这使得我们能够对模型——包括自动化、监控和部署——进行全面控制。
- en: 'The machine learning workflows consist of multiple stages, some of which can
    be skipped and some of which may be repeated:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作流程包括多个阶段，其中一些可以跳过，而另一些可能需要重复执行：
- en: '*Collect and load/ingest data.* The first stage is to collect the data required
    for the process and load it into the environment where you will execute your machine
    learning experiment.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*收集和加载/摄取数据。* 第一个阶段是收集所需的数据，并将其加载到执行机器学习实验的环境中。'
- en: '*Explore and validate the data.* Next, explore the data you have collected
    and evaluate its quality. This stage often involves statistical testing of how
    well the training data represents real-world events, its distribution, and the
    variety in the dataset. This is also referred to as *exploratory data analysis*
    (EDA).'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*探索和验证数据。* 接下来，探索您收集的数据并评估其质量。这个阶段通常涉及统计测试，测试训练数据如何代表真实世界事件，以及数据集的分布和多样性。这也被称为*探索性数据分析*（EDA）。'
- en: '*Clean/preprocess the data.* After stage 2, you might reach the conclusion
    that the data is *noisy*. A noisy dataset is one with columns that do not contribute
    to the training at all—for example, rows with null values, or long string values.
    They require more processing power but don’t improve model accuracy. In this stage,
    data scientists will run statistical tests on the data to validate the correlation
    between features and analyze which features provide value as is, which require
    more preprocessing or engineering, and which are redundant.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*清理/预处理数据*。在第2阶段之后，您可能会得出结论，数据存在*噪音*。噪声数据集是指不对训练有任何贡献的列，例如具有空值或长字符串值的行。它们需要更多的处理能力，但并不提高模型的准确性。在这个阶段，数据科学家将对数据进行统计测试，验证特征之间的相关性，并分析哪些特征原样提供价值，哪些需要更多预处理或工程处理，以及哪些是多余的。'
- en: '*Extract features/perform feature engineering.* The previous stage outputs
    the data columns as features. These are the descriptors of the data, used as the
    inputs to the machine learning model. Features in machine learning are often external
    to the original data, meaning we need to enrich the existing data with data from
    other sources. That requires us to develop the code to compute and produce these
    features and enrich the dataset with them before training the model. There are
    many ways to do this, and it often requires domain expertise. Alternatively, the
    features may already be present in another dataset, in which case all we need
    to do is merge the two datasets into one before training the model.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*提取特征/执行特征工程*。前一阶段将数据列作为特征输出。这些是数据的描述符，作为机器学习模型的输入使用。机器学习中的特征通常是原始数据之外的内容，意味着我们需要用来自其他来源的数据丰富现有数据。这要求我们开发代码来计算和生成这些特征，并在训练模型之前用它们丰富数据集。有许多方法可以做到这一点，通常需要领域专业知识。或者，这些特征可能已经存在于另一个数据集中，这种情况下，我们只需将两个数据集合并成一个，然后再训练模型。'
- en: '*Split the data into a training set and a validation set.* The training set
    is used for training the machine learning model, and the validation set is for
    evaluating the performance of the model on unseen data.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*将数据分成训练集和验证集*。训练集用于训练机器学习模型，验证集用于评估模型在未见数据上的表现。'
- en: '*Train and tune the model.* Feed the training data to the machine learning
    algorithm, and adjust the parameters to improve performance. Validate the outcome
    using the dedicated validation dataset. The validation process takes place in
    the development environment, either locally on your machine or in a development/experimentation
    environment in the cloud. The outcome of this stage is the model.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*训练和调整模型*。将训练数据提供给机器学习算法，并调整参数以提高性能。使用专门的验证数据集验证结果。验证过程在开发环境中进行，可以是在本地机器上或云中的开发/实验环境中。这一阶段的结果是模型。'
- en: '*Evaluate the model with test data.* This is the last testing stage before
    the model is pushed to production. In this stage, you again measure the model’s
    performance on previously unseen data, this time testing it in a production-like
    setting. After this stage, you might want to go back and revisit stage 6.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用测试数据评估模型*。这是模型推向生产之前的最后测试阶段。在这个阶段，再次测量模型在之前未见数据上的表现，在生产环境中进行测试。在这一阶段之后，您可能需要返回并重新审视第6阶段。'
- en: '*Deploy the model.* During this stage, data scientists together with machine
    learning and production engineers package the model and deploy it to production
    with all its requirements.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*部署模型*。在这个阶段，数据科学家与机器学习和生产工程师一起打包模型，并将其部署到生产环境中，并满足其所有要求。'
- en: '*Monitor the model.* In production, the model must constantly be monitored
    for drift (different types of drift are discussed in [Chapter 10](ch10.xhtml#deployment_patterns_for_machine_learnin)).
    It is crucial to continually evaluate the model’s value to the business and know
    when to replace it.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*监控模型*。在生产中，必须持续监控模型是否存在漂移（关于不同类型漂移的讨论详见[第10章](ch10.xhtml#deployment_patterns_for_machine_learnin)）。持续评估模型对业务的价值，并知道何时替换模型至关重要。'
- en: Each of these stages is repeatable on its own, and it may be that given a specific
    result you will want to complete the whole process again. For example, in the
    case of model drift, the data and the model are not representative of the business
    problem, and you will need to start the process over from the beginning.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每个阶段都可以单独重复，并且可能会根据特定结果需要完成整个过程。例如，在模型漂移的情况下，数据和模型不再代表业务问题，您需要从头开始执行该流程。
- en: Each stage is unique and highly dependent on the data, the system requirements,
    your knowledge, the algorithms in use, the existing infrastructure, and the desired
    outcome.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个阶段都是独特的，并且高度依赖于数据、系统需求、您的知识、使用的算法、现有基础设施和期望的结果。
- en: Stages 3 to 6 are often considered the experimental phase of machine learning.
    You will want to iterate repeatedly and produce multiple versions of the data
    and the model until you find the best version of the model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段3到6通常被视为机器学习的实验阶段。您将希望反复迭代并生成数据和模型的多个版本，直到找到最佳版本的模型。
- en: To learn more about machine learning workflows and automating them using pipelines
    with TensorFlow and TensorBoard, read [*Building Machine Learning Pipelines*](https://oreil.ly/building-ml)
    by Hannes Hapke and Catherine Nelson (O’Reilly).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于使用TensorFlow和TensorBoard构建机器学习流水线并自动化它们的信息，请阅读《[*Building Machine Learning
    Pipelines*](https://oreil.ly/building-ml)》（由Hannes Hapke和Catherine Nelson撰写，O’Reilly出版）。
- en: Tools and Technologies in the Machine Learning Pipeline
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习流水线中的工具和技术
- en: '[Figure 1-2](#a_high_level_view_of_the_ml_pipeline_an) shows an overview of
    the machine learning pipeline and some of the tools that may be used in each stage.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-2](#a_high_level_view_of_the_ml_pipeline_an)显示了机器学习流水线的概览以及可能在每个阶段使用的一些工具。'
- en: '![](assets/smls_0102.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0102.png)'
- en: Figure 1-2\. A high-level view of the machine learning pipeline and tools used
    in each stage
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2\. 机器学习流水线的高层视图及每个阶段使用的工具
- en: We’ll use various tools and platforms in the tutorials in this book to complete
    the different stages (you can, of course, decide to replace any of these tools
    with a different tool of your choice). For experimenting with data ingestion,
    preprocessing, and feature engineering, we will use Jupyter, which provides a
    UI and a backend server. We’ll write the code in a notebook in the UI, and the
    backend will package it and send it over to the Spark engine.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的教程中，我们将使用各种工具和平台来完成不同阶段的任务（当然，您可以选择用您选择的其他工具替换其中任何工具）。为了实验数据摄取、预处理和特征工程，我们将使用提供UI和后端服务器的Jupyter。我们将在UI中的笔记本中编写代码，后端将其打包并发送到Spark引擎。
- en: In the model-building stage (corresponding to stages 6 and 7 in the machine
    learning workflow described earlier), we train, validate, and tune the model.
    We will use multiple servers and backends in this stage, including Jupyter, PyTorch,
    TensorFlow, Horovod, Petastorm, and MLflow, to orchestrate operations, cache the
    data, and transition the workflow from a Spark cluster to a deep learning cluster.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建阶段（对应于先前描述的机器学习工作流程中的第6和第7阶段），我们会训练、验证和调整模型。在此阶段，我们将使用多个服务器和后端，包括Jupyter、PyTorch、TensorFlow、Horovod、Petastorm和MLflow，来编排操作、缓存数据，并将工作流从Spark集群转换为深度学习集群。
- en: Finally, to deploy and serve our models, we will use Spark and MLflow. We’ll
    load the model from the MLflow storage server and serve it with Spark or as a
    REST API with a Python function.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了部署和提供我们的模型，我们将使用Spark和MLflow。我们将从MLflow存储服务器加载模型，并使用Spark或作为Python函数的REST
    API提供服务。
- en: Note
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In most organizations, developing an end-to-end machine learning pipeline requires
    a dedicated team whose members have various sets of skills, as well as an integrated
    development environment (IDE) like PyCharm that provides rich developer tools
    and code autocompletion, dedicated scripts for continuous integration/continuous
    deployment (CI/CD), and much more. For the educational purposes of this book,
    we’ll stick to Jupyter notebooks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数组织中，开发端到端的机器学习流水线需要一个专门的团队，团队成员具有各种技能集，以及一个集成开发环境（IDE）如PyCharm，提供丰富的开发者工具和代码自动完成，专门用于持续集成/持续部署（CI/CD）的脚本等。出于本书的教育目的，我们将坚持使用Jupyter笔记本。
- en: Distributed Computing Models
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式计算模型
- en: '*Distributed computing* is the use of *distributed systems*, where multiple
    machines work together as a single unit, to solve a computational problem. A program
    that runs inside such a system is called a *distributed program*, and the process
    of writing such a program is known as *distributed programming*. That’s what we’ll
    be doing in this book. Our goal is to find the best way to divide a problem into
    separate tasks that multiple machines can solve in parallel through message communication.
    There are different distributed computing models for machine learning, and we
    can categorize these into two groups: *general-purpose* models that we can tweak
    to support distributed machine learning applications, and *dedicated* computing
    models specifically designed for running machine learning algorithms.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*分布式计算* 是使用*分布式系统*解决计算问题的技术，多台机器共同作为一个单一单位工作。在这样的系统中运行的程序称为*分布式程序*，编写这样的程序的过程称为*分布式编程*。这正是我们在本书中要做的事情。我们的目标是找到将问题划分为多个任务的最佳方法，这些任务可以通过消息通信并行解决。机器学习的不同分布式计算模型可以分为两类：可以调整以支持分布式机器学习应用的*通用*模型，以及专门设计用于运行机器学习算法的*专用*计算模型。'
- en: General-Purpose Models
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用模型
- en: General-purpose distributed computing models allow users to write a custom data
    processing flow using a defined abstraction. Apache Spark is a general-purpose
    distributed computing engine that, at its heart, implements the MapReduce programming
    model and has more recently been extended to support the barrier model. You’ll
    learn about both of those in this section, as well as some other distributed computing
    models (MPI [Message Passing Interface] and shared memory) that are available
    with TensorFlow and PyTorch.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通用分布式计算模型允许用户使用定义的抽象编写自定义数据处理流。Apache Spark 是一种通用分布式计算引擎，其核心实现了MapReduce编程模型，并最近扩展支持屏障模型。在本节中，您将了解这两种模型，以及一些其他分布式计算模型（MPI
    [消息传递接口] 和共享内存），这些模型在TensorFlow和PyTorch中都有提供。
- en: MapReduce
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MapReduce
- en: The MapReduce programming model was inspired by the [functional programming
    paradigm](https://oreil.ly/YrPPz). Google introduced the MapReduce algorithm in
    2004, in a research paper discussing how its search engine processes large-scale
    data. As developers or data science practitioners, we specify a *map* function
    that processes a key/value pair to generate a set of intermediate key/value pairs
    and a *reduce* function that merges all the intermediate values associated with
    the same intermediate key. This approach is an extension of the split-apply-combine
    strategy for data analysis. In practice, every task is split into multiple map
    and reduce functions. Data is partitioned and distributed over various nodes/machines,
    and each chunk of data is processed on a dedicated node. Many solutions aim to
    preserve data locality as much as possible, where the partitioned data is local
    to the node processing it. A logic function is applied to the data on that node,
    then a shuffle operation of moving the data over the network is performed to combine
    the data from the different nodes, and a reduce operation is performed on the
    combined output from the mappers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 编程模型的灵感来自[函数式编程范式](https://oreil.ly/YrPPz)。Google 在2004年引入了MapReduce算法，该算法在研究论文中讨论了其搜索引擎如何处理大规模数据。作为开发人员或数据科学从业者，我们指定一个*映射*函数来处理键/值对，生成一组中间键/值对，以及一个*减少*函数来合并与同一中间键相关联的所有中间值。这种方法是数据分析中分割-应用-合并策略的扩展。在实践中，每个任务都被分成多个映射和减少函数。数据被分区和分布在各个节点/机器上，每个数据块在专用节点上进行处理。许多解决方案旨在尽可能保持数据的本地性，其中分区数据对于处理它的节点是本地的。在该节点上应用逻辑函数，然后执行网络上移动数据的洗牌操作，将来自不同节点的数据组合在一起，并对来自映射器的组合输出执行减少操作。
- en: If necessary, another split-apply-combine round can then be performed on the
    output of the reducer. Examples of open source solutions that implement these
    concepts in one way or another include Apache Spark, Hadoop MapReduce, and Apache
    Flink. We’ll talk about the MapReduce model in more detail throughout the book.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，可以对减少器的输出进行另一个分割-应用-合并循环。实现这些概念的开源解决方案的示例包括Apache Spark、Hadoop MapReduce和Apache
    Flink。我们将在整本书中更详细地讨论MapReduce模型。
- en: MPI
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MPI
- en: Another interesting general-purpose distributed computing model is the Message
    Passing Interface (MPI) programming model. This is the most flexible model available
    today, and it was designed for high-performance, scalable, and portable distributed
    computing. MPI is a message passing interface that models a parallel program running
    on a distributed-memory system. It standardizes the communication between a set
    of processors by defining the data types that can be sent between the processors.
    Each processor has a unique identifier, and each *communicator* is a set of processors
    ordered in a specific topology. MPI is a low-level standard that can be implemented
    in hardware, compilers, wrappers, etc. Different versions of it have been implemented
    commercially by HP, Intel, Microsoft, and others.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的通用分布式计算模型是消息传递接口（Message Passing Interface，MPI）编程模型。这是目前最灵活的模型，旨在实现高性能、可伸缩和便携的分布式计算。MPI
    是一种消息传递接口，用于在分布式内存系统上模拟并行程序的运行。它通过定义可在处理器之间发送的数据类型来标准化处理器集之间的通信。每个处理器都有唯一的标识符，而每个*通信器*是按特定拓扑顺序排序的一组处理器。MPI
    是一个低级标准，可以在硬件、编译器、包装器等中实现。HP、Intel、Microsoft 等公司都已经商业化实现了不同版本的 MPI。
- en: MPI provides functions like `MPI_``Bcast`, which broadcasts a message to all
    processors in the communicator, sharing the data; `MPI_``Alltoall`, which sends
    all data to all nodes; and `MPI_Reduce` and `MPI_Allreduce`, which are similar
    to MapReduce and Apache Spark’s `reduce` functions. You can think of the interface
    as a set of building blocks for distributed frameworks that provides functionality
    for distributed machine learning.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: MPI 提供了像 `MPI_Bcast`（广播消息到通信器中的所有处理器，共享数据）、`MPI_Alltoall`（将所有数据发送到所有节点）、`MPI_Reduce`
    和 `MPI_Allreduce`（类似于 MapReduce 和 Apache Spark 的 `reduce` 函数）等函数。您可以将这个接口视为分布式框架的构建块，提供了分布式机器学习功能。
- en: The downside of MPI lies in its low-level permissiveness. It can be pretty labor-intensive
    and error-prone to execute and implement complex operations with MPI; it requires
    explicitly managing data type distribution, sending and receiving functionality,
    and fault tolerance and generally demands the developer to think about distributed
    arrays, data frames, hash tables, trees, etc. MPI is often used for deep learning
    workloads. Horovod’s core functions are based on MPI concepts, such as `size`,
    `rank`, `local_rank`, `allreduce`, `allgather`, and `broadcast`. For distributed
    computing, TensorFlow provides support for MPI as part of its communication protocol.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: MPI 的缺点在于其低级别的宽容性。使用 MPI 执行和实现复杂操作可能非常费力且容易出错；它要求显式管理数据类型分布、发送和接收功能以及容错，通常要求开发者考虑分布式数组、数据框架、哈希表、树等。MPI
    经常用于深度学习工作负载。Horovod 的核心功能基于 MPI 概念，如 `size`、`rank`、`local_rank`、`allreduce`、`allgather`
    和 `broadcast`。对于分布式计算，TensorFlow 提供了 MPI 的支持作为其通信协议的一部分。
- en: Barrier
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 屏障
- en: A barrier is a synchronization method that is commonly used in parallel computing
    and is implemented in distributed computing frameworks like Apache Spark. A task
    or job is split into dependent stages or subtasks that need to be completed before
    processing can continue in the next stage. A barrier makes a group of machines
    stop at a certain point and wait for the rest of the machines to finish their
    computation before they can move on together to the next stage of the computation
    logic. Barrier models can be implemented in hardware and software, and stages
    can take many shapes, from directed acyclic graphs (DAGs) to trees or sequential
    operations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 屏障（barrier）是一种在并行计算中常用的同步方法，被实现在诸如 Apache Spark 等分布式计算框架中。任务或作业被分割成依赖阶段或子任务，在处理可以继续到下一个阶段之前需要完成这些阶段或子任务。屏障使得一组机器在某一点停止，并等待其他机器完成计算，然后它们可以共同前进到计算逻辑的下一个阶段。屏障模型可以在硬件和软件中实现，阶段可以采用许多形状，从有向无环图（DAG）到树形结构或顺序操作。
- en: Although it is a general-purpose distributed computing model, the barrier model
    enables diverse distributed machine learning algorithms. For example, in deep
    learning, each layer in the stacked artificial neural network is a stage, and
    each stage’s computation depends on the output of the previous stage. Barrier
    models enable the management of many layers of training in this case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它是一个通用的分布式计算模型，但屏障模型使得各种分布式机器学习算法得以实现。例如，在深度学习中，堆叠的人工神经网络中的每一层都是一个阶段，每个阶段的计算依赖于前一阶段的输出。屏障模型在这种情况下可以管理许多层次的训练。
- en: Shared memory
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共享内存
- en: 'Shared memory models have a long history: they originated in operating systems
    like POSIX and Windows, where processes running on the same machine needed to
    communicate over a shared address space. Distributed shared memory models try
    to meet the same need when multiple nodes/users are communicating over the network
    in a distributed environment and require access to the same data from various
    machines. Today, there is no one partitioned global address space but an in-memory
    or fast database that provides strong consistency.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存模型有着悠久的历史：它们起源于像POSIX和Windows这样的操作系统，这些操作系统中运行在同一台机器上的进程需要通过共享地址空间进行通信。分布式共享内存模型试图在分布式环境中满足同样的需求，当多个节点/用户在网络上通信并需要从各种机器上访问相同的数据时。今天，并不存在一个分区全局地址空间，而是提供强一致性的内存或快速数据库。
- en: Strong consistency in a distributed shared memory environment means that all
    access to the data by all processes and nodes is consistent. Ensuring this is
    not an easy task. One of TensorFlow’s distributed strategies implements a shared
    memory model; you will learn all about it and its pros and cons in [Chapter 8](ch08.xhtml#tensorflow_distributed_ml_approach).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式共享内存环境中，强一致性意味着所有进程和节点对数据的访问都是一致的。确保这一点并不容易。TensorFlow的一个分布式策略实现了一个共享内存模型；你将在[第8章](ch08.xhtml#tensorflow_distributed_ml_approach)中详细了解它及其优缺点。
- en: Dedicated Distributed Computing Models
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专用分布式计算模型
- en: Dedicated distributed computing models are models that were developed to support
    a specific need in the machine learning development cycle. Often, they leverage
    the general-purpose models as building blocks to construct a more user-friendly
    framework that data science practitioners can use out of the box. You can make
    use of them with TensorFlow and PyTorch.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 专用的分布式计算模型是为支持机器学习开发周期中的特定需求而开发的模型。通常，它们利用通用模型作为构建块，以构建更易于使用的框架，数据科学从业者可以直接使用。您可以在TensorFlow和PyTorch中使用它们。
- en: 'One example of a dedicated distributed computing model for machine learning
    workloads is a *parameter server*. TensorFlow implements this as part of its distribution
    strategy for training models. Parameter servers leverage the shared memory approach:
    you have a dedicated group of servers guaranteeing strong consistency of the data
    that serves the workers with consistent information. The parameters are the weights
    and precomputed features the machine learning algorithm requires during its training
    and retraining lifecycle. In some cases, the parameters can fit into one machine’s
    memory, but in real-life use cases where there are billions of parameters, having
    a cluster of parameter servers is a requirement. We will look at this in more
    detail when we discuss the different TensorFlow distributed computing strategies
    in [Chapter 8](ch08.xhtml#tensorflow_distributed_ml_approach).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个专用于机器学习工作负载的分布式计算模型示例是*参数服务器*。TensorFlow作为其模型训练分布策略的一部分实现了这一点。参数服务器利用了共享内存的方法：您有一组专用的服务器来保证为工作节点提供一致的数据信息。这些参数是机器学习算法在其训练和重新训练生命周期中所需的权重和预计算特征。在某些情况下，这些参数可以适应一台机器的内存，但在实际应用场景中，当存在数十亿个参数时，需要一组参数服务器的集群。在我们讨论不同的TensorFlow分布式计算策略时，我们将详细讨论这一点，这将在[第8章](ch08.xhtml#tensorflow_distributed_ml_approach)中进行。
- en: With research and industry investing heavily in distributed machine learning,
    it’s only a matter of time until more models are developed. It’s always a good
    practice to keep an eye out for new developments. Hopefully, by the end of this
    book, you will have all the tools and information you need to make an educated
    decision about which of the various distributed computing models to use and how
    to leverage it to meet your business’s technological needs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 随着研究和行业在分布式机器学习上的大量投资，更多的模型开发只是时间问题。随时关注新的发展始终是一个良好的实践。希望通过本书的结尾，您将拥有所有必要的工具和信息，以便对使用哪种分布式计算模型以及如何利用它来满足您企业的技术需求做出明智的决策。
- en: Now that you are familiar with these concepts, let’s take a look at the bigger
    picture of distributed machine learning architecture and where each of these concepts
    comes into play.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经熟悉了这些概念，让我们来看看分布式机器学习架构的整体情况以及这些概念如何发挥作用。
- en: Introduction to Distributed Systems Architecture
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式系统架构简介
- en: 'We’ll begin with a brief discussion of network topologies. *Topologies* are
    how we organize computers to form distributed systems. We can divide them into
    two types: *physical* topologies that describe how the computers are arranged
    and connected, and *logical* topologies that describe the way data flows in the
    system and how the computers exchange information over the network. Multinode
    computer topologies are typically scaled physically by adding more computers (aka
    nodes).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从简要讨论网络拓扑开始。*拓扑*是我们组织计算机形成分布式系统的方式。我们可以将其分为两种类型：*物理*拓扑描述了计算机如何排列和连接，而*逻辑*拓扑描述了数据在系统中的流动方式以及计算机如何通过网络交换信息。多节点计算机拓扑通常通过增加更多计算机（也称为节点）来物理扩展。
- en: Engineers often discuss topologies in the final architecture conversation. The
    architectural requirements stem from the project’s goals, the data, the system’s
    behavior, and the existing software tools that are in use. For data scientists,
    the main goal is to define the distributed model training methods and how the
    models will be deployed and served.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师们通常在最终架构讨论中讨论拓扑。架构需求源于项目的目标、数据、系统的行为以及正在使用的现有软件工具。对于数据科学家来说，主要目标是定义分布式模型训练方法以及模型将如何部署和提供服务。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In some cases, you might identify a gap where the available software tools are
    not addressing the project’s needs well enough or are too complex to integrate
    into your solution, and you’ll need to source or develop new tools. This is an
    advanced scenario that we won’t be addressing in this book.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能会发现现有软件工具未能很好地满足项目需求，或者过于复杂而无法整合到解决方案中，因此您需要寻找或开发新的工具。这是一个我们在本书中不会探讨的高级场景。
- en: The nodes that form the topology of a distributed system are connected through
    a network in a specific architectural pattern designed to improve the load-handling
    capability and optimize speed and resource use. The architectural choices that
    are made in the design phase will impact each node’s role in the topology, how
    they communicate, and the overall system’s resilience to failure.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 形成分布式系统拓扑的节点通过网络连接在一起，这种特定的架构模式旨在提高负载处理能力并优化速度和资源使用。在设计阶段做出的架构选择将影响每个节点在拓扑中的角色，它们如何通信以及整个系统对故障的韧性。
- en: As well as understanding the physical topology of a distributed system, you
    should be aware of the differences between centralized and decentralized systems,
    how the machines interact, the modes of communication that are supported, and
    how the system handles security and failures. You can think of these as building
    blocks as you design your system.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了理解分布式系统的物理拓扑外，您还应了解集中式和分散式系统之间的区别，计算机如何交互，支持的通信模式以及系统如何处理安全性和故障。在设计系统时，您可以将这些视为构建块。
- en: Centralized Versus Decentralized Systems
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集中式与分散式系统
- en: In a *centralized* system, all the nodes depend on a single node to make decisions.
    Such a system benefits from greater control over decisions yet is more prone to
    failures as the decision-making node becomes a single point of failure that can
    bring the whole system down.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在*集中式*系统中，所有节点依赖于单一节点做出决策。这样的系统在控制决策方面具有更大的优势，但由于决策节点成为单点故障，系统更容易遭受故障而导致整个系统崩溃。
- en: In a *decentralized* system topology, the nodes are independent and make their
    own decisions. Each node stores and operates on its own data, so there is no single
    point of failure. This means the system is more tolerant of faults; however, it
    also means that the decisions made by the individual nodes need to be coordinated
    and reconciled.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在*分散式*系统拓扑中，节点是独立的并且自行做出决策。每个节点存储和操作自己的数据，因此没有单一的故障点。这意味着系统更能容忍故障；然而，这也意味着需要协调和调解各个节点做出的决策。
- en: 'Decentralized systems can benefit from a multicloud/hybrid cloud architecture,
    where machine nodes reside in different regions and with different cloud providers.
    An example is a network of connected Internet of Things (IoT) devices: each device
    is independent but shares data over the network with other devices and/or the
    cloud, depending on its internet connectivity. The topology you choose will affect
    the communication methods the devices can use and their possible roles within
    the network. When it comes to training models, a decentralized approach means
    that every model is being trained on its own. We’ll talk more about the implications
    of this design decision later in this chapter, when we look at ensemble methods.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 去中心化系统可以受益于多云/混合云架构，其中机器节点位于不同地区并与不同的云提供商合作。一个例子是连接的物联网（IoT）设备网络：每个设备是独立的，但根据其互联网连接，在网络上与其他设备和/或云共享数据。您选择的拓扑结构将影响设备可以使用的通信方法以及它们在网络中可能扮演的角色。当涉及训练模型时，去中心化方法意味着每个模型都在进行自己的训练。在本章后面，当我们研究整合方法时，我们将更多地讨论这种设计决策的影响。
- en: Interaction Models
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互模型
- en: 'The architecture of the interaction model defines how the nodes in a system
    communicate over the network, what their roles are in the system, and what responsibilities
    come along with those roles. We’ll look at three possible architectures in this
    section: client/server, peer-to-peer, and geo-distributed. There are other architectures
    in use and in development that are not covered here, but these are the ones you
    are most likely to encounter.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 交互模型的架构定义了系统中的节点如何通过网络通信、他们在系统中的角色以及这些角色所带来的责任。在本节中，我们将介绍三种可能的架构：客户端/服务器、点对点和地理分布式。这里没有涉及的其他正在使用和开发中的架构，但这些是您最有可能遇到的。
- en: Client/server
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户端/服务器
- en: In the client/server interaction model, there is a clear definition of responsibilities.
    Tasks are divided between clients, which issue requests, and servers, which provide
    the responses to those requests. The role of a node can change subject to the
    structure and needs of the system, but this depends on whether the servers are
    stateless (storing no state) or stateful (storing state that the next operations
    are based on).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端/服务器交互模型中，责任有明确定义。任务在客户端之间分配，客户端发出请求，服务器提供对这些请求的响应。一个节点的角色可以根据系统的结构和需求而改变，但这取决于服务器是无状态的（不存储状态）还是有状态的（存储下一个操作所依赖的状态）。
- en: Peer-to-peer
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 点对点
- en: 'In the peer-to-peer (P2P) interaction model, the workload is partitioned between
    the nodes, or peers. All nodes have equal privileges and can share information
    directly without relying on a dedicated central server. Every peer can be both
    a client and a server. This topology is more permissive and cheaper to implement,
    as there is no need to bind a machine to a specific responsibility. However, it
    does have some downsides: each node needs to have a full copy of the data, and
    because all data is exchanged over the network without a dedicated coordinator,
    multiple copies can reach the same node.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在点对点（P2P）交互模型中，工作负载在节点或对等体之间分割。所有节点具有相同的特权，并且可以直接共享信息，而无需依赖于专用的中央服务器。每个对等节点既可以是客户端也可以是服务器。这种拓扑更加宽容且更便宜实现，因为不需要将机器绑定到特定的责任。但是，它也有一些缺点：每个节点都需要具有数据的完整副本，并且因为所有数据都是通过网络交换而没有专用协调员，多个副本可能到达同一节点。
- en: Geo-distributed
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 地理分布式
- en: The geo-distributed interaction model is most commonly seen in geo-distributed
    cloud data centers. It aims to solve challenges related to issues such as data
    privacy and resource allocation. One concern is that the latency of peer-to-peer
    communication in the geo-distributed model may be high, depending on the distance
    between the nodes. Therefore, when developing distributed machine learning workloads
    based on this interaction model, we need to provide a clear definition of how
    the nodes communicate and in which cases. An example of when the geo-distributed
    interaction model is a good choice is to enable federated learning with IoT/edge
    devices, where data cannot be centralized in one data center. Developing a system
    to train a model on each device across multiple decentralized nodes and assemble
    the output to create one cohesive model allows us to benefit from the data insights
    of all the devices, without exchanging private information.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 地理分布式交互模型最常见于地理分布式云数据中心。它旨在解决与数据隐私和资源分配等问题相关的挑战。一个问题是，地理分布模型中点对点通信的延迟可能会很高，这取决于节点之间的距离。因此，在基于此交互模型开发分布式机器学习工作负载时，我们需要清晰定义节点如何通信以及在哪些情况下。地理分布式交互模型是一个良好选择的示例是启用物联网/边缘设备的联邦学习，其中数据无法集中在一个数据中心。开发一个系统，在多个分布式节点上对每个设备训练模型并组装输出以创建一个统一的模型，使我们能够从所有设备的数据洞察中受益，而不用交换私密信息。
- en: Communication in a Distributed Setting
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布设置中的通信
- en: How our nodes communicate in a distributed environment has a significant impact
    on failure mechanisms, security, and throughput. Communication can be synchronous
    or asynchronous, depending on the needs of the distributed computation model.
    For example, a parameter server (a dedicated machine learning computing model
    mentioned earlier in this chapter) can be implemented with asynchronous or synchronous
    communications, and TensorFlow supports both synchronous and asynchronous training
    for distributing training with data parallelism.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的节点在分布式环境中的通信方式对故障机制、安全性和吞吐量有重要影响。通信可以是同步的或异步的，取决于分布式计算模型的需求。例如，参数服务器（本章前面提到的一种专用机器学习计算模型）可以采用异步或同步通信实现，而TensorFlow支持使用数据并行进行同步和异步训练。
- en: Distributing machine learning workloads across more than one machine requires
    partitioning the data and/or the program itself, to divide the workload evenly
    across all machines. The decision of whether to use asynchronous or synchronous
    communications between the machines affects compute time and can lead to bottlenecks.
    For instance, shuffling data over the network can improve accuracy and help reduce
    overfitting. However, shuffling often involves writing data to the local disk
    before sending it over the network; this results in more input/output (I/O) operations,
    increasing the overall computation time and creating a bottleneck on the local
    disk, as well as a large amount of communication overhead. For such tasks, you
    need to examine the communication approach you take carefully.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在多台机器上分发机器学习工作负载需要对数据和/或程序本身进行分区，以便将工作负载均匀地分布在所有机器上。决定使用机器之间的异步或同步通信方式会影响计算时间，并可能导致瓶颈。例如，通过网络洗牌数据可以提高准确性并帮助减少过拟合。然而，洗牌通常涉及将数据写入本地磁盘后再发送到网络，这会导致更多的输入/输出（I/O）操作，增加总体计算时间并在本地磁盘上造成瓶颈，以及大量的通信开销。对于这样的任务，您需要仔细考虑采取的通信方法。
- en: Asynchronous
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异步
- en: 'The underlying mechanism for asynchronous communication is a *queue*. Requests
    to a given node are placed in a queue to be executed and can eventually return
    a result or not. This mechanism is useful in systems where information exchange
    is not dependent on time, as there is no need to receive a response right away.
    You can think of it like a text message: you send a text to your friend asking
    about dinner plans for next Saturday, knowing that you will likely eventually
    get a response, but you don’t need it right away. Asynchronous communication allows
    for a flow of messages in a distributed system without blocking any processes
    while waiting for a reply; it’s generally preferred when possible.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 异步通信的基本机制是*队列*。对特定节点的请求被放置在队列中执行，并最终可能返回结果或者不返回。在信息交换不依赖于时间的系统中，这种机制非常有用，因为无需立即接收响应。你可以将其想象成发送短信给朋友询问下周六晚餐计划，知道你可能最终会收到回复，但你并不急需它。异步通信允许在分布式系统中传递消息流，而不会阻塞任何等待回复的进程；通常情况下，尽可能使用它是更好的选择。
- en: Synchronous
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 同步
- en: The requirement for synchronous communication arises from the computer science
    function stack, where functions must be executed in a specific order—meaning that
    if a node sends a request to another node, it can’t continue processing further
    function logic while waiting for the response. You may use synchronous communication
    for specific distributed machine learning cases and leverage dedicated hardware
    like special network cables when necessary (some cloud vendors enable you to configure
    the network bandwidth^([2](ch01.xhtml#ch01fn2))). Suppose you want to make dinner
    plans with your friend for tonight. Your actions will depend on your friend’s
    food preferences and availability and your chosen restaurant’s availability. You
    know that if you decide on a popular restaurant and don’t book a table now, there
    won’t be any space left. What do you do? Rather than sending a text, you pick
    up the phone and call your friend to collect the information synchronously; both
    of you are now blocked as you talk on the phone. You get the necessary information
    and continue to the following stage, which is contacting the restaurant.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 需要同步通信的要求源于计算机科学的函数堆栈，其中函数必须按特定顺序执行 — 意味着如果一个节点发送请求给另一个节点，在等待响应期间无法继续处理后续函数逻辑。在特定的分布式机器学习案例中，可以使用同步通信，并在必要时利用专用硬件如特殊网络电缆（一些云供应商允许您配置网络带宽^([2](ch01.xhtml#ch01fn2))）。假设你想和朋友今晚安排晚餐计划。你的行动将取决于朋友的食物偏好和可用性，以及你选择的餐厅的可用性。你知道如果选择一家热门餐厅并且现在不预订桌子，那么可能就没有位置了。你会怎么做？你不会发短信，而是打电话给朋友，同步收集信息；你们两个现在都在电话中阻塞。你获取必要的信息并继续到下一个阶段，即联系餐厅。
- en: 'Now that you have an idea of some of the main architectural considerations
    in a distributed machine learning topology, let’s take a look at a technique that
    has been gaining in popularity in machine learning applications in recent years:
    ensemble learning.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您对分布式机器学习拓扑结构的一些主要架构考虑有了一些了解，让我们来看看近年来在机器学习应用中日益流行的一种技术：集成学习。
- en: Introduction to Ensemble Methods
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成方法介绍
- en: '*Ensemble* machine learning methods use multiple machine learning algorithms
    to produce a single model with better performance and less bias and variance than
    the individual algorithms could achieve on their own. Ensemble methods are often
    designed around supervised learning and require a clear definition of model aggregation
    that can be used during prediction time. We’ll begin by exploring why these methods
    are useful, then look at the main types of ensemble methods in machine learning
    and some specific examples.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*集成*机器学习方法利用多个机器学习算法生成一个性能更好、偏差和方差较少的单一模型。集成方法通常围绕监督学习设计，并要求在预测时有明确的模型聚合定义。我们将从探讨这些方法为何有用开始，然后看看机器学习中主要类型的集成方法及一些具体示例。'
- en: High Versus Low Bias
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高偏差与低偏差
- en: Bias is a major problem in machine learning, and reducing bias is one of the
    machine learning engineer’s main goals. A model with high bias makes too many
    assumptions about the results, leading to *overfitting* to the training data.
    Such a model tends to have difficulty making accurate predictions about new data
    that doesn’t exactly conform to the data it has already seen and will perform
    badly on test data and in production. Conversely, a model with low bias incorporates
    fewer assumptions about the data. Taken to an extreme, this can also be problematic
    as it can result in *underfitting*, where the model fails to learn enough about
    the data to classify it accurately. Models with high bias tend to have low variance,
    and vice versa. You can think of variance as the ability of a machine learning
    algorithm to deal with fluctuations in the data.^([3](ch01.xhtml#ch01fn3))
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差是机器学习中的一个主要问题，减少偏差是机器学习工程师的主要目标之一。高偏差的模型对结果做出太多假设，导致*过拟合*训练数据。这样的模型往往难以对新数据进行准确预测，因为新数据可能与已经观察到的数据不完全一致，在测试数据和生产环境中表现不佳。相反，低偏差的模型对数据做出的假设较少。如果过度，这也可能是一个问题，因为它可能导致*欠拟合*，即模型未能充分学习数据以进行准确分类。高偏差的模型往往具有低方差，反之亦然。方差可以被看作是机器学习算法处理数据波动能力的指标。^([3](ch01.xhtml#ch01fn3))
- en: Often, bias can also derive from the machine learning algorithm itself. For
    example, linear regression is a simple algorithm that learns fast but frequently
    has high bias, especially when used to model the relationship between two variables
    when no real linear (or close to linear) correlation exists between them. It all
    depends on the underlying relationship of the features.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的情况是，偏差也可能来自机器学习算法本身。例如，线性回归是一种简单的算法，学习速度快，但往往存在较高的偏差，特别是在用于模拟两个变量之间没有真实的线性（或接近线性）相关性的关系时。所有这些都取决于特征之间的潜在关系。
- en: Types of Ensemble Methods
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成方法的类型
- en: In many cases, ensemble methods turn out to be more accurate than single models;
    by combining the individual predictions of all the component models, they are
    able to produce more robust results. In ensembles, each model is called a *learner*.
    We define the relationship between the learners based on the desired goal.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，集成方法的准确性比单一模型更高；通过结合所有组件模型的个体预测，它们能够产生更健壮的结果。在集成中，每个模型被称为一个*学习器*。我们根据期望的目标定义学习器之间的关系。
- en: When we want to reduce variance, we often build dependencies between the learners
    by training them in a sequential manner. For example, we might train one decision
    tree at a time, with each new tree trained to correct the mistakes of the previous
    trees in the series. This strategy of building multiple learners with the goal
    of reducing previous learners’ mistakes is known as *boosting*. The ensemble model
    makes its final predictions by weighting the votes, calculating the majority vote,
    or calculating an overall sum that acts as the prediction or classification. An
    example is the gradient-boosted trees classifier (`GBTClassifier`) implemented
    in Spark MLlib. This is an ensemble technique to iteratively combine decision
    trees using a deterministic averaging process; the algorithm’s goal is to minimize
    the information lost in training/mistakes (more on this in [Chapter 5](ch05.xhtml#feature_engineering)).
    It’s important to be aware, however, that the iterative combination of the trees
    can sometimes lead to overfitting.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要减少方差时，通常通过依次训练学习器来建立它们之间的依赖关系。例如，我们可以一次训练一棵决策树，每棵新树都根据系列中先前树的错误进行训练修正。这种构建多个学习器以减少先前学习器错误的策略被称为*提升*。集成模型通过加权投票、计算多数投票或计算作为预测或分类的总体总和来进行最终预测。一个例子是在Spark
    MLlib中实现的梯度提升树分类器（`GBTClassifier`）。这是一种集成技术，通过确定性平均过程迭代地组合决策树；该算法的目标是在训练中最小化信息损失/错误（更多信息请参阅[第5章](ch05.xhtml#feature_engineering)）。然而，需要注意的是，树的迭代组合有时可能导致过拟合。
- en: To avoid  overfitting, we might prefer training the learners independently in
    parallel and combining their predictions using *bagging* or *stacking*. With the
    bagging technique (short for *bootstrap aggregation*), we train each learner (typically
    all using the same machine learning algorithm) on a different part of the dataset,
    with the goal of reducing variance and overfitting and improving the accuracy
    of predictions on previously unseen data. The outcome of this ensemble method
    is a combined model where each learner makes its prediction independently and
    the algorithm collects all the votes and produces a final prediction. An example
    is the `Random​Forest​Classi⁠fier` implemented in Spark MLlib (random forest is
    an ensemble technique to combine independent decision trees).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过拟合，我们可能更喜欢并行独立训练学习者，并使用*装袋*或*叠加*来组合它们的预测。使用装袋技术（即*自助聚合*），我们在数据集的不同部分上训练每个学习者（通常都使用相同的机器学习算法），目的是减少方差和过拟合，提高对之前未见数据的预测准确性。这种集成方法的结果是一个组合模型，其中每个学习者都独立作出预测，算法收集所有投票并产生最终预测。一个例子是在Spark
    MLlib中实现的`Random​Forest​Classi⁠fier`（随机森林是一种结合独立决策树的集成技术）。
- en: Stacking is similar to bagging, in that it involves building a set of independent
    learners and combining their predictions using an ensemble function that takes
    the output from all the learners and reduces it into a single score. However,
    with stacking, the learners are usually of different types, rather than all using
    the same learning algorithm, which means they make different assumptions and are
    less likely to make the same kinds of errors. You can use any type of machine
    learning model as the combiner that aggregates the predictions. A linear model
    is often used, but it can be nonlinear, taking in the learners’ scores together
    with the given data—for example, a neural network where the base learners are
    decision trees. This approach is more advanced and can help uncover deeper relationships
    between the variables.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 叠加类似于装袋，它涉及构建一组独立的学习者，并使用一个集成函数来组合它们的预测，该函数将所有学习者的输出减少为单一分数。然而，叠加中的学习者通常是不同类型的，而不是都使用相同的学习算法，这意味着它们做出不同的假设，更不可能出现相同类型的错误。您可以使用任何类型的机器学习模型作为组合器来聚合预测。通常使用线性模型，但也可以是非线性的，将学习者的分数与给定数据一起输入，例如基础学习者是决策树的神经网络。这种方法更加先进，可以帮助揭示变量之间更深的关系。
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Ensemble methods are said to be *homogeneous* when the learners have the same
    base learning algorithm. Ensembles whose learners have different base learning
    algorithms are referred to as *heterogeneous* ensembles. Boosting and bagging
    are considered homogeneous ensemble methods, while stacking is a heterogeneous
    method.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法在学习者使用相同基础学习算法时被称为*同质*。而学习者使用不同基础学习算法时，则称为*异质*集成。提升（Boosting）和装袋（Bagging）被认为是同质集成方法，而叠加（Stacking）是一种异质方法。
- en: Distributed Training Topologies
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式训练拓扑结构
- en: You can leverage cluster topologies to improve training and serving of ensemble
    models. Let’s take a look at a couple of examples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以利用集群拓扑结构来改善集成模型的训练和服务。让我们看几个例子。
- en: Centralized ensemble learning
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集中式集成学习
- en: Centralized systems often use a client/server architecture, where client nodes
    are communicating directly with centralized server nodes. It resembles a star
    topology in computer networks. In a distributed machine learning deployment approach,
    this means that all the requests for predictions, classifications, etc., from
    the distributed models go through the main servers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 集中式系统通常使用客户端/服务器架构，其中客户端节点直接与集中式服务器节点通信。这类似于计算机网络中的星型拓扑。在分布式机器学习部署方法中，这意味着来自分布式模型的所有预测、分类等请求都通过主服务器进行。
- en: Whether there’s one server node or several that act as the final decision makers,
    there is a strict hierarchical logic for aggregation at the server level that
    happens in a centralized location. This topology is dedicated specifically to
    a distributed model workload and is not general-purpose. For example, consider
    the random forest ensemble learning method. `RandomForest` is a bagging algorithm
    that can be used for classification or regression, depending on the nature of
    the data, and aims to mitigate overfitting, as described earlier. A random forest
    consists of a collection of decision trees. When you decide to leverage `RandomForest`
    as your algorithm, the program making the queries will interact as a client interacting
    with the main server nodes. Those server nodes will send the queries to the tree
    nodes, collect answers (the output of the model) from the trees, aggregate the
    output based on ensemble logic, and return the answers to the client. The individual
    trees in the ensemble may be trained on completely different or overlapping datasets.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否有一个或多个充当最终决策者的服务器节点，服务器级别的聚合都有严格的层次逻辑，在集中的位置进行。这种拓扑结构专门用于分布式模型工作负载，并非通用。例如，考虑随机森林集成学习方法。`RandomForest`是一种装袋算法，可以根据数据的性质用于分类或回归，并旨在减少过拟合，如前所述。随机森林包含一组决策树。当你决定将`RandomForest`作为你的算法时，程序将作为客户端与主服务器节点交互。这些服务器节点将查询发送到树节点，收集来自树的答案（模型的输出），根据集成逻辑聚合输出，并将答案返回给客户端。集成中的各个树可以在完全不同或重叠的数据集上训练。
- en: Decentralized decision trees
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分散的决策树
- en: Decision trees can be deployed in a decentralized topology as well. You can
    use this approach when you want to provide answers on edge devices and are constrained
    by data privacy concerns, internet bandwidth, and strict time requirements for
    the responses. Decentralized decision trees are useful for Edge AI, where the
    algorithm is processed and the model is served locally on the device. Each node
    does not need to be permanently connected to the network, though it can leverage
    the network to improve the accuracy of its predictions and avoid overfitting.
    In this case, when an edge node receives a query requesting it to make a prediction,
    it sends the query to its parent and child nodes, which in turn send the query
    to their parents and children, and each node calculates and broadcasts its response.
    Each node will have its own aggregation function and can decide whether or not
    to use it based on whether its response is available within the specified time
    constraints. To keep communication overhead to a minimum, you can limit the number
    of “hops,” to define how far a query can travel. This constraint enforces a *node
    neighborhood*. A node’s neighborhood can change based on network and edge device
    availability.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树也可以在分散的拓扑结构中部署。当你希望在边缘设备上提供答案，并受到数据隐私、互联网带宽和响应时间严格要求的限制时，可以采用这种方法。分散的决策树对边缘人工智能很有用，其中算法在设备上本地处理并提供模型服务。每个节点不需要永久连接到网络，但可以利用网络提高预测的准确性并避免过度拟合。在这种情况下，当边缘节点收到一个请求进行预测时，它将查询发送给其父节点和子节点，这些节点又将查询发送给它们的父节点和子节点，每个节点计算并广播其响应。每个节点都有自己的聚合函数，并根据其响应是否在指定时间限制内可用来决定是否使用它。为了将通信开销降到最低，可以限制“跳数”，定义查询可以传播的最远距离。这个约束强制执行一个*节点邻域*。节点的邻域可以根据网络和边缘设备的可用性而变化。
- en: Centralized, distributed training with parameter servers
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用参数服务器进行集中式、分布式训练
- en: In a centralized distributed training topology, the entire workload is processed
    in one data center. The machines are well connected and communicate over a shared
    network. The dataset and the training workload are spread out among the client
    nodes, and the server nodes maintain globally shared parameters. The server nodes
    act as parameter servers that all the client nodes share access to and consume
    information from—a global shared memory. They have to have fast access to the
    information and often leverage in-memory data structures. One family of machine
    learning algorithms that can leverage this topology is deep learning algorithms.
    With this approach, the parameters are broadcasted and replicated across all the
    machines, and each client node separately calculates its own part of the main
    function. The variables are created on the parameter servers and shared and updated
    by the client or worker nodes in each step.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在集中式分布式训练拓扑结构中，整个工作负载在一个数据中心中处理。机器之间连接良好，并通过共享网络进行通信。数据集和训练工作负载分布在客户端节点中，服务器节点维护全局共享参数。服务器节点充当参数服务器，所有客户端节点共享对其访问并从中获取信息的权限—即全局共享内存。它们必须快速访问信息，并经常利用内存中的数据结构。可以利用这种拓扑结构的一类机器学习算法是深度学习算法。采用这种方法，参数在所有机器上广播和复制，并且每个客户端节点分别计算其主函数的一部分。变量在参数服务器上创建，并由客户端或工作节点在每个步骤中共享和更新。
- en: '[Chapter 8](ch08.xhtml#tensorflow_distributed_ml_approach) discusses this strategy
    in detail, with code examples illustrating making use of this topology with TensorFlow.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](ch08.xhtml#tensorflow_distributed_ml_approach)详细讨论了这一策略，代码示例说明了如何利用TensorFlow与这种拓扑结构。'
- en: Centralized, distributed training in a P2P topology
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集中式、分布式训练在点对点拓扑中
- en: In a peer-to-peer topology, there are no client and server roles. All nodes
    can communicate with all of the other nodes, and each node has its own copy of
    the parameters. This is useful for leveraging data parallelism when there are
    a fixed number of parameters that can fit into the nodes’ memory, the logic itself
    does not change, and the nodes can share their outcomes in a peer-to-peer manner.
    *Gossip learning* is one example of a method that uses this approach. Each node
    computes its model based on the dataset available to it and performs independent
    calls to its peers on the network to share its model with them. The nodes then
    each combine their current model with their neighbors’ models. As with decision
    trees in a decentralized deployment environment, this topology should be restricted
    with time constraints, and the maximum number of edges each node will broadcast
    the information to should be defined. With the P2P topology, you might also want
    to specify a protocol like MPI to standardize the workload.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在点对点拓扑结构中，没有客户端和服务器角色。所有节点可以与其他节点通信，并且每个节点都有自己的参数副本。当节点的内存能容纳的参数数量固定时，逻辑本身不会改变，并且节点可以以点对点的方式共享其结果时，利用数据并行性非常有用。*传话学习*
    是使用这种方法的一个例子。每个节点根据其可用的数据集计算其模型，并独立地调用网络上的同行以与它们分享其模型。然后每个节点将其当前模型与其邻居的模型结合起来。在去中心化部署环境中，就像决策树一样，应该限制这种拓扑结构的时间约束，并定义每个节点将广播信息到的最大边数。使用P2P拓扑结构时，您可能还希望指定诸如MPI之类的协议以标准化工作负载。
- en: The Challenges of Distributed Machine Learning Systems
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式机器学习系统的挑战
- en: Rome was not built in a day, but they were laying bricks every hour.
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 罗马并非一日建成，但每小时都在砌砖。
- en: ''
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —John Heywood
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: —约翰·海伍德
- en: Although you’re just beginning your journey into distributed machine learning,
    it’s important to be aware of some of the challenges that lie ahead. Working with
    distributed machine learning is significantly different from developing machine
    learning workloads that will run on one machine, and ultimately it’s your responsibility
    to build a system that meets the defined requirements. However, experienced practitioners
    will tell you that *all requirements are negotiable* and *what can fail will fail*.
    Both of these are true and should be kept in mind as you weigh your efforts throughout
    the process.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管您刚刚开始涉足分布式机器学习的旅程，但重要的是要意识到前方可能存在的一些挑战。与开发在单台机器上运行的机器学习工作负载不同，处理分布式机器学习工作负载是显著不同的，最终您有责任构建符合定义要求的系统。然而，经验丰富的从业者会告诉您*所有要求都是可以协商的*，*可能失败的事情最终会失败*。这两点都是真实的，您在权衡整个过程中应该记在心里。
- en: Performance
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: Improving performance is the fundamental goal of implementing a distributed
    system. Achieving higher throughput and performing end-to-end machine learning
    computations faster are critical justifications for distributed machine learning
    systems. There are many approaches you can take to improve performance, depending
    on your goal, your data, and the behavior of your system. Let’s take a look at
    some of the things you should consider and some problems you will likely face.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 提升性能是实施分布式系统的基本目标。实现更高的吞吐量和更快的端到端机器学习计算对于分布式机器学习系统至关重要。根据您的目标、数据和系统行为，有许多方法可以提高性能。让我们看看您应该考虑的一些事项以及您可能会遇到的一些问题。
- en: Data parallelism versus model parallelism
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据并行与模型并行
- en: In computer science, distributed computing often goes hand in hand with parallel
    computing. Parallel computing on a single node/computer means using that single
    node’s multiple processors to perform various tasks at the same time. This is
    also called *task-parallel processing*. In comparison, in the context of distributed
    computing, parallel computing refers to using numerous nodes to perform tasks,
    with each node operating in parallel. When discussing distributed computation,
    parallel computation is a given and won’t be mentioned explicitly.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，分布式计算通常与并行计算相辅相成。在单个节点/计算机上的并行计算意味着利用该单个节点的多个处理器同时执行各种任务。这也被称为*任务并行处理*。相比之下，在分布式计算的背景下，并行计算指的是使用多个节点来执行任务，每个节点都并行操作。在讨论分布式计算时，并行计算是一种基本的概念，不会被显式提及。
- en: One of the most significant sources of confusion when approaching distributed
    machine learning is the lack of a clear understanding of what precisely is distributed
    across the nodes. In the machine learning workflow/lifecycle, you preprocess your
    data, perform feature engineering to extract relevant features, enrich the data,
    and finally ingest it into your machine learning algorithm along with a set of
    *hyperparameters* (parameters whose values are used to control the learning process,
    also known as the machine learning algorithm’s tuning parameters). During the
    learning process, the values/data ingested are affected by the hyperparameters,
    so it is recommended that you try out a wide range of hyperparameters to ensure
    that you identify the best possible model to use in production.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及分布式机器学习时，最显著的困惑源之一是对节点间分布的确切理解不清楚。在机器学习工作流/生命周期中，您预处理数据，进行特征工程以提取相关特征，丰富数据，最终将其与一组*超参数*（用于控制学习过程的参数值，也称为机器学习算法的调优参数）一同输入到机器学习算法中。在学习过程中，输入的值/数据受超参数的影响，因此建议您尝试广泛的超参数范围，以确保找到在生产中使用的最佳模型。
- en: 'Dealing with a large set of data and a large set of tuning parameters raises
    the issue of how to manage your resources and the training process efficiently.
    Generally speaking, there are two approaches to training machine learning algorithms
    at scale. You can have the same algorithm with the same hyperparameters duplicated
    across all the nodes, with every machine running the same logic on its own piece
    of the data. Conversely, you can have each node running a different part of the
    algorithm, on the same set of data. [Figure 1-3](#data_parallelism_and_model_parallelism)
    illustrates the difference between these approaches: with *data parallelism*,
    the data is split into shards or partitions, and those partitions are distributed
    among the nodes, while with *model parallelism*, the model itself is split into
    pieces and distributed across machines.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大量数据和大量调优参数带来了如何有效管理资源和训练过程的问题。一般来说，有两种训练机器学习算法的方法。你可以在所有节点上复制相同的算法和相同的超参数，每台机器在自己的数据片段上运行相同的逻辑。相反，你也可以让每个节点运行算法的不同部分，但在同一组数据上运行。[图1-3](#data_parallelism_and_model_parallelism)展示了这些方法的不同之处：*数据并行*将数据分割成碎片或分区，并将这些分区分布在节点之间，而*模型并行*则将模型本身分割成片段，并分布在多台机器上。
- en: '![](assets/smls_0103.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0103.png)'
- en: Figure 1-3\. Data parallelism and model parallelism
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3\. 数据并行和模型并行
- en: With data parallelism, each node runs the same computation logic, which means
    that code must also be distributed across the nodes. From node to node, the data
    input changes, but all the nodes run the same code, as illustrated in [Figure 1-4](#data_parallelism_the_same_logic_is_dist).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据并行化，每个节点运行相同的计算逻辑，这意味着代码也必须分布到所有节点。从一个节点到另一个节点，数据输入发生变化，但所有节点都运行相同的代码，如[图 1-4](#data_parallelism_the_same_logic_is_dist)所示。
- en: '![](assets/smls_0104.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0104.png)'
- en: 'Figure 1-4\. Data parallelism: the same logic is distributed to all the machines,
    and each machine runs the logic with its local data'
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4\. 数据并行化：相同的逻辑分布到所有机器上，每台机器使用本地数据运行该逻辑。
- en: With model parallelism, multiple nodes each execute different pieces of the
    machine learning algorithm, and the distributed outputs are then assembled to
    produce the model itself. This approach is suitable for algorithms that can be
    parallelized by representing them in a directed acyclic graph, where the vertices
    represent the computations and the edges represent the data flow.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型并行化，多个节点分别执行机器学习算法的不同部分，然后将分布式输出组装起来生成模型本身。这种方法适用于可以通过有向无环图表示并行化的算法，其中顶点代表计算，边代表数据流。
- en: One challenge in serving an existing model is that sometimes the model itself
    can’t fit into one machine’s memory and requires adjustments to be served optimally.
    Think about the random forest ensemble method. It may be that the whole forest
    can fit into a single machine’s memory, but what if the forest has billions of
    trees? One option (which requires dedicated tools and hardware) is to divide the
    model into subsets, placing each one on a different machine, and have the machines
    communicate in a well-defined manner to serve the model efficiently. This approach
    is often employed by deep learning frameworks such as PyTorch and TensorFlow.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供现有模型时的一个挑战是，有时模型本身无法适应单台机器的内存，并且需要进行调整以实现最佳服务。想想随机森林集成方法。也许整个森林可以适应单台机器的内存，但如果森林有数十亿棵树呢？一种选项（需要专用工具和硬件）是将模型分割成子集，将每个子集放置在不同的机器上，并以定义良好的方式使机器有效地进行通信以提供模型。这种方法通常被深度学习框架（如PyTorch和TensorFlow）采用。
- en: Combining data parallelism and model parallelism
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合数据并行化和模型并行化
- en: Combining data and model parallelism is not at all straightforward, due to the
    nature of the existing open source tools and the complexity of building a dedicated
    system that will leverage both. However, whereas in the past we had to choose
    between data parallelism tools such as Apache Spark and model parallelism tools
    such as PyTorch, today many of these tools support each other, either natively
    or through extensions like Petastorm, Horovod, and others.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 结合数据并行化和模型并行化并不是一件简单的事，这是因为现有开源工具的性质以及构建利用这两者的专用系统的复杂性。然而，过去我们不得不在数据并行化工具（如Apache
    Spark）和模型并行化工具（如PyTorch）之间做选择，如今许多这些工具通过原生支持或通过Petastorm、Horovod等扩展支持彼此。
- en: The need to combine both types of parallelism can have a significant effect
    on how long it takes to produce a new model, serve it, and start using it for
    prediction. For example, GPT-3 (Generative Pre-trained Transformer 3), a model
    developed by OpenAI, uses deep learning to produce human-like text. At its maximum
    capacity of 175 billion parameters, it is estimated that it would take 355 years
    and cost $4.6 million to train this model with a Tesla v100 GPU. For most companies,
    even with a far smaller number of parameters, this is an expensive and extremely
    slow option. Not only that, but it takes multiple tries to find suitable hyperparameters
    that will yield accurate results. We won’t discuss this GPT-3 further in the book,
    but it is important to know that it exists.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 需要同时结合这两种并行方式会显著影响生成新模型、提供服务和开始用于预测所需的时间。例如，由OpenAI开发的GPT-3（生成式预训练转换器3）使用深度学习生成类似人类的文本。在其最大容量的1750亿个参数的情况下，估计使用Tesla
    v100 GPU训练这个模型需要355年，并且成本高达460万美元。对于大多数公司来说，即使参数远少于此，这都是一种昂贵且极其缓慢的选项。不仅如此，还需要多次尝试以找到能产生准确结果的合适超参数。我们不会在本书中进一步讨论这个GPT-3，但重要的是知道它的存在。
- en: Deep learning
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习
- en: Deep learning algorithms pose a particular challenge for distributed machine
    learning performance. Deep learning is based on an artificial neural network (ANN)
    with feature learning, which means that the system discovers the features automatically
    from the raw data. Training a deep learning model requires forward computation
    and/or backward propagation. *Forward computation* is the act of feeding the data
    forward in the neural network (NN) to calculate the outcome. *Backward propagation*,
    or *backpropagation*, is the act of feeding the “loss” of accuracy backward into
    the neural network through the layers in order to understand how much of that
    loss every node is responsible for, and updating the weights of the neural network
    layers accordingly. To simplify it, you can think of it as feeding inaccuracies
    back into the model to fix them.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法对分布式机器学习性能提出了特殊挑战。深度学习基于具有特征学习的人工神经网络（ANN），这意味着系统可以自动从原始数据中发现特征。训练深度学习模型需要进行前向计算和/或反向传播。*前向计算*是将数据前向馈送到神经网络（NN）中以计算结果。*反向传播*是将精度损失向后馈送到神经网络中，通过各层了解每个节点对该损失的贡献，并相应地更新神经网络层的权重。简单来说，可以将其视为将不准确性反馈到模型中以修正它们。
- en: Both forward computation and backpropagation inherently require sequential computation.
    Each layer or stage must wait for input from the former stage. While we can distribute
    each stage on its own, the model training as a whole is sequential. As a result,
    we still need orchestration to enforce a sequence or some form of an automated
    pipeline during the training. One scheduling algorithm that enables us to run
    distributed deep learning workloads is gang scheduling. Based on this algorithm,
    the community introduced *barrier execution mode* in Apache Spark 2.4, which allows
    us to create groups of machines that work together on a stage and proceed to the
    next stage only when they have all finished. Barrier execution mode is part of
    Project Hydrogen, which aims to enable a greater variety of distributed machine
    learning workloads on the general-purpose Apache Spark framework.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 前向计算和反向传播本质上需要顺序计算。每个层或阶段必须等待前一个阶段的输入。虽然我们可以将每个阶段分布在各自的机器上，但整个模型的训练仍然是顺序的。因此，在训练期间我们仍然需要协调以强制执行顺序或某种形式的自动化流水线。一种调度算法可以使我们运行分布式深度学习工作负载，即团体调度。基于此算法，社区在Apache
    Spark 2.4中引入了*屏障执行模式*，允许我们创建一组机器，它们在一个阶段上共同工作，并且只有当它们全部完成后才能继续下一个阶段。屏障执行模式是Project
    Hydrogen的一部分，旨在在通用Apache Spark框架上启用更多样化的分布式机器学习工作负载。
- en: Resource Management
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源管理
- en: Deciding how to split cluster resources is one of the biggest challenges in
    a distributed system. When you add in distributed machine learning workloads as
    well, it gets even more complicated. The reason for that is the need to improve
    performance by pairing the software with dedicated hardware. And it’s not only
    the GPU versus CPU conversation—today, Intel, NVIDIA, Google, and other companies
    are producing machines that are built with dedicated hardware chips for AI. These
    *AI accelerators* are built for high-performance massive parallel computation
    that goes beyond the traditional threaded algorithms. What’s more, many machine
    learning algorithms are still evolving. This is why Microsoft introduced field-programmable
    gate array (FPGA) chips to its cloud, as part of Project Catapult to enable fast
    real-time AI serving of deep learning. FPGAs have a reconfigurable design, which
    makes it easier to adjust the hardware as needed after software updates.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 决定如何分配集群资源是分布式系统中最大的挑战之一。当你还要考虑分布式机器学习工作负载时，情况变得更加复杂。其原因在于需要通过将软件与专用硬件配对来提高性能。而不仅仅是GPU与CPU的讨论
    —— 今天，英特尔、NVIDIA、谷歌和其他公司正在生产配备专用硬件芯片的机器，用于AI。这些*AI加速器*专为高性能大规模并行计算而构建，超越了传统的线程算法。此外，许多机器学习算法仍在不断发展。这就是为什么微软在其云中引入可编程门阵列（FPGA）芯片的原因，作为Project
    Catapult的一部分，以便在软件更新后实现快速实时深度学习AI服务。FPGA具有可重新配置的设计，使其能够根据需要调整硬件配置。
- en: Resource sharing is also a challenge in a distributed environment, when there
    are competing workloads. When there is a need for 10 machines but only 5 are available,
    the software can either use what it has or wait for more machines to become available.
    This creates a bottleneck and can result in great pain. Think about a scenario
    where you are running machine learning training in a production environment to
    save on resources, and your training workloads compete for resources with your
    product’s real-time workloads. You might find yourself in trouble with customers
    at that point. This is why it’s best to have multiple environments for critical
    workloads and long-term/offline workloads. But what if your model is critical,
    and training it now on fresh data might allow you to discover an unforeseen real-time
    trend in the industry (and missing out on this might result in revenue loss)?
    You might need to maintain two environments for critical workloads, though that
    can be costly and results in a low return on investment (ROI).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布环境中，资源共享也是一个挑战，特别是在有竞争工作负载时。例如，需要 10 台机器，但只有 5 台可用时，软件可以选择使用现有资源或等待更多机器可用。这会导致瓶颈，并可能造成严重问题。想象一下，在生产环境中运行机器学习训练以节省资源，而你的训练工作负载与产品的实时工作负载竞争资源。在这种情况下，你可能会遇到客户的问题。因此，最好为关键工作负载和长期/离线工作负载准备多个环境。但如果你的模型非常关键，现在使用新数据进行训练可能会帮助你发现行业中未预料到的实时趋势（而错过这一点可能会导致收入损失）。也许你需要为关键工作负载维护两个环境，尽管这可能成本高昂且投资回报率低。
- en: Reconfiguring and resource sharing are only part of the story. Another challenge
    is automating the decision of when to use GPUs versus CPUs or FPGAs and other
    hardware options that are available in the market. With the cloud and a sufficient
    budget, we can get all the hardware we need, but again, we have to think about
    ROI. What should we do? How can we automate this decision? There is as yet no
    definitive answer to that question, but the happy news is that more and more software
    and hardware solutions are introducing support for one another. For example, NVIDIA
    created RAPIDS, which is a suite of open source libraries layered on top of NVIDIA
    CUDA processors. CUDA enables GPU acceleration of data science processes. With
    RAPIDS support for Apache Spark 3.0 accelerating not only data science workloads
    but also ETL/data preparation, we can potentially build a cluster based on it
    that will power both data preparation and model training and serving, eliminating
    the need to automate a switch in resources (although the question of ROI remains).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 重新配置和资源共享只是问题的一部分。另一个挑战是自动化决定何时使用GPU与CPU或FPGA等市场上其他可用的硬件选项。在有足够预算的情况下，我们可以获取所有需要的硬件，但同样需要考虑投资回报率。我们应该怎么做？如何自动化这个决策？对于这个问题还没有明确的答案，但好消息是，越来越多的软件和硬件解决方案开始相互支持。例如，NVIDIA
    创建了RAPIDS，这是一套在NVIDIA CUDA处理器之上的开源库。CUDA可以加速数据科学过程的GPU加速。有了RAPIDS对Apache Spark
    3.0的支持，不仅可以加速数据科学工作负载，还可以加速ETL/数据准备，我们可以构建一个基于它的集群，用于数据准备、模型训练和服务，从而无需自动化切换资源（尽管投资回报率的问题仍然存在）。
- en: Fault Tolerance
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容错性
- en: 'Fault tolerance is what allows a distributed system to continue operating properly
    in case of a failure. In distributed machine learning, failures can take two forms:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式系统中，容错性是指在发生故障时仍能保证系统正常运行的能力。在分布式机器学习中，故障可以采取两种形式：
- en: Typical failure of a machine that can be detected and mitigated
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可检测和缓解的典型机器故障
- en: Undetected failure of a machine that produced bad output
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未检测到机器故障导致产生错误输出
- en: 'Let’s start with the first one. To better understand the need for a fault-tolerant
    procedure, ask yourself: If we distribute our workload to a cluster of 1,000 computational
    nodes, what will happen if one of those nodes crashes? Is there a way to fix it
    other than just restarting the job from the very beginning?'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个问题开始。为了更好地理解容错程序的必要性，请问自己：如果我们将工作负载分布到 1,000 个计算节点的集群中，如果其中一个节点崩溃会发生什么？除了重新从头开始重新启动作业之外，还有其他解决方法吗？
- en: 'When one of the stages fails, do we need to recompute everything? The answer
    is no. Today, many distributed computation frameworks have a built-in procedure
    for fault tolerance: they achieve it by replicating the data and writing information
    to disk between stages for faster recovery. Other frameworks leave defining this
    mechanism up to us. For example, in TensorFlow with synchronous training, if one
    of the workers fails and we didn’t provide a fault tolerance procedure, the whole
    cluster will fail. This is why, when deciding on the TensorFlow distribution strategy,
    we need to pay attention to the fault tolerance mechanism. On the other hand,
    Apache Spark does not expose this decision to us. Rather, it has a built-in hidden
    mechanism that we cannot tweak from the machine learning API itself. Sticking
    with the automatic data-parallel workload fault tolerance mechanism in Spark saves
    us a lot of time by eliminating the need to think through the possible failure
    cases and solutions.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当其中一个阶段失败时，我们需要重新计算所有内容吗？答案是否定的。如今，许多分布式计算框架都具有内置的容错程序：它们通过复制数据并在阶段之间将信息写入磁盘以实现更快的恢复。其他框架将定义这种机制的任务交给我们。例如，在TensorFlow中进行同步训练时，如果一个工作节点失败而我们没有提供容错程序，整个集群将失败。这就是为什么在决定TensorFlow分布策略时，我们需要注意容错机制。另一方面，Apache
    Spark不向我们公开这个决定。相反，它有一个内置的隐藏机制，我们无法从机器学习API本身调整。在Spark中坚持使用自动数据并行工作负载容错机制可以节省大量时间，因为我们无需考虑可能的失败案例和解决方案。
- en: The second type of failure is specific to distributed machine learning, as it
    directly impacts the performance of the machine learning algorithm itself. In
    this case, we can look at it as if we have a Byzantine adversary machine or an
    intentionally or unintentionally faulty agent. Faulty agents (or adversaries)
    can harm the machine learning model’s performance by exposing erroneous data.
    It’s hard to mitigate such behavior, and the effects are highly dependent on the
    algorithm we use. Detecting such failures is one reason why it’s important to
    monitor machine learning models, as discussed in [Chapter 10](ch10.xhtml#deployment_patterns_for_machine_learnin).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种失败类型特定于分布式机器学习，直接影响机器学习算法本身的性能。在这种情况下，我们可以将其视为存在拜占庭对手机或故意或无意地存在故障代理。故障代理（或对手）可以通过暴露错误数据来损害机器学习模型的性能。很难减轻这种行为的影响，而这种影响高度依赖于我们使用的算法。检测这类失败是监控机器学习模型的重要原因之一，正如在[第10章](ch10.xhtml#deployment_patterns_for_machine_learnin)中讨论的那样。
- en: Privacy
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私
- en: Discussions of privacy in machine learning generally focus on protecting data
    collected from users/customers or protecting the model and parameters themselves.
    The model and its parameters can be the company’s intellectual property, and it
    might be important to keep them private (for example, in a financial market system).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中关于隐私的讨论通常集中在保护从用户/客户收集的数据或保护模型和参数本身。模型及其参数可以是公司的知识产权，保持它们的私密性可能很重要（例如，在金融市场系统中）。
- en: One option for enforcing data privacy is avoiding the restrictions of centralizing
    data. That is, we want to build a model without uploading the members’ training
    data to centralized servers. To do that, we can leverage techniques like *federated
    learning*. With this approach, we train the algorithm on the edge devices, with
    each device using its own data. The devices then each exchange a summary of the
    model they built with other devices or a dedicated server. However, this approach
    is not foolproof. An adversarial attack can happen during the training itself,
    with the attacker getting hold of some or all of the data or the results of the
    training. This can easily occur during federated learning when the attacker takes
    an active part in the training process through their edge devices.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 实施数据隐私的一种选择是避免集中数据的限制。也就是说，我们希望构建一个模型，而无需将成员的训练数据上传到集中服务器。为此，我们可以利用*federated
    learning*等技术。采用这种方法，我们在边缘设备上训练算法，每个设备使用自己的数据。然后，设备将他们构建的模型摘要与其他设备或专用服务器交换。然而，这种方法并非万无一失。在训练过程中可能会发生对抗性攻击，攻击者获取一部分或全部数据或训练结果。这在联合学习中很容易发生，当攻击者通过他们的边缘设备积极参与训练过程时。
- en: Let’s assume we have found a way to securely centralize the data or train the
    models without doing this. There is still a chance that a malicious actor can
    recover information about the data (statistics about a specific population, classification
    classes, and more) that we used to train the model by interacting with the model
    itself. As you can probably tell by now, there is no one-size-fits-all solution
    when it comes to ensuring privacy in machine learning—it requires dedicated technologies
    and architectures. Although privacy in distributed machine learning is a fascinating
    topic, it’s also a big one, and we won’t discuss it further in this book.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已找到一种安全地集中数据或训练模型而无需这样做的方法。仍存在一个可能性，即恶意行为者通过与模型本身交互，可以恢复关于数据（关于特定人群的统计数据、分类类别等）的信息，这些信息被用于训练模型。正如您现在可能已经注意到的那样，在确保机器学习中的隐私性方面，并没有一种适合所有情况的解决方案——这需要专用的技术和架构。虽然分布式机器学习中的隐私性是一个引人入胜的话题，但它也是一个庞大的话题，本书不会进一步讨论它。
- en: Portability
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可移植性
- en: 'Portability ties back to the general challenges of a distributed system. When
    we add dedicated computing hardware such as multiple types of GPUs paired with
    the software we build, it makes moving the workloads from one cluster to another
    more difficult. In the early days of the cloud, many companies used the “lift
    and shift” migration strategy, moving their workloads and applications to the
    cloud without redesigning them. However, in many cases this resulted in higher
    costs since they didn’t take advantage of the environment’s features. Specifically,
    in the cloud, native features were built with optimizations for specific workloads.
    The same happens with a distributed machine learning approach: the various types
    of hardware available, the requirements, and the need to improve ROI by reducing
    development, storage, and compute costs can impact portability.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 可移植性与分布式系统的一般挑战相关。当我们添加专用的计算硬件，如多种类型的 GPU，与我们构建的软件配对时，将工作负载从一个集群移动到另一个集群就变得更加困难。在云计算的早期阶段，许多公司采用了“提起和转移”迁移策略，将其工作负载和应用程序迁移到云端而无需重新设计它们。然而，在许多情况下，这导致了更高的成本，因为它们未能充分利用环境的特性。特别是在云中，本地功能是专为特定工作负载优化而构建的。分布式机器学习方法也是如此：各种类型的硬件、需求以及通过降低开发、存储和计算成本来提高投资回报率的需求，都可能影响可移植性。
- en: There are other challenges that are inherent to distributed systems that don’t
    relate directly to machine learning but can still have an impact on machine learning
    workloads, like trust or zero trust systems, network overhead, ensuring consistency,
    and more. We won’t cover those issues in this book, but you should consider them
    when designing your product strategy.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式系统固有的其他挑战与机器学习无直接关系，但仍可能对机器学习工作负载产生影响，比如信任或零信任系统、网络开销、确保一致性等。本书不会涵盖这些问题，但在设计产品策略时，您应考虑它们。
- en: Setting Up Your Local Environment
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置您的本地环境
- en: Now that you understand the landscape better, let’s set you up for success!
    Many of the code samples in this book are available for you in the book’s [GitHub
    repository](https://oreil.ly/smls-git). To gain hands-on experience with them,
    you should set up a learning environment, which you can run locally on your machine.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您对局势有了更好的了解，让我们为您成功设定！本书中的许多代码示例都可以在书籍的 [GitHub 仓库](https://oreil.ly/smls-git)
    中找到。为了亲自体验它们，您应设置一个学习环境，在本地机器上运行它们。
- en: You’ll want to go through this setup process twice, first for the tutorials
    in Chapters [2](ch02.xhtml#introduction_to_spark_and_pyspark)–[6](ch06.xhtml#training_models_with_spark_mllib)
    and then for the tutorials in Chapters [7](ch07.xhtml#bridging_spark_and_deep_learning_framew)–[10](ch10.xhtml#deployment_patterns_for_machine_learnin).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要两次完成此设置过程，首先是为第 [2](ch02.xhtml#introduction_to_spark_and_pyspark)–[6](ch06.xhtml#training_models_with_spark_mllib)
    章节中的教程，然后是为第 [7](ch07.xhtml#bridging_spark_and_deep_learning_framew)–[10](ch10.xhtml#deployment_patterns_for_machine_learnin)
    章节中的教程。
- en: Chapters 2–6 Tutorials Environment
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2–6 章节教程环境
- en: 'To follow along with the tutorials in Chapters [2](ch02.xhtml#introduction_to_spark_and_pyspark)
    to [6](ch06.xhtml#training_models_with_spark_mllib), make sure you have the latest
    version of Docker installed on your machine and follow these steps:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随第 [2](ch02.xhtml#introduction_to_spark_and_pyspark) 到 [6](ch06.xhtml#training_models_with_spark_mllib)
    章节的教程，请确保您的机器上已安装最新版本的 Docker，并按照以下步骤操作：
- en: Run Docker.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 Docker。
- en: 'In a terminal window/command line, run the following command:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端窗口/命令行中运行以下命令：
- en: '[PRE0]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This pulls an image of a PySpark Jupyter notebook with Apache Spark 3.1.1,
    which includes most of the libraries we’ll use. You will learn how to add the
    rest later. After executing this command, you will get a response like this:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将拉取一个带有Apache Spark 3.1.1的PySpark Jupyter笔记本镜像，其中包含大部分我们将使用的库。稍后您将学会如何添加其余部分。执行此命令后，您将获得如下响应：
- en: '[PRE1]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Tip
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Getting an error about AMD? Use this command instead:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 遇到AMD的错误？请改用以下命令：
- en: '[PRE2]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Copy the last URL with the `token` parameter. It will look something like this,
    but you will have your own token:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制带有`token`参数的最后一个URL。它看起来像这样，但您将拥有自己的令牌：
- en: '[PRE3]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Paste it in your browser. This will be your Jupyter tutorial environment.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将其粘贴到浏览器中。这将成为您的Jupyter教程环境。
- en: Clone or download the book’s repo.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆或下载本书的仓库。
- en: Extract (unzip) the files and upload the notebooks and data files into Jupyter
    using the Upload button (see [Figure 1-5](#jupyter_upload_button)).
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解压文件并使用上传按钮将笔记本和数据文件上传到Jupyter（参见[图 1-5](#jupyter_upload_button)）。
- en: '![](assets/smls_0105.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0105.png)'
- en: Figure 1-5\. Jupyter Upload button
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-5\. Jupyter上传按钮
- en: 'The `pyspark-notebook` Docker image is very simple at this stage: it simply
    contains Jupyter and PySpark, the main tools used in this book. [Figure 1-6](#jupyter_docker_stacks_images)
    shows how the images in [Jupyter Docker Stacks](https://oreil.ly/SdkSP) are stacked.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在目前的阶段，`pyspark-notebook` Docker镜像非常简单：它仅包含本书中使用的主要工具Jupyter和PySpark。[图 1-6](#jupyter_docker_stacks_images)展示了[Jupyter
    Docker Stacks](https://oreil.ly/SdkSP)中的镜像堆叠。
- en: '![](assets/smls_0106.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0106.png)'
- en: Figure 1-6\. Jupyter Docker Stacks images
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-6\. Jupyter Docker Stacks镜像
- en: Chapters 7–10 Tutorials Environment
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7至10章教程环境
- en: 'The tutorials in Chapters [7](ch07.xhtml#bridging_spark_and_deep_learning_framew)–[10](ch10.xhtml#deployment_patterns_for_machine_learnin)
    require PySpark, PyTorch, Petastorm, TensorFlow, and everything related to the
    build, deploy, and serve parts of the machine learning lifecycle. You can follow
    the same steps outlined in the previous section to set up an environment for these
    chapters. In addition, to work with PyTorch, you’ll need to install it directly
    from the Jupyter terminal. [Figure 1-7](#installing_pytorch_in_your_environment)
    shows you how to do this using the following conda command on macOS (note that
    during this process, you may be prompted to answer some questions about the installation):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第7章至第10章的教程需要PySpark、PyTorch、Petastorm、TensorFlow以及与机器学习生命周期中的构建、部署和服务相关的一切内容。您可以按照前一节中概述的相同步骤设置这些章节的环境。此外，为了使用PyTorch，您需要直接从Jupyter终端安装它。[图 1-7](#installing_pytorch_in_your_environment)展示了如何在macOS上使用以下conda命令进行此操作（在此过程中，您可能需要回答一些关于安装的问题）：
- en: '[PRE4]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](assets/smls_0107.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/smls_0107.png)'
- en: Figure 1-7\. Installing PyTorch in your environment
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-7\. 在您的环境中安装PyTorch
- en: 'You may need additional resources for the tutorials in these chapters—for example,
    you might need more RAM for faster execution. To configure that, you can leverage
    the `docker` command with the `--memory` and `--memory-swap` tags. Make sure to
    define the amounts according to your machine’s capabilities:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这些章节的教程可能需要额外的资源，例如，你可能需要更多的RAM来加快执行速度。为了配置这些，你可以使用`docker`命令，并使用`--memory`和`--memory-swap`标签。请根据你的计算机性能定义适当的内存量：
- en: '[PRE5]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Summary
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we covered the very basics of distributed machine learning,
    touching lightly on a number of complex topics: machine learning workflows, distributed
    computing models, network topologies, distributed training and serving, and much
    more. As you know, Apache Spark supports parallel data processing across clusters
    or computer processors in real time. The framework is based on the MapReduce paradigm,
    which has numerous advantages when it comes to processing data, analytics, and
    machine learning algorithms. However, it also has some limitations, particularly
    with regard to deep learning workloads (as a result, I’ll show you how to bridge
    from Spark into deep learning frameworks in [Chapter 7](ch07.xhtml#bridging_spark_and_deep_learning_framew)).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了分布式机器学习的基础知识，轻描淡写地涉及了许多复杂的主题：机器学习工作流程、分布式计算模型、网络拓扑、分布式训练和服务等等。正如您所知，Apache
    Spark支持跨集群或计算机处理器的并行数据处理。该框架基于MapReduce范式，在处理数据、分析和机器学习算法方面具有众多优势。然而，它在处理深度学习工作负载方面也存在一些限制（因此，我将向您展示如何在[第7章](ch07.xhtml#bridging_spark_and_deep_learning_framew)中从Spark过渡到深度学习框架）。
- en: The next chapter provides a quick introduction to PySpark, to get you up to
    speed or help you brush up on the basics. [Chapter 3](ch03.xhtml#managing_the_ml_experiment_lifecycle_wi)
    will get you started with machine learning lifecycle management using MLflow and
    show you how to package your experiments so that you can follow along with the
    tutorials in the rest of the book. You will learn how to utilize PySpark for your
    machine learning needs in Chapters [4](ch04.xhtml#data_ingestioncomma_preprocessingcomma),
    [5](ch05.xhtml#feature_engineering), and [6](ch06.xhtml#training_models_with_spark_mllib).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章提供了PySpark的快速介绍，帮助您快速上手或者帮助您复习基础知识。[第三章](ch03.xhtml#managing_the_ml_experiment_lifecycle_wi)将带您开始使用MLflow进行机器学习生命周期管理，并展示如何打包您的实验，以便您可以跟随本书其余部分的教程。在第四章，第五章和第六章中，您将学习如何利用PySpark进行机器学习需求的数据摄入，预处理和特征工程，以及训练模型。
- en: ^([1](ch01.xhtml#ch01fn1-marker)) Depending on how you define it, this era of
    computer science history began in either 2012 or 2016—the amount of digital data
    estimated to exist in the world exceeded 1 zettabyte in 2012, and Cisco Systems
    announced that global IP traffic hit 1.2 zettabytes in 2016.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.xhtml#ch01fn1-marker)) 根据您的定义方式，计算机科学历史的这个时代始于2012年或2016年——据估计，世界上存在的数字数据量在2012年超过了1
    ZB，并且Cisco Systems宣布，全球IP流量在2016年达到了1.2 ZB。
- en: ^([2](ch01.xhtml#ch01fn2-marker)) The bandwidth indicates how much data can
    be transmitted over a network connection in a given amount of time over a wired
    or wireless connection.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch01.xhtml#ch01fn2-marker)) 带宽指示了在有线或无线连接中，在给定时间内可以传输多少数据。
- en: ^([3](ch01.xhtml#ch01fn3-marker)) If the variance of the model is low, the sampled
    data will be close to what the model predicted. If the variance is high, the model
    will perform well on the training data but not on new data.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch01.xhtml#ch01fn3-marker)) 如果模型的方差较低，则采样数据会接近模型的预测结果。如果方差较高，则模型在训练数据上表现良好，但在新数据上表现可能不佳。
