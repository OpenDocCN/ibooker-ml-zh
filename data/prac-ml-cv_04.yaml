- en: Chapter 4\. Object Detection and Image Segmentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章\. 目标检测与图像分割
- en: 'So far in this book, we have looked at a variety of machine learning architectures
    but used them to solve only one type of problem—that of classifying (or regressing)
    an entire image. In this chapter, we discuss three new vision problems: object
    detection, instance segmentation, and whole-scene semantic segmentation ([Figure 4-1](#from_left_to_rightcolon_object_detection)).
    Other more advanced vision problems like image generation, counting, pose estimation,
    and generative models are covered in Chapters [11](ch11.xhtml#advanced_vision_problems)
    and [12](ch12.xhtml#image_and_text_generation).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们已经看过各种机器学习架构，但仅用于解决一种类型的问题——整个图像的分类（或回归）。在本章中，我们讨论三个新的视觉问题：目标检测、实例分割和整场景语义分割（[图
    4-1](#from_left_to_rightcolon_object_detection)）。其他更高级的视觉问题，如图像生成、计数、姿态估计和生成模型，将在第[11章](ch11.xhtml#advanced_vision_problems)和第[12章](ch12.xhtml#image_and_text_generation)中涵盖。
- en: '![](Images/pmlc_0401.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0401.png)'
- en: 'Figure 4-1\. From left to right: object detection, instance segmentation, and
    whole-scene semantic segmentation. Images from [Arthropods](https://oreil.ly/sRrvU)
    and [Cityscapes](https://oreil.ly/rs9zf) datasets.'
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 从左至右：目标检测、实例分割和整场景语义分割。图像来自[节肢动物](https://oreil.ly/sRrvU)和[城市街景](https://oreil.ly/rs9zf)数据集。
- en: Tip
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The code for this chapter is in the *04_detect_segment* folder of the book’s
    [GitHub repository](https://github.com/GoogleCloudPlatform/practical-ml-vision-book).
    We will provide file names for code samples and notebooks where applicable.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于书籍的[*04_detect_segment*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book)文件夹中的
    GitHub 仓库中。我们会在适当的地方提供代码示例和笔记本的文件名。
- en: Object Detection
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测
- en: Seeing is, for most of us, so effortless that, as we glimpse a butterfly from
    the corner of our eye and turn our head to enjoy its beauty, we don’t even think
    about the millions of visual cells and neurons at play, capturing light, decoding
    the signals, and processing them into higher and higher levels of abstraction.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数人来说，看是如此轻松，以至于当我们从眼角瞥见一只蝴蝶并转过头来欣赏它的美丽时，我们甚至不会考虑到数百万的视觉细胞和神经元在起作用，捕捉光线、解码信号，并将它们处理成越来越高级的抽象。
- en: We saw in [Chapter 3](ch03.xhtml#image_vision) how image recognition in ML works.
    However, the models presented in that chapter were built to classify an image
    as whole—they could not tell us where in the image a flower was. In this section,
    we will look at ways to build ML models that can provide this location information.
    This is a task known as *object detection* ([Figure 4-2](#an_object_detection_taskdot_image_from_a)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第三章](ch03.xhtml#image_vision)中看到了机器学习中图像识别的工作原理。然而，在该章中介绍的模型是为了将整个图像分类而构建的，它们不能告诉我们花朵在图像中的具体位置。在本节中，我们将探讨构建能够提供位置信息的机器学习模型的方法。这个任务被称为*目标检测*（[图
    4-2](#an_object_detection_taskdot_image_from_a)）。
- en: '![](Images/pmlc_0402.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0402.png)'
- en: Figure 4-2\. An object detection task. Image from [Arthropods dataset](https://oreil.ly/sRrvU).
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 一个目标检测任务。来自[节肢动物数据集](https://oreil.ly/sRrvU)的图像。
- en: In fact, convolutional layers do identify and locate the things they detect.
    The convolutional backbones from [Chapter 3](ch03.xhtml#image_vision) already
    extract some location information. But in classification problems, the networks
    make no use of this information. They are trained on an objective where location
    does not matter. A picture of a butterfly is classified as such wherever the butterfly
    appears in the image. On the contrary, for object detection, we will add elements
    to the convolutional stack to extract and refine the location information and
    train the network to do so with maximum accuracy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，卷积层确实可以识别和定位它们检测到的事物。来自[第三章](ch03.xhtml#image_vision)的卷积主干已经提取了一些位置信息。但在分类问题中，网络不利用这些信息。它们是在一个位置不重要的目标上训练的。蝴蝶的图片会在图片中的任何位置被分类为蝴蝶。相反，在目标检测中，我们将在卷积堆栈中添加元素来提取和精炼位置信息，并训练网络以达到最大精度。
- en: The simplest approach is to add something to the end of a convolutional backbone
    to predict bounding boxes around detected objects. That’s the YOLO (You Only Look
    Once) approach, and we will start there. However, a lot of important information
    is also contained at intermediate levels in the convolutional backbone. To extract
    it, we will build more complex architectures called feature pyramid networks (FPNs)
    and illustrate their use with RetinaNet.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是在卷积主干结构的末端添加一些内容，以预测检测到的对象周围的边界框。这就是YOLO（You Only Look Once）的方法，我们将从这里开始。然而，在卷积主干结构的中间层也包含了许多重要信息。为了提取这些信息，我们将构建更复杂的架构，称为特征金字塔网络（FPNs），并使用RetinaNet进行说明。
- en: In this section, we will be using the [Arthropod Taxonomy Orders Object Detection
    dataset](https://oreil.ly/sRrvU) (Arthropods for short), which is freely available
    on [Kaggle.com](http://kaggle.com). The dataset contains seven categories—Coleoptera
    (beetles), Aranea (spiders), Hemiptera (true bugs), Diptera (flies), Lepidoptera
    (butterflies), Hymenoptera (bees, wasps, and ants), and Odonata (dragonflies)—as
    well as bounding boxes. Some examples are shown in [Figure 4-3](#some_examples_from_the_arthropods_datase).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将使用[节肢动物分类目标检测数据集](https://oreil.ly/sRrvU)（简称为节肢动物），该数据集在[Kaggle.com](http://kaggle.com)上免费提供。该数据集包含七个类别——鞘翅目（甲壳虫）、蛛形目（蜘蛛）、半翅目（真虫）、双翅目（飞蝇）、鳞翅目（蝴蝶）、膜翅目（蜜蜂、胡蜂和蚂蚁）和蜻蜓目（蜻蜓）——以及边界框。一些示例显示在[图 4-3](#some_examples_from_the_arthropods_datase)中。
- en: '![](Images/pmlc_0403.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0403.png)'
- en: Figure 4-3\. Some examples from the Arthropods dataset for object detection.
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 节肢动物数据集中的一些目标检测示例。
- en: Besides YOLO, this chapter will also address the RetinaNet and Mask R-CNN architectures.
    Their implementations can be found in the [TensorFlow Model Garden’s official
    vision repository](https://oreil.ly/FYKgH). We will be using the new implementations
    located, at the time of writing, in the “beta” folder of the repository.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除了YOLO，本章还将讨论RetinaNet和Mask R-CNN架构。它们的实现可以在[TensorFlow Model Garden官方视觉存储库](https://oreil.ly/FYKgH)中找到，我们将使用存储库中“beta”文件夹内的最新实现。
- en: Example code showing how to apply these detection models on a custom dataset
    such as Arthropods can be found in [*04_detect_segment* on GitHub](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/tree/master/04_detect_segment),
    in the folder corresponding to [Chapter 4](#object_detection_and_image_segmentation).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 展示如何在诸如节肢动物之类的自定义数据集上应用这些检测模型的示例代码可以在[*04_detect_segment* on GitHub](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/tree/master/04_detect_segment)中找到，对应于[第四章](#object_detection_and_image_segmentation)。
- en: In addition to the TensorFlow Model Garden, there is also an excellent [step-by-step
    implementation of RetinaNet](https://oreil.ly/LWG3c) on the [keras.io](http://keras.io)
    website.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了TensorFlow Model Garden，还可以在[keras.io](http://keras.io)网站上找到关于RetinaNet的优秀的[逐步实现](https://oreil.ly/LWG3c)。
- en: YOLO
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YOLO
- en: '[YOLO (you only look once)](https://arxiv.org/abs/1506.02640) is the simplest
    object detection architecture. It is not the most accurate, but it’s one of the
    fastest when it comes to prediction times. For that reason, it is used in many
    real-time systems like security cameras. The architecture can be based on any
    convolutional backbone from [Chapter 3](ch03.xhtml#image_vision). Images are processed
    through the convolutional stack as in the image classification case, but the classification
    head is replaced with an object detection and classification head.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[YOLO（你只看一次）](https://arxiv.org/abs/1506.02640)是最简单的物体检测架构之一。它并非最精确，但在预测时间方面是最快的之一，因此被广泛应用于诸如安全摄像头之类的实时系统中。该架构可以基于第三章中的任何卷积主干结构进行构建。图像通过卷积堆栈处理，如同图像分类情况一样，但分类头被替换为物体检测和分类头。'
- en: More recent variations of the YOLO architecture exist ([YOLOv2](https://arxiv.org/abs/1612.08242),
    [YOLOv3](https://arxiv.org/abs/1804.02767), [YOLOv4](https://arxiv.org/abs/2004.10934)),
    but we will not be covering them here. We will use YOLOv1 as our first stepping-stone
    into object detection architectures, because it is the simplest one to understand.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO的更多最新变体架构存在（[YOLOv2](https://arxiv.org/abs/1612.08242)、[YOLOv3](https://arxiv.org/abs/1804.02767)、[YOLOv4](https://arxiv.org/abs/2004.10934)），但我们这里不会涵盖它们。我们将使用YOLOv1作为进入物体检测架构的第一步，因为它是最简单的。
- en: YOLO grid
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: YOLO 网格
- en: YOLOv1 (hereafter referred to as “YOLO” for simplicity) divides a picture into
    a grid of *N*x*M* cells—for example, 7x5 ([Figure 4-4](#the_yolo_griddot_each_grid_cell_predicts)).
    For each cell, it tries to predict a bounding box for an object that would be
    centered in that cell. The predicted bounding box can be larger than the cell
    from which it originates; the only constraint is that the center of the box is
    somewhere inside the cell.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv1（以下简称为“YOLO”）将图像划分为一个 *N*x*M* 的网格单元格，例如 7x5（详见 [图 4-4](#the_yolo_griddot_each_grid_cell_predicts)）。对于每个单元格，它尝试预测一个包围框，其中心位于该单元格中。预测的边界框可以比其起源的单元格更大；唯一的约束是框的中心必须位于单元格内的某处。
- en: What does it mean to predict a bounding box? Let’s take a look.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 预测边界框意味着什么？让我们来看看。
- en: '![](Images/pmlc_0404.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0404.png)'
- en: Figure 4-4\. The YOLO grid. Each grid cell predicts a bounding box for an object
    whose center is somewhere in that cell. Image from [Arthropods dataset](https://oreil.ly/sRrvU).
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. YOLO 网格。每个网格单元预测一个包围框，其中心位于该单元格的某处。来自 [节肢动物数据集](https://oreil.ly/sRrvU)
    的图像。
- en: Object detection head
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标检测头
- en: 'Predicting a bounding box amounts to predicting six numbers: the four coordinates
    of the bounding box (in this case, the *x* and *y* coordinates of the center,
    and the width and height), a confidence factor which tells us if an object has
    been detected or not, and finally, the class of the object (for example, “butterfly”).
    The YOLO architecture does this directly on the last feature map, as generated
    by the convolutional backbone it is using.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 预测边界框涉及预测六个数值：边界框的四个坐标（例如，中心的 *x* 和 *y* 坐标，以及宽度和高度），一个置信度因子，告诉我们是否检测到了对象，最后是对象的类别（例如，“蝴蝶”）。YOLO
    架构直接在它正在使用的卷积主干生成的最后特征图上执行此操作。
- en: In [Figure 4-5](#a_yolo_detection_head_predictscomma_for), the *x*- and *y*-coordinate
    calculations use a hyperbolic tangent (tanh) activation so that the coordinates
    fall in the [–1, 1] range. They will be the coordinates of the center of the detection
    box, relative to the center of the grid cell they belong to.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 4-5](#a_yolo_detection_head_predictscomma_for) 中，*x* 和 *y* 坐标的计算使用双曲正切（tanh）激活函数，以确保坐标落在
    [–1, 1] 范围内。这些坐标将是检测框的中心相对于其所属的网格单元中心的位置。
- en: '![](Images/pmlc_0405.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0405.png)'
- en: Figure 4-5\. A YOLO detection head predicts, for every grid cell, a bounding
    box (x, y, w, h), the confidence C of there being an object in this location,
    and the class of the object.
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. YOLO 检测头为每个网格单元预测一个边界框 (x, y, w, h)，此处的置信度 C 表示在该位置检测到对象的可能性，以及对象的类别。
- en: Width and height (*w*, *h*) calculations use a sigmoid activation so as to fall
    in the [0, 1] range. They will represent the size of the detection box relative
    to the entire image. This allows detection boxes to be bigger than the grid cell
    they originate in. The confidence factor, *C*, is also in the [0, 1] range. Finally,
    a softmax activation is used to predict the class of the detected object. The
    tanh and sigmoid functions are depicted in [Figure 4-6](#the_tanh_and_sigmoid_activation_function).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 宽度和高度 (*w*, *h*) 的计算使用 sigmoid 激活函数，以确保落在 [0, 1] 范围内。它们表示检测框相对于整个图像的大小。这样可以使检测框大于其起源网格单元的尺寸。置信度因子
    *C* 也在 [0, 1] 范围内。最后，使用 softmax 激活函数预测检测到的对象的类别。图中展示了 tanh 和 sigmoid 函数，详见 [图 4-6](#the_tanh_and_sigmoid_activation_function)。
- en: '![](Images/pmlc_0406.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0406.png)'
- en: Figure 4-6\. The tanh and sigmoid activation functions. Tanh outputs values
    in the [–1, 1] range, while the sigmoid function outputs them in the [0, 1] range.
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 双曲正切和 sigmoid 激活函数。双曲正切输出在 [–1, 1] 范围内，而 sigmoid 函数输出在 [0, 1] 范围内。
- en: An interesting practical question is how to obtain a feature map of exactly
    the right dimensions. In the example from [Figure 4-4](#the_yolo_griddot_each_grid_cell_predicts),
    it must contain exactly 7 * 5 * (5 + 7) values. The 7 * 5 is because we chose
    a 7x5 YOLO grid. Then, for each grid cell, five values are needed to predict a
    box (*x*, *y*, *w*, *h*, *C*), and seven additional values are needed because,
    in this example, we want to classify arthropods into seven categories (Coleoptera,
    Aranea, Hemiptera, Diptera, Lepidoptera, Hymenoptera, Odonata).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的实际问题是如何获得完全正确尺寸的特征映射。在 [图 4-4](#the_yolo_griddot_each_grid_cell_predicts)
    的示例中，它必须包含确切的 7 * 5 * (5 + 7) 个值。其中的 7 * 5 是因为我们选择了一个 7x5 的 YOLO 网格。然后，对于每个网格单元，需要五个值来预测一个框
    (*x*, *y*, *w*, *h*, *C*)，再加上七个额外的值，因为在这个示例中，我们想将节肢动物分类为七类（鞘翅目、蜘蛛目、半翅目、双翅目、鳞翅目、膜翅目、蜻蜓目）。
- en: 'If you control the convolutional stack, you could try to tune it to get exactly
    7 * 5 * 12 (420) outputs at the end. However, there is an easier way: flatten
    whatever feature map the convolutional backbone is returning and feed it through
    a fully connected layer with exactly that number of outputs. You can then reshape
    the 420 values into a 7x5x12 grid, and apply the appropriate activations as in
    [Figure 4-5](#a_yolo_detection_head_predictscomma_for). The authors of the YOLO
    paper argue that the fully connected layer actually adds to the accuracy of the
    system.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您控制卷积堆栈，可以尝试调整它，确保最终输出正好是7 * 5 * 12（420）个。但是，还有一种更简单的方法：将卷积主干返回的任何特征映射展平，并通过具有正好这些输出数量的全连接层进行馈送。然后，您可以将这420个值重塑为一个7x5x12的网格，并像[图4-5](#a_yolo_detection_head_predictscomma_for)中那样应用适当的激活函数。
    YOLO论文的作者认为，全连接层实际上提高了系统的准确性。
- en: Loss function
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'In object detection, as in any supervised learning setting, the correct answers
    are provided in the training data: ground truth boxes and their classes. During
    training the network predicts detection boxes, and it has to take into account
    errors in the boxes’ locations and dimensions as well as misclassification errors,
    and also penalize detections of objects where there aren’t any. The first step,
    though, is to correctly pair ground truth boxes with predicted boxes so that they
    can be compared. In the YOLO architecture, if each grid cell predicts a single
    box, this is straightforward. A ground truth box and a predicted box are paired
    if they are centered in the same grid cell (see [Figure 4-4](#the_yolo_griddot_each_grid_cell_predicts)
    for easier understanding).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标检测中，与任何监督学习环境一样，训练数据提供了正确答案：地面真实框及其类别。在训练期间，网络预测检测框，必须考虑框的位置和尺寸错误以及误分类错误，并惩罚未检测到任何对象的情况。然而，第一步是正确地将地面真实框与预测框配对，以便进行比较。在YOLO架构中，如果每个网格单元预测一个单一框，这是直接的。只要地面真实框和预测框位于同一个网格单元的中心（参见[图4-4](#the_yolo_griddot_each_grid_cell_predicts)以便更容易理解）。
- en: However in the YOLO architecture, the number of detection boxes per grid cell
    is a parameter. It can be more than one. If you look back to [Figure 4-5](#a_yolo_detection_head_predictscomma_for),
    you can see that it’s easy enough for each grid cell to predict 10 or 15 (*x*,
    *y*, *w*, *h*, *C*) coordinates instead of 5 and generate 2 or 3 detection boxes
    instead of 1\. But pairing these predictions with ground truth boxes requires
    more care. This is done by computing the *intersection over union* (IOU; see [Figure 4-7](#the_iou_metricdot))
    between all ground truth boxes and all predicted boxes within a grid cell, and
    selecting the pairings where the IOU is the highest.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在YOLO架构中，每个网格单元的检测框数是一个参数。它可以多于一个。如果您回顾[图4-5](#a_yolo_detection_head_predictscomma_for)，您会看到每个网格单元预测10或15个（*x*，*y*，*w*，*h*，*C*）坐标而不是5，并生成2或3个检测框而不是1。但是，将这些预测与地面真实框配对需要更多注意。这是通过计算网格单元内所有地面真实框与所有预测框之间的交并比（IOU；参见[图4-7](#the_iou_metricdot)）来完成，并选择IOU最高的配对。
- en: '![](Images/pmlc_0407.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0407.png)'
- en: Figure 4-7\. The IOU metric.
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7. IOU指标。
- en: 'To summarize, ground truth boxes are assigned to grid cells by their centers
    and to the prediction boxes within these grid cells by IOU.  With the pairings
    in place, we can now calculate the different parts of the loss:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，地面真实框是通过它们的中心分配给网格单元，并通过IOU将预测框分配给这些网格单元内的预测框。有了这些配对关系，我们现在可以计算损失的不同部分：
- en: Object presence loss
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存在损失
- en: 'Each grid cell that has a ground truth box computes:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 每个具有地面真实框的网格单元计算：
- en: <math><mrow><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>o</mi><mi>b</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>C</mi><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></mrow></mrow></math>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>o</mi><mi>b</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>C</mi><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></mrow></mrow></math>
- en: Object absence loss
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对象缺失损失
- en: 'Each grid cell that does not have a ground truth box computes:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个不具有地面真实框的网格单元计算：
- en: <math><mrow><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>n</mi><mi>o</mi><mi>o</mi><mi>b</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>(</mo><mn>0</mn><mo>−</mo><mi>C</mi><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup><mo>=</mo><msup><mrow><mi>C</mi></mrow><mrow><mn>2</mn></mrow></msup></mrow></mrow></mrow></math>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>n</mi><mi>o</mi><mi>o</mi><mi>b</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>(</mo><mn>0</mn><mo>−</mo><mi>C</mi><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup><mo>=</mo><msup><mrow><mi>C</mi></mrow><mrow><mn>2</mn></mrow></msup></mrow></mrow></mrow></math>
- en: Object classification loss
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对象分类损失
- en: 'Each grid cell that has a ground truth box computes:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每个有真实框的网格单元格计算：
- en: <math><mrow><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi></mrow></msub><mo>=</mo><mrow><mi>c</mi><mi>r</mi><mi>o</mi><mi>s</mi><msub><mrow><mi>s</mi></mrow><mrow><mo>−</mo></mrow></msub><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mrow><mo>(</mo><mi>p</mi><mo>,</mo><mover><mrow><mi>p</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>)</mo></mrow></mrow></mrow></mrow></math>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi></mrow></msub><mo>=</mo><mrow><mi>c</mi><mi>r</mi><mi>o</mi><mi>s</mi><msub><mrow><mi>s</mi></mrow><mrow><mo>−</mo></mrow></msub><mi>e</mi><mi>n</mi><mi>t</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>y</mi><mrow><mo>(</mo><mi>p</mi><mo>,</mo><mover><mrow><mi>p</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>)</mo></mrow></mrow></mrow></mrow></math>
- en: where *p̂* is the vector of predicted class probabilities and *p* is the one-hot-encoded
    target class.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*p̂*是预测类别概率的向量，*p*是独热编码的目标类别。
- en: Bounding box loss
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框损失
- en: 'Each predicted box/ground truth box pairing contributes (predicted coordinate
    marked with a hat, the other coordinate is the ground truth):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每个预测框和真实框的配对都会产生贡献（预测坐标标有帽子，另一个坐标是真实的）：
- en: <math><mrow><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>b</mi><mi>o</mi><mi>x</mi></mrow></msub><mo>=</mo><mrow><mo>(</mo><mi>x</mi><mo>−</mo><mover><mrow><mi>x</mi></mrow><mrow><mo>^</mo></mrow></mover><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow><mrow><mo>+</mo><mrow><mrow><mo>(</mo><mi>y</mi><mo>−</mo><mover><mrow><mi>y</mi></mrow><mrow><mo>^</mo></mrow></mover><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow><mo>+</mo><msup><mrow><mo>(</mo><msqrt><mrow><mi>w</mi></mrow></msqrt><mo>−</mo><msqrt><mrow><mover><mrow><mi>w</mi></mrow><mrow><mo>^</mo></mrow></mover></mrow></msqrt><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup><mo>+</mo><mrow><mo>(</mo><msqrt><mrow><mi>h</mi></mrow></msqrt><mo>−</mo><msqrt><mrow><mover><mrow><mi>h</mi></mrow><mrow><mo>^</mo></mrow></mover></mrow></msqrt><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></mrow></mrow></mrow></mrow></math>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>b</mi><mi>o</mi><mi>x</mi></mrow></msub><mo>=</mo><mrow><mo>(</mo><mi>x</mi><mo>−</mo><mover><mrow><mi>x</mi></mrow><mrow><mo>^</mo></mrow></mover><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow><mrow><mo>+</mo><mrow><mrow><mo>(</mo><mi>y</mi><mo>−</mo><mover><mrow><mi>y</mi></mrow><mrow><mo>^</mo></mrow></mover><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow><mo>+</mo><msup><mrow><mo>(</mo><msqrt><mrow><mi>w</mi></mrow></msqrt><mo>−</mo><msqrt><mrow><mover><mrow><mi>w</mi></mrow><mrow><mo>^</mo></mrow></mover></mrow></msqrt><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup><mo>+</mo><mrow><mo>(</mo><msqrt><mrow><mi>h</mi></mrow></msqrt><mo>−</mo><msqrt><mrow><mover><mrow><mi>h</mi></mrow><mrow><mo>^</mo></mrow></mover></mrow></msqrt><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></mrow></mrow></mrow></mrow></math>
- en: Notice here that the difference in box sizes is computed on the square roots
    of the dimensions. This is to mitigate the effect of large boxes, which tend to
    overwhelm the loss.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里，盒子大小的差异是在维度的平方根上计算的。这是为了减轻大盒子的影响，大盒子往往会压倒损失。
- en: 'Finally, all the loss contributions from the grid cells are added together,
    with weighting factors. A common problem in object detection losses is that small
    losses from numerous cells with no object in them end up overpowering the loss
    from a lone cell that predicts a useful box. Weighting different parts of the
    loss can alleviate this problem. The authors of the paper used the following empirical
    weights:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，所有网格单元格的损失贡献加在一起，并带有加权因子。目标检测损失中常见的问题是，大量没有物体的单元格中的小损失最终会压倒预测有用框的孤立单元格的损失。加权损失的不同部分可以缓解这个问题。论文的作者使用了以下经验权重：
- en: <math><mrow><mrow><mtable><mtr><mtd><msub><mrow><mi>λ</mi></mrow><mrow><mi>o</mi><mi>b</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn></mtd><mtd><msub><mrow><mi>λ</mi></mrow><mrow><mi>n</mi><mi>o</mi><mi>o</mi><mi>b</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0.5</mn></mtd><mtd><msub><mrow><mi>λ</mi></mrow><mrow><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi></mrow></msub><mo>=</mo><mn>1</mn></mtd><mtd><msub><mrow><mi>λ</mi></mrow><mrow><mi>b</mi><mi>o</mi><mi>x</mi></mrow></msub><mo>=</mo><mn>5</mn></mtd></mtr></mtable></mrow></mrow></math>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mtable><mtr><mtd><msub><mrow><mi>λ</mi></mrow><mrow><mi>o</mi><mi>b</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn></mtd><mtd><msub><mrow><mi>λ</mi></mrow><mrow><mi>n</mi><mi>o</mi><mi>o</mi><mi>b</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>0.5</mn></mtd><mtd><msub><mrow><mi>λ</mi></mrow><mrow><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi></mrow></msub><mo>=</mo><mn>1</mn></mtd><mtd><msub><mrow><mi>λ</mi></mrow><mrow><mi>b</mi><mi>o</mi><mi>x</mi></mrow></msub><mo>=</mo><mn>5</mn></mtd></mtr></mtable></mrow></mrow></math>
- en: YOLO limitations
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: YOLO的限制
- en: The biggest limitation is that YOLO predicts a single class per grid cell and
    will not work well if multiple objects of different kinds are present in the same
    cell.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的限制是，YOLO每个网格单元预测一个单一类别，在同一个单元中存在多种不同类型对象时效果不佳。
- en: 'The second limitation is the grid itself: a fixed grid resolution imposes strong
    spatial constraints on what the model can do. YOLO models will typically not do
    well on collections of small objects, like a flock of birds, without careful tuning
    of the grid to the dataset.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个限制是网格本身：固定的网格分辨率对模型能够执行的任务施加了强烈的空间约束。对于小物体的集合（例如一群鸟），要使YOLO模型表现良好，需要仔细调整网格以适应数据集。
- en: Also, YOLO tends to localize objects with relatively low precision. The main
    reason for that is that it works on the last feature map from the convolutional
    stack, which is typically the one with the lowest spatial resolution and contains
    only coarse location signals.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，YOLO倾向于以相对较低的精度定位对象。主要原因在于它工作在卷积堆栈的最后一个特征图上，通常这个特征图具有最低的空间分辨率，并且仅包含粗略的位置信号。
- en: Despite these limitations, the YOLO architecture is very simple to implement,
    especially with a single detection box per grid cell, which makes it a good choice
    when you want to experiment with your own code.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，YOLO架构非常简单易实现，特别是每个网格单元仅有一个检测框，这使得它成为你想要使用自己的代码进行实验时的一个不错的选择。
- en: Note that it is not the case that every object is detected by looking at the
    information in a single grid cell. In a sufficiently deep convolutional neural
    network (CNN), every value in the last feature map, from which detection boxes
    are computed, depends on all the pixels of the original image.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，并不是每个对象都是通过查看单个网格单元中的信息来检测的。在足够深的卷积神经网络（CNN）中，从中计算检测框的最后一个特征图中的每个值都依赖于原始图像的所有像素。
- en: 'If a higher accuracy is needed, you can step up to the next level: RetinaNet.
    It incorporates a number of ideas that improve upon the basic YOLO architecture,
    and is regarded, at the time of writing, as the state of the art of so-called
    *single-shot detectors*.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要更高的准确性，可以升级到更高级别的RetinaNet。它融合了多项改进基于基本的YOLO架构，并且被认为是所谓的*单阶段检测器*的技术最先进。
- en: RetinaNet
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RetinaNet
- en: '[RetinaNet](https://arxiv.org/abs/1708.02002), as compared to YOLOv1, has several
    innovations in its architecture and in the design of its losses. The neural network
    design includes feature pyramid networks which combine information extracted at
    multiple scales. The detection head predicts boxes starting from *anchor boxes*
    that change the bounding box representation to make training easier. Finally,
    the loss innovations include the focal loss, a loss specifically designed for
    detection problems, a smooth L1 loss for box regression, and non-max suppression.
    Let’s look at each of these in turn.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[RetinaNet](https://arxiv.org/abs/1708.02002)，相较于YOLOv1，在其架构和损失设计上有多项创新。神经网络设计包括特征金字塔网络，结合了多尺度提取的信息。检测头从*锚框*开始预测框，改变边界框表示以便于训练。最后，损失创新包括焦点损失，专门为检测问题设计的损失，平滑的L1损失用于框回归，以及非极大值抑制。让我们依次看看这些创新。'
- en: Feature pyramid networks
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征金字塔网络
- en: When an image is processed by a CNN, the initial convolutional layers pick up
    low-level details like edges and textures. Further layers combine them into features
    with more and more semantic value. At the same time, pooling layers in the network
    reduce the spatial resolution of the feature maps (see [Figure 4-8](#feature_maps_at_various_stages_of_a_cnnd)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当图像经过 CNN 处理时，初始卷积层捕捉低级细节，如边缘和纹理。进一步的层将它们组合成具有更多语义价值的特征。同时，网络中的池化层降低特征图的空间分辨率（见
    [图 4-8](#feature_maps_at_various_stages_of_a_cnnd)）。
- en: '![](Images/pmlc_0408.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0408.png)'
- en: Figure 4-8\. Feature maps at various stages of a CNN. As information progresses
    through the neural network, its spatial resolution decreases but its semantic
    content increases from low-level details to high-level objects.
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. CNN 不同阶段的特征图。随着信息在神经网络中传播，其空间分辨率减少，但语义内容从低级细节到高级对象逐渐增加。
- en: The YOLO architecture only uses the last feature map for detection. It is able
    to correctly identify objects, but its localization accuracy is limited. Another
    idea would be to try and add a detection head at every stage. Unfortunately, in
    this approach, the heads working from the early feature maps would localize objects
    rather well but would have difficulty labeling them. At that early stage, the
    image has only gone through a couple of convolutional layers, which is not enough
    to classify it. Higher-level semantic information, like “this is a rose,” needs
    tens of convolutional layers to emerge.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 架构仅使用最后一个特征图进行检测。它能正确识别物体，但其定位精度有限。另一种思路是尝试在每个阶段添加检测头。然而，在这种方法中，从早期特征图开始工作的检测头可能定位物体相当好，但标签化却困难重重。在这个早期阶段，图像只经过了几个卷积层，这还不足以对其进行分类。像“这是一朵玫瑰”的更高级语义信息需要数十个卷积层才能显现出来。
- en: Still, one popular detection architecture, called the single-shot detector (SSD),
    is based on this idea. The authors of the [SSD paper](https://arxiv.org/abs/1512.02325)
    made it work by connecting their multiple detection heads to multiple feature
    maps, all located toward the end of the convolutional stack.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有一种流行的检测架构叫做单次检测器（SSD），它基于这个思想。[SSD 论文的作者](https://arxiv.org/abs/1512.02325)通过将多个检测头连接到多个特征图使其成功运行，这些特征图都位于卷积堆栈的末端。
- en: What if we could combine all feature maps in a way that would surface both good
    spatial information and good semantic information at all scales? This can be done
    with a couple of additional layers forming a [feature pyramid network](https://arxiv.org/pdf/1612.03144.pdf).
    [Figure 4-9](#comparison_of_yolocomma_ssdcomma_and_fpn) offers a schematic view
    of an FPN compared to the YOLO and SSD approaches, while [Figure 4-10](#a_feature_pyramid_network_in_detaildot_f)
    presents the detailed design.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能以一种方式结合所有特征图，使其在所有尺度上都能提供良好的空间信息和语义信息，那会怎样？这可以通过添加几个附加层形成 [特征金字塔网络](https://arxiv.org/pdf/1612.03144.pdf)
    来实现。[图 4-9](#comparison_of_yolocomma_ssdcomma_and_fpn) 提供了FPN与YOLO和SSD方法的示意图对比，而
    [图 4-10](#a_feature_pyramid_network_in_detaildot_f) 则呈现了详细设计。
- en: '![](Images/pmlc_0409.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0409.png)'
- en: Figure 4-9\. Comparison of YOLO, SSD, and FPN architectures and where, in the
    convolutional stack, they connect their detection head(s).
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 4-9\. Y   图 4-9\. YOLO、SSD 和 FPN 架构的比较，以及它们在卷积堆栈中连接检测头的位置。
- en: '![](Images/pmlc_0410.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0410.png)'
- en: Figure 4-10\. A feature pyramid network in detail. Feature maps are extracted
    from various stages of a convolutional backbone, and 1x1 convolutions squeeze
    every feature map to the same number of channels. Upsampling (nearest neighbor)
    then makes their spatial dimensions compatible so that they can be added up. The
    final 3x3 convolutions smooth out upsampling artifacts. Typically no activation
    functions are used in the FPN layers.
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. 特征特征金字塔网络详细信息。特征图从卷积主干的各个阶段提取，并且 1x1 卷积将每个特征图压缩到相同数量的通道。上采样（最近邻）然后使它们的空间尺寸兼容，以便可以相加。最后的
    3x3 卷积平滑了上采样的伪影。通常在 FPN 层中不使用激活函数。
- en: 'Here is what is happening in the FPN in [Figure 4-10](#a_feature_pyramid_network_in_detaildot_f):
    in the downward path (convolutional backbone), convolutional layers gradually
    refine the semantic information in the feature maps, while pooling layers scale
    the feature maps down in their spatial dimensions (the *x* and *y* dimensions
    of the image). In the upward path, feature maps from the bottom layers containing
    good high-level semantic information get upsampled (using a simple nearest neighbor
    algorithm) so that they can be added, element-wise, to feature maps higher up
    in the stack. 1x1 convolutions are used in the lateral connections to bring all
    feature maps to the same channel depth and make the additions possible. The [FPN
    paper](https://arxiv.org/pdf/1612.03144.pdf), for example, uses 256 channels everywhere.
    The resulting feature maps now contain semantic information at all scales, which
    was the initial goal. They are further processed through a 3x3 convolution, mostly
    to smooth out the effects of the upsampling.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 4-10](#a_feature_pyramid_network_in_detaildot_f)中，FPN中发生的情况如下：在向下路径（卷积主干）中，卷积层逐渐优化特征图中的语义信息，而池化层缩小特征图的空间尺寸（图像的
    *x* 和 *y* 尺寸）。在向上路径中，来自底层的包含良好高级语义信息的特征图被上采样（使用简单的最近邻算法），以便它们可以逐元素添加到堆栈中更高的特征图中。1x1卷积用于侧向连接，将所有特征图带到相同的通道深度，并使添加成为可能。例如，[FPN论文](https://arxiv.org/pdf/1612.03144.pdf)中在所有地方使用256个通道。现在生成的特征图包含所有尺度的语义信息，这是最初的目标。它们通过3x3卷积进一步处理，主要是为了平滑上采样的效果。
- en: There are typically no nonlinearities in the FPN layers. The authors of the
    FPN paper found them to have little impact.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: FPN层通常没有非线性。FPN论文的作者发现它们影响不大。
- en: A detection head can now take the feature maps at each resolution and produce
    box detections and classifications. The detection head can itself have multiple
    designs, which we will cover in the next two sections. It will, however, be shared
    across all the feature maps at different scales. This is why it was important
    to bring all the feature maps to the same channel depth.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在检测头可以获取每个分辨率的特征图，并生成框检测和分类。检测头本身可以具有多种设计，我们将在接下来的两节中进行介绍。然而，它将在不同尺度的所有特征图之间共享。这就是为什么将所有特征图带到相同的通道深度是重要的。
- en: The nice thing about the FPN design is that it is independent of the underlying
    convolutional backbone. Any convolutional stack from [Chapter 3](ch03.xhtml#image_vision)
    will do, as long as you can extract intermediate feature maps from it—typically
    four to six, at various scales. You can even use a pretrained backbone. Typical
    choices are ResNet or EfficientNet, and pretrained versions of them can be found
    in [TensorFlow Hub](https://tfhub.dev/).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: FPN设计的好处在于它独立于底层卷积主干。只要您可以从中提取中间特征图，来自[第3章](ch03.xhtml#image_vision)的任何卷积堆栈都可以使用，通常在各种尺度上为四到六个。您甚至可以使用预训练的主干。典型选择是ResNet或EfficientNet，它们的预训练版本可以在[TensorFlow
    Hub](https://tfhub.dev/)中找到。
- en: There are multiple levels in a convolutional stack where features can be extracted
    and fed into the FPN. For each desired scale, many layers output feature maps
    of the same dimensions (see [Figure 3-26](ch03.xhtml#the_squeezenet_architecture_with_eightee)
    in the previous chapter). The best choice is the last feature map of a given block
    of layers outputting similarly sized features, just before a pooling layer halves
    the resolution again. This feature map is likely to contain the strongest semantic
    features.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积堆栈中有多个层次，可以从中提取特征并输入到FPN中。对于每个期望的尺度，许多层输出相同尺寸的特征图（见前一章的[图 3-26](ch03.xhtml#the_squeezenet_architecture_with_eightee)）。最佳选择是给定层块的最后一个特征图，在池化层再次减半分辨率之前输出的特征大小相似的层块。这个特征图可能包含最强的语义特征。
- en: It is also possible to extend an existing pretrained backbone with additional
    pooling and convolutional layers, for the sole purpose of feeding an FPN. These
    additional feature maps are typically small and therefore fast to process. They
    correspond to the lowest spatial resolution (see [Figure 4-8](#feature_maps_at_various_stages_of_a_cnnd))
    and can therefore improve the detection of large objects. The [SSD paper](https://arxiv.org/abs/1512.02325)
    actually used this trick, and [RetinaNet](https://arxiv.org/abs/1708.02002) does
    as well, as you will see in the architecture diagram later ([Figure 4-15](#complete_view_of_the_retinanet_architect)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过额外的池化和卷积层扩展现有的预训练骨干网络，唯一目的是用于提供FPN的输入。这些额外的特征图通常很小，因此处理速度快。它们对应于最低的空间分辨率（参见[图4-8](#feature_maps_at_various_stages_of_a_cnnd)），因此可以改善大物体的检测。[SSD论文](https://arxiv.org/abs/1512.02325)和[RetinaNet](https://arxiv.org/abs/1708.02002)都使用了这个技巧，稍后您将在架构图中看到（参见[图4-15](#complete_view_of_the_retinanet_architect))。
- en: Anchor boxes
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 锚点
- en: In the YOLO architecture, detection boxes are computed as deltas relative to
    a set of base boxes (Δ*x* = *x* – *x*[0], Δ*y* = *y* – *y*[0], Δ*w* = *w* – *w*[0],
    Δ*h* = *h* – *h*[0] are often referred to as “deltas” relative to some base box
    *x*[0], *y*[0], *w*[0], *h*[0] because of the Greek letter Δ, usually chosen to
    represent a ”difference”). In that case, the base boxes were a simple grid overlaid
    on the image (see [Figure 4-4](#the_yolo_griddot_each_grid_cell_predicts)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在YOLO架构中，检测框是相对于一组基础框计算的增量（Δ*x* = *x* - *x*[0]，Δ*y* = *y* - *y*[0]，Δ*w* = *w*
    - *w*[0]，Δ*h* = *h* - *h*[0]，通常被称为“增量”相对于一些基础框 *x*[0]， *y*[0]， *w*[0]， *h*[0]，因为希腊字母Δ，通常用来表示“差异”）。在这种情况下，基础框是覆盖在图像上的一个简单网格（参见[图4-4](#the_yolo_griddot_each_grid_cell_predicts)）。
- en: More recent architectures have expanded on this idea by explicitly defining
    a set of so-called “anchor boxes” with various aspect ratios and scales (examples
    in [Figure 4-11](#examples_of_anchor_boxes_of_various_size)). Predictions are
    again small variations of the size and position of the anchors. The goal is to
    help the neural network predict small values around zero rather than large ones.
    Indeed, neural networks are able to solve complex nonlinear problems because they
    use nonlinear activation functions between their layers. However, most activation
    functions (sigmoid, ReLU) exhibit a nonlinear behavior around zero only. That’s
    why neural networks are at their best when they predict small values around zero,
    and it’s why predicting detections as small deltas relative to anchor boxes is
    helpful. Of course, this only works if there are enough anchor boxes of various
    sizes and aspect ratios that any object detection box can be paired (by max IOU)
    with an anchor box of closely matching position and dimensions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 较新的架构通过明确定义一组所谓的“锚点框”，具有各种长宽比和尺度（例如[图4-11](#examples_of_anchor_boxes_of_various_size)中的示例）。预测再次是关于锚点位置和尺寸的小变化。目标是帮助神经网络预测接近零的小值而不是大值。事实上，神经网络能够解决复杂的非线性问题，因为它们在各层之间使用非线性激活函数。然而，大多数激活函数（sigmoid、ReLU）只在接近零时表现出非线性行为。这就是为什么神经网络在预测接近零的小值时表现最佳的原因，也是为什么相对于锚点框预测检测框作为小增量是有帮助的。当然，这仅在有足够多的各种尺寸和长宽比的锚点框时才有效，这些锚点框能够与任何物体检测框（通过最大IOU）配对得非常接近其位置和尺寸。
- en: '![](Images/pmlc_0411.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0411.png)'
- en: Figure 4-11\. Examples of anchor boxes of various sizes and aspect ratios used
    to predict detection boxes. Image from [Arthropods dataset](https://oreil.ly/sRrvU).
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-11\. 展示了用于预测检测框的各种大小和长宽比的锚框示例。图片来自[节肢动物数据集](https://oreil.ly/sRrvU)。
- en: 'We will describe in detail the approach taken in the RetinaNet architecture,
    as an example. RetinaNet uses nine different anchor types with:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细描述RetinaNet架构中采用的方法，以此为例。RetinaNet使用了九种不同类型的锚点：
- en: 'Three different aspect ratios: 2:1, 1:1, 1:2'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三种不同的长宽比：2:1, 1:1, 1:2
- en: 'Three different sizes: 2⁰, 2^⅓, 2^⅔ (≃ 1, 1.3, 1.6)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三种不同尺寸：2⁰, 2^⅓, 2^⅔ (≃ 1, 1.3, 1.6)
- en: They are depicted in [Figure 4-12](#the_nine_different_anchor_types_used_in).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在[图4-12](#the_nine_different_anchor_types_used_in)中展示。
- en: '![](Images/pmlc_0412.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0412.png)'
- en: Figure 4-12\. The nine different anchor types used in RetinaNet. Three aspect
    ratios and three different sizes.
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12\. RetinaNet使用的九种不同锚点类型。三种长宽比和三种不同尺寸。
- en: 'Anchors, along with the feature maps computed by an FPN, are the inputs from
    which detections are computed in RetinaNet. The sequence of operations is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 锚点连同由FPN计算的特征图，是RetinaNet中计算检测的输入。操作顺序如下：
- en: The FPN reduces the input image into five feature maps (see [Figure 4-10](#a_feature_pyramid_network_in_detaildot_f)).
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FPN 将输入图像减少到五个特征图（参见 [图 4-10](#a_feature_pyramid_network_in_detaildot_f)）。
- en: Each feature map is used to predict bounding boxes relative to anchors at regularly
    spaced locations throughout the image. For example, a feature map of size 4x6
    with 256 channels will use 24 (4 * 6) anchor locations in the image (see [Figure 4-13](#conceptual_view_of_the_retinanet_detecti)).
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个特征图用于相对于图像中均匀分布的位置的锚点预测边界框。例如，大小为 4x6 且具有 256 通道的特征图将使用图像中的 24（4 * 6）个锚点位置（参见
    [图 4-13](#conceptual_view_of_the_retinanet_detecti)）。
- en: The detection head uses multiple convolutional layers to convert the 256-channel
    feature map into exactly 9 * 4 = 36 channels, yielding 9 detection boxes per location.
    The four numbers per detection box represent the deltas relative to the center
    (*x*, *y*), the width, and the height of the anchor. The precise sequence of the
    layers that compute detections from the feature maps is shown in [Figure 4-15](#complete_view_of_the_retinanet_architect).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测头使用多个卷积层将 256 通道特征图转换为确切的 9 * 4 = 36 个通道，从而每个位置产生 9 个检测框。每个检测框的四个数字表示相对于锚点的中心
    (*x*, *y*)、宽度和高度的增量。计算从特征图到检测的层序列如 [图 4-15](#complete_view_of_the_retinanet_architect)
    所示。
- en: Finally, each feature map from the FPN, since it corresponds to a different
    scale in the image, will use different scales of anchor boxes.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，每个 FPN 的特征图由于对应于图像中的不同尺度，将使用不同尺度的锚框。
- en: '![](Images/pmlc_0413.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0413.png)'
- en: Figure 4-13\. Conceptual view of the RetinaNet detection head. Each spatial
    location in a feature map corresponds to a series of anchors in the image, all
    centered at the same point. For clarity, only three such anchors are shown in
    the illustration, but RetinaNet would have nine at every location.
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-13\. RetinaNet 检测头的概念视图。特征图中的每个空间位置对应于图像中相同点处的一系列锚点。为了清晰起见，图示仅显示了三个这样的锚点，但
    RetinaNet 每个位置会有九个。
- en: 'The anchors themselves are spaced regularly across the input image and sized
    appropriately for each level of the feature pyramid. For example in RetinaNet,
    the following parameters are used:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 锚点本身在输入图像上均匀分布，并且针对特征金字塔的每个级别进行了适当的尺寸设置。例如，在 RetinaNet 中，使用以下参数：
- en: The feature pyramid has five levels corresponding to scales P[3], P[4], P[5],
    P[6], and P[7] in the backbone. Scale P*[n]* represents a feature map 2*^n* times
    smaller in width and height than the input image (see the complete RetinaNet view
    in [Figure 4-15](#complete_view_of_the_retinanet_architect)).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征金字塔有五个级别，对应于骨干网中的 P[3]、P[4]、P[5]、P[6] 和 P[7] 尺度。尺度 P*[n]* 表示特征图比输入图像缩小了 2*^n*
    倍（参见 [图 4-15](#complete_view_of_the_retinanet_architect) 的完整 RetinaNet 视图）。
- en: Anchor base sizes are 32x32, 64x64, 128x128, 256x256, 512x512 pixels, at each
    feature pyramid level respectively (= 4 * 2*^n*, if *n* is the scale level).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 锚框基准尺寸分别为 32x32、64x64、128x128、256x256 和 512x512 像素，在每个特征金字塔级别上分别为 (= 4 * 2*^n*,
    若 *n* 是尺度级别)。
- en: Anchor boxes are considered for every spatial location of every feature map
    in the feature pyramid, which means that the boxes are spaced every 8, 16, 32,
    64, or 128 pixels across the input image at each feature pyramid level, respectively
    (= 2*^n*, if *n* is the scale level).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特征金字塔的每个特征图中的每个空间位置都考虑了锚框，这意味着在每个特征金字塔级别上，锚框在输入图像上每 8、16、32、64 或 128 像素之间均匀分布（=
    2*^n*, 若 *n* 是尺度级别）。
- en: The smallest anchor box is therefore 32x32 pixels while the largest one is 812x1,624
    pixels.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最小的锚框为 32x32 像素，而最大的为 812x1,624 像素。
- en: Note
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The anchor box settings must be tuned for every dataset so that they correspond
    to the detection box characteristics actually found in the training data. This
    is typically done by resizing input images rather than changing the anchor box
    generation parameters. However, on specific datasets with many small detections,
    or, on the contrary, mostly large objects, it can be necessary to tune the anchor
    box generation parameters directly.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 锚框设置必须针对每个数据集进行调整，以便与训练数据中实际发现的检测框特征相对应。通常通过调整输入图像的大小而不是改变锚框生成参数来完成这一点。然而，在具有许多小检测或者相反大部分为大对象的特定数据集上，可能需要直接调整锚框生成参数。
- en: The last step is to compute a detection loss. For that, predicted detection
    boxes must be paired with ground truth boxes so that detection errors can be evaluated.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是计算检测损失。为此，必须将预测的检测框与地面真实框配对，以便评估检测错误。
- en: 'The assignment of ground truth boxes to anchor boxes is based on the IOU metric
    computed between each set of boxes in one input image. All pairwise IOUs are computed
    and are arranged in a matrix with *N* rows and *M* columns, *N* being the number
    of ground truth boxes and *M* the number of anchor boxes. The matrix is then analyzed
    by columns (see [Figure 4-14](#the_pairwise_iou_metric_is_computed_betw)):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 地面真实框分配给锚框基于计算出的每个输入图像中每组框之间的IOU度量。所有成对的IOU都计算并排列成一个*N*行和*M*列的矩阵，其中*N*是地面真实框的数量，*M*是锚框的数量。然后按列分析该矩阵（见[图 4-14](#the_pairwise_iou_metric_is_computed_betw)）：
- en: An anchor is assigned to the ground truth box that has the largest IOU in its
    column, provided it is more than 0.5.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个锚框的列中最大IOU大于0.5，则该锚框被分配给其列中具有最大IOU的地面真实框。
- en: An anchor box that has no IOU greater than 0.4 in its column is assigned to
    detect nothing (i.e., the background of the image).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个锚框如果其列中没有大于0.4的IOU值，则被分配为不检测任何东西（即图像的背景）。
- en: Any unassigned anchor at this point is marked to be ignored during training.
    Those are anchors with IOUs in the intermediate regions between 0.4 and 0.5.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此时任何未分配的锚框都标记为在训练期间忽略。这些是具有中间区域IOU在0.4和0.5之间的锚框。
- en: Now that every ground truth box is paired with exactly one anchor box, it is
    possible to compute box predictions, classifications, and the corresponding losses.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每个地面真实框都与一个锚框精确配对，可以计算框预测、分类以及相应的损失。
- en: '![](Images/pmlc_0414.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0414.png)'
- en: Figure 4-14\. The pairwise IOU metric is computed between all ground truth boxes
    and all anchor boxes to determine their pairings. Anchors without a meaningful
    intersection with a ground truth box are deemed “background” and trained to detect
    nothing.
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-14\. 计算所有地面真实框和所有锚框之间的成对IOU度量，以确定它们的配对关系。没有与地面真实框有意义交集的锚框被称为“背景”，并且被训练以不检测任何东西。
- en: Architecture
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 架构
- en: The detection and classification heads transform the feature maps from the FPN
    into class predictions and bounding box deltas. Feature maps are three-dimensional.
    Two of their dimensions correspond to the *x* and *y* dimensions of the image
    and are called *spatial dimensions*; the third dimension is their number of channels.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 检测和分类头部将FPN中的特征图转换为类别预测和边界框变化量。特征图是三维的。它们的两个维度对应于图像的*x*和*y*维度，称为*空间维度*；第三个维度是它们的通道数。
- en: 'In RetinaNet, for every spatial location in every feature map, the following
    parameters are predicted (with *K* = the number of classes and *B* = the number
    of anchor box types, so in our case *B*=9):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在RetinaNet中，对于每个特征图中的每个空间位置，预测以下参数（其中*K*为类别数量，*B*为锚框类型数量，因此在我们的情况下*B*=9）：
- en: The class prediction head predicts *B* * *K* probabilities, one set of probabilities
    for every anchor type. This in effect predicts one class for every anchor.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别预测头部预测*B* * *K*个概率，每种锚框类型预测一组概率。实际上，这为每个锚框预测一个类别。
- en: The detection head predicts *B* * 4 = 36 box deltas Δ*x*, Δ*y*, Δ*w*, Δ*h*.
    Bounding boxes are still parameterized by their center (*x*, *y*) as well as their
    width and height (*w*, *h*).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测头部预测*B* * 4 = 36个框变量 Δ*x*, Δ*y*, Δ*w*, Δ*h*。边界框仍然通过它们的中心(*x*, *y*)以及宽度和高度(*w*,
    *h*)参数化。
- en: Both heads share a similar design, although with different weights, and the
    weights are shared across all scales in the feature pyramid.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然两个头部具有类似的设计，但权重不同，并且这些权重在特征金字塔的所有尺度中都是共享的。
- en: '[Figure 4-15](#complete_view_of_the_retinanet_architect) represents a complete
    view of the RetinaNet architecture. It uses a ResNet50 (or other) backbone. The
    FPN extracts features from backbone levels P[3] though P[7,] where P*[n]* is the
    level where the feature map is reduced by a factor of 2*^n* in its width and height
    compared to the original image. The FPN part is described in detail in [Figure 4-10](#a_feature_pyramid_network_in_detaildot_f).
    Every feature map from the FPN is fed through both a classification and a box
    regression head.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-15](#complete_view_of_the_retinanet_architect)展示了RetinaNet架构的完整视图。它使用ResNet50（或其他）骨干网络。FPN从骨干网络的P[3]到P[7,]级别提取特征，其中P*[n]*是特征图相对于原始图像宽度和高度缩小了2*^n*倍的级别。FPN部分在[图 4-10](#a_feature_pyramid_network_in_detaildot_f)中有详细描述。FPN的每个特征图都通过分类和框回归头部。'
- en: '![](Images/pmlc_0415.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0415.png)'
- en: Figure 4-15\. Complete view of the RetinaNet architecture. K is the number of
    target classes. B is the number of anchor boxes at each position, which is nine
    in RetinaNet.
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-15\. RetinaNet架构的完整视图。K是目标类别的数量。B是每个位置的锚框数量，在RetinaNet中为9个。
- en: The RetinaNet FPN taps into the three last scale levels available from the backbone.
    The backbone is extended with 2 additional layers using a stride of 2 to provide
    2 additional scale levels to the FPN. This architectural choice allows RetinaNet
    to avoid processing very large feature maps, which would be time-consuming. The
    addition of the last two coarse scale levels also improves the detection of very
    large objects.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: RetinaNet FPN利用来自骨干网络的最后三个尺度级别。骨干网络通过步幅为2增加了2个额外的层次，以提供FPN的两个额外尺度级别。这种架构选择使RetinaNet避免处理非常大的特征图，这将是耗时的。增加最后两个粗糙尺度级别还改善了非常大物体的检测。
- en: The classification and box regression heads themselves are made from a simple
    sequence of 3x3 convolutions. The classification head is designed to predict *K*
    binary classifications for every anchor, which is why it ends on a sigmoid activation.
    It looks like we are allowing multiple labels to be predicted for every anchor,
    but actually the goal is to allow the classification head to output all zeros,
    which will represent the “background class” corresponding to no detections. A
    more typical activation for classification would be softmax, but the softmax function
    cannot output all zeros.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 分类和框回归头部本身由一系列简单的3x3卷积组成。分类头部设计用于预测每个锚点的*K*个二进制分类，这也是为什么它以sigmoid激活结束。看起来我们允许多个标签预测每个锚点，但实际上的目标是允许分类头部输出全零，代表“背景类”，即没有检测到的情况。分类的更典型的激活函数可能是softmax，但softmax函数不能输出全零。
- en: 'The box regression ends with no activation function. It is computing the differences
    between the center coordinates (*x*, *y*), width, and height of the anchor box
    and detection box. Some care must be taken to allow the regressor to work in the
    [–1, 1] range at all levels in the feature pyramid. The following formulas are
    used to achieve that:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 框回归不使用激活函数结束。它计算锚框和检测框的中心坐标（*x*、*y*）、宽度和高度之间的差异。在特征金字塔的所有级别上，必须小心地允许回归器在[-1,
    1]范围内工作。以下公式用于实现这一点：
- en: X[pixels] = X × U × W[A] + X[A]
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X[pixels] = X × U × W[A] + X[A]
- en: Y[pixels] = Y × U × H[A] + Y[A]
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y[pixels] = Y × U × H[A] + Y[A]
- en: W[pixels] = W[A] × e^(W × V)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: W[pixels] = W[A] × e^(W × V)
- en: H[pixels] = H[A] × e^(H × V)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H[pixels] = H[A] × e^(H × V)
- en: In these formulas, X[A], Y[A], W[A], and H[A] are the coordinates of an anchor
    box (center coordinates, width, height), while X, Y, W, and H are the predicted
    coordinates relative to the anchor box (deltas). X[pixels], Y[pixels], W[pixels],
    and H[pixels] are the actual coordinates, in pixels, of the predicted box (center
    and size). U and V are modulating factors that correspond to the expected variance
    of the deltas relative to the anchor box. Typical values are U=0.1 for coordinates,
    and V=0.2 for sizes. You can verify that values in the [–1, 1] range for predictions
    result in predicted boxes that fall within ±10% of the position of the anchor
    and within ±20% of its size.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些公式中，X[A]、Y[A]、W[A]和H[A]是锚框的坐标（中心坐标、宽度、高度），而X、Y、W和H是相对于锚框的预测坐标（增量）。X[pixels]、Y[pixels]、W[pixels]和H[pixels]是预测框的实际像素坐标（中心和大小）。U和V是调制因子，对应于增量相对于锚框的预期方差。预测值在[-1,
    1]范围内会导致预测框的位置在锚点位置的±10%范围内，大小在其±20%范围内。
- en: Focal loss (for classification)
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 焦点损失（用于分类）
- en: How many anchor boxes are considered for one input image? Looking back at [Figure 4-15](#complete_view_of_the_retinanet_architect),
    with an example input image of 640x960 pixels, the five different feature maps
    in the feature pyramid represent 80 * 120 + 40 * 60 + 20 * 30+ 10 * 15 + 5 * 7
    = 12,785 locations in the input image. With 9 anchor boxes per location, that’s
    slightly over 100K anchor boxes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个输入图像考虑多少个锚框？回顾[图4-15](#complete_view_of_the_retinanet_architect)中的例子输入图像为640x960像素，特征金字塔中的五个不同特征图代表了输入图像中的12,785个位置。每个位置有9个锚框，总计略超过100K个锚框。
- en: This means that 100K predicted boxes will be generated for every input image.
    In comparison, there are 0 to 20 ground truth boxes per image in a typical application.
    The problem this creates in detection models is that the loss corresponding to
    background boxes (boxes assigned to detect nothing) can overwhelm the loss corresponding
    to useful detections in the total loss. This happens even if background detections
    are already well trained and produce a small loss. This small value multiplied
    by 100K can still be orders of magnitude larger than the detection loss for actual
    detections. The end result is a model that cannot be trained.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着每个输入图像将生成100K个预测框。相比之下，典型应用中每个图像通常有0到20个地面实况框。这在检测模型中造成的问题是，与实际检测的损失相比，分配给背景框（分配为检测无内容的框）的损失可能会压倒总损失。即使背景检测已经训练良好并产生了很小的损失，这个小值乘以100K仍可能比实际检测的检测损失大几个数量级。最终的结果是无法训练的模型。
- en: 'The [RetinaNet paper](https://arxiv.org/abs/1708.02002) suggested an elegant
    solution to this problem: the authors tweaked the loss function to produce much
    smaller values on empty backgrounds. They call this the *focal loss*. Here are
    the details.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[RetinaNet论文](https://arxiv.org/abs/1708.02002)提出了这个问题的一个优雅解决方案：作者们调整了损失函数，使空背景的损失值大幅降低。他们称之为*焦点损失*。以下是详细信息。'
- en: 'We have already seen that RetinaNet uses a sigmoid activation to generate class
    probabilities. The output is a series of binary classifications, one for every
    class. A probability of 0 for every class means “background”; i.e., nothing to
    detect here. The classification loss used is the binary cross-entropy. For every
    class, it is computed from the actual binary class label *y* (0 or 1) and the
    predicted probability for the class *p* using the following formula:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到RetinaNet使用sigmoid激活来生成类别概率。输出是一系列二元分类，每个类别一个。每个类别的概率为0意味着“背景”；即，在这里没有什么可以检测的。使用的分类损失是二元交叉熵。对于每个类别，它根据实际的二元类别标签
    *y*（0或1）和预测的类别概率 *p* 使用以下公式计算：
- en: <math><mrow><mrow><mi>C</mi><mi>E</mi><mrow><mo>(</mo><mi>y</mi><mo>,</mo><mi>p</mi><mo>)</mo><mo>=</mo><mo>−</mo><mi>y</mi><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mi>p</mi><mo>)</mo><mo>−</mo><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo>)</mo><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></mrow></math>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>C</mi><mi>E</mi><mrow><mo>(</mo><mi>y</mi><mo>,</mo><mi>p</mi><mo>)</mo><mo>=</mo><mo>−</mo><mi>y</mi><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mi>p</mi><mo>)</mo><mo>−</mo><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo>)</mo><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></math>
- en: 'The focal loss is the same formula with a small modification:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 焦点损失（focal loss）是稍作修改后的同一公式：
- en: <math><mrow><mrow><mi>F</mi><mi>L</mi><mrow><mrow><mo>(</mo><mi>y</mi><mo>,</mo><mi>p</mi><mo>)</mo></mrow><mo>=</mo><mo>−</mo><mi>y</mi><mo>⋅</mo><mrow><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><msup><mrow><mo>)</mo></mrow><mrow><mi>γ</mi></mrow></msup></mrow><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mrow><mrow><mo>(</mo><mi>p</mi><mo>)</mo></mrow><mo>−</mo><mrow><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo>)</mo></mrow><mo>⋅</mo><msup><mrow><mi>p</mi></mrow><mrow><mi>γ</mi></mrow></msup><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></math>
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>F</mi><mi>L</mi><mrow><mrow><mo>(</mo><mi>y</mi><mo>,</mo><mi>p</mi><mo>)</mo></mrow><mo>=</mo><mo>−</mo><mi>y</mi><mo>⋅</mo><mrow><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><msup><mrow><mo>)</mo></mrow><mrow><mi>γ</mi></mrow></msup></mrow><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mrow><mrow><mo>(</mo><mi>p</mi><mo>)</mo></mrow><mo>−</mo><mrow><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo>)</mo></mrow><mo>⋅</mo><msup><mrow><mi>p</mi></mrow><mrow><mi>γ</mi></mrow></msup><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></math>
- en: 'For *γ*=0 this is exactly the binary cross-entropy, but for higher values of
    *γ* the behavior is slightly different. To simplify, let’s only consider the case
    of background boxes that do not belong to any class (i.e., where *y*=0 for all
    classes):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *γ*=0，这就是二元交叉熵，但对于更高的 *γ* 值，行为略有不同。为简化起见，让我们只考虑不属于任何类别的背景框（即对于所有类别 *y*=0
    的情况）：
- en: <math><mrow><mrow><mi>F</mi><msub><mrow><mi>L</mi></mrow><mrow><mi>b</mi><mi>k</mi><mi>g</mi></mrow></msub><mrow><mrow><mo>(</mo><mi>p</mi><mo>)</mo></mrow><mo>=</mo><mo>−</mo><msup><mrow><mi>p</mi></mrow><mrow><mi>γ</mi></mrow></msup></mrow><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo></mrow></mrow></mrow></math>
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>F</mi><msub><mrow><mi>L</mi></mrow><mrow><mi>b</mi><mi>k</mi><mi>g</mi></mrow></msub><mrow><mrow><mo>(</mo><mi>p</mi><mo>)</mo></mrow><mo>=</mo><mo>−</mo><msup><mrow><mi>p</mi></mrow><mrow><mi>γ</mi></mrow></msup></mrow><mo>⋅</mo><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo></mrow></mrow></mrow></math>
- en: And let’s plot the values of the focal loss for various values of *p* and *γ*
    ([Figure 4-16](#focal_loss_for_various_values_of_gammado)).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来绘制各种*p*和*γ*值下的聚焦损失的数值（[图4-16](#focal_loss_for_various_values_of_gammado)）。
- en: 'As you can see in the figure, with *γ*=2, which was found to be an adequate
    value, the focal loss is much smaller than the regular cross-entropy loss, especially
    for small values of *p*. For background boxes, where there is nothing to detect,
    the network will quickly learn to produce small class probabilities *p* across
    all classes. With the cross-entropy loss, these boxes, even well classified as
    “background” with *p*=0.1 for example, would still be contributing a significant
    amount: CE(0.1) = 0.05\. The focal loss is 100 times less: FL(0.1) = 0.0005.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在图中所见，当*γ*=2时（发现这是一个合适的值），聚焦损失远小于常规的交叉熵损失，特别是对于*p*接近于0的情况。对于背景框，网络会迅速学习在所有类别上产生小的类别概率*p*。使用交叉熵损失，即使像*p*=0.1这样明显分类为“背景”的框，仍会贡献相当数量的损失：CE(0.1)
    = 0.05。聚焦损失则小100倍：FL(0.1) = 0.0005。
- en: With the focal loss, it becomes possible to add the losses from all anchor boxes—all
    100K of them—and not worry about the total loss being overwhelmed by thousands
    of small losses from easy-to-classify background boxes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用聚焦损失函数，可以将所有锚框的损失相加，不必担心来自易于分类的背景框的成千上万个小损失会压倒总损失。
- en: '![](Images/pmlc_0416.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0416.png)'
- en: Figure 4-16\. Focal loss for various values of γ. For γ=0, this is the cross-entropy
    loss. For higher values of γ, the focal loss greatly de-emphasizes easy-to-classify
    background regions where p is close to 0 for every class.
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-16\. 各种*γ*值下的聚焦损失。当*γ*=0时，即为交叉熵损失。对于*γ*较大的值，聚焦损失会大幅弱化易于分类的背景区域，即对于每个类别*p*接近于0的情况。
- en: Smooth L1 loss (for box regression)
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平滑L1损失（用于框回归）
- en: 'Detection boxes are computed by a regression. For regressions, the two most
    common losses are L1 and L2, also called *absolute loss* and *squared loss*. Their
    formulas are (computed between a target value *a* and the predicted value *â*):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 检测框通过回归计算。对于回归，最常见的损失函数是L1和L2，也称为*绝对损失*和*平方损失*。它们的公式是（计算目标值*a*和预测值*â*之间的）：
- en: <math><mrow><mrow><mtable columnalign="left"><mtr><mtd><mi>L</mi><mn>1</mn><mrow><mo>(</mo><mi>a</mi><mo>,</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>)</mo></mrow><mo>=</mo><mrow><mo>|</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>|</mo></mrow></mtd></mtr><mtr><mtd><mi>L</mi><mn>2</mn><mrow><mo>(</mo><mi>a</mi><mo>,</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>)</mo><mo>=</mo><mrow><mo>(</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></mrow></mtd></mtr></mtable></mrow></mrow></math>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mtable columnalign="left"><mtr><mtd><mi>L</mi><mn>1</mn><mrow><mo>(</mo><mi>a</mi><mo>,</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>)</mo></mrow><mo>=</mo><mrow><mo>|</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>|</mo></mrow></mtd></mtr><mtr><mtd><mi>L</mi><mn>2</mn><mrow><mo>(</mo><mi>a</mi><mo>,</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>)</mo><mo>=</mo><mrow><mo>(</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></mrow></mtd></mtr></mtable></mrow></mrow></math>
- en: The problem with the L1 loss is that its gradient is the same everywhere, which
    is not great for learning. The L2 loss is therefore preferred for regressions—but
    it suffers from a different problem. In the L2 loss, differences between the predicted
    and target values are squared, which means that the loss tends to get very large
    as the prediction and the target grow apart. This becomes problematic if you have
    some outliers, like a couple of bad points in the data (for example, a target
    box with the wrong size). The result will be that the network will try to fit
    the bad data point at the expense of everything else, which is not good either.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: L1 损失的问题在于，其梯度处处相同，这对学习并不理想。因此，对于回归任务，更倾向于使用 L2 损失，但它也存在不同的问题。在 L2 损失中，预测值和目标值之间的差异被平方，这意味着随着预测值和目标值的偏离增大，损失会变得非常大。如果数据中存在异常值，比如一些错误的数据点（例如，目标框大小错误），这将带来问题。结果是网络会试图拟合这些错误的数据点，而忽视其他一切，这也是不理想的。
- en: 'A good compromise between the two is the *Huber loss,* or *smooth L1 loss*
    (see [Figure 4-17](#lonecomma_ltwocomma_and_huber_losses_for)). It behaves like
    the L2 loss for small values and like the L1 loss for large values. Close to zero,
    it has the nice property that its gradient is larger when the differences are
    larger, and therefore it pushes the network to learn more where it is making the
    biggest mistakes. For large values, it becomes linear instead of quadratic and
    avoids being thrown off by a couple of bad target values. Its formula is:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 两者之间的一个良好折中方案是*Huber损失*或*平滑L1损失*（参见[图 4-17](#lonecomma_ltwocomma_and_huber_losses_for)）。它在小值处的行为类似于L2损失，在大值处的行为类似于L1损失。接近零时，其梯度特性良好，差异越大时梯度越大，因此它推动网络在犯最大错误的地方学习更多。对于大值，它变为线性而非二次，并避免被一些错误的目标值所扰乱。其公式如下：
- en: <math><mrow><mrow><mtable><mtr><mtd columnalign="right"><msub><mrow><mi>L</mi></mrow><mrow><mi>δ</mi></mrow></msub><mrow><mrow><mo>(</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>)</mo></mrow><mo>=</mo><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></mstyle><mrow><mo>(</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></mrow></mtd><mtd><mtext>for</mtext><mrow><mo>|</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>|</mo><mo>≤</mo><mi>δ</mi></mrow></mtd></mtr><mtr><mtd
    columnalign="right"><mrow><mtable><mtr><mtd><msub><mrow><mi>L</mi></mrow><mrow><mi>δ</mi></mrow></msub><mo>=</mo><mi>δ</mi><mrow><mo>(</mo><mrow><mo>|</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>|</mo><mo>−</mo><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></mstyle><mi>δ</mi></mrow><mo>)</mo></mrow></mtd></mtr></mtable></mrow></mtd><mtd><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow></math>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mtable><mtr><mtd columnalign="right"><msub><mrow><mi>L</mi></mrow><mrow><mi>δ</mi></mrow></msub><mrow><mrow><mo>(</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>)</mo></mrow><mo>=</mo><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></mstyle><mrow><mo>(</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup></mrow></mrow></mtd><mtd><mtext>对于</mtext><mrow><mo>|</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>|</mo><mo>≤</mo><mi>δ</mi></mrow></mtd></mtr><mtr><mtd
    columnalign="right"><mrow><mtable><mtr><mtd><msub><mrow><mi>L</mi></mrow><mrow><mi>δ</mi></mrow></msub><mo>=</mo><mi>δ</mi><mrow><mo>(</mo><mrow><mo>|</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>|</mo><mo>−</mo><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mfrac><mrow><mn>1</mn></mrow><mrow><mn>2</mn></mrow></mfrac></mrow></mstyle><mi>δ</mi></mrow><mo>)</mo></mrow></mtd></mtr></mtable></mrow></mtd><mtd><mtext>否则</mtext></mtd></mtr></mtable></mrow></mrow></math>
- en: 'Where *δ* is an adjustable parameter. *δ* is the value around which the behavior
    switches from quadratic to linear. Another formula can be used to avoid the piecewise
    definition:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *δ* 是可调参数。*δ* 是行为从二次转线性的临界点。还有另一个公式可以避免分段定义：
- en: <math><mrow><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>δ</mi></mrow></msub><mrow><mrow><mo>(</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>)</mo></mrow><mo>=</mo><msup><mrow><mi>δ</mi></mrow><mrow><mn>2</mn></mrow></msup><mrow><mo>(</mo><mrow><msqrt><mrow><mn>1</mn><mo>+</mo><mrow><mo>(</mo><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mfrac><mrow><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover></mrow><mrow><mi>δ</mi></mrow></mfrac></mrow></mstyle><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup><mo>−</mo><mn>1</mn></mrow></mrow></msqrt></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><msub><mrow><mi>L</mi></mrow><mrow><mi>δ</mi></mrow></msub><mrow><mrow><mo>(</mo><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover><mo>)</mo></mrow><mo>=</mo><msup><mrow><mi>δ</mi></mrow><mrow><mn>2</mn></mrow></msup><mrow><mo>(</mo><mrow><msqrt><mrow><mn>1</mn><mo>+</mo><mrow><mo>(</mo><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mfrac><mrow><mi>a</mi><mo>−</mo><mover><mrow><mi>a</mi></mrow><mrow><mo>^</mo></mrow></mover></mrow><mrow><mi>δ</mi></mrow></mfrac></mrow></mstyle><msup><mrow><mo>)</mo></mrow><mrow><mn>2</mn></mrow></msup><mo>−</mo><mn>1</mn></mrow></mrow></msqrt></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>
- en: 'This alternate form does not give the exact same values as the standard Huber
    loss, but it has the same behavior: quadratic for small values, linear for large
    ones. In practice, either form will work well in RetinaNet, with *δ*=1.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这种备选形式与标准的Huber损失并不完全相同，但行为相似：对于小值是二次的，对于大值是线性的。在实践中，RetinaNet中任一形式都能很好地运作，*δ*=1。
- en: '![](Images/pmlc_0417.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0417.png)'
- en: Figure 4-17\. L1, L2, and Huber losses for regression. The desirable behaviors
    are quadratic for small values and linear for large ones. The Huber loss has both.
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-17\. 用于回归的L1、L2和Huber损失。对于小值，期望的行为是二次的，对于大值则是线性的。Huber损失同时具有这两种特性。
- en: Non-maximum suppression
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非最大抑制
- en: A detection network using numerous anchor boxes, such as RetinaNet, usually
    produces multiple candidate detections for every target box. We need an algorithm
    to select a single detection box for every detected object.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大量锚框的检测网络（如RetinaNet），通常会为每个目标框生成多个候选检测结果。我们需要一种算法为每个检测到的对象选择一个单一的检测框。
- en: '*Non-maximum suppression* (NMS) takes box overlap (IOU) and class confidence
    into account to select the most representative box for a given object ([Figure 4-18](#on_the_leftcolon_multiple_detections_for)).'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*非最大抑制*（NMS）考虑框重叠（IOU）和类别置信度，选择给定对象的最具代表性的框（[图 4-18](#on_the_leftcolon_multiple_detections_for)）。'
- en: '![](Images/pmlc_0418.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0418.png)'
- en: 'Figure 4-18\. On the left: multiple detections for the same object. On the
    right: a single box remaining after non-max suppression. Image from [Arthropods
    dataset](https://oreil.ly/sRrvU).'
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-18\. 左侧：同一对象的多个检测结果。右侧：经过非最大抑制后，剩余的单一框。图像来源于[节肢动物数据集](https://oreil.ly/sRrvU)。
- en: 'The algorithm uses a simple “greedy” approach: for every class, it considers
    the overlap (IOU) between all the predicted boxes. If two boxes overlap more than
    a given value *A* (IOU > *A*), it keeps the one with the highest class confidence.
    In Python-like pseudocode, for one given class:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法采用简单的“贪婪”方法：对于每一类别，它考虑所有预测框之间的重叠（IOU）。如果两个框的重叠大于给定值*A*（IOU > *A*），则保留具有最高类别置信度的框。在类似Python的伪代码中，对于一给定类别：
- en: '[PRE0]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: NMS works quite well in practice but it can have some unwanted side effects.
    Notice that the algorithm relies on a single threshold value (*A*). Changing this
    value changes the box filtering, especially for adjacent or overlapping objects
    in the original image. Take a look at the example in [Figure 4-19](#objects_close_to_each_other_create_a_pro).
    If the threshold is set at *A*=0.4, then the two boxes detected in the figure
    will be regarded as “overlapping” for the same class and the one with the lowest
    class confidence (the one on the left) will be discarded. That is obviously wrong.
    There are two butterflies to detect in this image and, before NMS, both were detected
    with a high confidence.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，NMS效果非常好，但可能会有一些意外的副作用。请注意，该算法依赖于单一阈值值（*A*）。更改此值会改变框过滤，尤其是对于原始图像中相邻或重叠的对象。看一看[图 4-19](#objects_close_to_each_other_create_a_pro)中的示例。如果将阈值设为*A*=0.4，则图中检测到的两个框将被视为“重叠”，属于同一类别，且类别置信度较低的那个（左侧的那个）将被丢弃。这显然是错误的。在此图像中有两只蝴蝶需要检测，在进行NMS之前，两者都以高置信度被检测到。
- en: '![](Images/pmlc_0419.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0419.png)'
- en: Figure 4-19\. Objects close to each other create a problem for the non-max suppression
    algorithm. If the NMS threshold is 0.4, the box detected on the left will be discarded,
    which is wrong. Image from [Arthropods dataset](https://oreil.ly/sRrvU).
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-19\. 彼此接近的对象对非最大抑制算法构成问题。如果NMS阈值为0.4，左侧检测到的框将被丢弃，这是错误的。图片来自[节肢动物数据集](https://oreil.ly/sRrvU)。
- en: Pushing the threshold value higher will help, but if it’s too high the algorithm
    will fail to merge boxes that correspond to the same object. The usual value for
    this threshold is *A*=0.5, but it still causes objects that are close together
    to be detected as one.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 将阈值设置得更高会有所帮助，但如果太高，算法将无法合并对应于同一对象的框。此阈值的通常值为*A*=0.5，但仍会导致彼此接近的对象被检测为一个。
- en: 'A slight variation on the basic NMS algorithm is called [*Soft-NMS*](https://arxiv.org/abs/1704.04503).
    Instead of removing non-maximum overlapping boxes altogether, it lowers their
    confidence score by the factor:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 基本NMS算法的轻微变体称为[*Soft-NMS*](https://arxiv.org/abs/1704.04503)。它不完全移除非最大重叠框，而是通过因子降低它们的置信度分数：
- en: <math><mrow><mrow><mi mathvariant="italic">exp</mi><mo>⁡</mo><mrow><mrow><mo>(</mo><mo>−</mo><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mfrac><mrow><mi>I</mi><mi>O</mi><msup><mrow><mi>U</mi></mrow><mrow><mn>2</mn></mrow></msup></mrow><mrow><mi>σ</mi></mrow></mfrac></mrow></mstyle><mo>)</mo></mrow></mrow></mrow></mrow></math>
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi mathvariant="italic">exp</mi><mo>⁡</mo><mrow><mrow><mo>(</mo><mo>−</mo><mstyle
    displaystyle="true" scriptlevel="0"><mrow><mfrac><mrow><mi>I</mi><mi>O</mi><msup><mrow><mi>U</mi></mrow><mrow><mn>2</mn></mrow></msup></mrow><mrow><mi>σ</mi></mrow></mfrac></mrow></mstyle><mo>)</mo></mrow></mrow></mrow></mrow></math>
- en: with *σ* being an adjustment factor that tunes the strength of the Soft-NMS
    algorithm. A typical value is *σ*=0.5\. The algorithm is applied by considering
    the box with the highest confidence score for a given class (the *max box*), and
    decreasing the scores for all other boxes by this factor. The max box is then
    put aside and the operation is repeated on the remaining boxes until none remain.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*σ*是调整Soft-NMS算法强度的调整因子。典型值为*σ*=0.5\. 该算法通过考虑给定类别的置信度得分最高的框（*max box*），并将所有其他框的分数减少该因子来应用。然后将最大框放在一边，并在剩余的框上重复操作，直到没有框剩余。'
- en: For nonoverlapping boxes (IOU=0), this factor is 1\. The confidence factors
    of boxes that do not overlap the max box are thus not affected. The factor gradually,
    but continuously, decreases as boxes overlap more with the max box. Highly overlapping
    boxes (IOU=0.9) get their confidence factor decreased by a lot (×0.2), which is
    the expected behavior because they are redundant with the max box and we want
    to get rid of them.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不重叠的框（IOU=0），此因子为1\. 不重叠最大框的置信度因子因此不受影响。随着框与最大框的重叠程度增加，这个因子会逐渐但连续地减小。高度重叠的框（IOU=0.9）其置信度因子会大幅度减少（×0.2），这是预期的行为，因为它们与最大框重叠，我们希望摆脱它们。
- en: Since the Soft-NMS algorithm does not discard any boxes, a second threshold,
    based on the class confidence, is used to actually prune the list of detections.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Soft-NMS算法不丢弃任何框，因此基于类别置信度使用第二个阈值来实际修剪检测列表。
- en: The effect of Soft-NMS on the example from [Figure 4-19](#objects_close_to_each_other_create_a_pro)
    is shown in [Figure 4-20](#objects_close_to_each_other_as_handled_b).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Soft-NMS对来自[图 4-19](#objects_close_to_each_other_create_a_pro)的例子的影响显示在[图 4-20](#objects_close_to_each_other_as_handled_b)中。
- en: '![](Images/pmlc_0420.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0420.png)'
- en: Figure 4-20\. Objects close to each other as handled by Soft-NMS. The detection
    box on the left is not deleted, but its confidence factor is reduced from 78%
    to 55%. Image from [Arthropods dataset](https://oreil.ly/sRrvU).
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-20\. Soft-NMS处理的彼此接近的对象。左侧的检测框未被删除，但其置信度因子从78%降低到55%。图片来自[节肢动物数据集](https://oreil.ly/sRrvU)。
- en: Note
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In TensorFlow, both styles of non-max suppression are available. Standard NMS
    is called `tf.image.non_max_suppression`, while Soft-NMS is called `tf.image.non_max_suppression_with_scores`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，有两种非最大抑制的风格可用。标准NMS称为`tf.image.non_max_suppression`，而Soft-NMS称为`tf.image.non_max_suppression_with_scores`。
- en: Other considerations
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他考虑事项
- en: In order to reduce the amount of data needed, it is customary to use a pretrained
    backbone.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少所需的数据量，通常使用预训练的主干网络。
- en: Classification datasets are much easier to put together than object detection
    datasets. That’s why readily available classification datasets are typically much
    larger than object detection datasets. Using a pretrained backbone from a classifier
    allows you to combine a generic large classification dataset with a task-specific
    object detection dataset and obtain a better object detector.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 分类数据集比目标检测数据集更容易组合。这就是为什么现成的分类数据集通常比目标检测数据集大得多。使用来自分类器的预训练骨干允许您将通用的大型分类数据集与特定任务的目标检测数据集结合起来，并获得更好的目标检测器。
- en: The pretraining is done on a classification task. Then the classification head
    is removed and the FPN and detection heads are added, initialized at random. The
    actual object detection training is performed with all weights trainable, which
    means that the backbone will be fine-tuned while the FPN and detection head train
    from scratch.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练是在分类任务上完成的。然后，移除分类头，并添加随机初始化的 FPN 和检测头。实际的目标检测训练是通过训练所有权重来完成的，这意味着骨干将进行微调，而
    FPN 和检测头则从头开始训练。
- en: Since detection datasets tend to be smaller, data augmentation (which we will
    cover in more detail in [Chapter 6](ch06.xhtml#preprocessing)) plays an important
    part in training. The basic data augmentation technique is to cut fixed-sized
    crops out of the training images at random, and at random zoom factors (see [Figure 4-21](#data_augmentation_for_detection_training)).
    With target bounding boxes adjusted appropriately, this allows you to train the
    network with the same object at different locations in the image, at different
    scales and with different parts of the background visible.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于检测数据集往往较小，数据增强（我们将在[第 6 章](ch06.xhtml#preprocessing)中详细介绍）在训练中起着重要作用。基本的数据增强技术是从训练图像中随机裁剪固定大小的区域，并使用随机的缩放因子（见[图
    4-21](#data_augmentation_for_detection_training)）。通过适当调整目标边界框，这使您可以训练网络，使同一对象在图像中的不同位置、不同尺度和不同背景部分可见。
- en: '![](Images/pmlc_0421.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0421.png)'
- en: Figure 4-21\. Data augmentation for detection training. Fixed-size images are
    cut at random from each training image, potentially at different zoom factors.
    Target box coordinates are recomputed relative to the new boundaries. This provides
    more training images and more object locations from the same initial training
    data. Image from [Arthropods dataset](https://oreil.ly/sRrvU).
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-21\. 目标检测训练的数据增强。从每个训练图像中随机裁剪固定大小的图像，可能具有不同的缩放因子。目标框坐标相对于新边界重新计算。这提供了更多的训练图像和来自相同初始训练数据的更多对象位置。图像来自[节肢动物数据集](https://oreil.ly/sRrvU)。
- en: A practical advantage of this technique is that it also provides fixed-sized
    training images to the neural network. You can train directly on a training dataset
    made up of images of different sizes and aspect ratios. The data augmentation
    takes care of getting all the images to the same size.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的一个实际优势是它还为神经网络提供了固定大小的训练图像。您可以直接在由不同大小和长宽比图像组成的训练数据集上进行训练。数据增强会处理将所有图像调整到相同大小的工作。
- en: Finally, what drives training and hyperparameter tuning are metrics. Object
    detection problems have been the subject of multiple large-scale contests where
    detection metrics have been carefully standardized; this topic is covered in detail
    in [“Metrics for Object Detection”](ch08.xhtml#metrics_for_object_detection) in
    [Chapter 8](ch08.xhtml#model_quality_and_continuous_evaluation).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，推动训练和超参数调整的是指标。目标检测问题已经成为多个大规模竞赛的主题，其中检测指标已经得到了精心标准化；这个主题在[“目标检测的指标”](ch08.xhtml#metrics_for_object_detection)中有详细介绍，位于[第
    8 章](ch08.xhtml#model_quality_and_continuous_evaluation)。
- en: 'Now that we have looked at object detection, let’s turn our attention to another
    class of problems: image segmentation.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过目标检测，让我们转向另一类问题：图像分割。
- en: Segmentation
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割
- en: Object detection finds bounding boxes around objects and classifies them. *Instance
    segmentation* adds, for every detected object, a pixel mask that gives the shape
    of the object. *Semantic segmentation*, on the other hand, does not detect specific
    instances of objects but classifies every pixel of the image into a category like
    “road,” “sky,” or “people.”
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测找到物体周围的边界框并对其进行分类。*实例分割* 对于每个检测到的对象，添加一个像素掩码，显示对象的形状。*语义分割* 则不检测特定的对象实例，而是将图像的每个像素分类为“道路”、“天空”或“人”等类别。
- en: Mask R-CNN and Instance Segmentation
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Mask R-CNN 和实例分割
- en: YOLO and RetinaNet, which we covered in the previous section, are examples of
    single-shot detectors. An image traverses them only once to produce detections.
    Another approach is to use a first neural network to suggest potential locations
    for objects to be detected, then use a second network to classify and fine-tune
    the locations of these proposals. These architectures are called *region proposal
    networks* (RPNs).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO和RetinaNet是我们在前一节中介绍的单次检测器的示例。一张图像只需经过它们一次即可生成检测结果。另一种方法是使用第一个神经网络提出对象可能的位置进行检测，然后使用第二个网络对这些提议的位置进行分类和微调。这些架构被称为*区域提议网络*（RPNs）。
- en: 'They tend to be more complex and therefore slower than single-shot detectors,
    but are also more accurate. There is a long list of RPN variants, all based on
    the original “regions with CNN features” idea: [R-CNN](https://arxiv.org/abs/1311.2524),
    [Fast R-CNN](https://arxiv.org/abs/1504.08083), [Faster R-CNN](https://arxiv.org/abs/1506.01497),
    and more. The state of the art, at the time of writing, is [Mask R-CNN](https://arxiv.org/abs/1703.06870),
    and that’s the architecture we are going to dive into next.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 它们往往比单次检测器更复杂，因此速度较慢，但也更准确。基于原始“带有CNN特征的区域”理念，有一长串的RPN变种：[R-CNN](https://arxiv.org/abs/1311.2524),
    [Fast R-CNN](https://arxiv.org/abs/1504.08083), [Faster R-CNN](https://arxiv.org/abs/1506.01497)，等等。截至撰写本文时，最先进的是[Mask
    R-CNN](https://arxiv.org/abs/1703.06870)，这也是我们接下来要深入探讨的架构。
- en: The main reason why it is important to be aware of architectures like Mask R-CNN
    is not their marginally superior accuracy, but the fact that they can be extended
    to perform instance segmentation tasks. In addition to predicting a bounding box
    around detected objects, they can be trained to predict their outline—i.e., find
    every pixel belonging to each detected object ([Figure 4-22](#instance_segmentation_involves_detecting)).
    Of course, training them remains a supervised training task and the training data
    will have to contain ground truth segmentation masks for all objects. Unfortunately,
    masks are more time-consuming to generate by hand than bounding boxes and therefore
    instance segmentation datasets are harder to find than simple object detection
    datasets.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 了解Mask R-CNN等架构的重要性主要不在于它们略微优越的准确性，而在于它们可以扩展到执行实例分割任务。除了预测围绕检测到的对象的边界框外，它们还可以训练以预测它们的轮廓——即找到每个检测到的对象所属的每个像素（[图
    4-22](#instance_segmentation_involves_detecting)）。当然，训练它们仍然是一个监督训练任务，训练数据将必须包含所有对象的地面真实分割掩码。不幸的是，与简单的目标检测数据集相比，生成掩码需要更多时间，因此实例分割数据集更难找到。
- en: '![](Images/pmlc_0422.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0422.png)'
- en: Figure 4-22\. Instance segmentation involves detecting objects and finding all
    the pixels that belong to each object. The objects in the images are shaded with
    a pixel mask. Image from [Arthropods dataset](https://oreil.ly/sRrvU).
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-22\. 实例分割涉及检测物体并找到属于每个物体的所有像素。图像中的物体被着色以形成像素掩码。图片来自[节肢动物数据集](https://oreil.ly/sRrvU)。
- en: Let’s look at RPNs in detail, first analyzing how they perform classic object
    detection, then how to extend them for instance segmentation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看RPNs，首先分析它们如何执行经典目标检测，然后如何扩展它们进行实例分割。
- en: Region proposal networks
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 区域提议网络
- en: 'An RPN is a simplified single-shot detection network that only cares about
    two classes: objects and background. An “object” is anything labeled as such in
    the dataset (any class), and “background” is the designated class for a box that
    does not contain an object.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: RPN是一个简化的单次检测网络，只关心两类：对象和背景。在数据集中标记为“对象”的是任何被标记为该类的东西（任何类），而“背景”是不包含对象的框的指定类别。
- en: 'An RPN can use an architecture similar to the RetinaNet setup we looked at
    earlier: a convolutional backbone, a feature pyramid network, a set of anchor
    boxes, and two heads. One head is for predicting boxes and the other is for classifying
    them as object or background (we are not predicting segmentation masks yet).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RPN可以使用类似于我们之前查看的RetinaNet设置的架构：一个卷积主干网络，一个特征金字塔网络，一组锚定框和两个头部。一个头部用于预测框，另一个用于对其进行分类，作为对象或背景（我们尚未预测分割掩码）。
- en: 'The RPN has its own loss function, computed from a slightly modified training
    dataset: the class of any ground truth object is replaced with a single class
    “object.” The loss function used for boxes is, as in RetinaNet, the Huber loss.
    For classes, since this is a binary classification, binary cross-entropy is the
    best choice.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: RPN 有其自己的损失函数，从稍微修改的训练数据集中计算：任何地面真实对象的类别都被替换为单一类别“对象”。与 RetinaNet 类似，用于盒子的损失函数是
    Huber 损失。对于类别，由于这是二元分类，二元交叉熵是最佳选择。
- en: Boxes predicted by the RPN then undergo non-max suppression. The top *N* boxes,
    sorted by their probability of being an “object,” are regarded as box proposals
    or *regions of interest* (ROIs) for the next stage. *N* is usually around one
    thousand, but if fast inference is important, it can be as little as 50\. ROIs
    can also be filtered by a minimal “object” score or a minimal size. In the TensorFlow
    Model Garden implementation, these thresholds are available even if they are set
    to zero by default. Bad ROIs can still be classified as “background” and rejected
    by the next stage, so letting them through at the RPN level is not a big problem.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: RPN 预测的框然后经过非极大值抑制。按照它们被视为框提议或感兴趣区域（ROIs）的“对象”概率排序的前 *N* 个框。*N* 通常约为一千，但如果快速推断很重要，则可以少至50。ROIs
    也可以通过最小“对象”分数或最小尺寸进行过滤。在 TensorFlow 模型园的实现中，即使默认设置为零，这些阈值也是可用的。坏的 ROIs 仍然可以被分类为“背景”并在下一阶段被拒绝，所以在
    RPN 级别放行它们不是一个大问题。
- en: One important practical consideration is that the RPN can be simple and fast
    if needed (see the example in [Figure 4-23](#a_simple_region_proposal_networkdot_the)).
    It can use the output of the backbone directly, instead of using an FPN, and its
    classification and detection heads can use fewer convolutional layers. The goal
    is only to compute approximate ROIs around likely objects. They will be refined
    and classified in the next step.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的实际考虑是，如果需要的话，RPN 可以简单快速（参见[图 4-23](#a_simple_region_proposal_networkdot_the)的示例）。它可以直接使用主干网络的输出，而不是使用
    FPN，并且其分类和检测头可以使用较少的卷积层。其目标仅是计算可能对象周围的近似 ROIs。它们将在下一步中进行细化和分类。
- en: '![](Images/pmlc_0423.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0423.png)'
- en: Figure 4-23\. A simple region proposal network. The output from the convolutional
    backbone is fed through a two-class classification head (object or background)
    and a box regression head. B is the number of anchor boxes per location (typically
    three). An FPN can be used as well.
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-23\. 简单的区域提议网络。从卷积主干网络输出的数据通过一个两类分类头（对象或背景）和一个盒子回归头进行处理。B 是每个位置的锚框数量（通常是三个）。也可以使用
    FPN。
- en: For example, the Mask R-CNN implementation in the TensorFlow Model Garden uses
    an FPN in its RPN but uses only three anchors per location, with aspect ratios
    of 0.5, 1.0, and 2.0, instead of the nine anchors per location used by RetinaNet.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 TensorFlow 模型园的 Mask R-CNN 实现中，在其 RPN 中使用了 FPN，但每个位置只使用了三个锚点，纵横比分别为 0.5、1.0
    和 2.0，而不是 RetinaNet 使用的每个位置九个锚点。
- en: R-CNN
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: R-CNN
- en: We now have a set of proposed regions of interest. What next?
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一组提议的感兴趣区域。接下来呢？
- en: Conceptually, the R-CNN idea ([Figure 4-24](Images/#conceptual_view_of_an_r-cnndot_images_go))
    is to crop the images along the ROIs and run the cropped images through the backbone
    again, this time with a full classification head attached to classify the objects
    (in our example, into “butterfly,” “spider,” etc.).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念上，R-CNN 的想法（[图 4-24](Images/#conceptual_view_of_an_r-cnndot_images_go)）是沿着
    ROIs 裁剪图像并再次通过主干网路运行裁剪后的图像，这次附带一个完整的分类头来对对象进行分类（在我们的例子中，可以是“蝴蝶”，“蜘蛛”等）。
- en: '![](Images/pmlc_0424.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0424.png)'
- en: 'Figure 4-24\. Conceptual view of an R-CNN. Images go through the backbone twice:
    the first time to generate regions of interest and the second time to classify
    the contents of these ROIs. Image from [Arthropods dataset](https://oreil.ly/sRrvU).'
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-24\. R-CNN 的概念视图。图像通过主干网路（backbone）两次传递：第一次生成感兴趣区域（ROIs），第二次对这些ROIs中的内容进行分类。图片来自
    [节肢动物数据集](https://oreil.ly/sRrvU)。
- en: In practice, however, this is too slow. The RPN can generate somewhere in the
    region of 50 to 2,000 proposed ROIs, and running them all through the backbone
    again would be a lot of work. Instead of cropping the image, the smarter thing
    to do is to crop the feature map directly, then run prediction heads on the result,
    as depicted in [Figure 4-25](#a_faster_r-cnn_or_mask_r-cnn_designdot_a).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这种方法太慢了。RPN 可以生成大约 50 到 2,000 个建议的 ROIs，如果再次全部通过骨干网络运行将是一项繁重的工作。与其裁剪图像，更明智的做法是直接裁剪特征图，然后在结果上运行预测头部，如图
    [4-25](#a_faster_r-cnn_or_mask_r-cnn_designdot_a) 所示。
- en: '![](Images/pmlc_0425.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0425.png)'
- en: Figure 4-25\. A faster R-CNN or Mask R-CNN design. As previously, the backbone
    generates a feature map and the RPN predicts regions of interest from it (only
    the result is shown). Then the ROIs are mapped back onto the feature map, and
    features are extracted and sent to the prediction heads for classification and
    more. Image from [Arthropods dataset](https://oreil.ly/sRrvU).
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-25\. 更快的 R-CNN 或 Mask R-CNN 设计。如前所述，骨干网络生成一个特征图，RPN 从中预测感兴趣的区域（仅显示结果）。然后将
    ROIs 映射回特征图，并从中提取特征发送到预测头部进行分类等。图片来源于 [Arthropods 数据集](https://oreil.ly/sRrvU)。
- en: 'This is slightly more complex when an FPN is used. The feature extraction is
    still performed on a given feature map, but in a FPN there are several feature
    maps to choose from. A ROI therefore must first be assigned to the most relevant
    FPN level. The assignment is usually done using this formula:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 FPN 时，这会稍微复杂些。特征提取仍然在给定的特征图上执行，但在 FPN 中有几个特征图可供选择。因此，ROI 必须首先分配给最相关的 FPN
    级别。分配通常使用以下公式进行：
- en: <math><mrow><mrow><mi>n</mi><mo>=</mo><mi>f</mi><mi>l</mi><mi>o</mi><mi>o</mi><mi>r</mi><mrow><mo>(</mo><msub><mrow><mi>n</mi></mrow><mrow><mn>0</mn></mrow></msub><mo>+</mo><msub><mrow><mi
    mathvariant="italic">log</mi><mo>⁡</mo></mrow><mrow><mn>2</mn></mrow></msub><mrow><mo>(</mo><msqrt><mrow><mi>w</mi><mi>h</mi></mrow></msqrt><mo>/</mo><mn>224</mn><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mrow></math>
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mi>n</mi><mo>=</mo><mi>f</mi><mi>l</mi><mi>o</mi><mi>o</mi><mi>r</mi><mrow><mo>(</mo><msub><mrow><mi>n</mi></mrow><mrow><mn>0</mn></mrow></msub><mo>+</mo><msub><mrow><mi
    mathvariant="italic">log</mi><mo>⁡</mo></mrow><mrow><mn>2</mn></mrow></msub><mrow><mo>(</mo><msqrt><mrow><mi>w</mi><mi>h</mi></mrow></msqrt><mo>/</mo><mn>224</mn><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mrow></math>
- en: 'where *w* and *h* are the width and height of the ROI, and *n*[0] is the FPN
    level where typical anchor box sizes are closest to 224\. Here, *floor* stands
    for rounding down to the most negative number. For example, here are the typical
    Mask R-CNN settings:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 *w* 和 *h* 分别是 ROI 的宽度和高度，*n*[0] 是 FPN 级别，典型的锚框大小最接近于 224\. 这里，*floor* 表示向最负数方向取整。例如，以下是典型的
    Mask R-CNN 设置：
- en: 'Five FPN levels, P[2], P[3], P[4], P[5], and P[6] (reminder: level P*[n]* represents
    a feature map 2*^n* times smaller in width and height than the input image)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 五个 FPN 级别，P[2]、P[3]、P[4]、P[5] 和 P[6]（提醒：级别 P*[n]* 表示比输入图像宽度和高度小 2*^n* 倍的特征图）
- en: Anchor box sizes of 32x32, 64x64, 128x128, 256x256, and 512x512 on their respective
    levels (same as in RetinaNet)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在它们各自的级别上的锚框大小为 32x32、64x64、128x128、256x256 和 512x512（与 RetinaNet 中相同）
- en: '*n*[0] = 4'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n*[0] = 4'
- en: With these settings, we can verify that (for example) an ROI of 80x160 pixels
    would get assigned to level P[3] and an ROI of 200x300 to level P[4], which makes
    sense.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些设置，我们可以验证，例如 80x160 像素的 ROI 将被分配到 P[3] 级别，而 200x300 的 ROI 将被分配到 P[4] 级别，这是合理的。
- en: ROI resampling (ROI alignment)
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ROI 重采样（ROI alignment）
- en: Special care is needed when extracting the feature maps corresponding to the
    ROIs. The feature maps must be extracted and resampled correctly. The Mask R-CNN
    paper’s authors discovered that any rounding error made during this process adversely
    affects detection performance. They called their precise resampling method *ROI
    alignment*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取对应于 ROIs 的特征图时需要特别小心。必须正确地提取和重采样特征图。Mask R-CNN 论文的作者们发现，在此过程中的任何舍入误差都会对检测性能产生不利影响。他们称其精确的重采样方法为
    *ROI alignment*。
- en: For example, let’s take an ROI of 200x300 pixels. It would be assigned to FPN
    level P[4], where its size relative to the P[4] feature map becomes (200 / 2⁴,
    300 / 2⁴) = (12.5, 18.75). These coordinates should not be rounded. The same applies
    to its position.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一个 200x300 像素的 ROI。它将被分配到 FPN 级别 P[4]，其相对于 P[4] 特征图的大小为 (200 / 2⁴, 300
    / 2⁴) = (12.5, 18.75)。这些坐标不应该被舍入。其位置也是如此。
- en: The features contained in this 12.5x18.75 region of the P[4] feature map must
    then be sampled and aggregated (using either max pooling or average pooling) into
    a new feature map, typically of size 7x7\. This is a well-known mathematical operation
    called *bilinear interpolation,* and we won’t dwell on it here. The important
    point to remember is that cutting corners here degrades performance.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: P[4] 特征图中的这个 12.5x18.75 区域包含的特征然后必须被采样和聚合（使用最大池化或平均池化）成一个新的特征图，通常大小为 7x7。这是一个众所周知的数学操作，称为*双线性插值*，我们在这里不详细讨论它。重要的是要记住，在这里马虎会降低性能。
- en: Class and bounding box predictions
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别和边界框预测
- en: 'The rest of the model is pretty standard. The extracted features go through
    multiple prediction heads in parallel—in this case:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的其余部分非常标准。提取的特征通过多个预测头并行处理——在这种情况下：
- en: A classification head to assign a class to each object suggested by the RPN,
    or classify it as background
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个分类头，为由 RPN 提出的每个对象分配一个类别，或将其分类为背景
- en: A box refinement head that further adjusts the bounding box
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步调整边界框的框修正头
- en: To compute detection and classification losses, the same target box assignment
    algorithm is used as in RetinaNet, described in the previous section. The box
    loss is also the same (Huber loss). The classification head uses a softmax activation
    with a special class added for “background.” In RetinaNet it was a series of binary
    classifications. Both work, and this implementation detail is not important. The
    total training loss is the sum of the final box and classification losses as well
    as the box and classification losses from the RPN.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算检测和分类损失，使用与 RetinaNet 中描述的相同目标框分配算法。框损失也相同（Huber 损失）。分类头使用 softmax 激活，并添加一个特殊的“背景”类。在
    RetinaNet 中，这是一系列二元分类。两者都有效，这个实现细节并不重要。总训练损失是最终框和分类损失的总和，以及 RPN 的框和分类损失。
- en: 'The exact design of the class and detection heads is given later, in [Figure 4-30](#the_mask_r-cnn_architecturedot_n_is_the).
    They are also very similar to what was used in RetinaNet: a straight sequence
    of layers, shared between all levels of the FPN.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 类别和检测头的确切设计稍后给出，在[图 4-30](#the_mask_r-cnn_architecturedot_n_is_the)中。它们与 RetinaNet
    中使用的非常相似：一系列直接的层，在 FPN 的所有级别之间共享。
- en: 'Mask R-CNN adds a third prediction head for classifying individual pixels of
    objects. The result is a pixel mask depicting the silhouette of the object (see
    [Figure 4-19](#objects_close_to_each_other_create_a_pro)). It can be used if the
    training dataset contains corresponding target masks. Before we explain how it
    works, however, we need to introduce a new kind of convolution, one capable of
    creating pictures rather than filtering and distilling them: transposed convolutions.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN 添加了第三个预测头部，用于对对象的每个像素进行分类。其结果是一个像素掩模，描绘了对象的轮廓（见[图 4-19](#objects_close_to_each_other_create_a_pro)）。如果训练数据集包含相应的目标掩模，则可以使用它。然而，在我们解释它的工作原理之前，我们需要介绍一种新的卷积类型，它能够创建图像而不是仅仅过滤和提炼它们：转置卷积。
- en: Transposed convolutions
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转置卷积
- en: '*Transposed convolutions*, sometimes also called *deconvolutions*, perform
    a learnable upsampling operation. Regular upsampling algorithms like nearest neighbor
    upsampling or bilinear interpolation are fixed operations. Transposed convolutions,
    on the other hand, involve learnable weights.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '*转置卷积*，有时也称为*反卷积*，执行可学习的上采样操作。常规的上采样算法如最近邻上采样或双线性插值是固定操作。而转置卷积则涉及可学习的权重。'
- en: Note
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The name “transposed convolution” comes from the fact that in the matrix representation
    of a convolutional layer, which we are not covering in this book, the transposed
    convolution is performed using the same convolutional matrix as an ordinary convolution,
    but transposed.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 名称“转置卷积”来源于卷积层的矩阵表示中的事实，我们在本书中未涵盖，即使用与普通卷积相同的卷积矩阵进行转置卷积。
- en: 'The transposed convolution pictured in [Figure 4-26](#transposed_convolutiondot_each_pixel_of)
    has a single input and a single output channel. The best way to understand what
    it does is to imagine that it is painting with a brush on an output canvas. The
    brush is a 3x3 filter. Every value of the input image is projected through the
    filter on the output. Mathematically, every element of the 3x3 filter is multiplied
    by the input value and the result is added to whatever is already on the output
    canvas. The operation is then repeated at the next position: in the input we move
    by 1, and in the output we move with a configurable stride (2 in this example).
    Any stride larger than 1 results in an upsampling operation. The most frequent
    settings are stride 2 with a 2x2 filter or stride 3 with a 3x3 filter.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-26](#transposed_convolutiondot_each_pixel_of)中描绘的转置卷积具有单个输入和单个输出通道。理解它的最佳方法是想象它在输出画布上用刷子绘画。刷子是一个3x3的滤波器。输入图像的每个值都通过滤波器投射到输出上。数学上，3x3滤波器的每个元素都与输入值相乘，结果加到输出画布上已有的内容上。然后在下一个位置重复操作：在输入中移动1，输出中使用可配置步长（本例中为2）。大于1的步长会导致上采样操作。最常见的设置是步长为2，使用2x2滤波器，或步长为3，使用3x3滤波器。'
- en: If the input is a feature map with multiple channels, the same operation is
    applied to each channel independently, with a new filter each time; then all the
    outputs are added element by element, resulting in a single output channel.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入是具有多个通道的特征图，则对每个通道独立应用相同的操作，每次使用一个新的滤波器；然后将所有输出元素逐元素相加，生成单个输出通道。
- en: It is of course possible to repeat this operation multiple times on the same
    feature map, with a new set of filters each time, which results in a feature map
    with multiple channels.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，可以在同一特征图上多次重复此操作，每次使用新的滤波器集，从而生成具有多个通道的特征图。
- en: In the end, for a multichannel input and a multichannel output, the weights
    matrix of a transposed convolution will have the shape shown in [Figure 4-27](#the_weights_matrix_of_a_transposed_convo).
    This is, by the way, the same shape as a regular convolutional layer.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多通道输入和多通道输出，转置卷积的权重矩阵形状如[图 4-27](#the_weights_matrix_of_a_transposed_convo)所示。顺便说一句，这与常规卷积层的形状相同。
- en: '![](Images/pmlc_0426.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0426.png)'
- en: Figure 4-26\. Transposed convolution. Each pixel of the original image (top)
    multiplies a 3x3 filter, and the result is added to the output. In a transposed
    convolution of stride 2, the output window moves by a step of 2 for every input
    pixel, creating a larger image (shifted output window pictured with a dashed outline).
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-26\. 转置卷积。原始图像的每个像素（顶部）都乘以一个3x3的滤波器，并将结果加到输出上。在步长为2的转置卷积中，输出窗口对每个输入像素移动2步，从而创建一个更大的图像（移动的输出窗口用虚线轮廓描绘）。
- en: '![](Images/pmlc_0427.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0427.png)'
- en: Figure 4-27\. The weights matrix of a transposed convolutional layer, sometimes
    also called a “deconvolution.” At the bottom is the schematic notation of deconvolutional
    layers that will be used for the models in this chapter.
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-27\. 转置卷积层的权重矩阵，有时也称为“反卷积”。底部是本章模型将使用的反卷积层的示意符号。
- en: Instance segmentation
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实例分割
- en: Let’s get back to Mask R-CNN, and its third prediction head that classifies
    individual pixels of objects. The output is a pixel mask outlining the silhouette
    of the object (see [Figure 4-22](#instance_segmentation_involves_detecting)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到Mask R-CNN及其第三个预测头，用于对对象的各个像素进行分类。输出是描绘对象轮廓的像素掩码（参见[图 4-22](#instance_segmentation_involves_detecting)）。
- en: Mask R-CNN and other RPNs work on a single ROI at a time, with a fairly high
    probability that this ROI is actually interesting, so they can do more work per
    ROI and with a higher precision. Instance segmentation is one such task.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN和其他RPN一次处理一个ROI，相当有可能该ROI确实有趣，因此可以对每个ROI执行更多工作并提高精度。实例分割就是这样一个任务。
- en: The instance segmentation head uses transposed convolution layers to upsample
    the feature map into a black-and-white image that is trained to match the silhouette
    of the detected object.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 实例分割头部使用转置卷积层将特征图上采样为一个训练成与检测到的对象轮廓相匹配的黑白图像。
- en: '[Figure 4-30](#the_mask_r-cnn_architecturedot_n_is_the) shows the complete
    Mask R-CNN architecture.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-30](#the_mask_r-cnn_architecturedot_n_is_the)展示了完整的Mask R-CNN架构。'
- en: '![](Images/pmlc_0430.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0430.png)'
- en: Figure 4-30\. The Mask R-CNN architecture. N is the number of ROIs proposed
    by the RPN, and K is the number of classes; “deconv” denotes a transposed convolutional
    layer, which upsamples the feature maps to predict an object mask.
  id: totrans-254
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-30\. Mask R-CNN 架构。N 是由 RPN 提出的 ROIs 数量，K 是类别数；“deconv” 表示转置卷积层，用于上采样特征图以预测目标掩膜。
- en: Notice that the mask head produces one mask per class. This seems to be redundant
    since there is a separate classification head. Why predict *K* masks for one object?
    In reality, this design choice increases the segmentation accuracy because it
    allows the segmentation head to learn class-specific hints about objects.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，掩膜头为每个类别生成一个掩膜。这似乎有些冗余，因为还有一个单独的分类头。为什么为一个对象预测 *K* 个掩膜？事实上，这种设计选择增加了分割精度，因为它允许分割头部学习有关对象的类别特定提示。
- en: 'Another implementation detail is that the resampling and alignment of the feature
    maps to the ROIs is actually performed twice: once with a 7x7x256 output for the
    classification and detection head, and again with different settings (resampling
    to 14x14x256) specifically for the mask head to give it more detail to work with.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个实施细节是特征图对 ROIs 的重新采样和对齐实际上执行了两次：一次是用于分类和检测头部的 7x7x256 输出，再次是使用不同设置（重新采样为
    14x14x256），专门用于掩膜头以提供更多详细信息。
- en: The segmentation loss is a simple pixel-by-pixel binary cross-entropy loss,
    applied once the predicted mask has been rescaled and upsampled to the same coordinates
    as the ground truth mask. Note that only the mask predicted for the predicted
    class is taken into account in the loss calculation. Other masks computed for
    the wrong classes are ignored.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 分割损失是简单的逐像素二元交叉熵损失，应用于预测的掩膜在重新缩放和上采样到与地面实况掩膜相同坐标后。请注意，在损失计算中仅考虑预测类别的预测掩膜。计算其他错误类别的掩膜将被忽略。
- en: We now have a complete picture of how Mask R-CNN works. One thing to notice
    is that with all the improvements added to the R-CNN family of detectors, Mask
    R-CNN is now a “two-pass” detector in name only. The input image effectively goes
    through the system only once. The architecture is still slower than RetinaNet
    but achieves a slightly higher detection accuracy and adds instance segmentation.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们完全理解了 Mask R-CNN 的工作原理。需要注意的一点是，尽管 R-CNN 家族的检测器都进行了改进，但 Mask R-CNN 现在在名义上只是一个“双通道”检测器。输入图像实际上只通过系统一次。该架构仍然比
    RetinaNet 慢，但实现了稍微更高的检测精度，并增加了实例分割。
- en: An extension of RetinaNet with an added mask head exists ([RetinaMask](https://arxiv.org/abs/1901.03353)),
    but it does not outperform Mask R-CNN. Interestingly, the paper notes that adding
    the mask head and associated loss actually improves the accuracy of bounding box
    detections (the other head). A similar effect might explain some of the improved
    accuracy of Mask R-CNN too.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 还存在一种扩展自 RetinaNet 的模型，称为 [RetinaMask](https://arxiv.org/abs/1901.03353)，但其性能不如
    Mask R-CNN。有趣的是，论文指出，添加掩膜头和相关损失实际上提高了边界框检测的准确性（另一个头部）。类似的效果可能也解释了 Mask R-CNN 的一些改进精度。
- en: 'One limitation of the Mask R-CNN approach is that the predicted object masks
    are fairly low resolution: 28x28 pixels. The similar but not exactly equivalent
    problem of semantic segmentation has been solved with high-resolution approaches.
    We’ll explore this in the next section.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Mask R-CNN 方法的一个局限性是预测的对象掩膜分辨率相对较低：28x28 像素。类似但不完全等效的语义分割问题已经通过高分辨率方法解决。我们将在下一节中探讨这个问题。
- en: U-Net and Semantic Segmentation
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: U-Net 和语义分割
- en: In semantic segmentation, the goal is to classify every pixel of the image into
    global classes like “road,” “sky,” “vegetation,” or “people” (see [Figure 4-31](#in_semantic_image_segmentationcomma_ever)).
    Individual instances of objects, like individual people, are not separated. All
    “people” pixels across the entire image are part of the same “segment.”
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义分割中，目标是将图像中的每个像素分类到全局类别，如“道路”、“天空”、“植被”或“人”（见 [图 4-31](#in_semantic_image_segmentationcomma_ever)）。对象的个别实例，如个别人，未分隔。整个图像中所有“人”像素都属于同一“段”。
- en: '![](Images/pmlc_0431.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0431.png)'
- en: Figure 4-31\. In semantic image segmentation, every pixel in the image is assigned
    a category (like “road,” “sky,” “vegetation,” or “building”). Notice that “people,”
    for example, is a single class across the whole image. Objects are not individualized.
    Image from [Cityscapes](https://www.cityscapes-dataset.com).
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-31\. 在语义图像分割中，图像中的每个像素都被分配一个类别（如“道路”、“天空”、“植被”或“建筑”）。请注意，例如，“人”是整个图像中的一个单一类别。来自
    [Cityscapes](https://www.cityscapes-dataset.com) 的图像。
- en: For semantic image segmentation, a simple and quite often sufficient approach
    is called [U-Net](https://oreil.ly/yrwBW). The U-Net is a convolutional network
    architecture that was designed for biomedical image segmentation (see [Figure 4-32](#the_u-net_architecture_was_designed_to_s))
    and won a cell tracking competition in 2015.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语义图像分割，一个简单且通常足够的方法称为 [U-Net](https://oreil.ly/yrwBW)。U-Net 是一个卷积网络架构，专为生物医学图像分割设计（见
    [图 4-32](#the_u-net_architecture_was_designed_to_s)），并在 2015 年的细胞跟踪竞赛中获胜。
- en: '![](Images/pmlc_0432.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0432.png)'
- en: Figure 4-32\. The U-Net architecture was designed to segment biomedical images
    such as these microscopy cell images. Images from [Ronneberger et al., 2015](https://oreil.ly/yrwBW).
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-32\. U-Net 架构旨在分割生物医学图像，例如这些显微镜细胞图像。来自 [Ronneberger et al., 2015](https://oreil.ly/yrwBW)
    的图像。
- en: 'The U-Net architecture is represented in [Figure 4-33](#the_u-net_architecture_consists_of_mirro).
    A U-Net consists of an encoder which downsamples an image to an encoding (the
    lefthand side of architecture), and a mirrored decoder which upsamples the encoding
    back to the desired mask (the righthand side of the architecture). The decoder
    blocks have a number of skip connections (depicted by the horizontal arrows in
    the center) that directly connect from the encoder blocks. These skip connections
    copy features at a specific resolution and concatenate them channel-wise with
    specific feature maps in the decoder. This brings information at various levels
    of semantic granularity from the encoder directly into the decoder. (Note: cropping
    may be necessary on the skip connections because of slight size misalignments
    of the feature maps in corresponding levels of the encoder and decoder. Indeed,
    U-Net uses all convolutions without padding, which means that border pixels are
    lost at each layer. This design choice is not fundamental though, and padding
    can be used as well.)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net 架构在 [图 4-33](#the_u-net_architecture_consists_of_mirro) 中表示。U-Net 包括一个编码器，将图像降采样为编码（架构的左侧），以及一个镜像的解码器，将编码上采样回所需的掩模（架构的右侧）。解码器块具有多个跳过连接（在中心显示的水平箭头），直接连接从编码器块复制特征。这些跳过连接将特定分辨率的特征从编码器直接传递到解码器，并按通道将它们串联在一起。这样做可以将编码器各级别的语义信息直接引入解码器。
    （注意：由于编码器和解码器相应级别的特征图大小可能略有不对齐，因此可能需要在跳过连接上进行裁剪。实际上，U-Net 使用所有卷积操作时没有填充，这意味着每一层的边界像素都会丢失。不过，这种设计选择并非必须，也可以使用填充。）
- en: '![](Images/pmlc_0433.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0433.png)'
- en: Figure 4-33\. The U-Net architecture consists of mirrored encoder and decoder
    blocks that take on a U shape when depicted as shown here. Skip connections concatenate
    feature maps along the depth axis (channels). K is the target number of classes.
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-33\. U-Net 架构由镜像编码器和解码器块组成，当如此展示时呈现出 U 形。跳过连接沿深度轴（通道）串联特征图。K 是目标类别数量。
- en: Images and labels
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像和标签
- en: To illustrate U-Net image segmentation we’ll use the [Oxford Pets dataset](https://oreil.ly/GNyKx),
    where each of the input images contains a label mask as shown in [Figure 4-34](Images/#training_images_left_parenthesistop_rowr).
    The label is an image in which pixels are assigned one of three integer values
    depending on whether they are background, the object outline, or the object interior.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 U-Net 图像分割，我们将使用 [Oxford 宠物数据集](https://oreil.ly/GNyKx)，其中每个输入图像都包含标签掩模，如
    [图 4-34](Images/#training_images_left_parenthesistop_rowr) 所示。该标签是一个图像，其中像素根据它们是背景、对象轮廓还是对象内部而被分配为三个整数值之一。
- en: '![](Images/pmlc_0434.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0434.png)'
- en: Figure 4-34\. Training images (top row) and labels (bottom row) from the Oxford
    Pets dataset.
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-34\. Oxford 宠物数据集的训练图像（顶部行）和标签（底部行）。
- en: 'We’ll treat these three pixel values as the index of class labels and train
    the network to carry out multiclass classification:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这三个像素值视为类标签的索引，并训练网络进行多类分类：
- en: '[PRE1]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The complete code is available in [*04b_unet_segmentation.ipynb* on GitHub](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/04_detect_segment/04b_unet_segmentation.ipynb).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在GitHub上的[*04b_unet_segmentation.ipynb*](https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/04_detect_segment/04b_unet_segmentation.ipynb)中找到。
- en: Architecture
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 架构
- en: 'Training a U-Net architecture from scratch requires a lot of trainable parameters.
    As discussed in [“Other considerations”](#other_considerations) it’s difficult
    to label datasets for tasks such as object detection and segmentation. Therefore,
    to use the labeled data efficiently, it is better to use a pretrained backbone
    and employ transfer learning for the encoder block. As in [Chapter 3](ch03.xhtml#image_vision),
    we can use a pretrained MobileNetV2 to create the encoding:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始训练U-Net架构需要大量可训练参数。如[“其他考虑”](#other_considerations)中讨论的那样，对于对象检测和分割等任务，标记数据集很困难。因此，为了有效使用标记数据，最好使用预训练的主干网络，并为编码器块使用迁移学习。正如[第3章](ch03.xhtml#image_vision)中所述，我们可以使用预训练的MobileNetV2来创建编码器：
- en: '[PRE2]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The decoder side will consist of upsampling layers to get back to the desired
    mask shape. The decoder also needs feature maps from specific layers of the encoder
    (skip connections). The layers of the MobileNetV2 model that we need can be obtained
    by name as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器侧将包括上采样层，以恢复到所需的掩模形状。解码器还需要来自编码器特定层的特征图（跳跃连接）。我们需要的MobileNetV2模型的层可以按名称获取如下：
- en: '[PRE3]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The “down stack” or lefthand side of the U-Net architecture then consists of
    the image as input, and these layers as outputs. We are carrying out transfer
    learning, so the entire lefthand side does not need weight adjustments:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net架构的“下堆栈”或左侧包括图像作为输入，以及这些层作为输出。我们正在进行迁移学习，因此整个左侧不需要调整权重：
- en: '[PRE4]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Upsampling in Keras can be accomplished using a `Conv2DTranspose` layer. We
    also add batch normalization and nonlinearity to each step of the upsampling:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，可以使用`Conv2DTranspose`层来实现上采样。我们还在每个上采样步骤中添加批量归一化和非线性：
- en: '[PRE5]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Each stage of the decoder up stack is concatenated with the corresponding layer
    of the encoder down stack:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的每个上堆栈阶段都与编码器的相应层连接起来：
- en: '[PRE6]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Training
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练
- en: 'We can display the predictions on a few selected images using a Keras callback:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Keras回调在几个选定的图像上显示预测：
- en: '[PRE7]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The result of doing so on the Oxford Pets dataset is shown in [Figure 4-35](#the_predicted_mask_on_the_input_image_im).
    Note that the model starts out with garbage (top row), as one would expect, but
    then learns which pixels correspond to the animal and which pixels correspond
    to the background.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做在牛津宠物数据集上的结果显示在[图 4-35](#the_predicted_mask_on_the_input_image_im)中。请注意，模型从垃圾（顶部行）开始，正如人们所预期的那样，但然后学会了哪些像素对应于动物，哪些像素对应于背景。
- en: '![](Images/pmlc_0435.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/pmlc_0435.png)'
- en: Figure 4-35\. The predicted mask on the input image improves epoch by epoch
    as the model is trained.
  id: totrans-294
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-35\. 在模型训练时，输入图像上的预测掩模逐步改善。
- en: However, because the model is trained to predict each pixel as background, outline,
    or interior independently of the other pixels, we see artifacts such as unclosed
    regions and disconnected pixels. The model doesn’t realize that the region corresponding
    to the cat should be closed. That is why this approach is mostly used on images
    where the segments to be detected do not need to be contiguous, like the example
    in [Figure 4-31](#in_semantic_image_segmentationcomma_ever), where the “road,”
    “sky,” and “vegetation” segments often have discontinuities.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于模型训练为独立预测每个像素为背景、轮廓或内部，我们看到诸如未闭合区域和不连通像素等伪影。模型没有意识到与猫对应的区域应该是封闭的。这就是为什么这种方法主要用于不需要连续性的图像上，例如[图 4-31](#in_semantic_image_segmentationcomma_ever)中的示例，其中“道路”、“天空”和“植被”段经常有不连续性。
- en: An example of an application is in self-driving algorithms, to detect the road.
    Another is in satellite imagery, where a U-Net architecture was used to solve
    the hard problem of distinguishing clouds from snow; both are white, but snow
    coverage is useful ground-level information, whereas cloud obstruction means that
    the image needs to be retaken.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 应用示例包括自动驾驶算法，用于检测道路。另一个是卫星图像，其中使用U-Net架构解决了区分云与雪的难题；两者都是白色的，但雪覆盖是有用的地面信息，而云的遮挡意味着需要重新拍摄图像。
- en: Summary
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we looked at object detection and image segmentation methods.
    We started with YOLO, considering its limitations, and then discussed RetinaNet,
    which innovates over YOLO in terms of both the architecture and the losses used.
    We also discussed Mask R-CNN to carry out instance segmentation and U-Net to carry
    out semantic segmentation.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了目标检测和图像分割方法。我们从 YOLO 开始，考虑了其局限性，然后讨论了 RetinaNet，它在架构和使用的损失方面对 YOLO
    进行了创新。我们还讨论了 Mask R-CNN 用于实例分割和 U-Net 用于语义分割。
- en: In the next chapters, we will delve more deeply into different parts of the
    computer vision pipeline using the simple transfer learning image classification
    architecture from [Chapter 3](ch03.xhtml#image_vision) as our core model. The
    pipeline steps remain the same regardless of the backbone architecture or the
    problem being solved.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更深入地研究计算机视觉流程的不同部分，使用简单的迁移学习图像分类架构作为我们的核心模型，该架构源自[第三章](ch03.xhtml#image_vision)。无论是骨干架构还是解决的问题，流程步骤都保持不变。
