- en: Chapter 7\. From Theory to Practice
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章。从理论到实践
- en: Real-world ML projects are rarely straightforward. You don’t always know what
    exact fairness metric to implement or how robust you need the model inference
    to be. Creating trustworthy ML systems almost always involves trading off between
    technical considerations and *human decisions* like budget considerations, finding
    a balance between trust and utility, and aligning stakeholders toward a common
    goal. As an ML expert and practitioner, you are capable of handling the technical
    aspects. But when it comes to human-in-the-loop decisions, you may not be required
    to make all of those (and perhaps you shouldn’t). However, it’s important to have
    at least a high-level understanding of the concepts involved in both human and
    technical decisions in order to effectively align trustworthy ML development with
    the broader organizational picture.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 真实世界的机器学习项目很少是直截了当的。你并不总是知道要实施什么确切的公平度量标准，或者模型推断需要多么稳健。创建可信任的机器学习系统几乎总是涉及在技术考虑和*人类决策*（如预算考虑、在信任和效用之间找到平衡，并使利益相关者朝着共同目标努力）之间进行权衡。作为一个机器学习专家和实践者，你有能力处理技术方面的问题。但是当涉及到人为决策时，你可能并不需要做所有这些决定（也许你不应该）。然而，理解涉及人类和技术决策的概念至少具有高层次的理解是非常重要的，以便有效地将可信任的机器学习开发与更广泛的组织格局对齐。
- en: In this chapter, we’ll share with you tools for actually implementing the trustworthy
    ML methods we’ve discussed in earlier chapters in messy, production-grade systems.
    We’ll start by reviewing some additional technical factors you might need to address
    before pushing a model to production—​such as causality, sparsity, and uncertainty—​in
    Part I. From there, we’ll move on to Part II to discuss how to effectively collaborate
    with stakeholders beyond the development team.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将与您分享一些工具，用于在混乱的生产级系统中实际实施我们在前几章中讨论过的可信机器学习方法。我们将首先回顾一些可能需要在推送模型到生产之前解决的额外技术因素，例如因果性、稀疏性和不确定性—​在第一部分。从那里开始，我们将转向第二部分，讨论如何有效地与开发团队之外的利益相关者合作。
- en: 'Part I: Additional Technical Factors'
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一部分：额外的技术因素
- en: There are some additional technical considerations you might need to think about
    while incorporating one or more trust elements in your ML project. These are somewhat
    different than the concepts discussed in [Chapter 5](ch05.html#chapter5). Specifically,
    they are already established scientific concepts and tools that are becoming more
    and more relevant to ML—​and trusted ML—​applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在将一个或多个信任元素纳入您的机器学习项目时，您可能需要考虑一些额外的技术因素。这些因素与[第五章](ch05.html#chapter5)中讨论的概念有所不同。具体而言，它们已经是已确立的科学概念和工具，这些概念和工具在机器学习应用中变得越来越重要—​以及可信的机器学习应用。
- en: Causal Machine Learning
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 因果机器学习
- en: Suppose you want to model whether a user clicks an online ad they receive as
    a function of who is clicking the ad, what their recent activity history is, the
    subject of the ad, and the time of the day. How do you make sure that a specific
    user segment is more or less likely to click on ads? Just throwing everything
    as input features into a click prediction model and looking at variable importance
    isn’t the best idea. Maybe certain user segments just spend more time on the internet
    during a certain time of the day and hence click on more ads during those times.
    How do you go beyond such interactions—which affect data collection itself—to
    extract true cause-effect relationships from your data? Causal inference is the
    answer here. Conventional ML depends on observational data. Data collection generally
    doesn’t concern itself with cause-effect relationships between a few features
    while controlling for the effect of other features. The connections between features
    a typical ML model infers by analyzing observational datasets are simply *associations*,
    not *causations*. Concepts and tools from the field of causal inference are helpful
    in navigating these deficiencies.^([1](ch07.html#idm45621832115168))
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要建模用户是否点击他们收到的在线广告，作为点击广告的功能，点击者是谁，他们最近的活动历史是什么，广告的主题，以及时间是什么。如何确保特定的用户段更有可能或不太可能点击广告？仅将所有输入特征投入点击预测模型并查看变量重要性并不是最好的主意。也许某些用户段在一天的某个特定时间更多地花费时间上网，因此在这些时间内点击更多的广告。如何超越这种影响——这些影响影响数据收集本身——从数据中提取真正的因果关系？因果推断就是答案。传统的机器学习依赖于观测数据。数据收集通常不涉及一些特征之间的因果关系，同时控制其他特征的效果。典型机器学习模型通过分析观测数据集推断的特征之间的连接仅仅是*关联*，而不是*因果关系*。因果推断领域的概念和工具有助于弥补这些缺陷。^([1](ch07.html#idm45621832115168))
- en: Steps to causal inference
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因果推断的步骤
- en: 'Causal inference follows four general steps:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因果推断遵循四个一般步骤：
- en: '*Step 1: Create a model of a causality problem*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一步：创建因果问题模型*'
- en: This is analogous to creating a hypothesis in the scientific method. This step
    might involve defining the model as a detailed causal graph. Alternatively, it
    could just be sets of names of variables that correspond to relevant categories
    like common causes or instrumental variables.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于科学方法中的假设生成。这一步骤可能涉及将模型定义为详细的因果图。或者，它可能只是变量名称集合，这些名称对应于像共同原因或工具变量之类的相关类别。
- en: '*Step 2: Identify a target estimand*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二步：确定目标估计量*'
- en: This is the process of identifying the causal effect on the variables of interest.
    There are many tools for this step, including ML-based tools.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是识别感兴趣变量的因果效应的过程。有许多工具可以用于这一步骤，包括基于机器学习的工具。
- en: '*Step 3: Determine the strength of the causal effect*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*第三步：确定因果效应的强度*'
- en: Drawing a causal arrow from one variable to another isn’t enough. Much as you
    might use correlation coefficients to determine the strength of a linear relationship,
    you’ll also need to estimate the strength of a causal effect. Even if there is
    a causal relationship, it can still be weak.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个变量向另一个变量绘制因果箭头并不足够。就像你可能使用相关系数来确定线性关系的强度一样，你还需要估计因果效应的强度。即使存在因果关系，它可能仍然很弱。
- en: '*Step 4: Subjecting the causal model to refutation*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*第四步：使因果模型经受反驳*'
- en: Typically, in ML, you want to create a model that’s the best fit for the data.
    In causal inference, you want to create a *causal* model that represents the best
    hypothesis for how causality works in the data. Even if you have identified the
    causal effects and estimated their strengths, you should still test a few plausible
    alternative hypotheses. [Table 7-1](#table-perturb) lists a few things to try,
    with notes about what ideal behavior should look like.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在机器学习中，你希望创建最适合数据的模型。在因果推断中，你希望创建一个*因果*模型，它代表了如何在数据中工作的最佳假设。即使你已经确定了因果效应并估计了它们的强度，你仍然应该测试几个可能的替代假设。[表7-1](#table-perturb)列出了一些尝试的内容，并注明理想行为应该是什么样的。
- en: Table 7-1\. Example considerations for potentially perturbing
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-1。潜在扰动的示例考虑因素
- en: '| Action | Description | Ideal |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | 描述 | 理想 |'
- en: '| --- | --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Random common cause | Does the estimation method change its estimate after
    you add an independent random variable as a common cause to the dataset? | It
    shouldn’t |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 随机共同原因 | 在你向数据集添加独立随机变量作为共同原因后，估计方法是否改变了其估计值？ | 不应该改变 |'
- en: '| Placebo treatment | What happens to the estimated causal effect when you
    replace the true treatment variable with an independent random variable? | The
    effect should go to zero |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 安慰剂治疗 | 当你用一个独立随机变量替换真实的治疗变量时，估计的因果效应会发生什么变化？ | 效应应该趋近于零 |'
- en: '| Simulated outcome | What happens to the estimated causal effect when you
    replace the dataset with a simulated dataset based on a known data-generating
    process closest to the given dataset? | It should match the effect parameter from
    the data-generating process |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 模拟结果 | 当你用基于已知数据生成过程的模拟数据集替换原始数据集时，估计的因果效应会发生什么变化？ | 它应该与数据生成过程中的效应参数匹配 |'
- en: '| Unobserved common causes | How sensitive is the effect estimate when you
    add an additional common cause (often called a *confounder*) to the dataset that
    is correlated with the treatment and the outcome? | It should not be too sensitive
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 未观察到的共同原因 | 当你向数据集中添加一个额外的与治疗和结果相关的共同原因（通常称为*混杂变量*）时，估计的效应有多敏感？ | 不应该太敏感
    |'
- en: '| Data subsets validation | Does the estimated effect change significantly
    when you replace the given dataset with a randomly selected subset? | It should
    not |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 数据子集验证 | 当你用随机选取的子集替换原始数据集时，估计的效应会发生显著变化吗？ | 不应该发生 |'
- en: '| Bootstrap validation | Does the estimated effect change significantly when
    you replace the given dataset with bootstrap resamples from the same dataset?
    | It should not |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 自举验证 | 当你用来自相同数据集的自举重采样替换原始数据集时，估计的效应会发生显著变化吗？ | 不应该发生 |'
- en: Tools for causal inference
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因果推断工具
- en: '*Structural causal models* (SCM) are a mainstay of causal inference. ML methods
    in causal inference are based on representations of ML models as SCMs, with the
    help of cause-effect reasoning and domain knowledge.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*结构因果模型*（SCM）是因果推断的一个重要工具。机器学习中的因果推断方法基于将机器学习模型表示为SCM，借助因果推理和领域知识。'
- en: 'A *structural causal model* is defined as the 4-tuple *(D, E, f, P[e])*, where:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*结构因果模型*被定义为4元组*(D, E, f, P[e])*，其中：'
- en: '*D* is a set of endogenous variables, variables that can be influenced by changing
    the values of one or more of the other variables.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D* 是一组内生变量，可以通过改变其他变量的值来影响它们。'
- en: '*E* is a set of exogenous variables, the values of which are not possible to
    manipulate by changing other variables.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E* 是一组外生变量，其值不可能通过改变其他变量来操控。'
- en: '*f = { f[1], f[2], …​, f[n]}* is a set of functions that represent causal mechanisms
    involving members of *D* and *E*: <math alttext="d Subscript i Baseline equals
    f Subscript i Baseline left-parenthesis upper P a left-parenthesis d Subscript
    i Baseline right-parenthesis comma upper E Subscript i Baseline right-parenthesis"><mrow><msub><mi>d</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>f</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>P</mi> <mi>a</mi> <mrow><mo>(</mo> <msub><mi>d</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>,</mo> <msub><mi>E</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
    . Here the endogenous variable *d[i]* is modeled as a function *f[i]* of other
    endogenous variables *Pa(d[i])* and one or more exogenous variables <math alttext="upper
    E Subscript i Baseline subset-of-or-equal-to upper E"><mrow><msub><mi>E</mi> <mi>i</mi></msub>
    <mo>⊆</mo> <mi>E</mi></mrow></math> .'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f = { f[1], f[2], …​, f[n]}* 是一组函数，代表涉及*D*和*E*成员的因果机制：<math alttext="d Subscript
    i Baseline equals f Subscript i Baseline left-parenthesis upper P a left-parenthesis
    d Subscript i Baseline right-parenthesis comma upper E Subscript i Baseline right-parenthesis"><mrow><msub><mi>d</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>f</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>P</mi> <mi>a</mi> <mrow><mo>(</mo> <msub><mi>d</mi> <mi>i</mi></msub> <mo>)</mo></mrow>
    <mo>,</mo> <msub><mi>E</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
    。这里的内生变量*d[i]*被建模为其他内生变量*Pa(d[i])*和一个或多个外生变量<math alttext="upper E Subscript i
    Baseline subset-of-or-equal-to upper E"><mrow><msub><mi>E</mi> <mi>i</mi></msub>
    <mo>⊆</mo> <mi>E</mi></mrow></math> 的函数*f[i]*。'
- en: '*P[e]* is a probability distribution over the elements of *E*.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P[e]* 是*E*元素的概率分布。'
- en: Think of an SCM as a formal way of approaching the steps of causal inference.
    For the first step of creating a causal model, you can take the SCM with mathematical
    specifications for the functions in *f* as a formal model of the causality problem
    you are dealing with. The second step of identifying a target estimand corresponds
    to *estimating* a causal mechanism *f[i]* within a family of functions. Step 3—​determining
    the strength of causal effect—​is analogous to testing for the effect size of
    an *f[i]* or its parameters. Finally, you can encode the fourth step of subjecting
    the causal model to refutations as testing out an alternate formulations of the
    mechanisms *f* within an SCM.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 将SCM视为正式解决因果推断步骤的一种方式。对于创建因果模型的第一步，你可以将带有*f*函数数学规范的SCM作为你正在处理的因果问题的正式模型。识别目标估计量的第二步对应于在一组函数中估计因果机制*f[i]*。第三步——确定因果效应的强度——类似于测试*f[i]*或其参数的效应大小。最后，你可以将因果模型进行反驳的第四步视为测试SCM中*f*机制的替代形式。
- en: 'Grounding these steps in the earlier example, an SCM based on the variables
    involved would look like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些步骤与之前的例子联系起来，基于涉及的变量，一个基于SCM的模型如下所示：
- en: '*D* consists of the following features: clicked or not (*C*), user segment
    (*S*), and user history (*H*).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D* 包含以下特征：点击或未点击（*C*），用户细分（*S*），以及用户历史（*H*）。'
- en: '*E* consists of the following features: ad topic (*A*), and time of day (*T*).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E* 包含以下特征：广告主题（*A*），以及一天中的时间（*T*）。'
- en: '*f* consists of these functions:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f* 包括这些函数：'
- en: <math alttext="upper C equals f 1 left-parenthesis upper S comma upper H comma
    upper A comma upper T right-parenthesis" display="block"><mrow><mi>C</mi> <mo>=</mo>
    <msub><mi>f</mi> <mn>1</mn></msub> <mrow><mo>(</mo> <mi>S</mi> <mo>,</mo> <mi>H</mi>
    <mo>,</mo> <mi>A</mi> <mo>,</mo> <mi>T</mi> <mo>)</mo></mrow></mrow></math><math
    alttext="upper S equals f 2 left-parenthesis upper H comma upper A comma upper
    T right-parenthesis" display="block"><mrow><mi>S</mi> <mo>=</mo> <msub><mi>f</mi>
    <mn>2</mn></msub> <mrow><mo>(</mo> <mi>H</mi> <mo>,</mo> <mi>A</mi> <mo>,</mo>
    <mi>T</mi> <mo>)</mo></mrow></mrow></math><math alttext="upper H equals f 3 left-parenthesis
    upper S comma upper A comma upper T right-parenthesis" display="block"><mrow><mi>H</mi>
    <mo>=</mo> <msub><mi>f</mi> <mn>3</mn></msub> <mrow><mo>(</mo> <mi>S</mi> <mo>,</mo>
    <mi>A</mi> <mo>,</mo> <mi>T</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <math alttext="upper C equals f 1 left-parenthesis upper S comma upper H comma
    upper A comma upper T right-parenthesis" display="block"><mrow><mi>C</mi> <mo>=</mo>
    <msub><mi>f</mi> <mn>1</mn></msub> <mrow><mo>(</mo> <mi>S</mi> <mo>,</mo> <mi>H</mi>
    <mo>,</mo> <mi>A</mi> <mo>,</mo> <mi>T</mi> <mo>)</mo></mrow></mrow></math><math
    alttext="upper S equals f 2 left-parenthesis upper H comma upper A comma upper
    T right-parenthesis" display="block"><mrow><mi>S</mi> <mo>=</mo> <msub><mi>f</mi>
    <mn>2</mn></msub> <mrow><mo>(</mo> <mi>H</mi> <mo>,</mo> <mi>A</mi> <mo>,</mo>
    <mi>T</mi> <mo>)</mo></mrow></mrow></math><math alttext="upper H equals f 3 left-parenthesis
    upper S comma upper A comma upper T right-parenthesis" display="block"><mrow><mi>H</mi>
    <mo>=</mo> <msub><mi>f</mi> <mn>3</mn></msub> <mrow><mo>(</mo> <mi>S</mi> <mo>,</mo>
    <mi>A</mi> <mo>,</mo> <mi>T</mi> <mo>)</mo></mrow></mrow></math>
- en: Finally, for *P[e]* assume that the distributions of ad topics is inferred from
    historical data and time can be uniformly distributed across the day.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，对于*P[e]*，假设广告主题的分布是根据历史数据推断出来的，而时间可以在一天中均匀分布。
- en: A causal model would estimate the function *f[1]* and determine the strength
    of the causal effects while accounting for the confounding effects codified by
    *f[2]* and *f[3]*. Finally, to know whether this causal model actually works in
    practice, you can run an A/B test by randomly selecting two groups of users and
    picking users from each group to serve the same ads on the same time of the day.
    For the first group, the users predicted by the causal model as highly likely
    to click on the ad are picked, while for the second group, random users are picked.
    If the average percentage of clicks generated is significantly higher for the
    first group, you know that the causal model makes sense (i.e., is better than
    a random guess).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个因果模型将估计函数*f[1]*并确定因果效应的强度，同时考虑到*f[2]*和*f[3]*编码的混杂效应。最后，要知道这个因果模型在实践中是否有效，你可以通过随机选择两组用户并从每组用户中选择同一广告在同一时间内服务的用户来运行A/B测试。对于第一组，选取因果模型预测高点击广告的用户，而对于第二组，则随机选取用户。如果第一组生成的平均点击百分比显著高于第二组，你就知道因果模型是合理的（即比随机猜测更好）。
- en: Tip
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Can you think of a statistical test to use for testing if the difference between
    the click percentages in the two user groups is significant?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想到一个统计测试用来检验两个用户组之间点击百分比的差异是否显著吗？
- en: Causal inference spans a broad range of techniques, from ML-based to non-ML
    statistical inference. There’s been an explosion of new tools and techniques in
    the space, far more than what we can cover in this chapter. Many of the large
    ML conferences have designated [special tracks and workshops](https://oreil.ly/Jeokv)
    on the subject.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因果推断涵盖了从基于机器学习到非机器学习统计推断的广泛技术范围。在这个领域出现了大量新工具和技术，远远超出了我们可以在本章节中覆盖的范围。许多大型机器学习会议都专门设有[特别的赛道和研讨会](https://oreil.ly/Jeokv)。
- en: 'When considering causal inference tools, look for well-maintained tools with
    wide coverage of techniques. The four best options we recommend are:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑因果推断工具时，请寻找覆盖技术范围广泛的维护良好的工具。我们推荐的四个最佳选项是：
- en: '*[CMU’s Causal-learn](https://oreil.ly/bTIV7)*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*[CMU的Causal-learn](https://oreil.ly/bTIV7)*'
- en: For ML-based techniques, this is perhaps the best package to make sure your
    bases are covered when it comes to time-tested statistical causal inference. Causal-learn
    is a Python translation and extension of the Java-based [Tetrad](https://oreil.ly/avaIx)
    and offers [causal search methods](https://oreil.ly/FdVlA) (searching through
    a causal graph and nominating causal variables), [conditional independence tests](https://oreil.ly/OnNFS)
    (testing whether two variables are independent given a set of conditioning variables),
    and [scoring functions](https://oreil.ly/vRLWc), which are useful in building
    Bayesian models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于机器学习的技术来说，这可能是确保您在经受时间考验的统计因果推断方面的最佳选择。Causal-learn是Java-based [Tetrad](https://oreil.ly/avaIx)的Python翻译和扩展，提供了[因果搜索方法](https://oreil.ly/FdVlA)（通过因果图搜索和提名因果变量），[条件独立性测试](https://oreil.ly/OnNFS)（测试给定一组条件变量时两个变量是否独立），以及[评分函数](https://oreil.ly/vRLWc)，这些在构建贝叶斯模型中非常有用。
- en: '*[QuantumBlack’s CausalNex](https://oreil.ly/wCTBd)*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*[QuantumBlack的CausalNex](https://oreil.ly/wCTBd)*'
- en: CausalNex dives deeper into neural networks than does Causal-learn. Specifically,
    CausalNex heavily leverages Bayesian networks, and it aims to encode domain knowledge
    in graph models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: CausalNex比Causal-learn更深入地探讨了神经网络。具体来说，CausalNex大量利用贝叶斯网络，并旨在在图模型中编码领域知识。
- en: '*[Uber’s CausalML](https://oreil.ly/LzSf6)*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*[Uber的CausalML](https://oreil.ly/LzSf6)*'
- en: Like CausalNex, Uber’s CausalML emphasizes the ML algorithms for causal inference.
    However, it offers a wider variety of algorithms, including tree-based algorithms
    (such as [Uplift trees based on KL divergence](https://oreil.ly/GMXzR)), meta-learner
    algorithms (including [S-learner and T-learner](https://oreil.ly/bUxG0), doubly
    robust learners), instrumental variables algorithms (such as two-stage least squares),
    and TensorFlow-based neural network algorithms (including [CEVAE](https://arxiv.org/abs/1705.08821)
    and [DragonNet](https://arxiv.org/abs/1906.02120)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于CausalNex，Uber的CausalML专注于因果推断的机器学习算法。然而，它提供了更广泛的算法选择，包括基于树的算法（例如基于KL散度的提升树算法[Uplift
    trees based on KL divergence](https://oreil.ly/GMXzR)），元学习算法（包括[S-learner和T-learner](https://oreil.ly/bUxG0)，双重稳健学习者），工具变量算法（例如两阶段最小二乘法），以及基于TensorFlow的神经网络算法（包括[CEVAE](https://arxiv.org/abs/1705.08821)和[DragonNet](https://arxiv.org/abs/1906.02120)）。
- en: '*[doWhy](https://oreil.ly/Jm9s1)*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*[DoWhy](https://oreil.ly/Jm9s1)*'
- en: Similar to CausalML, DoWhy (named for Judea Pearl’s [“do-calculus”](https://arxiv.org/abs/1305.5506))
    is an open source library (originally maintained by Microsoft) that covers multiple
    algorithms for causal inference, based on both statistical and ML methods. DoWhy
    has a few features that make it particularly useful. First, it is extensible with
    some of Microsoft’s other causal ML libraries such as EconML and CausalML (not
    to be confused with the Uber CausalML library discussed previously). It also has
    a built-in high-level Pandas API. This is helpful, since most causal inference
    methods are geared toward tabular and time series data. The Pandas API also allows
    you to easily create mock datasets for testing. DoWhy is also much stronger than
    CausalML in providing automatic refutation tools.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于CausalML，DoWhy（以Judea Pearl的[“do-calculus”](https://arxiv.org/abs/1305.5506)命名）是一个开源库（最初由微软维护），涵盖了基于统计和机器学习方法的多种因果推断算法。DoWhy具有一些特性使其特别有用。首先，它可以与微软的其他因果机器学习库（如EconML和CausalML，不要与之前讨论的Uber
    CausalML库混淆）相扩展。它还具有内置的高级Pandas API。这非常有帮助，因为大多数因果推断方法都针对表格和时间序列数据。Pandas API还允许您轻松创建用于测试的模拟数据集。DoWhy在提供自动反驳工具方面也比CausalML强大得多。
- en: Let’s look at a small piece of code to understand how DoWhy helps encode a causal
    structure into ML workflows.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一段小代码片段，了解DoWhy如何帮助将因果结构编码到机器学习工作流中。
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: While these packages are useful, causal ML still follows the [garbage in, garbage
    out](https://oreil.ly/2ItFq) principle. Your ability to draw conclusions about
    causality will depend on the quality of your dataset and on how well you follow
    the process of creating the hypothesis graph, testing it, and trying to refute
    it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些包很有用，因果机器学习仍然遵循“[垃圾进，垃圾出](https://oreil.ly/2ItFq)”的原则。你对因果性的结论能力将取决于数据集的质量以及你如何跟随创建假设图、测试和试图反驳的过程。
- en: Causality and trust
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因果性与信任
- en: Using an SCM, it is possible to embed domain knowledge into causal models that
    are inherently explainable (that is global explanations, see [Chapter 3](ch03.html#chapter3))
    using regularization and to produce post hoc explanations. For local explanations,
    counterfactuals are useful in evaluating model outputs under alternate *what-if*
    scenarios, for instance, by supplying the model input examples with the values
    of some input features changed, then observing the outputs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SCM，可以将领域知识嵌入到因果模型中，这些模型本质上是可解释的（即全局解释，参见[第3章](ch03.html#chapter3)），使用正则化并生成事后解释。对于局部解释，反事实在评估模型输出在替代*假设*场景下非常有用，例如通过提供模型输入示例并更改某些输入特征的值，然后观察输出。
- en: '*Counterfactual* explanations differ from non-causal feature attribution methods.
    Non-causal methods are based on changing *only* the values of the input features
    being evaluated. By contrast, counterfactual methods observe the model outputs
    at input points with changed values for the evaluated features *and* for other
    input features affected by the evaluated features, based on the underlying causal
    model.^([2](ch07.html#idm45621831819760)) The concept of counterfactuals may be
    applied to fairness and robustness too.^([3](ch07.html#idm45621831818272)) In
    general, evaluating synthetic counterfactual samples free from the confounding
    effects of real observational data allows for a more precise evaluation of trust
    metrics.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*反事实* 解释与非因果特征归因方法不同。非因果方法基于仅改变被评估输入特征的值。相比之下，反事实方法观察模型输出，输入点的值改变为评估特征以及受评估特征影响的其他输入特征，基于潜在的因果模型。^([2](ch07.html#idm45621831819760))
    反事实的概念也可应用于公平性和鲁棒性。^([3](ch07.html#idm45621831818272)) 总的来说，评估不受真实观测数据混淆效应的合成反事实样本，允许更精确地评估信任度指标。'
- en: Sparsity and Model Compression
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏性与模型压缩
- en: Deploying large-scale ML models in industry applications is costly, since training
    them requires a lot of computing power and memory. Resource constraints become
    even more acute when it is time to deploy models into environments such as mobile
    phones. Generally, trained deep-learning model objects—and even random forest,
    or [XGBoost](https://oreil.ly/sbhXW)—contain numerous parameters to aid in the
    highly granular decision process. To train objects for on-the-edge ML, you need
    to compress the model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在工业应用中部署大规模机器学习模型成本高昂，因为训练它们需要大量的计算能力和内存。当部署模型到移动电话等环境时，资源约束变得更加严峻。通常情况下，训练好的深度学习模型对象，甚至随机森林或[XGBoost](https://oreil.ly/sbhXW)，包含大量参数，以帮助高度细粒度的决策过程。为了在边缘机器学习中训练对象，你需要压缩模型。
- en: 'By default, the training process of conventional neural networks (NN) and deep-learning
    models is *dense*: it sets the weights and biases of all nodes to non-zero values.
    You can probably guess that not all nodes contribute equally to model performance.
    Nodes with weights very close to zero contribute very little, so if you set those
    weights to exactly zero, there will be little to no impact on performance. This
    is what *sparse neural networks* do: a number of their weights are hard-coded
    to zero. Sparse neural networks not only help in model compression but also go
    a long way to improve generalization by preventing overfitting.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，传统神经网络（NN）和深度学习模型的训练过程是*密集*的：它设置所有节点的权重和偏差为非零值。你可以猜测，不是所有节点对模型性能贡献相同。权重接近零的节点贡献非常少，因此如果将这些权重设置为零，对性能几乎没有影响。这就是*稀疏神经网络*所做的事情：它们的一些权重硬编码为零。稀疏神经网络不仅有助于模型压缩，还通过防止过拟合大大改善了泛化能力。
- en: Pruning
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精简
- en: A simple way to sparsify trained neural networks is to just drop low-magnitude
    weights. [Frankle and Carbin](https://oreil.ly/EikoA) popularized this approach.
    They compared finding just the right set of parameters to fit a neural network
    to the data involved with playing the lottery. Training a dense NN is like buying
    a lot of tickets to increase your odds of winning. But what if there was a way
    to figure out which lottery tickets are more likely to win? Then, you could spend
    less money while still guaranteeing a high amount of winnings. Similarly, if you
    could isolate the most important weights and biases behind the performance of
    a trained dense NN, you’d be able to set the rest of them to zero while still
    maintaining good performance.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使神经网络训练结果变得稀疏的一个简单方法是仅丢弃低幅度权重。[Frankle and Carbin](https://oreil.ly/EikoA) 提倡了这种方法。他们将找到适合神经网络数据的参数集与参与彩票游戏的情况做了比较。训练密集神经网络就像购买大量彩票以增加中奖概率。但如果有办法找出哪些彩票更有可能中奖，你可以花更少的钱，同时保证高额奖金。类似地，如果你能够分离出训练后密集神经网络性能背后最重要的权重和偏差，你就能将其余的设为零，同时仍然保持良好的性能。
- en: The systematic process of setting some parameters to zero in a dense NN is called
    *pruning*. As you prune, consider some trade-offs. For instance, you might need
    to balance the amount of pruning you do with performance metrics such as accuracy,
    optimal strategies for specific datasets or data types, and high-level design
    choices such as hardware and software architecture.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在密集神经网络中将一些参数设为零的系统化过程称为 *修剪（pruning）*。在修剪时，需要考虑一些权衡。例如，你可能需要在修剪量和性能指标（如准确性）、特定数据集或数据类型的最佳策略，以及硬件和软件架构等高级设计选择之间进行平衡。
- en: 'There are three steps to obtaining sparse NNs: training, pruning, and fine-tuning.
    When the training process has converged for a NN model, its empirical risk—that
    is, the average loss over training data—is minimal. If you prune the set of weights
    *W* for this NN by setting some weights to zero, this *will* degrade your model’s
    performance on the training data. As a result, you’ll need to retrain your model.
    This is called *fine-tuning*. Generally, fine-tuning is performed for a predefined
    number of iterations.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 获得稀疏神经网络的三个步骤包括：训练、修剪和微调。当神经网络模型的训练过程收敛时，其经验风险——即对训练数据的平均损失——是最小的。如果你通过将一些权重设为零来修剪这个NN的权重集合
    *W*，这将会降低模型在训练数据上的性能。因此，你需要重新训练你的模型。这就是所谓的 *微调*。通常，微调是在预定义的迭代次数内进行的。
- en: 'Given training data *X* and a family of NNs defined as <math alttext="f left-parenthesis
    x comma dot right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mo>·</mo> <mo>)</mo></mrow></math> , with the function *f* parametrized by a
    weight matrix *W*, the generic process of obtaining a sparse NN through pruning
    looks like the following algorithm:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 给定训练数据 *X* 和作为 <math alttext="f left-parenthesis x comma dot right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mo>·</mo> <mo>)</mo></mrow></math> 定义的一组神经网络，其中函数
    *f* 的参数是权重矩阵 *W*，通过修剪获取稀疏神经网络的一般过程看起来像以下算法：
- en: Inputs
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: 'Feature matrix: <math alttext="upper X element-of double-struck upper R Superscript
    n times p"><mrow><mi>X</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mi>n</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow></math>'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 特征矩阵：<math alttext="upper X element-of double-struck upper R Superscript n times
    p"><mrow><mi>X</mi> <mo>∈</mo> <msup><mi>ℝ</mi> <mrow><mi>n</mi><mo>×</mo><mi>p</mi></mrow></msup></mrow></math>
- en: 'Number of iterations: *N*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数：*N*
- en: Steps
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤
- en: <math alttext="upper W left-arrow i n i t i a l i z e left-parenthesis right-parenthesis"><mrow><mi>W</mi>
    <mo>←</mo> <mi>i</mi> <mi>n</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi> <mi>a</mi> <mi>l</mi>
    <mi>i</mi> <mi>z</mi> <mi>e</mi> <mo>(</mo> <mo>)</mo></mrow></math>
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <math alttext="upper W left-arrow i n i t i a l i z e left-parenthesis right-parenthesis"><mrow><mi>W</mi>
    <mo>←</mo> <mi>i</mi> <mi>n</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi> <mi>a</mi> <mi>l</mi>
    <mi>i</mi> <mi>z</mi> <mi>e</mi> <mo>(</mo> <mo>)</mo></mrow></math>
- en: <math alttext="upper W left-arrow t r a i n upper T o upper C o n v e r g e
    n c e left-parenthesis f left-parenthesis upper X semicolon upper W right-parenthesis
    right-parenthesis"><mrow><mi>W</mi> <mo>←</mo> <mi>t</mi> <mi>r</mi> <mi>a</mi>
    <mi>i</mi> <mi>n</mi> <mi>T</mi> <mi>o</mi> <mi>C</mi> <mi>o</mi> <mi>n</mi> <mi>v</mi>
    <mi>e</mi> <mi>r</mi> <mi>g</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <mi>e</mi> <mo>(</mo>
    <mi>f</mi> <mo>(</mo> <mi>X</mi> <mo>;</mo> <mi>W</mi> <mo>)</mo> <mo>)</mo></mrow></math>
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <math alttext="upper W left-arrow t r a i n upper T o upper C o n v e r g e
    n c e left-parenthesis f left-parenthesis upper X semicolon upper W right-parenthesis
    right-parenthesis"><mrow><mi>W</mi> <mo>←</mo> <mi>t</mi> <mi>r</mi> <mi>a</mi>
    <mi>i</mi> <mi>n</mi> <mi>T</mi> <mi>o</mi> <mi>C</mi> <mi>o</mi> <mi>n</mi> <mi>v</mi>
    <mi>e</mi> <mi>r</mi> <mi>g</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <mi>e</mi> <mo>(</mo>
    <mi>f</mi> <mo>(</mo> <mi>X</mi> <mo>;</mo> <mi>W</mi> <mo>)</mo> <mo>)</mo></mrow></math>
- en: <math alttext="upper M left-arrow o n e s left-parenthesis n comma p right-parenthesis"><mrow><mi>M</mi>
    <mo>←</mo> <mi>o</mi> <mi>n</mi> <mi>e</mi> <mi>s</mi> <mo>(</mo> <mi>n</mi> <mo>,</mo>
    <mi>p</mi> <mo>)</mo></mrow></math>
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <math alttext="upper M left-arrow o n e s left-parenthesis n comma p right-parenthesis"><mrow><mi>M</mi>
    <mo>←</mo> <mi>o</mi> <mi>n</mi> <mi>e</mi> <mi>s</mi> <mo>(</mo> <mi>n</mi> <mo>,</mo>
    <mi>p</mi> <mo>)</mo></mrow></math>
- en: 'for *i* in 1 : *N* do'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i* 在 1 到 *N* 之间执行
- en: <math alttext="upper M left-arrow p r u n e left-parenthesis upper M comma s
    c o r e left-parenthesis upper W right-parenthesis right-parenthesis"><mrow><mi>M</mi>
    <mo>←</mo> <mi>p</mi> <mi>r</mi> <mi>u</mi> <mi>n</mi> <mi>e</mi> <mo>(</mo> <mi>M</mi>
    <mo>,</mo> <mi>s</mi> <mi>c</mi> <mi>o</mi> <mi>r</mi> <mi>e</mi> <mo>(</mo> <mi>W</mi>
    <mo>)</mo> <mo>)</mo></mrow></math>
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <math alttext="upper M left-arrow p r u n e left-parenthesis upper M comma s
    c o r e left-parenthesis upper W right-parenthesis right-parenthesis"><mrow><mi>M</mi>
    <mo>←</mo> <mi>p</mi> <mi>r</mi> <mi>u</mi> <mi>n</mi> <mi>e</mi> <mo>(</mo> <mi>M</mi>
    <mo>,</mo> <mi>s</mi> <mi>c</mi> <mi>o</mi> <mi>r</mi> <mi>e</mi> <mo>(</mo> <mi>W</mi>
    <mo>)</mo> <mo>)</mo></mrow></math>
- en: <math alttext="upper W left-arrow f i n e upper T u n e left-parenthesis f left-parenthesis
    upper X semicolon upper M circled-dot upper W right-parenthesis right-parenthesis"><mrow><mi>W</mi>
    <mo>←</mo> <mi>f</mi> <mi>i</mi> <mi>n</mi> <mi>e</mi> <mi>T</mi> <mi>u</mi> <mi>n</mi>
    <mi>e</mi> <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>X</mi> <mo>;</mo> <mi>M</mi> <mo>⊙</mo>
    <mi>W</mi> <mo>)</mo> <mo>)</mo></mrow></math>
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <math alttext="upper W left-arrow f i n e upper T u n e left-parenthesis f left-parenthesis
    upper X semicolon upper M circled-dot upper W right-parenthesis right-parenthesis"><mrow><mi>W</mi>
    <mo>←</mo> <mi>f</mi> <mi>i</mi> <mi>n</mi> <mi>e</mi> <mi>T</mi> <mi>u</mi> <mi>n</mi>
    <mi>e</mi> <mo>(</mo> <mi>f</mi> <mo>(</mo> <mi>X</mi> <mo>;</mo> <mi>M</mi> <mo>⊙</mo>
    <mi>W</mi> <mo>)</mo> <mo>)</mo></mrow></math>
- en: return *M, W*
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 *M, W*
- en: The pruning in step 5 applies a *score function* to each element of *W*, based
    on which it sets some elements of the mask *M* to zero. Think of the score function
    as a threshold. It could be as simple as absolute value, or as complex as how
    much an element of *W* contributes to the activation function of a layer. You
    can find pruning methods in the literature that deal with the details in the preceding
    algorithm. This includes designing novel score functions, fine-tuning methods,
    scheduling the pruning iterations, or structuring the pruning process to prune
    weights individually, by group, or other logics.^([4](ch07.html#idm45621831716192))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步中的修剪对 *W* 的每个元素应用一个 *得分函数（score function）*，基于这个函数将 *M* 的一些元素设为零。可以将得分函数看作一个阈值。它可以简单到绝对值，也可以复杂到考虑到
    *W* 的元素对层激活函数的贡献程度。文献中可以找到处理上述算法细节的修剪方法。这包括设计新颖的得分函数、微调方法、调度修剪迭代或结构化修剪过程，以便通过个别权重、按组或其他逻辑修剪权重。^([4](ch07.html#idm45621831716192))
- en: Sparse training
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稀疏训练
- en: Pruning-based methods are somewhat ad hoc in nature. There are many techniques
    available for scoring, fine-tuning, and pruning. What combination of these techniques
    gives the best results will depend on your task and dataset. Compared to post-processing
    an already trained NN, sparse *training* methods provide more general performance
    guarantees on what algorithm works for which class of tasks—​in theory, at least.
    Robert Tibshirani proposed the first ever method for sparse training, called *least
    absolute shrinkage and selection operator* (LASSO),^([5](ch07.html#idm45621831705184))
    designed to work for linear regression. Since then the theory of sparse penalized
    models has become quite well-established.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基于修剪的方法在某种程度上是临时性的。有许多技术可用于评分、微调和修剪。这些技术的组合将取决于您的任务和数据集，哪种算法对哪类任务的效果最佳。与后处理已训练的神经网络相比，稀疏*训练*方法在理论上为哪种算法适用于哪类任务提供了更广泛的性能保证。罗伯特·提布什拉尼最早提出了用于稀疏训练的方法，称为*最小绝对值收缩和选择算子*（LASSO）^([5](ch07.html#idm45621831705184))，设计用于线性回归。自那时起，稀疏惩罚模型的理论已经相当成熟。
- en: 'Sparse training involves optimization of a penalized risk function. With the
    notation in this section, the set of weights produced by a sparse training process
    can be written as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏训练涉及优化一个惩罚风险函数。在本节的符号表示中，由稀疏训练过程产生的权重集可以写成如下形式：
- en: <math><mrow><mover accent="true"><mi>W</mi> <mo>^</mo></mover> <mo>=</mo> <msub><mtext>argmax</mtext>
    <mrow><mi>W</mi><mo>∈</mo><mi>𝒲</mi></mrow></msub> <mfenced close="}" open="{"
    separators=""><mi>L</mi> <mo>(</mo> <mi>Y</mi> <mo>,</mo> <mi>f</mi> <mo>(</mo>
    <mi>X</mi> <mo>;</mo> <mi>W</mi> <mo>)</mo> <mo>)</mo> <mo>+</mo> <mi>λ</mi> <mi>P</mi>
    <mo>(</mo> <mi>W</mi> <mo>)</mo></mfenced></mrow></math>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mover accent="true"><mi>W</mi> <mo>^</mo></mover> <mo>=</mo> <msub><mtext>argmax</mtext>
    <mrow><mi>W</mi><mo>∈</mo><mi>𝒲</mi></mrow></msub> <mfenced close="}" open="{"
    separators=""><mi>L</mi> <mo>(</mo> <mi>Y</mi> <mo>,</mo> <mi>f</mi> <mo>(</mo>
    <mi>X</mi> <mo>;</mo> <mi>W</mi> <mo>)</mo> <mo>)</mo> <mo>+</mo> <mi>λ</mi> <mi>P</mi>
    <mo>(</mo> <mi>W</mi> <mo>)</mo></mfenced></mrow></math>
- en: Here *Y* is the set of output features, <math alttext="script upper W"><mi>𝒲</mi></math>
    is the set of all possible *W* matrices over which the optimization is run, <math
    alttext="upper L left-parenthesis dot comma dot right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mo>·</mo> <mo>,</mo> <mo>·</mo> <mo>)</mo></mrow></math> is the loss
    function, and <math alttext="upper P left-parenthesis dot right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mo>·</mo> <mo>)</mo></mrow></math> is a *penalty function*. Taking
    the penalty function as the *L[1]* norm, that is <math alttext="upper P left-parenthesis
    upper W right-parenthesis equals parallel-to upper W parallel-to"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>W</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mrow><mo>∥</mo><mi>W</mi><mo>∥</mo></mrow>
    <mn>1</mn></msub></mrow></math> , imposes sparsity on the the values of calculated
    weights in the solution <math alttext="ModifyingAbove upper W With caret"><mover
    accent="true"><mi>W</mi> <mo>^</mo></mover></math> . The tuning parameter <math
    alttext="lamda"><mi>λ</mi></math> controls the upper bound above which a value
    of <math alttext="ModifyingAbove upper W With caret"><mover accent="true"><mi>W</mi>
    <mo>^</mo></mover></math> will be set to 0. There are many ways of selecting the
    optimal <math alttext="lamda"><mi>λ</mi></math> , such as cross-validation and
    [information criteria](https://oreil.ly/X2uCX).^([6](ch07.html#idm45621831663760))
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Here *Y* 是输出特征的集合，<math alttext="script upper W"><mi>𝒲</mi></math> 是所有可能的 *W*
    矩阵的集合，优化运行在这些矩阵上，<math alttext="upper L left-parenthesis dot comma dot right-parenthesis"><mrow><mi>L</mi>
    <mo>(</mo> <mo>·</mo> <mo>,</mo> <mo>·</mo> <mo>)</mo></mrow></math> 是损失函数，而 <math
    alttext="upper P left-parenthesis dot right-parenthesis"><mrow><mi>P</mi> <mo>(</mo>
    <mo>·</mo> <mo>)</mo></mrow></math> 是一个*惩罚函数*。将惩罚函数定义为 *L[1]* 范数，即 <math alttext="upper
    P left-parenthesis upper W right-parenthesis equals parallel-to upper W parallel-to"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>W</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mrow><mo>∥</mo><mi>W</mi><mo>∥</mo></mrow>
    <mn>1</mn></msub></mrow></math> ，对于解 <math alttext="ModifyingAbove upper W With
    caret"><mover accent="true"><mi>W</mi> <mo>^</mo></mover></math> 的权重值施加稀疏性。调节参数
    <math alttext="lamda"><mi>λ</mi></math> 控制了 <math alttext="ModifyingAbove upper
    W With caret"><mover accent="true"><mi>W</mi> <mo>^</mo></mover></math> 值超过该上限时被设为0的上界。选择最优
    <math alttext="lamda"><mi>λ</mi></math> 的方法有很多，比如交叉验证和[信息准则](https://oreil.ly/X2uCX)^([6](ch07.html#idm45621831663760))。
- en: Although promising, sparse training methods are very computationally intensive
    when applied to models more complex than logistic regression. Further, modern
    deep learning software and hardware are optimized for *dense* matrix computations,
    so pruning is much easier to implement. A few recent papers are starting to propose
    realistic and scalable sparse NN training procedures.^([7](ch07.html#idm45621831656864))
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有希望，但将稀疏训练方法应用于比逻辑回归更复杂的模型时，计算密集度非常高。此外，现代深度学习软件和硬件都针对*密集*矩阵计算进行了优化，因此修剪（pruning）要容易得多。最近的几篇论文开始提出现实和可扩展的稀疏神经网络训练过程。^([7](ch07.html#idm45621831656864))
- en: Note
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The discussion in this section applies mainly to neural network–based models.
    Other techniques, like those based on SVMs or decision trees, can also be subject
    to their own sparsity-inducing training methods.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论主要适用于基于神经网络的模型。其他技术，如基于支持向量机（SVM）或决策树的技术，也可以采用它们自己的稀疏诱导训练方法。
- en: Trust elements in sparse models
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稀疏模型中的信任元素
- en: One advantage of sparse models is that they’re somewhat easier to interpret.
    A sparse NN has far fewer latent variables to keep track of than a dense NN. You
    can simply ignore some weights when interpreting many of the internals in a pruned
    model (about 90% sparsity or more), since they lead to dead ends. This can greatly
    reduce the amount of information you’d need to process to interpret the prediction
    from a sparse NN model. Still, for very large models, like those for image segmentation
    or natural language processing, working with fewer weights might not be enough
    to lighten the burden.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏模型的一个优势是它们更容易解释。稀疏神经网络比密集神经网络少得多的潜变量需要跟踪。在解释修剪模型（约90%或更多的稀疏度）中的许多内部时，您可以简单地忽略一些权重，因为它们会导致死胡同。这可以大大减少您需要处理的信息量，以解释稀疏神经网络模型的预测。然而，对于像图像分割或自然语言处理的非常大的模型来说，使用更少的权重可能不足以减轻负担。
- en: Even though sparse models seem to improve generalization, they also tend to
    *forget* some information. For example, Hooker et al. showed that even though
    NNs can be pruned to high sparsity with little impact to *top-line metrics* such
    as top 1% or 5% accuracy, this comes at the cost of performance degradation in
    a small subset of samples, which they call *compression identified exemplars*
    (CIE).^([8](ch07.html#idm45621831645600)) In a later paper,^([9](ch07.html#idm45621831643664))
    the same team showed that CIEs are, in fact, more likely to contain underrepresented
    attribute values than non-CIEs, so they can exacerbate the fairness concerns in
    the original model. In general, CIEs are more likely to have a high influence
    on the training process. Pruned models are also highly sensitive to noise and
    corruption. Thus, it’s possible that pruning has robustness and privacy implications.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管稀疏模型似乎提高了泛化能力，但它们也倾向于*忘记*某些信息。例如，Hooker等人表明，尽管神经网络可以被修剪到高稀疏度而对*顶线指标*（如1%或5%的准确率）几乎没有影响，但这是以在少数样本中性能下降为代价的，他们称之为*压缩识别示例*（CIE）。^([8](ch07.html#idm45621831645600))
    在后续的一篇论文中，^([9](ch07.html#idm45621831643664)) 同一团队表明，CIE实际上更可能包含比非CIE更少代表的属性值，因此它们可能加剧原始模型中的公平性问题。总的来说，CIE更可能对训练过程产生较高的影响。修剪模型还对噪声和损坏非常敏感。因此，修剪可能具有鲁棒性和隐私方面的影响。
- en: Uncertainty Quantification
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不确定性量化
- en: In the previous section, you saw that for use cases where storing the weights
    for a large trained neural network is a concern, a concise internal representation
    of the network that preserves (most of) its predictive performance is desirable.
    In other situations, you may also want to know how certain the model’s decision
    is. A common example is when you are comparing the performance between two models
    and want to know if their performance is significantly different. There are multiple
    ways to quantify uncertainty depending on where in the decision process you focus.
    Model uncertainty measures can also be part of a fail-safe mechanism that sends
    an alert to the ML development team if a certain uncertainty measure drops below
    a critical threshold, triggering human-in-the-loop incident responses. In [“Sparsity
    and Model Compression”](#sec-sparsity), you reduced the numbers of latent variables
    that can contribute to a model. In a sense, you reduced the *functional uncertainty*,
    or the uncertainty that lies in the function that takes in the output. Beyond
    functional uncertainty, [*aleatoric* and *epistemic* uncertainty](https://oreil.ly/CQYqj)
    are two concepts that arise a lot in this space. Respectively, they refer to uncertainty
    around the inputs and around the outputs, though they can be easily confused.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，你看到对于存储大型训练过的神经网络权重是一个问题的用例，保留（大部分）预测性能的简洁的网络内部表示是可取的。在其他情况下，你可能还想知道模型决策的确定程度。一个常见的例子是，当你比较两个模型的性能并想知道它们的性能是否有显著差异时。根据决策过程中你关注的位置，有多种方式来量化不确定性。模型不确定性度量也可以作为故障安全机制的一部分，如果某个不确定性度量低于临界阈值，则触发人在环路的事件响应，向ML开发团队发送警报。在
    [“稀疏性和模型压缩”](#sec-sparsity) 中，你减少了可以对模型做出贡献的潜在变量的数量。在某种意义上，你减少了 *功能性不确定性*，或者说是涉及到输出的函数中的不确定性。超越功能性不确定性，[*随机性*
    和 *认知* 不确定性](https://oreil.ly/CQYqj) 是在这一领域经常出现的两个概念。它们分别指的是围绕输入和输出的不确定性，尽管它们很容易混淆。
- en: Aleatoric uncertainty
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机性不确定性
- en: Even if the true output label for a model input is within the distribution of
    accepted outputs, the inputs themselves may fall outside the training distribution
    of input data. In other words, even if an input is legitimate and should create
    an acceptable output, the training algorithm may not be able to compute it properly.
    This uncertainty, referring to input data (within the appropriate problem space)
    failing to be matched with other data with the same ground truth, is also referred
    to as *aleatoric uncertainty*. For example, suppose you have an MNIST classifier
    that was trained to distinguish the digits 0–9. You could input an image that
    belongs to one of those classes but has a shape between that of two very similar
    classes (e.g., 1 and 7 or 6 and 0). This is an example of aleatoric uncertainty.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 即使模型输入的真实输出标签在接受的输出分布之内，输入本身可能仍然落在训练数据输入分布之外。换句话说，即使一个输入是合法的并且应该产生一个可接受的输出，训练算法可能无法正确计算它。这种不确定性，指的是输入数据（在适当的问题空间内）未能与其他具有相同地面真实的数据匹配，也被称为
    *随机性不确定性*。例如，假设你有一个MNIST分类器，训练它可以区分数字0到9。你可以输入一个属于这些类之一但形状介于两个非常相似类别之间的图像（例如1和7或6和0）。这就是随机性不确定性的一个例子。
- en: Cases of high aleatoric uncertainty are harder to solve with alternative problem
    formulations than cases of high epistemic uncertainty (see the following section).
    This is part of why aleatoric uncertainty is still a large problem in medical
    diagnosis, despite the amount of time and resources applied to solving it.^([10](ch07.html#idm45621831625888))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 高随机性不确定性案例比高认知不确定性案例的替代问题表述更难解决（见下一节）。这也是为什么尽管花费了大量时间和资源来解决它，但随机性不确定性在医学诊断中仍然是一个大问题的一部分。^([10](ch07.html#idm45621831625888))
- en: Epistemic uncertainty
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 认知不确定性
- en: '*Epistemic uncertainty* refers to ground truth output decisions that fall outside
    the distributions of previously known outputs. As an example, imagine that instead
    of feeding in an image of a handwritten digit, you feed in something completely
    alien to the previously mentioned MNIST classifier. You might feed in a handwritten
    letter, or a typeset letter, or something that’s not even a letter or digit but
    say a picture of a dog.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*认知不确定性* 指的是地面真实输出决策落在先前已知输出分布之外的情况。例如，想象一下，你不是输入一个手写数字的图像，而是输入一些完全不同于先前提到的MNIST分类器的东西。你可能输入一封手写的字母，或者一封排版的字母，或者甚至不是字母或数字而是一幅狗的图片。'
- en: For most classifiers with hard-coded outputs, there is usually no option for
    “does not belong to any recognized classes.” This is unless you’re specifically
    creating a one-versus-all classifier with a designated [*garbage class*](https://oreil.ly/h6uqr).
    Even then, it’s difficult to account for all possible (and theoretically infinite)
    ways the input could be outside the training distribution. A few types of ML model
    architectures take this into account. For example, image segmentation models in
    general have a *background class* that represents everything that’s not within
    the boundary for the object of interest.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数硬编码输出的分类器来说，通常没有“不属于任何已识别类别”的选项。这是除非你专门创建一个具有指定[*垃圾类*](https://oreil.ly/h6uqr)的一对所有分类器。即使如此，很难考虑到输入可能在训练分布之外的所有可能方式（理论上是无限的）。一些ML模型架构考虑到了这一点。例如，一般的图像分割模型具有一个*背景类*，代表所有不在感兴趣对象边界内的内容。
- en: For general quantification of epistemic uncertainty there are many formulaic
    approaches, depending on exactly how the output structure is defined. Let’s explore
    three such approaches in the context of one-hot encoded or binary classifiers.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于普遍量化认知不确定性，有许多公式化的方法，具体取决于输出结构的定义方式。让我们在一个独热编码或二元分类器的上下文中探讨这三种方法。
- en: 'There are three common ways to calculate epistemic uncertainty for ML decision
    making. The most straightforward way to measure uncertainty from a classifier
    model is *classification uncertainty*: <math alttext="upper U left-parenthesis
    x right-parenthesis equals 1 minus upper P left-parenthesis ModifyingAbove x With
    caret vertical-bar x right-parenthesis"><mrow><mi>U</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>-</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>^</mo></mover> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    , where *x* is the instance to be predicted and <math alttext="upper P left-parenthesis
    ModifyingAbove x With caret vertical-bar x right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>^</mo></mover> <mo>|</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> is the most likely prediction. For example, if you have
    classes `[0,1,2]` and classification probabilities `[0.1,0.2,0.7]`, the most likely
    class according to the classifier is 2 with uncertainty 0.3. Say you have three
    instances with class probabilities.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种常见方法可以计算ML决策制定的认知不确定性。从分类器模型中测量不确定性的最直接方法是*分类不确定性*：<math alttext="upper U
    left-parenthesis x right-parenthesis equals 1 minus upper P left-parenthesis ModifyingAbove
    x With caret vertical-bar x right-parenthesis"><mrow><mi>U</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>-</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>^</mo></mover> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，其中*x*是要预测的实例，<math
    alttext="upper P left-parenthesis ModifyingAbove x With caret vertical-bar x right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>^</mo></mover> <mo>|</mo> <mi>x</mi>
    <mo>)</mo></mrow></math>是最有可能的预测。例如，如果你有类别 `[0,1,2]` 和分类概率 `[0.1,0.2,0.7]`，则根据分类器，最可能的类别是2，不确定性为0.3。假设你有三个带有类别概率的实例。
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The corresponding uncertainties are `1 - proba.max(axis=1)`, or `array([0.15,
    0.4, 0.39])` (in short, the second class is the most uncertain). This is useful
    for class-specific uncertainty, that is, if you are uncertain whether the predictions
    for a class are accurate or not. But you also want to take into account differences
    *between* classifications, that is how much uncertainty does a class prediction
    contain—​whether it is correct or not.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的不确定性为 `1 - proba.max(axis=1)`，或者 `array([0.15, 0.4, 0.39])`（简言之，第二类最不确定）。这对于特定类别的不确定性很有用，即如果你不确定某个类别的预测是否准确。但你也希望考虑到分类之间的差异，即一个类别预测包含多少不确定性——它是否正确或不正确。
- en: '*Classification margin* is the difference in probability between the first
    most likely prediction and the second most likely. Mathematically, it is defined
    as <math alttext="upper M left-parenthesis x right-parenthesis equals upper P
    left-parenthesis ModifyingAbove x 1 With caret vertical-bar x right-parenthesis
    minus upper P left-parenthesis ModifyingAbove x 2 With caret vertical-bar x right-parenthesis"><mrow><mi>M</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo>
    <mover accent="true"><msub><mi>x</mi> <mn>1</mn></msub> <mo>^</mo></mover> <mo>|</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo> <mi>P</mi> <mrow><mo>(</mo> <mover accent="true"><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>^</mo></mover> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    . Here, <math alttext="ModifyingAbove x 1 With caret"><mover accent="true"><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>^</mo></mover></math> is the most likely class, and <math
    alttext="ModifyingAbove x 2 With caret"><mover accent="true"><msub><mi>x</mi>
    <mn>2</mn></msub> <mo>^</mo></mover></math> is the second most likely. Using the
    same example as for classification uncertainty, for the class probabilities given
    in the matrix `proba`, the corresponding margins are'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*分类间隔* 是指第一最可能预测与第二最可能预测之间的概率差异。数学上定义为 <math alttext="upper M left-parenthesis
    x right-parenthesis equals upper P left-parenthesis ModifyingAbove x 1 With caret
    vertical-bar x right-parenthesis minus upper P left-parenthesis ModifyingAbove
    x 2 With caret vertical-bar x right-parenthesis"><mrow><mi>M</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo> <mover accent="true"><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>^</mo></mover> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>-</mo>
    <mi>P</mi> <mrow><mo>(</mo> <mover accent="true"><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>^</mo></mover> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> 。这里，<math
    alttext="ModifyingAbove x 1 With caret"><mover accent="true"><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>^</mo></mover></math> 是最可能的类别，而<math alttext="ModifyingAbove
    x 2 With caret"><mover accent="true"><msub><mi>x</mi> <mn>2</mn></msub> <mo>^</mo></mover></math>
    是第二可能的类别。使用与分类不确定性相同的示例，对于矩阵`proba`中给定的类概率，相应的间隔为'
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When you are querying for labels, this strategy selects the sample with the
    smallest margin, since the smaller the decision margin is, the less sure the decision.
    In this case, the sample with the smallest margin would be the third sample.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当您查询标签时，这种策略选择具有最小间隔的样本，因为决策间隔越小，决策越不确定。在这种情况下，具有最小间隔的样本将是第三个样本。
- en: '*Classification entropy* gives an approach to uncertainty that’s more grounded
    in information theory. In information theory, we have the concept of entropy of
    a random variable, or the average level of “information,” “surprise,” or “uncertainty”
    inherent to the variable’s possible outcomes. If we have a random variable like
    a dice roll, we’d expect the probabilities of the possible outcomes to be equal.
    However, since a good classifier will favor one outcome over the others in response
    to the input, the output logits become far more predictable than a random variable
    (and thus less “surprising” or “uncertain.” Mathematically, it’s simply the [Shannon
    entropy](https://oreil.ly/0niIu) defined over the distribution of predicted class
    probabilities for a sample: <math alttext="upper H left-parenthesis x right-parenthesis
    equals minus sigma-summation Underscript k Endscripts p Subscript k Baseline log
    left-parenthesis p Subscript k Baseline right-parenthesis"><mrow><mi>H</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <msub><mo>∑</mo> <mi>k</mi></msub>
    <msub><mi>p</mi> <mi>k</mi></msub> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <msub><mi>p</mi> <mi>k</mi></msub> <mo>)</mo></mrow></mrow></math> , where *p[k]*
    is the probability of the sample belonging to the *k*-th class.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*分类熵* 提供了一种更基于信息论的不确定性方法。在信息论中，我们有一个随机变量的熵，或者说是变量可能结果的“信息”、“惊讶”或“不确定性”的平均水平。如果我们有像掷骰子这样的随机变量，我们期望可能结果的概率是相等的。然而，由于一个好的分类器会偏向某个输出而不是其他输出，输出的logits比随机变量更可预测（因此更少“惊讶”或“不确定”）。数学上，它简单地是[Shannon熵](https://oreil.ly/0niIu)在样本的预测类概率分布上定义：<math
    alttext="upper H left-parenthesis x right-parenthesis equals minus sigma-summation
    Underscript k Endscripts p Subscript k Baseline log left-parenthesis p Subscript
    k Baseline right-parenthesis"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <msub><mo>∑</mo> <mi>k</mi></msub> <msub><mi>p</mi> <mi>k</mi></msub>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow></mrow></math> ，其中*p[k]* 是样本属于第*k*类的概率。'
- en: Heuristically, the entropy is proportional to the average number of guesses
    you’d have to make to find the true class. Let’s come back to our example from
    before. Here, the corresponding entropies are
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 就直觉而言，熵与你找到真实类别所需的平均猜测次数成正比。让我们回到之前的例子。在这里，相应的熵是
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you repeat this process for many random samples, you would get a *distribution*
    of the uncertainty values. [Figure 7-1](#img-uncertainty) gives the distributions
    of the three types of uncertainties. The closer a distribution is to uniform,
    the larger that specific type of uncertainty is. Proximity to a corner of the
    triangle indicates high predicted probability for the outcome to have that specific
    label.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对许多随机样本重复此过程，你会得到一个*分布*，显示不确定性值。[图7-1](#img-uncertainty)展示了三种不确定性的分布。分布越接近均匀，特定类型的不确定性越大。三角形的角落越接近，说明特定标签的预测概率越高。
- en: '![ptml 0701](assets/ptml_0701.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0701](assets/ptml_0701.png)'
- en: Figure 7-1\. Representations of uncertainty distributions
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 不确定性分布的表现形式
- en: For a code walkthrough of the preceding example of implementing classification
    uncertainty, margin uncertainty, and classification entropy in a three-class classification
    problem, see [this notebook](https://oreil.ly/oydWO).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何在一个三类分类问题中实现分类不确定性、边际不确定性和分类熵的代码演示，请参见[这个笔记本](https://oreil.ly/oydWO)。
- en: Tip
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Let’s look back for a moment at [“Deep Dive: Adversarial Attacks in Computer
    Vision”](ch04.html#deepdive-cv). Among the uncertainty metrics you learned here,
    which do you think is suitable for quantifying the ambiguity in predicted probabilities
    for the different images?'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微回顾一下[“深入了解：计算机视觉中的对抗性攻击”](ch04.html#deepdive-cv)。在这里学到的不确定性度量中，你认为哪一个适合用来量化不同图像预测概率中的模糊性？
- en: Confidence intervals
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 置信区间
- en: As you saw in the previous section, aleatoric and epistemic uncertainty can
    be quantified by simple dimensionless numbers. However, when presenting uncertainty
    to stakeholders, you’ll usually want to choose a more intuitive visual representation.
    For regression-based models, you can present the outputs in the form of [confidence
    intervals (CI)](https://oreil.ly/WZfBO). Most data scientists and ML practitioners
    should be familiar with placing bars that indicate the upper and lower estimates.
    Typically, you can do this by calculating the [standard error](https://oreil.ly/oSt6U)
    of the mean response, then using that to build a CI of that mean response.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在前一节中所看到的，通过简单的无量纲数字可以量化误差不确定性和认知不确定性。然而，当向利益相关者呈现不确定性时，通常会选择更直观的视觉表示。对于基于回归的模型，你可以以[置信区间（CI）](https://oreil.ly/WZfBO)的形式呈现输出。大多数数据科学家和机器学习从业者应该熟悉放置指示上下限估计的柱状图。通常，你可以通过计算均值响应的[标准误差](https://oreil.ly/oSt6U)，然后用它来构建这个均值响应的
    CI。
- en: Bootstrap resampling
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bootstrap 重抽样
- en: One way you can estimate the standard error and produce CIs is through bootstrap
    resampling. Broadly speaking, bootstrapping approximates the difference between
    the true and sample data distributions using the difference between the data and
    *resampled* data distributions. It generates variants of the dataset at hand (or
    parameters estimated from it), hoping that these variants can give intuition about
    the uncertainty in the data-generating process and the parameters of that process.
    For a code walkthrough of implementing bootstrap in scikit-learn, see [this notebook](https://oreil.ly/n0wi7).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过自助重抽样（bootstrap resampling）来估计标准误差并生成置信区间。广义来说，bootstrapping 使用数据与*重抽样*数据分布之间的差异来近似真实数据与样本数据分布的差异。它生成手头数据集（或从中估计的参数）的变体，希望这些变体能让我们对数据生成过程及其参数的不确定性有直观的了解。要在
    scikit-learn 中实现 bootstrap 的代码演示，请参见[这个笔记本](https://oreil.ly/n0wi7)。
- en: 'Bootstrap resampling has three main variants. Let’s look at them in a supervised
    model setting, where the observed dataset *D* is composed of an input feature
    matrix *X* and an output feature vector *y*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Bootstrap 重抽样有三个主要变体。让我们在监督模型设置中看看它们，观察观察到的数据集*D*由输入特征矩阵*X*和输出特征向量*y*组成：
- en: Nonparametric bootstrap
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数 bootstrap
- en: You directly sample from the observed dataset *D*, perhaps thousands of times,
    to build your variant datasets. Sampling is generally done *with replacement*,
    i.e., one data point can be picked multiple times.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你直接从观察到的数据集*D*中进行样本采样，可能要重复成千上万次，以构建你的变体数据集。采样通常是*有放回*的，即可以多次选取同一个数据点。
- en: Parametric bootstrap
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 bootstrap
- en: 'You start with a parametric model to fit the data: <math alttext="y equals
    f left-parenthesis upper X semicolon theta right-parenthesis plus epsilon"><mrow><mi>y</mi>
    <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>X</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo> <mo>+</mo>
    <mi>ϵ</mi></mrow></math> , with <math alttext="epsilon"><mi>ϵ</mi></math> being
    a vector of random errors. Then you use parameter estimates <math alttext="ModifyingAbove
    theta With caret"><mover accent="true"><mi>θ</mi> <mo>^</mo></mover></math> (which
    are a function of data *D*) as proxies of the true parameters <math alttext="theta
    Superscript asterisk"><msup><mi>θ</mi> <mo>*</mo></msup></math> to generate a
    large number of datasets from the parametric model <math alttext="y Subscript
    r Baseline equals f left-parenthesis upper X semicolon ModifyingAbove theta With
    caret right-parenthesis plus upper P left-parenthesis ModifyingAbove epsilon With
    caret right-parenthesis"><mrow><msub><mi>y</mi> <mi>r</mi></msub> <mo>=</mo> <mi>f</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>;</mo> <mover accent="true"><mi>θ</mi> <mo>^</mo></mover>
    <mo>)</mo></mrow> <mo>+</mo> <mi>P</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ϵ</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow></mrow></math> , where <math alttext="ModifyingAbove
    epsilon With caret equals y minus f left-parenthesis upper X semicolon ModifyingAbove
    theta With caret right-parenthesis"><mrow><mover accent="true"><mi>ϵ</mi> <mo>^</mo></mover>
    <mo>=</mo> <mi>y</mi> <mo>-</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>^</mo></mover> <mo>)</mo></mrow></mrow></math>
    is the fitted residual vector, and *P* denotes a permutation. You then estimate
    <math alttext="ModifyingAbove theta With caret"><mover accent="true"><mi>θ</mi>
    <mo>^</mo></mover></math> for each of these new datasets.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 您从参数模型开始拟合数据：<math alttext="y equals f left-parenthesis upper X semicolon theta
    right-parenthesis plus epsilon"><mrow><mi>y</mi> <mo>=</mo> <mi>f</mi> <mo>(</mo>
    <mi>X</mi> <mo>;</mo> <mi>θ</mi> <mo>)</mo> <mo>+</mo> <mi>ϵ</mi></mrow></math>
    ，其中 <math alttext="epsilon"><mi>ϵ</mi></math> 是一个随机误差向量。然后，您使用参数估计 <math alttext="ModifyingAbove
    theta With caret"><mover accent="true"><mi>θ</mi> <mo>^</mo></mover></math>（这些是数据*D*的函数）作为真实参数
    <math alttext="theta Superscript asterisk"><msup><mi>θ</mi> <mo>*</mo></msup></math>
    的代理，从参数模型 <math alttext="y Subscript r Baseline equals f left-parenthesis upper
    X semicolon ModifyingAbove theta With caret right-parenthesis plus upper P left-parenthesis
    ModifyingAbove epsilon With caret right-parenthesis"><mrow><msub><mi>y</mi> <mi>r</mi></msub>
    <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <mo>+</mo> <mi>P</mi> <mrow><mo>(</mo> <mover
    accent="true"><mi>ϵ</mi> <mo>^</mo></mover> <mo>)</mo></mrow></mrow></math> 生成大量数据集，其中
    <math alttext="ModifyingAbove epsilon With caret equals y minus f left-parenthesis
    upper X semicolon ModifyingAbove theta With caret right-parenthesis"><mrow><mover
    accent="true"><mi>ϵ</mi> <mo>^</mo></mover> <mo>=</mo> <mi>y</mi> <mo>-</mo> <mi>f</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>;</mo> <mover accent="true"><mi>θ</mi> <mo>^</mo></mover>
    <mo>)</mo></mrow></mrow></math> 是拟合残差向量，*P*表示置换。然后，您为每个新数据集估计 <math alttext="ModifyingAbove
    theta With caret"><mover accent="true"><mi>θ</mi> <mo>^</mo></mover></math>。
- en: Wild bootstrap
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 野生自举
- en: 'What if the amount of random variance in the data is not constant throughout?
    This is a general version of the parametric bootstrap where you still use <math
    alttext="ModifyingAbove theta With caret"><mover accent="true"><mi>θ</mi> <mo>^</mo></mover></math>
    to generate new datasets, but instead of permuting the residuals, you *perturb*
    them: <math alttext="y Subscript r Baseline equals left-parenthesis f left-parenthesis
    upper X semicolon ModifyingAbove theta With caret right-parenthesis plus ModifyingAbove
    epsilon With caret v"><mrow><msub><mi>y</mi> <mi>r</mi></msub> <mrow><mo>=</mo>
    <mo>(</mo> <mi>f</mi></mrow> <mrow><mo>(</mo> <mi>X</mi> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <mo>+</mo> <mover accent="true"><mi>ϵ</mi>
    <mo>^</mo></mover> <mi>v</mi></mrow></math> , where *v* is a vector of independent
    draws from a random variable with mean 0 and variance 1.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据中的随机变异量并非始终保持恒定，那么这是参数自举的一般版本，其中您仍然使用 <math alttext="ModifyingAbove theta
    With caret"><mover accent="true"><mi>θ</mi> <mo>^</mo></mover></math> 生成新数据集，但不是置换残差，而是*扰动*它们：<math
    alttext="y Subscript r Baseline equals left-parenthesis f left-parenthesis upper
    X semicolon ModifyingAbove theta With caret right-parenthesis plus ModifyingAbove
    epsilon With caret v"><mrow><msub><mi>y</mi> <mi>r</mi></msub> <mrow><mo>=</mo>
    <mo>(</mo> <mi>f</mi></mrow> <mrow><mo>(</mo> <mi>X</mi> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <mo>+</mo> <mover accent="true"><mi>ϵ</mi>
    <mo>^</mo></mover> <mi>v</mi></mrow></math> ，其中*v*是一个均值为0、方差为1的随机变量的独立抽样向量。
- en: Let’s look at an example of using bootstrap confidence intervals in regression
    models. For simplicity, we’ll take a nonparametric (i.e., sampling directly from
    the dataset) approach. Let’s sample one thousand subsets of the data of a given
    size, fit a linear regression model to each sample, then record intercepts and
    coefficients of each regression model. From these you can get the 95% CIs by obtaining
    the 97.5% and 2.5% percentiles of the intercepts and coefficients. Using intervals
    based on percentiles also means not making assumptions about the underlying data
    distribution. Now you can estimate the uncertainty of these model predictions.
    Based on the [standard assumptions for linear regression](https://oreil.ly/mx0yT),
    you can approximate the variance of a value of the outcome given input feature
    values using the prediction residuals. From this variance, you can calculate the
    standard error, a measure of how well you’re estimating *y*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个使用自助法置信区间在回归模型中的例子。为简单起见，我们将采用非参数方法（即直接从数据集中抽样）。让我们从给定大小的数据中抽样一千个子集，对每个样本拟合线性回归模型，然后记录每个回归模型的截距和系数。从这些数据中，您可以通过获取截距和系数的97.5%和2.5%分位数来获得95%的置信区间。使用基于分位数的区间还意味着不对底层数据分布做出假设。现在，您可以估计这些模型预测的不确定性。根据[线性回归的标准假设](https://oreil.ly/mx0yT)，您可以使用预测残差来近似给定输入特征值的结果值的方差。从这个方差中，您可以计算标准误差，这是衡量您对*y*估计精度的一种指标。
- en: <math><mrow><msup><mi>σ</mi> <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo>
    <mi>i</mi> <mi>N</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>d</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>d</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mrow><mi>N</mi><mo>-</mo><mn>2</mn></mrow></mfrac>
    <mo>,</mo></mrow></math><math><mrow><mtext>Var</mtext> <mrow><mo>(</mo> <mover
    accent="true"><mi>α</mi> <mo>^</mo></mover> <mo>+</mo> <mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <msub><mi>x</mi> <mi>d</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mtext>Var</mtext> <mrow><mo>(</mo> <mover accent="true"><mi>α</mi> <mo>^</mo></mover>
    <mo>)</mo></mrow> <mo>+</mo> <mtext>Var</mtext> <mrow><mo>(</mo> <mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <msubsup><mi>x</mi> <mi>d</mi> <mn>2</mn></msubsup>
    <mo>+</mo> <mn>2</mn> <msub><mi>x</mi> <mi>d</mi></msub> <mtext>Cov</mtext> <mrow><mo>(</mo>
    <mover accent="true"><mi>α</mi> <mo>^</mo></mover> <mo>,</mo> <mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <mo>,</mo></mrow></math><math><mrow><mtext>Var</mtext>
    <mrow><mo>(</mo> <mover accent="true"><mi>α</mi> <mo>^</mo></mover> <mo>+</mo>
    <mover accent="true"><mi>β</mi> <mo>^</mo></mover> <msub><mi>x</mi> <mi>d</mi></msub>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mfenced close=")"
    open="(" separators=""><mfrac><mn>1</mn> <mi>m</mi></mfrac> <mo>+</mo> <mfrac><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>d</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup> <mrow><mo>∑</mo><msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mfenced> <mo>,</mo></mrow></math><math><mrow><msub><mi>s</mi>
    <msub><mi>μ</mi> <mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></msub></msub> <mo>=</mo>
    <msqrt><mrow><mtext>Var</mtext> <mo>(</mo> <mover accent="true"><mi>α</mi> <mo>^</mo></mover>
    <mo>+</mo> <mover accent="true"><mi>β</mi> <mo>^</mo></mover> <msub><mi>x</mi>
    <mi>d</mi></msub> <mo>)</mo></mrow></msqrt> <mo>,</mo></mrow></math><math><mrow><mi>C</mi>
    <mi>I</mi> <mo>=</mo> <mo>[</mo> <msub><mi>μ</mi> <mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></msub>
    <mo>-</mo> <msub><mi>t</mi> <mrow><mn>1</mn><mo>-</mo><mi>α</mi><mo>/</mo><mn>2</mn><mo>,</mo><mi>n</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <msub><mi>s</mi> <msub><mi>μ</mi> <mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></msub></msub>
    <mo>,</mo> <msub><mi>μ</mi> <mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></msub>
    <mo>+</mo> <msub><mi>t</mi> <mrow><mn>1</mn><mo>-</mo><mi>α</mi><mo>/</mo><mn>2</mn><mo>,</mo><mi>n</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <msub><mi>s</mi> <msub><mi>μ</mi> <mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></msub></msub>
    <mo>]</mo> <mo>.</mo></mrow></math>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><msup><mi>σ</mi> <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo>
    <mi>i</mi> <mi>N</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>d</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>d</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow> <mrow><mi>N</mi><mo>-</mo><mn>2</mn></mrow></mfrac>
    <mo>,</mo></mrow></math><math><mrow><mtext>Var</mtext> <mrow><mo>(</mo> <mover
    accent="true"><mi>α</mi> <mo>^</mo></mover> <mo>+</mo> <mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <msub><mi>x</mi> <mi>d</mi></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mtext>Var</mtext> <mrow><mo>(</mo> <mover accent="true"><mi>α</mi> <mo>^</mo></mover>
    <mo>)</mo></mrow> <mo>+</mo> <mtext>Var</mtext> <mrow><mo>(</mo> <mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <msubsup><mi>x</mi> <mi>d</mi> <mn>2</mn></msubsup>
    <mo>+</mo> <mn>2</mn> <msub><mi>x</mi> <mi>d</mi></msub> <mtext>Cov</mtext> <mrow><mo>(</mo>
    <mover accent="true"><mi>α</mi> <mo>^</mo></mover> <mo>,</mo> <mover accent="true"><mi>β</mi>
    <mo>^</mo></mover> <mo>)</mo></mrow> <mo>,</mo></mrow></math><math><mrow><mtext>Var</mtext>
    <mrow><mo>(</mo> <mover accent="true"><mi>α</mi> <mo>^</mo></mover> <mo>+</mo>
    <mover accent="true"><mi>β</mi> <mo>^</mo></mover> <msub><mi>x</mi> <mi>d</mi></msub>
    <mo>)</mo></mrow> <mo>=</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mfenced close=")"
    open="(" separators=""><mfrac><mn>1</mn> <mi>m</mi></mfrac> <mo>+</mo> <mfrac><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>d</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup> <mrow><mo>∑</mo><msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mfenced> <mo>,</mo></mrow></math><math><mrow><msub><mi>s</mi>
    <msub><mi>μ</mi> <mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></msub></msub> <mo>=</mo>
    <msqrt><mrow><mtext>Var</mtext> <mo>(</mo> <mover accent="true"><mi>α</mi> <mo>^</mo></mover>
    <mo>+</mo> <mover accent="true"><mi>β</mi> <mo>^</mo></mover> <msub><mi>x</mi>
    <mi>d</mi></msub> <mo>)</mo></mrow></msqrt> <mo>,</mo></mrow></math><math><mrow><mi>C</mi>
    <mi>I</mi> <mo>=</mo> <mo>[</mo> <msub><mi>μ</mi> <mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></msub>
    <mo>-</mo> <msub><mi>t</mi> <mrow><mn>1</mn><mo>-</mo><mi>α</mi><mo>/</mo><mn>2</mn><mo>,</mo><mi>n</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <msub><mi>s</mi> <msub><mi>μ</mi> <mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></msub></msub>
    <mo>,</mo> <msub><mi>μ</mi> <mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></msub>
    <mo>+</mo> <msub><mi>t</mi> <mrow><mn>1</mn><mo>-</mo><mi>α</mi><mo>/</mo><mn>2</mn><mo>,</mo><mi>n</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <msub><mi>s</mi> <msub><mi>μ</mi> <mrow><mi>y</mi><mo>|</mo><mi>x</mi></mrow></msub></msub>
    <mo>]</mo> <mo>.</mo></mrow></math>
- en: The resulting CI is great, but this only accounts for drift in the *mean* response
    of *Y*. If you want to get intervals for all possible values of *Y* for a given
    *X* value, you need to calculate the *prediction interval*. The derivation of
    the prediction interval is similar to that of the CI, except you include the variance
    of our dependent variable *Y* when calculating the standard error, leading to
    wider intervals.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的CI很好，但这只涵盖了*Y*的*平均*响应的漂移。如果您想要为给定*X*值的所有可能*Y*值计算区间，您需要计算*预测区间*。预测区间的推导与CI的类似，只是在计算标准误差时包括了我们的因变量*Y*的方差，导致区间更宽。
- en: Are you certain I can trust you?
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 您确定我能相信您吗？
- en: Calculating uncertainty estimates such as CIs for trust metrics is an obvious
    way to incorporate uncertainty quantification into trustworthy ML pipelines. This
    is useful extra information. For example, suppose the disparate impact of binary
    outcomes from an ML model for a hiring use case is 1.1. While at face value the
    model seems unbiased, different widths of the 95% CI may lead to different conclusions.
    A CI of [1.08, 1.12] would reaffirm the conclusion of unbiasedness by the point
    estimate of 1.1. On the other hand, a much wider CI, say [0.9, 1.3], would diminish
    the trust in the conclusion. Based on feedback from the domain experts with whom
    you are collaborating, this may prompt you to revisit the data collection process
    to find out whether or not the 1.1 value is simply an artifact of the specific
    dataset analyzed.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 计算诸如信任度度量的CI等不确定性估计是将不确定性量化纳入可信ML管道的一种明显方式。这是有用的额外信息。例如，假设用于招聘用例的ML模型的二元结果的不平等影响为1.1。尽管表面上看来，模型似乎没有偏差，但95%的CI的不同宽度可能导致不同的结论。CI为[1.08,
    1.12]将通过1.1的点估计重新确认无偏的结论。另一方面，更宽的CI，比如[0.9, 1.3]，可能会减弱对结论的信任。根据与您合作的领域专家的反馈，这可能促使您重新审视数据收集过程，以确定1.1值是否仅仅是特定数据集分析的结果。
- en: 'Part II: Implementation Challenges'
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：实施挑战
- en: Now that you know about the technical factors beyond aspects of trust that may
    be relevant for your efforts to build a trustworthy ML pipeline, it’s time to
    shift gears. Let’s talk about the systemic considerations that go into designing
    trustworthy ML systems. Outside strict methodological research settings, ML work
    does not happen in a vacuum. This cannot be truer than in the typical modern tech
    company setting. As a part of well-defined product initiatives, the ML development
    team *needs* to interact with people outside the team. Depending on these stakeholders’
    familiarity with ML and trustworthy ML concepts, you might face one or more challenges
    you need to navigate to make progress on a product development journey that effectively
    incorporates aspects of trust.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已了解超越信任方面的技术因素可能对建立可信ML管道的努力有所帮助，现在是转变思路的时候了。让我们讨论设计可信ML系统所需的系统考虑因素。在严格的方法论研究设置之外，ML工作并非在真空中进行。这在典型的现代科技公司设置中更是如此。作为明确定义的产品倡议的一部分，ML开发团队*需要*与团队外的人员进行互动。根据这些利益相关者对ML和可信ML概念的熟悉程度，您可能会面对一个或多个挑战，您需要解决这些挑战以在有效整合信任方面的产品开发旅程中取得进展。
- en: Motivating Stakeholders to Develop Trustworthy ML Systems
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激励利益相关者开发可信ML系统
- en: 'As you progress along the journey of applying trustworthy ML principles in
    business settings, chances are that you will face questions and comments from
    stakeholders beyond your team that fall along these lines:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用可信ML原则于商业环境中的旅程中，随着您的进展，很可能会面对来自团队以外的利益相关者的问题和评论，内容大致如下：
- en: Why do we need trustworthy methods in the first place?
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们为什么需要可信的方法？
- en: Your model is already optimized for best performance.Won’t placing these extra
    conditions on the model degrade its accuracy, area under the curve (AUC), or other
    performance metrics?
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的模型已经优化以获得最佳性能。将这些额外条件放在模型上是否会降低其准确性、曲线下面积（AUC）或其他性能指标？
- en: If you want a fair ML model, just don’t use data on sensitive features!
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您希望得到一个公平的ML模型，就不要使用涉及敏感特征的数据！
- en: I don’t know if you have the budget for all this extra work.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我不知道您是否有预算来完成所有这些额外工作。
- en: 'There are two lines of reasoning for adding one or more trust elements into
    an applied ML workflow: (a) debt management and (b) risk management. When these
    two things are done properly, the benefits of trustworthy ML development far outweigh
    the perceived cost of “extra” work.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个或多个信任元素到应用的机器学习工作流程有两种推理线索：(a) 债务管理和 (b) 风险管理。当这两件事做得恰当时，可信任的机器学习开发带来的好处远远超过“额外”工作的感知成本。
- en: Debt management
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 债务管理
- en: Ward Cunningham proposed the term *technical debt* in 1992 to represent the
    hidden long-term costs to software systems incurred by perpetual fast development
    cycles.^([11](ch07.html#idm45621831208160)) As the creator of [Agile](https://oreil.ly/j2VFi),
    Cunningham knew a thing or two about good software development. He realized that
    perpetually building things and adding new functionalities does not come cheap.
    If not channeled properly, a fast-paced development culture creates redundancies
    and dependencies that make the underlying product difficult to troubleshoot and
    maintain even as it matures in its capabilities. Debt management work such as
    refactoring code to minimize dependencies, cleaning up duplicate code, and writing
    proper documentation does cost some developer cycles. However, it protects the
    product against the potentially higher long-term costs of running a brittle, patched-together
    system.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 1992年，沃德·坎宁安提出了术语*技术债务*，用于表示由于永久快速开发周期而给软件系统带来的隐藏长期成本^([11](ch07.html#idm45621831208160))。作为[敏捷开发](https://oreil.ly/j2VFi)的创始人，坎宁安对良好的软件开发有一些了解。他意识到，永远地构建和添加新功能并不便宜。如果不适当地引导，快节奏的开发文化会造成冗余和依赖，使基础产品在其能力不断增强的同时难以排除故障和维护。像重构代码以最小化依赖、清理重复代码和撰写适当文档等债务管理工作确实需要一些开发者周期。然而，这些工作保护产品免受运行脆弱、拼凑在一起的系统可能导致的更高长期成本的影响。
- en: In addition to technical debt, ML systems may need to contend with some unique
    and more systematic maintenance problems. Examples of ML-specific technical debt
    include dependencies on external data with poor documentation and/or of dubious
    quality, data dependencies on black box ML models, and reproducibility concerns
    due to randomness during model training and inference (especially when the outputs
    of one model are the inputs of another). Real ML systems have a relatively small
    amount of code dedicated to just model training and inference. These code components
    need a complex infrastructure to properly do their job, incurring additional trust
    debt. Adding trust elements to this system means creating even more dependencies
    and interactions that go beyond the ML infrastructure (see [Figure 7-2](#img-system)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 除了技术债务，机器学习系统可能还需要处理一些独特且更系统化的维护问题。机器学习特定技术债务的例子包括依赖于文档质量差和/或质量可疑的外部数据、对黑盒机器学习模型的数据依赖以及由于模型训练和推理期间的随机性而导致的可重现性问题（尤其是当一个模型的输出是另一个模型的输入时）。真实的机器学习系统仅有相对较少的代码专门用于模型训练和推理。这些代码组件需要一个复杂的基础设施来正确执行其工作，从而产生额外的信任债务。在这个系统中添加信任元素意味着创建更多超出机器学习基础设施范围的依赖和互动（参见[图 7-2](#img-system)）。
- en: Real ML systems are composed of many components above and beyond just data and
    code. The ML system itself is a part of a broader company-wide ecosystem of initiatives.
    Trust considerations (circles) are applicable to many components of the business,
    both inside and outside the ML system. Thus, to make ML systems trustworthy, these
    additional debts need the attention of the ML team—​and the product team in general.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 真实的机器学习系统由许多超出数据和代码之外的组件组成。机器学习系统本身是公司范围内各种倡议的一部分。信任考虑（圆圈）适用于业务的许多组件，无论是在机器学习系统内部还是外部。因此，为了使机器学习系统可信任，这些额外的债务需要机器学习团队以及整体产品团队的关注。
- en: '![ptml 0702](assets/ptml_0702.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0702](assets/ptml_0702.png)'
- en: Figure 7-2\. Technical and nontechnical components of a ML system, with those
    with potential trust considerations marked by circles
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 机器学习系统的技术和非技术组件，其中有潜在信任考虑的组件标记为圆圈
- en: Risk management
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 风险管理
- en: 'The model risk management (MRM) framework has gained popularity in the financial
    sector as a best-practice rubric to ensure that data analytics projects meet regulatory
    goals and align with core institutional values *while* maintaining reproducible
    performance guarantees. Think of trustworthy ML practices as an enhanced form
    of MRM. It’s helpful to think about the evolution of an organization’s trustworthy
    ML capabilities in three stages:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融领域，模型风险管理（MRM）框架已经因其作为最佳实践规范，以确保数据分析项目达到监管目标并与核心机构价值观保持一致的能力而受到欢迎。将值得信赖的机器学习实践视为MRM的增强形式是很有帮助的。有助于思考组织信任的机器学习能力的三个阶段的演变：
- en: '*Setting standards*'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*设定标准*'
- en: The first stage includes setting formal processes and best practices for incorporating
    trust elements into ML workflows and product design.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个阶段包括为将信任元素纳入机器学习工作流程和产品设计中制定正式流程和最佳实践。
- en: '*Implementation*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*实施*'
- en: The second stage includes implementing the guidelines into actual projects,
    as well as training practitioners.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段包括将指南实施到实际项目中，以及培训从业人员。
- en: '*Efficiency*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*效率*'
- en: Third, ensuring efficiency includes actually extracting value from trusted ML
    practices through gathering feedback to improve future implementations, optimizing
    resource management, and validating methods in the field.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，确保效率包括通过收集反馈以改进未来的实施、优化资源管理和验证领域方法，从而实际提取可信机器学习实践的价值。
- en: This three-stage process helps offset trust debt and protects against significant
    operational risks. Such risks include falling out of compliance with current or
    future regulations for ML-powered applications and negative PR if anything goes
    wrong.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个三阶段过程有助于抵消信任债务，并防止面临重大运营风险。这些风险包括不符合当前或未来ML应用程序的法规要求，以及如果出现问题可能引起的负面公关。
- en: While there are costs for getting the *trust* risk management process started—in
    stages 1 and 2—it is important to focus on stage 3, which is mostly about offsetting
    such costs and even turning profits over time. Research has shown that the *trust-utility
    tradeoff* is often a red herring. For example, [Rodolfa et al.](https://oreil.ly/Ii11e)
    shows that it is indeed possible to use ML to allocate benefits fairly and equitably
    in resource-constrained practical situations, with little to no decline in model
    performance.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在阶段1和2启动*信任*风险管理过程会有成本，但专注于第3阶段非常重要，这主要是为了抵消这些成本，甚至随着时间的推移实现盈利。研究表明*信任-效用权衡*通常是一个不切实际的问题。例如，[Rodolfa等人](https://oreil.ly/Ii11e)表明，在资源受限的实际情况下，确实可以使用机器学习公平和公正地分配利益，几乎不会影响模型性能下降。
- en: Trust Debts
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信任债务
- en: Let’s now dive deeper into a few aspects of both technical and non-technical
    (ethical) trust debts.^([12](ch07.html#idm45621831185904)) Both types of debts
    are categorized in terms of the system components in [Figure 7-2](#img-system).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨技术和非技术（伦理）信任债务的几个方面。^([12](ch07.html#idm45621831185904)) 这两种类型的债务根据系统组件在[图 7-2](#img-system)中进行分类。
- en: Technical trust debt
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 技术信任债务
- en: 'Components of technical trust debt roughly map to the lifecycle stages of a
    typical ML project. The broad idea is to form and adhere to technical best practices
    for ML system development that make sense under the constraints of our company
    and ML organization:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 技术信任债务的组成部分大致映射到典型机器学习项目的生命周期阶段。总体思路是制定并坚持符合我们公司和机器学习组织约束条件的技术最佳实践：
- en: Data collection
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集
- en: Specific datasets are often siloed into groups with concentrated domain expertise.
    It can be hard to get proper guidance on the access policies of specific sensitive
    datasets. For these logistical reasons, an ML team may be discouraged in collating
    data sources that could aid them in building trusted applications. Even when they
    do manage to collate them, the owners of data sources sometimes update them without
    properly tracking the changes. This can lead to models that worked one day mysteriously
    not working the next.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 特定数据集通常被分组到具有集中领域专业知识的群体中。很难获得关于特定敏感数据集访问政策的适当指导。出于这些后勤原因，一个机器学习团队可能不愿意整合能够帮助他们构建受信任应用程序的数据源。即使他们设法整合了这些数据源，数据源的所有者有时会在没有适当跟踪更改的情况下更新它们。这可能导致一天工作正常的模型在第二天就神秘地失效。
- en: Tools like Data Version Control ([DVC](https://dvc.org)) can help you track
    which versions of a dataset your team is using, and it can help you fix versions
    for specific experiments. You might think of DVC as Git for datasets. Adding DVC
    to a project is straightforward.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 类似数据版本控制（[DVC](https://dvc.org)）这样的工具可以帮助您跟踪团队使用的数据集版本，并为特定实验固定版本。您可以将DVC视为数据集的Git。将DVC添加到项目中非常简单。
- en: '[PRE4]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Just make sure you add a few internal files to Git.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 只需确保向Git添加几个内部文件。
- en: '[PRE5]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Versioning with DVC is simple.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DVC进行版本控制很简单。
- en: '[PRE6]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If you’ve already initialized your project, you can use DVC to directly download
    datasets as well.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经初始化了项目，可以使用DVC直接下载数据集。
- en: '[PRE7]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In addition, `dvc get` can download any file or directory tracked in a DVC repository.
    It works like `wget`, but for DVC or Git repos. In this case, we are downloading
    the latest version of the *data.xml* file from the dataset registry repo (the
    data source).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`dvc get`可以下载DVC仓库中跟踪的任何文件或目录。它类似于`wget`，但用于DVC或Git仓库。在这种情况下，我们从数据集注册仓库（数据源）下载*data.xml*文件的最新版本。
- en: Data verification
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 数据验证
- en: Even when the required data is available and accessible, deficiencies in data
    collection processes may hinder trustworthy ML deployment. For example, procuring
    quality third-party demographic data has been an ongoing issue in applied algorithmic
    fairness.^([13](ch07.html#idm45621831158800))
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 即使所需数据已经可用且易于访问，数据收集过程中的缺陷可能会阻碍可信的机器学习部署。例如，在应用算法公平性中，获取优质第三方人口统计数据一直是一个持续存在的问题。^([13](ch07.html#idm45621831158800))
- en: Monitoring the health of critical data dependencies is an ongoing process. Even
    when a source supplies you with high-quality data in the first iteration of your
    model, you need to put checks in place to make sure that this quality level remains
    for future iterations as well.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 监控关键数据依赖项的健康状态是一个持续进行的过程。即使源数据在模型的第一次迭代中提供了高质量的数据，您也需要确保将来的迭代中仍然保持这种质量水平。
- en: Feature extraction
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取
- en: One major hurdle in implementing ML fairness methods is the risk that information
    about sensitive features will seep into the data through correlated non-sensitive
    proxy features. This is why creating fair algorithms isn’t as simple as just not
    including sensitive features in your datasets.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 实施机器学习公平方法的一个主要障碍是，关于敏感特征的信息可能通过相关的非敏感代理特征渗入数据中。这就是为什么创建公平算法并不像简单地在数据集中不包含敏感特征那样简单。
- en: For other aspects of trust, you’ll need to use domain knowledge extensively
    when you craft features in standard data science workflows. Without well-reasoned
    extract-transform-load (ETL) pipelines, ML systems are difficult to interpret
    and troubleshoot—hence, trust. Even if the features *aren’t* sensitive, unseen
    correlated features can make ML training both more brittle and more expensive.
    Tools like analysis of covariance ([ANCOVA](https://oreil.ly/Zm4Sg)) are commonly
    used to identify correlated features. However, ANCOVA has been used in many scenarios
    where some of its assumptions definitely do not apply.^([14](ch07.html#idm45621831148064))
    This is a practical application for the causal inference techniques we mentioned
    in [“Causal Machine Learning”](#causalinf).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于信任的其他方面，当您在标准数据科学工作流程中设计特征时，需要广泛运用领域知识。没有合理推理的提取-转换-加载（ETL）管道，机器学习系统难以解释和排错，因此难以建立信任。即使特征*不*敏感，未见过的相关特征也可能使机器学习训练更加脆弱和更加昂贵。诸如协方差分析（[ANCOVA](https://oreil.ly/Zm4Sg)）之类的工具通常用于识别相关特征。然而，在许多场景中，ANCOVA被使用时，其某些假设显然不适用。^([14](ch07.html#idm45621831148064))这是我们在[“因果机器学习”](#causalinf)中提到的因果推断技术的实际应用。
- en: Process management
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 过程管理
- en: ML projects inside an organization rarely happen in a linear and isolated manner.
    Rather, they tend to focus on a handful of application domains, leverage similar
    ETL components, and interact with each other through reuse and output chaining.
    Given this interconnectedness, trust debt incurred in one project can cascade
    down to other projects affected by the current project, or the same source of
    debt can affect multiple projects in tandem. If you are retraining a model at
    any step in this pipeline, it’s important to also retrain any downstream models
    that would be affected by a change of outputs. They should be retrained in the
    order in which information passes through the pipeline.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 组织内的ML项目很少是线性和孤立的。相反，它们往往集中在少数应用领域，利用类似的ETL组件，并通过重用和输出链接相互交互。鉴于这种相互连接性，一个项目中产生的信任债务可能会向受当前项目影响的其他项目级联传递，或者同一债务源可能会影响多个项目。如果在管道的任何步骤中重新训练模型，则重要的是也重新训练受输出变化影响的任何下游模型。它们应按照信息通过管道的顺序进行重新训练。
- en: Feedback
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈
- en: Feedback loops can result in technical trust debt directly or indirectly. *Direct
    feedback loops* arise when the data collected for a model comes from the population
    where the inferences from previous iterations of the model are being served ([Figure 7-3](#img-feedback)).
    With *indirect* or *hidden* feedback loops, two or more models influence each
    other through intermediate real-world steps. In both situations, trust debts can
    cascade by amplifying problems in one or more aspects of trust.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈循环可能直接或间接导致技术信任债务。*直接反馈循环*发生在模型所采集数据的人群中，这些数据用于为模型的先前迭代提供推断服务([图7-3](#img-feedback))。而*间接*或*隐藏*的反馈循环则是指两个或更多模型通过中间的现实世界步骤相互影响。在这两种情况下，信任债务可以通过放大一个或多个信任方面的问题而级联扩展。
- en: '![ptml 0703](assets/ptml_0703.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![ptml 0703](assets/ptml_0703.png)'
- en: Figure 7-3\. Examples of direct feedback loops that can be present in a ML pipeline
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3\. ML管道中可能存在的直接反馈循环示例
- en: Monitoring
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 监控
- en: Technical trust debts can arise in any or all of the preceding categories during
    any part of an organization’s ML journey. As you deploy, update, and iterate upon
    more and more ML models, new needs may arise, while old ones go away. Thus, it
    is important not only to put a process in place to perform code and infrastructure
    due diligence but also to reevaluate that process periodically.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 技术信任债务可以在组织的ML旅程的任何部分或所有部分中产生。随着您部署、更新和迭代越来越多的ML模型，可能会出现新的需求，而旧的需求则可能消失。因此，不仅需要建立一个进行代码和基础设施尽职调查的流程，还需要定期重新评估该流程的有效性。
- en: Ethical debt
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 道德债务
- en: Nontechnical trust debt, often called *ethical debt*, arises when ML workflows
    and/or the business decisions guiding ML workflows are misaligned with ethical
    principles. Ethical debt is more dangerous than technical trust debt, in that
    its consequences can reach far beyond the organization and affect real people
    in serious, sometimes life-and-death ways. As cybersecurity engineer Catherine
    Petrozzino notes, “Unlike technical debt which is an a priori organizational decision,
    ethical debt is exacerbated by the reality that some ethical problems with AI
    solutions can only be detected after they are deployed.”^([15](ch07.html#idm45621831122656))
    Cases in point are the [death of Elaine Herzberg](https://oreil.ly/D5cLv) involving
    an Uber self-driving test vehicle and ML-based hiring algorithms continuing to
    exhibit demographic bias even when designed to combat the same problem.^([16](ch07.html#idm45621831120016))
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 非技术信任债务，通常称为*道德债务*，是在机器学习工作流程和/或指导机器学习工作流程的业务决策与伦理原则不一致时产生的。道德债务比技术信任债务更为危险，因为其后果可能远远超出组织范围，影响到真实人员，有时甚至涉及生死。正如网络安全工程师凯瑟琳·佩特罗齐诺指出，“与技术债务不同，道德债务加剧的原因在于，某些人工智能解决方案的伦理问题只能在部署后才能检测到。”^([15](ch07.html#idm45621831122656))
    比如，[Elaine Herzberg的死亡](https://oreil.ly/D5cLv)与Uber自动驾驶测试车有关，以及基于ML的招聘算法继续展现出人口统计偏见，即使设计用于解决相同问题也是如此。^([16](ch07.html#idm45621831120016))
- en: The study of ethical debt in ML is fairly new. Let’s look at the current treatment
    of this topic distilled down into a few categories:^([17](ch07.html#idm45621831117744))
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 研究ML中的道德债务相对较新。让我们将当前对这一主题的处理总结为几个类别:^([17](ch07.html#idm45621831117744))
- en: Assumption
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 假设
- en: Even well-intentioned, trustworthy ML implementations sometimes fall short of
    their goals due to faulty or out-of-context assumptions. For example, the disparate
    impact (DI) thresholds of 0.8 and 1.2 are informed by the [EEOC’s 80-20 rule](https://oreil.ly/IG3QC).
    This rule was originally proposed in the context of combating hiring discrimination,
    but today it is taken for granted in probably all applications of the DI metric
    in algorithmic fairness literature.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是出于良好意图的、值得信赖的机器学习实现有时也会由于错误的或上下文无关的假设而未能达到其目标。例如，不同影响（DI）阈值为0.8和1.2是基于[平等就业机会委员会的80-20规则](https://oreil.ly/IG3QC)。该规则最初是在打击招聘歧视的背景下提出的，但如今在算法公平性文献中DI指标的所有应用中都被视为理所当然。
- en: Post facto determination
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 后事实确定
- en: In the recent past, a number of ethical harms caused by ML applications have
    come to light as the people harmed have spoken up about their experiences. Adding
    elements of trust to ML-based solutions does make them less likely to occur. However,
    doing so reactively to plug known deficiencies, without proactively expanding
    or characterizing the application’s deficiencies still to be fixed, only leaves
    room for future mishaps.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的过去，由于受害者讲述他们的经历，ML应用造成的一些伦理伤害已经显露出来。将信任元素添加到基于ML的解决方案中确实会减少这些事件的发生。然而，仅仅对已知缺陷进行反应性补救而不积极扩展或描述应用程序尚待修复的缺陷，只会为未来的不幸留下空间。
- en: Human limitations
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 人类限制
- en: Organizational decision making is a collective and iterative process. The implementation
    details of an ML project are no exception. The collective judgment of human teams
    is often constrained by gaps in perception, lack of cultural diversity, and shortfalls
    in resources and education.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 组织决策是一个集体和迭代的过程。ML项目的实施细节也不例外。人类团队的集体判断往往受到感知差距、文化多样性不足以及资源和教育的短缺的限制。
- en: Automation bias
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化偏见
- en: Humans in the loop of automated decision-making systems are prone to *automation
    bias*, or overreliance on machine-generated outputs. This is specifically a problem
    in ML explainability, where the main goal is to ensure that explanations of an
    ML’s decisions “make sense” to the end user. Some studies published in 2020 and
    2021 have found that humans *do* tend to overly trust outputs from post hoc explanation
    methods and that it is possible to mislead users with rogue explanations.^([18](ch07.html#idm45621831095712))^,^([19](ch07.html#idm45621831093472))
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 自动决策系统中的人类往往容易出现*自动化偏见*，或者对机器生成的输出过度依赖。这在ML解释性方面特别是个问题，主要目标是确保ML决策的解释对最终用户“讲得通”。一些2020年和2021年发表的研究发现，人们确实倾向于过度信任事后解释方法的输出，而且可能通过不良解释误导用户。^([18](ch07.html#idm45621831095712))^,^([19](ch07.html#idm45621831093472))
- en: Paying off trust debt
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 清偿信任负债
- en: Formally measuring technical debt in ML is challenging. For ethical debt, what
    further complicates matters is the fact that those who are guilty of lackluster
    practices are not always the ones paying back the debt. End users are the people
    most affected by the negative consequences of less-than-trustworthy algorithms,
    and such consequences most often affect disadvantaged segments of the user base.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML中正式衡量技术债务具有挑战性。对于伦理债务，更复杂的是，那些犯下不良实践的人并不总是在偿还债务。最终用户是受不可信赖算法负面影响最多的人群，这些影响往往最常影响到处于劣势的用户群体。
- en: To some extent, general engineering best practices can help keep technical trust
    debt in check. This includes writing proper documentation, sharing knowledge,
    minimizing dependencies, and implementing tests to measure and adjust for the
    effects of adding new functionality to an ML-based solution. In particular, it’s
    helpful to create documentation focused on recording trust-specific considerations
    and learning from documented information about similar past projects. Let’s briefly
    discuss two such approaches here, but defer a detailed treatment to [Chapter 8](ch08.html#chapter8).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在一定程度上，一般的工程最佳实践可以帮助控制技术信任负债。这包括撰写适当的文档、分享知识、最小化依赖关系，并实施测试来衡量和调整为基于ML的解决方案添加新功能的影响。特别是，有助于创建专注于记录信任特定考虑因素并从类似过去项目的文档信息中学习的文档。让我们在这里简要讨论两种方法，但详细处理推迟到[第8章](ch08.html#chapter8)。
- en: Model cards
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 模型卡片
- en: '*Model cards* are structured documents that provide context for and transparency
    into the development and performance of an ML model.^([20](ch07.html#idm45621831080464))
    There are a variety of model card implementations out there, for example, the
    TensorFlow ecosystem’s [Model Card Toolkit](https://oreil.ly/XtBuH).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型卡*是结构化文档，提供ML模型开发和性能的背景和透明度。^([20](ch07.html#idm45621831080464)) 目前有多种模型卡实现，例如TensorFlow生态系统的[模型卡工具包](https://oreil.ly/XtBuH)。'
- en: Model cards are useful for B2C use cases, but they may not be sufficient for
    internal development teams. A growing company will be more concerned with how
    easily new details about the ML pipeline can be added without hassle. Model cards
    also focus mainly on the ML model, while ignoring the much larger ML pipeline
    surrounding the model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 模型卡对B2C用例很有用，但对内部开发团队可能不够。不断发展的公司更关注如何轻松添加关于ML管道的新细节。模型卡主要关注ML模型，而忽视围绕模型的更大的ML管道。
- en: What you as a developer want is not only ML model cards, but also ML directed
    acyclic graphs (DAGs). This would be a much more detailed way of describing the
    network of dependencies as information flows through the pipeline and reaches
    the various decision points. For an implementation of DAGs in model documentation
    and maintenance, check out [DAG cards](https://oreil.ly/iaj8O), which build upon
    model cards with dependency graphs built using [Metaflow](https://metaflow.org)
    and [Weights and Biases](https://wandb.ai/site).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开发者，您不仅需要ML模型卡，还需要ML有向无环图（DAGs）。这将更详细地描述信息通过管道流动并到达各种决策点的依赖网络。关于模型文档和维护中的DAGs的实现，请查看使用[Metaflow](https://metaflow.org)和[Weights
    and Biases](https://wandb.ai/site)构建的[DAG卡](https://oreil.ly/iaj8O)。
- en: Important Aspects of Trust
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信任的重要方面
- en: So, your team has managed to convince stakeholders that not only does their
    product’s ML algorithm need to be accurate, but it is also important to do due
    diligence regarding trust. Congratulations! Now, how will you decide which aspects
    to prioritize in your specific project? Do you need your algorithm to be fair?
    Explainable? Private? Robust? Secure? All of these? Some of them? How much fairness,
    explainability, etc. do you need? Let’s talk a bit about how to move forward in
    this situation.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您的团队已经成功说服利益相关者，他们的产品ML算法不仅需要准确，还需要进行信任的尽职调查。恭喜！现在，您如何决定在特定项目中优先考虑哪些方面？您的算法需要公平吗？可解释吗？私密吗？健壮吗？安全吗？全部？部分？您需要多少公平性、可解释性等？让我们谈一谈在这种情况下如何前进。
- en: 'This decision-making process has three steps: assessing your needs to decide
    which aspects of trust to prioritize, deciding on metrics for each aspect, and
    deciding on thresholds for those metrics.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决策过程有三个步骤：评估您的需求以决定优先考虑信任的哪些方面，为每个方面确定指标，以及确定这些指标的阈值。
- en: 'The first step, assessing needs, means weighing your project’s need for each
    aspect of trust and categorizing the needs by priority. Let’s consider three categories
    of needs:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，评估需求，意味着权衡项目对每个信任方面的需求，并按优先级分类这些需求。让我们考虑三类需求：
- en: '*High need*'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*高需求*'
- en: It is absolutely essential for the algorithm to be fair, explainable, private,
    robust, and safe.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 算法公平、可解释、私密、健壮和安全绝对至关重要。
- en: '*Medium need*'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*中等需求*'
- en: It would be nice for the algorithm to be fair, explainable, private, robust,
    and safe.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 算法公平、可解释、私密、健壮和安全会很好。
- en: '*Low need*'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*低需求*'
- en: It is not too important for the algorithm to be fair, explainable, private,
    robust, and safe.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 算法公平、可解释、私密、健壮和安全并不是太重要。
- en: The best way to figure this out is to ask a number of questions. Some of these
    questions are general; others are specific to one or more trust elements. See
    [Table 7-2](#table-trust-questions) for an example list of questions. Not every
    company or product has the same priorities, so feel free to take, modify, and
    expand parts of this table to suit your own context.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 弄清楚这一点的最佳方法是提出一些问题。其中一些问题是通用的，其他则是特定于一个或多个信任元素。有关示例问题列表，请参见[表格 7-2](#table-trust-questions)。并非每家公司或产品的优先级都相同，因此请随时根据您自己的情况采取、修改和扩展这个表格的部分。
- en: Table 7-2\. Sample questions for needs assessment of trust elements with relevant
    trust elements tick-marked if the answer is *yes*
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7-2\. 用于信任元素需求评估的示例问题列表，如果答案是*是*，则相关的信任元素会打勾。
- en: '| Question | Fairness | Explainability | Privacy | Robustness | Safety |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 公平性 | 可解释性 | 隐私性 | 健壮性 | 安全性 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Is there a regulation or law that applies to this use case? | ![16](assets/tick.png)
    | ![16](assets/tick.png) | ![16](assets/tick.png) | ![16](assets/tick.png) | ![16](assets/tick.png)
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 是否有适用于此使用案例的法规或法律？ | ![16](assets/tick.png) | ![16](assets/tick.png) | ![16](assets/tick.png)
    | ![16](assets/tick.png) | ![16](assets/tick.png) |'
- en: '| Will we use data on individuals? | ![16](assets/tick.png) | ![16](assets/tick.png)
    | ![16](assets/tick.png) |  | ![16](assets/tick.png) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 我们会使用个人数据吗？ | ![16](assets/tick.png) | ![16](assets/tick.png) | ![16](assets/tick.png)
    |  | ![16](assets/tick.png) |'
- en: '| Are there disadvantaged segments of the population who may be disparately
    impacted by the biased outputs of this model? | ![16](assets/tick.png) |  |  |  |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 是否有受影响的人群分部会因为模型的偏见输出而受到不公平的影响？ | ![16](assets/tick.png) |  |  |  |  |'
- en: '| Are there humans who need to understand how this model functions? |  | ![16](assets/tick.png)
    |  |  |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 是否有需要了解这个模型功能的人类？ |  | ![16](assets/tick.png) |  |  |  |'
- en: '| Are there any potential data quality issues? | ![16](assets/tick.png) | ![16](assets/tick.png)
    |  | ![16](assets/tick.png) |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 是否存在任何潜在的数据质量问题？ | ![16](assets/tick.png) | ![16](assets/tick.png) |  | ![16](assets/tick.png)
    |  |'
- en: '| Do we know of attacks where data or model components could be exposed in
    an unintended manner? |  |  | ![16](assets/tick.png) | ![16](assets/tick.png)
    | ![16](assets/tick.png) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 我们是否知道有攻击可能会使数据或模型组件以意外方式暴露？ |  |  | ![16](assets/tick.png) | ![16](assets/tick.png)
    | ![16](assets/tick.png) |'
- en: Let’s consider an example. Suppose you are working for an educational technology
    (EdTech) company in the US, building a product that automatically grades individuals’
    writing samples. The US government’s [Office of Civil Rights prohibits discrimination](https://oreil.ly/jY6Iw)
    on the basis of race, color, national origin, sex, disability, and age. While
    your company does not directly use individuals’ demographic data to grade their
    writing, there is still potential for discrimination. Studies have shown that
    automated grading systems can be fooled by text that uses sophisticated words
    but poor writing quality.^([21](ch07.html#idm45621831020192)) In such situations,
    people whose educational background may not have exposed them to such words may
    be at a disadvantage.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子。假设你在美国一家教育技术（EdTech）公司工作，正在开发一款能够自动评分个人写作样本的产品。美国政府的[民权办公室禁止基于种族、肤色、国籍、性别、残疾和年龄的歧视](https://oreil.ly/jY6Iw)。虽然你的公司并不直接使用个人的人口统计数据来评分他们的写作，但仍存在歧视的潜力。研究表明，自动评分系统可能会受到使用复杂词汇但写作质量不佳的文本所欺骗。在这种情况下，那些教育背景未曾接触到这些词汇的人可能处于不利地位。
- en: It is important to ensure that models are being trained on examples where quality
    of writing is not so heavily correlated with the number of syllables in each word
    or, better yet, on examples in which large words are misused as [malapropisms](https://oreil.ly/cO5Kw).
    It would be helpful to have a human in the loop to spot places where the writer’s
    use of large words not only does not contribute to the essay’s quality but actively
    detracts from it.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 确保模型在例子上训练时重要的一点是，写作质量不应过分与每个词的音节数相关，或者更好的情况是，训练在大词被误用为[滑稽错误](https://oreil.ly/cO5Kw)的例子上。最好能够有人参与以发现作家使用大词不仅不提升文章质量，反而降低其质量的地方。
- en: 'In this example, the trust elements with the highest need are:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，最需要的信任要素包括：
- en: '*Fairness* across writing samples collected across relevant demographic categories'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*公正性*，跨相关人口统计类别收集的写作样本'
- en: '*Robustness* to changes in vernacular and level of vocabulary sophistication'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对于方言和词汇复杂程度的变化具有鲁棒性*'
- en: '*Explainability*, so that teachers and students are able to understand how
    to improve their writing'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释性*，以便教师和学生能够理解如何改善他们的写作'
- en: After you’ve assessed needs, the next phase is to decide on metrics and their
    thresholds. This is very specific to your domain of application and use case.
    Generally, your stakeholders should have the requisite domain knowledge to ascertain
    which metrics would be most useful.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估需求后，下一步是决定指标及其阈值。这对于您的应用领域和使用案例非常具体。通常来说，您的利益相关者应该具备领域知识，以确定哪些指标最为有用。
- en: A good general rule of thumb is to first check to see if any default rules of
    the domain apply. If not, use statistical significance tests. In our disparate
    impact (DI) example, for an ML project in the hiring domain, the EEOC 80-20 rule
    applies. However, for other applications of DI, you could check if it is extreme
    enough to warrant fairness concerns. You might use bootstrap sampling or permutation
    to generate a null distribution for DI, then obtain the *p*-value for the computed
    metric with respect to that null distribution.^([22](ch07.html#idm45621831006480))
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的一般规则是首先检查领域中是否适用任何默认规则。如果没有，请使用统计显著性检验。在我们的不平等影响（DI）示例中，对于招聘领域的 ML 项目，适用
    EEOC 80-20 规则。然而，对于 DI 的其他应用，可以检查其是否足够极端，从而引起公平性关切。您可以使用自举抽样或置换来生成 DI 的零假设分布，然后针对该零假设分布获取计算度量的
    *p* 值。^([22](ch07.html#idm45621831006480))
- en: Evaluation and Feedback
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估和反馈
- en: 'After everything is said and done, you need to evaluate if, and how successfully,
    you incorporated the trust elements into your analysis. This is a bit more involved
    than just checking that acceptable ranges or thresholds for all relevant metrics
    are satisfied. Some element-specific considerations that you may need to tackle
    are as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有事情都解决之后，您需要评估您如何成功地将信任元素纳入您的分析中。这比仅仅检查所有相关指标的可接受范围或阈值是否满足要复杂一些。您可能需要解决一些特定于元素的考虑，如下：
- en: Fairness
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性
- en: Choose a pre-processing, in-processing, or post-processing mitigation technique
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择预处理、处理中或后处理的缓解技术
- en: Perform risk assessment of proceeding with or without mitigation of demographic
    bias at the data, model, or prediction level
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对是否进行人口统计偏见的数据、模型或预测级别的缓解进行风险评估
- en: Ensure that unbiasing the algorithm for specific user segments does not introduce
    additional bias
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保为特定用户段调整算法不会引入额外的偏见
- en: Explainability
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性
- en: Stakeholder feedback regarding effectiveness of the explainability techniques
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利益相关者对可解释性技术效果的反馈
- en: Ensuring that the explanations are actually augmenting the expertise of human
    SMEs, i.e., they are not suffering automation bias by being overreliant on the
    explanations
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保解释实际上增强了人类专家的专业知识，即它们不会因过度依赖解释而遭受自动化偏见
- en: Privacy
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护
- en: Choice of modeling step to introduce differential privacy (DP) noise and the
    optimal amount of DP noise for privacy-utility trade-off
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 选择引入差分隐私（DP）噪音的建模步骤以及隐私效用权衡的最佳 DP 噪音量
- en: Robustness and safety
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性和安全性
- en: Making sure that robust methods that protect against certain adversarial attacks
    do not make the ML system more vulnerable against other relevant attacks
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保保护免受特定对抗性攻击的鲁棒方法不会使 ML 系统更容易受到其他相关攻击
- en: Prioritizing robustness/safety specifications and ensuring high-priority specifications
    are given higher weights or always satisfied
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先考虑鲁棒性/安全性规范，并确保高优先级规范获得更高权重或始终被满足
- en: Trustworthiness and MLOps
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可信度和 MLOps
- en: It’s important to incorporate trust elements into your ML pipelines, but it’s
    also important to ensure that your pipelines stay functional over time, even under
    changing configurations. Thus, in this section, we discuss a number of engineering
    aspects of trust that can complicate things, including challenges around scaling,
    data drift, and model monitoring and observability.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 将信任元素纳入 ML 流水线是很重要的，但同时也要确保您的流水线在随时间变化的配置下保持功能正常。因此，在本节中，我们讨论了一些会复杂化事情的信任工程方面，包括扩展、数据漂移以及模型监控和可观察性方面的挑战。
- en: Scaling challenges
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展挑战
- en: Beyond logistical limitations, your ability to incorporate trust elements into
    ML pipelines may be constrained by how much computing power your company can realistically
    afford. For example, both the in-processing techniques discussed in [Chapter 1](ch01.html#chapter1)
    are quite computationally involved. For adversarial debiasing, this is a result
    of the iterative optimization of two neural networks. For HGR, the culprit is
    the computation-intensive kernel density calculation, to calculate the joint density
    of the output and sensitive feature(s). Among explainability methods, exact calculation
    of SHAP values is notoriously computation-intensive.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 除了后勤限制之外，您将信任元素纳入ML管道的能力可能受到公司实际可以承受的计算能力的限制。例如，讨论的两种处理技术在计算上都非常复杂。对于对抗性去偏差，这是两个神经网络的迭代优化结果。对于HGR，罪魁祸首是计算密集的核密度计算，用于计算输出和敏感特征的联合密度。在可解释性方法中，计算SHAP值的精确计算因计算密集而臭名昭著。
- en: When faced with scaling challenges, look around and get creative! Oftentimes
    you’ll find a solution in the literature or open source community that can solve
    your problem. For example, FastSHAP provides fast, but approximate, computation
    of Shapley values.^([23](ch07.html#idm45621830978528)) Instead of using a non-scalable
    Python library, try writing your own code in Scala—or use a package like [LiFT](https://oreil.ly/n5Ll3)—to
    perform fairness evaluation for large volumes of data.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 面对扩展挑战时，环顾四周，发挥创造力！往往你会在文献或开源社区中找到解决方案，可以解决你的问题。例如，FastSHAP提供了快速但近似的Shapley值计算。^([23](ch07.html#idm45621830978528))
    与其使用不可扩展的Python库，不如尝试用Scala编写自己的代码，或者使用类似[LiFT](https://oreil.ly/n5Ll3)的包，在大量数据的情况下执行公平性评估。
- en: Data drift
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据漂移
- en: Aspects of incoming data used for building an ML model may change over time.
    For example, features may be deleted or added, new categories may be added to
    a categorical feature, data quality might improve or deteriorate, and distributions
    of one or more key features may shift. You need to have a process in place for
    continuously integrating such changes into the model-training and evaluation cycle—and
    that extends to trust elements as well. Shifted or otherwise modified features
    affect a trust metric’s estimated value *and* estimation uncertainty.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 用于构建ML模型的入站数据的各个方面可能随时间变化。例如，特征可能会被删除或添加，分类特征可能会添加新的类别，数据质量可能会改善或恶化，一个或多个关键特征的分布可能会发生变化。您需要建立一个流程，持续将这些变化整合到模型训练和评估周期中，并且这也涉及到信任元素。变化或修改后的特征会影响信任度量的估计值
    *以及* 估计不确定性。
- en: Model monitoring and observability
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型监控与可观察性
- en: These two well-known concepts in DevOps are equally relevant for MLOps.^([24](ch07.html#idm45621830966240))
    Broadly speaking, *monitoring* refers to collecting—often through sampling—metrics,
    logs, or other artifacts from software systems to enable post hoc analysis and
    learning. In comparison, *observability* is a property of the software system
    itself. It allows the system to make key structured artifacts available to the
    development team so they can take meaningful action quickly to troubleshoot the
    system.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个在DevOps中广为人知的概念对MLOps同样适用。^([24](ch07.html#idm45621830966240)) 一般来说，*监控*
    指的是通过抽样收集软件系统中的指标、日志或其他文物，以支持事后分析和学习。相比之下，*可观察性* 是软件系统本身的属性。它使得系统可以向开发团队提供关键的结构化文物，以便他们能够快速采取有意义的行动来排查系统问题。
- en: Method-agnostic monitoring of trust elements in ML models broadly corresponds
    to computing and recording metrics for every training and/or inference cycle (feature
    importance, bias metrics, and explanations of certain representative data points),
    and further analysis of the timelines of one or more such metrics, to troubleshoot
    pipeline issues in future iterations of training and inference.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 与ML模型中信任元素的方法无关的监控广泛对应于计算并记录每个训练和/或推理周期的指标（特征重要性、偏见指标和某些代表性数据点的解释），并进一步分析一个或多个此类指标的时间线，以排除将来训练和推理迭代中的管道问题。
- en: By contrast, observability in trust metrics is more proactive. Specific combinations
    of trust elements can be combined with one or more technical factors—such as causality,
    sparsity, and uncertainty quantification—to design focused resources. In the context
    of the EdTech example earlier, say you want to measure the fairness of predictions
    toward a demographic group of interest, because there is a known insufficient
    sample issue for that demographic group. When you calculate the bias metric, you
    would do well to monitor uncertainty. This is an example of trust element observability.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在信任度量中的可观察性更具前瞻性。特定的信任元素组合可以与一个或多个技术因素结合使用——比如因果关系、稀疏性和不确定性量化——来设计专注的资源。在之前的EdTech示例中，假设你想要衡量对一个人群利益的预测公平性，因为已知该人群的样本不足问题。当你计算偏差度量时，最好监控不确定性。这是信任元素可观察性的一个例子。
- en: Techniques
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 技术
- en: We’ll wrap this chapter up by introducing the basics of a few methods that are
    helpful in tackling drift and other quality issues in data or model metrics, enabling
    monitoring and observability.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过介绍几种方法的基础来结束这一章，这些方法有助于解决数据或模型指标中的漂移和其他质量问题，从而实现监控和可观察性。
- en: Anomaly detection
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异常检测
- en: Think of *anomalies* as outliers with context. They are not only rare events
    in a stream of otherwise normal data points but often represent problems of an
    underlying feature. For example, if the five most recent test datasets show a
    sudden drop in grading accuracy among a sensitive demographic group, this most
    likely points to a broader issue about the quality of the writing samples in newer
    datasets. If you have only one outlier, however, it may or may not be just a rare
    observation as part of a *normal* data stream.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 将*异常*视为带有背景信息的异常值。它们不仅是在通常数据点流中的罕见事件，而且经常代表潜在特征问题。例如，如果最近的五个测试数据集显示某一敏感人群的评分准确性突然下降，这很可能指向新数据集中写作样本质量的更广泛问题。然而，如果只有一个异常值，它可能只是正常数据流中的一个罕见观察结果，也可能不是。
- en: 'Here are two popular methods of anomaly detection:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是两种流行的异常检测方法：
- en: '*STL* (seasonal-trend decomposition using Loess) fits a Loess smoother to time
    series data to remove temporal components such as seasonality and trend, then
    examines the residuals to designate points above or below certain thresholds as
    anomalies. The thresholds are set at a certain multiple of the interquartile range
    (or IQR, defined as the difference between the 75th and 25th percentiles).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*STL*（Loess季节趋势分解）将Loess平滑器拟合到时间序列数据中，以移除季节性和趋势等时间组成部分，然后检查残差以确定高于或低于某些阈值的点是否为异常值。阈值设置为四分位距（IQR）的某个倍数（IQR定义为第75百分位数与第25百分位数之差）。'
- en: The *generalized extreme studentized deviate* (GESD) test iteratively adjusts
    detection thresholds by removing higher-percentile sample points from the data
    considered for threshold computation. GESD is more computation-intensive than
    STL, but less prone to false positives.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*广义极端学生化偏差*（GESD）测试通过从考虑用于阈值计算的数据中移除更高百分位数的样本点来迭代调整检测阈值。GESD比STL更消耗计算资源，但误报率较低。'
- en: Change point detection
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 变点检测
- en: 'Change point detection methods aim to detect the points within a time-series
    dataset where the data-generating process itself changes. This is a more permanent
    change than anomaly detection. There are two types of change point detection methods:
    offline and online. Offline methods do post hoc analysis of the time series, while
    online methods detect change points in a data *stream*. As you can guess, online
    change point detection in data/model metrics is very relevant to the MLOps problems
    we have discussed, in particular when observability is coupled with knowledge
    of the parameters that can cause such changes.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 变点检测方法旨在检测时间序列数据集中数据生成过程本身发生变化的点。这种变化比异常检测更为永久。有两种变点检测方法：离线和在线。离线方法对时间序列进行事后分析，而在线方法检测数据流中的变点。正如你可以猜到的那样，数据/模型指标中的在线变点检测与我们讨论过的MLOps问题非常相关，特别是当可观察性与可能导致这些变化的参数的知识结合在一起时。
- en: Control charts
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 控制图
- en: 'Statistical process control charts have been a mainstay of industrial quality
    control methods for decades. Control charts incorporate even more causal knowledge
    into the timeline being analyzed than do anomaly or change point detection.^([25](ch07.html#idm45621830937216))
    To put it simply, a number of well-defined metrics are calculated for subgroups
    of interest and tracked over time. These could include:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 数十年来，统计过程控制图表一直是工业质量控制方法的支柱。控制图表在分析时间线时比异常或变更点检测包含更多因果知识。^([25](ch07.html#idm45621830937216))
    简而言之，为感兴趣的子群体计算多个明确定义的指标，并随时间跟踪这些指标。这些指标可以包括：
- en: Mean and standard deviation ( <math alttext="x overbar"><mover accent="true"><mi>x</mi>
    <mo>¯</mo></mover></math> and *s* chart)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值和标准差（ <math alttext="x overbar"><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></math>
    和 *s* 图表）
- en: Fraction of positive/negative predictions (*p* and *np* chart)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正负预测的比例（*p* 和 *np* 图表）
- en: The combination of multiple time points through a moving average (EWMA chart)
    or cumulative sums (CUSUM chart)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过移动平均线（EWMA图表）或累积和（CUSUM图表）结合多个时间点
- en: Custom aggregate metrics (see example in [Table 7-3](#table-control))
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义聚合指标（见[表 7-3](#table-control)中的示例）
- en: In the trusted MLOps context of the EdTech example, control charts can be used
    to ensure quality of *trustworthiness* of ML model outcomes. See [Table 7-3](#table-control)
    for sample metrics that can be tracked across time for this purpose.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在教育科技示例的可信MLOps背景中，控制图表可用于确保ML模型结果的*可信度*质量。详见[表 7-3](#table-control)中，为此目的跨时间跟踪的示例指标。
- en: Table 7-3\. Example metrics to be tracked to ensure quality control of trust
    elements
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表7-3\. 示例指标，用于跟踪信任元素的质量控制
- en: '| Trust element | Additional factor | Metric | Aggregation |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 信任元素 | 附加因素 | 指标 | 聚合 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Fairness |  | Disparate impact | Custom: ratio of proportions over sensitive
    versus non-sensitive subgroups |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 公平性 |  | 不平等影响 | 自定义：敏感与非敏感子群体比例的比率 |'
- en: '| Fairness |  | Statistical parity | Mean over all samples |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 公平性 |  | 统计平等性 | 所有样本的平均值 |'
- en: '| Robustness |  | Prediction difference in presence and absence of one or more
    sophisticated words | Mean over all samples |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 鲁棒性 |  | 存在一个或多个复杂词语时预测差异 | 所有样本的平均值 |'
- en: '| Robustness | Uncertainty | Prediction difference in presence and absence
    of one or more sophisticated words | SD over all samples |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 鲁棒性 | 不确定性 | 存在一个或多个复杂词语时预测差异 | 所有样本的标准差 |'
- en: '| Explainability |  | LIME feature importance difference in presence and absence
    of a feature value | Mean over all samples |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 可解释性 |  | LIME特征重要性在存在和不存在特征值时的差异 | 所有样本的平均值 |'
- en: '| Explainability | Uncertainty | LIME feature importance difference in presence
    and absence of a feature value | SD over all samples |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 可解释性 | 不确定性 | LIME特征重要性在存在和不存在特征值时的差异 | 所有样本的标准差 |'
- en: '| Explainability | Causality | Difference in counterfactual explanations |
    Mean over all samples |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 可解释性 | 因果性 | 反事实解释的差异 | 所有样本的平均值 |'
- en: '| Explainability and fairness |  | LIME feature importance difference in presence
    and absence of a feature value | Difference of means over sensitive and non-sensitive
    samples |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 可解释性和公平性 |  | LIME特征重要性在存在和不存在特征值时的差异 | 对敏感和非敏感样本的平均差异 |'
- en: Tip
  id: totrans-282
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: As an exercise, try to design metrics for quality control of more than one trust
    element, just like the last row in [Table 7-3](#table-control). Give it a try!
    After you come up with some examples, try to augment additional factors, too.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，尝试设计用于多个信任元素质量控制的指标，就像[表 7-3](#table-control)中的最后一行。试试吧！在提出一些示例后，尝试增加附加因素。
- en: Conclusion
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: When implementing real-world ML projects, there’s more to consider than deciding,
    coding up, and evaluating trust metrics. This chapter has covered several essential
    technical and nontechnical considerations.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施真实世界的ML项目时，除了决策、编码和评估信任度指标外，还需要考虑更多因素。本章涵盖了几个重要的技术和非技术考虑事项。
- en: The technical considerations include evaluating cause and effect relationships,
    compressing your model, and estimating uncertainty when necessary. Also, don’t
    forget to check for how these considerations interact with trust elements!
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 技术考虑事项包括评估因果关系、压缩模型以及必要时估算不确定性。还要记得检查这些考虑事项如何与信任元素互动！
- en: You should always, always get buy-in from stakeholders beyond the ML team. As
    you work toward this, remember that technical and ethical debt management and
    risk management are two key reasons trustworthy ML methods are important to the
    broader organization.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '-   总是确保从ML团队以外的利益相关者那里获得支持。在此过程中，请记住，技术和伦理债务管理以及风险管理是促使值得信赖的ML方法对更广泛组织重要的两个关键原因。'
- en: Finally, be mindful of scaling issues in large-scale trustworthy ML implementations.
    Even after you deploy the system in production, it’s a good idea to continue monitoring
    and troubleshooting.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '-   最后，要注意大规模值得信赖的ML实施中的规模问题。即使在将系统投入生产后，继续监控和解决问题也是一个好主意。'
- en: ^([1](ch07.html#idm45621832115168-marker)) Check out Judea Pearl’s books for
    deeper dives into causal inference, such as [*The Book of Why*](https://oreil.ly/LUcZh)
    (Basic Books, 2018).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.html#idm45621832115168-marker)) 深入研究因果推断的书籍，例如Judea Pearl的著作 [*“为什么的书”*](https://oreil.ly/LUcZh)
    (Basic Books, 2018)。
- en: ^([2](ch07.html#idm45621831819760-marker)) Christoph Molnar gives a great explanation
    and examples of counterfactual explanations in his [book on interpretable ML](https://oreil.ly/JvCHc).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.html#idm45621831819760-marker)) Christoph Molnar 在他的 [“可解释机器学习”](https://oreil.ly/JvCHc)
    一书中对反事实解释进行了深入解释和举例。
- en: ^([3](ch07.html#idm45621831818272-marker)) Matt Kusner et al., [“Counterfactual
    Fairness”](https://oreil.ly/I2r9t), *NeurIPS-2017* (2017); Timothy Christensen
    and Benjamin Connault, [“Counterfactual Sensitivity and Robustness”](https://arxiv.org/abs/1904.00989),
    *arXiv preprint* (2019).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch07.html#idm45621831818272-marker)) Matt Kusner 等人的研究，[“反事实公平性”](https://oreil.ly/I2r9t)，*NeurIPS-2017*
    (2017)；Timothy Christensen 和 Benjamin Connault 的研究，[“反事实敏感性和鲁棒性”](https://arxiv.org/abs/1904.00989)，*arXiv
    预印本* (2019)。
- en: ^([4](ch07.html#idm45621831716192-marker)) For more technical details on specific
    methods, we recommend the review papers by [R. Reed](https://oreil.ly/TcUjt) and
    [Davis Blalock et al.](https://oreil.ly/0VS7R), as well as the papers in their
    references.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch07.html#idm45621831716192-marker)) 如需更多关于特定方法的技术细节，请参阅[R. Reed](https://oreil.ly/TcUjt)及其合著者的综述论文，以及参考文献中的论文。
- en: '^([5](ch07.html#idm45621831705184-marker)) Robert Tibshirani, [“Regression
    Shrinkage and Selection via the Lasso”](https://oreil.ly/noLVr), *Journal of the
    Royal Statistical Society* Series B 58, no. 1 (1996): 267–88.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '^([5](ch07.html#idm45621831705184-marker)) Robert Tibshirani 的研究，[“Lasso回归收缩和选择”](https://oreil.ly/noLVr)，*Royal
    Statistical Society B系列期刊* 58卷1期 (1996): 267–88。'
- en: '^([6](ch07.html#idm45621831663760-marker)) You can see an intuitive visualization
    of some of these concepts in Uber AI’s ICML 2019 poster [“Deconstructing Lottery
    Tickets: Zeros, Signs, and the Supermask”](https://oreil.ly/iLQ9e).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch07.html#idm45621831663760-marker)) 您可以在Uber AI的ICML 2019海报 [“解构彩票：零、符号和超蒙版”](https://oreil.ly/iLQ9e)
    中直观地看到这些概念的可视化。
- en: ^([7](ch07.html#idm45621831656864-marker)) For example, check out the recent
    papers by [Selima Curci et al.](https://arxiv.org/abs/2102.01732) and [Shiwei
    Liu et al.](https://doi.org/10.1007/s00521-021-05727-y) that have code available
    in GitHub.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.html#idm45621831656864-marker)) 例如，查看Selima Curci等人以及Shiwei Liu等人的最新论文，这些论文在GitHub上有可用的代码。
- en: ^([8](ch07.html#idm45621831645600-marker)) Sara Hooker et al., [“What Do Compressed
    Deep Neural Networks Forget?”](https://arxiv.org/abs/1911.05248), *arXiv preprint*
    (2019).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch07.html#idm45621831645600-marker)) Sara Hooker 等人的研究，[“压缩深度神经网络忘却了什么？”](https://arxiv.org/abs/1911.05248)，*arXiv
    预印本* (2019)。
- en: ^([9](ch07.html#idm45621831643664-marker)) Sara Hooker et al., [“Characterising
    Bias in Compressed Models”](https://arxiv.org/abs/2010.03058), *arXiv preprint*
    (2020).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch07.html#idm45621831643664-marker)) Sara Hooker 等人的研究，[“压缩模型中的偏差表征”](https://arxiv.org/abs/2010.03058)，*arXiv
    预印本* (2020)。
- en: '^([10](ch07.html#idm45621831625888-marker)) Abhaya Indrayan, *Medical Biostatistics*
    (Boca Raton, FL: CRC Press, 2008).'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '^([10](ch07.html#idm45621831625888-marker)) Abhaya Indrayan 的著作，*医学生物统计学* (Boca
    Raton, FL: CRC Press, 2008)。'
- en: ^([11](ch07.html#idm45621831208160-marker)) Ward Cunningham, [“The WyCash Portfolio
    Management System”](https://oreil.ly/E5k5J), *OOPSLA ’92 Experience Report*, March
    26, 1992.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch07.html#idm45621831208160-marker)) Ward Cunningham 的报告，[“WyCash 投资组合管理系统”](https://oreil.ly/E5k5J)，*OOPSLA
    ’92 经验报告*，1992年3月26日。
- en: ^([12](ch07.html#idm45621831185904-marker)) For a detailed treatment of technical
    debt in ML, see D. Sculley et al., [“Hidden Technical Debt in Machine Learning
    Systems”](https://oreil.ly/GtJ3E), *NeurIPS Proceedings* (2015). For a technical
    perspective on this paper and debt management best practices for a modern ML workflow,
    see author Matthew McAteer’s [blog post, “Nitpicking Machine Learning Technical
    Debt”](https://oreil.ly/M6Yix) from 2020.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch07.html#idm45621831185904-marker)) 关于机器学习中技术债务的详细讨论，请参阅 D. Sculley
    等人的[“Hidden Technical Debt in Machine Learning Systems”](https://oreil.ly/GtJ3E)，*NeurIPS
    Proceedings* (2015)。关于本文和现代机器学习工作流中债务管理的技术视角，请参阅作者 Matthew McAteer 在2020年的[博客文章，“Nitpicking
    Machine Learning Technical Debt”](https://oreil.ly/M6Yix)。
- en: '^([13](ch07.html#idm45621831158800-marker)) McKane Andrus et al., [“What We
    Can’t Measure, We Can’t Understand: Challenges to Demographic Data Procurement
    in the Pursuit of Fairness”](https://dl.acm.org/doi/10.1145/3442188.3445888),
    *FaccT-2021* (March 2021): 249–60.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '^([13](ch07.html#idm45621831158800-marker)) McKane Andrus 等人，在[“What We Can’t
    Measure, We Can’t Understand: Challenges to Demographic Data Procurement in the
    Pursuit of Fairness”](https://dl.acm.org/doi/10.1145/3442188.3445888)，*FaccT-2021*
    (2021年3月): 249–60。'
- en: ^([14](ch07.html#idm45621831148064-marker)) Matthieu Renard, [“One of The Most
    Common Mistakes When Running an ANOVA in R”](https://oreil.ly/0UyT7), *Towards
    Data Science* (blog), January 2, 2020.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch07.html#idm45621831148064-marker)) Matthieu Renard，在[“One of The Most
    Common Mistakes When Running an ANOVA in R”](https://oreil.ly/0UyT7)，*Towards
    Data Science* (博客)，2020年1月2日。
- en: '^([15](ch07.html#idm45621831122656-marker)) Catherine Petrozzino, [“Who Pays
    for Ethical Debt in AI?”](https://oreil.ly/E31jV), *AI and Ethics* 1 (January
    2021): 205–8.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '^([15](ch07.html#idm45621831122656-marker)) Catherine Petrozzino，在[“Who Pays
    for Ethical Debt in AI?”](https://oreil.ly/E31jV)，*AI and Ethics* 1 (2021年1月):
    205–8。'
- en: ^([16](ch07.html#idm45621831120016-marker)) Miranda Bogen, [“All the Ways Hiring
    Algorithms Can Introduce Bias”](https://oreil.ly/BTR1X), *Harvard Business Review*,
    May 6, 2019.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch07.html#idm45621831120016-marker)) Miranda Bogen，在[“All the Ways Hiring
    Algorithms Can Introduce Bias”](https://oreil.ly/BTR1X)，*Harvard Business Review*，2019年5月6日。
- en: ^([17](ch07.html#idm45621831117744-marker)) For a detailed treatment of the
    motivations, see Casey Fiesler and Natalie Garrett, [“Ethical Tech Starts With
    Addressing Ethical Debt”](https://oreil.ly/ihDM6), *Wired*, September 16, 2020
    and Petrozzino, “Who Pays for Ethical Debt in AI?” 205–8.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch07.html#idm45621831117744-marker)) 关于动机的详细讨论，请参阅 Casey Fiesler 和 Natalie
    Garrett 的[“Ethical Tech Starts With Addressing Ethical Debt”](https://oreil.ly/ihDM6)，*Wired*，2020年9月16日
    和 Petrozzino 的“Who Pays for Ethical Debt in AI?” 205–8。
- en: '^([18](ch07.html#idm45621831095712-marker)) In [“‘How Do I Fool You?’: Manipulating
    User Trust via Misleading Black Box Explanations”](https://dl.acm.org/doi/10.1145/3375627.3375833),
    *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society* (February
    2020): 79–85, Himabindu Lakkaraju and Osbert Bastani show that, just like any
    other ML method, explanation methods can be fooled by adversaries.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '^([18](ch07.html#idm45621831095712-marker)) 在[“‘How Do I Fool You?’: Manipulating
    User Trust via Misleading Black Box Explanations”](https://dl.acm.org/doi/10.1145/3375627.3375833)，Himabindu
    Lakkaraju 和 Osbert Bastani 在 *Proceedings of the AAAI/ACM Conference on AI, Ethics,
    and Society* (2020年2月): 79–85 中表明，像其他任何机器学习方法一样，解释方法也容易受到对手的欺骗。'
- en: '^([19](ch07.html#idm45621831093472-marker)) To learn about how humans perceive
    the outcomes from an explanation method, see Harmanpreet Kaur et al., [“Interpreting
    Interpretability: Understanding Data Scientists’ Use of Interpretability Tools
    for Machine Learning”](https://dl.acm.org/doi/abs/10.1145/3313831.3376219), *Proceedings
    of the 2020 CHI Conference on Human Factors in Computing Systems* (April 2020):
    1–14 and Forough Poursabzi-Sangdeh et al., [“Manipulating and Measuring Model
    Interpretability”](https://dl.acm.org/doi/10.1145/3411764.3445315), *Proceedings
    of the 2021 CHI Conference on Human Factors in Computing Systems*, no. 237 (May
    2021): 1–52.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '^([19](ch07.html#idm45621831093472-marker)) 关于人类如何理解解释方法的结果，请参阅 Harmanpreet
    Kaur 等人的[“Interpreting Interpretability: Understanding Data Scientists’ Use of
    Interpretability Tools for Machine Learning”](https://dl.acm.org/doi/abs/10.1145/3313831.3376219)，*Proceedings
    of the 2020 CHI Conference on Human Factors in Computing Systems* (2020年4月): 1–14
    和 Forough Poursabzi-Sangdeh 等人的[“Manipulating and Measuring Model Interpretability”](https://dl.acm.org/doi/10.1145/3411764.3445315)，*Proceedings
    of the 2021 CHI Conference on Human Factors in Computing Systems*，no. 237 (2021年5月):
    1–52。'
- en: '^([20](ch07.html#idm45621831080464-marker)) Margaret Mitchell et al., [“Model
    Cards for Model Reporting”](https://dl.acm.org/doi/10.1145/3287560.3287596), *Proceedings
    of the Conference on Fairness, Accountability, and Transparency* (January 2019):
    220–9.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '^([20](ch07.html#idm45621831080464-marker)) Margaret Mitchell 等人，在[“Model Cards
    for Model Reporting”](https://dl.acm.org/doi/10.1145/3287560.3287596)，*Proceedings
    of the Conference on Fairness, Accountability, and Transparency* (2019年1月): 220–9。'
- en: '^([21](ch07.html#idm45621831020192-marker)) Evelin Amorim et al., [“Automated
    Essay Scoring in the Presence of Biased Ratings”](https://oreil.ly/sxS65), *ACL
    Anthology* 1 (2018): 229–37.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch07.html#idm45621831020192-marker)) Evelin Amorim 等人，[“在偏见评分存在时的自动化作文评分”](https://oreil.ly/sxS65)，*ACL
    Anthology* 1（2018年）：229–37。
- en: '^([22](ch07.html#idm45621831006480-marker)) Cyrus DiCiccio et al., [“Evaluating
    Fairness Using Permutation Tests”](https://dl.acm.org/doi/10.1145/3394486.3403199),
    *KDD-2020* (August 2020): 1467–77.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch07.html#idm45621831006480-marker)) Cyrus DiCiccio 等人，[“使用排列测试评估公平性”](https://dl.acm.org/doi/10.1145/3394486.3403199)，*KDD-2020*（2020年8月）：1467–77。
- en: '^([23](ch07.html#idm45621830978528-marker)) Neil Jethani et al., [“FastSHAP:
    Real-Time Shapley Value Estimation”](https://arxiv.org/abs/2107.07436), *International
    Conference on Learning Representations* (2021).'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch07.html#idm45621830978528-marker)) Neil Jethani 等人，[“FastSHAP：实时Shapley值估计”](https://arxiv.org/abs/2107.07436)，*International
    Conference on Learning Representations*（2021年）。
- en: '^([24](ch07.html#idm45621830966240-marker)) Jayne Groll, [“Monitoring vs. Observability:
    What’s the Difference in DevOps?”](https://oreil.ly/Rq7YC), *The Enterprisers
    Project*, September 17, 2021.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch07.html#idm45621830966240-marker)) Jayne Groll，[“监控与可观察性：在DevOps中有何区别？”](https://oreil.ly/Rq7YC)，*The
    Enterprisers Project*，2021年9月17日。
- en: ^([25](ch07.html#idm45621830937216-marker)) *Control Charts* by John Murdoch
    (Palgrave) is a great resource on the theory and applications of control charts.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch07.html#idm45621830937216-marker)) *控制图* 由 John Murdoch（Palgrave）撰写，是关于控制图理论和应用的重要资源。
