- en: Chapter 7\. How to Build a Dataset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章\. 如何构建数据集
- en: The dataset is the foundation of any edge AI project. With a great dataset,
    every task in the workflow becomes both easier and less risky—from selecting the
    right algorithm to understanding your hardware requirements and evaluating real-world
    performance.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是任何边缘AI项目的基础。有了一个优秀的数据集，从选择正确的算法到理解硬件要求和评估真实世界性能，工作流中的每个任务都变得更容易，风险也更小。
- en: Datasets are indisputably critical for machine learning projects, where data
    is used directly for training models. However, data is vital even if your edge
    AI application doesn’t require machine learning. Datasets are necessary in order
    to select effective signal processing techniques, design heuristic algorithms,
    and test applications under realistic conditions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在机器学习项目中至关重要，数据直接用于模型训练。然而，即使你的边缘AI应用程序不需要机器学习，数据仍然至关重要。数据集在选择有效的信号处理技术、设计启发式算法以及在真实条件下测试应用程序时是必需的。
- en: Collecting a dataset is typically the most difficult, time-consuming, and expensive
    part of any edge AI project. It’s also the most likely place you will make terrible,
    hard-to-detect mistakes that can doom your project to failure. This chapter is
    designed to introduce today’s best practices for building an edge AI dataset.
    It’s probably the most important section of this book.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 收集数据集通常是任何边缘AI项目中最困难、耗时和昂贵的部分。这也是你可能会犯严重且难以检测到的错误，从而使项目注定失败的最有可能的地方。本章旨在介绍当今构建边缘AI数据集的最佳实践。这可能是本书中最重要的部分。
- en: What Does a Dataset Look Like?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集是什么样子？
- en: 'Every dataset is made up of a bunch of individual items, known as *records*,
    each of which contains one or more pieces of information, known as *features*.
    Each feature may be a completely different data type: numbers, time series, images,
    and text are all common. This structure is shown in [Figure 7-1](#dataset_figure).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集由许多单独的项目组成，称为*记录*，每个记录包含一个或多个信息片段，称为*特征*。每个特征可能是完全不同的数据类型：数字、时间序列、图像和文本都很常见。这种结构如图[7-1](#dataset_figure)所示。
- en: '![A diagram showing a stack of records, each with features.](assets/aiae_0701.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![显示一堆具有特征的记录的图表。](assets/aiae_0701.png)'
- en: Figure 7-1\. A dataset contains many records, each of which may contain many
    features; features can have different data types.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1\. 一个数据集包含许多记录，每个记录可能包含许多特征；特征可以具有不同的数据类型。
- en: There are many different names for these components of datasets. Records are
    commonly referred to as *rows*, *samples*, *items*, *examples*, or *instances*.
    Features are also known as *columns* or *fields*.^([1](ch07.html#idm45988813520160))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集的这些组成部分，有许多不同的名称。记录通常被称为*行*、*样本*、*项目*、*示例*或*实例*。特征也被称为*列*或*字段*。^([1](ch07.html#idm45988813520160))
- en: Many datasets also contain *labels*, which are a special kind of feature that
    indicates the desired output of a model trained on that dataset—for example, the
    class returned by a classifier, or the bounding boxes returned by an object-detection
    model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据集还包含*标签*，这是一种特殊的特征，指示训练于该数据集的模型的期望输出。例如，分类器返回的类别或对象检测模型返回的边界框。
- en: It is common for datasets to include something called *metadata*. This is special
    data that describes the data itself. For example, a record may include metadata
    that indicates the exact model of sensor its features were collected with, the
    precise date and time it was captured, or the sampling rate of the signal that
    makes up one of its features.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集通常包含一种称为*元数据*的内容。这是描述数据本身的特殊数据。例如，记录可能包含元数据，指示其特征所收集的传感器型号，捕获的精确日期和时间，或组成其特征之一的信号的采样率。
- en: Tip
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Datasets can be stored in many different ways: on a filesystem, in a database,
    in the cloud, or even in filing cabinets and cardboard boxes.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以以许多不同的方式存储：在文件系统、数据库、云中，甚至是文件柜和纸箱中。
- en: The structure of a dataset often evolves substantially during development. This
    may include changes in what its records and features represent. For example, imagine
    you are building a dataset of vibration data from industrial machines, since you
    wish to train a classifier to distinguish between different operational states.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的结构在开发过程中经常会大幅度演变。这可能包括其记录和特征表示的变化。例如，想象一下，你正在构建来自工业机器振动数据的数据集，因为你希望训练一个分类器来区分不同的操作状态。
- en: You may begin by capturing 24 hours of data from 10 different machines. In this
    case, each record represents a specific period of time from a particular machine.
    You might then divide these records up, splitting each 24-hour record into sections
    that correspond to different operational states, and then adding the appropriate
    labels. Next, you might perform feature engineering on each record, creating additional
    features that can be fed into a machine learning model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从10台不同的机器上捕获24小时的数据来开始。在这种情况下，每条记录代表了特定机器的特定时间段。接下来，您可以将这些记录分割成对应于不同操作状态的部分，并添加适当的标签。接着，您可能会对每条记录进行特征工程，创建可以输入到机器学习模型中的额外特征。
- en: The Ideal Dataset
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理想的数据集
- en: 'An ideal dataset has the following properties:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的数据集具备以下特性：
- en: Relevant
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性
- en: Your dataset should contain information that is useful for the problem you are
    trying to solve. For example, if you’re building a system that uses heart rate
    sensor data to estimate athletic performance, you’ll need a dataset that includes
    both heart rate sensor data and some measure of performance. If you’re planning
    on using a particular type of sensor, it’s typically important that your dataset
    was collected using a similar device. If you’re trying to solve a classification
    problem, it’s important that your dataset contains discriminative information
    about the classes you care about.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据集应包含对您正在尝试解决的问题有用的信息。例如，如果您正在构建一个系统，该系统使用心率传感器数据来估计运动表现，您将需要一个包括心率传感器数据和某种性能指标的数据集。如果您计划使用特定类型的传感器，通常重要的是您的数据集是使用类似设备收集的。如果您正在尝试解决分类问题，重要的是您的数据集包含关于您关心的类别的区分信息。
- en: Representative
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 代表性
- en: To be representative, a dataset must include information about all of the different,
    varied types of conditions that might be encountered in the real world. For example,
    a dataset to be used in a health monitoring application would need to include
    data from a wide enough range of individuals to cover all of the different types
    of people who might be using the application. Unrepresentative datasets will result
    in bias, as described in [“Black Boxes and Bias”](ch02.html#black_boxes_and_bias).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要具有代表性，数据集必须包括关于可能在现实世界中遇到的所有不同类型条件的信息。例如，在用于健康监测应用的数据集中，需要包含来自足够广泛的个体的数据，以涵盖可能使用该应用程序的所有不同类型的人群。代表性不足的数据集会导致偏见，如在[“黑匣子和偏见”](ch02.html#black_boxes_and_bias)中所述。
- en: Balanced
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡性
- en: Beyond being merely representative, ideal datasets contain a good *balance*
    of information from all of the relevant types of conditions. Many types of machine
    learning algorithms work best with balanced datasets, including deep learning
    models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了仅仅具有代表性外，理想的数据集还包含了来自所有相关类型条件的良好*平衡*信息。许多类型的机器学习算法在使用平衡数据集时效果最佳，包括深度学习模型。
- en: For example, in the US, 76% of commuters use a car to get to work, while only
    10% of commuters use a bicycle.^([2](ch07.html#idm45988813500880)) If we were
    training a model to count vehicles driving across town, it would be important
    to use equal amounts of data for cars and bicycles—even though bikes would represent
    a minority of encounters. Otherwise, the model may perform better at identifying
    cars than bikes. This is another common way for bias to enter your system.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在美国，有76%的通勤者使用汽车上下班，而只有10%的通勤者使用自行车。^([2](ch07.html#idm45988813500880))如果我们要训练一个模型来计算穿过城市的车辆数量，使用相等量的汽车和自行车数据是很重要的——尽管自行车只占少数。否则，模型可能会更擅长识别汽车而不是自行车。这是偏见进入系统的另一种常见方式。
- en: Reliable
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠性
- en: An ideal dataset is consistently accurate. It contains as few errors as possible,
    and if there are errors, they exist uniformly across the data—as opposed to being
    concentrated in certain categories.^([3](ch07.html#idm45988813497616)) If there’s
    noise in your data (which is common for sensor applications), it should be the
    same type and magnitude of noise that is present under real-world conditions.
    For example, we might want to train a classification model to identify different
    genres of music, using a dataset of music samples. In our dataset, it’s important
    that each sample is labeled with the correct genre, and that the samples include
    a similar amount of background noise as the expected real-world conditions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的数据集应该保持持续的准确性。它应尽可能少地包含错误，即使有错误，也应均匀地分布在数据中，而不是集中在某些类别中。^([3](ch07.html#idm45988813497616))
    如果数据中存在噪音（这在传感器应用中很常见），则应该是与真实世界条件下存在的相同类型和大小的噪音。例如，我们可能希望使用音乐样本数据集来训练一个分类模型，以识别不同的音乐流派。在我们的数据集中，每个样本都正确标记了流派，并且样本包含与预期的真实世界条件中相似数量的背景噪音。
- en: Well formatted
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 良好格式化的
- en: The same data can be formatted in numerous different ways. For example, images
    can be represented in an infinite variety of different formats, resolutions, and
    color depths. In an ideal dataset, the data is formatted in the way that best
    suits the tasks you are using it for. At the very least, it’s helpful for a dataset
    to have consistent formatting throughout its samples.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 同一数据可以以许多不同的方式进行格式化。例如，图像可以以无限种不同的格式、分辨率和色彩深度表示。在理想的数据集中，数据以最适合您使用的方式进行格式化。至少，数据集在其样本中应具有一致的格式。
- en: Well documented
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 良好记录的
- en: 'It’s critically important to understand where a dataset came from, how it was
    collected, and what all of its fields mean. Without this information, you will
    not be able to determine if the dataset meets your requirements. For example,
    imagine you wish to use a dataset of sensor data sourced from the internet. Without
    good documentation, you will have no way of knowing whether the data is relevant:
    it may come from sensors that are not equivalent to the ones you intend to use.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 理解数据集的来源、采集方式以及所有字段的含义至关重要。如果没有这些信息，您将无法确定数据集是否符合您的需求。例如，假设您希望使用来自互联网的传感器数据集。没有良好的文档，您将无法确定数据的相关性：它可能来自与您打算使用的传感器不等效的传感器。
- en: Appropriately sized
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 适当大小的
- en: A machine learning model can learn the hidden rules in almost any system—as
    long as it is provided with sufficient data. For example, you may wish to train
    a model to identify different types of tennis shots using accelerometer data.
    If the dataset only includes a few samples of each shot, the model may struggle
    to learn a general representation of what characterizes each one. To generalize,
    more samples for each shot type may be necessary—the more the better.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型可以学习几乎任何系统中的隐藏规则，只要它提供足够的数据。例如，您可能希望使用加速度计数据来训练一个模型来识别不同类型的网球击球方式。如果数据集仅包含每种击球方式的少数样本，模型可能难以学习每种击球方式的一般表现。为了概括，可能需要更多的每种击球类型的样本——数量越多越好。
- en: However, larger datasets result in longer training times, and they are more
    difficult to work with from a technical perspective. In addition, different problems
    require different amounts of data to solve, so for every project there is a point
    of diminishing returns for collecting further data. Your goal should be to collect
    enough data to solve your problem.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，更大的数据集会导致更长的训练时间，并且从技术角度来看，处理起来更加困难。此外，不同的问题需要不同数量的数据来解决，因此对于每个项目来说，进一步收集数据会遭遇收益递减的情况。您的目标应该是收集足够的数据来解决您的问题。
- en: If you are mostly using your dataset for testing (versus training a model),
    you can get away with a smaller one—but it’s important that your dataset is big
    enough to be representative and balanced.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您主要用数据集进行测试（而不是训练模型），您可以使用较小的数据集，但重要的是数据集足够大，以代表性和平衡性。
- en: As you might expect, it’s tricky to create a dataset with all of these ideal
    properties. As you build your dataset, you will likely have to do some work to
    get it into shape.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能预期的那样，创建一个具有所有这些理想特性的数据集是棘手的。在构建数据集时，您可能需要进行一些工作来使其符合要求。
- en: Datasets for Evaluation
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估数据集
- en: While building and testing an edge AI system in the lab requires one type of
    dataset, evaluating its performance under *real-world conditions* may require
    another. [Chapter 10](ch10.html#deploying_evaluating_and_supporting_edge_ai_applications)
    will introduce several ways to evaluate edge AI systems and will explain how to
    collect the right types of data for the task.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验室中构建和测试边缘AI系统需要一种类型的数据集，而在*真实世界条件*下评估其性能可能需要另一种。[第10章](ch10.html#deploying_evaluating_and_supporting_edge_ai_applications)将介绍几种评估边缘AI系统的方法，并解释如何收集任务所需的正确类型的数据。
- en: Every AI project involves the distillation of domain expertise from a human
    mind into a computer system. The process of building a dataset is where the majority
    of this work happens. It must be conducted with care, intent, and careful consideration.
    The good news is that if you get it right, you’ll massively increase your chances
    of success.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 每个AI项目都涉及将领域专业知识从人类思维中提炼到计算机系统中的过程。构建数据集的过程是这项工作的主要部分。它必须以谨慎、目的明确和深思熟虑的方式进行。好消息是，如果你做对了，成功的机会将大大增加。
- en: Datasets and Domain Expertise
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集和领域专业知识
- en: Domain experts, also known as subject matter experts (SMEs), are the people
    with deep knowledge about the problem you are trying to tackle. No matter what
    the niche, there are people who have studied, experienced, and learned the subject
    inside out.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 领域专家，也称为主题专家（SMEs），是那些对你试图解决的问题有深入了解的人。无论是什么领域，总有些人对其进行了深入的研究、经验积累并彻底了解该主题。
- en: It’s important to view domain expertise in your problem area as potentially
    distinct from the knowledge required to work with AI algorithms, signal processing,
    embedded engineering, or hardware design. While it’s possible for a domain expert
    to also have skills in these areas, the fact that somebody is, say, a machine
    learning expert does not automatically make them qualified to design AI systems
    to solve any problem under the sun.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决问题领域中，将领域专业知识视为与处理AI算法、信号处理、嵌入式工程或硬件设计所需知识可能不同是很重要的。虽然领域专家可能也具备这些领域的技能，但某人是机器学习专家并不意味着他们自动具备设计AI系统解决任何问题的资格。
- en: For example, imagine you are building an edge AI product for the healthcare
    market. In addition to hardware and software engineers, and people versed in building
    AI applications,^([4](ch07.html#idm45988813475392)) your team will need to include
    domain experts who have a genuine understanding of the healthcare problem you
    are trying to solve. Otherwise, you’ll risk building a product that does not work
    in the way you expect it to.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，想象一下，你正在为医疗市场构建一款边缘AI产品。除了硬件和软件工程师以及精通构建AI应用的人员^([4](ch07.html#idm45988813475392))，你的团队还需要包括真正理解你试图解决的医疗问题的领域专家。否则，你将面临建造一个不符合预期工作方式的产品的风险。
- en: Datasets and domain experts are intimately connected. Every AI product reflects
    the dataset used to develop, train, and test it. When products use machine learning,
    the algorithms are dictated directly by the data. But even handcoded algorithms
    are only as good as the data that is used to test them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集和领域专家密切相关。每个AI产品都反映了用于开发、训练和测试它的数据集。当产品使用机器学习时，算法直接由数据决定。但即使是手工编码的算法，其表现也取决于用于测试它们的数据。
- en: This means that the outcomes of your entire project are dictated by the quality
    of your dataset. Moreover, the only people in your organization who are qualified
    to *understand* that quality are your domain experts. Their knowledge of the problem
    you are trying to solve must guide the construction and curation of your dataset.
    No matter how many talented data science experts you have on your team, their
    skills will be redundant without proper insight into the problem at hand.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你整个项目的结果由你数据集的质量决定。此外，你组织中唯一有资格*理解*这种质量的人就是你的领域专家。他们对你试图解决的问题的深刻了解必须指导数据集的构建和策划。无论你团队中有多少有才华的数据科学专家，如果他们没有对手头问题的适当见解，他们的技能都将是多余的。
- en: In essence, your dataset acts as the main vector for domain expertise both within
    your product and your organization. Since it is constructed using knowledge from
    domain experts, it ends up being a representation of their knowledge in digital
    form—almost like an application programming interface (API) that provides access
    to their captured insights.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，你的数据集在你的产品和组织中作为领域专业知识的主要向量。由于它是使用领域专家的知识构建的，最终它成为了他们知识的数字形式表现——几乎像一个应用程序编程接口(API)，提供对他们捕获的见解的访问。
- en: This encoded knowledge will be used by the rest of your team to help build your
    application. For example, the engineers working on your algorithms will use the
    dataset to tune or train them, and those responsible for testing your application
    will use it to ensure that it works well in all the situations you need it to.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码知识将被您团队的其他成员用来帮助构建您的应用程序。例如，负责您算法的工程师将使用数据集来调整或训练它们，而负责测试您应用程序的人将使用它来确保它在您需要的所有情况下都能正常运行。
- en: All of this makes it critical that you have sufficient domain expertise on hand.
    In addition, since your domain experts may not necessarily be experts in building
    and assessing datasets, you will need them to work closely with the members of
    your team who have data science skills. It will take collaboration to build an
    effective dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都使得您必须有足够的领域专业知识。此外，由于您的领域专家不一定是构建和评估数据集的专家，您将需要他们与具有数据科学技能的团队成员密切合作。合作建立有效的数据集是必要的。
- en: But what if you don’t have access to domain expertise? The answer is frank and
    perhaps unwelcome. If your team lacks domain expertise in your problem area, it
    would be irresponsible for you to attempt to build a product. You will lack not
    only the knowledge to build an effective product but also the insight to understand
    whether you have built an *ineffective* one.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果您没有获得领域专业知识怎么办？答案很坦率，也许会不受欢迎。如果您的团队在您的问题领域缺乏专业知识，那么试图构建一个产品将是不负责任的。您不仅缺乏构建有效产品的知识，还缺乏理解是否构建了一个*无效*产品的洞察力。
- en: Data, Ethics, and Responsible AI
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据、伦理和负责任的人工智能
- en: The quality of your dataset will shape your application’s social consequences
    more than any other factor. No matter how carefully you have worked to investigate
    the ethical issues around your project, and to design an application that delivers
    benefit while being safe, the limitations of your dataset dictate your ability
    to understand and avoid unintentional harm.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据集的质量将比任何其他因素更多地塑造您的应用程序的社会影响。不管您如何用心去调查围绕您的项目的伦理问题，并设计一个既有利于人又安全的应用程序，您的数据集的限制将决定您理解和避免无意伤害的能力。
- en: 'From the perspective of responsible AI, your dataset provides two core things:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从负责任的人工智能的角度来看，您的数据集提供了两个核心要素：
- en: The raw construction material for the system of algorithms that you are attempting
    to create
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您试图创建的算法系统的原始构建材料
- en: Your most powerful tool for understanding the performance of your system
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解系统性能的最有力工具
- en: Your dataset is your only detailed representation of the real-world situation
    that your system is designed to interact with. Your entire application development
    feedback loop is mediated by it. As raw construction material, if your dataset
    is in any way lacking it will invariably and unavoidably lead to bad performance
    of your system. Even worse, the same failings will impact your ability to understand—or
    even notice—that the system is underperforming.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据集是您的系统旨在与之交互的真实世界情况的唯一详细表现。您整个应用程序开发反馈循环都受它的调节。作为原始的构建材料，如果您的数据集在任何方面有所不足，将不可避免地导致您的系统表现不佳。更糟糕的是，同样的缺陷将影响您理解—甚至注意到—系统性能不佳的能力。
- en: This is especially true for edge AI projects, since the nature of their edge
    deployment means that it is often challenging to capture information about how
    they are performing in the field. Your dataset often represents your only chance
    at evaluating your model’s performance with any real precision.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于边缘人工智能项目来说，尤其如此，因为它们的边缘部署性质意味着很难在现场捕捉关于它们在进行中的表现的信息。您的数据集往往代表了您唯一的机会来以真正的精度评估您的模型性能。
- en: With this in mind, it’s beyond critical that you spend enough time on getting
    this part right.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个想法，花足够的时间确保这一部分做得正确至关重要。
- en: This tragedy highlights one of the greatest challenges in dataset construction.
    The real world is varied to an almost absurd degree. There are a near infinite
    variety of human beings, bicycles, plastic bags, roads, and lighting conditions.
    It’s impossible for a dataset to ever capture all of the possible combinations
    of these things.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这场悲剧突显了数据集构建中最大的挑战之一。现实世界的变化多种多样到近乎荒谬的程度。有着接近无限种类的人类、自行车、塑料袋、道路和光照条件。一个数据集不可能捕获到所有这些事物的可能组合。
- en: Further, there are so many possible combinations of variations that even a domain
    expert may be unaware of some of them. For example, even if an expert on urban
    traffic was tasked with identifying objects that are critical for inclusion in
    a self-driving dataset, they may not have thought of including a bicycle loaded
    with plastic bags.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有许多可能的变体组合，即使领域专家也可能不了解其中一些。例如，即使是城市交通专家被要求识别自动驾驶数据集中需要包含的关键对象，他们可能没有考虑到包裹着塑料袋的自行车的情况。
- en: Minimizing Unknowns
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最小化未知因素
- en: As in Donald Rumsfeld’s infamous quote, in dataset creation there are both “known
    unknowns” and “unknown unknowns.” The only way to build an effective dataset is
    to minimize both of them. There are two main ways to do this.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如唐纳德·拉姆斯菲尔德（Donald Rumsfeld）臭名昭著的一句话中所说，在数据集创建中存在“已知未知”和“未知的未知”。建立有效数据集的唯一方法是尽量减少这两者。有两种主要方法可以做到这一点。
- en: The first, and most effective, is to limit the scope of the situation your model
    is going to interact with. A general-purpose, self-driving system could be considered
    a nightmare scenario for dataset construction. Self-driving cars must navigate
    across vast distances of messy reality, from city streets to country roads, encountering
    almost anything that can possibly be imagined. There’s no possible way you can
    build a dataset that is representative of all that variety.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种，也是最有效的方法是限制模型将要交互的情境范围。一个通用的自动驾驶系统可能被视为数据集构建的噩梦场景。自动驾驶汽车必须在杂乱的现实中穿行，从城市街道到乡村道路，几乎遇到任何可以想象到的情况。你不可能构建一个代表所有这些多样性的数据集。
- en: In contrast, consider a self-driving golf cart that is restricted to driving
    around a golf course. While it’s still possible that it might encounter a bicycle
    as it roams around the fairways, it’s quite unlikely—so it may be easier to build
    a dataset that is representative of the typical set of circumstances that exist
    in normal use. In the case of a self-driving car, the principle of limited scope
    may guide you to limit vehicle operation to the geographic area that its algorithms
    were trained on.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，考虑一个受限于高尔夫球场内驾驶的自动驾驶高尔夫车。虽然它仍然可能在球场上遇到自行车，但这种可能性很小——因此，可能更容易构建一个代表正常使用情况下的典型环境的数据集。对于自动驾驶汽车来说，限制范围的原则可能会指导您将车辆操作限制在算法训练的地理区域内。
- en: The second way to avoid unknowns is to improve your domain expertise. The more
    expert knowledge available about a situation, the less “unknown unknowns” there
    may be. If Uber had employed a more effective panel of urban transportation experts
    to help build and evaluate their dataset, then they might potentially have averted
    a tragedy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种避免未知因素的方法是提高你的领域专业知识。对于一个情境有更多专业知识可用，可能会减少“未知的未知”。如果Uber雇用了更有效的城市交通专家小组来帮助构建和评估他们的数据集，他们可能会避免一场悲剧。
- en: 'On a practical level, we can also derive a firm rule from this insight: we
    should never build edge AI applications for real-world usage in areas where we
    do not have access to domain expertise. Without domain expertise, the field of
    “unknown unknowns” is unbounded in size. It’s almost guaranteed that we will run
    into them.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际层面上，我们也可以从这一见解中得出一个明确的规则：我们不应在缺乏领域专业知识的情况下为真实世界的应用构建边缘AI应用程序。没有领域专业知识，领域的“未知的未知”规模是不受限制的。几乎可以保证我们会遇到它们。
- en: Ensuring Domain Expertise
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保领域专业知识
- en: The amazing tools that now exist to assist with training machine learning models
    have massively lowered the barriers to entry. Unfortunately, this creates the
    temptation for developers to build applications in areas where they lack domain
    expertise.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在存在的惊人工具大大降低了训练机器学习模型的门槛。不幸的是，这种情况会诱使开发人员在缺乏领域专业知识的领域构建应用程序。
- en: During the COVID-19 pandemic, thousands of well-meaning researchers and engineers
    created projects designed to diagnose infection using medical imagery. A 2021
    review published in *Nature Machine Intelligence*^([5](ch07.html#idm45988813423152))
    identified 2,212 such studies. Of these, only 62 passed a quality review, and
    not a single model was recommended for potential clinical use. The majority of
    issues found could likely have been resolved had clinical and machine learning
    domain expertise been applied.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 COVID-19 大流行期间，成千上万的研究人员和工程师设计了项目，旨在使用医学影像诊断感染。2021年发表在*自然机器智能*（[5](ch07.html#idm45988813423152)）上的综述识别了2,212项此类研究。其中只有62项通过了质量审查，没有一个模型被推荐用于潜在的临床使用。发现的主要问题如果能应用临床和机器学习领域的专业知识可能本可以解决。
- en: The peer review system of academia provides a mechanism to analyze and critique
    attempts to solve problems with AI. However, in industry there is no such system.
    Models are deployed inside black box systems, with no accompanying documentation,
    and are allowed to interact with real-world systems in an unmediated and unmonitored
    way. This massively increases the chance that a catastrophic issue may make it
    into production.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 学术界的同行评审系统为分析和批评试图用 AI 解决问题提供了机制。然而，在工业界没有这样的系统。模型部署在黑盒系统内部，没有随附的文档，并允许与真实世界系统直接交互而无监控。这极大地增加了灾难性问题进入生产的可能性。
- en: Those of us who work in edge AI have a profound responsibility to build systems
    for ensuring adequate quality, both internally within organizations and through
    cross-organizational collaboration. A focus on dataset quality, and the corresponding
    deployment of domain knowledge, must be at the heart of any serious effort.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在边缘 AI 领域工作的人有责任为保证适当的质量建立系统，既在组织内部又通过组织间的协作。重点放在数据集质量和相应的领域知识部署在任何严肃努力的核心。
- en: Data-Centric Machine Learning
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据中心化机器学习
- en: Traditionally, machine learning practitioners have focused on selecting the
    best combination of feature engineering and learning algorithm to get good performance
    on a particular task. In this framework, datasets are considered fixed elements
    that are rarely manipulated beyond some basic cleanup. They provide an input,
    and a reference for correctness, but they are not considered something to be tuned
    and tweaked.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，机器学习从业者专注于选择最佳的特征工程和学习算法组合，以在特定任务上获得良好性能。在这个框架中，数据集被认为是固定的元素，很少在基本清理之外进行操作。它们提供输入和正确性参考，但不被视为需要调整和微调的东西。
- en: In recent years, it has been increasingly recognized that datasets should not
    be thought of as static objects. The makeup of a dataset has a strong impact on
    the performance of models that are trained on it, and practitioners have begun
    to modify datasets in order to achieve better performance on tasks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，越来越多的人意识到数据集不应被视为静态对象。数据集的构成对于训练在其上的模型性能有很大影响，从业者已经开始修改数据集以在任务上获得更好的性能。
- en: This new way of thinking is referred to as “data-centric machine learning.”
    In a data-centric workflow, more emphasis is placed on improving the quality of
    datasets—as opposed to tweaking the parameters of algorithms.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新的思维方式被称为“数据中心化机器学习”。在数据中心化的工作流中，更加注重改进数据集的质量，而不是调整算法参数。
- en: Data-centric ML follows the age-old computing principle of [“garbage in, garbage
    out”](https://oreil.ly/NJ8I2)—the idea that it is unreasonable to expect a computer
    program to make good decisions if it is provided with poor-quality input.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心化的机器学习遵循古老的计算原则[“垃圾进，垃圾出”](https://oreil.ly/NJ8I2)——即如果提供的输入质量低劣，期望计算机程序做出良好决策是不合理的。
- en: 'Data-centric workflows and tools help developers understand the quality of
    their data and how to remedy issues within it. This could involve:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心化的工作流和工具帮助开发人员了解其数据的质量及如何解决其中的问题。这可能包括：
- en: Fixing or removing mislabeled samples
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修复或删除错误标记的样本
- en: Removing outliers
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除异常值
- en: Adding specific data to improve representation
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加特定数据以改善表示
- en: Resampling data to improve balance
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新取样数据以改善平衡
- en: Adding and removing data to account for drift
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加和移除数据以解决漂移问题
- en: It’s important to acknowledge that all of these tasks require domain knowledge.
    In some respects, the shift toward data-centric ML is a recognition of the importance
    of domain knowledge in getting satisfactory performance from machine learning
    systems.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 承认所有这些任务都需要领域知识是很重要的。在某些方面，向数据为中心的机器学习的转变是承认领域知识在从机器学习系统中获得满意性能方面的重要性。
- en: Note
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Drift is the idea that the real world changes over time. Datasets and models
    must be continually updated to account for it. We’ll cover drift in detail later
    in this chapter.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移是指现实世界随时间变化的概念。数据集和模型必须不断更新以应对这一变化。我们将在本章稍后详细讨论漂移。
- en: The data-centric approach considers datasets to be living entities that require
    regular maintenance. This maintenance is worthwhile because it both reduces the
    amount of algorithm work that needs to be done to train an effective model and
    reduces the amount of data that is required. A high-quality dataset with fewer
    samples is often superior to a low-quality dataset that has more.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据为中心的方法认为数据集是需要定期维护的活体实体。这种维护是值得的，因为它既减少了训练有效模型所需的算法工作量，又减少了所需的数据量。一个质量高的数据集与较少样本通常比一个具有更多样本的低质量数据集优越。
- en: Successful real-world projects often combine a data-centric approach with modern
    tools that automate the discovery of effective algorithmic parameters (such as
    AutoML systems, which we learned about in [“Automated machine learning (AutoML)”](ch05.html#automl_tools)).
    Presented with high-quality data, these tools can do an excellent job of exploring
    the design space and coming up with effective models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的现实项目通常将数据为中心的方法与自动发现有效算法参数的现代工具结合起来（例如我们在[“自动化机器学习（AutoML）”](ch05.html#automl_tools)中了解到的AutoML系统）。在提供高质量数据的情况下，这些工具可以很好地探索设计空间，并提出有效的模型。
- en: This is the approach recommended by this book. It empowers domain experts to
    focus on the data that reflects their areas of expertise, while handing the grunt
    work of algorithm tuning to an automated system. These automated systems rely
    on high-quality data in order to evaluate models and select the best one for a
    task. By focusing on dataset quality, developers simultaneously improve both the
    raw inputs to the system and the mechanism for evaluating it.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书推荐的方法是这样的。它赋予领域专家专注于反映其专业领域的数据，同时将算法调优的苦力工作交给自动化系统。这些自动化系统依赖高质量的数据来评估模型，并选择最适合任务的模型。通过专注于数据集的质量，开发者同时改进了系统的原始输入和评估机制。
- en: Estimating Data Requirements
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估算数据需求
- en: The most common question that people ask during the initial stages of an edge
    AI project is “How much data do I need?” Unfortunately, this isn’t a simple question
    to answer. Data requirements vary massively from project to project.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘AI项目的初始阶段，人们经常问的最常见问题是：“我需要多少数据？”不幸的是，这并不是一个简单的问题。数据需求因项目而异巨大。
- en: Typically, the data requirements of machine learning projects are much higher
    than those that rely only on signal processing, heuristics, and other handcoded
    algorithms. In these cases, you will primarily use data for testing—so while you
    will still need enough to guarantee that your dataset is representative, you won’t
    need the vast numbers of examples of each type of condition that is required by
    many machine learning algorithms.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，机器学习项目的数据需求远高于仅依赖信号处理、启发式和其他手工编码算法的项目。在这些情况下，您主要将数据用于测试——因此，虽然您仍需要足够的数据以保证数据集的代表性，但不需要像许多机器学习算法所需的大量每种类型条件的示例。
- en: The best way to know the data requirements for a problem is to look for precedents.
    Are there examples of this type of problem being solved that give you a sense
    of how much data is required?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 理解问题的数据需求最佳方法是寻找先例。是否有解决这类问题的示例，可以让您对所需数据量有所了解？
- en: The web is your best friend in this regard. A quick search will turn up scientific
    papers, benchmarks, open source projects, and technical blog posts that can provide
    a ton of insight. For example, the website [Papers with Code](https://oreil.ly/P8opj)
    has a “State-of-the-Art” section that lists benchmark datasets for various tasks
    and the performance that has been attained on them over time.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，网络是您的最佳伙伴。快速搜索将找到科学论文、基准测试、开源项目和技术博客文章，这些资源可以提供大量见解。例如，网站 [Papers with
    Code](https://oreil.ly/P8opj) 有一个“最新技术”部分，列出了各种任务的基准数据集以及它们随时间的性能。
- en: If we were developing a keyword-spotting application, we could take a look at
    the [results for the Google Speech Commands dataset](https://oreil.ly/OuLiV),
    which at the time of writing has been solved with 98.37% accuracy. Digging into
    [the dataset itself](https://oreil.ly/gLy_i) tells us that the task involves classifying
    among 10 keywords, and that the dataset has 1.5–4k utterances for each keyword.
    If our task is sufficiently similar, these numbers give us a ballpark figure for
    how much data we might need.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在开发一个关键词识别应用程序，我们可以查看[Google语音命令数据集的结果](https://oreil.ly/OuLiV)，在撰写本文时，其准确率已达到98.37%。深入研究[数据集本身](https://oreil.ly/gLy_i)告诉我们，该任务涉及对10个关键词进行分类，数据集中每个关键词有1.5-4k次语音发声。如果我们的任务足够相似，这些数字可以给我们一个大致需要多少数据的估计。
- en: 'Another good idea is to explore tools in your problem domain that are specifically
    designed to work with minimal data. Deep learning models can be especially data
    hungry: are there classical ML alternatives that could fit your use case? If your
    problem requires deep learning, are there any pretrained feature extractors available
    that might fit your use case, via transfer learning, or could you train one using
    an existing dataset?'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个好主意是探索您的问题领域中专门设计用于少量数据工作的工具。深度学习模型可能尤其对数据需求严苛：是否有经典的机器学习替代方案可以适应您的用例？如果您的问题需要深度学习，是否有任何预训练的特征提取器可用于通过迁移学习适应您的用例，或者您是否可以使用现有数据集训练一个？
- en: For example, in the keyword-spotting domain, a paper from researchers at Harvard,
    [“Few-Shot Keyword Spotting in Any Language”](https://oreil.ly/3conT) (Mazumder
    et al., 2021), provides evidence that a keyword-spotting model can be trained
    with only five examples of a keyword, along with a substantially larger dataset
    to verify its performance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在关键词识别领域，哈佛研究人员的一篇论文[“任何语言的少样本关键词识别”](https://oreil.ly/3conT)（Mazumder等人，2021年）提供了证据，表明关键词识别模型只需五个示例和一个大得多的数据集来验证其性能。
- en: '[Table 7-1](#data_requirements_estimates) provides a relative indication of
    how much data is required to train machine learning models for some common tasks.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 7-1](#data_requirements_estimates) 提供了一些常见任务训练机器学习模型所需数据量的相对指标。'
- en: Table 7-1\. Data requirements for common tasks
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-1\. 常见任务的数据需求
- en: '| Task | Relative data requirements | Notes |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 相对数据需求 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Time series classification | Low | DSP can do a lot of the hard work, making
    this task easier to train. |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 时间序列分类 | 低 | 数字信号处理可以完成大部分繁重工作，使得这一任务更容易训练。 |'
- en: '| Time series regression | Medium | This is more challenging than classification
    due to finer-grained labels. |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 时间序列回归 | 中等 | 由于更精细的标签，这比分类更具挑战性。 |'
- en: '| Nonvoice audio classification | Medium | Varied data is required to account
    for the diversity of background noise and environmental acoustics. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 非语音音频分类 | 中等 | 需要多样化的数据来考虑背景噪声和环境声学的多样性。 |'
- en: '| Voice audio classification | Low or High | This typically demanded many hours
    of data, but new few-shot techniques reduce this. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 语音音频分类 | 低或高 | 通常需要大量数据，但新的少样本技术可以减少这一需求。 |'
- en: '| Image classification in visible spectrum | Low | Transfer learning using
    models trained on public datasets makes this a relatively simple task. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 在可见光谱中的图像分类 | 低 | 使用在公共数据集上训练过的模型进行迁移学习使得这一任务相对简单。 |'
- en: '| Object detection in visible spectrum | Medium | Transfer learning is available,
    but this is more challenging than classification. |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 在可见光谱中的目标检测 | 中等 | 虽然可以进行迁移学习，但这比分类更具挑战性。 |'
- en: '| Vision models for nonvisible spectrum | High | Transfer learning is not typically
    available, increasing data requirements. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 非可见光谱的视觉模型 | 高 | 通常不提供迁移学习，增加了数据需求。 |'
- en: It’s important to remember that even these relative requirements are highly
    approximate—they may vary greatly from project to project, which is why it’s difficult
    to give exact amounts. Data requirements will continue to evolve as new tooling
    and technology becomes available. The more common a task, the more likely there
    are signal processing or learning techniques that can help reduce data requirements.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，即使是这些相对需求也是非常粗略的——它们可能因项目而异，这就是为什么很难给出确切的数量。随着新的工具和技术的出现，数据需求将继续发展。任务越常见，就越有可能有信号处理或学习技术可以帮助减少数据需求。
- en: The largest datasets in machine learning are the massive text datasets used
    for training language models from scratch. This is typically not a required task
    in edge AI, which limits the upper bounds of datasets that we’re required to deal
    with.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中最大的数据集是用于从头开始训练语言模型的大规模文本数据集。这通常不是边缘AI中必须处理的任务，这限制了我们需要处理的数据集的上限。
- en: A Practical Workflow for Estimating Data Requirements
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个估算数据需求的实用工作流程
- en: After our initial research, the next step is to dig out the tools and start
    doing some experimentation. Our core task here is to understand whether, given
    sufficient data, the feature engineering and machine learning pipeline we have
    selected will be able to achieve good enough results.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初步研究之后，下一步是找出工具并开始做一些实验。我们的核心任务是了解在足够的数据情况下，我们选择的特征工程和机器学习流程能否达到足够好的结果。
- en: This task naturally ends up as part of the iterative approach to application
    development, which we’ll be covering in more detail in [Chapter 9](ch09.html#developing_edge_ai_applications).
    For now, we’ll go over the relevant tasks at a high level.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这项任务自然成为应用开发迭代方法的一部分，我们将在[第9章](ch09.html#developing_edge_ai_applications)中更详细地介绍。现在，我们将以高层次概述相关任务。
- en: Note
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Defining what “good enough results” means for our project is an important step
    that will be explored in [“Scoping a Solution”](ch08.html#scoping_a_solution).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为我们的项目定义“足够好的结果”是一个重要的步骤，将在[“方案范围”](ch08.html#scoping_a_solution)中探讨。
- en: 'Here is the basic process for estimating data requirements:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是估算数据需求的基本过程：
- en: Capture and refine a small dataset. To be effective in estimating data requirements,
    this dataset should meet all the requirements of the ideal dataset described earlier
    in the chapter, aside from being appropriately sized. The rest of this chapter
    will help you understand the processes required to get it into good shape.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 捕捉和细化一个小数据集。为了有效地估算数据需求，这个数据集应该满足本章前面描述的理想数据集的所有要求，除了适当的大小。本章的其余部分将帮助您了解将其整理成良好形状所需的过程。
- en: Based on your research into potential model types, select a candidate model.
    It’s a good idea to begin with the simplest model that seems reasonable, since
    simpler models are typically easiest to train. Don’t fall into the trap of wanting
    to try a hot new technology without having ruled out the simple and elegant alternatives.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据你对潜在模型类型的研究，选择一个候选模型。从最简单且合理的模型开始是个好主意，因为简单的模型通常最容易训练。不要陷入只想尝试热门新技术而不排除简单且优雅替代方案的陷阱。
- en: Divide your dataset into multiple, same-sized chunks. Each chunk should have
    close to the same balance and distribution as the original dataset. To achieve
    this, you should use stratified random sampling.^([6](ch07.html#idm45988813347872))
    Begin with approximately eight chunks.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分成多个大小相同的块。每个块应该具有接近原始数据集的平衡和分布。为了实现这一点，您应该使用分层随机抽样。^([6](ch07.html#idm45988813347872))
    开始时大约有八个块。
- en: Train a simple model on one chunk of the dataset and record the resulting performance
    metrics. It may be helpful to use a hyperparameter optimization tool, as described
    in [“Automated machine learning (AutoML)”](ch05.html#automl_tools), to rule out
    the effects of hyperparameter selection.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据集的一个块上训练一个简单的模型，并记录结果的性能指标。使用超参数优化工具可能会有所帮助，正如在[“自动化机器学习（AutoML）”](ch05.html#automl_tools)中描述的那样，以排除超参数选择的影响。
- en: Add another chunk to your training data, so it’s now made up of two chunks worth
    of data. Train the same model again, from scratch (continuing to use hyperparameter
    optimization if you decided to use it) and record the metrics again.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将另一个块添加到您的训练数据中，这样它现在由两个块的数据组成。再次从头开始训练同样的模型（如果决定使用超参数优化，则继续使用），并再次记录指标。
- en: Continue the process, adding a chunk of data, training the model, and collecting
    the performance metrics, until you are using the entire dataset.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续这个过程，添加一块数据，训练模型，收集性能指标，直到使用整个数据集。
- en: Plot the performance metrics on a chart. It will look something like one of
    the charts in [Figure 7-2](#data_estimation_figure).
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图表上绘制性能指标。它会看起来像[图 7-2](#data_estimation_figure)中的一个图表。
- en: '![Two charts showing performance increasing with the amount of data.](assets/aiae_0702.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![两个图表显示随着数据量增加而性能提高。](assets/aiae_0702.png)'
- en: 'Figure 7-2\. Each chart shows how a performance metric (in this case, accuracy)
    changes with the number of records. The chart on the left shows a situation where
    adding more data would likely result in better performance. The chart on the right
    shows a plateau: adding more data of the same type would be unlikely to result
    in much performance improvement.'
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 每个图表显示性能指标（在本例中为准确性）随记录数的变化。左侧的图表显示添加更多数据可能会导致更好的性能的情况。右侧的图表显示平台期：添加更多相同类型的数据不太可能导致性能显著改善。
- en: In both charts, we can see that the model’s performance increases every time
    we add more data. By looking at the shape of the curve, we can understand the
    impact that new samples are having. In the lefthand chart, the curve indicates
    that performance would likely continue to increase if we were to add more data.
    The trend line provides a way to approximately estimate how much data would be
    required to achieve a given performance.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个图表中，我们可以看到每次添加更多数据时模型的性能都会提高。通过观察曲线的形状，我们可以理解新样本对性能的影响。在左侧的图表中，曲线表明如果我们继续添加数据，性能很可能会继续提高。趋势线提供了一个大致估算所需数据量的方式。
- en: 'In the righthand chart we can see that the model has already reached a performance
    plateau. Adding more of the same type of data is unlikely to have any effect.
    In this case, it may be worth testing a different machine learning model or algorithm,
    or trying to improve on our feature engineering. You might also consider improving
    your dataset in ways beyond just increasing its size: perhaps it contains a lot
    of noise that could be reduced.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧的图表中，我们可以看到模型已经达到了性能平台期。添加更多相同类型的数据不太可能产生任何效果。在这种情况下，也许值得测试不同的机器学习模型或算法，或者尝试改进我们的特征工程。您还可以考虑通过提高数据集的方式来改进，而不仅仅是增加其大小：也许它包含了大量可以减少的噪声。
- en: Of course, this technique depends entirely on the assumption that our dataset
    is “ideal.” In reality, there are likely to be issues with your dataset—and limitations
    of your feature engineering and machine learning algorithm—that prevent the real-world
    performance from matching the trend line as data is added. However, it’s still
    useful to obtain a ballpark figure—it can help you plan for the effort of collecting
    more data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种技术完全依赖于我们数据集是“理想的”这一假设。实际情况是，您的数据集可能存在问题，而您的特征工程和机器学习算法的限制可能会导致随着数据的增加，实际性能无法与趋势线匹配。然而，获得一个大致的数字仍然是有用的——它可以帮助您计划收集更多数据的工作量。
- en: This technique will *not* tell you whether your dataset is representative, balanced,
    or reliable. These parts are entirely up to you.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术 *不* 会告诉您您的数据集是否具有代表性、平衡或可靠性。这些部分完全取决于您自己。
- en: Getting Your Hands on Data
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取您手上的数据
- en: A large part of the challenge of constructing a high-quality dataset is sourcing
    the data itself. These are some of the typical ways that data can be obtained:^([7](ch07.html#idm45988813326288))
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 构建高质量数据集的挑战的一大部分是数据来源本身。以下是一些典型的获取数据的方式：^([7](ch07.html#idm45988813326288))
- en: Collecting an entirely new dataset from scratch
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始收集全新的数据集
- en: Outsourcing the collection of data to another team or a third party
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据收集外包给另一个团队或第三方
- en: Using data from a public dataset
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用来自公共数据集的数据
- en: Repurposing existing data from a partner or collaborator
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从合作伙伴或合作者处重新利用现有数据
- en: Repurposing existing data from an internal data store
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从内部数据存储重新利用现有数据
- en: Reusing data from a previous successful AI project
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从先前成功的AI项目中重用数据
- en: As you can see, there are a range of potential options. However, it’s unlikely
    that all of them will be available for a given project. For example, if this is
    your first edge AI project you may not have any existing data to repurpose.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，有多种潜在的选择。然而，对于特定的项目来说，不太可能所有这些选择都可用。例如，如果这是您的第一个边缘AI项目，您可能没有任何现有数据可重新利用。
- en: 'Each of these sources represents a different compromise between two important
    things: risk of quality issues and effort (which translates into cost). [Figure 7-3](#quality_cost_figure)
    shows how each of them compare.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据源代表两个重要事物之间的不同权衡：质量问题的风险和努力（转化为成本）。[图 7-3](#quality_cost_figure)显示了它们的比较情况。
- en: '![A plot showing the various data sources with effort/cost on the X axis and
    quality risk on the Y.](assets/aiae_0703.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![显示各种数据源，X轴上是努力/成本，Y轴上是质量风险的绘图。](assets/aiae_0703.png)'
- en: Figure 7-3\. Data sources organized by quality risk and effort/cost.
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 按质量风险和努力/成本组织的数据源。
- en: The more control of the data collection process you have, the better you can
    guarantee quality. By far, the best option is being able to reuse data that you
    have used successfully in the past (6). If you’re lucky enough to be able to take
    this option, you’ll already know the quality of the data, and you won’t have to
    invest much effort to reuse it—as long as it remains relevant.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你对数据收集过程的控制越多，就越能保证质量。到目前为止，最佳选择是能够重复使用你过去成功使用过的数据（6）。如果你有幸能够选择这个选项，你已经知道数据的质量，不需要投入太多精力才能重复使用——只要它保持相关性。
- en: It’s quite common for organizations to have existing stores of data that can
    be reused for AI projects (5). In these cases, it’s potentially possible to understand
    the quality of the data since it was collected internally. However, it may require
    a bit of effort to get it into the form required for your AI project. For example,
    a manufacturer may already be collecting machine data using an existing IoT system.
    In this case, the data’s provenance and the collection techniques are known, which
    helps reduce risk. However, the data may not be in a ready-to-use form and will
    likely require some cleanup. Existing data often lacks labels, which are expensive
    to add.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 组织通常会有现有的数据存储，可以重新用于AI项目（5）。在这些情况下，有可能了解数据的质量，因为它是在内部收集的。然而，可能需要一些工作将其整理成AI项目所需的形式。例如，制造商可能已经使用现有的物联网系统收集机器数据。在这种情况下，了解数据的来源和收集技术有助于降低风险。然而，这些数据可能并非现成可用，通常需要进行一些清理。现有数据通常缺乏标签，添加标签的成本较高。
- en: Often, data may be available from a partner organization or collaborator (4).
    In this case, since someone else has collected the data, there’s no way to guarantee
    quality—and some cleanup may be required to make it usable.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据往往可以从合作伙伴组织或合作者处获得（4）。在这种情况下，由于其他人已经收集了数据，无法保证质量——可能需要进行一些清理才能使其可用。
- en: The same is true for public datasets (3), which are typically used for academic
    research. Public datasets have the advantage of being scrutinized by many pairs
    of eyes, and may have useful benchmarks available, but they tend to either be
    cobbled together from low-quality data sources and contain a lot of errors or
    be very small. They may require significant cleanup to be usable, and they may
    contain biases that are not documented or obvious.^([8](ch07.html#idm45988813312688))
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对公共数据集（3）也是如此，这些数据集通常用于学术研究。公共数据集的优势在于经过多人审核，可能有有用的基准可用，但它们往往要么由低质量数据源拼凑而成且包含大量错误，要么非常小。可能需要进行大量清理才能使用，且可能存在未记录或明显的偏见。^([8](ch07.html#idm45988813312688))
- en: It may be possible to outsource data collection to another team in your organization,
    or to a third party (2)—there are entire companies that exist to assist with data
    collection and labeling. While in theory you have significant control over the
    data collection process, this may still involve significant risk, since it’s very
    difficult to guarantee that the third party will follow the correct procedures.
    This is typically a costly approach.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据收集外包给你组织中的另一个团队或第三方公司（2）可能是可行的——存在一些公司专门提供数据收集和标记的服务。虽然理论上你对数据收集过程有很大控制权，但仍存在显著风险，因为很难保证第三方会遵循正确的程序。这通常是一种昂贵的方法。
- en: The approach that has the lowest risk is getting your hands dirty and collecting
    the data yourself (1). When the people who are designing the dataset and algorithms
    are the same ones who are leading the data collection effort, the risk of miscommunication
    or undetected error is minimized (assuming they have the required domain knowledge
    to do the job right). Unfortunately, this is also the most costly approach.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最低风险的方法是亲自动手收集数据（1）。当设计数据集和算法的人与领导数据收集工作的人是同一批人时，误解或未检测到的错误的风险将被最小化（假设他们具备必要的领域知识来做好这项工作）。不幸的是，这也是最昂贵的方法。
- en: The Unique Challenges of Capturing Data at the Edge
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 捕捉边缘数据的独特挑战
- en: The more common the use case, the more likely you are to find an easily accessible
    dataset that has been reviewed for quality. This makes life difficult for many
    edge applications, since there’s a massive variety of niche use cases and exotic
    sensors. In addition, commercial entities don’t tend to share their datasets since
    they represent potential competitive advantage.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用情况越普遍，您可能越有可能找到易于访问的经过质量审核的数据集。这对许多边缘应用来说是个难题，因为有大量的利基用例和外来传感器。此外，商业实体不倾向于共享其数据集，因为它们代表潜在的竞争优势。
- en: 'If you need to collect your own data, there are some specific challenges to
    navigate:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要收集自己的数据，那么需要解决一些具体的挑战：
- en: Connectivity and bandwidth
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 连通性和带宽
- en: Edge compute is often used in applications where bandwidth and connectivity
    is limited. This means that it can be difficult to collect data in the field.
    For example, if you are building an AI-powered camera for monitoring the movement
    of farm animals, you might wish to collect images of animals from the field. However,
    this may not be possible given the remote locations of many farms and the lack
    of connectivity.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在带宽和连接性有限的应用中经常使用边缘计算。这意味着在现场收集数据可能会很困难。例如，如果您正在为监控农场动物移动而建立一个AI摄像头，您可能希望从田野中收集动物的图像。然而，由于许多农场的偏远位置和缺乏连接性，这可能是不可能的。
- en: To get around this issue, you could temporarily install networking hardware
    on-site (for example, a satellite connection might be used in remote regions)—or
    rely on *sneakernet* capabilities.^([9](ch07.html#idm45988813292576)) This is
    very expensive, but it may only need to be done temporarily, at the beginning
    of a project.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，您可以临时在现场安装网络硬件（例如，在偏远地区可能会使用卫星连接）—或者依赖于*sneakernet*功能。^([9](ch07.html#idm45988813292576))
    这非常昂贵，但可能只需要在项目开始阶段暂时完成。
- en: Brownfield hardware
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 棕地硬件
- en: As we learned in [“Greenfield and Brownfield Projects”](ch02.html#brownfield),
    it’s quite common to deploy edge AI applications on existing hardware. Unfortunately,
    the hardware was not always designed with data collection in mind. To succeed
    at data collection, brownfield hardware needs sufficient memory to store samples,
    sufficient networking capability to upload them, and sufficient energy budget
    to permit the process to occur frequently.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[“绿地和棕地项目”](ch02.html#brownfield)中学到的那样，将边缘AI应用部署到现有硬件上是相当普遍的。不幸的是，该硬件并不总是设计用于数据收集。为了成功进行数据收集，棕地硬件需要足够的内存来存储样本，足够的网络能力来上传它们，并且足够的能源预算来允许该过程频繁发生。
- en: To work around this problem, it may make sense to temporarily install new hardware
    on-site that is better suited to the challenge of collecting data. Dedicated industrial
    [data loggers](https://oreil.ly/3qfG1) exist for this purpose, and industrial-grade
    rapid IoT development platforms like [Arduino Pro](https://www.arduino.cc/pro)
    can be convenient to use.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这个问题，暂时在现场安装更适合收集数据挑战的新硬件可能是有意义的。专用的工业[数据记录仪](https://oreil.ly/3qfG1)专为此目的而设计，而像[Arduino
    Pro](https://www.arduino.cc/pro)这样的工业级快速物联网开发平台也很方便使用。
- en: Greenfield hardware
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 绿地硬件
- en: If an edge AI project involves the creation of new hardware, working hardware
    is likely not available until some time has passed. This can be a major challenge
    since it’s important to make progress with dataset and algorithm development in
    parallel with the hardware development process. It’s tricky to even know what
    hardware is required until at least some algorithm development has been done.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果边缘AI项目涉及创建新硬件，则可能要过一段时间才能获得可用的工作硬件。这可能是一个重大挑战，因为在硬件开发过程中与数据集和算法开发并行进行非常重要。甚至在完成一些算法开发之前，很难知道需要什么样的硬件。
- en: In this case, it’s important to try and get some representative data as quickly
    as possible. Similar to the brownfield case, it could make sense to use a rapid
    IoT development platform to start collecting data before your production hardware
    is ready.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，尽快获取一些代表性数据非常重要。类似于棕地案例，可能有必要使用快速的物联网开发平台开始收集数据，即使在生产硬件准备好之前也是如此。
- en: Sensor differences
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器差异
- en: Sometimes, the sensor hardware currently available in the field may not be identical
    to the hardware you plan to use in a new device. In some cases, even the placement
    of a sensor may be different enough to cause problems.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，目前在现场可用的传感器硬件可能与您计划在新设备中使用的硬件不完全相同。在某些情况下，甚至传感器的放置位置可能有足够的不同，会引起问题。
- en: If you suspect sensor differences might be a challenge, you should try as early
    as possible to evaluate the sensor data side by side and determine whether it
    is different enough to present a problem. If so, you can use the same approach
    recommended for working with inadequate brownfield hardware.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你怀疑传感器差异可能是一个挑战，你应尽早评估传感器数据并确定其是否足够不同以造成问题。如果是这样，你可以使用与处理不充分的既有硬件类似的方法推荐的方法。
- en: Labeling
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 标记
- en: One of the biggest challenges in working with edge AI data is the availability
    of labels. For example, imagine you are collecting accelerometer data from the
    ear tag of a farm animal with the goal of classifying how it is spending its time
    between eating, walking, and sleeping. Even in a situation where it is trivial
    to collect the raw sensor data, it may be challenging to correlate this data with
    the actual activity of the animal. If you could already identify the animal’s
    activity using the data, your project would not be necessary!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理边缘AI数据时，最大的挑战之一是标签的可用性。例如，假设你正在收集农场动物耳标上的加速度计数据，目标是对其在进食、行走和睡眠之间的时间分配进行分类。即使在收集原始传感器数据相对容易的情况下，将这些数据与动物的实际活动相关联可能是具有挑战性的。如果你已经能够使用数据识别动物的活动，那么你的项目可能就没有必要了！
- en: To work around this problem, you can try to collect additional data that may
    not be available during the normal operation of the device you are designing.
    For example, during initial data collection, you may choose to collect both accelerometer
    data and video from a camera that shows the animal’s activity, with timestamps
    for both. You can then use the video to help you label the data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，你可以尝试收集在设计设备正常操作期间可能不可用的额外数据。例如，在初始数据收集期间，你可以选择同时收集加速度计数据和显示动物活动的摄像机视频，并为两者都标记时间戳。然后，你可以使用视频来帮助标记数据。
- en: Storing and Retrieving Data
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储和检索数据
- en: As you begin to collect data, you’ll need somewhere to store it. You’ll also
    need a mechanism for getting data from devices into your data store, and from
    your data store to your training and testing infrastructure.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始收集数据时，你需要一个地方来存储它。你还需要一个机制，将数据从设备传输到数据存储中，并从数据存储传输到你的训练和测试基础设施。
- en: Storage requirements vary massively depending on how much data you expect your
    dataset to contain. The more data you have, the more sophisticated your solution
    needs to be. That said, edge AI datasets are typically relatively small and are
    unlikely to require technologies designed to operate at massive scale.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 存储需求因预期数据集大小而大不相同。数据量越大，解决方案就需要越复杂。尽管如此，边缘AI数据集通常相对较小，不太可能需要设计用于大规模操作的技术。
- en: When choosing a solution, it’s always preferable to go with the simplest you
    can get away with. If you’re dealing with a quantity of data that can fit happily
    on a single workstation, there’s no need to invest in fancy technology. The more
    direct access you have to your data for easy exploration and experimentation,
    the better—so from a convenience perspective, the ideal option is always your
    local filesystem.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择解决方案时，最好选择尽可能简单的解决方案。如果你处理的数据量可以轻松适应单台工作站，那么没有必要投资于高级技术。对于便利性而言，你对数据的直接访问越便捷，探索和实验的效果就越好，所以从便捷性的角度来看，最理想的选择总是你的本地文件系统。
- en: '[Table 7-2](#data_stores) provides a quick reference to a variety of data storage
    solutions, with the advantages and disadvantages of each.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 7-2](#data_stores) 提供了各种数据存储解决方案的快速参考，以及每种解决方案的优势和劣势。'
- en: Table 7-2\. Data storage solutions
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-2\. 数据存储解决方案
- en: '| Storage type | Advantages | Disadvantages |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 存储类型 | 优势 | 劣势 |'
- en: '| --- | --- | --- |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Local filesystem | Fast, simple, and easy to work with | No API, no backups,
    no distributed training; upper limit of a few terabytes |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 本地文件系统 | 快速、简单且易于操作 | 没有API、没有备份、没有分布式训练；最大容量仅为几TB |'
- en: '| Network or cloud filesystem | Accessible by multiple machines; can store
    massive datasets | Slower than local filesystem; complex to set up mounts |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 网络或云文件系统 | 可供多台机器访问；可以存储大规模数据集 | 比本地文件系统慢；挂载设置复杂 |'
- en: '| Cloud object storage | Simple APIs for reading and writing data; massive
    scale | Data must be downloaded to use |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 云对象存储 | 读写数据的简单API；可以实现大规模存储 | 必须下载数据才能使用 |'
- en: '| Feature store | Data can be versioned and tracked; can store metadata; can
    query data | Data must be downloaded to use; more complex and costly than simple
    storage |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 特征存储 | 数据可以进行版本控制和跟踪；可以存储元数据；可以查询数据 | 必须下载数据才能使用；比简单存储更复杂且更昂贵 |'
- en: '| End-to-end platform | Designed specifically for edge AI; data exploration
    tools built in; tight integration with data capture, training, and testing | More
    costly than simple storage |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 端到端平台 | 特别为边缘AI设计；内置数据探索工具；与数据捕获、训练和测试紧密集成 | 比简单存储更昂贵 |'
- en: Data stored on a local filesystem is incredibly easy to use and can be accessed
    very fast. Even when using sophisticated cloud storage, data is typically copied
    to the local filesystem before training a model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在本地文件系统上的数据非常容易使用，并且可以非常快速地访问。即使在使用复杂的云存储时，数据通常也会在训练模型之前复制到本地文件系统。
- en: However, it’s risky to store all of your valuable data on a single machine without
    a backup. It’s also inconvenient if the data needs to be accessed by multiple
    people. Network shares, including cloud-based filesystems—like Amazon FSx, Azure
    Files, and Google Cloud Filestore—address this issue. However, they are relatively
    complex to access—they must be mounted as drives within an operating system.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在没有备份的情况下将所有宝贵数据存储在单台机器上是有风险的。如果数据需要多人访问，这也很不方便。网络共享，包括基于云的文件系统——如Amazon
    FSx、Azure Files和Google Cloud Filestore——可以解决这个问题。但是，要访问它们相对复杂——它们必须作为驱动器挂载在操作系统中。
- en: Cloud object storage services—like Amazon S3, Azure Blob Storage, and Google
    Cloud Storage—provide HTTP APIs that make it much easier to get data in and out.
    These APIs can even be used by embedded devices to upload data from the edge,
    assuming the hardware is capable enough. However, they have slower access speeds
    than drive mounts, so data is typically downloaded to a local disk before it is
    used.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 云对象存储服务——如Amazon S3、Azure Blob Storage和Google Cloud Storage——提供了HTTP API，使得数据的进出变得更加简单。这些API甚至可以被嵌入式设备使用，从边缘上传数据，假设硬件足够能力。然而，它们的访问速度比驱动器挂载慢，因此通常在使用之前会将数据下载到本地磁盘。
- en: Feature stores are a relatively new trend in dataset storage. They are designed
    to offer simple APIs for data access and storage, along with additional features
    such as data versioning and the ability to query data. Feature store offerings
    from major providers include Amazon SageMaker Feature Store, Azure Databricks
    Feature Store, and Google Cloud Vertex AI Feature Store. There are also open source
    equivalents you can host on your own infrastructure, such as Feast.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储是数据集存储的一个相对新的趋势。它们旨在提供简单的数据访问和存储API，以及数据版本控制和查询数据等附加功能。主要提供者的特征存储包括Amazon
    SageMaker Feature Store、Azure Databricks Feature Store和Google Cloud Vertex AI
    Feature Store。你还可以在自己的基础设施上托管开源等价物，如Feast。
- en: There are now several end-to-end platforms designed specifically for creating
    edge AI applications. Some of these include their own data storage solutions.
    These are typically equivalent to feature stores, but with the benefit of being
    designed specifically for edge AI projects. They may include tools for exploring
    and understanding sensor data or provide integration points with embedded software
    development tools. They are designed to integrate tightly with the other stages
    in the deep learning workflow. We learned more about these tools in [“Tools of
    the Trade”](ch05.html#tools_for_edge_ai_development).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有几个专门设计用于创建边缘AI应用程序的端到端平台。其中一些包括他们自己的数据存储解决方案。这些通常相当于特征存储，但其设计专门用于边缘AI项目。它们可能包括用于探索和理解传感器数据的工具，或者提供与嵌入式软件开发工具的集成点。它们被设计为与深度学习工作流程的其他阶段紧密集成。我们在[“工具”](ch05.html#tools_for_edge_ai_development)章节中学到了更多关于这些工具的信息。
- en: Getting Data into Stores
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据存入存储中
- en: 'If you’re capturing sensor data for a project, how do you get it into your
    data store? The answer depends on your particular circumstances:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在为一个项目捕获传感器数据，那么如何将其存储到你的数据存储中？答案取决于你的具体情况：
- en: There is good connectivity on-site
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现场连接良好
- en: If you have enough connectivity, bandwidth, and energy to send data directly
    from the edge, you can push data directly to APIs from your edge devices. This
    is easiest if you’re using an end-to-end platform for edge AI that has APIs designed
    specifically for on-device use.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有足够的连接性、带宽和能量直接从边缘设备发送数据，你可以将数据直接推送到API中。如果你使用的是为边缘AI专门设计的端到端平台，并且其API专门设计用于设备上使用，则这样做最容易。
- en: Another good option is to use an IoT platform. You can upload data to the platform
    using its purpose-built APIs, and then use another system to copy the data from
    the IoT platform into your dataset.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个很好的选择是使用物联网平台。您可以使用其专门构建的API将数据上传到平台，然后使用另一个系统将数据从物联网平台复制到您的数据集中。
- en: It’s generally not a great idea to try to upload data directly to a cloud object
    store from an embedded device. Since the APIs were not designed for embedded use,
    they tend to use inefficient data structures, and their client libraries may not
    fit on small targets. This is less of an issue when working with embedded Linux
    devices, which have greater capabilities and access to a full OS.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 直接从嵌入式设备上传数据到云对象存储不是一个好主意。因为这些API并不是为嵌入式使用设计的，它们倾向于使用效率低下的数据结构，而它们的客户端库可能无法适应小型目标。在使用嵌入式Linux设备时，这个问题较小，因为它们具有更大的功能和对完整操作系统的访问权限。
- en: There is poor or no connectivity on-site
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现场连接性差或无连接性
- en: If you lack decent connectivity, or you don’t have the energy budget to send
    data from the very edge of the network, you may have to install some hardware
    to allow data to be stored at the edge and collected periodically.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您缺乏良好的连接性，或者能量预算不足以从网络边缘发送数据，您可能需要安装一些硬件，允许数据在边缘存储并定期收集。
- en: This could mean modifying your existing hardware to add data storage. It might
    also mean adding another independent system, situated nearby, that is able to
    receive and store data from the device that is generating it. This separate system
    could be equipped with better connectivity, or it could be physically collected
    on a periodic basis.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能意味着修改您现有的硬件以增加数据存储。这也可能意味着添加另一个独立系统，放置在附近，能够接收并存储由生成数据的设备产生的数据。这个独立系统可以配备更好的连接性，或者可以定期物理收集数据。
- en: Collecting Metadata
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集元数据
- en: As we learned earlier, an ideal dataset is well documented. When designing your
    system for collecting data, you should be sure to capture as much information
    as possible about the context in which the data is being collected.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前学到的，理想的数据集需要有良好的文档记录。在设计数据收集系统时，您应该确保尽可能多地捕获有关数据收集环境的上下文信息。
- en: 'This additional information, known as metadata, can be included in your dataset
    alongside the sensor data itself. It may include things such as:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此附加信息，即元数据，可以与传感器数据本身一起包含在您的数据集中。它可能包括诸如：
- en: The date and time that data was captured
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获数据的日期和时间
- en: The specific device that collected the data
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集数据的具体设备
- en: The exact model of sensors used
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用传感器的确切型号
- en: The location of the device on the data collection site
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备在数据收集站点上的位置
- en: Any people involved with the collection of the data
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与数据收集相关的任何人员
- en: Metadata can be relevant to an entire dataset, to any subset of its records,
    or to individual records themselves. The paper [“Datasheets for Datasets”](https://oreil.ly/8cF1f)
    (Gebru et al., 2018) defines a standard for collecting documentation that describes
    a dataset in aggregate, along with subsets of its records. While this is extremely
    valuable and should be considered a best practice, there are major benefits to
    collecting metadata on a more structured, granular, and machine-readable basis.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据可以涉及整个数据集，其任何子集，或者单个记录本身。文章[“数据集数据表”](https://oreil.ly/8cF1f)（Gebru等人，2018）定义了一种收集描述数据集及其记录子集的文档的标准。尽管这非常有价值且应被视为最佳实践，但在更结构化、粒度更细和可机器读取的基础上收集元数据也具有重大好处。
- en: In many cases, you will be collecting samples of data that relate to individual
    entities. For example, you might be monitoring vibrations from a specific machine,
    capturing samples of keywords spoken by particular human beings, or logging biosignal
    data from individual farm animals.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，您将收集与个体实体相关的数据样本。例如，您可能正在监测特定机器的振动，捕获特定人类发言的关键词样本，或者记录个体农场动物的生物信号数据。
- en: 'In these cases, it’s crucially important to capture as much relevant metadata
    as possible about each individual entity. In the case of a machine, you might
    capture:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，关键是尽可能多地捕获有关每个单独实体的相关元数据。例如对于一台机器，您可能会捕获：
- en: The exact make and model
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确的制造和型号
- en: The machine’s production run
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器的生产周期
- en: The place the machine is installed
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器安装的地点
- en: The work the machine is being used for
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器正在使用的工作
- en: 'In the case of a person who is speaking keywords, you might try and capture
    any conceivable property that might affect their voice. For example:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个说关键词的人，您可能会尝试捕捉可能影响其语音的任何可想象的属性。例如：
- en: Physical characteristics, such as age, gender, or medical conditions
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身体特征，例如年龄、性别或医疗条件
- en: Cultural characteristics, such as accent, race, or nationality
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文化特征，例如口音、种族或国籍
- en: Personal characteristics, such as profession or income level
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个人特征，例如职业或收入水平
- en: 'You should attach this metadata to the individual samples that it relates to.
    This will allow you to split your dataset into subgroups according to metadata.
    You can use this ability to understand two things in great depth:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该将这些元数据附加到与之相关的各个样本上。这将允许您根据元数据将您的数据集分成子组。您可以利用这种能力来深入理解两件事：
- en: During algorithm development, you will understand the makeup of your dataset
    and where you are missing representation and balance.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在算法开发过程中，您将了解您的数据集的组成以及您缺少代表性和平衡的地方。
- en: During evaluation of your system, you will understand the weak areas of your
    model in terms of subgroups of your dataset.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估您的系统时，您将了解到在数据集的子组中，您的模型存在的弱点。
- en: For example, imagine you are training a model to detect faults in a machine.
    By analyzing your metadata, you may discover that most of your data samples have
    come from machines of a specific production run. In this case, you may wish to
    collect data from other production runs to improve representation of your dataset.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下，您正在训练一个检测机器故障的模型。通过分析您的元数据，您可能会发现您的大多数数据样本来自特定生产批次的机器。在这种情况下，您可能希望收集来自其他生产批次的数据，以改善数据集的代表性。
- en: In a different situation, you may be using a keyword dataset to evaluate a keyword-spotting
    model. By cross referencing the model’s performance on different data samples
    with the samples’ metadata, you may discover that the model performs better on
    samples taken from older speakers versus younger ones. In this case, you may be
    able to collect more training data from younger speakers in order to improve the
    performance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种情况下，您可能会使用关键词数据集来评估关键词检测模型。通过交叉引用模型在不同数据样本上的表现与样本的元数据，您可能会发现模型在来自年长者的样本上表现更好。在这种情况下，您可能希望收集更多来自年轻说话者的训练数据，以改善性能。
- en: In this way, metadata helps reduce risk. Without sample-level metadata, you
    are blind to the composition of your dataset and the way your model performs on
    different groups within it. When you’re armed with detailed information about
    the provenance of your data, you’re able to build better products.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，元数据有助于降低风险。如果没有样本级元数据，您将对数据集的构成和模型在其中不同组别的表现方式一无所知。当您掌握了关于数据来源的详细信息时，您能够构建更好的产品。
- en: Ensuring Data Quality
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保数据质量
- en: 'Earlier in this chapter we listed the properties that an ideal dataset should
    have:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早些时候，我们列出了理想数据集应具备的属性：
- en: Relevant
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关
- en: Representative
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表性
- en: Balanced
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡的
- en: Reliable
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可靠的
- en: Well formatted
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式良好
- en: Well documented
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好文档化
- en: Appropriately sized
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适当大小的
- en: As we learned in [“Data-Centric Machine Learning”](#data_centric_ml), a high-quality
    dataset reduces both the amount of data that is required and the impact of algorithm
    choice on creating an effective system. It’s a lot easier for machine learning
    systems to get useful results when they are trained and evaluated with good data.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[“数据中心机器学习”](#data_centric_ml)中学到的，高质量的数据集既减少了所需数据量，又减少了算法选择对创建有效系统的影响。当机器学习系统使用良好的数据进行训练和评估时，它们更容易获得有用的结果。
- en: But what is the best way to understand the quality of your dataset? The truth
    is that it comes down to domain expertise. If you have deep insight into the problem
    domain you are tackling, you’ll be able to draw on that insight to help evaluate
    your data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，了解数据集质量的最佳方法是什么？事实上，这归结于领域专业知识。如果您对解决的问题领域有深刻的见解，您将能够利用这些见解来帮助评估您的数据。
- en: Ensuring Representative Datasets
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保代表性数据集
- en: The most important property of a dataset is that it is representative. The reason
    for this is that the goal of an AI algorithm is to model a real-world situation
    in order to make decisions. The only mechanism it has for learning about the real
    world is the dataset used to train or design it. This means that if a dataset
    is not representative, the resulting algorithms will fail to represent the real
    world.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集最重要的属性是其代表性。这是因为AI算法的目标是模拟真实世界的情况以做出决策。它学习真实世界的唯一机制就是用于训练或设计它的数据集。这意味着如果数据集不具代表性，生成的算法将无法真实反映现实世界。
- en: For example, imagine you are building an AI system to help recognize different
    types of plant disease, using photographs of afflicted plants. If your dataset
    does not include photographs of the correct plants, or the appropriate symptoms,
    there is no way the AI system you design can be effective, no matter how sophisticated
    its algorithms.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下，您正在构建一个AI系统来帮助识别不同类型的植物病害，使用患病植物的照片。如果您的数据集中没有正确植物的照片或适当的症状，无论其算法多么复杂，您设计的AI系统都无法有效。
- en: Even worse, since the dataset is also used to evaluate the system’s performance,
    we’ll have no idea that there is even a problem until we deploy the model to the
    field.^([11](ch07.html#idm45988812089344))
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，由于数据集还用于评估系统的性能，我们在部署模型到现场之前甚至不知道存在问题。^([11](ch07.html#idm45988812089344))
- en: This is where domain expertise comes in. If you are an expert in plant diseases,
    you can use your knowledge to help understand whether the dataset is representative
    of real-world conditions. For example, perhaps your dataset is missing photos
    of some species of plants that are affected by the disease you wish to identify.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是领域专业知识的作用所在。如果您是植物疾病的专家，您可以利用自己的知识帮助理解数据集是否代表了真实世界的情况。例如，也许您的数据集缺少受到您希望识别的疾病影响的某些植物物种的照片。
- en: Metadata is incredibly helpful for this process. If your dataset contains metadata
    that indicates the plant species in each photograph, a domain expert can simply
    review the list of species and immediately notice whether one is missing.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据对此过程非常有帮助。如果您的数据集包含指示每张照片中植物物种的元数据，领域专家可以简单地查看物种列表，并立即注意到是否有物种缺失。
- en: Another useful way to use metadata is to plot the distributions of specific
    metadata attributes throughout the data. For example, you might choose to plot
    the numbers of samples that belong to each species. If this distribution does
    not look sensible with regards to real-world conditions, you may need to collect
    more data. For example, you may have significantly more records from one species
    than another, as in [Figure 7-4](#metadata_species_figure).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个使用元数据的有用方法是绘制特定元数据属性在数据中的分布图。例如，您可以选择绘制属于每种物种的样本数量。如果这种分布与现实世界的条件不合理，您可能需要收集更多数据。例如，您可能从一种物种中获得的记录要比另一种物种多得多，如[图7-4](#metadata_species_figure)所示。
- en: '![A chart showing the percentage of records from each species.](assets/aiae_0704.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![显示每个物种记录百分比的图表。](assets/aiae_0704.png)'
- en: Figure 7-4\. This dataset features many more records for some species than others.
    This may cause problems with fairness; for example, it’s likely your algorithm
    will perform much better with species B than with species F.
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4。该数据集中某些物种的记录明显多于其他物种。这可能会导致公平性问题；例如，您的算法在物种B上的表现可能远远优于物种F。
- en: In addition to the dataset at large, it’s important that representation is maintained
    across labels. For example, you should ensure there is equally good representation
    of every affected species within each class of plant disease you are attempting
    to identify.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 除了整体数据集外，重要的是在各个标签中保持代表性。例如，您应确保在试图识别的每一类植物病害中，每种受影响的物种都有同等好的代表性。
- en: If your dataset represents individual entities, you should also check to ensure
    that your dataset is balanced with regards to those entities. For instance, perhaps
    all of the photos of one species were taken from a single plant, while the photos
    of another species were taken from multiple plants.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据集代表个体实体，还应检查确保数据集在这些实体方面平衡。例如，可能某种植物的所有照片来自一个植株，而另一种植物的照片则来自多个植株。
- en: A domain expert should be able to help identify the axes of data that are important
    to explore in this way. But what if adequate metadata is not available? This is
    very common when using data that was not deliberately collected for a specific
    project. In this case, you’ll have to embark on a systemic review of the dataset.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 领域专家应该能够帮助确定以这种方式探索的数据轴。但如果没有足够的元数据怎么办？当使用未专门收集为特定项目的数据时，这种情况非常常见。在这种情况下，您将不得不对数据集进行系统性审查。
- en: Reviewing Data by Sampling
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过抽样审查数据
- en: The challenge of reviewing data quality, especially when there is limited accompanying
    metadata, is that it’s often infeasible to look over every sample of data individually.
    Datasets can be gigantic, and the time of domain experts is precious (and expensive).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量审查的挑战在于，特别是在伴随的元数据有限的情况下，往往无法逐个样本地审查每个数据。数据集可能非常庞大，领域专家的时间宝贵（也很昂贵）。
- en: Fortunately, *sampling* gives us a way to review data without having to inspect
    every last item. For a given dataset, a sufficiently sized random sample of records
    will have approximately the same representation and balance as the larger dataset.
    This smaller sample can be inspected thoroughly by a domain expert in order to
    understand the quality of the dataset as a whole.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，*抽样*为我们提供了一种审查数据的方法，而无需检查每一个项目。对于给定的数据集，记录的足够大的随机样本将具有与更大数据集几乎相同的表示和平衡。这个较小的样本可以被领域专家彻底检查，以了解整个数据集的质量。
- en: The tricky part is determining how large a sample needs to be. It needs to be
    large enough to have a reasonable probability of including the characteristics
    we care about but small enough to be reviewed in a reasonable time.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 棘手的部分在于确定样本需要多大。它必须足够大，以有合理的概率包含我们关心的特征，但又小到可以在合理的时间内审查。
- en: For example, imagine our domain expert is trying to understand whether the dataset
    contains enough instances of a certain species of plant. To do this, they can
    count the instances of that plant species in a sample of the data and calculate
    the ratio between instances of that species and any others. But how big a sample
    size is required in order for us to assume that the ratio between plant species
    is equivalent between the sample and the entire dataset?
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下我们的领域专家正在尝试确定数据集是否包含足够数量的某种植物物种的实例。为了做到这一点，他们可以计算数据样本中该植物物种的实例数量，并计算该物种与其他物种实例之间的比率。但是，需要多大的样本量才能使我们假设样本和整个数据集之间的植物物种比率是等价的呢？
- en: 'There’s actually a formula we can use to estimate the sample size. It looks
    like this:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上有一个公式可以用来估计样本大小。它看起来像这样：
- en: <math alttext="upper S a m p l e s i z e equals StartFraction left-parenthesis
    upper Z s c o r e right-parenthesis squared asterisk s t a n d a r d d e v i a
    t i o n asterisk left-parenthesis 1 minus s t a n d a r d d e v i a t i o n right-parenthesis
    Over left-parenthesis m a r g i n o f e r r o r right-parenthesis squared EndFraction"
    display="block"><mrow><mi>S</mi> <mi>a</mi> <mi>m</mi> <mi>p</mi> <mi>l</mi> <mi>e</mi>
    <mi>s</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <mo>=</mo> <mfrac><mrow><msup><mrow><mo>(</mo><mi>Z</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>*</mo><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>a</mi><mi>r</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>v</mi><mi>i</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>*</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>a</mi><mi>r</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>v</mi><mi>i</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>)</mo></mrow></mrow>
    <msup><mrow><mo>(</mo><mi>m</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>i</mi><mi>n</mi><mi>o</mi><mi>f</mi><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mfrac></mrow></math>
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper S a m p l e s i z e equals StartFraction left-parenthesis
    upper Z s c o r e right-parenthesis squared asterisk s t a n d a r d d e v i a
    t i o n asterisk left-parenthesis 1 minus s t a n d a r d d e v i a t i o n right-parenthesis
    Over left-parenthesis m a r g i n o f e r r o r right-parenthesis squared EndFraction"
    display="block"><mrow><mi>S</mi> <mi>a</mi> <mi>m</mi> <mi>p</mi> <mi>l</mi> <mi>e</mi>
    <mi>s</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <mo>=</mo> <mfrac><mrow><msup><mrow><mo>(</mo><mi>Z</mi><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>*</mo><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>a</mi><mi>r</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>v</mi><mi>i</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>*</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>a</mi><mi>r</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>v</mi><mi>i</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>)</mo></mrow></mrow>
    <msup><mrow><mo>(</mo><mi>m</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>i</mi><mi>n</mi><mi>o</mi><mi>f</mi><mi>e</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mfrac></mrow></math>
- en: In this formula, the *margin of error* represents the amount of difference we’re
    willing to tolerate between the ratio in our sample and in our full dataset. It’s
    common to set this to 5%, meaning we’ll be OK with the ratio in our sample being
    either 2.5% higher or 2.5% lower than the ratio in the entire dataset.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，*误差边界* 表示我们愿意容忍的样本比率与整个数据集比率之间的差异量。通常将其设置为5%，这意味着我们可以接受我们样本中的比率比整个数据集中的比率高出或低出2.5%。
- en: The Z score expresses our confidence level, or how confident we need to be that
    the number we get will *actually* fall within the bounds of our margin of error.
    A reasonable confidence level is 95%, which would give us a Z score of 1.96,^([12](ch07.html#idm45988812028592))
    assuming a dataset of typical size (anything more than tens of thousands of samples).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Z分数表达了我们的置信水平，或者说我们需要确信得到的数字实际上会在我们误差边界的范围内。一个合理的置信水平是95%，这将给我们一个Z分数为1.96，^([12](ch07.html#idm45988812028592))
    假设数据集的大小是典型的（超过几万个样本）。
- en: Finally, the standard deviation represents how much we expect the data to vary.
    Since there’s no real way to know this ahead of time, we can just play it safe
    and set it to 0.5, which maximizes the sample size.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，标准偏差表示我们期望数据变化的程度。由于没有办法预先知道这个值，我们可以安全地将其设为0.5，这样可以最大化样本量。
- en: 'If we plug all of these together, we’ll get the following:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将所有这些都整合在一起，我们将得到以下结果：
- en: <math alttext="upper S a m p l e s i z e equals StartFraction left-parenthesis
    1.96 right-parenthesis squared asterisk 0.5 asterisk left-parenthesis 1 minus
    0.5 right-parenthesis Over left-parenthesis 0.05 right-parenthesis squared EndFraction
    equals StartFraction 0.9604 Over 0.0025 EndFraction equals 384.16" display="block"><mrow><mi>S</mi>
    <mi>a</mi> <mi>m</mi> <mi>p</mi> <mi>l</mi> <mi>e</mi> <mi>s</mi> <mi>i</mi> <mi>z</mi>
    <mi>e</mi> <mo>=</mo> <mfrac><mrow><msup><mrow><mo>(</mo><mn>1</mn><mo lspace="0%"
    rspace="0%">.</mo><mn>96</mn><mo>)</mo></mrow> <mn>2</mn></msup> <mo>*</mo><mn>0</mn><mo
    lspace="0%" rspace="0%">.</mo><mn>5</mn><mo>*</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mn>0</mn><mo
    lspace="0%" rspace="0%">.</mo><mn>5</mn><mo>)</mo></mrow></mrow> <msup><mrow><mo>(</mo><mn>0</mn><mo
    lspace="0%" rspace="0%">.</mo><mn>05</mn><mo>)</mo></mrow> <mn>2</mn></msup></mfrac>
    <mo>=</mo> <mfrac><mrow><mn>0</mn><mo lspace="0%" rspace="0%">.</mo><mn>9604</mn></mrow>
    <mrow><mn>0</mn><mo lspace="0%" rspace="0%">.</mo><mn>0025</mn></mrow></mfrac>
    <mo>=</mo> <mn>384</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>16</mn></mrow></math>
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper S a m p l e s i z e equals StartFraction left-parenthesis
    1.96 right-parenthesis squared asterisk 0.5 asterisk left-parenthesis 1 minus
    0.5 right-parenthesis Over left-parenthesis 0.05 right-parenthesis squared EndFraction
    equals StartFraction 0.9604 Over 0.0025 EndFraction equals 384.16" display="block"><mrow><mi>S</mi>
    <mi>a</mi> <mi>m</mi> <mi>p</mi> <mi>l</mi> <mi>e</mi> <mi>s</mi> <mi>i</mi> <mi>z</mi>
    <mi>e</mi> <mo>=</mo> <mfrac><mrow><msup><mrow><mo>(</mo><mn>1</mn><mo lspace="0%"
    rspace="0%">.</mo><mn>96</mn><mo>)</mo></mrow> <mn>2</mn></msup> <mo>*</mo><mn>0</mn><mo
    lspace="0%" rspace="0%">.</mo><mn>5</mn><mo>*</mo><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mn>0</mn><mo
    lspace="0%" rspace="0%">.</mo><mn>5</mn><mo>)</mo></mrow></mrow> <msup><mrow><mo>(</mo><mn>0</mn><mo
    lspace="0%" rspace="0%">.</mo><mn>05</mn><mo>)</mo></mrow> <mn>2</mn></msup></mfrac>
    <mo>=</mo> <mfrac><mrow><mn>0</mn><mo lspace="0%" rspace="0%">.</mo><mn>9604</mn></mrow>
    <mrow><mn>0</mn><mo lspace="0%" rspace="0%">.</mo><mn>0025</mn></mrow></mfrac>
    <mo>=</mo> <mn>384</mn> <mo lspace="0%" rspace="0%">.</mo> <mn>16</mn></mrow></math>
- en: Since there’s no such thing as a fraction of a sample, we can round the sample
    size up to 385\. This tells us we’ll need to randomly sample 385 items in order
    to have 95% confidence that the ratio of one species to another lies within 5%
    of the value we see in our random sample.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有部分样本的概念，我们可以将样本量四舍五入至385。这告诉我们，我们需要随机抽样385个项目，以确保我们有95%的置信度，认为一个物种与另一个物种的比率与我们在随机样本中看到的值相差不超过5%。
- en: 'It turns out that this number doesn’t vary too much with the size of the dataset,
    at least for datasets of sizes that are relevant for machine learning. It’s most
    sensitive to changes in the margin of error: if you want a margin of error of
    only 1%, you’ll need to review a sample with 9,604 items. Qualtrics provides a
    [handy online calculator](https://oreil.ly/wEjUk) that makes it easy to experiment.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这个数字在数据集大小方面并没有太大的变化，至少对于机器学习相关的数据集来说是这样。它对误差边界的变化最为敏感：如果您只希望有1%的误差边界，那么您将需要审查一个包含9,604个项目的样本。Qualtrics提供了一个[方便的在线计算器](https://oreil.ly/wEjUk)，使实验变得容易。
- en: All this goes to say that it should generally suffice to randomly select a few
    hundred samples from your dataset.^([13](ch07.html#idm45988812001408)) This should
    be a manageable number to review and will also give you some reasonable insight
    into whether your dataset has acceptable quality.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都表明，通常随机选择你的数据集中的几百个样本应该足够了。^([13](ch07.html#idm45988812001408)) 这应该是一个可以审查的管理数量，并且还会为您提供一些关于数据集是否具有可接受质量的合理见解。
- en: 'Of course, this assumes that the subgroups you are looking for are large enough
    to fit within the error bounds. For example, if a plant species represents less
    than 5% of the data, then it’s unlikely we will find it in a sample of 385 items.
    However, if you are hunting for underrepresented subgroups, then this will still
    be a helpful result: it will guide you to add more data, eventually allowing the
    group to be detectable with random sampling.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这假设您正在寻找的子组足够大，可以适应误差边界。例如，如果植物物种占数据的不到5%，那么在385个项目的样本中，我们不太可能找到它。然而，如果您正在寻找代表少数的子组，这仍然是一个有用的结果：它将指导您添加更多数据，最终使得该组在随机抽样中可检测。
- en: Label Noise
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标签噪声
- en: Beyond representativeness, another major source of dataset quality issues comes
    from something called *label noise*. Labels give us the values that we are trying
    to use AI to predict. For example, if we’re training a plant disease classifier,
    a photo of an unhealthy plant might be labeled with the exact disease that is
    afflicting it. Labels don’t have to be classes, though—for example, if we are
    solving a regression problem we would expect the data to be labeled with the number
    that we are trying to predict.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 超越典型性，数据集质量问题的另一个主要来源是所谓的*标签噪声*。标签为我们提供了我们试图用AI预测的数值。例如，如果我们正在训练植物病害分类器，一张不健康植物的照片可能会被标记为确切的病害。不过，标签不一定是类别，例如，如果我们正在解决一个回归问题，我们期望数据标记为我们试图预测的数字。
- en: Unfortunately, the labels attached to data are not always correct. Since most
    data is labeled by human beings, it’s common for errors to creep in. These errors
    can be quite significant. A research team from MIT found that an average of 3.4%
    of samples are incorrectly labeled across a set of commonly used public datasets^([14](ch07.html#idm45988811991744))—they
    even [built a website to showcase the errors](https://oreil.ly/vrWZI).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，附加到数据的标签并不总是正确的。由于大多数数据都是由人类标记的，错误往往会悄然而入。这些错误可能相当显著。麻省理工学院的一个研究团队发现，在一组常用的公共数据集中，平均有3.4%的样本标记错误^([14](ch07.html#idm45988811991744))—他们甚至[建立了一个网站来展示这些错误](https://oreil.ly/vrWZI)。
- en: Label noise isn’t a total catastrophe. Machine learning models are pretty good
    at learning to cope with noise. But it does have a significant impact,^([15](ch07.html#idm45988811988208))
    and to squeeze the most performance out of your models it can be worth trying
    to clean up noisy labels. The constraints of edge AI already place a premium on
    model performance. Cleaning up noisy labels may deliver a good return on investment
    versus spending more time on algorithm design or model optimization.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 标签噪声并非完全是灾难性的。机器学习模型在学习如何应对噪声方面表现得相当不错。但它确实会产生显著影响，^([15](ch07.html#idm45988811988208))
    而为了最大限度地提高模型性能，清理嘈杂的标签可能是值得尝试的。边缘AI的约束已经在模型性能上投入了很多。清理嘈杂的标签可能会带来比花更多时间在算法设计或模型优化上更高的回报。
- en: The simplest way to identify label noise is by reviewing random samples of data,
    but with large datasets this can be like looking for needles in a haystack. Instead
    of sampling randomly, it’s better to focus the search more intelligently.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 识别标签噪声的最简单方法是审查数据的随机样本，但对于大型数据集来说，这就像在干草堆里寻找针一样困难。与随机抽样不同，更明智的做法是更智能地聚焦搜索。
- en: A good method for doing this is by hunting for outliers within a class. If a
    sample is misclassified, it likely appears significantly different from the other
    members of the class it is mislabeled as. For simple data, this may be easy using
    standard data science tools. For high-dimensional data, like images or audio,
    it can be more challenging.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的方法是在一个类内寻找异常值。如果一个样本被错误分类，它可能与其误标记的类中的其他成员显著不同。对于简单的数据，可以使用标准的数据科学工具来轻松实现。对于像图像或音频这样的高维数据，这可能会更具挑战性。
- en: The end-to-end edge AI platform that Edge Impulse uses has an interesting solution
    to this problem. Edge Impulse’s feature explorer uses an unsupervised dimensionality
    reduction algorithm to project complex data into a simplified 2D space, where
    proximity correlates with similarity. This approach [makes it easy to spot outliers](https://oreil.ly/_9-Ny),
    as in [Figure 7-5](#outlier_detected_figure).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Edge Impulse 使用的端到端边缘 AI 平台对这个问题有一个有趣的解决方案。Edge Impulse 的特征探索器使用无监督降维算法将复杂数据投影到简化的二维空间，其中的接近性与相似性相关。这种方法[使得可以轻松发现异常值](https://oreil.ly/_9-Ny)，如[图
    7-5](#outlier_detected_figure)中所示。
- en: '![A screenshot of the Data Explorer in Edge Impulse.](assets/aiae_0705.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![Edge Impulse 中的数据探索器截图。](assets/aiae_0705.png)'
- en: Figure 7-5\. Each dot represents a sample of data, and the distance between
    dots represents their similarity. Outliers, such as those highlighted with arrows,
    are unusual samples. When samples appear close to those belonging to other classes,
    it’s worth investigating to see if they have been mislabeled.
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. 每个点代表数据的一个样本，点与点之间的距离代表它们的相似性。异常值，例如用箭头标出的那些，是不寻常的样本。当样本出现在属于其他类别的样本附近时，值得调查，看看它们是否被误标记了。
- en: Another simple way to hunt for noisy class labels is to assume that a model
    trained on the data will be *less confident* at classifying noisy samples. If
    training samples are ranked in order of a trained model’s confidence at assigning
    them a class, it’s likely that mislabeled samples will appear toward the bottom
    of the list.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种寻找嘈杂类标签的简单方法是假设一个训练在数据上的模型对分类嘈杂样本的置信度较低。如果训练样本按照训练模型在分配类别时的置信度排序，很可能误标记的样本会出现在列表的底部。
- en: In datasets for problems other than classification, label noise looks a bit
    different. For example, label noise in a regression dataset consists of error
    in the target values, while label noise in an object detection or segmentation
    dataset means the bounding boxes or segmentation maps do not line up well with
    the objects they are supposed to enclose.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在除分类之外的问题数据集中，标签噪声看起来有所不同。例如，回归数据集中的标签噪声包括目标值中的误差，而在物体检测或分割数据集中的标签噪声意味着边界框或分割图与它们应该围绕的对象不匹配。
- en: Label noise detection and mitigation is an ongoing area of study. If you have
    a particularly noisy dataset it may be worth digging into the scientific literature—a
    quick search for “label noise” in [Google Scholar](https://scholar.google.com)
    will serve you well.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 标签噪声的检测和缓解是一个持续研究的领域。如果你有一个特别嘈杂的数据集，深入科学文献可能会有所帮助——在[Google 学术](https://scholar.google.com)中快速搜索“label
    noise”会对你有所裨益。
- en: Avoiding label noise
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 避免标签噪声
- en: Label noise typically occurs as a result of human error during the labeling
    of the data. Humans aren’t great at producing reliable results for repetitive
    tasks like data labeling, even if they have the right knowledge. In addition,
    sometimes it’s unclear from the data what the correct label should be. For example,
    even medical experts do not always agree whether or not a diagnostic image shows
    a disease.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 标签噪声通常是由于在数据标记过程中的人为错误导致的。人类在进行数据标记等重复性任务时并不擅长产生可靠的结果，即使他们有正确的知识也是如此。此外，有时从数据中并不清楚正确的标签应该是什么。例如，即使是医疗专家在判断一张诊断图像是否显示疾病时也不总是一致的。
- en: 'In many cases, labeling errors are the result of a misunderstanding of the
    labeling task. In projects that require nontrivial amounts of labeling work, it’s
    important to provide a “rater guide”: a handbook for data labelers. The guide
    should include examples that clearly illustrate the guidelines. Over the course
    of a project, it can be updated with any interesting or unclear examples that
    are found.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，标签错误是对标签任务理解不足的结果。在需要大量标签工作的项目中，提供一个“评分员指南”是很重要的：一本为数据标记者准备的手册。该指南应包括清晰说明准则的示例。在项目的过程中，可以更新任何有趣或不明确的示例。
- en: To minimize the impact of human error, it may be useful to use multiple labelers.
    If the labelers disagree on a label, the sample can be flagged for closer inspection.
    If there’s no clear answer for a given sample, a voting system can be used to
    come up with a definitive label—or the sample can be rejected. The correct course
    of action will vary depending on the project and will require the application
    of domain expertise.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大限度减少人为错误的影响，使用多个标注者可能是有用的。如果标注者在标签上存在分歧，可以将样本标记为需要进一步检查。如果对于某个样本没有明确的答案，可以使用投票系统来得出确定的标签，或者可以拒绝该样本。正确的操作方式将根据项目的不同而异，并需要应用领域专业知识。
- en: Common Data Errors
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的数据错误
- en: 'Representation and balance problems are large, structural issues that reflect
    the way a dataset is designed, while label noise is a result of the collection
    process that impacts individual samples. Similar to label noise, there are a multitude
    of common errors that can affect your data on a per-sample level. Here are some
    of the common issues seen in edge AI projects:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表示和平衡问题是大的结构性问题，反映了数据集设计的方式，而标签噪声是影响个别样本的收集过程的结果。与标签噪声类似，还有许多常见错误可能会影响到边缘AI项目中的数据。以下是一些在边缘AI项目中常见的问题：
- en: Label noise
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 标签噪声
- en: As described in detail in [“Label Noise”](#label_noise), it is common to find
    problems with the way that data is labeled, due to human or machine error.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[“标签噪声”](#label_noise)中详细描述的那样，由于人为或机器错误，数据标签的问题是常见的。
- en: Missing values
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值
- en: For a variety of reasons, some records in your dataset may be missing values
    for certain features. For example, a bug in a data collection script might lead
    to a value not being written to the correct place. This is quite common, and one
    of the most important data preparation tasks is figuring out the best way to address
    missing values.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 由于各种原因，数据集中的某些记录可能会缺少某些特征的值。例如，数据收集脚本中的错误可能导致值未被写入正确的位置。这种情况非常普遍，而最重要的数据准备任务之一就是找出处理缺失值的最佳方法。
- en: Sensor problems
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器问题
- en: Technical issues with sensors can result in major data quality issues. Common
    problems affecting sensors include excessive amounts of noise, incorrect calibration,
    changes in ambient conditions that impact sensor readings, and degradation that
    leads to changes in values over time.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器的技术问题可能会导致主要的数据质量问题。影响传感器的常见问题包括过多的噪声、不正确的校准、影响传感器读数的环境条件变化，以及随时间推移导致数值变化的退化。
- en: Incorrect values
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 不正确的值
- en: Sometimes the values in a dataset do not reflect what was measured. For example,
    a reading may be corrupted while being transmitted from one place to another.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 有时数据集中的值不反映实际测量的内容。例如，传输过程中可能会使读数损坏。
- en: Outliers
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值
- en: An outlier is a value that is far outside of the expected range. Sometimes outliers
    can be natural, but often they are a symptom of things like sensor issues or unexpected
    variations in ambient conditions.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是远超出预期范围的值。有时异常值可能是自然的，但通常它们是传感器问题或环境条件意外变化的症状。
- en: Inconsistent scaling
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 不一致的缩放
- en: The same value can be represented in many different ways in a digital system.
    For example, a temperature reading could be in Celsius or Fahrenheit, and a sensor
    value may be normalized or not. If different scaling is used for values for the
    same feature, perhaps when data from two datasets are combined, problems will
    result.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 同一个值可以在数字系统中以多种不同的方式表示。例如，温度读数可以是摄氏度或华氏度，传感器值可能已经标准化或未标准化。如果对同一特征的值使用了不同的缩放，比如将两个数据集的数据合并，就会出现问题。
- en: Inconsistent representation
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 不一致的表示
- en: Beyond scaling, there are many other ways representation can vary. For example,
    a data point might be stored as either a 16-bit floating point value between 0
    and 1 or an 8-bit integer between 0 and 255\. The pixel order in a color image
    might be either red, green, blue, or blue, green, red. An audio file may be compressed
    as MP3 or exist as a raw buffer of samples. Inconsistent representation can lead
    to a lot of difficulties. It’s important to document this stuff well—perhaps even
    in metadata attached to each sample.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 除了缩放之外，表示方法还有很多其他的变化方式。例如，一个数据点可以被存储为0到1之间的16位浮点值，或者是0到255之间的8位整数。彩色图像中的像素顺序可以是红色、绿色、蓝色，或者是蓝色、绿色、红色。音频文件可以被压缩为MP3格式，也可以存在为一组样本的原始缓冲区。不一致的表示方法可能会导致许多困难。重要的是要充分记录这些内容，也许甚至要附加在每个样本中的元数据中。
- en: Unexpected rates
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 意外的比率
- en: A particularly nasty subtype of inconsistent representation is inconsistency
    in sampling rates. For example, a dataset may contain some samples collected at
    8 kHz (8,000 times per second) and some collected at 16 kHZ. If they aren’t processed
    differently, they’ll seem to contain very different values. It’s especially bad
    when variations in sample rate and bit depth combine—at a glance, it’s very hard
    to tell an 8 kHz 16-bit sample from a 16 kHz 8-bit sample!
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 不一致表示的一个特别讨厌的子类型是采样率不一致。例如，数据集可能包含以8 kHz（每秒8000次）采集的样本和以16 kHz采集的样本。如果它们没有被不同处理，它们看起来将包含非常不同的值。当采样率和比特深度的变化结合在一起时尤为糟糕——一眼看去，很难区分8
    kHz 16位样本和16 kHz 8位样本！
- en: Insecure data
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 不安全的数据
- en: If you’re collecting data from the field, it’s imperative that you have secure
    mechanisms for collecting and transporting it. For example, you might cryptographically
    sign samples in a way that guarantees they have not been tampered with before
    being stored. If an attacker has the ability to tamper with your data, they can
    directly influence the resulting algorithms, distorting your system in their favor.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在从现场收集数据，确保有安全的机制来收集和传输数据至关重要。例如，你可以使用加密方式签名样本，以确保它们在存储之前没有被篡改。如果攻击者能够篡改你的数据，他们可以直接影响最终的算法，从而扭曲你的系统使其偏向他们的利益。
- en: Nearly every AI project will involve work to fix some of these types of errors.
    In [“Data Cleaning”](#data_cleaning), we’ll encounter some of the methods used
    for addressing these issues.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个人工智能项目都需要处理这些类型错误的工作。在[“数据清洗”](#data_cleaning)中，我们将介绍一些用于解决这些问题的方法。
- en: Drift and Shift
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 漂移和变化
- en: Everything changes and nothing stands still.
  id: totrans-291
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 万物皆变，无一物常存。
- en: ''
  id: totrans-292
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Heraclitus of Ephesus, 535–475 BC
  id: totrans-293
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 爱菲索斯的赫拉克利特，公元前535-475年
- en: 'A dataset is just a snapshot in time: it represents the state of a system during
    the period when it was collected. Since the real world tends to change over time,
    even the highest-quality dataset can start to get a bit stale. This process of
    change is known by a few terms, including *drift*, *concept drift*, and *shift*.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集只是时间的一个快照：它代表了在收集期间系统的状态。由于现实世界随时间而变化，即使是最高质量的数据集也可能变得有些陈旧。这种变化过程以几个术语来描述，包括*漂移*、*概念漂移*和*变化*。
- en: When drift occurs, a dataset is no longer representative of the current state
    of the real-world system. This means that any model or algorithm developed with
    the dataset will be based on a faulty understanding of the system, and it probably
    won’t perform well once deployed.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生漂移时，数据集不再代表实际系统的当前状态。这意味着使用该数据集开发的任何模型或算法都是基于对系统的错误理解，并且一旦部署后可能表现不佳。
- en: 'Drift can happen in a few different ways. Let’s explore them in the context
    of a dataset that captures the vibration of an industrial machine measured during
    normal use:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移可以以几种不同的方式发生。让我们在一个捕捉工业机器振动的数据集的背景下探讨这些方式：
- en: Sudden change
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 突然变化
- en: Sometimes there’s an abrupt change in real-world conditions. For example, workers
    might move a vibration sensor to a different part of the machine, suddenly changing
    the nature of the motion it picks up.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候现实世界的条件会突然改变。例如，工人可能会将振动传感器移动到机器的不同部位，从而突然改变了其所感知到的运动特性。
- en: Gradual change
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 逐渐变化
- en: Signals may change gradually over time. For example, the machine’s moving parts
    may gradually wear down over time, slowly changing the nature of their vibration.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 信号可能会随时间逐渐变化。例如，机器的运动部件随着时间逐渐磨损，慢慢改变其振动的特性。
- en: Cyclic change
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 周期性变化
- en: It’s common for changes to happen in cycles, or seasonally. For example, the
    machine’s vibration may change with the ambient temperature of its location, which
    varies between summer and winter.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 变化往往会以循环或季节性的形式发生。例如，机器的振动可能会随其所处位置的环境温度变化而变化，而这种温度在夏季和冬季之间会有所不同。
- en: Because change is inevitable, drift is one of the most common problems faced
    by AI projects. It can happen in everything from physical configuration (like
    the placement of sensors) to cultural evolution (like the gradual shift in language
    and pronunciation over time).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 因为变化是不可避免的，漂移是人工智能项目面临的最常见问题之一。它可以发生在从物理配置（如传感器的放置位置）到文化演变（如语言和发音逐渐变化）的任何方面。
- en: Managing drift requires keeping your dataset updated over time, which we’ll
    talk about more in [“Building a Dataset over Time”](#building_a_dataset_over_time).
    It also requires monitoring the performance of your model in the field, which
    we will cover in the following chapters.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 管理漂移需要随时间更新您的数据集，我们将在[“随时间构建数据集”](#building_a_dataset_over_time)中更详细地讨论。它还需要监控您模型在实际场景中的性能，这将在接下来的章节中涉及。
- en: Thanks to drift, an edge AI project is never really “finished”—it will almost
    always require ongoing effort in either monitoring or maintenance.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 由于漂移的存在，边缘人工智能项目永远不会真正“完成” —— 它几乎总是需要在监控或维护方面进行持续努力。
- en: The Uneven Distribution of Errors
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误的不均匀分布
- en: As we’ve seen, there are many different types of errors that can afflict a dataset.
    To achieve a high-quality dataset, you’ll need to keep track of errors and make
    sure they stay within acceptable levels. However, it’s important to not only measure
    the presence or absence of errors—but also how they affect different subsets of
    your data.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，数据集可能存在许多不同类型的错误。为了获得高质量的数据集，您需要追踪错误并确保它们保持在可接受的水平内。然而，重要的不仅仅是测量错误的存在或缺失，还要考虑它们如何影响数据的不同子集。
- en: 'For example, imagine you are solving a classification problem with a balanced
    dataset of 10 classes. Across your dataset, you estimate via sampling that there
    is around 1% label noise: 1 in 100 data samples are incorrectly labeled. From
    an algorithmic point of view, this may feel acceptable. Perhaps you’ve trained
    a machine learning model on the data, and it appears to be effective based on
    its accuracy.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您正在解决一个具有 10 个类别的平衡数据集的分类问题。通过抽样估计，您的数据集中大约有 1% 的标签噪声：即每 100 个数据样本中有一个标记错误。从算法的角度来看，这可能是可以接受的。也许您已经在数据上训练了一个机器学习模型，并且根据其准确性来看，它似乎是有效的。
- en: But what if the 1% of incorrect labels are not evenly (or *symmetrically*) distributed
    across the dataset but instead are concentrated *asymmetrically* in a single class?
    Instead of 1 in 100 samples being mislabeled, 1 in *10* of the data items in this
    class might be labeled incorrectly. This could be enough to seriously impact the
    performance of your model for this class. Even worse, it will impact your ability
    to measure the performance in the same way that you can for other classes.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果这 1% 的错误标签并不均匀（或*对称地*）分布在数据集中，而是*不对称地*集中在单一类别中呢？在这个类别中，而不是每 100 个样本中有一个标记错误，而是每*10*个数据项中有一个可能会被错误标记。这可能足以严重影响您模型在这一类别的性能。更糟糕的是，它将影响您测量该类别性能的能力，就像您对其他类别的测量方式一样。
- en: Errors can also be asymmetric across subgroups that are not necessarily classes.
    For example, perhaps your dataset happens to include data that was collected from
    three different models of cars. If the sensors installed in one of the car models
    were faulty, the data for those models might contain errors. This is even more
    dangerous than when errors are asymmetric across classes because the impact is
    less easy to detect using standard performance metrics.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 错误也可能在不一定是类别的子群体之间不对称。例如，也许您的数据集包含从三种不同车型收集的数据。如果其中一种车型安装的传感器有问题，那么这些车型的数据可能包含错误。这比错误在类别之间不对称更危险，因为使用标准性能指标来检测其影响要困难得多。
- en: Asymmetric errors are likely to result in bias in your algorithms since they
    impact the performance of your system more for certain subgroups. When you’re
    looking for error in your data, you should take extra care to consider error rates
    from subgroups of your data, even if the overall level of error seems acceptable.
    As usual, domain expertise will be extremely helpful in determining the subgroups
    and how to best inspect them.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 不对称的错误可能会导致算法中的偏差，因为它们对某些子群体的系统性能影响更大。在查找数据中的错误时，您应特别注意考虑来自数据子群体的错误率，即使总体错误水平看似可接受。通常情况下，领域专业知识在确定子群体及其最佳检查方法方面将非常有帮助。
- en: Preparing Data
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'Going from raw data to a high-quality dataset is a long road with many steps.
    In this next section, we’ll walk that road and begin to understand the process.
    These will be our stops along the way:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始数据到高质量数据集的过程是一条漫长的道路，包含许多步骤。在接下来的部分，我们将沿着这条道路前行，开始理解这个过程。以下是我们沿途的站点：
- en: Labeling
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注
- en: Formatting
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式化
- en: Cleaning
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗
- en: Feature engineering
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Splitting
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 划分
- en: Data augmentation
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强
- en: One of these items, feature engineering, is really part of the algorithm development
    work we’ll be covering in [Chapter 9](ch09.html#developing_edge_ai_applications).
    However, it deserves a mention here because of the way its results are used in
    the process of refining your dataset.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一项，特征工程，实际上是我们将在[第9章](ch09.html#developing_edge_ai_applications)中涵盖的算法开发工作的一部分。然而，它在这里值得一提，因为它的结果在优化数据集的过程中如何使用。
- en: Our journey’s milestones assume you’ve already collected some initial raw data.
    You likely don’t have a fully representative or balanced dataset yet, but you
    have made a solid start. The data preparation process will help guide you as you
    grow and improve your dataset.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程里程碑假设您已经收集了一些初始原始数据。您可能还没有完全代表性或平衡的数据集，但您已经有了一个坚实的开始。数据准备过程将指导您在成长和改进数据集的过程中。
- en: Labeling
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记
- en: 'A typical edge AI dataset reflects a mapping between a set of raw inputs—for
    example, some time series sensor data—and a description of what those inputs *mean*.
    Our task is often to build or train a system of algorithms that can perform this
    mapping automatically: when presented with a set of raw inputs, it tells us what
    those inputs mean. Our application can then use that assumed meaning in order
    to make intelligent decisions.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的边缘AI数据集反映了一组原始输入（例如一些时间序列传感器数据）与这些输入*含义*的映射。我们的任务通常是构建或训练一系列算法系统，能够自动执行这种映射：当提供一组原始输入时，它告诉我们这些输入的含义。然后，我们的应用程序可以利用这个假设的含义来做出智能决策。
- en: 'In most datasets, that description of *meaning* comes in the form of labels.
    As we’ve seen, creating reliable algorithms requires high-quality labels. There
    are a few different ways that data can be labeled, and any given project may use
    a combination of them:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数数据集中，这种*含义*的描述以标签的形式呈现。正如我们所见，创建可靠的算法需要高质量的标签。数据可以通过几种不同的方式进行标记，任何项目可能会结合使用其中的几种方式：
- en: Labeling using features
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征进行标记
- en: Some datasets are labeled using their own features. For example, imagine we’re
    building a virtual sensor—a system that uses the signal from several cheap sensors
    to predict the output of one sensor that is higher quality but prohibitively expensive.
    In this case, our dataset would need to contain readings from both the cheap sensors
    and the expensive one. The readings from the expensive sensor would be used as
    labels.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据集使用它们自己的特征进行标记。例如，想象我们正在构建一个虚拟传感器系统——该系统使用几个廉价传感器的信号来预测一个昂贵但成本高昂的传感器的输出。在这种情况下，我们的数据集需要包含来自廉价传感器和昂贵传感器的读数。昂贵传感器的读数将用作标签。
- en: Features may also be processed before they are used as labels. For example,
    imagine we wish to train an ML model to predict whether it is daytime or nighttime
    based on sensor data. We might use the timestamp of each row in our dataset, along
    with information about the local sunrise and sunset wherever the data was collected,
    to determine whether it was captured during daytime or nighttime.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在用作标签之前，特征可能会经过处理。例如，想象我们希望训练一个机器学习模型，根据传感器数据预测是白天还是黑夜。我们可以使用数据集中每行的时间戳，以及数据收集地点的当地日出和日落信息，来确定数据是在白天还是黑夜捕获的。
- en: Manual labeling
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 手动标记
- en: 'Most datasets are labeled deliberately, by human beings. For some datasets
    this is easy: if samples are collected during specific events, it may be obvious
    what their labels should be. For example, imagine you’re collecting a dataset
    of vibration data from a vehicle, labeling it as either “moving” or “idle.” In
    this case, if you’re sitting in the vehicle at the time, you already know how
    each sample should be labeled.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据集是由人类有意标记的。对于某些数据集来说，这很容易：如果样本是在特定事件期间收集的，它们的标签可能是显而易见的。例如，想象你正在收集车辆的振动数据集，并将其标记为“移动”或“空闲”。在这种情况下，如果你当时坐在车里，你已经知道每个样本应该如何标记。
- en: 'In other cases, labeling may be a tedious manual process where a human being
    looks at each record in a previously unlabeled dataset and determines what the
    correct label should be. This process can be challenging: for example, it may
    require some training or skill to determine the correct label to apply. In some
    cases, even well-trained experts may find it hard to agree on the correct labels—medical
    imaging data often suffers from this problem.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，标记可能是一个繁琐的手动过程，其中人类需要查看以前未标记的数据集中的每条记录，并确定正确的标签应该是什么。这个过程可能很具挑战性：例如，可能需要一些培训或技能来确定应用正确的标签。在某些情况下，即使是经过良好培训的专家也可能很难就正确的标签达成一致——医学影像数据经常面临这个问题。
- en: Even if a task is easy, human beings will naturally make mistakes. Manual labeling
    is one of the most common causes of dataset quality issues. It’s also the most
    expensive to detect and correct, so it’s worth making sure you get it right.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 即使任务很简单，人类也会自然而然地犯错误。手动标记是数据集质量问题最常见的原因之一。它也是最难检测和纠正的，因此值得确保您做对了。
- en: Automated labeling
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 自动标注
- en: Depending on your dataset, it may be possible to apply labels automatically.
    For example, imagine you’re planning to train a tiny, on-device ML model that
    can identify different species of animals from photographs. You may already have
    access to a large, highly accurate ML model that is able to perform this task
    but is much too big to fit on an embedded device. You could potentially use this
    large model to label your dataset automatically, avoiding the need for human effort.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的数据集，可能可以自动应用标签。例如，想象一下您计划训练一个微型的设备上机器学习模型，可以从照片中识别不同物种的动物。您可能已经可以访问一个大型，高度准确的机器学习模型，能够执行此任务，但太大而无法安装在嵌入式设备上。您可以潜在地使用这个大型模型自动标记您的数据集，避免人力劳动的需要。
- en: This approach can save a lot of time, but it’s not always possible. Even if
    it is possible, it’s smart to assume that the automated system will make some
    mistakes and that you’ll need some process for identifying and fixing them.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以节省大量时间，但并非总是可行。即使可行，假设自动化系统会出现一些错误并需要一些流程来识别和修正它们也是明智的。
- en: It’s worth bearing in mind that there’s often a difference between the labels
    of the large existing model and the model you are trying to train. For example,
    imagine you’re building a system to recognize wildlife sounds. Your goal is to
    deploy a tiny model that can identify a sound as being made by either a bird or
    a mammal. If your large model is designed to identify individual species, you’ll
    have to map each of these to either the “bird” or “mammal” label.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 值得记住的是，大型现有模型的标签与您试图训练的模型之间通常存在差异。例如，想象一下您正在构建一个识别野生动物声音的系统。您的目标是部署一个能够识别声音是由鸟类还是哺乳动物发出的微型模型。如果您的大型模型设计用于识别单个物种，那么您将不得不将每个物种映射到“鸟类”或“哺乳动物”标签之一。
- en: Assisted labeling
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助标注
- en: 'It’s possible to design a hybrid approach between manual and automated labeling
    that provides the best of both worlds: direct human insight combined with the
    automation of tedious tasks. For example, imagine you are tasked with drawing
    bounding boxes around specific objects in a dataset of images. In an assisted
    labeling system, a computer vision model may highlight areas of interest in each
    image so you can inspect them and decide which require bounding boxes to be drawn.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 可以设计一种混合方法，介于手动和自动化标注之间，既能提供直接的人类洞察力，又能自动完成繁琐的任务。例如，假设你被要求在图像数据集中的特定对象周围绘制边界框。在辅助标注系统中，计算机视觉模型可能会突出显示每个图像中感兴趣的区域，以便您检查并决定哪些需要绘制边界框。
- en: Not all problems require labels
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并非所有问题都需要标签。
- en: Depending on the problem you’re trying to solve, you may not even need labels—although
    most of the time you will.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您试图解决的问题，您甚至可能不需要标签 — 尽管大多数情况下您会需要。
- en: In [“Classical machine learning”](ch04.html#classical_ml), we encountered the
    ideas of *supervised* and *unsupervised* learning. In supervised learning, a machine
    learning algorithm learns to predict a label given a set of input data. In unsupervised
    learning, the model learns a representation of the data that can be used in some
    other task.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“经典机器学习”](ch04.html#classical_ml)中，我们遇到了*监督*和*无监督*学习的概念。在监督学习中，机器学习算法学习预测给定一组输入数据的标签。在无监督学习中，模型学习数据的表示，可以在某些其他任务中使用。
- en: 'Unsupervised algorithms do not require labels. For example, imagine we’re training
    a clustering algorithm for anomaly detection.^([16](ch07.html#idm45988811875792))
    The algorithm does not need labeled data; it just attempts to learn the innate
    properties of an unlabeled dataset. In this case, it could be argued that the
    labels are *implicit*: since the clustering algorithm must be trained on data
    representing normal (nonanomalous) values, it follows that your training dataset
    must have been carefully curated to ensure that it only contains nonanomalous
    values.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督算法不需要标签。例如，想象一下我们正在为异常检测训练聚类算法。[^16] 该算法不需要标记数据；它只是尝试学习未标记数据的固有属性。在这种情况下，可以说标签是*隐含的*：因为聚类算法必须在表示正常（非异常）值的数据上进行训练，因此您的训练数据集必须经过精心策划，以确保它仅包含非异常值。
- en: If you suspect you may be able to solve your problem with an unsupervised algorithm,
    you should try it out as an experiment early on in your process. You might find
    that you can get away without labeling much data, which would be a big savings
    in cost, time, and risk. However, it’s likely that the majority of problems will
    turn out to require supervised learning.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您怀疑可能能够使用无监督算法解决问题，应该在流程的早期阶段尝试一下。您可能会发现可以不用标记太多数据，这将大大节省成本、时间和风险。然而，大多数问题可能最终需要使用监督学习。
- en: Even if you’re using an unsupervised algorithm, it is typically important to
    have some labeled data to use for testing. For example, if you’re solving an anomaly
    detection problem, you’ll need to obtain some examples of both normal values and
    anomalous values. These examples will need to be labeled so that you can use them
    in evaluating the performance of your model.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用无监督算法，通常也很重要拥有一些标记数据用于测试。例如，如果解决异常检测问题，需要获取一些正常值和异常值的示例。这些示例需要标记，以便用于评估模型的性能。
- en: Semi-supervised and active learning algorithms
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 半监督学习和主动学习算法
- en: Labeling is one of the most expensive and time-consuming aspects of dataset
    collection. This means it’s common to have access to a large pool of unlabeled
    data and a smaller amount that has labels. Many organizations interested in edge
    AI may have stores of IoT data that they have been collecting over long periods
    of time. It is plentiful—but unlabeled.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 标记是数据集收集中最昂贵和耗时的方面之一。这意味着通常可以访问大量未标记的数据和少量已标记的数据。许多对边缘人工智能感兴趣的组织可能拥有长时间收集的物联网数据存储。这些数据虽然丰富，但却是未标记的。
- en: '*Semi-supervised learning* and *active learning* are two techniques designed
    to help make use of this type of data. The concept underlying both is that a model
    partially trained on a small, labeled dataset can be used to help label more data.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '*半监督学习*和*主动学习*是两种旨在帮助利用这类数据的技术。这两者背后的概念是，模型部分训练于一个小的标记数据集上，可以用来帮助标记更多的数据。'
- en: Semi-supervised learning, shown in [Figure 7-6](#semi_supervised_figure), begins
    with a large, unlabeled dataset. First, a small subset of this dataset is labeled,
    and a model is trained on the labeled records. This model is then used to make
    predictions on a batch of unlabeled records. These predictions are used to label
    the data. Some of them will likely be incorrect, but that’s OK.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习，如[图 7-6](#semi_supervised_figure)所示，从一个大的未标记数据集开始。首先，从这个数据集中选择一个小的子集进行标记，并在这些标记记录上训练模型。然后，该模型用于对一批未标记记录进行预测。这些预测结果用于标记数据。其中一些可能是错误的，但没关系。
- en: '![A diagram showing the process of semi-supervised learning](assets/aiae_0706.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![显示半监督学习过程的图表](assets/aiae_0706.png)'
- en: Figure 7-6\. Semi-supervised learning.
  id: totrans-349
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. 半监督学习。
- en: These newly labeled records are then combined with the original labeled data
    and a new model is trained using all of it.^([17](ch07.html#idm45988811862176))
    The new model should be at least a bit better than the old one, even though it
    was trained on data that the old one helped to label. The process is then repeated,
    with progressively more data being labeled until the model is good enough for
    production use.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些新标记的记录与原始标记数据合并，并使用所有这些数据训练新模型^([17](ch07.html#idm45988811862176))。新模型应该至少比旧模型好一点，即使它是在旧模型帮助标记数据的基础上训练的。然后重复此过程，逐渐标记更多数据，直到模型足够用于生产。
- en: The second technique, active learning, is a little different. The process, shown
    in [Figure 7-7](#active_learning_figure), begins in the same way, with an initial
    model being trained on the small amount of labeled data that is available. However,
    the next step is different. Instead of automatically labeling a random sample
    of data, the model is used to help *select* a set of records from the dataset
    that look like they would be the most useful ones to label. A domain expert is
    then asked to label these samples, and a new model is trained that makes use of
    them.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种技术，主动学习，有些不同。该过程，如[图 7-7](#active_learning_figure)所示，以同样的方式开始，即一个初始模型在可用的少量标记数据上进行训练。然而，下一步不同。模型不是自动标记随机样本数据，而是帮助*选择*数据集中看起来最有用的一组记录进行标记。然后请领域专家标记这些样本，并训练一个新模型利用它们。
- en: '![A diagram showing the process of semi-supervised learning](assets/aiae_0707.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![显示半监督学习过程的图表](assets/aiae_0707.png)'
- en: Figure 7-7\. Active learning.
  id: totrans-353
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7\. 主动学习。
- en: The selection process is designed to maximize *information gain* by identifying
    which of the unlabeled samples contain the most information that will help the
    model learn. The two most common selection strategies are known as *uncertainty
    sampling* and *diversity sampling*, and they can be used either individually or
    in combination.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 选择过程旨在通过识别未标记样本中包含的最有助于模型学习的信息来最大化*信息增益*。两种最常见的选择策略称为*不确定性采样*和*多样性采样*，可以单独或组合使用。
- en: Uncertainty sampling is based on *confidence*. If the initial model appears
    confident at classifying a record then it can be assumed that there isn’t much
    more information to be gained from using that record in training. If a model *isn’t*
    confident about a particular record, that gives us a signal that the model has
    not seen many samples similar to it and does not know what to make of it. It’s
    these samples that it’s most impactful for us to label and add to the dataset.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性采样基于*置信度*。如果初始模型在分类记录时显得自信，那么可以假设从该记录中无法获取更多信息用于训练。如果模型对特定记录*不*自信，这表明模型没有见过许多类似样本，并且不知道如何处理它。这些样本对我们来说标记和添加到数据集中是最有影响力的。
- en: Diversity sampling involves using statistical techniques to understand which
    of the samples best represent the underlying distribution of the data. For example,
    we might attempt to find a way to quantify the similarity between any two samples.
    To select new samples to label, we’d look for those that seem the most different
    to those in our existing labeled dataset.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性采样涉及使用统计技术来理解哪些样本最能代表数据的基本分布。例如，我们可能会尝试找到一种方法来量化任意两个样本之间的相似性。为了选择要标记的新样本，我们会寻找那些与我们现有已标记数据集中的样本看起来最不同的样本。
- en: This overall process—selecting a few samples to label, incorporating the new
    samples into the training data alongside the existing labeled samples, and retraining
    the model—happens as many times as necessary to get a model that performs well.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程——选择少量样本进行标记，将新样本与现有标记样本一起添加到训练数据中，并重新训练模型——将根据需要重复多次，以获得性能良好的模型。
- en: While these techniques are relatively new, they work very well. The section
    on [“Labeling tools”](#labelling_tools) gives some examples that can help you
    use them.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些技术相对较新，但它们非常有效。关于[“标注工具”](#labelling_tools)的部分提供了一些示例，可以帮助您使用它们。
- en: That said, active learning tools are a potential source of bias in the labeling
    process. To evaluate them, it’s a good idea to compare their results to the results
    of labeling randomly selected samples (instead of those selected by your active
    learning workflow). This will allow you to better understand the type of model
    that your active learning process is creating.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，主动学习工具是标记过程中偏见的潜在来源。为了评估它们，建议将它们的结果与随机选择的样本的结果进行比较（而不是由您的主动学习工作流程选择的样本）。这将帮助您更好地了解您的主动学习过程正在创建的模型类型。
- en: Bias in labeling
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标签偏见
- en: In [“Label Noise”](#label_noise), we discussed how label noise is a significant
    problem in datasets. One major source of label noise is bias in the labeling process.
    When this occurs, the dataset ends up reflecting the biases of the people and
    tools who are doing the labeling—rather than reflecting the underlying situation
    you are trying to model.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“标签噪音”](#label_noise)中，我们讨论了标签噪音在数据集中的重要问题。标签噪音的一个主要来源是标记过程中的偏见。当这种情况发生时，数据集反映的是进行标记的人和工具的偏见，而不是反映您试图建模的潜在情况。
- en: This is just one way that bias during labeling can impact the quality of your
    dataset. Unfortunately, these types of problems are common to the point of being
    almost inevitable. In addition, since your dataset is your most powerful tool
    for evaluating your system, any resulting bias in your system may be hard to detect.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是标记过程中偏见可能影响数据集质量的一种方式。不幸的是，这些类型的问题几乎不可避免。此外，由于数据集是评估系统的最强大工具，因此系统中可能存在的任何偏见可能难以检测。
- en: 'The best way to avoid labeling quality issues is to have a rigorous procedure
    for evaluating the correctness of your labels. This might include:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 避免标签质量问题的最佳方法是制定严格的评估标签正确性的程序。这可能包括：
- en: Using legitimate domain experts who have deep experience with the subject matter
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用在主题方面拥有深厚经验的合法领域专家
- en: Following a documented labeling protocol established by domain experts
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵循由领域专家建立的详细标记协议
- en: Relying on multiple labelers who can check each other’s work
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖多个标注者可以相互检查工作
- en: Evaluating samples of your labeled data for quality
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估你标记数据样本的质量
- en: This will increase the cost and complexity of your labeling process. If the
    cost of producing a high-quality dataset is beyond what you can afford, your project
    may not be feasible at the current budget. It’s better to abort a project than
    release a harmful system to production.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这将增加标注过程的成本和复杂性。如果生成高质量数据集的成本超出预算承受范围，你的项目可能无法实现。与其发布一个有害系统到生产环境，还不如终止项目。
- en: Labeling bias is not only a feature of data that has been labeled by hand. If
    you use an automatic system to help label your data, any biases present in this
    system will also be reflected in your dataset. For example, imagine you are using
    the output of a large pretrained model to help label records in a new dataset
    that will be used to train an edge AI model. If the performance of the large model
    is not uniform across all of the subgroups in your dataset, your labels will reflect
    these same biases.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 标注偏见不仅仅存在于手动标记的数据中。如果你使用自动系统帮助标记数据，系统中存在的任何偏见也会反映在你的数据集中。例如，想象一下，你正在使用一个大型预训练模型的输出来帮助标记新数据集中的记录，以训练边缘AI模型。如果大型模型在数据集的所有子组中表现不均匀，你的标签也会反映出这些偏见。
- en: Labeling tools
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记工具
- en: 'There are several different categories of tools that can help with labeling
    data. The best choice will vary depending on the project:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同类别的工具可以帮助标记数据。最佳选择将根据项目而异：
- en: Annotation tools
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注释工具
- en: Crowdsourced labeling
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 众包标注
- en: Assisted and automated labeling
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辅助和自动标注
- en: Semi-supervised and active learning
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习和主动学习
- en: Let’s explore each one in turn.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们依次探讨每一个。
- en: Annotation tools
  id: totrans-377
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释工具
- en: If your data needs to be labeled or evaluated by a human being, they’ll need
    to use some kind of tool. For example, imagine you’re building a dataset of photographs
    labeled with any animals they contain. You’ll likely need some form of user interface
    that can display each photograph and allow a domain expert to specify the animals
    that they see.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据需要人工标注或评估，他们将需要使用某种工具。例如，想象你正在构建一个包含任何动物的照片数据集。你可能需要某种用户界面来显示每张照片，并允许领域专家指定他们看到的动物。
- en: 'The complexity of these tools will vary depending on the data. A labeling interface
    for a dataset used for image classification will be relatively simple; it just
    needs to show photographs and allow the user to specify labels. An interface for
    an object detection dataset will need to be more complex: the user will have to
    draw bounding boxes around the objects they care about.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具的复杂性将根据数据的不同而变化。用于图像分类数据集的标注界面将相对简单；它只需显示照片并允许用户指定标签。用于对象检测数据集的界面将需要更复杂：用户将需要绘制围绕他们关心的对象的边界框。
- en: More exotic types of data, such as time series sensor data, may require more
    sophisticated tools that can help visualize the data in a way that a domain expert
    can understand.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 更奇特的数据类型，例如时间序列传感器数据，可能需要更复杂的工具，可以帮助以领域专家能理解的方式可视化数据。
- en: Annotation tools are pretty much a requirement for interacting with a dataset
    of sensor data in any meaningful way. They’re needed not only for labeling but
    also for visualizing and editing existing labels, since the evaluation of labels
    is an important part of the process.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 注释工具在与传感器数据集交互中几乎是不可或缺的要求。它们不仅用于标注，还用于可视化和编辑现有的标签，因为评估标签是过程的重要组成部分。
- en: 'Annotation tools are available as both open source and commercial software.
    Some things to look out for are:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 注释工具有开源和商业软件两种。一些需要注意的事项包括：
- en: Support for the data type you are working with
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持你正在处理的数据类型
- en: Support for the problem you are trying to solve (e.g., classification versus
    regression)
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持你试图解决的问题（例如，分类与回归）
- en: Collaborative features, so multiple people can work on labeling
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协作功能，以便多人可以共同进行标注
- en: Automation, and other features explained later in this section
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化和本节后面解释的其他功能
- en: Crowdsourced labeling
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 众包标注
- en: It’s common for a team to have more data to label than they can handle internally.
    In this case, it may be useful to use crowdsourced labeling tools. These tools
    allow you to define a labeling task and then recruit members of the public to
    help complete it. The people helping to label your data may be compensated financially,
    receiving a small amount of money for each sample they label, or they might be
    volunteers.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 一个团队通常会有更多需要标注的数据，超出他们内部能够处理的范围。在这种情况下，使用众包标注工具可能会很有用。这些工具允许您定义一个标注任务，然后招募公众成员来帮助完成。帮助标注数据的人可以得到一定的经济补偿，每标注一个样本可能会得到一小笔钱，或者他们可能是志愿者。
- en: The big advantage of crowdsourced labeling is that it can help you quickly label
    large datasets that would otherwise take prohibitively long. However, since the
    labeling process depends on minimally trained members of the public, you can’t
    rely on any domain expertise.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 众包标注的一大优势在于它可以帮助您快速标注大量数据集，否则可能需要耗费极长的时间。然而，由于标注过程依赖于接受过最低培训的公众成员，您不能依赖任何领域专业知识。
- en: 'This may put some tasks out of reach: for example, anything that requires sophisticated
    technical knowledge. Even for simpler tasks, you are likely to end up with far
    more quality issues than you would if your data was labeled by a domain expert.
    In addition, there’ll be some significant overhead involved with defining the
    task clearly enough that members of the public can understand it. To get a good
    result, you’ll have to educate your labelers on how to accurately complete the
    task.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会使一些任务变得不可达：例如，任何需要复杂技术知识的任务。即使是简单的任务，您最终可能会遇到比由领域专家标注数据时更多的质量问题。此外，清晰定义任务的工作量很大，以至于公众成员能够理解。为了取得好的结果，您需要教育标注人员如何准确完成任务。
- en: 'Beyond quality issues, there are also confidentiality concerns: crowdsourcing
    may not be an option if your dataset contains sensitive, private, or proprietary
    information. In addition, crowdsourced datasets are potentially subject to manipulation
    by malicious actors.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 除了质量问题，还存在机密性问题：如果您的数据集包含敏感、私密或专有信息，众包可能不是一个选项。此外，众包数据集有可能会受到恶意行为者的操控。
- en: Assisted and automated labeling
  id: totrans-392
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 辅助和自动化标注
- en: Assisted and automated labeling tools use some kind of automation to help humans
    (whether domain experts or crowdsourced labelers) rapidly label large amounts
    of data. On the simple end, this might involve using basic signal processing algorithms
    to help highlight areas of interest or suggest labels. More sophisticated tools
    may use machine learning models to help. The following examples of assisted labeling
    tools are taken from Edge Impulse.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助和自动化标注工具使用某种形式的自动化来帮助人类（无论是领域专家还是众包标注人员）快速标注大量数据。在简单的情况下，这可能涉及使用基本信号处理算法来帮助突出感兴趣的区域或建议标签。更复杂的工具可能使用机器学习模型来辅助。以下示例的辅助标注工具来自Edge
    Impulse。
- en: First, [this object detection labeling tool](https://oreil.ly/IkzTs) makes it
    easier to draw bounding boxes around objects in a sequence of images. It uses
    an object tracking algorithm to identify previously labeled items in subsequent
    frames, as shown in [Figure 7-8](#object_tracking_figure).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，[这个目标检测标注工具](https://oreil.ly/IkzTs)使得在一系列图像中绘制边界框更容易。它使用物体跟踪算法来识别后续帧中已标记的物体，如[图 7-8](#object_tracking_figure)所示。
- en: '![A screenshot of the object detection labeling tool in Edge Impulse, showing
    labeling assisted by an object tracking algorithm.](assets/aiae_0708.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![Edge Impulse中目标检测标注工具的截图，显示由物体跟踪算法辅助的标注。](assets/aiae_0708.png)'
- en: Figure 7-8\. Object tracking for labeling in Edge Impulse; cars that are labeled
    are tracked between successive frames.
  id: totrans-396
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-8\. 在Edge Impulse中用于标注的物体跟踪；标记的汽车在连续的帧之间被追踪。
- en: In a more complex example of labeling tools, [the data explorer in Edge Impulse
    Studio](https://oreil.ly/Qxs5j) uses a clustering algorithm to help visualize
    data, with similar samples appearing closer together, allowing users to quickly
    label samples based on those they are adjacent to. This is shown in [Figure 7-9](#data_explorer_figure).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在标注工具的更复杂示例中，[Edge Impulse Studio中的数据探索器](https://oreil.ly/Qxs5j)使用聚类算法来帮助可视化数据，相似的样本会靠近彼此，让用户能够基于相邻的样本快速标注样本。这显示在[图 7-9](#data_explorer_figure)中。
- en: '![A screenshot of the data explorer tool in Edge Impulse.](assets/aiae_0709.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![Edge Impulse中数据探索器工具的截图。](assets/aiae_0709.png)'
- en: Figure 7-9\. The data explorer in Edge Impulse being used to label a keyword-spotting
    dataset.
  id: totrans-399
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9\. Edge Impulse中数据浏览器用于标记关键词识别数据集的截图。
- en: Finally, entire pretrained models can be used to help automatically label data.
    For example, [Figure 7-10](#assisted_labelling_figure) shows the use of [an object
    detection model pretrained on a public dataset](https://oreil.ly/IZMoT) in order
    to label instances of 80 known classes of objects.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以使用完整预训练模型来帮助自动标记数据。例如，[图7-10](#assisted_labelling_figure)展示了使用[在公共数据集上预训练的对象检测模型](https://oreil.ly/IZMoT)来标记80种已知对象类别的实例。
- en: Assisted labeling can save time and effort by shifting work from the human labeler
    to an automated system. However, since the automated system is unlikely to be
    perfect, it shouldn’t be used alone—there needs to be a human “in the loop” to
    ensure good quality.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助标记可以通过将工作从人工标记者转移至自动化系统来节省时间和精力。然而，由于自动化系统不太可能完美无缺，不能单独使用——必须有人类“在其中”，以确保质量良好。
- en: '![A screenshot of the object detection labeling tool in Edge Impulse, showing
    labeling assisted by a pre-trained model.](assets/aiae_0710.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![Edge Impulse中对象检测标记工具的截图，显示由预训练模型辅助进行的标记。](assets/aiae_0710.png)'
- en: Figure 7-10\. Data can be labeled automatically using pretrained models, as
    shown in this screenshot from Edge Impulse.
  id: totrans-403
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10\. 可以使用预训练模型自动标记数据，正如Edge Impulse中的这一屏幕截图所示。
- en: Semi-supervised and active learning
  id: totrans-404
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 半监督和主动学习
- en: As discussed in [“Semi-supervised and active learning algorithms”](#semi_supervised_and_active_learning),
    various techniques exist that can help reduce the burden of labeling a dataset
    by using a partially trained model to assist. These methods are similar to assisted
    labeling, but they’re especially exciting because they can intelligently reduce
    the amount of labeling that needs to be done. For example, an active learning
    tool may suggest a small subset of the data that needs to be labeled by hand in
    order to accurately provide automatic labels for the rest of the data.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 正如讨论中提到的[“半监督和主动学习算法”](#semi_supervised_and_active_learning)，存在各种技术可以通过使用部分训练模型来帮助减少标记数据集的负担。这些方法类似于辅助标记，但尤其令人兴奋的是它们可以智能地减少需要进行的标记量。例如，主动学习工具可能建议手动标记数据的一个小子集，以便为其余数据准确提供自动标签。
- en: Both techniques involve an iterative process of labeling a subset of data, training
    a model, and then determining labels for the next set of data. Over multiple iterations
    you will end up with an effective dataset.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术都涉及标记数据子集、训练模型，然后确定下一批数据的标签的迭代过程。经过多次迭代，您将获得一个有效的数据集。
- en: An interesting variant of active learning can be found in [Edge Impulse Studio’s
    data explorer](https://oreil.ly/sDAif). The data explorer can use a partially
    trained model to help visualize an unlabeled dataset as clusters.^([18](ch07.html#idm45988811773088))
    These clusters can be used to guide the labeling process, with the goal of ensuring
    clusters are distinct and that each contains at least some labeled samples. [Figure 7-11](#custom_embedding_figure)
    shows a dataset being clustered based on a partially trained model.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习的一个有趣变体可以在[Edge Impulse Studio的数据浏览器](https://oreil.ly/sDAif)中找到。数据浏览器可以使用部分训练模型来帮助可视化未标记的数据集作为聚类。^[18](ch07.html#idm45988811773088)
    这些聚类可用于引导标记过程，确保聚类是明显的，并且每个聚类至少包含一些标记样本。[图7-11](#custom_embedding_figure)展示了基于部分训练模型对数据集进行聚类的情况。
- en: '![A screenshot of the data explorer in Edge Impulse, showing samples clustered
    based on a partially trained model.](assets/aiae_0711.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![Edge Impulse中数据浏览器的截图，显示基于部分训练模型进行聚类的样本。](assets/aiae_0711.png)'
- en: Figure 7-11\. Data is clustered according to the output of a partially trained
    model; the visualization can be used to improve data quality by guiding labeling
    or identifying ambiguous samples.
  id: totrans-409
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-11\. 根据部分训练模型的输出对数据进行聚类；该可视化可以通过引导标记或识别模糊样本来改善数据质量。
- en: As we’ve seen, data labeling has a major impact on the quality of AI systems.
    While there are sophisticated tools available that can reduce the required work,
    labeling will typically represent a large portion of the hours spent on an AI
    project.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，数据标记对AI系统的质量有重大影响。虽然有复杂的工具可用来减少所需的工作量，但标记通常会占据AI项目花费的大部分时间。
- en: Formatting
  id: totrans-411
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 格式化
- en: There’s an almost infinite variety of formats that can be used to store data
    on disk. These range from simple binary representations to special formats designed
    specifically for training machine learning models.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎无限种格式可以用来存储磁盘上的数据。这些范围从简单的二进制表示到专门设计用于训练机器学习模型的特殊格式。
- en: Part of the data preparation process is bringing data together from disparate
    sources and making sure it is formatted in a convenient way. For example, you
    may need to pull sensor data from an IoT platform and write it into a binary format
    in preparation for training a model.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备过程的一部分是将来自不同来源的数据汇集在一起，并确保以方便的方式格式化。例如，您可能需要从物联网平台提取传感器数据，并将其写入二进制格式，以便为模型训练做准备。
- en: 'Each data format has different benefits and drawbacks. Here are some of the
    common varieties:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 每种数据格式都有不同的优缺点。以下是一些常见的变体：
- en: Text formats
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 文本格式
- en: Formats like CSV (comma-separated values) and JSON (JavaScript Object Notation)
    store data as text. For example, the CSV format stores data as text separated
    by delimiters, commonly either commas (hence the name) or tabs. It is very simple
    to work with since you can read and edit the values with any text editor. However,
    text-based formats are very inefficient—the files take up more space than binary
    formats, and they require more computational overhead to access and process.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 类似CSV（逗号分隔值）和JSON（JavaScript对象表示法）的格式将数据存储为文本。例如，CSV格式将数据存储为以分隔符分隔的文本，通常是逗号（因此得名）或制表符。它非常容易处理，因为您可以使用任何文本编辑器读取和编辑值。然而，文本格式非常低效——文件占用的空间比二进制格式大，并且需要更多的计算开销来访问和处理。
- en: CSV and JSON files are fine for small datasets that can be read entirely into
    memory, but with larger datasets that must be read from disk, it is better to
    translate the data into a binary format first.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: CSV和JSON文件适用于可以完全读入内存的小数据集，但对于必须从磁盘读取的大数据集，最好先将数据转换为二进制格式。
- en: Image and audio files
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和音频文件
- en: Images and audio are common data types and have their own typical formats (think
    JPEG images and WAV audio files). It’s pretty common to store image and audio
    datasets as separate files on disk. While this isn’t the fastest possible solution,
    it’s good enough in many cases. Datasets stored in this way have the benefit of
    being easy to read and modify without any special tools. They are typically used
    along with manifest files (see the tip [“Manifest Files”](#manifest_files)).
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和音频是常见的数据类型，并有它们自己的典型格式（如JPEG图像和WAV音频文件）。将图像和音频数据集存储为独立的文件在磁盘上是相当常见的。虽然这不是可能的最快解决方案，在许多情况下这已经足够了。以这种方式存储的数据集有利于易于阅读和修改，而无需任何特殊工具。它们通常与清单文件一起使用（参见提示[“清单文件”](#manifest_files)）。
- en: Some specialized types of data, such as medical imagery, have their own special
    formats that encode metadata, such as position and orientation.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 一些特殊类型的数据，如医疗成像，有其自己的特殊格式，可以编码元数据，如位置和方向。
- en: Direct access binary formats
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 直接访问二进制格式
- en: A binary data format is one that stores data in its native form (sequences of
    binary bits) as opposed to encoded in a secondary format (as in text-based formats).
    For example, in a binary format the number `1337` would be stored directly in
    memory as the binary values `10100111001`. In a text-based format the same number
    might be represented in a much larger value due to the overhead of text encoding.
    For example, in the text encoding known as UTF-8 the number `1337` would be represented
    in the bits `00110001001100110011001100110111`.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制数据格式指的是以其本机形式（二进制位序列）存储数据，而不是编码为二级格式（如文本格式）。例如，在二进制格式中，数字`1337`会直接存储在内存中作为二进制值`10100111001`。在文本格式中，由于文本编码的开销，同样的数字可能会以更大的值表示。例如，在名为UTF-8的文本编码中，数字`1337`会以比特位`00110001001100110011001100110111`表示。
- en: In a direct access binary format, many data records are stored in a single binary
    file. The file also contains metadata that allows the program reading it to understand
    the meaning of each field within a record. The format is designed so that any
    record in the dataset can be accessed in constant time.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接访问二进制格式中，许多数据记录存储在单个二进制文件中。文件还包含元数据，使得读取它的程序可以理解记录中每个字段的含义。该格式设计成数据集中的任何记录都可以在常数时间内访问。
- en: Some common direct access binary formats include NPY (used by the Python mathematical
    computing library NumPy) and Apache Parquet. Different formats have different
    performance trade-offs, so it’s useful to select the appropriate one for your
    specific situation.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的直接访问二进制格式包括NPY（由Python数学计算库NumPy使用）和Apache Parquet。不同的格式有不同的性能权衡，因此选择适合特定情况的格式非常有用。
- en: Sequential binary formats
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序二进制格式
- en: Sequential binary formats, such as TFRecord, are designed to maximize efficiency
    for certain tasks, such as training machine learning models. They provide fast
    access in a specific, preset order.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序二进制格式（例如TFRecord）旨在最大化某些任务（例如训练机器学习模型）的效率。它们以特定的、预设的顺序提供快速访问。
- en: Sequential formats can be very compact and are fast to read. However, they are
    not as easy to explore as other data formats. Typically, transforming a dataset
    into a sequential format would be done as a last step before training a machine
    learning model. They are only really used for large datasets, where efficiency
    savings result in significant reductions in cost.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序格式可以非常紧凑且读取速度快。然而，它们不像其他数据格式那样易于探索。通常，在训练机器学习模型之前，将数据集转换为顺序格式将作为最后一步进行。它们只在大型数据集中真正使用，其中效率节省显著降低了成本。
- en: Manifest Files
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清单文件
- en: A manifest file is a special file that acts as an index to the rest of your
    dataset. For example, a manifest file for an image dataset may list the names
    of all of the image files that are intended to be used during training. A common
    format for manifest files is CSV.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 清单文件是一种特殊文件，用作数据集其余部分的索引。例如，图像数据集的清单文件可以列出所有预期在训练期间使用的图像文件的名称。清单文件的常见格式是CSV。
- en: Since a text-based manifest file is simple and easy to work with, it’s a convenient
    way to keep track of your data. Creating a sample of your dataset is as simple
    as selecting some of the rows of your manifest file at random.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于文本的清单文件简单且易于使用，这是跟踪数据的便捷方式。创建数据集样本就像随机选择清单文件中的一些行一样简单。
- en: Your dataset will typically occupy several different formats along its journey.
    For example, you may start with data from several different sources, perhaps in
    a mixture of text-based and binary formats. You may then choose to aggregate the
    data together and store it in a direct access binary format before cleaning and
    processing it. Finally, in some cases you might choose to translate that same
    dataset into a sequential binary format for training.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据集通常会在其旅程中占据几种不同的格式。例如，您可能从几个不同的来源获取数据，可能是混合的基于文本和二进制格式。然后，您可以选择将数据聚合在一起并将其存储在直接访问的二进制格式中，然后进行清理和处理。最后，在某些情况下，您可能会选择将相同的数据集转换为顺序二进制格式以进行训练。
- en: Data Cleaning
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清洗
- en: As you start to pull your dataset together into a common format, you’ll need
    to make sure that all of the values it contains meet a consistent standard for
    quality. In [“Common Data Errors”](#common_data_errors), we encountered the main
    types of problems that you will see in datasets.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开始将数据集整合到一个通用格式中时，您需要确保其包含的所有值都符合质量一致的标准。在[“常见数据错误”](#common_data_errors)中，我们遇到了您将在数据集中看到的主要问题类型。
- en: 'Errors can creep in during any step of the process of collecting and curating
    a dataset. Here are some examples of errors occurring at different stages:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集和整理数据集的任何步骤中，都可能出现错误。以下是不同阶段出现的一些错误示例：
- en: Outliers in raw sensor data caused by faulty hardware
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于故障硬件引起的原始传感器数据中的异常值
- en: Inconsistencies in data formats when aggregating data from different devices
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从不同设备聚合数据时的数据格式不一致
- en: Missing values due to issues joining data from multiple sources
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于多个来源的数据连接问题导致的缺失值
- en: Incorrect values due to bugs in feature engineering
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于特征工程中的错误而导致的不正确值
- en: 'Cleaning a dataset is a process that involves several steps:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 清理数据集是一个涉及多个步骤的过程：
- en: Auditing the data using sampling to identify types of error (you can use the
    same approach to sampling discussed in [“Reviewing Data by Sampling”](#reviewing_data_by_sampling))
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用抽样审计数据以识别错误类型（您可以使用[“通过抽样审查数据”](#reviewing_data_by_sampling)中讨论的相同方法来抽样）
- en: Writing code to fix or obviate the types of errors you have noticed
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码以修复或避免您注意到的错误类型
- en: Evaluating the results to prove the issues have been fixed
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估结果以证明问题已经被解决
- en: Automating step 2 so that you can fix the entire dataset and apply the same
    fixes to any new samples that are added in the future
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动化步骤2，以便您可以修复整个数据集，并将相同的修复方法应用于将来添加的任何新样本。
- en: Unless your dataset is quite small (for example, it is less than a gigabyte)
    it will typically make sense to operate on samples of data rather than the entire
    dataset. Since large datasets take a lot of time to process, working with samples
    will reduce the feedback loop between identifying issues, fixing them, and evaluating
    the fixes.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 除非您的数据集非常小（例如，小于1GB），否则通常应该对数据样本进行操作而不是整个数据集。由于处理大型数据集需要大量时间，使用样本将减少在识别问题、修复问题和评估修复方案之间的反馈循环时间。
- en: Once you have a fix that you are happy with on a sample of data you can confidently
    apply the fix to the entire dataset. That said, it’s still a good idea to evaluate
    the entire dataset as a final step to make sure there are not issues that were
    missed during sampling.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您在数据样本上找到满意的修复方法，您可以自信地将修复方法应用于整个数据集。尽管如此，最后一步还是评估整个数据集以确保没有漏掉的问题是一个好主意。
- en: Auditing your dataset
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 审计您的数据集
- en: The problems listed in [“Common Data Errors”](#common_data_errors) are typical
    of the types of problems you will run into. But how do you figure out which errors
    are present in your dataset?
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“常见数据错误”](#common_data_errors)中列出的问题通常是您会遇到的问题类型。但是，如何确定数据集中存在哪些错误呢？
- en: The most powerful tools for identifying data cleanliness issues are those that
    allow you to view your dataset (or a representative sample of it) as a summary.
    This could mean creating a table that shows descriptive statistics and types that
    are present for a particular field. It may also mean plotting the distribution
    of values in a chart, allowing a domain expert to assess whether the distribution
    is in line with expectations.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 识别数据清洁问题的最强大工具是允许您将数据集（或其代表样本）视为摘要的工具。这可能意味着创建一个表格，显示特定字段的描述统计和存在的类型。这也可能意味着绘制图表中数值的分布，以便领域专家评估分布是否符合预期。
- en: 'The Python library [pandas](https://pandas.pydata.org) is a fantastic tool
    for exploring and summarizing datasets. Once loaded into a pandas data structure,
    a [`DataFrame`](https://oreil.ly/69vWh), the values of a dataset can be summarized.
    For example, the following command prints a statistical summary for the values
    in a time series:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: Python库[pandas](https://pandas.pydata.org)是探索和总结数据集的绝佳工具。一旦加载到pandas数据结构中，即[`DataFrame`](https://oreil.ly/69vWh)，数据集的值可以进行总结。例如，以下命令打印了时间序列值的统计摘要：
- en: '[PRE0]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By looking at the statistics, we can see that the values for this time series
    are centered around 0.5, with a standard deviation of 0.13\. We can use domain
    expertise to understand whether these values seem reasonable.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看统计数据，我们可以看到此时间序列的值集中在0.5左右，标准差为0.13。我们可以利用领域专业知识来判断这些值是否合理。
- en: 'Even better, the Python library [Matplotlib](https://matplotlib.org) allows
    us to visualize our data. For example, we can easily print a [histogram](https://oreil.ly/nfCXD)
    for our data frame:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，Python库[Matplotlib](https://matplotlib.org)允许我们可视化我们的数据。例如，我们可以轻松打印一个[直方图](https://oreil.ly/nfCXD)来显示我们的数据框架：
- en: '[PRE1]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The resulting plot is shown in [Figure 7-12](#histogram_example). The sensor
    readings clearly form a normal distribution.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图7-12](#histogram_example)中。传感器读数明显形成了正态分布。
- en: '![A histogram showing a normal distribution with a mean of 0.5 but with outliers
    around 1.5](assets/aiae_0712.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![一个直方图显示均值为0.5的正态分布，但在1.5附近有异常值](assets/aiae_0712.png)'
- en: Figure 7-12\. A histogram of a value in an example dataset.
  id: totrans-456
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-12。一个示例数据集中数值的直方图。
- en: From the histogram, we can see that the data is mostly centered around 0.5—but
    a few points have a value around 1.5\. A domain expert can interpret this to understand
    whether the distribution seems appropriate. For example, perhaps a sensor issue
    has resulted in some outliers that do not reflect accurate readings. Once we’ve
    identified an issue we can dig deeper to determine the appropriate fix.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 从直方图中，我们可以看到数据大部分集中在0.5左右，但是有一些点的数值约为1.5。领域专家可以解释这一点，以了解分布是否合适。例如，可能是传感器问题导致一些异常值，这些值并不反映准确的读数。一旦我们确定了问题，我们可以深入挖掘以确定适当的修复方法。
- en: There are limitless ways to summarize data using common data science tools such
    as those in the Python and R ecosystems. An engineer or data scientist working
    on an edge AI project must be able to collaborate with domain experts to help
    explore the data and identify errors.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 使用常见的数据科学工具（例如Python和R生态系统中的工具）可以以无限的方式总结数据。在进行边缘AI项目的工程师或数据科学家必须能够与领域专家合作，帮助探索数据并识别错误。
- en: Fixing issues
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修正问题
- en: Once you’ve discovered an error in your dataset, you will need to take some
    action. The type of action you will be able to take depends on the kind of error
    you have found and the overall context within which you are collecting data.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在数据集中发现错误，您将需要采取一些行动。您能够采取的具体行动取决于您发现的错误类型以及您正在收集数据的整体上下文。
- en: 'These are the main methods at your disposal when addressing errors:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是处理错误时您可以使用的主要方法：
- en: Amending values
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修正值
- en: Substituting values
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 替换值
- en: Excluding records
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除记录
- en: In addition, once you have addressed any issues in the dataset you may need
    to address whatever upstream problem was the cause.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，一旦解决了数据集中的任何问题，您可能需要解决导致上游问题的问题。
- en: Amending values
  id: totrans-466
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 修正值
- en: 'In some cases, it may be possible to fix errors entirely. Here are some examples:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，可能可以完全修复错误。以下是一些例子：
- en: Inconsistencies in data formats might be addressed by converting to the correct
    format.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据格式不一致可能通过转换为正确的格式来解决。
- en: Missing values may be found and filled in if the data is available from another
    source.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据可从其他来源获取，则可以发现并填充缺失值。
- en: Faulty values due to bugs in feature engineering code can be fixed.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于功能工程代码中的错误导致的故障值可以被修复。
- en: Typically, you can only fix errors entirely if the original raw data is available
    somewhere. In some cases, you may still not be able to find the correct value.
    For example, if some of your data was mistakenly captured at too low a frequency
    then it will not be possible to recover the original signal—just an approximation.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，只有在原始原始数据仍然可用的情况下，才能完全修复错误。在某些情况下，您仍然可能找不到正确的值。例如，如果一些数据错误地以过低的频率捕获，则将无法恢复原始信号，只能近似。
- en: Substituting values
  id: totrans-472
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 替换值
- en: 'If you can’t fix an error, you may still be able to substitute a reasonable
    value. Here are some examples of this happening:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法修复错误，仍可能替换为合理值。以下是此类事件发生的一些例子：
- en: Missing values might be replaced with the mean of that field across the entire
    dataset.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值可以用整个数据集中该字段的均值替换。
- en: Outliers might be clipped or moderated to a reasonable value.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常值可能会被裁剪或调整到合理的值。
- en: Low-frequency or low-resolution data can be interpolated to approximate higher
    detail.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以对低频或低分辨率数据进行插值以近似更高的细节。
- en: Substitution allows you to make use of a record even if some of the information
    it contains is missing. However, in exchange, it will introduce some noise to
    your dataset. Some machine learning algorithms are good at tolerating noise, but
    whether the information preserved is worth the added noise is a judgment call
    that will have to be made based on the application.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 替代使您能够利用记录，即使其中一些信息是缺失的。然而，作为交换，这将为您的数据集引入一些噪音。一些机器学习算法擅长容忍噪音，但保留的信息是否值得增加的噪音，这是一个基于应用的判断调用。
- en: Excluding records
  id: totrans-478
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 排除记录
- en: 'In some cases, errors may be unsalvageable, requiring you to discard affected
    records from your dataset. Here are some examples:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，错误可能是无法挽救的，这将需要您从数据集中丢弃受影响的记录。以下是一些例子：
- en: A missing value may render a record unusable.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值可能导致记录无法使用。
- en: Data from a faulty sensor may be beyond repair.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自故障传感器的数据可能无法修复。
- en: Some records may be from sources that do not meet data security standards.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些记录可能来自未达到数据安全标准的来源。
- en: Rather than just deleting records that have problems, it’s a good idea to mark
    them as problematic but store them somewhere. This will help you keep track of
    the kinds of issues that are occurring, and which types of records are being affected.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅是删除有问题的记录，标记它们为有问题的是个好主意，但是把它们存档起来。这样有助于跟踪出现的问题类型和受影响的记录类型。
- en: The correct way to address an error depends entirely on the context of your
    dataset and application. To obtain good results it’s important that both domain
    expertise and data science experience are applied.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 解决错误的正确方式完全取决于数据集和应用的上下文。为了获得良好的结果，应同时应用领域专业知识和数据科学经验。
- en: Evaluation and automation
  id: totrans-485
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估和自动化
- en: Once you’ve fixed the errors in a sample or subset of your data, you should
    perform another audit. This will help catch any problems that your efforts may
    have inadvertently introduced—along with any issues that may have been masked
    by the issues you have fixed. For example, you may remove the most egregious outliers
    from your dataset only to discover that there were other, less extreme outliers
    that are still a concern.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 在修复数据样本或子集中的错误后，应进行另一次审计。这将有助于捕获您的努力可能无意中引入的任何问题，以及可能被您修复的问题掩盖的问题。例如，您可能仅移除数据集中最严重的异常值，却发现仍存在其他较小但仍值得关注的异常值。
- en: Once you’ve validated your fixes for a subset you can apply the fixes to your
    entire dataset. For large datasets, you’ll need to automate this as part of a
    data pipeline (see [“Data Pipelines”](#data_pipelines)). Perform the same sort
    of sampling-driven audit with more of your dataset until you are confident that
    the issues have been adequately resolved.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证了对子集的修复后，可以将修复应用到整个数据集中。对于大型数据集，您需要将此过程自动化作为数据流程的一部分（参见[“数据流水线”](#data_pipelines)）。对更多数据集进行基于抽样的审计，直到您确信问题已得到充分解决。
- en: Tip
  id: totrans-488
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Keep a copy of your original, unimproved dataset so that you can roll back to
    it if you need. This will help you experiment without feeling afraid of making
    mistakes and losing data.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 保留原始未改进的数据集副本，以便在需要时可以回滚。这将帮助您在实验时不必担心犯错和数据丢失。
- en: It’s important that you keep track of the types of records that have been affected
    by errors. It may be the case that errors are disproportionately impacting certain
    subgroups of your data. For example, imagine you are training a classification
    model on sensor data. You may have a serious problem with some of your sensor
    readings that requires the associated records to be discarded. If these problems
    affect one of your classes more than the others, it may impact the performance
    of your classifier.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要追踪受错误影响的记录类型。错误可能会不成比例地影响数据的某些子组。例如，假设您正在对传感器数据进行分类模型训练。您可能会发现某些传感器读数存在严重问题，需要丢弃相关记录。如果这些问题影响某个类别比其他类别更多，可能会影响分类器的性能。
- en: With this in mind, you should make sure your dataset still has a good standard
    of quality (as we explored in [“Ensuring Data Quality”](#data_quality)) *after*
    any fixes have been applied.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，您应确保在应用任何修复后，数据集仍然具有良好的质量标准（正如我们在[“确保数据质量”](#data_quality)中探讨的那样）。
- en: Keep track of how common various types of errors are in your dataset. If the
    proportion of bad records is high, it may be worth trying to fix any upstream
    cause before spending too much time on trying to repair the damage.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 要记录数据集中各种类型错误的普遍程度。如果坏记录的比例很高，可能值得在花费大量时间修复损坏之前，尝试修复任何上游原因。
- en: As your dataset grows, it will change—and it’s possible for new issues to be
    introduced. To help identify any problems, it’s a great idea to create automated
    assertions based on your initial evaluation. For example, if you’ve worked hard
    to improve your dataset by removing extreme outliers, you should create an automated
    test that proves that the dataset has the expected amount of variance. You can
    run this test every time you add new records, ensuring you catch any new problems.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据集的增长，可能会出现新问题。为了帮助识别任何问题，根据您的初始评估创建自动断言是一个好主意。例如，如果您通过删除极端异常值来改进数据集，应该创建一个自动化测试来证明数据集具有预期的变化量。每次添加新记录时运行此测试，以确保捕捉任何新问题。
- en: Fixing balance issues
  id: totrans-494
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决平衡问题
- en: 'So far we’ve discussed how to fix errors in the values of a dataset. However,
    one of the most common problems with datasets is that they are unbalanced: they
    contain uneven numbers of records for their various subgroups. In [“Ensuring Representative
    Datasets”](#ensuring_representative_datasets), we used the example of a dataset
    of images showing plant diseases. In this context, if a dataset had a higher number
    of images showing one plant species than another it may be considered unbalanced.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了如何修复数据集中值的错误。然而，数据集最常见的问题之一是不平衡：各子组的记录数量不均衡。在[“确保代表性数据集”](#ensuring_representative_datasets)中，我们以显示植物疾病图像的数据集为例。在这种情况下，如果数据集中显示某种植物物种图像的数量比其他植物物种多，则可能被认为是不平衡的。
- en: The best way to fix balance issues in a dataset is to collect more data for
    the underrepresented subgroups. For example, we could go back into the field and
    collect more images for the underrepresented plant species. However, this isn’t
    always feasible.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中解决平衡问题的最佳方法是为低估的子组收集更多数据。例如，我们可以回到现场为低估的植物物种收集更多图像。然而，这并不总是可行的。
- en: If you have to make do, you can potentially address balance issues by *oversampling*
    the underrepresented groups. To do this, you might duplicate some of the records
    for these groups until all subgroups have the same number of records. You could
    also *undersample* the overrepresented groups by throwing some of their records
    away.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你必须应付，你可以通过*过采样*低估的组来潜在地解决平衡问题。为此，你可以复制这些组的一些记录，直到所有子组具有相同数量的记录。你还可以通过*欠采样*过度表现的组来丢弃它们的一些记录。
- en: This technique can be useful when you’re building a dataset to use to train
    a machine learning model. Since models’ learning is typically guided by aggregated
    loss values for an entire dataset if a subgroup is underrepresented it won’t have
    much impact on the model’s learning. Balancing out the numbers through sampling
    can help.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建用于训练机器学习模型的数据集时，这种技术可能很有用。由于模型的学习通常是通过整个数据集的聚合损失值来指导的，如果某个子组被低估，它对模型学习的影响就不大。通过采样平衡各个子组的数量可以有所帮助。
- en: However, oversampling will not help if you simply do not have enough data to
    represent the true variance of the affected subgroup in the real world. For example,
    if one species in our plant dataset is only represented by images from a single
    plant, oversampling them may not lead to a well-performing model—since in the
    real world, there is a lot of variation between individual plants.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你的数据量不足以真实反映受影响子组的真实方差，过采样并不能帮助你。例如，如果我们植物数据集中的某个物种仅由单一植物的图像表示，过采样可能不会导致一个表现良好的模型——因为在现实世界中，各个个体植物之间存在大量变化。
- en: You should also be careful about using oversampled data to evaluate a system.
    The results of your evaluation will be less reliable for the subgroups that you
    have oversampled.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 你在使用过采样数据评估系统时也应该小心。你的评估结果对于你过采样的子组会更不可靠。
- en: An equivalent technique to oversampling is the *weighting* of subgroups during
    training. In this technique, each subgroup is assigned a weight—a factor that
    controls its contribution to either the training or evaluation process. Subgroups
    can be given weights that correct for any balance issues. For example, a subgroup
    that is underrepresented might be given a higher weight than a subgroup that is
    overrepresented.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 一种与过采样等效的技术是在训练过程中对子组进行*加权*。在这种技术中，每个子组被分配一个权重，该权重控制其对训练或评估过程的贡献。可以为子组分配权重以纠正任何平衡问题。例如，一个低估的子组可能被赋予比高估子组更高的权重。
- en: Some datasets are naturally unbalanced. For example, in object recognition datasets
    the areas in images that contain objects are commonly smaller than the areas that
    do not.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 有些数据集天然存在不平衡。例如，在对象识别数据集中，包含对象的图像区域通常比不包含对象的区域要小。
- en: In these cases, where resampling may not work, weighting is often used to increase
    the contribution of the underrepresented data to the training of the model.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，当重新采样可能无效时，通常使用加权来增加低估数据对模型训练的贡献。
- en: Feature Engineering
  id: totrans-504
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: The majority of edge AI projects will involve some feature engineering work
    (see [“Feature Engineering”](ch04.html#feature_engineering)). This could be as
    simple as scaling features (see the sidebar [“Feature Scaling”](ch04.html#feature_scaling))—or
    it might involve extremely complex DSP algorithms.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数边缘AI项目都会涉及一些特征工程工作（参见[“特征工程”](ch04.html#feature_engineering)）。这可能只是简单地缩放特征（见侧边栏[“特征缩放”](ch04.html#feature_scaling)）——或者可能涉及极其复杂的DSP算法。
- en: Since ML models and other decision-making algorithms are run on features, not
    raw data, feature engineering is an important part of dataset preparation. Feature
    engineering will be guided by the iterative application development workflow that
    we’ll meet in [Chapter 9](ch09.html#developing_edge_ai_applications) but establishing
    a baseline for features will be necessary at the dataset preparation stage.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习模型和其他决策算法是在特征上运行而不是原始数据上运行的，特征工程是数据集准备的重要部分。特征工程将由我们在[第9章](ch09.html#developing_edge_ai_applications)中遇到的迭代应用开发工作流指导，但在数据集准备阶段建立特征的基线将是必要的。
- en: 'Doing some initial feature engineering will allow you to explore and understand
    your dataset in terms of features, not just raw data. Beyond this, some other
    important reasons for feature engineering at this stage are:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 进行一些初步的特征工程将使您能够按特征而不是仅仅原始数据的方式探索和理解数据集。在此阶段进行特征工程的一些其他重要原因包括：
- en: Scaling values so that they can be used as inputs to machine learning models
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放值以便作为机器学习模型的输入
- en: Combining values (see [“Combining Features and Sensors”](ch04.html#combining_features)),
    perhaps to perform sensor fusion
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合并值（参见[“合并特征和传感器”](ch04.html#combining_features)），也许进行传感器融合
- en: Precomputing DSP algorithms so that training runs faster^([19](ch07.html#idm45988809956400))
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预先计算DSP算法以加快训练速度^([19](ch07.html#idm45988809956400))
- en: It’s almost certain that you will want to iterate on feature engineering later
    in your development process—but the earlier you can begin this work, the better.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎可以肯定的是，您将希望在开发过程中的某个时候迭代特征工程 — 但您越早开始这项工作，效果会更好。
- en: Splitting Your Data
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割您的数据
- en: As we’ve seen, the workflow for AI projects involves an iterative process of
    algorithm development and evaluation. For reasons we’ll soon expand on, it’s important
    for us to structure our dataset so that it suits this iterative workflow.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，AI项目的工作流程涉及算法开发和评估的迭代过程。基于我们即将详细展开的原因，重要的是为数据集设计结构，使其适应这种迭代工作流程。
- en: 'This is typically done by splitting a dataset into three parts: training, validation,
    and testing.^([20](ch07.html#idm45988809949504)) Here is what each split is used
    for:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 典型情况下，通过将数据集分为三部分来完成此过程：训练、验证和测试。^([20](ch07.html#idm45988809949504)) 每个分割的用途如下：
- en: Training
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 训练
- en: The training split is used directly to develop an algorithm, typically by training
    a machine learning model.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 训练分割直接用于开发算法，通常是通过训练机器学习模型来完成。
- en: Validation
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 验证
- en: The validation split is used to evaluate the model during iterative development.
    Each time a new iteration is developed, performance is checked against the validation
    dataset.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 验证分割用于在迭代开发过程中评估模型。每次开发新迭代时，都会根据验证数据集检查性能。
- en: Testing
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 测试
- en: The testing split is “held out”—it’s kept aside until the very end of a project.
    It is used in a final pass to ensure that the model is able to perform well on
    data that it has never been exposed to before.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 测试分割“被保留” — 直到项目最后阶段才使用。它用于最终验证模型是否能够在以前未接触过的数据上表现良好。
- en: We use separate splits in order to detect overfitting. As discussed in [“Deep
    learning”](ch04.html#deep_learning), overfitting is when a model learns to get
    the correct answers on a specific dataset in a way that does not generalize to
    new data.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用单独的分割来检测过拟合。如[“深度学习”](ch04.html#deep_learning)所讨论的那样，过拟合是指模型在某个特定数据集上学习如何得出正确答案，但这种学习方式并不能泛化到新数据上。
- en: 'To identify overfitting, we can first train a model with the training split.
    We can then measure the model’s performance on both the training data and the
    validation data. For example, we might calculate the accuracy of a classification
    model on each split:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别过拟合，我们可以首先用训练分割训练模型。然后可以测量模型在训练数据和验证数据上的性能。例如，我们可以计算分类模型在每个分割上的准确率：
- en: '[PRE2]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If those numbers are similar, we know that our model is able to take what it
    has learned from the training split and use it to make accurate predictions about
    unseen data. This is what we want—the ability to *generalize*. However, if the
    model performs *less* well on the validation split it’s a sign that the model
    has overfit to the training split. It’s able to perform well on data it has seen
    before, but not on new data:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些数字相似，我们知道我们的模型能够利用从训练分割中学到的知识对未见数据做出准确预测。这正是我们想要的 — 能够*泛化*的能力。然而，如果模型在验证分割上表现*较差*，这表明模型对训练分割过拟合。它能够在之前见过的数据上表现良好，但在新数据上表现不佳：
- en: '[PRE3]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With significantly low accuracy on the validation split, it’s clear that the
    model is not performing well on unseen data. This is a strong signal that the
    model should be changed.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证分割上准确率显著低时，清楚地表明模型在未见数据上表现不佳。这是模型应该改进的强烈信号。
- en: But if the validation split allows us to detect overfitting, why do we also
    need the testing split? This is due to a very interesting quirk of iterative development
    in ML. As we know, our iterative workflow involves making a round of algorithm
    changes, testing them on the validation split, and then making more algorithm
    changes to try to improve performance.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果验证分割可以帮助我们检测过拟合，为什么我们还需要测试分割？这是机器学习迭代开发中一个非常有趣的现象。正如我们所知，我们的迭代工作流程包括对算法进行一轮更改，然后在验证分割上测试它们，然后再进行更多的算法更改，试图提高性能。
- en: As we iteratively tweak and change our model to try to get better performance
    on the validation split, we may end up fine-tuning the model to the point that
    it *happens* to work well for the training and validation data—but would not work
    well on unseen data.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们不断微调和更改我们的模型，以试图在验证分割上取得更好的性能时，我们可能会调整模型到一种*碰巧*在训练和验证数据上工作良好的程度，但在未见数据上表现不佳。
- en: 'In this case, the model has become overfit to the validation data *even though
    it was not directly trained on it*. Via our iterative process, information about
    the validation split has “leaked” into the model: we’ve repeatedly modified it
    in a way that was informed by the data in the validation split, resulting in overfitting.'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，即使模型并没有直接在验证数据上进行训练，模型也已经对验证数据过拟合。通过我们的迭代过程，关于验证分割的信息已经“泄漏”到模型中：我们反复修改它，这些修改是根据验证分割中的数据得出的，导致了过拟合。
- en: This phenomenon means we can’t necessarily trust the signal that our validation
    split is giving us. Fortunately, the testing split gives us a way around this
    problem. By keeping the testing split aside until the very end of our process,
    once all the iteration has been done, we can get a clear signal that tells us
    whether our model is *really* working on unseen data.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象意味着我们不能完全信任验证分割给我们的信号。幸运的是，测试分割为我们提供了解决这个问题的方法。通过将测试分割保留到我们的过程的最后阶段，当所有迭代都完成时，我们可以得到一个清晰的信号，告诉我们我们的模型*真的*在未见数据上工作。
- en: How is data split?
  id: totrans-531
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据如何分割？
- en: Data is typically split by random sampling, according to proportion. A common
    standard is to first split the data 80/20, with the 20% becoming the testing split.
    The 80% split is then itself split 80/20, with the 80% becoming the training split
    and the 20% becoming the validation. This is shown in [Figure 7-13](#pie_chart_data_split).
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通常通过随机抽样按比例进行分割。一个常见的标准是首先将数据按80/20分割，其中20%成为测试分割。然后将80%分割为80/20，其中80%成为训练分割，20%成为验证分割。这在[图7-13](#pie_chart_data_split)中显示。
- en: '![A pie chart showing the training, testing, and validation split.](assets/aiae_0713.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![显示训练、测试和验证分割的饼图](assets/aiae_0713.png)'
- en: Figure 7-13\. The dataset is split into chunks for training, testing, and validation.
  id: totrans-534
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-13。数据集被分割为用于训练、测试和验证的块。
- en: Depending on your dataset it may be reasonable to use smaller amounts of data
    in your validation and testing splits, keeping more of it for use in training.
    The key thing is that each split is a representative sample of the dataset as
    a whole. If your data has low variance, this may be achievable with a relatively
    small sample.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的数据集，使用较少量的数据进行验证和测试分割可能是合理的，保留更多数据用于训练。关键是，每个分割点都是整个数据集的代表样本。如果您的数据方差较低，这可能通过相对较小的样本来实现。
- en: Each split should also be representative in terms of the balance and diversity
    of the entire dataset. For example, the training, validation, and testing splits
    for a plant disease classification dataset should each contain the same approximate
    balance between different types of plant diseases.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据分割点也应在整个数据集的平衡和多样性方面具有代表性。例如，植物疾病分类数据集的训练、验证和测试分割点应该在不同类型植物疾病之间保持大致相同的平衡。
- en: If the dataset is well balanced and has a low ratio of subgroups to total number
    of samples, this can be achieved simply through random sampling. However, if there
    are many different subgroups, or if some are underrepresented, it may be a good
    idea to perform stratified sampling. In this technique, the splits are performed
    individually for each subgroup and then combined. This means each split will have
    the same balance as the overall dataset. A simple example is shown in [Figure 7-14](#data_split_stratified).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集平衡且子组与总样本数的比率较低，则可以通过随机抽样简单实现。然而，如果存在许多不同的子组，或者某些子组少数代表，执行分层抽样可能是一个好主意。在这种技术中，针对每个子组单独执行分割，然后合并。这意味着每个分割点将具有与整体数据集相同的平衡。一个简单的例子在[图7-14](#data_split_stratified)中显示。
- en: '![A diagram comparing random and stratified sampling.](assets/aiae_0714.png)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![比较随机抽样和分层抽样的图表。](assets/aiae_0714.png)'
- en: Figure 7-14\. Stratified sampling can help preserve the distribution of subgroups
    when a dataset is split.
  id: totrans-539
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-14\. 当数据集分割时，分层抽样可以帮助保留子群体的分布。
- en: Pitfalls when splitting data
  id: totrans-540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分割数据时的陷阱
- en: 'Splitting data incorrectly will deny you the ability to measure how your application
    performs on unseen data, which is likely to result in bad performance in the real
    world. Here are some common mistakes to avoid:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 错误地分割数据将使你无法测量你的应用在未见数据上的表现，这可能导致在现实世界中表现不佳。以下是一些常见的要避免的错误：
- en: Curating splits
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 精心策划的分割
- en: Validation and testing splits are supposed to be representative samples of the
    overall dataset. A big no-no is hand-selecting which records are included in each
    split. For example, imagine you decided to put the records you think are the most
    challenging in the training split—with the theory that it would help your model
    learn.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 验证和测试分割应该是整体数据集的代表性样本。手动选择每个分割中包含的记录是大忌。例如，想象一下，你决定将你认为最具挑战性的记录放在训练分割中——理论上这会帮助你的模型学习。
- en: If these records are not represented in the testing split, you won’t have any
    insight into how your model is really performing on them. On the other hand, if
    you put all your most challenging records in the testing split, your model won’t
    get the benefit of being trained on them.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些记录没有包含在测试分割中，你将无法真正了解模型在其上的表现。另一方面，如果你将所有最具挑战性的记录都放在测试分割中，你的模型将无法从它们的训练中受益。
- en: Choosing which records go into each split is a job for a random sampling algorithm,
    not something you should do by hand. The Python library scikit-learn has a good
    set of tools for performing dataset splits.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 决定哪些记录进入每个分割是随机抽样算法的工作，而不是你手动完成的事情。Python库scikit-learn提供了一套很好的工具来执行数据集分割。
- en: Balance and representation problems
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡和代表性问题
- en: As discussed earlier, it’s important that each split has the same balance, and
    that all splits are representative. This applies to both classes (for classification
    problems) and “unofficial” subgroups. For example, if your data is collected from
    several different types of sensors you should consider performing stratified sampling
    to ensure that an appropriate proportion of data from each sensor type is contained
    within each split.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面讨论的，每个分割具有相同的平衡和代表性非常重要。这适用于分类问题的类别和“非官方”子群体。例如，如果你的数据来自多种不同类型的传感器，你应该考虑执行分层抽样，以确保每个分割中包含来自每种传感器类型的适当比例的数据。
- en: Predicting the past
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 预测过去
- en: For models that perform predictions on time series data, things get a little
    more complicated. In the real world, we’re always trying to predict the *future*
    based on the *past*. This means that to accurately evaluate a time series model
    we need to make sure it is trained on earlier values and tested (and validated)
    on later ones. Otherwise, we may just be training a model that can predict *past*
    values based on current ones—which probably isn’t what we intended. This “leakage”
    of data backwards along the timeline is worth considering any time you are working
    with time series.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在时间序列数据上执行预测的模型，情况会变得更加复杂。在现实世界中，我们始终试图基于过去来预测未来。这意味着为了准确评估时间序列模型，我们需要确保它是在较早的值上进行训练，并在稍后的值上进行测试（和验证）。否则，我们可能只是训练了一个可以基于当前值预测过去值的模型——这可能不是我们的初衷。在处理时间序列时，数据沿时间线向后的这种“泄漏”值得在任何时候都要考虑。
- en: Duplicate values
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 重复值
- en: When working with large amounts of data, it’s easy for records to get duplicated.
    There may be duplicates in your original data, or they may creep in during whatever
    process you use for splitting data. Any duplicates between splits will harm your
    ability to measure overfitting, so they should be avoided.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大量数据时，记录很容易出现重复。你的原始数据中可能有重复，或者在分割数据的过程中可能会悄悄插入。任何分割之间的重复都会影响你测量过拟合能力的能力，因此应该避免。
- en: Changing splits
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 分割变更
- en: If you’re trying to compare multiple approaches using testing dataset performance,
    it’s important that you use the same testing split each time. If you use a different
    set of samples each time, you won’t be able to tell which model is better—any
    variations may merely be a result of the difference in split.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试使用测试数据集的性能比较多种方法，重要的是每次都使用相同的测试分割。如果每次使用不同的样本集，你将无法确定哪个模型更好——任何变化可能仅仅是分割差异的结果。
- en: Augmented testing data
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 增强的测试数据
- en: 'If you’re performing data augmentation (which we’ll learn about in the next
    section), only your training data should be augmented. Augmenting your validation
    and testing splits will dilute the insight they give you into real-world performance:
    you want them to be composed of pure real-world data. If you evaluate your model
    on augmented data you will have no guarantee that it works on nonaugmented data.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在执行数据增强（我们将在下一节学习），只应增强您的训练数据。增强您的验证和测试拆分将削弱它们对真实世界性能的洞察力：您希望它们由纯粹的真实世界数据组成。如果您在增强数据上评估模型，您将无法保证它在非增强数据上的工作效果。
- en: Data Augmentation
  id: totrans-556
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: Data augmentation is a technique designed to help make the most of limited data.
    It works by introducing random artificial variations into a dataset that simulate
    the types of variations that are naturally present in the real world.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是一种旨在帮助充分利用有限数据的技术。它通过向数据集引入模拟自然界中存在的类型变化的随机人工变化来工作。
- en: For example, an image might be augmented by modifying its brightness and contrast,
    rotating it, zooming into a specific region, or any combination of the above—as
    shown in [Figure 7-15](#minicat_augmentation).
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可以通过修改图像的亮度和对比度、旋转、放大特定区域或以上任意组合来增强图像 — 如[图7-15](#minicat_augmentation)所示。
- en: 'Any type of data can be augmented. For example, background noise can be mixed
    into audio, and time series can be transformed in many different ways. Augmentation
    can be performed both before and after feature engineering. Common augmentations
    include:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 任何类型的数据都可以增强。例如，可以将背景噪声混合到音频中，时间序列可以以许多不同的方式进行转换。增强可以在特征工程之前和之后进行。常见的增强包括：
- en: Additive
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 添加性
- en: Incorporating other signals, such as random noise, or background noise sampled
    from the real world
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 整合其他信号，例如来自现实世界的随机噪声或背景噪声
- en: Subtractive
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 减法
- en: Removing or obscuring values, or removing chunks of time or frequency bands
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 移除或模糊值，或移除时间或频率带的块
- en: Geometric
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 几何
- en: Rotating, shifting, squashing, stretching, or otherwise spatially manipulating
    a signal
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转、移位、压缩、拉伸或以其他方式空间操作信号
- en: Filter-based
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 基于过滤器的
- en: Increasing and decreasing properties of individual values by random amounts
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 通过随机量增加和减少单个值的特性
- en: '![An image of the author''s cat augmented in several ways.](assets/aiae_0715.png)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![作者猫的多种增强图像。](assets/aiae_0715.png)'
- en: Figure 7-15\. An image of the author’s cat augmented in several different ways.
  id: totrans-569
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-15。作者的猫以多种不同的方式增强。
- en: Augmentation increases the amount of variation in the training data. This can
    have the benefit of helping the model generalize. Since there is a lot of random
    variation, the model is forced to learn general underlying relationships rather
    than perfectly memorizing the entire dataset (which would result in overfitting).
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 增强增加了训练数据的变化量。这可以有助于模型泛化。由于存在大量随机变化，模型被迫学习一般的基础关系，而不是完全记忆整个数据集（这将导致过拟合）。
- en: It’s important that data augmentation is only applied to the training split
    of a dataset. Augmented records should not be included in validation or testing
    data since the goal is to evaluate the model’s performance on real data.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强只应用于数据集的训练拆分非常重要。增强的记录不应包含在验证或测试数据中，因为目标是评估模型在真实数据上的表现。
- en: Data augmentation is typically accomplished through libraries—most machine learning
    frameworks provide some built-in data augmentation features, and many data augmentation
    protocols have been documented in scientific literature and made available as
    open source code.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强通常是通过库完成的 — 大多数机器学习框架提供了一些内置的数据增强功能，并且许多数据增强协议已经在科学文献中记录并作为开源代码提供。
- en: Augmentation can either be performed *online* or *offline*. In online augmentation,
    random changes are applied to each record every time it is used during the training
    process. This is great, since it results in a huge amount of random variation.
    However, some augmentations can be computationally expensive, so it can potentially
    slow down training a lot.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 增强可以通过*在线*或*离线*方式进行。在在线增强中，每次在训练过程中使用记录时都会应用随机变化。这非常好，因为它会导致大量的随机变化。然而，一些增强可能计算成本高昂，因此可能会显著减慢训练速度。
- en: In offline augmentation, each record is randomly changed a specific number of
    times, and the changed versions are saved to disk as a larger, augmented dataset.
    This augmented dataset is then used to train a model. Since augmentation is done
    ahead of time, the training process is a lot faster. However, less variation is
    introduced when using offline augmentation because there are a finite (and usually
    limited) number of variants created of each record.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 在离线增强中，每条记录都会随机更改特定次数，并将更改后的版本保存为更大的增强数据集。然后使用这个增强数据集来训练模型。由于增强是提前完成的，训练过程速度更快。然而，在使用离线增强时引入的变化较少，因为每条记录创建的变体是有限的（通常是有限的）。
- en: The types of augmentations applied to a dataset can be varied, and different
    variations may result in models that perform better or worse. This means that
    the design of an augmentation scheme must be part of the overall iterative development
    workflow. This is one reason why it’s a bad idea to augment your validation or
    test datasets. If you were to do so, then any change to your augmentation scheme
    would also change your validation or test data. This would prevent you from comparing
    the performance of different models against the same datasets.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于数据集的增强类型可以多种多样，不同的变体可能导致模型表现更好或更差。这意味着增强方案的设计必须成为整体迭代开发工作流程的一部分。这也是为什么增强您的验证或测试数据集是个坏主意的原因之一。如果您这样做了，那么对增强方案的任何更改也会改变您的验证或测试数据。这将阻止您对比不同模型在相同数据集上的性能。
- en: Designing an appropriate set of augmentations is a task that requires domain
    expertise. For example, an expert should have insight into the best types of background
    noise to mix into an audio dataset based on the context of the application.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 设计适当的增强集是需要领域专业知识的任务。例如，专家应该根据应用程序的背景了解最佳的背景噪声类型以混合到音频数据集中。
- en: Data Pipelines
  id: totrans-577
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管道
- en: 'Over the course of this chapter, we’ve encountered a sequence of tasks and
    considerations that are applied to data:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的过程中，我们遇到了一系列应用于数据的任务和考虑因素：
- en: Capture
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获
- en: Storage
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储
- en: Evaluation
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估
- en: Labeling
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记
- en: Formatting
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式化
- en: Auditing
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审计
- en: Cleaning
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗
- en: Sampling
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采样
- en: Feature engineering
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Splitting
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割
- en: Augmentation
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强
- en: This sequence of tasks, in whatever order you perform them, can be thought of
    as a *data pipeline*. Your data pipeline begins out in the field, where data is
    generated by sensors and applications. It then brings data into your internal
    systems, where it is stored, joined together, labeled, examined and processed
    for quality, and made ready for use in training and evaluating AI applications.
    A simple data pipeline is shown in [Figure 7-16](#data_pipeline_figure).
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您以何种顺序执行这些任务，这一系列任务都可以被视为*数据管道*。您的数据管道始于现场，数据由传感器和应用程序生成。然后将数据带入您的内部系统，在那里它被存储、连接、标记、检查和处理以保证质量，并准备用于训练和评估AI应用程序。一个简单的数据管道如图[7-16](#data_pipeline_figure)所示。
- en: '![Diagram of a simple data pipeline consisting of capture, cleaning, labeling,
    auditing, and feature engineering.](assets/aiae_0716.png)'
  id: totrans-591
  prefs: []
  type: TYPE_IMG
  zh: '![简单数据管道的图示，包括捕获、清洗、标记、审计和特征工程。](assets/aiae_0716.png)'
- en: Figure 7-16\. A basic data pipeline for capturing and processing data; every
    project has a different data pipeline, and their complexity can vary greatly.
  id: totrans-592
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-16\. 用于捕获和处理数据的基本数据管道；每个项目都有一个不同的数据管道，它们的复杂程度可以大不相同。
- en: You should consider your data pipeline a critical piece of infrastructure. It
    should be implemented in clean, well-designed code that is well documented, versioned,
    and includes whatever information on dependencies is required to be able to run
    it repeatably.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 您应将您的数据管道视为基础设施的重要组成部分。它应该由干净、设计良好的代码实现，并且有良好的文档、版本控制，包括运行它所需的依赖信息。
- en: Any changes to your data pipeline have the potential for major downstream effects
    on your dataset, so it’s critical that you understand exactly what is being done—both
    during initial development and in the future.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据管道的任何更改都可能对数据集产生重大的下游影响，因此您必须确切了解在初始开发和未来期间正在进行的工作。
- en: The nightmare scenario is that the processes that led to the creation of a dataset
    are lost, since the data pipeline was not documented or can no longer be run.
    As we saw earlier, a dataset represents the distillation of domain expertise into
    an artifact that can be used to create algorithms.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 噩梦般的情景是，导致数据集创建的过程丢失，因为数据管道未记录或无法再运行。正如我们之前所看到的，数据集代表了领域专业知识的精髓，可以用来创建算法的工件。
- en: If the processes used to create that dataset are not documented, then it will
    no longer be possible to understand the decisions or engineering that went into
    its construction. This will make it extremely difficult to debug problems with
    the resulting AI systems and will make it very difficult to meaningfully improve
    on the system—even if new data becomes available.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用于创建该数据集的过程没有记录，那么将不再能够理解其构建中所涉及的决策或工程。这将极大地增加调试导致的AI系统问题的难度，并使得即使新数据可用，系统改进也非常困难。
- en: In edge AI, where highly complex sensor data is common, keeping good track of
    data pipelines is especially vital. Unfortunately, this nightmare scenario is
    very common! It’s only recently, with the rise of MLOps practices, that practitioners
    have been taking data pipelines as seriously as they deserve.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘人工智能中，高度复杂的传感器数据很常见，因此良好跟踪数据管道尤为重要。不幸的是，这种噩梦情景非常普遍！直到最近，随着MLOps实践的兴起，从业者们才开始像它们应得的那样重视数据管道。
- en: 'MLOps, a contraction of *machine learning operations*, is a field of engineering
    related to the operational management of machine learning projects. We’ll be digging
    into it fully across Chapters [9](ch09.html#developing_edge_ai_applications) and
    [10](ch10.html#deploying_evaluating_and_supporting_edge_ai_applications). One
    of the most important reasons to think about MLOps is to make it possible to improve
    ML applications over time, by adding new data and training better models. This
    is our most important tool for fighting against the big enemy of production ML
    projects: drift.'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps，即机器学习运营，是与机器学习项目的操作管理相关的工程领域。我们将在第[9](ch09.html#developing_edge_ai_applications)章和第[10](ch10.html#deploying_evaluating_and_supporting_edge_ai_applications)章中全面探讨它。考虑MLOps的最重要原因之一是，通过添加新数据和训练更好的模型，可以不断改进ML应用程序。这是我们对抗生产ML项目的重大敌人——数据漂移的最重要工具。
- en: Building a Dataset over Time
  id: totrans-599
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随着时间的推移构建数据集
- en: As we saw in [“Drift and Shift”](#drift_and_shift), the real world changes over
    time—often quite rapidly. Since our dataset is just a snapshot of a moment in
    time, it will eventually stop being representative. Any algorithms developed with
    a stale, outdated dataset will be ineffective in the field.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[“漂移与偏移”](#drift_and_shift)中所看到的，现实世界随时间变化——通常是相当快速的。由于我们的数据集只是时间片段的一个快照，它最终将不再具有代表性。使用陈旧、过时的数据集开发的任何算法将在实地中失效。
- en: The fight against drift is one strong reason why you should always be collecting
    more data. With a constant trickle of new data, you can make sure that you are
    training and deploying up-to-date models that perform well in the real world.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 抵御数据漂移是你始终应该收集更多数据的一个强有力原因。有了持续涓涓细流的新数据，你可以确保训练和部署最新模型，在实际世界中表现良好。
- en: Edge AI algorithms are often deployed to devices that must tolerate poor connectivity.
    This means it’s often very difficult to measure the performance of devices that
    are deployed in the field. This provides another key benefit of continuously collecting
    data.^([22](ch07.html#idm45988809820624)) With fresh data available, you can understand
    the performance of the same algorithms that have been deployed on devices operating
    in the real world. If performance starts to degrade, those devices may need to
    be replaced. Without fresh data, you’ll have no way of knowing it.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘AI算法通常部署到必须容忍较差连接的设备上。这意味着很难测量部署在实地操作设备上的性能。这提供了持续收集数据的另一个关键好处^([22](ch07.html#idm45988809820624))。有了新鲜数据，你可以了解到同样算法在实际操作中设备的性能。如果性能开始下降，可能需要更换这些设备。如果没有新鲜数据，你将无法知晓这一点。
- en: Beyond drift, it’s pretty much always helpful to have more data. More data means
    more natural variation in your dataset, which means superior models that are better
    able to generalize to real-world conditions.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 除了漂移，拥有更多数据几乎总是有益的。更多数据意味着数据集中更自然的变化，这意味着更能够推广到真实世界条件的优秀模型。
- en: From a data-centric ML perspective, data collection itself should be part of
    our iterative development feedback loop. If we recognize that our application
    or model is falling short in certain ways, we can identify different types of
    additional data that will help improve it. If we have a good system for continually
    improving our dataset we can close the feedback loop and build more effective
    applications.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中心的机器学习角度来看，数据收集本身应该是我们迭代开发反馈循环的一部分。如果我们意识到我们的应用程序或模型在某些方面表现不佳，我们可以识别出不同类型的额外数据，以帮助改进它。如果我们有一个良好的系统来持续改进我们的数据集，我们就可以关闭反馈循环，并构建更有效的应用程序。
- en: Well-engineered data pipelines are a critical tool in enabling continuous dataset
    growth. If you have a repeatable pipeline that can be run on new data, it will
    massively reduce the amount of friction involved with adding new records to your
    dataset. Without a reliable pipeline, it may prove too risky to add new data—there’s
    no guarantee it will consistently match your original dataset.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 精心设计的数据管道是实现数据集持续增长的关键工具。如果您有一个可重复运行新数据的管道，将大大减少添加新记录到数据集中所涉及的摩擦。没有可靠的管道，添加新数据可能会过于风险——无法保证它会与原始数据集一致。
- en: Summary
  id: totrans-606
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The creation of a dataset is a continuous process that starts at the very beginning
    of an edge AI project—and never really ends. In a modern, data-centric workflow,
    the dataset will evolve along with the design and requirements of your application.
    It will change with every iterative step in your project.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据集是边缘人工智能项目从一开始就持续进行的过程，并且从未真正结束。在现代的数据中心工作流程中，数据集将随着您应用程序的设计和需求的演变而发展。它会随着项目中每一步的迭代而改变。
- en: We’ll learn more about the role datasets play in the application development
    process in [Chapter 9](ch09.html#developing_edge_ai_applications). In [Chapter 8](ch08.html#designing_edge_ai_applications),
    we’ll focus on the way applications are designed.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 [第9章](ch09.html#developing_edge_ai_applications) 中了解数据集在应用程序开发过程中的角色。在
    [第8章](ch08.html#designing_edge_ai_applications) 中，我们将专注于应用程序的设计方式。
- en: ^([1](ch07.html#idm45988813520160-marker)) Bear in mind that samples at the
    dataset level are not the same thing as samples in an arbitrary digital signal.
    One dataset-level sample (aka record) might contain a feature that consists of
    multiple samples. As a multidisciplinary field, edge AI has a ton of these confusing
    terminology collisions!
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.html#idm45988813520160-marker)) 请记住，数据集级别的样本与任意数字信号中的样本不是同一回事。一个数据集级别的样本（也称为记录）可能包含一个由多个样本组成的特征。作为一个多学科领域，边缘人工智能有很多这些令人困惑的术语冲突！
- en: ^([2](ch07.html#idm45988813500880-marker)) According to [Statista Global Consumer
    Survey, 2022](https://oreil.ly/7qzc8).
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.html#idm45988813500880-marker)) 根据 [Statista 2022全球消费者调查](https://oreil.ly/7qzc8)。
- en: ^([3](ch07.html#idm45988813497616-marker)) See [“The Uneven Distribution of
    Errors”](#the_uneven_distribution_of_errors).
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch07.html#idm45988813497616-marker)) 参见 [“错误的不均匀分布”](#the_uneven_distribution_of_errors)。
- en: ^([4](ch07.html#idm45988813475392-marker)) [Chapter 9](ch09.html#developing_edge_ai_applications)
    will talk more about the composition of the teams required to build edge AI products.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch07.html#idm45988813475392-marker)) [第9章](ch09.html#developing_edge_ai_applications)
    将更多地讨论构建边缘人工智能产品所需的团队构成。
- en: '^([5](ch07.html#idm45988813423152-marker)) Michael Roberts et al., “Common
    Pitfalls and Recommendations for Using Machine Learning to Detect and Prognosticate
    for COVID-19 Using Chest Radiographs and CT Scans,” *Nat Mach Intell* 3(2021):
    199–217, [*https://doi.org/10.1038/s42256-021-00307-0*](https://doi.org/10.1038/s42256-021-00307-0).'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: '^([5](ch07.html#idm45988813423152-marker)) Michael Roberts等人，“使用胸部X射线和CT扫描检测和预后COVID-19的机器学习常见问题和建议”，*Nat
    Mach Intell* 3(2021): 199–217，[*https://doi.org/10.1038/s42256-021-00307-0*](https://doi.org/10.1038/s42256-021-00307-0)。'
- en: ^([6](ch07.html#idm45988813347872-marker)) This term is explained in [“How is
    data split?”](#how_is_data_split).
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch07.html#idm45988813347872-marker)) 这个术语在 [“数据如何分割？”](#how_is_data_split)
    中有解释。
- en: ^([7](ch07.html#idm45988813326288-marker)) Additionally, you may find yourself
    combining data from two or more of these sources.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.html#idm45988813326288-marker)) 此外，您可能会发现自己正在结合两个或更多来源的数据。
- en: ^([8](ch07.html#idm45988813312688-marker)) That said, even messy public datasets
    can be helpful in *evaluating* algorithms—they can be a good source of interesting
    corner cases.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch07.html#idm45988813312688-marker)) 话虽如此，即使是混乱的公共数据集在*评估*算法时也可能会有所帮助——它们可以成为有趣角落案例的良好来源。
- en: ^([9](ch07.html#idm45988813292576-marker)) The age-old practice of transmitting
    data by carrying a storage device from one place to another. See this [Wikipedia
    article](https://oreil.ly/gqK1e).
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch07.html#idm45988813292576-marker)) 将数据通过携带存储设备从一个地方传输到另一个地方的古老做法。参见这篇[Wikipedia文章](https://oreil.ly/gqK1e)。
- en: ^([10](ch07.html#idm45988813266720-marker)) In Sim2Real projects, synthetic
    data is used for training, and real-world data is used for testing.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch07.html#idm45988813266720-marker)) 在Sim2Real项目中，使用合成数据进行训练，并使用真实世界数据进行测试。
- en: ^([11](ch07.html#idm45988812089344-marker)) In this case our field may be literal,
    not metaphorical!
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch07.html#idm45988812089344-marker)) 在这种情况下，我们的领域可能是字面意义上的，而不是比喻性的！
- en: ^([12](ch07.html#idm45988812028592-marker)) The Z score can be looked up in
    a table such as the one [hosted by Wikipedia](https://oreil.ly/3pKd5).
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch07.html#idm45988812028592-marker)) Z分数可以在[维基百科托管的](https://oreil.ly/3pKd5)表格中查找。
- en: ^([13](ch07.html#idm45988812001408-marker)) To make sure the sample is truly
    random, it’s a good idea to use sampling tools such as those provided by [NumPy](https://oreil.ly/fuBiY).
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch07.html#idm45988812001408-marker)) 为了确保样本是真正随机的，使用类似[NumPy](https://oreil.ly/fuBiY)提供的采样工具是个好主意。
- en: ^([14](ch07.html#idm45988811991744-marker)) Curtis G. Northcutt et al., “Pervasive
    Label Errors in Test Sets Destabilize Machine Learning Benchmarks,” arXiv, 2021,
    [*https://oreil.ly/Zrcu1*](https://oreil.ly/Zrcu1).
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch07.html#idm45988811991744-marker)) Curtis G. Northcutt等人的文章，“Pervasive
    Label Errors in Test Sets Destabilize Machine Learning Benchmarks”，arXiv，2021年，[*https://oreil.ly/Zrcu1*](https://oreil.ly/Zrcu1)。
- en: ^([15](ch07.html#idm45988811988208-marker)) For a good exploration of the impact
    of label noise, check out Görkem Algan and Ilkay Ulusoy, “Label Noise Types and
    Their Effects on Deep Learning,” arXiv, 2020, [*https://oreil.ly/1LZKl*](https://oreil.ly/1LZKl).
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch07.html#idm45988811988208-marker)) 要了解标签噪声的影响，可以查看Görkem Algan和Ilkay
    Ulusoy的文章，“Label Noise Types and Their Effects on Deep Learning”，arXiv，2020年，[*https://oreil.ly/1LZKl*](https://oreil.ly/1LZKl)。
- en: ^([16](ch07.html#idm45988811875792-marker)) See [“Anomaly detection”](ch04.html#anomaly_detection).
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch07.html#idm45988811875792-marker)) 参见[“异常检测”](ch04.html#anomaly_detection)。
- en: ^([17](ch07.html#idm45988811862176-marker)) We might also use some mechanism
    that gives the manually labeled items more weight during training.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch07.html#idm45988811862176-marker)) 我们还可以使用某些机制，在训练期间给手动标记的项目更多的权重。
- en: ^([18](ch07.html#idm45988811773088-marker)) Activations are taken from a layer
    toward the end of the model, acting as embeddings.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch07.html#idm45988811773088-marker)) 激活来自模型末端的一层，作为嵌入。
- en: ^([19](ch07.html#idm45988809956400-marker)) During training, a machine learning
    model will be exposed to the entire dataset multiple times. Precomputing and caching
    DSP results avoids the need to run the DSP algorithms repeatedly on the same data,
    which can take a lot of unnecessary time.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch07.html#idm45988809956400-marker)) 在训练期间，机器学习模型将多次接触整个数据集。预计算和缓存DSP结果可以避免重复在相同数据上运行DSP算法，从而节省大量不必要的时间。
- en: ^([20](ch07.html#idm45988809949504-marker)) Some people may use slightly different
    terms for these, but the underlying best practice is universal.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch07.html#idm45988809949504-marker)) 有些人可能对这些使用略有不同的术语，但其背后的最佳实践是普遍适用的。
- en: ^([21](ch07.html#idm45988809923792-marker)) Since your testing split should
    be a random sample of your entire dataset, it’s hard to just go out and collect
    a new one. If your testing split was collected *after* the rest of your data,
    it may represent slightly different conditions, harming your ability to evaluate.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch07.html#idm45988809923792-marker)) 由于您的测试分割应该是整个数据集的随机样本，所以很难出去收集一个新的。如果您的测试分割是在其余数据之后收集的，可能代表了略有不同的条件，这会影响您评估的能力。
- en: ^([22](ch07.html#idm45988809820624-marker)) This is presuming that data can
    be collected in some other way that gets around the connectivity issues that deployed
    devices struggle with.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch07.html#idm45988809820624-marker)) 这假定数据可以以某种方式收集，以避免部署设备面临的连接问题。
