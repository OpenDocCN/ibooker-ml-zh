- en: Chapter 2\. Creating Your First Simulation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章\. 创建你的第一个模拟场景
- en: 'We’re going to get started by looking at a simple *simulation environment*:
    a ball agent that can roll around a platform. As we said earlier, we know it’s
    a lot to handle, but we think you’ll be able to cope with the levels of excitement
    and come through with a better understanding of machine learning and simulation
    with Unity.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的 *模拟环境* 开始：一个可以在平台上滚动的球代理。正如我们之前所说，我们知道这是一个很多处理的问题，但我们认为你能够应对这些激动人心的挑战，并且最终能够更好地理解
    Unity 中的机器学习和模拟。
- en: Everybody Remembers Their First Simulation
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个人都会记得他们的第一个模拟场景
- en: 'In this chapter we’re going to build a brand-new simulation environment using
    Unity, create an agent, and then train that agent to accomplish a task in the
    environment using reinforcement learning. It’s going to be a *very* simple simulation
    environment, but it will serve to demonstrate a number of important things:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 Unity 构建一个全新的模拟环境，创建一个代理，然后使用强化学习训练该代理在环境中完成任务。这将是一个非常简单的模拟环境，但它将展示许多重要内容：
- en: How straightforward it is to assemble a scene in Unity by using a small collection
    of simple objects
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过使用少量简单对象的小集合在 Unity 中组装场景是多么简单。
- en: How to use the Unity Package Manager to import the Unity side of the Unity ML-Agents
    Toolkit into Unity and set up a Unity project for machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Unity Package Manager 将 Unity ML-Agents 工具包的 Unity 侧导入 Unity，并为机器学习设置一个
    Unity 项目。
- en: How to set up a simple agent in your simulation object with the intention of
    enabling it to accomplish a task
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在你的模拟对象中设置一个简单的代理，以便使其能够完成任务
- en: How to take manual control of your agent to test the simulation environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何手动控制你的代理以测试模拟环境
- en: How to start a training run using the command-line tool (CLI) side of the Unity
    ML-Agents Toolkit, and how to bring up TensorBoard to monitor the training’s progress
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Unity ML-Agents Toolkit 命令行工具（CLI）开始训练运行，并如何启动 TensorBoard 监控训练进展
- en: How to bring a trained model file back into a Unity simulation environment and
    run the agent using the trained model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将训练好的模型文件带回 Unity 模拟环境并使用训练好的模型运行代理。
- en: By the end of this chapter, you’ll be comfortable enough with Unity and with
    using the ML-Agents Toolkit to dive into deeper, more complicated problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的学习，你将对 Unity 和使用 ML-Agents 工具包来解决更深层次、更复杂问题感到更加自如。
- en: Note
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: This chapter and a few of the subsequent ones won’t be peeling back the layers
    on the underlying machine learning algorithms (remember the word *practical* in
    this book’s title?), but we will start to look at the workings of the machine
    learning algorithms in time, *we promise*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章和后面几章不会揭示底层的机器学习算法（请记住本书标题中的“实用”一词？），但我们将开始逐步了解机器学习算法的工作原理，*我们承诺*。
- en: Our Simulation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的模拟
- en: 'Our first simulation is deceptively simple: a small environment with a ball
    in it, sitting on a floor in a void. The ball will be able to roll around, including
    falling off the floor and into the void. It will be the only element that’s controllable:
    it will be controllable by both the user (i.e., us, for testing purposes) and
    the reinforcement learning ML-Agents system.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个模拟场景看似简单：一个小环境中放置了一个球，它位于虚空中的地面上。球可以自由滚动，包括从地面上掉落到虚空中。它是唯一可控制的元素：既可以由用户（即我们，用于测试目的）控制，也可以由强化学习
    ML-Agents 系统控制。
- en: Thus, the ball will act as our *agent*, and its objective will be to get to
    the *target* as quickly as possible without falling off the *floor*. The simulation
    environment we’ll build was shown in [Figure 2-1](#fig:firstsim).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，球将作为我们的 *代理*，其目标是在不掉落到 *地面* 之外尽快到达 *目标*。我们将构建的模拟环境在 [Figure 2-1](#fig:firstsim)
    中展示过。
- en: '![psml 0201](assets/psml_0201.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0201](assets/psml_0201.png)'
- en: Figure 2-1\. The simulation we’ll be building
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. 我们将要构建的模拟场景
- en: 'Broadly, the steps to create any simulation environment and train one or more
    agents to operate within it are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上讲，创建任何模拟环境并训练一个或多个代理在其中运作的步骤如下：
- en: 'Build the environment in Unity: the environment is a physical simulation that
    contains objects.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Unity 中构建环境：这个环境是一个包含物体的物理模拟。
- en: 'Implement the machine learning elements: namely, we need an agent that operates
    within the environment.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现机器学习元素：即我们需要一个能在环境中运作的代理。
- en: Implement the code that will tell the agent how to observe the environment,
    how to carry out actions within the environment, how to calculate rewards it might
    receive for acting within the environment, and how to reset itself or the environment
    when it succeeds or fails at its task.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现代码，告诉代理如何观察环境，如何在环境中执行动作，如何计算可能收到的奖励，并在成功或失败时如何重置自身或环境。
- en: Train the agent in the environment.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在环境中训练代理。
- en: We’ll be performing each of these four steps in this chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中执行这四个步骤。
- en: Setting Up
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: For a rundown and discussion of the tools you’ll need for simulation and machine
    learning, refer back to [Chapter 1](ch01.html#introducing-the-tools). This section
    will give you a quick summary of the bits and pieces you’ll need to accomplish
    *this particular activity*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于仿真和机器学习所需的工具及其讨论，请参考[第一章](ch01.html#introducing-the-tools)。本节将为您提供关于完成*此特定活动*所需的各个部分的快速摘要。
- en: 'Specifically, to work on the activity in this chapter and build the simple
    simulation environment, you’ll need to do the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，为了完成本章的活动并构建简单的模拟环境，您需要执行以下操作：
- en: '*Install Unity 2021 or later.* This book isn’t here to teach you the basics
    of Unity (we wrote a [great book on that](https://www.oreilly.com/library/view/unity-game-development/9781491999141),
    if you’re keen), but it’s worth noting that the way Unity likes to be installed
    changes more often than the underlying material this book is teaching, so we recommend
    checking out [the Unity Installation Guide on the Unity website](https://oreil.ly/sEa5n)
    for the latest on installing Unity. Hop over there, get the right version of Unity
    installed, and come back. We’ll still be here.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*安装 Unity 2021 或更新版本.* 本书不会教授您 Unity 的基础知识（如果您感兴趣，我们已经写了一本[优秀的书籍](https://www.oreilly.com/library/view/unity-game-development/9781491999141)），但值得注意的是，Unity
    的安装方式经常变化，比这本书教授的基础材料更频繁，因此我们建议查看 Unity 网站上的[Unity安装指南](https://oreil.ly/sEa5n)以获取最新的安装信息。跳转到那里，安装适合的
    Unity 版本，然后回来。我们还会在这里等您。'
- en: Tip
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: While the Unity ML-Agents Toolkit works with any version of Unity newer than
    2018.4, we recommend that you install the latest 2021 version of Unity. You might
    find a 2021 LTS version of Unity. LTS stands for Long Term Support, and it is
    the version of Unity that the Unity team maintains for a designated period of
    time, with both bug and security fixes. It’s a safe bet to base your work on it
    if you’re doing this for production purposes, once you’re done learning (if there’s
    such a thing as being done learning). You can learn more about Unity LTS releases
    in the [Unity documentation](https://oreil.ly/1mtHI).
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然 Unity ML-Agents Toolkit 可与任何新于 2018.4 的 Unity 版本配合使用，但我们建议您安装最新的 2021 版本
    Unity。您可能会找到 Unity 的 2021 LTS 版本。LTS 代表长期支持，这是 Unity 团队在指定时间内维护的版本，包括 bug 和安全修复。如果您正在进行生产工作，并且已经学完了学习（如果有“学完学习”这样的事情的话），以它为基础是一个安全的选择。您可以在[Unity文档](https://oreil.ly/1mtHI)中了解更多关于
    Unity LTS 版本的信息。
- en: '*Install Python.* You’ll need to install a version of Python newer than or
    equal to Python 3.6.1 and older than (but excluding) Python 3.8\. If you don’t
    have a preference or you don’t have an existing Python environment, we recommend
    installing Python 3.7.8\. As we discussed in [Chapter 1](ch01.html#introducing-the-tools),
    much of the Unity ML-Agents Toolkit depends on Python.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*安装 Python.* 您需要安装一个新于或等于 Python 3.6.1 且旧于（但不包括）Python 3.8 的版本。如果您没有偏好或者没有现有的
    Python 环境，我们建议安装 Python 3.7.8。正如我们在[第一章](ch01.html#introducing-the-tools)中讨论的，Unity
    ML-Agents Toolkit 的大部分依赖于 Python。'
- en: Warning
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: At the time of this writing, the Unity ML-Agents Toolkit does not support Python
    3.8\. You’ll need to use Python 3.6.1 or newer, or any version of Python 3.7\.
    If you are using Windows, you’ll also need the x86-64 version of Python, as the
    toolkit is not compatible with the x86 version. If you’re running on a fancy Apple
    Silicon macOS device, you might want to run Python under Rosetta 2, but it also
    might work fine with Apple Silicon builds of Python. Things are changing fast
    in that respect. Check the book’s [website](https://oreil.ly/1efRA) for the latest
    on Apple Silicon and Unity for simulation.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在撰写本文时，Unity ML-Agents Toolkit 不支持 Python 3.8。您需要使用 Python 3.6.1 或更新版本，或任何 Python
    3.7 版本。如果您使用的是 Windows，则还需要 Python 的 x86-64 版本，因为该工具包不兼容 x86 版本。如果您在运行花哨的 Apple
    Silicon macOS 设备，则可能需要在 Rosetta 2 下运行 Python，但在 Apple Silicon 版本的 Python 下也可能正常工作。这方面的情况正在快速变化。查看本书的[网站](https://oreil.ly/1efRA)获取关于
    Apple Silicon 和 Unity 仿真的最新信息。
- en: To install Python, head to [the Python downloads page](https://oreil.ly/5dPG8)
    and grab the installer for your particular operating system. If you don’t want
    to install Python directly in this manner, it’s fine to use your operating system’s
    package manager (if it has one), or a comprehensive Python environment (we quite
    like Anaconda), as long as the *version* of Python you install meets the version
    and architecture versions that we noted a moment ago.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要安装Python，请访问[Python下载页面](https://oreil.ly/5dPG8)，获取适合你操作系统的安装程序。如果你不想直接以这种方式安装Python，也可以使用操作系统的包管理器（如果有的话），或者一个全面的Python环境（我们非常喜欢Anaconda），只要你安装的Python版本符合我们刚才提到的版本和架构版本。
- en: Note
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You’ll also need to make sure your Python installation comes with `pip` (or
    `pip3`), the Python package manager. The [Python documentation](https://oreil.ly/T8c4j)
    may help with this, if you’re having issues.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还需要确保你的Python安装包含`pip`（或`pip3`），即Python包管理器。如果安装出现问题，可以参考[Python文档](https://oreil.ly/T8c4j)。
- en: '*We strongly recommend that you use a virtual environment (“venv”) for your
    Unity ML-Agents work.* To learn more about creating a venv, you can follow the
    instructions in the [Python documentation](https://oreil.ly/qQPZj), or follow
    the basic steps we outline next.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*我们强烈建议你为Unity ML-Agents工作使用虚拟环境（“venv”）。* 要了解更多关于创建venv的信息，可以参考[Python文档](https://oreil.ly/qQPZj)，或者按照我们下面概述的基本步骤进行操作。'
- en: Note
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you have a preferred way of setting Python up on your machine, just do that.
    We’re not here to tell you how to live your life. If you’re comfortable with Python,
    then realistically all you need to do is make sure you obey the version restrictions
    of ML-Agents, get the right package installed, and have it available to run when
    you need it. Python is famously not fragile when it comes to multiple versions,
    right? (Authors’ note: we’re Australians, so this should be read with an Aussie
    accent, and dripping with respectful sarcasm.)'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你有在你的机器上设置Python的首选方式，就用那个。我们不会告诉你如何生活。如果你对Python感到满意，那么实际上你只需要确保遵守ML-Agents的版本限制，安装正确的包，并在需要时运行它即可。在涉及多个版本时，Python是出了名的不脆弱，对吧？（作者注：我们是澳大利亚人，所以这段话应该带有澳式口音，充满尊重的讽刺。）
- en: 'You can create a virtual environment like this:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以像这样创建一个虚拟环境：
- en: '[PRE0]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tip
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We recommend naming it `UnityMLVEnv` or something similar. But the name is your
    choice.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们建议将其命名为`UnityMLVEnv`或类似的名称。但是名称由你决定。
- en: 'And you can activate it like this:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以像这样激活它：
- en: '[PRE1]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Install the Python `mlagents` package.* Once you’ve got Python and a virtual
    environment for Unity ML-Agents to live in up and running, install the Python
    `mlagents` package by issuing the following command from inside the `venv`:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*安装Python `mlagents`包。* 一旦你安装了Python，并设置了Unity ML-Agents的虚拟环境，可以通过以下命令安装Python
    `mlagents`包：'
- en: '[PRE2]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Asking `pip`, the Python Package Manager, to fetch and install `mlagents` will
    also install all the dependencies for `mlagents`, which includes TensorFlow.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 询问`pip`，Python包管理器，获取并安装`mlagents`也将安装所有`mlagents`所需的依赖项，包括TensorFlow。
- en: '*Clone or download the Unity ML-Agents Toolkit GitHub repository.* You can
    clone the repository by issuing the following command:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*克隆或下载Unity ML-Agents Toolkit GitHub仓库。* 你可以使用以下命令克隆该仓库：'
- en: '[PRE3]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We largely assume that you’re an experienced user of your chosen operating system
    for development purposes. If you need guidance on accomplishing any of these setup
    steps, don’t despair! We recommend you [review the documentation](https://oreil.ly/xFL3F)
    to get up to speed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要假设你是你选择的操作系统的开发经验丰富的用户。如果你需要完成任何这些设置步骤的指导，请不要绝望！我们建议你查阅文档，以便快速上手。
- en: With the preceding four steps completed, you’ve completed the Python-related
    setup requirements. Next we’ll look at the Unity requirements.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成前面的四个步骤后，你已经完成了与Python相关的设置要求。接下来我们将看看Unity的要求。
- en: Creating the Unity Project
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建Unity项目
- en: 'The first step for creating a simulation environment is to create a brand-new
    Unity project. The Unity project is much like any other development project: it’s
    a collection of files, folders, and *things* that Unity declares to be a project.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 创建模拟环境的第一步是创建一个全新的Unity项目。Unity项目与任何其他开发项目非常相似：它是Unity声明为项目的文件、文件夹和*东西*的集合。
- en: Note
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Our screenshots will be from macOS because it’s the primary environment we use
    on a daily basis. All the tools that we’ll be using in this book work on macOS,
    on Windows, and in Linux, so feel free to use your preferred operating system.
    We’ll do our best to point out any glaring differences between macOS and the other
    operating systems as we go (but there aren’t many, as far as what we’re doing
    is concerned). We’ve tested all the activities on all the supported platforms,
    and everything worked (on our machines).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的屏幕截图将来自 macOS，因为这是我们日常使用的主要环境。本书中使用的所有工具在 macOS、Windows 和 Linux 上均可使用，所以请使用您偏好的操作系统。我们会尽量在操作系统之间指出显著差异（但在我们所做的事情方面，并没有太多的差异）。我们在所有支持的平台上测试了所有活动，一切都在我们的机器上运行良好。
- en: 'To create a project, make sure you’ve completed all the setup steps, and then
    do the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建项目，请确保您已完成所有设置步骤，然后执行以下操作：
- en: Open the Unity Hub and create a new 3D project. As shown in [Figure 2-2](#fig:first_unity_proj),
    we’ll name ours “BallWorld,” but feel free to get creative.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Unity Hub 并创建一个新的 3D 项目。如图 [Figure 2-2](#fig:first_unity_proj) 所示，我们将命名为“BallWorld”，但请随意发挥创意。
- en: '![psml 0202](assets/psml_0202.png)'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0202](assets/psml_0202.png)'
- en: Figure 2-2\. Creating the Unity project for our new environment
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 2-2\. 为我们的新环境创建 Unity 项目
- en: Select the Window menu → Package Manager, and use the [Unity Package Manager](https://oreil.ly/VTwnY)
    to install the ML-Agents Toolkit package (`com.unity.ml-agents`), as shown in
    [Figure 2-3](#fig:upm_mltoolkit).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择菜单 Window → Package Manager，并使用 [Unity Package Manager](https://oreil.ly/VTwnY)
    安装 ML-Agents Toolkit 包 (`com.unity.ml-agents`)，如图 [Figure 2-3](#fig:upm_mltoolkit)
    所示。
- en: '![psml 0203](assets/psml_0203.png)'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0203](assets/psml_0203.png)'
- en: Figure 2-3\. Installing the Unity ML-Agents Toolkit package
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 2-3\. 安装 Unity ML-Agents Toolkit 包
- en: Note
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Like everything these days, Unity has a package manager. It’s actually pretty
    good, but also like everything these days, it can sometimes be a little fragile.
    If you’re having problems, restart Unity and try again, or try the manual installation
    process explained in the next note.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 就像现在的所有东西一样，Unity 有一个包管理器。它实际上相当不错，但也像现在的所有东西一样，有时会有些脆弱。如果遇到问题，请重新启动 Unity 并重试，或者尝试下一个注意事项中解释的手动安装过程。
- en: It might take a few moments for the package to download and install. Once it’s
    finished downloading, you’ll see Unity import it, as shown in [Figure 2-4](#fig:ch02_packageimport).
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包下载和安装可能需要一些时间。下载完成后，您将看到 Unity 正在导入它，如图 [Figure 2-4](#fig:ch02_packageimport)
    所示。
- en: '![psml 0204](assets/psml_0204.png)'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0204](assets/psml_0204.png)'
- en: Figure 2-4\. The ML-Agents package being imported by Unity
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 2-4\. Unity 导入 ML-Agents 包
- en: 'If you want to install the package manually, or if for some reason (such as
    corporate policy) the Unity Package Manager isn’t an option, you can:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您想手动安装包，或者由于某些原因（如公司政策）无法使用 Unity 包管理器，则可以：
- en: Open the package manager by selecting the menu named Window → Package Manager.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择菜单 Window → Package Manager 打开包管理器。
- en: Click the + button in the package manager and select “Add package from disk…”.
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在包管理器中点击 + 按钮，并选择“从磁盘添加包…”。
- en: Find the com.unity.ml-agents folder within the copy of the Unity ML-Agents Toolkit
    you cloned in [“Setting Up”](#ch02-setup).
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您克隆的 Unity ML-Agents Toolkit 副本中找到 com.unity.ml-agents 文件夹，详见 [“设置”](#ch02-setup)。
- en: Select the *package.json* file.
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 *package.json* 文件。
- en: For more information, check the [Unity documentation](https://oreil.ly/XArSA)
    on installing a package from a local folder.
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请查看[Unity文档](https://oreil.ly/XArSA)，了解如何从本地文件夹安装包。
- en: Verify that you have an ML Agents folder under Packages in the Project view,
    as shown in [Figure 2-5](#fig:mlagentsinproject).
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认在项目视图的 Packages 下有一个 ML Agents 文件夹，如图 [Figure 2-5](#fig:mlagentsinproject)
    所示。
- en: '![psml 0205](assets/psml_0205.png)'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0205](assets/psml_0205.png)'
- en: Figure 2-5\. The project is ready to go if you can see the ML Agents folder
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 2-5\. 如果您能看到 ML Agents 文件夹，则项目已准备就绪
- en: Your project is ready to go. We recommend you push it to some sort of source
    control at this point, or duplicate it as a fresh starting point for all your
    ML-Agents work in this book.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您的项目已准备就绪。我们建议您在这一点上将其推送到某种源代码控制，或者将其复制为本书中所有 ML-Agents 工作的新起点。
- en: Tip
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You’ll need this basic Unity-powered starting point every time you want to create
    a fresh simulation environment in Unity. In other words, you’ll want a fresh Unity
    project open with the Unity package installed, and you’ll need to do that for
    each ML-Agents project you work on. The Python setup is effectively a one-off
    and sets up the Python components available on your machine, but the Unity setup
    needs to happen for each project you want to work with.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你想在 Unity 中创建一个新的模拟环境时，你都需要这个基本的 Unity 引擎起始点。换句话说，你需要打开一个带有安装了 Unity 包的新 Unity
    项目，并且每个你想要工作的 ML-Agents 项目都需要进行这样的设置。Python 设置实际上只需要一次，它设置了你机器上可用的 Python 组件，但是
    Unity 设置需要为你想要工作的每个项目都进行。
- en: Packages All the Way Down
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 包罗万象
- en: It might be slightly confusing which component is which, as there are many things
    with similar names. Let’s unpack that a little bit before we move on.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会令人稍感困惑，因为有许多名称相似的内容。在我们继续之前，让我们稍微解释一下。
- en: 'There are effectively three sets of things in play here that you could conceivably
    refer to as the “ML-Agents package.” They are:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里实际上有三组事物，你可以把它们都称为“ML-Agents 包”：
- en: The `mlagents` *Python package*. This is a Python package and we installed it
    earlier using the Python package manager `pip3`. It’s part of the Unity ML-Agents
    Toolkit and it’s distributed via the [Python Package Index](https://pypi.org/project/mlagents)
    (which is how you installed it using `pip3` earlier). When we’re talking about
    this *Python package* we’ll refer to it as `mlagents`.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 包 `mlagents`。这是一个 Python 包，我们之前使用 Python 包管理器 `pip3` 安装了它。它是 Unity ML-Agents
    Toolkit 的一部分，并通过 [Python 包索引](https://pypi.org/project/mlagents) 分发（这就是你之前使用 `pip3`
    安装它的方式）。当我们谈论这个 Python 包时，我们会称之为 `mlagents`。
- en: The `com.unity.ml-agents` *Unity package*. You install this package via the
    Unity Package Manager, as we did in [“Creating the Unity Project”](#creating-unity-project).
    It’s also part of the Unity ML-Agents Toolkit and it’s distributed via the Unity
    Package Manager (which can install it into a Unity project via either the automated
    process, where you choose it from the list of packages using the Unity Package
    Manager interface, or manually, where you give the Unity Package Manager a Git
    URL with a correctly formed Unity package inside it). When we’re talking about
    this *Unity package* we’ll refer to it variously as `com.unity.ml-agents`, ML-Agents,
    the Unity ML-Agents package, or the ML-Agents Toolkit. This is the package that
    allows you to use features of ML-Agents inside the Unity Editor and in the C#
    code you’ll write in Unity.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unity 包 `com.unity.ml-agents`。你可以通过 Unity 包管理器安装此包，就像我们在[“创建 Unity 项目”](#creating-unity-project)中所做的那样。它还是
    Unity ML-Agents Toolkit 的一部分，并通过 Unity 包管理器分发（可以通过自动化过程将其安装到 Unity 项目中，其中你可以从
    Unity 包管理器界面的列表中选择它，或者手动方式，其中你可以给 Unity 包管理器一个正确形式的 Unity 包的 Git URL）。当我们谈论这个
    Unity 包时，我们会将其称为 `com.unity.ml-agents`、ML-Agents、Unity ML-Agents 包或 ML-Agents
    Toolkit。这个包允许你在 Unity 编辑器内使用 ML-Agents 的功能，并在你在 Unity 中编写的 C# 代码中使用。
- en: Note
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You install the `mlagents` Python package, install the Unity ML-Agents package,
    and clone a local copy of the ML-Agents repository because each one of these installs
    a different set of things. The Python package gives you the ability to run certain
    commands in your terminal that allow you to train agents, the cloned Git repository
    gives you a useful collection of sample code and resources, and the Unity package
    gives you the pieces you need to make agents and other ML-Agents (or related)
    components inside Unity scenes. Each is useful.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你需要安装`mlagents` Python 包、安装 Unity ML-Agents 包，并克隆本地 ML-Agents 仓库的一个副本，因为每个安装都提供了不同的功能集。Python
    包使你能够在终端中运行特定命令来训练代理，克隆的 Git 仓库提供了一些有用的示例代码和资源，Unity 包提供了在 Unity 场景中创建代理和其他 ML-Agents（或相关）组件所需的组件。每个都非常有用。
- en: The cloned copy of the *Unity ML-Agents Toolkit GitHub repository*. This is
    a local copy of the contents of the GitHub repository that we suggest you clone
    or download as it contains useful documentation, sample files, and such. When
    we refer to this we’ll call it “your local copy of the ML-Agents repository” or
    something similar.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克隆的 *Unity ML-Agents Toolkit GitHub 仓库* 的副本。这是 GitHub 仓库内容的本地副本，我们建议你克隆或下载它，因为它包含了有用的文档、示例文件等。当我们提到这个时，我们会称其为“你的本地
    ML-Agents 仓库副本”或类似的名称。
- en: Warning
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'If you’re running Apple Silicon, you’re fine to use the Apple Silicon builds
    of Unity, but you’ll need to use Python in an Intel environment. The easiest and
    laziest way to do this is to run your Terminal app under Rosetta, but there are
    other ways too. It’s beyond the scope of this book to discuss Python on Apple
    Silicon, but if you’re in this position, there are plenty of resources online.
    Just remember: on macOS, Unity can be either platform, but ML-Agents and Python
    (for this use case) must be Intel.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是 Apple Silicon，可以使用 Apple Silicon 版本的 Unity，但在 Intel 环境下需要使用 Python。最简单和最懒的方法是在
    Rosetta 下运行您的终端应用程序，但也有其他方法。讨论 Python 在 Apple Silicon 上的使用超出了本书的范围，但如果您处于这种情况下，网上有很多资源。请记住：在
    macOS 上，Unity 可以是任何平台，但 ML-Agents 和 Python（对于这种用例）必须是 Intel。
- en: The Environment
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境
- en: With the fundamental setup out of the way, it’s time to actually start building
    your simulation environment in the Unity Editor.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基本设置完成后，现在可以在 Unity 编辑器中开始构建您的模拟环境了。
- en: 'This involves creating a scene in Unity that will serve as the simulation environment.
    The simulation environment we’re building has the following requirements:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及在 Unity 中创建一个作为模拟环境的场景。我们正在构建的模拟环境具有以下要求：
- en: A *floor* for the agent to move around on
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*地板*，供代理在其上移动
- en: A *target* for the agent to seek
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个代理要寻找的*目标*
- en: We’ll also need to create the agent itself, but we’ll cover that in [“The Agent”](#making-the-agent).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要创建代理本身，但我们将在[“代理”](#making-the-agent)中进行讨论。
- en: The Floor
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 地板
- en: The floor is where the agent will move around. It exists because our agent exists
    within the physics simulation engine of Unity, and if there were no floor, it
    would fall to the ground.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 地板是代理会移动的地方。它存在是因为我们的代理存在于 Unity 的物理模拟引擎内，如果没有地板，它将会掉落到地面上。
- en: Tip
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: There’s nothing special about the concept of a floor in Unity. We’ve just chosen
    to use a plane for the floor, but we could use any object that exists in the physics
    system and is large enough to *be* a floor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Unity 中，地板概念并不特殊。我们只是选择使用平面作为地板，但我们可以使用任何存在于物理系统中并且足够大的物体作为地板。
- en: 'To create the floor, make sure you have a Unity scene open and then follow
    these steps:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建地板，请确保您已打开一个 Unity 场景，然后按照以下步骤操作：
- en: Open the GameObject menu → 3D Object → Plane. Click on the new plane you created
    in the Hierarchy view, and using the Inspector view, set its name to “Floor” or
    something similar, as shown in [Figure 2-6](#fig:floor1). This is our floor.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 GameObject 菜单 → 3D Object → Plane。点击在 Hierarchy 视图中创建的新平面，在 Inspector 视图中，设置其名称为“Floor”或类似的名称，如
    [图 2-6](#fig:floor1) 所示。这是我们的地板。
- en: While the floor is selected, use the Transform Inspector to set its position
    to `(0, 0, 0)`, its rotation to `(0, 0, 0)`, and its scale to `(1, 1, 1)`, as
    shown in [Figure 2-7](#fig:floor2). We do this to ensure that the floor is in
    a sensible position and orientation, and is of sufficient scale. These values
    will probably already be the defaults.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当地板被选中时，使用 Transform Inspector 将其位置设置为`(0, 0, 0)`，旋转设置为`(0, 0, 0)`，缩放设置为`(1,
    1, 1)`，如 [图 2-7](#fig:floor2) 所示。我们这样做是为了确保地板处于合理的位置和方向，并且具有足够的比例。这些值可能已经是默认值了。
- en: '![psml 0206](assets/psml_0206.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0206](assets/psml_0206.png)'
- en: Figure 2-6\. Creating and naming the floor
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 创建并命名地板
- en: '![psml 0207](assets/psml_0207.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0207](assets/psml_0207.png)'
- en: Figure 2-7\. Setting the transforms of the floor
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 设置地板的变换
- en: Tip
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you need some help creating a Unity scene, check out the [Unity documentation](https://oreil.ly/yrzzq).
    The gist is that a scene contains objects, and you can make a new one using the
    Assets → Create → Scene menu.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要帮助创建 Unity 场景，请查看[Unity 文档](https://oreil.ly/yrzzq)。总体来说，一个场景包含对象，您可以使用
    Assets → Create → Scene 菜单创建新的场景。
- en: That’s all we need to do for the floor. We don’t need to create an empty void,
    as Unity conveniently supplies one for us (i.e., every scene is just a void, except
    for the things you add to it). Don’t forget to save the scene you’re working in
    using the File menu.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于地板，我们不需要创建空虚空，因为 Unity 已经为我们提供了一个（即每个场景只是一个虚空，除非你添加了物体）。不要忘记使用文件菜单保存您正在工作的场景。
- en: Tip
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can rename the default scene to something other than “SampleScene” by locating
    it in the Project view, right-clicking on it, and choosing Rename.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在 Project 视图中找到默认场景，右键单击它，然后选择重命名来将其名称改为不同于“SampleScene”的名称。
- en: The Target
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标
- en: The target is the *thing* that our agent will seek on the floor. Again, Unity
    has no special concept of a target; we’re just calling the cube we’re making a
    target, which is contextually relevant because of the plans we have for this scene
    and simulation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是我们的代理将在地板上寻找的*东西*。再次强调，Unity 没有特殊的目标概念；我们只是称呼我们正在制作的立方体为目标，因为这在场景和模拟计划中是相关的。
- en: 'To create the target in your scene, follow these steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要在你的场景中创建目标，请按照以下步骤操作：
- en: Open the GameObject menu → 3D Object → Cube. As with the floor, click on the
    new cube in the Hierarchy view, and use the Inspector to set its name to “Target”
    or something similar, as shown in [Figure 2-8](#fig:target1). This is going to
    be the target, and as you can see, it’s a very compelling target! It will probably
    be partially embedded in the floor plane.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开游戏对象菜单 → 3D 对象 → 立方体。与地板类似，点击层级视图中的新立方体，并使用检视器将其命名为“目标”或类似名称，如 [图 2-8](#fig:target1)
    所示。这将成为目标，正如你所见，它是一个非常引人注目的目标！它可能部分嵌入在地板平面中。
- en: '![psml 0208](assets/psml_0208.png)'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0208](assets/psml_0208.png)'
- en: Figure 2-8\. Creating and naming the target
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-8\. 创建并命名目标
- en: While the target is selected, as we did with the floor, use the Transform Inspector
    to set its position, rotation, and scale. The values in this case should be something
    like `(3, 0.5, 3)`, `(0, 0, 0)`, and `(1, 1, 1)`, respectively, as shown in [Figure 2-9](#fig:ch02_targettransform).
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当选择目标时，如同对地板操作一样，使用变换检视器设置其位置、旋转和缩放。在本例中，值应该类似于 `(3, 0.5, 3)`、`(0, 0, 0)` 和
    `(1, 1, 1)`，如 [图 2-9](#fig:ch02_targettransform) 所示。
- en: '![psml 0209](assets/psml_0209.png)'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0209](assets/psml_0209.png)'
- en: Figure 2-9\. The transform for the target and the environment so far
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-9\. 至今为止的目标和环境变换
- en: That’s all we need to do for the target. At this point, your environment should
    look similar to [Figure 2-9](#fig:ch02_targettransform). Don’t forget to save
    your scene again.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这就是我们为目标所需做的一切。此时，你的环境应该与 [图 2-9](#fig:ch02_targettransform) 类似。不要忘记再次保存你的场景。
- en: The Agent
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理
- en: The next step is to create the agent. The agent is the *thing* that moves around
    in the environment. It’s going to be a sphere that rolls around and seeks the
    target.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建代理。代理是在环境中移动的*东西*。它将是一个能够在周围滚动并寻找目标的球体。
- en: 'Follow these steps to create the agent:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建代理：
- en: Open the GameObject menu → 3D Object → Sphere. Name the sphere “Agent” or something
    similar.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开游戏对象菜单 → 3D 对象 → 球体。将球体命名为“代理”或类似名称。
- en: Use the Inspector and set the transform’s position, rotation, and scale to `(0,
    0.5, 0)`, `(0, 0, 0)`, and `(1, 1, 1)`, respectively. Your agent should be positioned
    on the floor similarly to ours, as shown in [Figure 2-10](#fig:agent2).
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用检视器设置变换的位置、旋转和缩放为 `(0, 0.5, 0)`、`(0, 0, 0)` 和 `(1, 1, 1)`。你的代理应该与地板上的位置类似，如
    [图 2-10](#fig:agent2) 所示。
- en: '![psml 0210](assets/psml_0210.png)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0210](assets/psml_0210.png)'
- en: Figure 2-10\. The agent in the scene
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-10\. 场景中的代理
- en: Use the Add Component button at the bottom of the agent’s Inspector, shown in
    [Figure 2-11](#fig:ch02_addcomponent), and add a Rigidbody component to the agent.
    You don’t need to change anything on the Rigidbody component.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代理的检视器底部使用“添加组件”按钮，如 [图 2-11](#fig:ch02_addcomponent) 所示，并向代理添加一个刚体组件。你不需要在刚体组件上做任何更改。
- en: '![psml 0211](assets/psml_0211.png)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0211](assets/psml_0211.png)'
- en: Figure 2-11\. The Add Component button
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-11\. 添加组件按钮
- en: 'That’s everything for the physical side of the agent. Next, we need to give
    the agent some logic and facilitate its connection to the machine learning side
    of things:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是代理的物理方面的全部内容。接下来，我们需要给代理添加一些逻辑，并促使其连接到机器学习的一面：
- en: Select the agent, and add a new Script component via the Inspector’s Add Component
    button, as shown in [Figure 2-12](#fig:agent3). Name the script “BallAgent,” and
    click “Create and Add.”
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择代理，并通过检视器的“添加组件”按钮添加一个新的脚本组件，如 [图 2-12](#fig:agent3) 所示。将脚本命名为“BallAgent”，然后点击“创建并添加”。
- en: '![psml 0212](assets/psml_0212.png)'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0212](assets/psml_0212.png)'
- en: Figure 2-12\. Creating the script for the agent
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-12\. 为代理创建脚本
- en: You should see a new Script component attached to your agent in its Inspector,
    as shown in [Figure 2-13](#fig:agent4).
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该在代理的检视器中看到一个新的脚本组件附加在上面，如 [图 2-13](#fig:agent4) 所示。
- en: '![psml 0213](assets/psml_0213.png)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0213](assets/psml_0213.png)'
- en: Figure 2-13\. The script is attached to the agent
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-13\. 脚本已附加到代理
- en: Double-click on the BallAgent script in the Unity Project view to open the script
    in your code editor.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Unity项目视图中双击球代理脚本以在代码编辑器中打开。
- en: Tip
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Basically, an agent that is built using Unity ML-Agents needs to have a script
    attached to it that tells Unity that its parent class is `Agent`, which is a class
    supplied by the ML-Agents Unity package. Being a child of `Agent` means we have
    to implement or override certain methods supplied by the Unity ML-Agents package.
    Those methods allow us to control and work with the agent for machine learning
    purposes.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基本上，使用Unity ML-Agents构建的代理需要附加一个告诉Unity其父类是`Agent`的脚本。`Agent`是ML-Agents Unity包提供的一个类。作为`Agent`的子类意味着我们必须实现或覆盖Unity
    ML-Agents包提供的某些方法。这些方法允许我们控制和处理代理以进行机器学习。
- en: 'Once the script is open, add the following code to the top, below the `using
    UnityEngine;` line:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦脚本打开，将以下代码添加到顶部，在`using UnityEngine;`行下面：
- en: '[PRE4]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These lines import the appropriate bits of the ML-Agents Unity package that
    we imported, and allow us to use them in code.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些行导入了我们导入的ML-Agents Unity包的适当部分，并允许我们在代码中使用它们。
- en: Find the `Update()` method and delete it. We won’t be using it for this simulation.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到`Update()`方法并删除它。在这个模拟中我们不会使用它。
- en: 'Find the definition of the `BallAgent` class and change it from this:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到`BallAgent`类的定义，并将其从这个状态更改为：
- en: '[PRE5]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'to this:'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到这里：
- en: '[PRE6]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This makes our new class, `BallAgent`, a child of `Agent` (which comes from
    ML-Agents), instead of the default `MonoBehaviour` (which is the parent of most
    non-ML Unity objects).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们的新类`BallAgent`成为`Agent`的子类（来自ML-Agents），而不是默认的`MonoBehaviour`（大多数非ML Unity对象的父类）。
- en: Those are the basics of setting up an agent in a simulation environment with
    Unity. Next we need to add some logic that provides the ability for our agent
    to roll about the place (well, the floor). Before we do that, we’re going to group
    the three objects we’ve made so far.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在Unity中设置模拟环境中代理的基础知识。接下来我们需要添加一些逻辑，使我们的代理能够在场景中移动（嗯，地板上）。在这之前，我们将对到目前为止制作的三个对象进行分组。
- en: 'Save the code in your code editor, and then switch back to the Unity Editor
    and, in your scene, do the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存在代码编辑器中，然后切换回Unity编辑器，在场景中执行以下操作：
- en: Open the GameObject menu → Create Empty to create an empty GameObject.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开GameObject菜单 → 创建空物体以创建一个空的GameObject。
- en: Select the empty GameObject and use the Inspector to name it “Training Area.”
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择空GameObject并使用检视面板将其命名为“训练区域”。
- en: Tip
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When you’re renaming something, instead of using the Inspector view, you can
    simply select it in the Hierarchy view and press your Return/Enter key to enter
    edit mode for the name in the Hierarchy view. Press Return/Enter again to save
    your new name.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你重命名某物件时，可以在层次视图中选择它，而不是使用检视面板，然后按下回车键/Enter键进入编辑模式。再次按下回车键/Enter键保存新名称。
- en: Set the Training Area GameObject’s position, rotation, and scale to `(0, 0,
    0)`, `(0, 0, 0)`, and `(1, 1, 1)`, respectively.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练区域的GameObject的位置、旋转和比例分别设置为`(0, 0, 0)`、`(0, 0, 0)`和`(1, 1, 1)`。
- en: In the Hierarchy, drag the floor, target, and agent into the Training Area.
    Your Hierarchy should now look like [Figure 2-14](#fig:ch02_hierarchysetup). Nothing
    in the scene should change position when you do this.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层次视图中，将地板、目标和代理拖放到训练区域中。此时你的层次视图应该像[图 2-14](#fig:ch02_hierarchysetup)一样。这样做时，场景中的任何位置都不应该改变。
- en: '![psml 0214](assets/psml_0214.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0214](assets/psml_0214.png)'
- en: Figure 2-14\. The Training Area
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-14\. 训练区域
- en: Don’t forget to save your scene when you’re done.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后别忘了保存场景。
- en: Starting and Stopping the Agent
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动和停止代理
- en: We’re going to train this agent using reinforcement learning. Training for reinforcement
    learning inside our Unity environment involves running through many *episodes*
    during which the agent attempts to reach the cube. In each episode, if the agent
    does something we want it to do, we want to reinforce that behavior by rewarding
    it, and vice versa if it does something we do not want it to do.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用强化学习来训练这个代理。在Unity环境中进行强化学习训练涉及运行许多*episode*，在每个episode中，代理试图达到立方体。在每个episode中，如果代理做了我们希望它做的事情，我们希望通过奖励来强化这种行为，反之亦然。
- en: An episode runs until the agent fails the task—in this case, that would be either
    falling off the floor into the void or running out of a predefined amount of time—or
    succeeds the task by reaching the target.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一个episode运行直到代理失败任务——在本例中，可能是掉落到虚空中或者在预定时间内耗尽——或者成功完成任务，即到达目标。
- en: At the beginning of each episode, a C# method on the `Agent` named `OnEp⁠i⁠s⁠o⁠d⁠e​B⁠e⁠g⁠i⁠n⁠()`
    is called to initialize the simulation environment for the new episode. This method
    sets up the environment for the episode. In most cases, you’ll be using this to
    randomize elements of the environment to facilitate the agent learning to succeed
    in the task under a range of conditions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个剧集的开始时，调用位于`Agent`上的C#方法`OnEp⁠i⁠s⁠o⁠d⁠e​B⁠e⁠g⁠i⁠n⁠()`来初始化新剧集的仿真环境。此方法设置剧集的环境。在大多数情况下，您将使用此方法随机化环境元素，以帮助代理在各种条件下学会成功完成任务。
- en: 'For our scenario, at the beginning of an episode the requirements are:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的场景，在一个剧集开始时，需求是：
- en: Make sure the ball agent is somewhere on the floor, and not falling into the
    abyss.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保球代理位于地板上某处，而不是掉进深渊。
- en: Move the target to a random location on the floor.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将目标移动到地板上的随机位置。
- en: To fulfill the first requirement, we need to have access to the agent’s Rigidbody
    component. The Rigidbody component is part of what allows Unity to simulate an
    object in its physics system.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足第一个要求，我们需要访问代理的Rigidbody组件。Rigidbody组件是Unity模拟对象在物理系统中所必需的一部分。
- en: Tip
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Learn more about the Unity Rigidbody component in the [Unity manual](https://oreil.ly/VqNjd).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Unity手册](https://oreil.ly/VqNjd)中了解更多关于Unity Rigidbody组件的信息。
- en: 'Open up the BallAgent script again. Since we need to reset the agent’s velocity
    (if it’s falling into the abyss) and also, eventually, move it around the floor,
    we need access to its `Rigidbody`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 再次打开BallAgent脚本。由于我们需要重置代理的速度（如果它掉进深渊）并且最终移动它到地板上，我们需要访问它的`Rigidbody`：
- en: The Rigidbody component we need here is on the same object this script will
    be attached to (unlike the transform of the target we needed a moment ago), so
    we can get a reference to it inside our script’s `Start()` method.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们这里需要的Rigidbody组件与此脚本将要附加到的相同对象上（不像刚才我们需要目标的变换），因此我们可以在脚本的`Start()`方法内获取对它的引用。
- en: 'First, we’ll need somewhere to store it when we get the reference. Add the
    following above the `Start()` method (inside the class but above, and outside
    of, the method):'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，当我们获取引用时，我们需要一个地方来存储它。在`Start()`方法之前（在类内但在方法之上和之外），添加以下内容：
- en: '[PRE7]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, inside the `Start()` method, add the following code to request a reference
    to the Rigidbody attached to the object that the script is attached to (and to
    store it in the `rigidBody` variable we created a moment ago):'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在`Start()`方法内部，添加以下代码来请求对附加到脚本的对象的Rigidbody的引用（并将其存储在我们之前创建的`rigidBody`变量中）：
- en: '[PRE8]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To fulfill the second requirement, we need to make sure the code can access
    the transform of the target so that we can move it to a new random location.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了满足第二个要求，我们需要确保代码可以访问目标的变换，以便我们可以将其移动到新的随机位置。
- en: Since this script is going to be attached to the agent, not the target, we’ll
    need to acquire the reference to the transform of the target.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于此脚本将附加到代理而不是目标，我们需要获取对目标变换的引用。
- en: 'To do this, inside the `BallAgent` class, before the `Start()` method, add
    a new `public` field of type `Transform`:'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为此，在`BallAgent`类的`Start()`方法之前，添加一个新的`public`类型为`Transform`的字段：
- en: '[PRE9]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '`public` fields in Unity components will get displayed by the Unity Editor
    in the Inspector. This means you can visually, or with drag-and-drop, choose which
    object is used. We didn’t need to do this with the `Rigidbody` earlier, because
    it doesn’t need to be exposed to the Unity Editor.'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Unity组件中的`public`字段将通过Unity编辑器显示在检查器中。这意味着您可以通过视觉或拖放选择要使用的对象。我们之前不需要对`Rigidbody`执行此操作，因为它不需要在Unity编辑器中公开。
- en: Save the script (and leave it open) and switch back to the Unity Editor. Find
    the script component attached to the agent, look for the newly created Target
    field in the Inspector, and select the small circular button next to it, as shown
    in [Figure 2-15](#fig:ch02_targetinspector).
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存脚本（并保持打开状态），然后切换回Unity编辑器。找到附加到代理的脚本组件，查找检查器中新创建的目标字段，并选择其旁边的小圆形按钮，如[图2-15](#fig:ch02_targetinspector)所示。
- en: '![psml 0215](assets/psml_0215.png)'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0215](assets/psml_0215.png)'
- en: Figure 2-15\. Changing the target in the script
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-15\. 在脚本中更改目标
- en: In the window that appears, double-click on the Target object, as shown in [Figure 2-16](#fig:ch02_selecttarget).
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在打开的窗口中，双击目标对象，如[图2-16](#fig:ch02_selecttarget)所示。
- en: '![psml 0216](assets/psml_0216.png)'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0216](assets/psml_0216.png)'
- en: Figure 2-16\. Choosing the target
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-16\. 选择目标
- en: Verify that the script component attached to the agent now shows the target’s
    transform in the Target field, as shown in [Figure 2-17](#fig:ch02_targetshowing).
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证已附加到代理的脚本组件现在在目标字段中显示目标的变换，如图 [2-17 图](#fig:ch02_targetshowing) 所示。
- en: Tip
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can also drag the Target object from the Hierarchy view into the slot in
    the Inspector, if you’d prefer.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您愿意，您还可以从 Hierarchy 视图将目标对象拖动到检查器中的插槽中。
- en: '![psml 0217](assets/psml_0217.png)'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0217](assets/psml_0217.png)'
- en: Figure 2-17\. Verifying the target’s transform is showing in the field
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-17\. 确认目标的变换在字段中显示
- en: 'Next, switch back to the script’s code and implement an empty `OnEp⁠i⁠s⁠o⁠d⁠e​B⁠e⁠g⁠i⁠n⁠()`
    method inside the class:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，切换回脚本代码，并在类内部实现一个空的 `OnEpisodeBegin()` 方法：
- en: '[PRE10]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Inside `OnEpisodeBegin()`, add the following code to check if the position
    of the agent’s `Rigidbody` is lower than the floor (which would mean the agent
    is falling) and, if it is, reset its momentum and move it back onto the floor:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `OnEpisodeBegin()` 内部，添加以下代码来检查代理的 `Rigidbody` 的位置是否低于地板（这意味着代理正在下落），如果是，则重置其动量并将其移回地板上：
- en: '[PRE11]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And finally, for the first requirement, add some code after the `if` statement
    to move the target to a new random position:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在 `if` 语句后添加一些代码，以将目标移动到新的随机位置：
- en: '[PRE12]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Don’t forget to save your code, just in case.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记保存您的代码，以防万一。
- en: Letting the Agent Observe the Environment
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让代理观察环境
- en: 'Next, we need to set up the agent to collect *observations* from the simulation
    environment. We’re going to pretend our agent can see its target so that it knows
    exactly where it is (its objective isn’t to figure out where the target is; it’s
    to *reach* the target): therefore, one of the observations it will have is the
    exact position of the target. This requires more coding.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置代理以从模拟环境中收集*观察*。我们将假装我们的代理可以看到其目标，以便准确知道其位置（我们的目标不是弄清楚目标在哪里；而是*到达*目标）：因此，它将有一个观察是目标位置的确切位置。这需要更多的编码。
- en: Observations are collected by adding *sensors* to our agent. Sensors can be
    added in code, or by attaching components to things in the Unity scene. For our
    first simulation, we’re going to do it all in code.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 观察是通过向我们的代理添加*传感器*来收集的。传感器可以通过代码添加，也可以通过将组件附加到 Unity 场景中的物体上添加。对于我们的第一个模拟，我们将完全通过代码完成。
- en: 'In our agent C# code, we need to do the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代理 C# 代码中，我们需要执行以下操作：
- en: 'Create an empty `CollectObservations()` method:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个空的 `CollectObservations()` 方法：
- en: '[PRE13]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, inside the method, add a sensor observation for the agent’s own position:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在方法内部，为代理自身位置添加传感器观察：
- en: '[PRE14]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We also need to add a sensor observation for the agent’s own `x` and `z` velocity
    (we don’t care about the `y` velocity, because the agent can’t move up or down):'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要为代理自身的 `x` 和 `z` 速度添加传感器观察（我们不关心 `y` 速度，因为代理不能上下移动）：
- en: '[PRE15]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And, finally,we need to add an observation for the target’s position:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要为目标位置添加一个观察：
- en: '[PRE16]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: That’s all for the observations!
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到这些即可！
- en: Letting the Agent Take Actions in the Environment
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让代理在环境中采取行动
- en: To achieve its goal of moving toward the target, the agent needs to be able
    to *move*.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现向目标移动的目标，代理需要能够*移动*。
- en: 'First, create an empty `OnActionReceived()` method:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个空的 `OnActionReceived()` 方法：
- en: '[PRE17]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, get access to the two continuous actions we need, one for `x` and one
    for `z`, allowing the ball to be controlled in all the directions in which it
    could roll:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，获取我们需要的两个连续动作的访问权限，一个用于 `x`，一个用于 `z`，允许球在可以滚动的所有方向上进行控制：
- en: '[PRE18]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create a zeroed `Vector3` to serve as a control signal:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个零向量 `Vector3` 作为控制信号：
- en: '[PRE19]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, change the `x` and `z` components of the control signal to take their
    values from the `X` and `Z` actions:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，更改控制信号的 `x` 和 `z` 分量，以从 `X` 和 `Z` 动作中获取它们的值：
- en: '[PRE20]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, using the `Rigidbody` we got a reference to earlier (the component
    attached to the agent), call the `AddForce()` function that’s available on a Unity
    `Rigidbody` to apply the relevant forces:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用我们之前获取的 `Rigidbody`（附加到代理的组件），调用 Unity `Rigidbody` 上可用的 `AddForce()` 函数来应用相关力量：
- en: '[PRE21]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: That’s it for now! The agent can now be controlled by the machine learning system.
    Don’t forget to save your code.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时就这样！现在代理可以由机器学习系统控制。不要忘记保存您的代码。
- en: Note
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The reason we create the `controlSignal` `Vector3` by using `Vector3.zero` initially
    is because we want the `y` component to be `0`. We could’ve achieved the same
    by creating an entirely empty `Vector3`, and then assigning `0` to `controlSignal.y`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初使用`Vector3.zero`创建`controlSignal` `Vector3`，因为我们希望`y`分量为`0`。我们可以通过创建一个完全空的`Vector3`，然后将`0`分配给`controlSignal.y`来实现相同的效果。
- en: Giving the Agent Rewards for Its Behavior
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为其行为给予奖励
- en: A fundamental component of reinforcement learning, as we mentioned in [Chapter 1](ch01.html#introducing-the-tools),
    is *rewards*. Reinforcement learning requires the reward signals in order to guide
    the agent to the optimal policy—that is, to do what we want it to do, or as close
    as possible to that. The *reinforcement* of reinforcement learning occurs by using
    reward signals to guide the agent toward the desired behavior (i.e., the optimal
    policy).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第 1 章](ch01.html#introducing-the-tools)中提到的那样，*奖励*是强化学习的基本组成部分。强化学习需要奖励信号来引导代理执行最优策略，即我们希望它执行或尽可能接近的操作。强化学习的*强化*是通过使用奖励信号来引导代理朝着期望的行为（即最优策略）的方向发生的。
- en: 'Inside your `OnActionReceived` method, after the existing code you just wrote,
    do the following:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的`OnActionReceived`方法中，在您刚刚编写的现有代码之后，执行以下操作：
- en: 'Store the distance to the target:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储到目标的距离：
- en: '[PRE22]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Check if the distance to the target is close enough that you have reached the
    target, and if it is, assign a reward of 1.0:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查到目标的距离是否足够接近，以确认是否已到达目标位置，如果是，则分配奖励值为 1.0：
- en: '[PRE23]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After assigning the reward, *inside* the `if` statement, because the target
    was reached, call `EndEpisode()` to finish the current training episode:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配奖励后，在`if`语句*内*，因为已达到目标，请调用`EndEpisode()`来完成当前的训练回合：
- en: '[PRE24]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now check if the agent has fallen off the platform and, if it has, end the
    episode then as well (no reward applies in this case):'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在检查代理是否已从平台上掉落，如果是，则同样结束此次训练回合（此情况下不适用奖励）：
- en: '[PRE25]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: With that done, you’ll want to save your code and return to the Unity Editor.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，您将希望保存代码并返回Unity编辑器。
- en: Finishing Touches for the Agent
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理的最后一步
- en: An agent doesn’t just require a script extending `Agent`; it also requires a
    few support scripts and settings in the Unity Editor. The ML-Agents package that
    we installed in Unity earlier brought along the scripts we need.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 一个代理不仅需要扩展`Agent`脚本；还需要在Unity编辑器中进行一些支持脚本和设置。我们之前在Unity中安装的ML-Agents包已经带来了所需的脚本。
- en: 'To add them to your agent, in the Unity Editor with your scene open, do the
    following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要将它们添加到您的代理中，在Unity编辑器中打开您的场景，执行以下操作：
- en: Select the agent in the Hierarchy, and click the Add Component button at the
    bottom of its Inspector.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Hierarchy中选择代理，然后在其检视器底部点击“添加组件”按钮。
- en: Search for, and add, a Decision Requester component, as shown in [Figure 2-18](#fig:ch02_adddecisionrequester).
    Verify that it’s added correctly by looking for the component in the agent’s Inspector,
    as shown in [Figure 2-19](#fig:ch02_adddecisionrequester2).
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[图 2-18](#fig:ch02_adddecisionrequester)中查找并添加决策请求组件。通过查看代理的检视器，验证组件是否正确添加，如[图 2-19](#fig:ch02_adddecisionrequester2)所示。
- en: '![psml 0218](assets/psml_0218.png)'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0218](assets/psml_0218.png)'
- en: Figure 2-18\. Adding a Decision Requester
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-18\. 添加决策请求器
- en: '![psml 0219](assets/psml_0219.png)'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0219](assets/psml_0219.png)'
- en: Figure 2-19\. The Decision Requester component is visible in the agent’s Inspector
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-19\. 决策请求器组件在代理的检视器中可见
- en: Use the slider to change the Decision Period to `10`.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用滑块将决策周期更改为`10`。
- en: Use the Add Component button again and add a Behavior Parameters component to
    the agent.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用“添加组件”按钮，并向代理添加一个行为参数组件。
- en: Verify that the Behavior Parameters component was added successfully, and update
    Behavior Name to “BallAgent,” Vector Observation Space Size to `8`, Continuous
    Actions to `2`, and Discrete Branches to `0`, as shown in [Figure 2-20](#fig:ch02_behaviorparameters).
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认行为参数组件已成功添加，并将行为名称更新为“BallAgent”，向量观测空间大小设置为`8`，连续动作设置为`2`，离散分支设置为`0`，如[图 2-20](#fig:ch02_behaviorparameters)所示。
- en: 'We set the Vector Observation Space Size to `8` because there are eight values
    being provided as observations. They are:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向量观测空间大小设置为`8`，因为提供了八个值作为观测。它们是：
- en: The three components of the vector representing the position of the target
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表目标位置的向量的三个组成部分
- en: The three components of the vector representing the position of the agent
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表代理位置的向量的三个组成部分
- en: The single value representing the agent’s X velocity
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表代理X速度的单个值
- en: The single value representing the agent’s Z velocity
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代表代理Z速度的单个值
- en: Check back to [“Letting the Agent Observe the Environment”](#ch02-observations),
    when we added each observation in code, for a refresher of the eight values we’re
    sending as observations.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[“让代理观察环境”](#ch02-observations)，当我们在代码中添加每个观察时，可以为八个值的复习提供帮助。
- en: 'Similarly, we set Continuous Actions to `2` because there are two actions.
    They are:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们将Continuous Actions设置为`2`，因为有两个动作。它们是：
- en: The force being applied on the x-axis
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用在x轴上的力
- en: The force being applied on the z-axis
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用在z轴上的力
- en: Again, check back to [“Letting the Agent Take Actions in the Environment”](#letting-agent-take-actions),
    when we added the `OnActionReceived` method for the code representing two actions.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 再次回到[“让代理在环境中采取行动”](#letting-agent-take-actions)，当我们为表示两个行动的代码添加了`OnActionReceived`方法时。
- en: '![psml 0220](assets/psml_0220.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0220](assets/psml_0220.png)'
- en: Figure 2-20\. Setting up the Behavior Parameters
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-20\. 设置行为参数
- en: It’s a good time to save your scene too.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在也是保存你的场景的好时机。
- en: Providing a Manual Control System for the Agent
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为代理提供手动控制系统
- en: One of the joys of building simulation environments for machine learning using
    a game engine is that you can take control of the agents in the simulation and
    test out the agent’s ability to exist in the environment, and even whether the
    goal is achievable.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用游戏引擎构建机器学习仿真环境的乐趣之一是，你可以控制仿真中的代理并测试代理在环境中存在的能力，甚至测试目标是否可达。
- en: 'To do this, we extend the `Heuristic()` method. Open your agent’s C# code again
    and follow these steps:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们扩展`Heuristic()`方法。再次打开你的代理的C#代码，并按照以下步骤操作：
- en: 'Implement an empty `Heuristic()` method:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个空的`Heuristic()`方法：
- en: '[PRE26]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Warning
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The `in` keyword is a parameter modifier in C#. This means it causes the arguments
    to be passed by reference. In this case, this effectively means you’re working
    directly with the actions of the agent, and not a copy of it that’s then passed
    somewhere else.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在C#中，`in`关键字是一个参数修饰符。这意味着它使参数通过引用传递。在这种情况下，这有效地意味着你直接使用代理的动作，而不是传递到其他地方的副本。
- en: 'Inside, add the following code, mapping the `0` index action to the horizontal
    input of Unity’s input system and the `1` index action to the vertical input (matching
    the way we mapped the `x` and `z` actions earlier, in [“Letting the Agent Take
    Actions in the Environment”](#letting-agent-take-actions)):'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在内部，添加以下代码，将`0`索引动作映射到Unity输入系统的水平输入，并将`1`索引动作映射到垂直输入（与我们在[“让代理在环境中采取行动”](#letting-agent-take-actions)中早些时候映射`x`和`z`动作的方式匹配）：
- en: '[PRE27]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: That’s it. You’ll want to save your code and return to the Unity Editor. Your
    manual control system is hooked up.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 就是这样。你会希望保存你的代码并返回Unity编辑器。你的手动控制系统已连接好。
- en: 'How do you *use* this control system? We’re glad you asked. To use the manual
    control system, you need to do the following:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用这个控制系统？我们很高兴你问了。要使用手动控制系统，你需要按以下步骤操作：
- en: Select the agent in the Hierarchy, and use the Inspector to set the Behavior
    Type to Heuristic Only.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Hierarchy中选择代理，使用Inspector将行为类型设置为Heuristic Only。
- en: Press the Play button in Unity. You can now use the arrow keys on your keyboard
    to control the agent. The agent should reset as expected if it falls into the
    void.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Unity中按下播放按钮。现在，你可以使用键盘上的箭头键来控制代理。如果代理掉入虚空，代理应如预期地重置。
- en: 'You can change the keys that are connected to the `Horizontal` and `Vertical`
    axes that we’re connecting to the actions arrow using Unity:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过Unity更改连接到动作箭头的`Horizontal`和`Vertical`轴的键：
- en: Open the Edit menu → Project Settings… and select Input Manager in the sidebar
    of the Project Settings view.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Edit菜单 → Project Settings…并在Project Settings视图的侧边栏中选择Input Manager。
- en: Find the Horizontal and Vertical axes and the associated Positive and Negative
    buttons, and change the mappings at your leisure, as shown in [Figure 2-21](#fig:ch02_inputunity).
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到Horizontal和Vertical轴以及相关的Positive和Negative按钮，并根据需要更改映射，如[图2-21](#fig:ch02_inputunity)所示。
- en: '![psml 0221](assets/psml_0221.png)'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0221](assets/psml_0221.png)'
- en: Figure 2-21\. Changing the keys mapped to input axes in Unity
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-21\. 在Unity中更改映射到输入轴的键
- en: You should save your scene now, as well.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应保存你的场景。
- en: Training with the Simulation
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用仿真进行训练
- en: Training the simulation is a multistep process, and it involves the creation
    of a configuration file using some provided Unity ML-Agents scripts, and quite
    some time depending on the power of your machine. To train, we’ll be using Python
    and the terminal, but first we need to do a little setup.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模拟是一个多步骤的过程，涉及使用一些提供的Unity ML-Agents脚本创建配置文件，并根据您的计算机性能需要一定的时间。为了训练，我们将使用Python和终端，但首先我们需要进行一些设置。
- en: Specifically, we need to create a YAML file to serve as the hyperparameters
    for our training. Then we’ll run the training using the `mlagent-learn` command
    from the `mlagents` Python package.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们需要创建一个YAML文件作为训练的超参数。然后，我们将使用`mlagents` Python包中的`mlagent-learn`命令来运行训练。
- en: Tip
  id: totrans-290
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: YAML is a useful storage configuration format and is designed to be as human-readable
    as possible. You can learn more about YAML from [Wikipedia](https://oreil.ly/XzIKU).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: YAML是一种有用的存储配置格式，旨在尽可能使人类可读。您可以从[Wikipedia](https://oreil.ly/XzIKU)了解更多关于YAML的信息。
- en: 'So, to train your ball agent, follow these steps:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要训练您的球体代理，请按照以下步骤操作：
- en: 'Create a new file named *BallAgent.yaml*, and include the following hyperparameters
    and values:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为*BallAgent.yaml*的新文件，并包含以下超参数和值：
- en: '[PRE28]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Save the new YAML file somewhere sensible. We like to put it in a *config/*
    directory near where we keep the Unity project, but there’s no need to keep it
    inside the Unity project (although you can if you want to).
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新的YAML文件保存在一个明智的位置。我们喜欢把它放在靠近Unity项目的*config/*目录中，但不必把它放在Unity项目内（尽管如果您愿意的话也可以）。
- en: Tip
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'We’re only using a very small batch and buffer size because this is such a
    simple simulation for training in: there’s not that many inputs and outputs, so
    making the batch and buffer sizes small speeds up the training. A more complex
    simulation environment, reward system, set of observations would warrant a different
    set of hyperparameter values. We’ll talk a little more about the potential hyperparameters
    later, in [Chapter 12](ch12.html#ch12-Beyond).'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们只使用非常小的批处理和缓冲区大小，因为这是一个非常简单的训练模拟：输入和输出不多，所以将批处理和缓冲区大小设得很小可以加快训练速度。更复杂的模拟环境、奖励系统和观测集合将需要不同的超参数值。稍后在[第12章](ch12.html#ch12-Beyond)中我们会更详细地讨论潜在的超参数。
- en: 'Next, in your terminal, inside the `venv` we created in [“Setting Up”](#ch02-setup),
    fire up the training process by running the following command:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 紧接着，在您的终端内，进入我们在[“设置”](#ch02-setup)中创建的`venv`，通过运行以下命令启动训练过程：
- en: '[PRE29]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Replace *config/BallAgent.yaml* with the path to the configuration file we just
    created.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用*config/BallAgent.yaml*替换刚创建的配置文件的路径。
- en: Once the command is up and running, you should see something that looks like
    [Figure 2-22](#fig:ch02_mlagentstraining). At this point, you can press the Play
    button in Unity.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦命令开始运行，您应该会看到类似于[图2-22](#fig:ch02_mlagentstraining)的东西。此时，您可以在Unity中点击播放按钮。
- en: '![psml 0222](assets/psml_0222.png)'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0222](assets/psml_0222.png)'
- en: Figure 2-22\. The ML-Agents process begins training
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-22\. ML-Agents开始训练过程
- en: You’ll know the training process is working when you see output that looks like
    [Figure 2-23](#fig:ch02_mlagentstraining2).
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您看到类似于[图2-23](#fig:ch02_mlagentstraining2)的输出时，您将知道训练过程正在工作中。
- en: Tip
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’re running a fancy Apple Silicon-powered macOS machine, you might want
    to run all of this under Rosetta 2.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您正在运行一台装有高级Apple Silicon芯片的macOS机器，您可能希望在Rosetta 2下运行所有这些操作。
- en: '![psml 0223](assets/psml_0223.png)'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0223](assets/psml_0223.png)'
- en: Figure 2-23\. The ML-Agents process during training
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-23\. 训练过程中的ML-Agents过程
- en: Monitoring the Training with TensorBoard
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorBoard监控训练
- en: 'While it may not look it, the `mlagents-learn` command is using PyTorch under
    the hood. PyTorch under the hood means you can use the amazing suite of Python
    machine learning tools: this doesn’t mean much for such a simple simulation, but
    we can at least discuss how to take a look at what’s going on under the hood during
    the training via TensorBoard.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可能看起来不像，`mlagents-learn`命令在幕后使用的是PyTorch。PyTorch的使用意味着您可以使用出色的一套Python机器学习工具：对于如此简单的模拟来说并没有太大意义，但至少我们可以讨论如何通过TensorBoard查看训练过程中的内部情况。
- en: Note
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Despite TensorBoard originating as part of the TensorFlow project, which is
    a different framework from PyTorch and was originally developed by a totally different
    team, TensorBoard has become a more generic support tool for machine learning
    frameworks and works with many other tools, including PyTorch.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 TensorBoard 最初作为 TensorFlow 项目的一部分起源，这是一个与 PyTorch 不同的框架，并且最初由完全不同的团队开发，但
    TensorBoard 已经成为一个更通用的机器学习框架支持工具，并与许多其他工具一起工作，包括 PyTorch。
- en: 'Follow these steps to monitor the training process via TensorBoard:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤通过 TensorBoard 监控训练过程：
- en: Open an additional terminal and change the directory to the location where the
    Unity ML-Agents Toolkit is installed.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开额外的终端，并切换到安装 Unity ML-Agents Toolkit 的位置。
- en: 'Activate your `venv`, and execute the following command:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活你的 `venv`，并执行以下命令：
- en: '`tensorboard --logdir=results --port=6006`'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`tensorboard --logdir=results --port=6006`'
- en: Tip
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you have issues starting TensorBoard, try installing a fresh copy using the
    command `pip3 install tensorboard` (don’t forget that you’ll need to be inside
    your `venv`!).
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果启动 TensorBoard 遇到问题，请尝试使用命令 `pip3 install tensorboard` 安装最新版本（别忘了你需要在你的 `venv`
    中执行！）。
- en: 'Once TensorBoard is up and running, you can open your web browser and go to
    the following URL: [*http://localhost:6006*](http://localhost:6006).'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 TensorBoard 运行起来，你可以打开你的网页浏览器，并访问以下网址：[*http://localhost:6006*](http://localhost:6006)。
- en: From this browser instance of TensorBoard, you can monitor the training process,
    as shown in [Figure 2-24](#fig:ch02_tensorboard). Of particular relevance at this
    point in your simulation journey are the `cumulative_reward` and `value_estimate`
    statistics, as they show how well the agent is performing the task (based on its
    reward). If `cumulative_reward` and `value_estimate` are approaching `1.0`, it’s
    likely that the agent has solved the problem of reaching the target (because the
    maximum reward the agent can earn is `1.0`).
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这个 TensorBoard 的浏览器实例中，你可以监控训练过程，如 [图 2-24](#fig:ch02_tensorboard) 所示。在你的模拟旅程中，特别相关的是
    `cumulative_reward` 和 `value_estimate` 统计信息，因为它们显示了代理执行任务的表现（基于其奖励）。如果 `cumulative_reward`
    和 `value_estimate` 接近 `1.0`，那么代理很可能已经解决了达到目标的问题（因为代理可以获得的最大奖励是 `1.0`）。
- en: '![psml 0224](assets/psml_0224.png)'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0224](assets/psml_0224.png)'
- en: Figure 2-24\. TensorBoard monitoring the training
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-24\. TensorBoard 监控训练
- en: When the Training Is Complete
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练完成时
- en: Eventually, the training process will complete and will save a *model file*.
    The training process will display a “Saved Model” message when this occurs.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，训练过程将完成，并在保存模型文件时显示 “Saved Model” 消息。
- en: 'Once the model file is saved, follow these steps to bring it into Unity and
    run the simulation using the model (instead of using the simulation to train)
    to watch your agent in action with the trained model:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型文件保存完毕，请按照以下步骤将其导入 Unity 并使用模型运行模拟（而不是使用模拟进行训练），观察你的代理如何运作：
- en: Locate the model file (it will be named *BallAgent.onnx* or *BallAgent.nn*,
    or similar), as shown in [Figure 2-25](#fig:ch02_nnfile).
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位模型文件（它的名称可能是 *BallAgent.onnx* 或 *BallAgent.nn* 等），如 [图 2-25](#fig:ch02_nnfile)
    所示。
- en: '![psml 0225](assets/psml_0225.png)'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0225](assets/psml_0225.png)'
- en: Figure 2-25\. The saved model file
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-25\. 已保存的模型文件
- en: Move the model file into the Unity project, either using your file manager or
    by dragging it into the Unity Project view. You should see the *.onnx* file in
    Unity, as shown in [Figure 2-26](#fig:ch02_trainedmodel).
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型文件移动到 Unity 项目中，可以使用文件管理器或将其拖放到 Unity 项目视图中。你应该在 Unity 中看到 *.onnx* 文件，如 [图 2-26](#fig:ch02_trainedmodel)
    所示。
- en: '![psml 0226](assets/psml_0226.png)'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0226](assets/psml_0226.png)'
- en: Figure 2-26\. The model in Unity
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-26\. Unity 中的模型
- en: Note
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The *.onnx* file that you generated here (and will generate in many other chapters
    in this book) is an Open Neural Network Exchange (ONNX) format file. ONNX is an
    open format for storing machine learning models and provides a common set of operators
    and a common file format to enable AI developers to use models with a variety
    of frameworks, tools, runtimes, and compilers. It’s an exciting open standard
    for machine learning, and you’re using it here!
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里生成的 *.onnx* 文件（以及本书中许多其他章节中生成的文件）是一种开放的神经网络交换（ONNX）格式文件。ONNX 是一种用于存储机器学习模型的开放格式，并提供了一组通用的运算符和共同的文件格式，以使
    AI 开发人员可以在各种框架、工具、运行时和编译器中使用模型。这是一个令人兴奋的开放机器学习标准，你在这里使用它！
- en: Select the agent in Unity, and drag the *.onnx* or *.nn* file from the Project
    view to the Model component in the Inspector. Your agent’s Inspector should look
    like [Figure 2-27](#fig:ch02_trainedmodelonagent).
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检查器中选择代理，并将项目视图中的*.onnx*或*.nn*文件从项目视图拖动到检查器中的模型组件。您的代理检查器应该看起来像[图 2-27](#fig:ch02_trainedmodelonagent)。
- en: '![psml 0227](assets/psml_0227.png)'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![psml 0227](assets/psml_0227.png)'
- en: Figure 2-27\. Attaching the trained model to the agent in Unity
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-27\. 将训练好的模型附加到 Unity 中的代理
- en: Tip
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can also use the circular button next to the model field in the Inspector
    to select from the *.onnx* or *.nn* files available in the project (you’ll only
    have one file to choose from right now, though!).
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您还可以在检查器中的模型字段旁边使用带有圆形按钮，从项目中可用的*.onnx*或*.nn*文件中进行选择（尽管现在只有一个文件可供选择！）。
- en: Run the simulation by pressing the Play button in Unity.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在 Unity 中按下播放按钮来运行模拟。
- en: You should see your agent repeatedly achieving its goal! Great success.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到您的代理反复实现其目标！非常成功。
- en: What’s It All Mean?
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这一切意味着什么？
- en: In this chapter, you built a very simple simulation environment in which a single
    agent is equipped to learn how to roll itself to a target (and ideally not roll
    itself off the edge of the world).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您构建了一个非常简单的模拟环境，在这个环境中，单个代理可以学会如何将自己滚向目标（理想情况下是不要把自己滚到世界的边缘）。
- en: 'This agent was able to observe its environment because `BallAgent` was given
    *observations* about:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代理能够观察其环境，因为`BallAgent`提供了关于以下内容的*观察*：
- en: Its own position
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它自己的位置
- en: Its own `x` velocity
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它自己的`x`速度
- en: Its own `z` velocity
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它自己的`z`速度
- en: The target’s position
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标的位置
- en: It’s important to remember that the observations provided are all an agent can
    know about its environment and the changes of state therein.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 值得记住的是，所提供的观察是代理对其环境以及其中状态变化的全部了解。
- en: Tip
  id: totrans-350
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We turned a simple sphere object in Unity into an agent by attaching a Script
    component to it that extends the `Agent` class. If we hadn’t done that, the sphere
    would not be an agent.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过向 Unity 的一个简单球体对象附加一个扩展`Agent`类的脚本组件，将其转换为一个代理。如果我们没有这样做，这个球就不会成为一个代理。
- en: Like how humans have senses that provide information about the world as we interact
    with it (sight, smell, touch, hearing, and taste) and the state of our own bodies
    (proprioception, interoception, and vestibular sense), agents need the same to
    determine their own state at different points in time and to observe how changes
    in their own state affect the state of the parts of their environment that relate
    to their objectives.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 就像人类通过与世界的互动所获取的感官信息（视觉、嗅觉、触觉、听觉和味觉）以及自身身体状态（本体感知、内感知和前庭感觉）一样，代理需具备相同的能力，以确定它们在不同时间点的状态，并观察自身状态的变化如何影响与其目标相关的环境部分的状态。
- en: 'For a ball that needs to seek a cube, knowing its position, speed, and objective
    position is sufficient. It doesn’t need to be told, for example, that there is
    a floor that ends at a certain point. This is because:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个需要寻找一个立方体的球来说，知道它的位置、速度和目标位置就足够了。例如，它不需要被告知地板在某一点结束。这是因为：
- en: A direct path to the target from any other point on the floor will never lead
    off the sides.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从地板上的任意其他点到目标的直接路径永远不会脱离边缘。
- en: The agent will learn from experience that going past a certain distance in each
    direction results in the episode ending without a reward.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理将通过经验学习，超出每个方向的一定距离将导致剧集结束而没有奖励。
- en: Other types of agents may require many more or complex observations. As we’ll
    see a bit later, ML-Agents has some built-in utilities for hooking agents up with
    cameras or LIDAR-like depth sensors, but you can pass whatever information you
    like to an agent. Observations can be anything you can boil down into a number
    or a numerical array to pass into the `AddObservation` method.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的代理可能需要更多或更复杂的观察。正如稍后将看到的那样，ML-Agents针对将代理与摄像机或类似激光雷达的深度传感器连接的一些内置实用工具，但您可以向代理传递任何您想要的信息。观察可以是您可以将其归结为数字或数字数组并传递到`AddObservation`方法中的任何东西。
- en: Warning
  id: totrans-357
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: It’s important that an agent is given only enough observations to figure out
    what it is supposed to do. Extra or redundant information may seem like it would
    help the agent figure out its task more quickly, but in reality it will give it
    more work to do to figure out which observations are important and correlate with
    success.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，代理只能获得足够的观察来理解它应该做什么。额外或冗余的信息可能看起来会帮助代理更快地解决其任务，但实际上它会让代理更加辛苦地找出哪些观察是重要的并与成功相关。
- en: Maybe the most impressive thing about an agent is that it doesn’t even know
    what its observations *are*; it just gets passed numbers with no field names or
    context. It learns what it needs to by observing how these numbers change as a
    result of various *actions*.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 一个智能体最令人印象深刻的地方可能在于，它甚至不知道自己的观察结果是什么；它只是接收到一些没有字段名或上下文的数字。它通过观察这些数字在各种*动作*下如何变化来学习所需的内容。
- en: Actions are the instructions an agent can follow to effect change in their environment.
    The `BallAgent` had the ability to add or remove force—and therefore roll or brake—in
    either of two directions. Together this allows for complete movement along a 2D
    plane, as shown in [Figure 2-28](#fig:ch02_planemovement).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 动作是智能体可以执行以改变环境的指令。`BallAgent`能够添加或移除力量，从而在两个方向上进行滚动或制动。这使得在二维平面上完成完整的移动成为可能，如图[2-28](#fig:ch02_planemovement)所示。
- en: '![psml 0228](assets/psml_0228.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![psml 0228](assets/psml_0228.png)'
- en: Figure 2-28\. Range of movement given two axes of potential force
  id: totrans-362
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-28\. 在两个潜力力方向上的运动范围
- en: But actions can be anything that enacts change in the environment, from physical
    movement of the agent (e.g., roll ball, move joint, flap wings, turn wheels),
    to high-level actions (e.g., walk forward one meter, stand up, turn around 180
    degrees), to changes in the environment (e.g., pick up object agent is standing
    on, open door in front of agent). The granularity and extent of the actions an
    agent should be given access to will depend on the specific task you wish it to
    learn.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 但是动作可以是任何导致环境变化的事物，从智能体的物理移动（例如，滚球、移动关节、扑打翅膀、转动轮子）到高级动作（例如，向前走一米、站起来、转身180度），再到环境的变化（例如，拾起智能体所站的物体、打开智能体面前的门）。智能体应该获得的动作的粒度和范围将取决于您希望它学习的具体任务。
- en: For example, if you want an agent to learn to walk, you probably don’t want
    its action to be something like “walk forward.” Because then you’ll have to write
    code to make it walk, and it won’t need to learn. But if you want your agent to
    learn how to complete a maze, you might provide coded instructions for movement
    so that it doesn’t have to learn how to walk and *then* learn how the maze works.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您希望一个智能体学会走路，您可能不希望它的动作是“向前走”。因为那样您将不得不编写代码让它走路，它就不需要学习了。但是，如果您希望您的智能体学会如何完成迷宫，您可以提供移动的编码指令，这样它就不必学会如何走路，然后再学习迷宫的工作。
- en: Tip
  id: totrans-365
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Think back to how we implemented the `OnActionReceived` method on `BallAgent`
    by explicitly adding force in the required direction. It didn’t need to learn
    how to add force to its own `RigidBody`; instead, it learned when it is appropriate
    to *decide* to move along either axis. Thus, any actions you wish to give an agent
    will need to be implemented by you; what the agent is learning is *when* and *in
    what order* to trigger the actions you give it to choose between.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们如何在`BallAgent`上实现`OnActionReceived`方法，通过明确在所需方向上添加力量。它不需要学习如何向自己的`RigidBody`添加力量；相反，它学习何时适合*决定*沿着哪个轴移动。因此，您希望给智能体的任何动作将需要由您实现；智能体学习的是何时以及以何种顺序触发您给它选择的动作。
- en: 'An agent’s job is to use the observations it has to assess the state of the
    environment in order to decide on the best response, and then execute the required
    series of actions. We guide the agent to select the desired response to various
    environmental states—and therefore work toward our intended objectives for the
    agent—using *rewards*. In the case of `BallAgent`, there is only one reward: +1.0
    when the agent reaches the target.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体的任务是利用其观察结果来评估环境的状态，以决定最佳响应，并执行必要的一系列动作。我们引导智能体选择对各种环境状态的所需响应，从而朝向我们对智能体的预期目标前进，使用*奖励*。在`BallAgent`的案例中，只有一个奖励：当智能体达到目标时为+1.0。
- en: Rewards are simply points awarded to or taken from the agent when certain conditions
    are met. Some rewards are intrinsic, such as in imitation learning where rewards
    are given automatically for similarity to example behavior. In reinforcement learning,
    explicit rewards must be defined.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励只是在满足某些条件时授予或收回智能体的分数。有些奖励是内在的，例如在模仿学习中，奖励是自动给予的，用于与示例行为的相似性。在强化学习中，必须定义显式奖励。
- en: These rewards are the only feedback an agent will receive about its behavior,
    so the rewards and the trigger conditions must be carefully decided. Over time,
    an agent will generally optimize for the behavior that has historically gotten
    it the most points, and this means a poorly designed rewards scheme can result
    in unexpected behavior.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这些奖励是代理程序将获得的唯一关于其行为的反馈，因此必须谨慎决定奖励和触发条件。随着时间的推移，代理程序通常会优化历史上给它带来最多分数的行为，这意味着设计不当的奖励方案可能导致意外的行为。
- en: For example, say we decided to help `BallAgent` along by giving it points for
    *approaching* the target. This might speed up training, as the time it takes for
    the agent to receive its first reward and get a hint of what it should be doing
    will be much earlier than if it is only rewarded for reaching the target.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们决定通过奖励`BallAgent` *接近* 目标来帮助它。这可能会加快训练，因为代理程序收到第一个奖励并得知应该做什么的时间会比仅奖励它达到目标要早得多。
- en: So, you decide that for this agent that starts out some distance away from the
    target (say, 10 units), you will reward it for each step based on how close it
    is to the target. When it has reached the target, the reward will be +10.0; when
    it is one unit away, it will receive +9.0, and so on. At the first step of the
    episode, it will not have moved and will receive +0.0.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您决定对于这个起始距离目标一定距离的代理（比如说，10个单位），您将根据它距离目标的接近程度来奖励它每一步。当它达到目标时，奖励为+10.0；当它距离目标一单位时，它将获得+9.0，依此类推。在第一步时，它还未移动，将收到+0.0的奖励。
- en: After some training, the average rewards the agent receives for each episode
    are high, so you end training. You start the agent up in inference mode and you
    see that during each episode, the agent approaches the object and then circles
    around it without touching it until the episode reaches its length limit and ends.
    What went wrong?
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一些训练后，代理程序每个episode平均接收的奖励很高，因此您结束了训练。您将代理程序启动到推断模式，并发现在每个episode期间，代理程序会接近对象，然后围绕它旋转而不触及它，直到episode达到其长度限制并结束。出了什么问题？
- en: 'You have a think and realize you’ve fallen into an easy trap in rewards design:
    you’ve made the agent optimize for the wrong thing. Three factors have come together
    to create this mess:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 您经过思考意识到，在奖励设计中掉入了一个简单的陷阱：您让代理程序优化了错误的事物。三个因素共同导致了这一混乱：
- en: Though an agent does consider how much it has to gain from potential actions
    at each step, it will tend to optimize for *the most points it can get in a whole
    episode*.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然代理程序确实考虑了每一步潜在行动的潜在收益，但它倾向于优化*整个episode中可以获得的最高分数*。
- en: 'In an ideal episode, `BallAgent` will go straight toward the target and reach
    it. Assuming it rolls about one unit per step, the agent will receive steadily
    increasing rewards: +0.0 in the first step, +1.0 in the second, all the way up
    to +10.0 when the agent reaches the goal, and *the episode ends after 10 steps
    with a reward of 55.0*.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在理想的episode中，`BallAgent`将直接向目标前进并达到它。假设它每步移动约一单位，那么代理将稳步增加奖励：第一步为+0.0，第二步为+1.0，直到代理到达目标时为+10.0，并且*episode在10步后以55.0的奖励结束*。
- en: If the `BallAgent` instead approaches to one unit away from the target, it will
    receive the same rewards for the first nine steps of the simulation (totaling
    45.0). Then, circling the target at this distance, it will receive +9.0 for each
    step the simulation allows it to continue. With `max_steps=500000` in our hyperparameters
    file, this behavior will continue for the remaining 499,991 steps and *the episode
    will forcibly end after half a million steps, with a reward of 4,499,964.0*.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`BallAgent`代替接近目标一单位，它将在仿真的前九步中获得相同的奖励（总计45.0）。然后，在此距离围绕目标旋转，它将每一步仿真允许继续获得+9.0。在我们的超参数文件中，`max_steps=500000`，此行为将在剩余的499,991步中继续，*episode将在达到半百万步后强制结束，并获得4,499,964.0的奖励*。
- en: You can see how the agent—given only reward points to go by—might think circling
    is the preferred behavior rather than touching the target.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，只给予奖励点数的代理程序可能认为绕圈是优选的行为，而不是触摸目标。
- en: 'So there you have it: simulation is built upon three main concepts—observations,
    actions, and rewards. With these three things, all manner of intelligent behavior
    can be learned by a simulated agent. An agent uses experience of observations
    it has collected, actions it has taken, and rewards it has received as it explores
    the environment it is placed in. At first, actions will be random, then will become
    more targeted as rewards are received and lessons are therefore learned about
    what the desirable behaviors are in the current scenario. During *training*, this
    behavior model of the agent is ever-changing, updating as reward feedback is received,
    but once training is over (when the agent runs in inference mode), it is locked
    in place and that will be the behavior the agent exhibits forever.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模拟是建立在三个主要概念上的：观察、行动和奖励。有了这三样东西，模拟代理可以学会各种智能行为。代理利用它收集到的观察经验、采取的行动以及获得的奖励来探索所处的环境。起初，行动会是随机的，随后随着获得奖励和学到的教训，行动将变得更加有针对性，适应当前场景中的理想行为。在*训练*过程中，代理的这种行为模型是不断变化的，随着奖励反馈的接收而更新，但一旦训练结束（当代理以推理模式运行时），它就被锁定了，那将是代理永远展示的行为。
- en: This model, which acts as a mapping of observations to the actions that will
    yield the highest rewards in that moment, is called a *policy*. This is the conventional
    term for the behavior model in reinforcement learning, but it’s also a class name
    in ML-Agents. The `Policy` class abstracts the decision-making process in a way
    that lets you swap what’s making the decision. That is what allows switching between
    heuristic control—which allowed you to control the agent with Unity’s input system
    earlier—and control via a neural network, as seen in [“When the Training Is Complete”](#training-complete).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型，它将观察映射到在那一刻产生最高奖励的行动上，被称为*策略*。这是强化学习中行为模型的传统术语，但也是ML-Agents中的一个类名。`Policy`类以一种方式抽象出决策过程，使你可以切换决策方式。这就是允许在启发式控制之间切换的东西——之前允许你使用Unity的输入系统控制代理——以及通过神经网络控制，如在[“训练完成时”](#training-complete)中所见。
- en: Coming Up Next
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 即将来临
- en: Simulation-based machine learning with ML-Agents is explored further throughout
    [Part II](part02.html#part_2) of this book, beginning with our next simulation
    activity in [Chapter 4](ch04.html#chapter-more-advanced-simulation). There we’ll
    build on what you’ve learned here to create a more complex simulation environment,
    with an agent that knows a lot more about its situation than the rolling ball
    in this example.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的[第二部分](part02.html#part_2)进一步探讨了基于模拟的机器学习与ML-Agents，在[第四章](ch04.html#chapter-more-advanced-simulation)中开始我们下一个模拟活动。在这里，我们将建立在你所学的基础上，创建一个更复杂的模拟环境，其中的代理比这个例子中的滚球更了解它的情况。
- en: You’ll learn how to feed multiple kinds of observations into your agent, and
    take advantage of the game engine to send *visual* input into your agent instead
    of raw numbers.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习如何将多种观察输入到你的代理中，并利用游戏引擎将*视觉*输入发送给你的代理，而不是原始数字。
- en: We’ll also look at how you can speed up the training process in more complex
    situations, by duplicating the simulation environment and training in parallel
    across many similar agents so that the neural network model can receive far more
    experiences. Exciting, right?
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨在更复杂的情况下如何加速训练过程，通过复制模拟环境并在许多类似代理之间并行训练，以便神经网络模型可以获得更多经验。令人兴奋，对吧？
